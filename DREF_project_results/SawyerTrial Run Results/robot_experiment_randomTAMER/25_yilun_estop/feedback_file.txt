Current timestep = 0. State = [[-0.2312745  -0.01584596]]. Action = [[-0.13562268  0.00687441  0.188631   -0.05632639]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.22633703 -0.01491374]]. Action = [[0.23068064 0.04803723 0.24686724 0.59094584]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.21435934 -0.00081217]]. Action = [[ 0.04332206  0.21872604  0.18178022 -0.18622363]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.19897558  0.02733848]]. Action = [[0.23367065 0.23120895 0.01779473 0.14667153]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Current timestep = 4. State = [[-0.18313755  0.04356878]]. Action = [[-0.10859254 -0.03227159 -0.18620792  0.05610216]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.18563454  0.04740742]]. Action = [[-0.08199739  0.02034625  0.14626229  0.41070712]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.19472131  0.05296971]]. Action = [[-0.16611288  0.02429155  0.17822182  0.9278029 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.21031065  0.06203132]]. Action = [[-0.11630061  0.07292765  0.13747287  0.9722059 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of 0
Current timestep = 8. State = [[-0.2214307  0.0639111]]. Action = [[ 0.08181846 -0.06014179  0.18047935  0.28973234]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Current timestep = 9. State = [[-0.21548034  0.05060047]]. Action = [[ 0.11478856 -0.16841547  0.22267625 -0.73225546]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Current timestep = 10. State = [[-0.21408796  0.04810352]]. Action = [[-0.05629939  0.15777236 -0.20095429 -0.46314353]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.20984103  0.0389109 ]]. Action = [[ 0.13166338 -0.24294709  0.02038759  0.26889992]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.20404604  0.0136862 ]]. Action = [[-0.0586288  -0.22114731  0.07108957 -0.44619203]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.21283619 -0.01916357]]. Action = [[-0.23631042 -0.23521547 -0.00660443  0.06039369]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Current timestep = 14. State = [[-0.22258651 -0.02730787]]. Action = [[-0.00451422  0.20335174 -0.10703819 -0.5450805 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.2208036  -0.01518626]]. Action = [[ 0.17446342  0.07611847 -0.01924029 -0.9444256 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.20531632 -0.01927115]]. Action = [[ 0.21933287 -0.17546676 -0.02590071  0.27641964]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Current timestep = 17. State = [[-0.19732352 -0.02762632]]. Action = [[-0.2289372  -0.00920078  0.1263389  -0.3634929 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.21423762 -0.01727625]]. Action = [[-0.24011977  0.21948981  0.07746482  0.27787638]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Current timestep = 19. State = [[-0.2401881  -0.00493487]]. Action = [[-0.21455619 -0.03536409  0.22789598  0.3179202 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of 0
Current timestep = 20. State = [[-0.26544487 -0.00466882]]. Action = [[-0.1277481  -0.14361843 -0.0020427   0.7127273 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
Current timestep = 21. State = [[-0.26544487 -0.00466882]]. Action = [[-0.11288179  0.19019902 -0.13644937  0.3947338 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
Current timestep = 22. State = [[-0.26544487 -0.00466882]]. Action = [[-0.14513128  0.24661121  0.1566869   0.8318193 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
Current timestep = 23. State = [[-0.26539004 -0.00467691]]. Action = [[-0.20349918 -0.16464151 -0.20786256  0.5646374 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
Current timestep = 24. State = [[-0.26539004 -0.00467691]]. Action = [[-0.17683549 -0.22997853  0.05710876  0.3367455 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
Current timestep = 25. State = [[-0.25951585 -0.00479951]]. Action = [[ 0.22011083 -0.0194637   0.18748957  0.41771352]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
Current timestep = 26. State = [[-0.24417509 -0.01851393]]. Action = [[ 0.16441342 -0.22409627 -0.11296496  0.2547096 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
Current timestep = 27. State = [[-0.22841273 -0.02279638]]. Action = [[0.05595574 0.22684464 0.19466996 0.4615512 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.22910266 -0.0115252 ]]. Action = [[-0.19282441 -0.04479621 -0.19678871 -0.44809842]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.2259783  -0.02346399]]. Action = [[ 0.23656973 -0.16995405 -0.18895786 -0.5922812 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.20743513 -0.02348356]]. Action = [[ 0.22985312  0.19866312 -0.13274622  0.95928323]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.1926049  -0.01120093]]. Action = [[-0.11835574  0.06577677  0.13588375  0.4627515 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.20312631  0.00386065]]. Action = [[-0.24035874  0.11258629 -0.11325234 -0.3781469 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.21055068  0.02574517]]. Action = [[0.16925502 0.20259264 0.1267514  0.8311167 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.2011753   0.04647759]]. Action = [[ 0.18724424  0.13161859 -0.24506418  0.6404829 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.18905953  0.06861591]]. Action = [[-0.16353078  0.11425954 -0.01354265 -0.9745949 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.19283876  0.07048117]]. Action = [[ 0.01201025 -0.13780704 -0.02788211 -0.53508556]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 0
Current timestep = 37. State = [[-0.19187196  0.06426106]]. Action = [[ 0.10718238  0.02769125  0.17953598 -0.00802326]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.18528678  0.05693543]]. Action = [[ 0.1096231  -0.13608222 -0.01980829  0.13730788]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.1758458  0.0439011]]. Action = [[ 0.00396022 -0.06822875  0.07706299 -0.9145444 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.17938013  0.02550513]]. Action = [[-0.1937469  -0.20802389 -0.04015021 -0.75368404]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.18299744  0.00352334]]. Action = [[ 0.05484751 -0.1340792   0.04797131 -0.7304812 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Current timestep = 42. State = [[-0.17982717 -0.0106405 ]]. Action = [[ 0.12607723 -0.04817186  0.06768733  0.7008283 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.17015514 -0.02182693]]. Action = [[ 0.13516182 -0.05193098  0.1918661  -0.20461094]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.15345965 -0.03096877]]. Action = [[ 0.12067896 -0.09647149  0.14042729 -0.40582538]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.13034551 -0.04666818]]. Action = [[ 0.22559917 -0.13097283  0.16518533  0.5303633 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Current timestep = 46. State = [[-0.11815567 -0.0490531 ]]. Action = [[-0.21137118  0.14802253  0.07556167 -0.94265676]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Current timestep = 47. State = [[-0.11730879 -0.03579028]]. Action = [[ 0.15137932  0.1452567  -0.0357403  -0.33488762]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.11876159 -0.0311939 ]]. Action = [[-0.23157582 -0.09270063  0.03966215  0.03930581]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.13026461 -0.03912261]]. Action = [[-0.0935898  -0.08987594  0.1999504  -0.90841   ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.13285139 -0.03305703]]. Action = [[ 0.22327149  0.24123949 -0.08045051  0.88470376]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.11825319 -0.02256362]]. Action = [[ 0.1678195  -0.15670656 -0.17962196 -0.04843926]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Current timestep = 52. State = [[-0.09759928 -0.03339992]]. Action = [[ 0.23794869 -0.05797866  0.21347511 -0.4441625 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.07654059 -0.04937961]]. Action = [[-0.20123564 -0.1521945  -0.08573642 -0.4923566 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.08888547 -0.04870351]]. Action = [[-0.24074255  0.20042259 -0.18789165  0.63383114]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.10746719 -0.02460135]]. Action = [[ 0.15198106  0.23694408 -0.03178369 -0.16855347]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.11137685 -0.00497723]]. Action = [[-0.15852001  0.00193214  0.12612635 -0.3840475 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Current timestep = 57. State = [[-0.11639821  0.00715877]]. Action = [[-0.07819632  0.15576851  0.07214728 -0.03876507]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.1163892   0.03170431]]. Action = [[ 0.23737222  0.21554351  0.01991296 -0.5668598 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Current timestep = 59. State = [[-0.11409193  0.04708723]]. Action = [[-0.1383368  -0.00938779 -0.08162974 -0.45847696]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.11355849  0.05848101]]. Action = [[ 0.12956035  0.14732563  0.09147799 -0.01550174]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.10303939  0.06976078]]. Action = [[0.18919513 0.03620422 0.10653666 0.64754033]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-0.09386849  0.06870878]]. Action = [[-0.18924612 -0.13158323 -0.23080699  0.3473158 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.09344272  0.06146027]]. Action = [[ 0.11272526 -0.04015712  0.07744843 -0.49971032]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.09567155  0.05974244]]. Action = [[-0.17921689  0.04678187  0.0715324  -0.41774154]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.09985497  0.04830517]]. Action = [[ 0.01324627 -0.22667627  0.14947915  0.25286496]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.10507914  0.04329354]]. Action = [[-0.05517417  0.1391148  -0.20748247  0.955194  ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Current timestep = 67. State = [[-0.10423502  0.03698651]]. Action = [[ 0.08092248 -0.19311155 -0.02400871  0.38305724]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.10176516  0.02466454]]. Action = [[ 0.0131422  -0.0116453   0.24304736  0.43601942]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Current timestep = 69. State = [[-0.10881272  0.01989146]]. Action = [[-0.22488713 -0.05333693  0.22866678  0.1399225 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.13358726  0.00094836]]. Action = [[-0.21993731 -0.18395813 -0.051819   -0.6400707 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Current timestep = 71. State = [[-0.14990172 -0.01173443]]. Action = [[ 0.01676127  0.01048014  0.11630303 -0.5772726 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.1461034  -0.00083999]]. Action = [[ 0.18890384  0.2314418   0.20439503 -0.8650547 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.14801607  0.0096696 ]]. Action = [[-0.15507206 -0.05290085 -0.19813205 -0.3197698 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.14433722  0.00495891]]. Action = [[ 0.22056925 -0.10460702  0.24364203 -0.6966599 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.14161645  0.00453617]]. Action = [[-0.08049728  0.10126704  0.15154648  0.9689983 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.14625616  0.00198029]]. Action = [[-0.14020094 -0.11608243 -0.19086018  0.86970484]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.1541201 -0.0130558]]. Action = [[-0.12186268 -0.17187056 -0.11778301  0.13879895]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Current timestep = 78. State = [[-0.16156441 -0.03976455]]. Action = [[ 0.07862735 -0.21805623 -0.17796037 -0.81659883]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.15908873 -0.06565875]]. Action = [[ 0.02896193 -0.08899775  0.23775423 -0.04103428]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.15096173 -0.08011909]]. Action = [[ 0.22267568 -0.08908537 -0.16770342  0.47297692]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.13489331 -0.09564332]]. Action = [[ 0.18483135 -0.16203058 -0.0120139   0.08166063]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.11064153 -0.11493199]]. Action = [[ 0.16692388 -0.08328894  0.19431949 -0.6451171 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.09419465 -0.1133861 ]]. Action = [[-0.00093749  0.16663605  0.01150817 -0.8189214 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.09295632 -0.1088351 ]]. Action = [[-0.09540521 -0.04333562  0.01940551 -0.17278695]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Current timestep = 85. State = [[-0.08998419 -0.11375426]]. Action = [[ 0.16748708 -0.09633523  0.02838489 -0.5823394 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Current timestep = 86. State = [[-0.07774411 -0.10696401]]. Action = [[ 0.11411479  0.2053842   0.06604755 -0.5773834 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.068098   -0.10447387]]. Action = [[-0.04494001 -0.14102311  0.1478968   0.35668182]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.07485665 -0.11062703]]. Action = [[-0.21934113 -0.00177751  0.09725517 -0.4537592 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Current timestep = 89. State = [[-0.08657391 -0.10300793]]. Action = [[-0.192979    0.1880151   0.18898606  0.6065899 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.10929035 -0.08494695]]. Action = [[-0.11727832  0.08259562  0.20373362 -0.7884208 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Current timestep = 91. State = [[-0.12676226 -0.08563673]]. Action = [[-0.11355816 -0.10382994 -0.03479646  0.2965132 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.13329776 -0.08133779]]. Action = [[ 0.11326408  0.16327119 -0.16011308  0.4229467 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.12566374 -0.06621341]]. Action = [[ 0.20732546  0.08933109 -0.02670437 -0.76362664]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.11470596 -0.05765351]]. Action = [[-0.09572184  0.00235587 -0.12444499  0.75245404]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.10969937 -0.05892015]]. Action = [[ 0.21180218 -0.07242151  0.2481857   0.33036613]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.09164228 -0.06509801]]. Action = [[ 0.24458823 -0.09257309  0.16045785  0.99362123]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.07481758 -0.07193957]]. Action = [[-0.05636118 -0.0072251  -0.05161297 -0.6756053 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.0746525  -0.08521652]]. Action = [[-0.02453017 -0.21098936 -0.05241427 -0.19450814]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.08099411 -0.11160712]]. Action = [[-0.23739615 -0.14997923  0.04699367 -0.39691597]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.10174683 -0.13559553]]. Action = [[-0.18095532 -0.14591552 -0.20708686 -0.68353003]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Current timestep = 101. State = [[-0.11598439 -0.15870439]]. Action = [[ 0.0374071  -0.17500944 -0.16268939  0.80996406]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
Current timestep = 102. State = [[-0.12060741 -0.16577557]]. Action = [[-0.12417537  0.16426814 -0.03010529 -0.60630625]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
Current timestep = 103. State = [[-0.12419687 -0.1577494 ]]. Action = [[0.01680964 0.05227679 0.20144767 0.84387636]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
Current timestep = 104. State = [[-0.12236544 -0.16221298]]. Action = [[ 0.14057988 -0.18987238 -0.05763604  0.13334954]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
Current timestep = 105. State = [[-0.12179742 -0.16850789]]. Action = [[-0.0811884   0.03904617  0.19742024 -0.8435452 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
Current timestep = 106. State = [[-0.1160762  -0.15916108]]. Action = [[ 0.18631133  0.1736666  -0.03009018  0.19731677]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
Current timestep = 107. State = [[-0.11747262 -0.1453112 ]]. Action = [[-0.2418266   0.0819779   0.06347483 -0.23701829]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
Current timestep = 108. State = [[-0.12300931 -0.14537804]]. Action = [[ 0.03406739 -0.1481119  -0.21645676  0.56089175]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
Current timestep = 109. State = [[-0.12745759 -0.15763143]]. Action = [[-0.09509718 -0.10731846  0.2164343   0.56549394]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
Current timestep = 110. State = [[-0.12549819 -0.1703696 ]]. Action = [[ 0.24661908 -0.08670869 -0.12970315 -0.88962466]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
Current timestep = 111. State = [[-0.11791559 -0.18921117]]. Action = [[ 0.06236696 -0.20730019  0.16252023  0.9658203 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
Current timestep = 112. State = [[-0.10450282 -0.19805725]]. Action = [[ 0.14650476  0.14014727 -0.22896484  0.7760484 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.08806521 -0.20110247]]. Action = [[ 0.08677989 -0.1817151  -0.12512685  0.9558946 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.08197907 -0.20872414]]. Action = [[-0.22709274  0.14031488 -0.15116662  0.56541824]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
Current timestep = 115. State = [[-0.08883113 -0.21097796]]. Action = [[ 0.00988999 -0.1270568  -0.10073836 -0.77681863]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.09721354 -0.21483764]]. Action = [[-0.12433827  0.03394538 -0.17667043 -0.97335994]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
Current timestep = 117. State = [[-0.10161072 -0.22614646]]. Action = [[ 0.17482579 -0.24108072  0.17145485  0.7145282 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0157, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.1000782  -0.24014807]]. Action = [[ 0.01306486  0.05853105  0.2342898  -0.30463743]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0190, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.09984594 -0.22803706]]. Action = [[-0.14687921  0.2396931   0.14826581  0.9399204 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
Current timestep = 120. State = [[-0.10110396 -0.2095343 ]]. Action = [[ 0.0678131   0.03543824  0.23630655 -0.9432411 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.08939003 -0.19121459]]. Action = [[ 0.21061563  0.19858947 -0.12086517  0.9736873 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
Current timestep = 122. State = [[-0.08769083 -0.17259584]]. Action = [[-0.21703652  0.07484081 -0.10122065  0.19026113]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
Current timestep = 123. State = [[-0.09877428 -0.16340667]]. Action = [[-0.14832938  0.04206127 -0.19022204 -0.96116656]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
Current timestep = 124. State = [[-0.10682308 -0.15935019]]. Action = [[ 0.05532393 -0.0160432  -0.11061881  0.47189295]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
Current timestep = 125. State = [[-0.11735946 -0.17146927]]. Action = [[-0.17833076 -0.21225643 -0.03114419  0.05945778]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
Current timestep = 126. State = [[-0.18885764 -0.16929476]]. Action = [[-0.21374586  0.21468657 -0.11649573  0.5315161 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 0
Current timestep = 127. State = [[-0.1804288 -0.1776073]]. Action = [[-0.06562878  0.22011063  0.11126637  0.2835046 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, True, False, False]
Current timestep = 128. State = [[-0.18064828 -0.16145042]]. Action = [[-0.01850933  0.09625778 -0.15464157 -0.2701031 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 128 is [True, False, False, True, False, False]
Current timestep = 129. State = [[-0.17298591 -0.14333606]]. Action = [[ 0.24444103  0.13598078  0.08357415 -0.36446333]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 129 is [True, False, False, True, False, False]
Current timestep = 130. State = [[-0.16480935 -0.13253853]]. Action = [[-0.16040234  0.01898929 -0.16327992 -0.39460903]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 130 is [True, False, False, True, False, False]
Current timestep = 131. State = [[-0.15890928 -0.12216173]]. Action = [[ 0.23620406  0.10052586 -0.07128328  0.8833604 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.1411072  -0.09591352]]. Action = [[ 0.17221159  0.24413103  0.18169245 -0.7401004 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 132 is [True, False, False, False, True, False]
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.1149523  -0.07144356]]. Action = [[ 0.18061781  0.09546253 -0.08805735 -0.8739159 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 133 is [True, False, False, False, True, False]
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.08883493 -0.06073835]]. Action = [[ 0.12231204  0.05589157  0.13168234 -0.01821846]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 134 is [True, False, False, False, True, False]
Current timestep = 135. State = [[-0.08263094 -0.06342393]]. Action = [[-0.09915352 -0.11299996  0.05365038 -0.5130279 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.08479773 -0.05407753]]. Action = [[-0.01069744  0.24263829  0.179183    0.5318804 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.09314513 -0.04325023]]. Action = [[-0.22227089 -0.11359073  0.12672889 -0.34589684]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 137 is [True, False, False, False, True, False]
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.09905714 -0.0613986 ]]. Action = [[ 0.20189065 -0.18008457 -0.17194636  0.34901476]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.10179726 -0.05807557]]. Action = [[-0.24035214  0.23579091  0.08747816  0.6300311 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 139 is [True, False, False, False, True, False]
Current timestep = 140. State = [[-0.11381498 -0.03314705]]. Action = [[-0.11342616  0.19595507 -0.1029081  -0.62873626]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.12838578 -0.00701839]]. Action = [[-0.17589687  0.127617   -0.1643101   0.5806732 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.13790111 -0.00646235]]. Action = [[ 0.21283829 -0.16899061  0.1607601   0.9585782 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.14167117 -0.02467024]]. Action = [[-0.21506174 -0.15766455  0.13348043  0.26850927]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.14954023 -0.04030877]]. Action = [[-0.00977696 -0.04477395  0.22152945 -0.42291766]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.15876868 -0.05369766]]. Action = [[-0.16789968 -0.12102956  0.01438287  0.51559806]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.1724662  -0.07348151]]. Action = [[ 0.07564163 -0.13896994 -0.03370641  0.78125954]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 146 is [True, False, False, False, True, False]
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.17040944 -0.0717245 ]]. Action = [[ 0.00921923  0.2177465   0.03409156 -0.09870571]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.17123087 -0.06402422]]. Action = [[ 0.00411162 -0.09571001 -0.08140777  0.01788759]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 148 is [True, False, False, False, True, False]
Current timestep = 149. State = [[-0.16836463 -0.05762967]]. Action = [[ 0.11157233  0.14303708  0.19136316 -0.7531068 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.15957864 -0.03986564]]. Action = [[ 0.13868576  0.1927498  -0.15419735 -0.8541523 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 150 is [True, False, False, False, True, False]
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(9.0380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.14804192 -0.02977764]]. Action = [[-0.1501155  -0.15289484 -0.03135966 -0.46031946]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.16097696 -0.03793232]]. Action = [[-0.22908348  0.00410071  0.05773497 -0.22987515]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 152 is [True, False, False, False, True, False]
Current timestep = 153. State = [[-0.1790109  -0.03233309]]. Action = [[-0.09118712  0.12431124  0.13777381  0.28627443]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.18313336 -0.03628213]]. Action = [[ 0.13654983 -0.2048678   0.13438272 -0.36819863]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 154 is [True, False, False, False, True, False]
Current timestep = 155. State = [[-0.18397798 -0.03785792]]. Action = [[-0.06299864  0.1491468  -0.10617605 -0.16379654]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.19650505 -0.02717191]]. Action = [[-0.24475515  0.08091471 -0.20385171  0.46456945]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.21612239 -0.03049901]]. Action = [[-0.05324104 -0.17196147  0.04741862 -0.5871623 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 157 is [True, False, False, False, True, False]
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.23810744 -0.0343807 ]]. Action = [[-0.23056474  0.12834832 -0.23169148  0.31748176]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.25654736 -0.01427388]]. Action = [[ 0.13448626  0.20686144 -0.10551871  0.04506648]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.2562599   0.00075088]]. Action = [[-0.1656233   0.1497423  -0.12961979  0.3744166 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 160 is [True, False, False, False, True, False]
Current timestep = 161. State = [[-0.25544688  0.00044714]]. Action = [[-0.01017126 -0.05347727 -0.02803035  0.14039254]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 161 is [True, False, False, False, True, False]
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of 0
Current timestep = 162. State = [[-0.25450417 -0.00049399]]. Action = [[-0.16071783  0.20417738  0.00608143 -0.60968953]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 162 is [True, False, False, False, True, False]
Current timestep = 163. State = [[-0.2549089   0.00095822]]. Action = [[ 0.00366205  0.0637334  -0.24671641  0.88516366]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.2537613  -0.00472731]]. Action = [[ 0.01385799 -0.15555522 -0.20987847 -0.23040241]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.25078797 -0.017507  ]]. Action = [[ 0.0338577  -0.10647246 -0.10752681 -0.6995107 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 165 is [True, False, False, False, True, False]
Current timestep = 166. State = [[-0.24962787 -0.02558281]]. Action = [[-0.23557793  0.15237117  0.11733854  0.33292007]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.24967125 -0.02764002]]. Action = [[-0.15702778 -0.18920825  0.21661222  0.34453988]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 167 is [True, False, False, False, True, False]
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.24237126 -0.01706173]]. Action = [[ 0.18888086  0.21869713 -0.06427866  0.9087846 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.23487127 -0.00104989]]. Action = [[-0.17916933  0.01001054  0.08175263  0.745662  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 169 is [True, False, False, False, True, False]
Current timestep = 170. State = [[-0.23808137  0.00219685]]. Action = [[ 0.06092316  0.04192322  0.15616924 -0.6087163 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.24221471  0.01175338]]. Action = [[-0.12507623  0.09877393  0.02132696 -0.81954783]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.25215042  0.01050193]]. Action = [[-0.09380621 -0.1627628   0.02278572  0.17429101]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 172 is [True, False, False, False, True, False]
Current timestep = 173. State = [[-0.25659457 -0.00142341]]. Action = [[ 0.0472365  -0.06338659 -0.21238822 -0.04066581]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.2566854  -0.00508136]]. Action = [[-0.22049102  0.20086616 -0.20076402 -0.2068221 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 174 is [True, False, False, False, True, False]
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.25806355 -0.00139073]]. Action = [[ 0.00719589  0.12605351 -0.12095131  0.6992717 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 175 is [True, False, False, False, True, False]
Current timestep = 176. State = [[-0.2602465   0.00633962]]. Action = [[-0.04228391  0.04027382  0.17993277 -0.88307863]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 176 is [True, False, False, False, True, False]
Current timestep = 177. State = [[-0.2529464   0.00140341]]. Action = [[ 0.23130575 -0.18640663 -0.09786431  0.8531797 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.24474697 -0.0160386 ]]. Action = [[-0.03596707 -0.12803264 -0.0834884  -0.69124126]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.23701605 -0.02669289]]. Action = [[ 0.16488135 -0.00529765  0.16726327 -0.8094474 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 179 is [True, False, False, False, True, False]
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.221105   -0.02722862]]. Action = [[ 0.11859226  0.04074022  0.13635987 -0.5549719 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 180 is [True, False, False, False, True, False]
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.20044461 -0.01706506]]. Action = [[ 0.20462877  0.15614617 -0.22174133  0.17426348]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 181 is [True, False, False, False, True, False]
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.18487504  0.00696497]]. Action = [[-0.1424573   0.21985877  0.17344415 -0.704646  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.1891802   0.02041798]]. Action = [[ 0.00872636 -0.04783808  0.20680106 -0.60998285]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 183 is [True, False, False, False, True, False]
Current timestep = 184. State = [[-0.19831619  0.02656566]]. Action = [[-0.22652663  0.08269063 -0.09772153  0.1327455 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 184 is [True, False, False, False, True, False]
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.20431688  0.02372998]]. Action = [[ 0.24426201 -0.18482612  0.21756941 -0.2832449 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.19148497  0.01753418]]. Action = [[ 0.11357284  0.08764946  0.19835612 -0.1297118 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.1855571   0.00920169]]. Action = [[-0.12023979 -0.1830639   0.21687004  0.9328327 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 187 is [True, False, False, False, True, False]
Current timestep = 188. State = [[-1.8908225e-01  1.6928266e-04]]. Action = [[-0.08853632  0.0037775   0.06583998 -0.8699181 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 188 is [True, False, False, False, True, False]
Current timestep = 189. State = [[-0.19048755  0.00574427]]. Action = [[ 0.139141    0.16215932 -0.2051541  -0.65150833]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.19641832  0.02033498]]. Action = [[-0.24015574  0.10641971  0.2397815   0.5956061 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 190 is [True, False, False, False, True, False]
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.2173355   0.02477375]]. Action = [[-0.22613457 -0.12611826 -0.04220544 -0.508186  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 191 is [True, False, False, False, True, False]
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.24222815  0.02736679]]. Action = [[-0.14843006  0.19575599  0.2111882  -0.893147  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 192 is [True, False, False, False, True, False]
Current timestep = 193. State = [[-0.24814326  0.02845389]]. Action = [[ 0.10819405 -0.2173153   0.04285726 -0.37684578]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.2412391   0.00860813]]. Action = [[ 0.17705017 -0.14979447  0.20554054 -0.47702283]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.22717051 -0.01282555]]. Action = [[ 0.14746284 -0.10399275 -0.09682158 -0.9189595 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.2110964  -0.03697844]]. Action = [[ 0.1529032  -0.24792664  0.17755854 -0.62666124]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.20487593 -0.05618025]]. Action = [[-0.20400357  0.0053302   0.23570424  0.6067599 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.20500155 -0.06742771]]. Action = [[ 0.18696964 -0.10546249  0.19465995 -0.9739512 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 198 is [True, False, False, False, True, False]
Current timestep = 199. State = [[-0.19164576 -0.07206378]]. Action = [[ 0.14555857  0.03792852 -0.07533541  0.44044054]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 199 is [True, False, False, False, True, False]
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.17944168 -0.07799746]]. Action = [[-0.05719724 -0.13240133  0.2065244  -0.97611433]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 200 is [True, False, False, False, True, False]
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 0
Current timestep = 201. State = [[-0.18040483 -0.0804113 ]]. Action = [[-0.0806579   0.17169505 -0.05202608  0.6692257 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.17949998 -0.08076107]]. Action = [[ 0.15231848 -0.17231417 -0.02757901 -0.2507041 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.17883688 -0.07895479]]. Action = [[-0.07231078  0.16993016 -0.0682267  -0.9458743 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 203 is [True, False, False, False, True, False]
Current timestep = 204. State = [[-0.1819651  -0.05930652]]. Action = [[-0.04711223  0.2163859  -0.03023191 -0.3531928 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.18951625 -0.03101062]]. Action = [[-0.15973265  0.20360145 -0.20676889 -0.8545886 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 205 is [True, False, False, False, True, False]
Current timestep = 206. State = [[-0.19464159  0.00088324]]. Action = [[ 0.1232757   0.2076481  -0.16221176 -0.19994015]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.1920191  0.0098006]]. Action = [[ 0.09910268 -0.13967599  0.15679377  0.87499034]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 207 is [True, False, False, False, True, False]
Current timestep = 208. State = [[-0.18733019  0.00146777]]. Action = [[-0.05535348 -0.09590888 -0.16543944  0.8727808 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 208 is [True, False, False, False, True, False]
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.19527325 -0.00889914]]. Action = [[-0.23949893 -0.02628352 -0.21766299  0.86434245]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.20820533 -0.01707657]]. Action = [[-0.09626934 -0.06272456  0.03300899 -0.64934415]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 210 is [True, False, False, False, True, False]
Current timestep = 211. State = [[-0.2279401  -0.03177132]]. Action = [[-0.22496371 -0.137538    0.0910238  -0.74671584]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.24429876 -0.03054927]]. Action = [[0.07564563 0.22158593 0.22251791 0.71404374]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 212 is [True, False, False, False, True, False]
Current timestep = 213. State = [[-0.2517737  -0.01019197]]. Action = [[-0.14190298  0.09182292  0.07053405  0.5376477 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 213 is [True, False, False, False, True, False]
Current timestep = 214. State = [[-0.2540995  -0.00375155]]. Action = [[ 0.16857594 -0.07119507  0.18198222 -0.8806502 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 214 is [True, False, False, False, True, False]
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.25034475 -0.01638189]]. Action = [[-0.04182686 -0.1730294   0.18148041  0.42484117]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.25037053 -0.02601648]]. Action = [[-0.20582165  0.04508245 -0.21755974 -0.5077986 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 216 is [True, False, False, False, True, False]
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.25493902 -0.04311949]]. Action = [[-0.09116077 -0.21173851  0.20982218 -0.6905102 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 217 is [True, False, False, False, True, False]
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.25945482 -0.04983716]]. Action = [[ 0.14927518  0.22837979  0.07001179 -0.1545071 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.2530306 -0.036203 ]]. Action = [[-0.22514358 -0.1123336  -0.19017902 -0.5381395 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 219 is [True, False, False, False, True, False]
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.25332606 -0.02889006]]. Action = [[-0.02718154  0.10558158 -0.02506702 -0.82822263]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.2476966  -0.02491103]]. Action = [[ 0.14466009 -0.06452993 -0.05761443 -0.5852718 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.23216577 -0.02540631]]. Action = [[ 0.1925647  -0.01057814  0.22337997 -0.89156497]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 222 is [True, False, False, False, True, False]
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.20543154 -0.03119484]]. Action = [[ 0.18109256 -0.10780966  0.12127465 -0.36300325]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.19591087 -0.02851859]]. Action = [[-0.20463736  0.17060035 -0.13685481  0.9725151 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 224 is [True, False, False, False, True, False]
Current timestep = 225. State = [[-0.20077945 -0.02655443]]. Action = [[-0.02201684 -0.11669949 -0.2118437   0.4203632 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 225 is [True, False, False, False, True, False]
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of 0
Current timestep = 226. State = [[-0.21123366 -0.03168695]]. Action = [[-0.14149417  0.0070135  -0.1828463  -0.17000061]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 226 is [True, False, False, False, True, False]
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.22678816 -0.02715205]]. Action = [[-0.19183066  0.10911769  0.15436077  0.87313163]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.23880693 -0.02309877]]. Action = [[ 0.17439863 -0.06328782  0.04588366  0.84865665]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 228 is [True, False, False, False, True, False]
Current timestep = 229. State = [[-0.23080257 -0.01460831]]. Action = [[ 0.10737646  0.20789504  0.1088562  -0.5083824 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.2296443 -0.0101668]]. Action = [[-0.2005233  -0.13510306 -0.18677512  0.11449528]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 230 is [True, False, False, False, True, False]
Current timestep = 231. State = [[-0.23325834 -0.00288573]]. Action = [[ 0.10475439  0.21099865 -0.0076738  -0.34887302]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.2395432   0.02464562]]. Action = [[-0.18195875  0.24769717  0.12939292 -0.5337809 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.23998213  0.03539711]]. Action = [[ 0.20181125 -0.17739503 -0.23396252  0.6685455 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 233 is [True, False, False, False, True, False]
Current timestep = 234. State = [[-0.23210913  0.03710919]]. Action = [[0.05297378 0.15969437 0.0456911  0.3392532 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 234 is [True, False, False, False, True, False]
Current timestep = 235. State = [[-0.23183855  0.0392726 ]]. Action = [[-0.1422257  -0.08845143  0.10947451  0.9395368 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 235 is [True, False, False, False, True, False]
Current timestep = 236. State = [[-0.24309285  0.0421752 ]]. Action = [[-0.19407499  0.08928132 -0.05322677 -0.8467465 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 236 is [True, False, False, False, True, False]
Current timestep = 237. State = [[-0.25688332  0.04628007]]. Action = [[-0.02728058 -0.02783591 -0.14465404  0.10187614]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 237 is [True, False, False, False, True, False]
Current timestep = 238. State = [[-0.26618537  0.03158114]]. Action = [[-0.08795589 -0.21496755  0.03261399 -0.97416615]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 238 is [True, False, False, False, True, False]
Current timestep = 239. State = [[-0.26560992  0.02797273]]. Action = [[ 0.17405567  0.18352968 -0.02314319  0.69190955]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.25438544  0.02317728]]. Action = [[ 0.23404986 -0.21384986  0.01733819  0.15996265]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 240 is [True, False, False, False, True, False]
Current timestep = 241. State = [[-0.23583469  0.01979857]]. Action = [[ 0.08194155  0.1260078  -0.02824423  0.45685637]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 241 is [True, False, False, False, True, False]
Current timestep = 242. State = [[-0.23025703  0.03686387]]. Action = [[-0.1153482   0.20415357  0.09279674  0.3409276 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 242 is [True, False, False, False, True, False]
Current timestep = 243. State = [[-0.23842579  0.04162825]]. Action = [[-0.10044056 -0.17020512 -0.11921743 -0.50933343]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 243 is [True, False, False, False, True, False]
Current timestep = 244. State = [[-0.23343417  0.046981  ]]. Action = [[ 0.18781191  0.19298759 -0.00377068 -0.68164444]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 244 is [True, False, False, False, True, False]
Current timestep = 245. State = [[-0.21762882  0.0652976 ]]. Action = [[ 0.18560201  0.16475263 -0.23427263 -0.44017112]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 245 is [True, False, False, False, True, False]
Current timestep = 246. State = [[-0.20008504  0.06987234]]. Action = [[ 0.06292573 -0.16354612  0.09292543 -0.96144766]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 246 is [True, False, False, False, True, False]
Current timestep = 247. State = [[-0.18940006  0.05799032]]. Action = [[ 0.12981367 -0.1081326  -0.02108286  0.9303992 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 247 is [True, False, False, False, True, False]
Current timestep = 248. State = [[-0.18180946  0.05158894]]. Action = [[-0.11899531  0.04102737 -0.10626528 -0.6532201 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 248 is [True, False, False, False, True, False]
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.17263572  0.06332178]]. Action = [[ 0.23318881  0.19626296 -0.15387093  0.9367161 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 249 is [True, False, False, False, True, False]
Current timestep = 250. State = [[-0.16423513  0.0700905 ]]. Action = [[-0.1923127  -0.11879158 -0.03184231 -0.6741259 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 250 is [True, False, False, False, True, False]
Current timestep = 251. State = [[-0.16472366  0.07752515]]. Action = [[ 0.12083948  0.20614326  0.01224309 -0.36100322]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 251 is [True, False, False, False, True, False]
Current timestep = 252. State = [[-0.1617956   0.09953737]]. Action = [[ 0.02710229  0.18815857  0.13571244 -0.8613238 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 252 is [True, False, False, False, True, False]
Current timestep = 253. State = [[-0.15057413 -0.00333642]]. Action = [[-0.02409816 -0.22268449 -0.1313875  -0.6041228 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 253 is [True, False, False, False, True, False]
Current timestep = 254. State = [[-0.13443671 -0.01765345]]. Action = [[-0.04081833 -0.20127189 -0.19222826 -0.34822947]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 254 is [True, False, False, False, True, False]
Current timestep = 255. State = [[-0.13616788 -0.02939843]]. Action = [[-0.07130238  0.03188941  0.1903213   0.9327998 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 255 is [True, False, False, False, True, False]
Current timestep = 256. State = [[-0.1444295 -0.0266916]]. Action = [[-0.18946072  0.06742889  0.00682771  0.5139847 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 256 is [True, False, False, False, True, False]
Current timestep = 257. State = [[-0.15343572 -0.01292554]]. Action = [[ 0.10550395  0.18992415 -0.15013832  0.24431753]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 257 is [True, False, False, False, True, False]
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.15822597  0.00866338]]. Action = [[-0.00459374  0.10895824 -0.04574275  0.4302677 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 258 is [True, False, False, False, True, False]
Current timestep = 259. State = [[-0.16153963  0.0232195 ]]. Action = [[-0.06775486  0.11029777  0.10060099  0.6703501 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 259 is [True, False, False, False, True, False]
Current timestep = 260. State = [[-0.17173916  0.0321901 ]]. Action = [[-0.24610719 -0.03553881  0.18238449 -0.7127613 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 260 is [True, False, False, False, True, False]
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.19075182  0.0363403 ]]. Action = [[-0.08867034  0.05456606 -0.2119381   0.6674998 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 261 is [True, False, False, False, True, False]
Current timestep = 262. State = [[-0.20519008  0.04811189]]. Action = [[-0.15147968  0.10622814  0.23403826  0.51481247]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 262 is [True, False, False, False, True, False]
Current timestep = 263. State = [[-0.22805102  0.04771926]]. Action = [[-0.20873928 -0.13038267 -0.01687963 -0.22684574]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 263 is [True, False, False, False, True, False]
Current timestep = 264. State = [[-0.2418827   0.02640096]]. Action = [[ 0.11785033 -0.2215353  -0.0335426  -0.32555676]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 264 is [True, False, False, False, True, False]
Current timestep = 265. State = [[-0.24828282  0.00442796]]. Action = [[-0.15266299 -0.10158154  0.19117439  0.71045506]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 265 is [True, False, False, False, True, False]
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.25802186 -0.01882341]]. Action = [[-0.03534025 -0.17028095  0.05494586  0.44753885]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 266 is [True, False, False, False, True, False]
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.2606197  -0.03315444]]. Action = [[-0.1132656   0.21064284  0.12805408 -0.7853345 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.26153615 -0.03044157]]. Action = [[-0.01575199  0.06933206  0.07450747  0.86092865]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 268 is [True, False, False, False, True, False]
Current timestep = 269. State = [[-0.26194668 -0.02867983]]. Action = [[-0.17652191  0.06325191  0.1942876  -0.9024591 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 269 is [True, False, False, False, True, False]
Current timestep = 270. State = [[-0.2598714  -0.01599592]]. Action = [[ 0.13043267  0.19104713  0.05900627 -0.8576074 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.24718897 -0.00279336]]. Action = [[ 0.23010367  0.00043529 -0.18277237  0.36896086]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 271 is [True, False, False, False, True, False]
Current timestep = 272. State = [[-0.22805889 -0.01034957]]. Action = [[ 0.13615686 -0.19932629 -0.02832426  0.44647932]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 272 is [True, False, False, False, True, False]
Current timestep = 273. State = [[-0.21200313 -0.03359205]]. Action = [[ 0.03746301 -0.17897438 -0.10441038  0.9305198 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 273 is [True, False, False, False, True, False]
Current timestep = 274. State = [[-0.2106221 -0.0488211]]. Action = [[-0.15267025 -0.01433147  0.14664134 -0.44038785]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 274 is [True, False, False, False, True, False]
Current timestep = 275. State = [[-0.21156427 -0.05002231]]. Action = [[0.09802729 0.06659076 0.10381749 0.2345109 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 275 is [True, False, False, False, True, False]
Current timestep = 276. State = [[-0.20637314 -0.05689757]]. Action = [[ 0.10993999 -0.15440787  0.00437978  0.7581403 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 276 is [True, False, False, False, True, False]
Current timestep = 277. State = [[-0.20218614 -0.05927022]]. Action = [[0.0167895  0.10933569 0.1852045  0.26728988]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.20311794 -0.05973458]]. Action = [[-0.14172204 -0.08364385 -0.13669172 -0.958297  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 278 is [True, False, False, False, True, False]
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.199379   -0.06657018]]. Action = [[ 0.21376759 -0.05696288 -0.16998962 -0.8336218 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 279 is [True, False, False, False, True, False]
Current timestep = 280. State = [[-0.19227865 -0.08018927]]. Action = [[-0.02536851 -0.18779385 -0.14777982  0.6002661 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.19135256 -0.09530027]]. Action = [[-0.0879842  -0.00414033  0.20536411 -0.06171483]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.19247536 -0.09648894]]. Action = [[-0.01339261  0.07698894 -0.15231313 -0.6675411 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 282 is [True, False, False, False, True, False]
Current timestep = 283. State = [[-0.20254508 -0.0829232 ]]. Action = [[-0.20717917  0.20801046  0.08684376 -0.83790296]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 283 is [True, False, False, False, True, False]
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.21486384 -0.05113831]]. Action = [[ 0.03138018  0.21813452 -0.06179667 -0.3531474 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 284 is [True, False, False, False, True, False]
Current timestep = 285. State = [[-0.21807194 -0.04234738]]. Action = [[-0.05138613 -0.12595737  0.22899264  0.73079765]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 285 is [True, False, False, False, True, False]
Current timestep = 286. State = [[-0.22076496 -0.05216589]]. Action = [[-0.04355106 -0.09007694 -0.01285714 -0.25484657]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.23112963 -0.05805834]]. Action = [[-0.1243557   0.02018955  0.11265981  0.8279989 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 287 is [True, False, False, False, True, False]
Current timestep = 288. State = [[-0.249187   -0.07084709]]. Action = [[-0.18986304 -0.18006761  0.12511331  0.61276245]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.26691058 -0.08194418]]. Action = [[-0.21951647  0.01231298  0.03421506 -0.7789248 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 289 is [True, False, False, False, True, False]
Current timestep = 290. State = [[-0.2698995  -0.08490058]]. Action = [[-0.05945234  0.08167511  0.21874312 -0.91030014]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 290 is [True, False, False, False, True, False]
Current timestep = 291. State = [[-0.26147887 -0.09703398]]. Action = [[ 0.23998997 -0.21101816  0.14686716 -0.7340332 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 291 is [True, False, False, False, True, False]
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.24469079 -0.10246688]]. Action = [[0.150725   0.2067967  0.12440535 0.64356446]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 292 is [True, False, False, False, True, False]
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.22190203 -0.0954029 ]]. Action = [[ 0.21173307 -0.13131979 -0.21261473  0.798821  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 293 is [True, False, False, False, True, False]
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[-0.20471773 -0.09586835]]. Action = [[-0.06116858  0.1239253   0.13705862  0.31699455]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 294 is [True, False, False, False, True, False]
Current timestep = 295. State = [[-0.20934695 -0.10474258]]. Action = [[-0.1117336  -0.23322102 -0.02548005 -0.15189415]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 295 is [True, False, False, False, True, False]
Current timestep = 296. State = [[-0.20816812 -0.11287488]]. Action = [[ 0.21378288  0.06942126  0.12259534 -0.40723735]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 296 is [True, False, False, False, True, False]
Current timestep = 297. State = [[-0.19795807 -0.10837244]]. Action = [[ 0.04113588  0.07072839 -0.08803478 -0.9134375 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 297 is [True, False, False, False, True, False]
Current timestep = 298. State = [[-0.18354654 -0.10357449]]. Action = [[ 0.22558826 -0.0245178  -0.15476224 -0.38994718]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 298 is [True, False, False, False, True, False]
Current timestep = 299. State = [[-0.16894186 -0.10327783]]. Action = [[-0.11482099  0.0073157  -0.01236966 -0.94710016]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 299 is [True, False, False, False, True, False]
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(3.1155e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.16565816 -0.10652436]]. Action = [[ 0.14014769 -0.0830898   0.13826972 -0.59227145]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 300 is [True, False, False, False, True, False]
Current timestep = 301. State = [[-0.16170892 -0.10512236]]. Action = [[-0.05154257  0.11258635  0.18700165 -0.10598397]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 301 is [True, False, False, False, True, False]
Current timestep = 302. State = [[-0.16526994 -0.11221296]]. Action = [[-0.14661868 -0.17918639  0.04143667  0.20897937]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 302 is [True, False, False, False, True, False]
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.18317579 -0.13413863]]. Action = [[-0.22916584 -0.15835895 -0.05825797  0.51569796]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 303 is [True, False, False, False, True, False]
Current timestep = 304. State = [[-0.19412118 -0.14040332]]. Action = [[ 0.08284709  0.10415626  0.21063215 -0.2941271 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 304 is [True, False, False, True, False, False]
Current timestep = 305. State = [[-0.18443337 -0.14468196]]. Action = [[ 0.23987436 -0.14982718 -0.05998813 -0.45133996]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 305 is [True, False, False, True, False, False]
Scene graph at timestep 305 is [True, False, False, True, False, False]
State prediction error at timestep 305 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.17332402 -0.16363226]]. Action = [[-0.04351193 -0.20689286 -0.20892468 -0.16872323]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 306 is [True, False, False, True, False, False]
Scene graph at timestep 306 is [True, False, False, True, False, False]
State prediction error at timestep 306 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.16689572 -0.18661514]]. Action = [[ 0.19996208 -0.09213841  0.22285837 -0.6241131 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 307 is [True, False, False, True, False, False]
Current timestep = 308. State = [[-0.14589499 -0.18364681]]. Action = [[ 0.11426377  0.18547678 -0.03519788 -0.8873273 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 308 is [True, False, False, True, False, False]
Current timestep = 309. State = [[-0.13763009 -0.1747819 ]]. Action = [[-0.09939508  0.04067445  0.2279914  -0.9200025 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 309 is [True, False, False, True, False, False]
Current timestep = 310. State = [[-0.13170694 -0.16099828]]. Action = [[0.14348    0.14612782 0.05816072 0.95181763]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 310 is [True, False, False, True, False, False]
Current timestep = 311. State = [[-0.13270165 -0.16147308]]. Action = [[-0.20367484 -0.18265899 -0.02716339 -0.68577874]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 311 is [True, False, False, True, False, False]
Current timestep = 312. State = [[-0.13446313 -0.17109917]]. Action = [[ 0.14835435 -0.04980202  0.07915449  0.7065772 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 312 is [True, False, False, True, False, False]
Current timestep = 313. State = [[-0.12533286 -0.184435  ]]. Action = [[ 0.18382788 -0.18532279  0.16724324 -0.23964983]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 313 is [True, False, False, True, False, False]
Current timestep = 314. State = [[-0.11425146 -0.19229658]]. Action = [[-0.11674222  0.13599169 -0.11125711 -0.00459588]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 314 is [True, False, False, True, False, False]
Current timestep = 315. State = [[-0.11908688 -0.20291422]]. Action = [[-0.12688479 -0.21428634 -0.04524267  0.49586582]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 315 is [True, False, False, True, False, False]
Scene graph at timestep 315 is [True, False, False, True, False, False]
State prediction error at timestep 315 is tensor(7.2779e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.1225146  -0.21848716]]. Action = [[ 0.20368603 -0.0705229   0.07668141  0.8564812 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 316 is [True, False, False, True, False, False]
Current timestep = 317. State = [[-0.10499856 -0.22659168]]. Action = [[ 0.1989378  -0.08921047  0.12709117  0.7403157 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 317 is [True, False, False, True, False, False]
Current timestep = 318. State = [[-0.08617985 -0.22975688]]. Action = [[ 0.01460755  0.1125119  -0.01922087 -0.22101903]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 318 is [True, False, False, True, False, False]
Current timestep = 319. State = [[-0.07182261 -0.23388655]]. Action = [[ 0.2326116  -0.164315   -0.05214112  0.30696023]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 319 is [True, False, False, True, False, False]
Current timestep = 320. State = [[-0.05070489 -0.25418672]]. Action = [[ 0.05345759 -0.2092324   0.06044611 -0.67914706]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 320 is [True, False, False, True, False, False]
Current timestep = 321. State = [[-0.0297461  -0.26315904]]. Action = [[ 0.2122361   0.13201427 -0.147492   -0.6207618 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 321 is [True, False, False, True, False, False]
Current timestep = 322. State = [[-0.01478098 -0.24825042]]. Action = [[-0.1587505   0.22749537 -0.20641848 -0.2684443 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 322 is [False, True, False, True, False, False]
Current timestep = 323. State = [[-0.00985296 -0.24026975]]. Action = [[ 0.1753521  -0.13518517  0.16217     0.3505367 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 323 is [False, True, False, True, False, False]
Scene graph at timestep 323 is [False, True, False, True, False, False]
State prediction error at timestep 323 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.00436217 -0.25088596]]. Action = [[-0.0153359  -0.14229727  0.06840026  0.12684107]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 324 is [False, True, False, True, False, False]
Current timestep = 325. State = [[-0.00580327 -0.26026604]]. Action = [[-0.09802681  0.02887264 -0.01076998  0.82506275]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 325 is [False, True, False, True, False, False]
Current timestep = 326. State = [[-0.01275761 -0.25915906]]. Action = [[-0.21161723  0.10628873  0.00682056  0.9704702 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 326 is [False, True, False, True, False, False]
Scene graph at timestep 326 is [False, True, False, True, False, False]
State prediction error at timestep 326 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.02671002 -0.2587318 ]]. Action = [[-0.02795224 -0.05684923  0.09705037  0.05486333]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 327 is [False, True, False, True, False, False]
Current timestep = 328. State = [[-0.03014915 -0.2682485 ]]. Action = [[ 0.13111639 -0.15655884 -0.17858429 -0.6844639 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 328 is [False, True, False, True, False, False]
Scene graph at timestep 328 is [False, True, False, True, False, False]
State prediction error at timestep 328 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.02623457 -0.26678732]]. Action = [[0.08410585 0.17412591 0.08017039 0.65087306]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 329 is [False, True, False, True, False, False]
Current timestep = 330. State = [[-0.021284   -0.26499197]]. Action = [[ 0.02003634 -0.12859549  0.01713508  0.1500448 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 330 is [False, True, False, True, False, False]
Current timestep = 331. State = [[-0.01272412 -0.2796244 ]]. Action = [[ 0.22500396 -0.23070213  0.18260884  0.84626734]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 331 is [False, True, False, True, False, False]
Scene graph at timestep 331 is [False, True, False, True, False, False]
State prediction error at timestep 331 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[ 0.01219607 -0.30066285]]. Action = [[ 0.16026479 -0.16831057  0.17340744 -0.881044  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 332 is [False, True, False, True, False, False]
Current timestep = 333. State = [[ 0.00665791 -0.30487463]]. Action = [[-2.4056713e-01  4.8934519e-03 -1.2931228e-04  7.9689598e-01]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 333 is [False, True, False, True, False, False]
Scene graph at timestep 333 is [False, True, False, True, False, False]
State prediction error at timestep 333 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[ 0.00130484 -0.3096244 ]]. Action = [[ 0.24532485 -0.24139386  0.06352949  0.5580398 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 334 is [False, True, False, True, False, False]
Current timestep = 335. State = [[ 0.00707551 -0.29919228]]. Action = [[ 0.20837736  0.17091393  0.18901253 -0.90722036]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 335 is [False, True, False, True, False, False]
Current timestep = 336. State = [[ 0.00980831 -0.28645903]]. Action = [[ 0.21646333 -0.16389161  0.20783675 -0.9244755 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 336 is [False, True, False, True, False, False]
Current timestep = 337. State = [[ 0.01981487 -0.28750262]]. Action = [[ 0.22910267 -0.10922378  0.11838627  0.51974154]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 337 is [False, True, False, True, False, False]
Current timestep = 338. State = [[ 0.03593074 -0.28623414]]. Action = [[-0.08860245  0.12511086  0.09982502 -0.87253374]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 338 is [False, True, False, True, False, False]
Current timestep = 339. State = [[ 0.03783161 -0.2808575 ]]. Action = [[ 0.06078908  0.02672964 -0.03568384 -0.5286691 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 339 is [False, True, False, True, False, False]
Current timestep = 340. State = [[ 0.0384328  -0.27729627]]. Action = [[-0.12487537  0.04490399 -0.08733448  0.66265965]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 340 is [False, True, False, True, False, False]
Current timestep = 341. State = [[ 0.03084177 -0.2833593 ]]. Action = [[-0.18775418 -0.11071831  0.23836905  0.7624576 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 341 is [False, True, False, True, False, False]
Scene graph at timestep 341 is [False, True, False, True, False, False]
State prediction error at timestep 341 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[ 0.01951919 -0.29112273]]. Action = [[ 0.18961722 -0.14860018 -0.18310718 -0.49638915]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 342 is [False, True, False, True, False, False]
Current timestep = 343. State = [[ 0.0228891  -0.28906524]]. Action = [[0.19423509 0.00439185 0.16887301 0.43792498]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 343 is [False, True, False, True, False, False]
Scene graph at timestep 343 is [False, True, False, True, False, False]
State prediction error at timestep 343 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[ 0.02470533 -0.28418154]]. Action = [[-0.11408991  0.08506882 -0.09432644  0.2035414 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 344 is [False, True, False, True, False, False]
Current timestep = 345. State = [[ 0.01918637 -0.28524643]]. Action = [[-0.13939919 -0.08608291 -0.11719465  0.7498375 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 345 is [False, True, False, True, False, False]
Current timestep = 346. State = [[ 0.01163967 -0.28883106]]. Action = [[ 0.18621963 -0.24314082  0.16426939  0.87625957]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 346 is [False, True, False, True, False, False]
Current timestep = 347. State = [[ 0.0077442 -0.2927944]]. Action = [[-0.06020972 -0.04396051  0.09962642 -0.47644585]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 347 is [False, True, False, True, False, False]
Current timestep = 348. State = [[ 0.00819612 -0.2863049 ]]. Action = [[ 0.13596898  0.15751821 -0.11218342 -0.9664254 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 348 is [False, True, False, True, False, False]
Current timestep = 349. State = [[ 0.01093944 -0.2763378 ]]. Action = [[ 0.06283674 -0.1387428   0.2356554   0.3516388 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 349 is [False, True, False, True, False, False]
Current timestep = 350. State = [[ 0.01117263 -0.2750065 ]]. Action = [[-0.03608222 -0.18950756  0.1907891   0.8624939 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 350 is [False, True, False, True, False, False]
Current timestep = 351. State = [[ 0.01048979 -0.26490304]]. Action = [[-0.14667432  0.19250235 -0.09930897 -0.82511157]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 351 is [False, True, False, True, False, False]
Current timestep = 352. State = [[ 0.00634181 -0.25193366]]. Action = [[ 0.13953972 -0.06206962  0.04971108  0.00702167]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 352 is [False, True, False, True, False, False]
Current timestep = 353. State = [[ 0.00980539 -0.2601854 ]]. Action = [[ 0.11971542 -0.17800145 -0.24401318 -0.6136122 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 353 is [False, True, False, True, False, False]
Current timestep = 354. State = [[ 0.01367579 -0.27635834]]. Action = [[ 0.03218627 -0.12395585  0.08931759  0.8516984 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 354 is [False, True, False, True, False, False]
Current timestep = 355. State = [[ 0.02908242 -0.28312165]]. Action = [[ 0.23316324  0.02577865  0.06150883 -0.61998665]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 355 is [False, True, False, True, False, False]
Current timestep = 356. State = [[ 0.04247543 -0.2821112 ]]. Action = [[-0.1879764   0.07859948 -0.18732962 -0.2222389 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 356 is [False, True, False, True, False, False]
Current timestep = 357. State = [[ 0.03903506 -0.28063062]]. Action = [[-0.12079439  0.04973909  0.06396586 -0.6587636 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 357 is [False, True, False, True, False, False]
Current timestep = 358. State = [[ 0.0345481  -0.27861008]]. Action = [[ 0.18001682  0.09891504 -0.09387404 -0.04461426]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 358 is [False, True, False, True, False, False]
Current timestep = 359. State = [[ 0.03653149 -0.2703713 ]]. Action = [[ 0.1351121   0.10599077  0.24546862 -0.24766934]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 359 is [False, True, False, True, False, False]
Current timestep = 360. State = [[ 0.04378599 -0.25066322]]. Action = [[ 0.14003462  0.14351267 -0.13314201  0.68666124]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 360 is [False, True, False, True, False, False]
Current timestep = 361. State = [[ 0.04781263 -0.23863393]]. Action = [[ 0.21486896  0.21550888 -0.0375331   0.249048  ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 361 is [False, True, False, True, False, False]
Current timestep = 362. State = [[ 0.04918224 -0.23700912]]. Action = [[ 0.21451831 -0.03456548 -0.19561745  0.18169093]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 362 is [False, True, False, True, False, False]
Current timestep = 363. State = [[ 0.04599198 -0.24467278]]. Action = [[-0.16833185 -0.13776462  0.07762915 -0.76877695]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 363 is [False, True, False, True, False, False]
Current timestep = 364. State = [[ 0.04056234 -0.2519146 ]]. Action = [[ 0.24685541 -0.06978822 -0.08406827 -0.70380837]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 364 is [False, True, False, True, False, False]
Current timestep = 365. State = [[ 0.03981413 -0.24099562]]. Action = [[-0.05981249  0.24115086 -0.18778856 -0.38341963]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 365 is [False, True, False, True, False, False]
Current timestep = 366. State = [[ 0.03948649 -0.22736417]]. Action = [[ 0.16996354 -0.03897054  0.17257872 -0.8443856 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 366 is [False, True, False, True, False, False]
Current timestep = 367. State = [[ 0.04089165 -0.2254824 ]]. Action = [[ 0.15436724 -0.03613275  0.18421072  0.29873347]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 367 is [False, True, False, True, False, False]
Current timestep = 368. State = [[ 0.0412774  -0.21484642]]. Action = [[-0.13393839  0.18974596  0.11789596 -0.85248953]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 368 is [False, True, False, True, False, False]
Current timestep = 369. State = [[ 0.03564609 -0.19345807]]. Action = [[-0.18809533  0.1856696   0.09849691  0.9599097 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 369 is [False, True, False, True, False, False]
Current timestep = 370. State = [[ 0.01960996 -0.17600605]]. Action = [[ 0.19039264  0.02823871 -0.00797509 -0.07019818]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 370 is [False, True, False, True, False, False]
Current timestep = 371. State = [[ 0.01051095 -0.16272974]]. Action = [[-0.1876513   0.2126568  -0.13243133  0.07997894]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 371 is [False, True, False, True, False, False]
Current timestep = 372. State = [[-0.00258538 -0.13245599]]. Action = [[ 0.15436876  0.1572603   0.22139537 -0.50867325]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 372 is [False, True, False, True, False, False]
Scene graph at timestep 372 is [False, True, False, True, False, False]
State prediction error at timestep 372 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[ 0.00205144 -0.11693137]]. Action = [[ 0.14925605 -0.05061024 -0.14009027 -0.81085384]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 373 is [False, True, False, True, False, False]
Current timestep = 374. State = [[ 0.00179033 -0.11930513]]. Action = [[-0.18731828 -0.00585598  0.13393062 -0.4839803 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 374 is [False, True, False, False, True, False]
Current timestep = 375. State = [[-0.00712794 -0.12460618]]. Action = [[-0.19812717 -0.02304809 -0.15927522  0.97594   ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 375 is [False, True, False, False, True, False]
Scene graph at timestep 375 is [False, True, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 0
Current timestep = 376. State = [[-0.02357977 -0.13989079]]. Action = [[ 0.1301567  -0.22703965 -0.08163397  0.39181757]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 376 is [False, True, False, False, True, False]
Scene graph at timestep 376 is [False, True, False, True, False, False]
State prediction error at timestep 376 is tensor(4.6046e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of -1
Current timestep = 377. State = [[-0.02110399 -0.15244395]]. Action = [[-0.00685871  0.05177754 -0.23272915  0.57063675]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 377 is [False, True, False, True, False, False]
Scene graph at timestep 377 is [False, True, False, True, False, False]
State prediction error at timestep 377 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 1
Current timestep = 378. State = [[-0.01914094 -0.14799699]]. Action = [[ 0.08251899  0.06300667  0.08712703 -0.35582894]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 378 is [False, True, False, True, False, False]
Current timestep = 379. State = [[-0.00940484 -0.14954443]]. Action = [[ 0.20395327 -0.1240152   0.1557644   0.5388384 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 379 is [False, True, False, True, False, False]
Current timestep = 380. State = [[-0.23440087  0.02989748]]. Action = [[ 0.06459218  0.05124214  0.12196699 -0.32780933]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 380 is [False, True, False, True, False, False]
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0460, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of 0
Current timestep = 381. State = [[-0.22578186  0.04799943]]. Action = [[ 0.07989466  0.23410133 -0.05574058  0.36391175]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 381 is [True, False, False, False, True, False]
Current timestep = 382. State = [[-0.2252461   0.07850815]]. Action = [[-0.152967    0.19332045  0.20072919 -0.52671885]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 382 is [True, False, False, False, True, False]
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.24164367  0.10533938]]. Action = [[-0.17455052  0.08150655  0.14848647  0.07226121]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 383 is [True, False, False, False, True, False]
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.24998903  0.11555185]]. Action = [[ 0.00504088  0.01005274  0.20810646 -0.63766176]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 384 is [True, False, False, False, True, False]
Current timestep = 385. State = [[-0.24127261  0.10795125]]. Action = [[ 0.23748153 -0.11951333  0.01324141  0.8020228 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 385 is [True, False, False, False, True, False]
Current timestep = 386. State = [[-0.23894088  0.11291755]]. Action = [[-0.09492517  0.22612062 -0.18502174  0.80528617]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 386 is [True, False, False, False, True, False]
Current timestep = 387. State = [[-0.23894866  0.13535506]]. Action = [[ 0.08580375  0.17749113 -0.04926543 -0.55827355]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 387 is [True, False, False, False, True, False]
Current timestep = 388. State = [[-0.23816435  0.15394923]]. Action = [[-0.1772843   0.01367316  0.04282647  0.9775295 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 388 is [True, False, False, False, False, True]
Current timestep = 389. State = [[-0.24228707  0.15813139]]. Action = [[ 0.02416697 -0.01700839  0.12176386 -0.43085635]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 389 is [True, False, False, False, False, True]
Current timestep = 390. State = [[-0.24225625  0.15753935]]. Action = [[-0.22460878 -0.07450789 -0.04563272  0.7048836 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.24416254  0.15283729]]. Action = [[-0.0993565  -0.09065382  0.02840903 -0.20686269]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 391 is [True, False, False, False, False, True]
Scene graph at timestep 391 is [True, False, False, False, False, True]
State prediction error at timestep 391 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.25312653  0.15002328]]. Action = [[-0.04810153  0.05825973 -0.18422787 -0.23408961]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 392 is [True, False, False, False, False, True]
Scene graph at timestep 392 is [True, False, False, False, False, True]
State prediction error at timestep 392 is tensor(3.2549e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.25727093  0.15470901]]. Action = [[-0.2279846  -0.08445853  0.08902708 -0.49296856]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.2515449  0.1621134]]. Action = [[ 0.2139805   0.17584974 -0.03180833 -0.6978954 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 394 is [True, False, False, False, False, True]
Current timestep = 395. State = [[-0.23333482  0.17979366]]. Action = [[ 0.20417824  0.15929621 -0.09945215  0.4784844 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 395 is [True, False, False, False, False, True]
Scene graph at timestep 395 is [True, False, False, False, False, True]
State prediction error at timestep 395 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of 0
Current timestep = 396. State = [[-0.20499778  0.19120869]]. Action = [[ 0.09977233 -0.11430088 -0.0724103   0.8279948 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 396 is [True, False, False, False, False, True]
Scene graph at timestep 396 is [True, False, False, False, False, True]
State prediction error at timestep 396 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of 1
Current timestep = 397. State = [[-0.19307144  0.19106418]]. Action = [[ 0.1639353   0.16204888  0.12080783 -0.2958324 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 397 is [True, False, False, False, False, True]
Current timestep = 398. State = [[-0.17368823  0.19567925]]. Action = [[ 0.04301992 -0.11776772 -0.01614779  0.7676048 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 398 is [True, False, False, False, False, True]
Current timestep = 399. State = [[-0.16178972  0.17856051]]. Action = [[ 0.09178609 -0.21323521 -0.10151437 -0.36875695]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 399 is [True, False, False, False, False, True]
Current timestep = 400. State = [[-0.15452516  0.1537278 ]]. Action = [[-0.05190547 -0.19471939  0.21382684  0.7199948 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.15276855  0.15060896]]. Action = [[ 0.07040417  0.2143628  -0.1092321  -0.88663733]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 401 is [True, False, False, False, False, True]
Current timestep = 402. State = [[-0.14893933  0.16292235]]. Action = [[ 0.07189232  0.09677082 -0.08087823  0.04454768]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 402 is [True, False, False, False, False, True]
Current timestep = 403. State = [[-0.13085717  0.17845535]]. Action = [[0.23217869 0.1317566  0.0434562  0.15209734]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 403 is [True, False, False, False, False, True]
Current timestep = 404. State = [[-0.11146601  0.17765832]]. Action = [[-0.04337557 -0.22247387  0.00806147  0.7591803 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 404 is [True, False, False, False, False, True]
Current timestep = 405. State = [[-0.10545366  0.1799881 ]]. Action = [[ 0.12796575  0.21532416 -0.22026852  0.54057336]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 405 is [True, False, False, False, False, True]
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[-0.09887542  0.20370553]]. Action = [[-0.18757299  0.1592238  -0.20229049 -0.6128865 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 406 is [True, False, False, False, False, True]
Current timestep = 407. State = [[-0.11350872  0.22724171]]. Action = [[-0.0591836   0.16177541 -0.24620949  0.49346876]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 407 is [True, False, False, False, False, True]
Current timestep = 408. State = [[-0.1236934   0.24221984]]. Action = [[-0.08103675  0.0103367   0.15753898  0.8083236 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 408 is [True, False, False, False, False, True]
Current timestep = 409. State = [[-0.12351809  0.24860997]]. Action = [[ 0.16349217  0.0961948  -0.10650575  0.3519479 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 409 is [True, False, False, False, False, True]
Current timestep = 410. State = [[-0.11817804  0.25266463]]. Action = [[0.00387642 0.00606027 0.15753388 0.37621784]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 410 is [True, False, False, False, False, True]
Current timestep = 411. State = [[-0.11354344  0.24305834]]. Action = [[ 0.01590481 -0.21811958 -0.02360287  0.56491923]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 411 is [True, False, False, False, False, True]
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.09971195  0.22472782]]. Action = [[ 0.24619067 -0.07782671  0.21527112 -0.8696946 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 412 is [True, False, False, False, False, True]
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of 1
Current timestep = 413. State = [[-0.08306642  0.22442928]]. Action = [[-0.19742444  0.06991506  0.02039027  0.5833591 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 413 is [True, False, False, False, False, True]
Current timestep = 414. State = [[-0.08456479  0.21839447]]. Action = [[-0.01028734 -0.22956614 -0.0506281   0.28565443]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 414 is [True, False, False, False, False, True]
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.08287308  0.20361504]]. Action = [[ 0.0415886   0.05419287  0.10328418 -0.46622598]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of 0
Current timestep = 416. State = [[-0.08094807  0.21114275]]. Action = [[ 0.16378105  0.15089387 -0.13303806 -0.9082879 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 416 is [True, False, False, False, False, True]
Current timestep = 417. State = [[-0.06901746  0.21891034]]. Action = [[ 0.16421562 -0.01397817 -0.03052963 -0.5253682 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 417 is [True, False, False, False, False, True]
Current timestep = 418. State = [[-0.05979636  0.21724513]]. Action = [[-0.12072101 -0.11435643  0.07323804  0.6226846 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 418 is [True, False, False, False, False, True]
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of 0
Current timestep = 419. State = [[-0.0526182   0.21327391]]. Action = [[ 0.2410866   0.09168378 -0.09085111 -0.7656244 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 419 is [True, False, False, False, False, True]
Current timestep = 420. State = [[-0.03040875  0.21676922]]. Action = [[ 0.2178469  -0.03151214 -0.15363126 -0.5990713 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 420 is [True, False, False, False, False, True]
Scene graph at timestep 420 is [False, True, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.00375486  0.22429883]]. Action = [[ 0.03519076  0.14975578  0.04982176 -0.7351554 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 421 is [False, True, False, False, False, True]
Current timestep = 422. State = [[-0.00256579  0.24354154]]. Action = [[-0.13084795  0.13939798 -0.14071399 -0.8445064 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 422 is [False, True, False, False, False, True]
Current timestep = 423. State = [[-0.01371397  0.26451978]]. Action = [[-0.15823144  0.11474234  0.05909336 -0.9337403 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 423 is [False, True, False, False, False, True]
Current timestep = 424. State = [[-0.01829722  0.2786021 ]]. Action = [[ 0.15889269  0.09397727  0.04765886 -0.11486042]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 424 is [False, True, False, False, False, True]
Current timestep = 425. State = [[-0.00646584  0.2898884 ]]. Action = [[ 0.23994035  0.09074479 -0.1603429  -0.18197429]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 425 is [False, True, False, False, False, True]
Current timestep = 426. State = [[0.01090561 0.30014998]]. Action = [[0.18430293 0.08996412 0.08416873 0.82205176]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 426 is [False, True, False, False, False, True]
Current timestep = 427. State = [[0.02556221 0.29525983]]. Action = [[ 0.24531946 -0.12253669  0.19120389 -0.7119517 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 427 is [False, True, False, False, False, True]
Scene graph at timestep 427 is [False, True, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[0.04938725 0.28135833]]. Action = [[ 0.00146684  0.03674036 -0.23114295  0.81881523]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 428 is [False, True, False, False, False, True]
Current timestep = 429. State = [[0.04877741 0.27855805]]. Action = [[-0.1930879 -0.1952001 -0.183542   0.9620969]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 429 is [False, True, False, False, False, True]
Current timestep = 430. State = [[0.04897856 0.27614468]]. Action = [[-0.1299039   0.22296095  0.19067085  0.76142216]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 430 is [False, True, False, False, False, True]
Current timestep = 431. State = [[0.04922173 0.27499214]]. Action = [[ 0.22775996  0.17710775 -0.20719796 -0.74589807]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 431 is [False, True, False, False, False, True]
Current timestep = 432. State = [[0.04957262 0.27386132]]. Action = [[0.12088019 0.1418049  0.21656865 0.36908245]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 432 is [False, True, False, False, False, True]
Current timestep = 433. State = [[0.05288808 0.26584607]]. Action = [[ 0.02403083 -0.1544023  -0.04024333  0.9299655 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 433 is [False, True, False, False, False, True]
Current timestep = 434. State = [[0.05723648 0.25466102]]. Action = [[-0.07240683 -0.0774543  -0.20608178 -0.19677585]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 434 is [False, False, True, False, False, True]
Current timestep = 435. State = [[0.05710801 0.24956957]]. Action = [[-0.08170578 -0.06474707  0.03651339  0.57969236]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 435 is [False, False, True, False, False, True]
Current timestep = 436. State = [[0.0510207 0.2557514]]. Action = [[-0.21268997  0.09215906 -0.09535506  0.06027782]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 436 is [False, False, True, False, False, True]
Current timestep = 437. State = [[0.04223965 0.26505092]]. Action = [[-0.21090558 -0.0735901   0.22419363  0.173931  ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 437 is [False, False, True, False, False, True]
Current timestep = 438. State = [[0.0383946  0.26798168]]. Action = [[ 0.14243811  0.0841023  -0.05086249  0.8866224 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 438 is [False, True, False, False, False, True]
Current timestep = 439. State = [[0.03678767 0.27074492]]. Action = [[ 0.06294402  0.07346421  0.17886186 -0.04730356]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 439 is [False, True, False, False, False, True]
Current timestep = 440. State = [[0.0327317 0.2790147]]. Action = [[ 0.10020065  0.17430851 -0.0772768  -0.48919284]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 440 is [False, True, False, False, False, True]
Current timestep = 441. State = [[0.03323752 0.2782523 ]]. Action = [[ 0.16275728 -0.08207604 -0.18527308  0.0601449 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 441 is [False, True, False, False, False, True]
Current timestep = 442. State = [[0.040774   0.26509652]]. Action = [[ 0.10442546 -0.09726325 -0.15267393 -0.632217  ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 442 is [False, True, False, False, False, True]
Scene graph at timestep 442 is [False, True, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[0.04696674 0.2560824 ]]. Action = [[-0.13624284 -0.01479371 -0.1683494   0.33452225]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 443 is [False, True, False, False, False, True]
Current timestep = 444. State = [[0.04207804 0.26578563]]. Action = [[-0.01928343  0.16393703  0.20347604 -0.70297587]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 444 is [False, True, False, False, False, True]
Current timestep = 445. State = [[0.04146282 0.26589283]]. Action = [[-0.01574691 -0.21572632  0.10522822  0.92579293]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 445 is [False, True, False, False, False, True]
Scene graph at timestep 445 is [False, True, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of 0
Current timestep = 446. State = [[0.04317001 0.2612515 ]]. Action = [[-0.23020358 -0.03601477  0.03136295  0.6306951 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 446 is [False, True, False, False, False, True]
Scene graph at timestep 446 is [False, True, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of 0
Current timestep = 447. State = [[0.03589844 0.27377677]]. Action = [[-0.05112678  0.2092407  -0.14276539  0.5773411 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 447 is [False, True, False, False, False, True]
Current timestep = 448. State = [[0.02426372 0.2926116 ]]. Action = [[-0.14538142 -0.02154274  0.13719475 -0.5721341 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 448 is [False, True, False, False, False, True]
Current timestep = 449. State = [[0.01767426 0.29986864]]. Action = [[-0.15469177 -0.06497702  0.11073986  0.17704272]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 449 is [False, True, False, False, False, True]
Scene graph at timestep 449 is [False, True, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[0.01383144 0.3017478 ]]. Action = [[-0.23291087  0.22346303  0.14618579  0.93711436]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 450 is [False, True, False, False, False, True]
Current timestep = 451. State = [[0.01383144 0.3017478 ]]. Action = [[0.17988342 0.22538698 0.1427508  0.626014  ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 451 is [False, True, False, False, False, True]
Current timestep = 452. State = [[0.01392235 0.2974884 ]]. Action = [[-0.12308507 -0.18642557  0.15005597 -0.4928347 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 452 is [False, True, False, False, False, True]
Scene graph at timestep 452 is [False, True, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of 1
Current timestep = 453. State = [[-0.09334593  0.22852722]]. Action = [[-0.14127935  0.19425878 -0.11871868 -0.00301188]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 453 is [False, True, False, False, False, True]
Current timestep = 454. State = [[-0.09879466  0.2370103 ]]. Action = [[ 0.18970385 -0.12174301 -0.15929785  0.14174223]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 0
Current timestep = 455. State = [[-0.09830119  0.23811781]]. Action = [[-0.0826364   0.13855201 -0.15019993 -0.926465  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 455 is [True, False, False, False, False, True]
Scene graph at timestep 455 is [True, False, False, False, False, True]
State prediction error at timestep 455 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.09450007  0.23237498]]. Action = [[ 0.18104574 -0.21394275 -0.1762869  -0.61072975]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 456 is [True, False, False, False, False, True]
Scene graph at timestep 456 is [True, False, False, False, False, True]
State prediction error at timestep 456 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of 1
Current timestep = 457. State = [[-0.08454759  0.21656716]]. Action = [[ 0.01350167  0.02817029 -0.16782235  0.55012   ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.07898378  0.21885121]]. Action = [[ 0.18641663  0.05710509 -0.17070624 -0.4816119 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.07827752  0.22876427]]. Action = [[-0.19937472  0.12425774  0.23621953 -0.9359866 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 459 is [True, False, False, False, False, True]
Current timestep = 460. State = [[-0.08818935  0.2432752 ]]. Action = [[-0.0933557   0.07756406  0.10779217  0.46719658]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 460 is [True, False, False, False, False, True]
Current timestep = 461. State = [[-0.08873132  0.2558374 ]]. Action = [[ 0.21966389  0.13082576 -0.23904711  0.8637917 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.083561    0.27993038]]. Action = [[-0.06630695  0.23669428  0.09663308  0.20513797]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.08256813  0.28801307]]. Action = [[ 0.01672113 -0.15904814 -0.08730575  0.6493238 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 463 is [True, False, False, False, False, True]
Current timestep = 464. State = [[-0.08663974  0.2859855 ]]. Action = [[-0.1842518  -0.01131448  0.02975509 -0.8458324 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 464 is [True, False, False, False, False, True]
Scene graph at timestep 464 is [True, False, False, False, False, True]
State prediction error at timestep 464 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.08610529  0.27340338]]. Action = [[ 0.19890952 -0.16861004  0.10698873 -0.9093706 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 465 is [True, False, False, False, False, True]
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.07180771  0.25416112]]. Action = [[ 0.15134692 -0.06807613 -0.19418126 -0.84924823]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 466 is [True, False, False, False, False, True]
Current timestep = 467. State = [[-0.0639376   0.24989216]]. Action = [[ 0.06283554  0.08116117 -0.0260669   0.59963226]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 467 is [True, False, False, False, False, True]
Current timestep = 468. State = [[-0.05404018  0.2507705 ]]. Action = [[ 0.09479848 -0.0379111  -0.230746    0.8800918 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 468 is [True, False, False, False, False, True]
Current timestep = 469. State = [[-0.04517867  0.25772735]]. Action = [[-0.01880866  0.13320881 -0.23658575 -0.20049655]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 469 is [True, False, False, False, False, True]
Current timestep = 470. State = [[-0.05155726  0.2756069 ]]. Action = [[-0.22371998  0.14921647  0.16021425 -0.4275084 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 470 is [False, True, False, False, False, True]
Current timestep = 471. State = [[-0.06100326  0.28940374]]. Action = [[ 0.20802653  0.1808624   0.10410857 -0.5834103 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 471 is [True, False, False, False, False, True]
Current timestep = 472. State = [[-0.06392469  0.28362784]]. Action = [[-0.09179723 -0.16322787  0.00301558 -0.4543969 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 472 is [True, False, False, False, False, True]
Scene graph at timestep 472 is [True, False, False, False, False, True]
State prediction error at timestep 472 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of 1
Current timestep = 473. State = [[-0.07217392  0.28102252]]. Action = [[-0.09185567  0.09421328  0.22696221 -0.5365717 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 473 is [True, False, False, False, False, True]
Current timestep = 474. State = [[-0.08440834  0.28974533]]. Action = [[-0.1689641  -0.0024852   0.18709594 -0.7919264 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 474 is [True, False, False, False, False, True]
Current timestep = 475. State = [[-0.10092594  0.28902462]]. Action = [[ 0.11442697  0.18617764  0.13784203 -0.08232307]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 475 is [True, False, False, False, False, True]
Current timestep = 476. State = [[-0.09587625  0.27644238]]. Action = [[ 0.2277242  -0.1975854   0.02463996  0.98441803]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 476 is [True, False, False, False, False, True]
Scene graph at timestep 476 is [True, False, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of 1
Current timestep = 477. State = [[-0.07884009  0.24683349]]. Action = [[ 0.17247519 -0.19287492 -0.074266    0.06391931]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 477 is [True, False, False, False, False, True]
Scene graph at timestep 477 is [True, False, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of 1
Current timestep = 478. State = [[-0.07236476  0.23646899]]. Action = [[-0.24020524  0.1258825  -0.02556829 -0.11673772]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 478 is [True, False, False, False, False, True]
Current timestep = 479. State = [[-0.07524309  0.23198861]]. Action = [[ 0.17670289 -0.2250858   0.22323376  0.71176267]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 479 is [True, False, False, False, False, True]
Current timestep = 480. State = [[-0.06108768  0.21024965]]. Action = [[ 0.20880228 -0.09992005  0.01536599  0.08751559]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 480 is [True, False, False, False, False, True]
Current timestep = 481. State = [[-0.04793466  0.20910105]]. Action = [[ 0.06580424  0.19336611 -0.1422631   0.84089065]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 481 is [True, False, False, False, False, True]
Current timestep = 482. State = [[-0.03668546  0.23151882]]. Action = [[ 0.12964743  0.19488439 -0.01917592  0.18499887]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 482 is [False, True, False, False, False, True]
Scene graph at timestep 482 is [False, True, False, False, False, True]
State prediction error at timestep 482 is tensor(4.0590e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.02408768  0.24728057]]. Action = [[-0.16938068 -0.07652813  0.2136347  -0.59774786]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 483 is [False, True, False, False, False, True]
Scene graph at timestep 483 is [False, True, False, False, False, True]
State prediction error at timestep 483 is tensor(9.1565e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 1
Current timestep = 484. State = [[-0.02667483  0.24920937]]. Action = [[0.0150952  0.08819038 0.17094827 0.26357877]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 484 is [False, True, False, False, False, True]
Current timestep = 485. State = [[-0.03202406  0.26442048]]. Action = [[-0.04419813  0.20763892  0.17024595 -0.20580006]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 485 is [False, True, False, False, False, True]
Current timestep = 486. State = [[-0.04126669  0.2698711 ]]. Action = [[-0.13120992 -0.17015085  0.15237087  0.7563572 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 486 is [False, True, False, False, False, True]
Current timestep = 487. State = [[-0.04197546  0.2691672 ]]. Action = [[0.11543661 0.09991309 0.05824989 0.41366315]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 487 is [False, True, False, False, False, True]
Current timestep = 488. State = [[-0.03458747  0.25798583]]. Action = [[ 0.15801293 -0.24213639 -0.13302626 -0.2948202 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 488 is [False, True, False, False, False, True]
Scene graph at timestep 488 is [False, True, False, False, False, True]
State prediction error at timestep 488 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 0
Current timestep = 489. State = [[-0.02009947  0.25055388]]. Action = [[ 0.1305424   0.19117904 -0.14661524  0.42352867]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 489 is [False, True, False, False, False, True]
Current timestep = 490. State = [[-0.00558564  0.26809296]]. Action = [[ 0.23338544  0.11457014 -0.15928534 -0.31329173]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 490 is [False, True, False, False, False, True]
Current timestep = 491. State = [[0.02403972 0.28478995]]. Action = [[0.2301634  0.13845277 0.16477907 0.7065414 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 491 is [False, True, False, False, False, True]
Current timestep = 492. State = [[0.05173862 0.2808733 ]]. Action = [[ 0.07803568 -0.22432692 -0.1412865  -0.4170683 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 492 is [False, True, False, False, False, True]
Current timestep = 493. State = [[0.06597791 0.26948023]]. Action = [[ 0.14061522  0.2441319   0.00468659 -0.94136876]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 493 is [False, False, True, False, False, True]
Current timestep = 494. State = [[0.06179846 0.28063038]]. Action = [[-0.22255702  0.19256335  0.23003694  0.38762105]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 494 is [False, False, True, False, False, True]
Current timestep = 495. State = [[0.04696352 0.3037231 ]]. Action = [[-0.17006469  0.08937842  0.05605942  0.17661774]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 495 is [False, False, True, False, False, True]
Current timestep = 496. State = [[0.03606915 0.3142152 ]]. Action = [[-0.114998    0.17642078 -0.07510832 -0.857322  ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 496 is [False, True, False, False, False, True]
Scene graph at timestep 496 is [False, True, False, False, False, True]
State prediction error at timestep 496 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[0.03271971 0.31148905]]. Action = [[-0.12006624 -0.10254043 -0.16040006  0.04281807]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 497 is [False, True, False, False, False, True]
Current timestep = 498. State = [[0.02111164 0.2931705 ]]. Action = [[-0.17035395 -0.2427064  -0.24712335  0.652768  ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 498 is [False, True, False, False, False, True]
Current timestep = 499. State = [[-0.00092951  0.2641658 ]]. Action = [[-0.16102496 -0.17911637  0.22122234  0.9094808 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 499 is [False, True, False, False, False, True]
Scene graph at timestep 499 is [False, True, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.02963436  0.25003263]]. Action = [[-0.06709281  0.11773223  0.13269597 -0.01565856]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 500 is [False, True, False, False, False, True]
Current timestep = 501. State = [[-0.04132051  0.24902576]]. Action = [[-0.1964821  -0.17346531 -0.1082387  -0.74547243]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 501 is [False, True, False, False, False, True]
Current timestep = 502. State = [[-0.06435242  0.24237154]]. Action = [[-0.09782106  0.05148268 -0.1389723   0.6221323 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 502 is [False, True, False, False, False, True]
Current timestep = 503. State = [[-0.07346967  0.23474497]]. Action = [[ 0.14171538 -0.10274851 -0.06792819 -0.7430345 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 503 is [True, False, False, False, False, True]
Current timestep = 504. State = [[-0.07065488  0.21534683]]. Action = [[ 0.00222513 -0.21477425  0.09258938 -0.31463242]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 504 is [True, False, False, False, False, True]
Current timestep = 505. State = [[-0.06547306  0.18540603]]. Action = [[ 0.18111843 -0.19379522 -0.14259017  0.92240703]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 505 is [True, False, False, False, False, True]
Scene graph at timestep 505 is [True, False, False, False, False, True]
State prediction error at timestep 505 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 505 of 1
Current timestep = 506. State = [[-0.0553164   0.16129965]]. Action = [[-0.17282791 -0.02666672  0.193492   -0.43330818]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 506 is [True, False, False, False, False, True]
Current timestep = 507. State = [[-0.24730287  0.12290302]]. Action = [[-0.1706976   0.02190778  0.08776397  0.23312259]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 507 is [True, False, False, False, False, True]
Current timestep = 508. State = [[-0.23446767  0.15158139]]. Action = [[ 0.18093586  0.24466133 -0.18133894 -0.3699804 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 508 is [True, False, False, False, True, False]
Current timestep = 509. State = [[-0.21265332  0.1700738 ]]. Action = [[ 0.137429    0.02668765 -0.03099737 -0.6583623 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 509 is [True, False, False, False, False, True]
Scene graph at timestep 509 is [True, False, False, False, False, True]
State prediction error at timestep 509 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.1907548   0.16721307]]. Action = [[-0.02520871 -0.21490715  0.02068222  0.02778733]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 510 is [True, False, False, False, False, True]
Current timestep = 511. State = [[-0.19249949  0.16601299]]. Action = [[ 0.0126633   0.21270347  0.15051332 -0.9225982 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 511 is [True, False, False, False, False, True]
Current timestep = 512. State = [[-0.19839263  0.16534235]]. Action = [[-0.17818294 -0.21326497 -0.11711293 -0.21602023]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 512 is [True, False, False, False, False, True]
Current timestep = 513. State = [[-0.20847222  0.16791297]]. Action = [[-0.13178673  0.16076446  0.14388806  0.12467504]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 513 is [True, False, False, False, False, True]
Scene graph at timestep 513 is [True, False, False, False, False, True]
State prediction error at timestep 513 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of 0
Current timestep = 514. State = [[-0.23217216  0.16554694]]. Action = [[-0.21673349 -0.22184896  0.15588576  0.8313072 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 514 is [True, False, False, False, False, True]
Current timestep = 515. State = [[-0.24402517  0.15774794]]. Action = [[ 0.13824117  0.13984394 -0.19776928 -0.04937989]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 515 is [True, False, False, False, False, True]
Current timestep = 516. State = [[-0.23951797  0.15345891]]. Action = [[ 0.06986219 -0.15247603  0.04547748  0.8499931 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 516 is [True, False, False, False, False, True]
Current timestep = 517. State = [[-0.23360233  0.13819022]]. Action = [[ 0.04047921 -0.10699952  0.18554106 -0.71357924]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 517 is [True, False, False, False, False, True]
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.21940942  0.13974737]]. Action = [[ 0.2348842   0.21529871  0.23823082 -0.41511655]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 518 is [True, False, False, False, False, True]
Current timestep = 519. State = [[-0.2018846   0.16199815]]. Action = [[ 0.07838666  0.20184204  0.22260648 -0.84973145]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 519 is [True, False, False, False, False, True]
Current timestep = 520. State = [[-0.19335963  0.18613042]]. Action = [[ 0.00947225  0.1350035  -0.06055897  0.5113349 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 520 is [True, False, False, False, False, True]
Current timestep = 521. State = [[-0.17996721  0.1932195 ]]. Action = [[ 0.21045327 -0.09392901 -0.2166335  -0.5130476 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 521 is [True, False, False, False, False, True]
Current timestep = 522. State = [[-0.16744418  0.19562443]]. Action = [[-0.03703351  0.06478393  0.13625139  0.12701869]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 522 is [True, False, False, False, False, True]
Current timestep = 523. State = [[-0.16679364  0.19489706]]. Action = [[-0.12432751 -0.10133451  0.08731812  0.16983438]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 523 is [True, False, False, False, False, True]
Scene graph at timestep 523 is [True, False, False, False, False, True]
State prediction error at timestep 523 is tensor(8.7491e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 1
Current timestep = 524. State = [[-0.1631338   0.19327928]]. Action = [[ 0.20294678  0.1034047  -0.16810043  0.6558795 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 524 is [True, False, False, False, False, True]
Current timestep = 525. State = [[-0.14568485  0.18361938]]. Action = [[ 0.19278467 -0.22991435 -0.20051707  0.25636327]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 525 is [True, False, False, False, False, True]
Scene graph at timestep 525 is [True, False, False, False, False, True]
State prediction error at timestep 525 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of 1
Current timestep = 526. State = [[-0.13047418  0.17822674]]. Action = [[-0.11452138  0.12942505  0.2437712   0.21856296]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 526 is [True, False, False, False, False, True]
Current timestep = 527. State = [[-0.12963584  0.18592834]]. Action = [[ 0.15057015  0.04863223 -0.02346158  0.55097866]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 527 is [True, False, False, False, False, True]
Current timestep = 528. State = [[-0.1299217   0.19913055]]. Action = [[-0.15255609  0.14291728  0.06892771 -0.28848517]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 528 is [True, False, False, False, False, True]
Current timestep = 529. State = [[-0.13708127  0.20850085]]. Action = [[-0.10318421 -0.07403699 -0.23090872 -0.95994925]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 529 is [True, False, False, False, False, True]
Current timestep = 530. State = [[-0.13607384  0.19598883]]. Action = [[ 0.02783585 -0.22059509 -0.09624696 -0.48593456]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 530 is [True, False, False, False, False, True]
Current timestep = 531. State = [[-0.14358218  0.18675338]]. Action = [[-0.19039895  0.05183348 -0.07714233 -0.078071  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 531 is [True, False, False, False, False, True]
Scene graph at timestep 531 is [True, False, False, False, False, True]
State prediction error at timestep 531 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 531 of 0
Current timestep = 532. State = [[-0.15167916  0.18216072]]. Action = [[ 0.06487137 -0.09568976 -0.04168448  0.8804867 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 532 is [True, False, False, False, False, True]
Scene graph at timestep 532 is [True, False, False, False, False, True]
State prediction error at timestep 532 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of 1
Current timestep = 533. State = [[-0.15448736  0.18432285]]. Action = [[-0.02391708  0.20522821  0.17347315 -0.9275312 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 533 is [True, False, False, False, False, True]
Current timestep = 534. State = [[-0.15172702  0.18488234]]. Action = [[ 0.18753296 -0.18288948  0.2314497   0.9276936 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 534 is [True, False, False, False, False, True]
Current timestep = 535. State = [[-0.14444523  0.16549662]]. Action = [[-0.13423288 -0.1807792   0.11586896 -0.5609815 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 535 is [True, False, False, False, False, True]
Current timestep = 536. State = [[-0.14179924  0.14392722]]. Action = [[ 0.10747188 -0.15139766  0.05406547  0.10903513]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 536 is [True, False, False, False, False, True]
Current timestep = 537. State = [[-0.14372891  0.14435188]]. Action = [[-0.13951388  0.24775308  0.14145437 -0.1372    ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 537 is [True, False, False, False, False, True]
Current timestep = 538. State = [[-0.15123296  0.15869232]]. Action = [[ 0.06851664  0.05379736 -0.07909927  0.5077443 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 538 is [True, False, False, False, False, True]
Current timestep = 539. State = [[-0.15406838  0.15552212]]. Action = [[-0.11975509 -0.16366667 -0.08775182  0.4692222 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 539 is [True, False, False, False, False, True]
Current timestep = 540. State = [[-0.16030218  0.15808374]]. Action = [[-0.01085529  0.17095262 -0.21794891  0.43005598]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 540 is [True, False, False, False, False, True]
Current timestep = 541. State = [[-0.17177594  0.16529255]]. Action = [[-0.20399928 -0.04563856 -0.07667109  0.5780692 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 541 is [True, False, False, False, False, True]
Current timestep = 542. State = [[-0.1897214   0.16260773]]. Action = [[-0.15585682 -0.07020113  0.14181122 -0.43967474]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 542 is [True, False, False, False, False, True]
Current timestep = 543. State = [[-0.19909716  0.16254146]]. Action = [[ 0.20527393  0.15076023 -0.18476056 -0.17407256]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 543 is [True, False, False, False, False, True]
Scene graph at timestep 543 is [True, False, False, False, False, True]
State prediction error at timestep 543 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of 1
Current timestep = 544. State = [[-0.19367428  0.15818025]]. Action = [[-0.05206381 -0.22458906  0.04758719  0.12778687]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 544 is [True, False, False, False, False, True]
Current timestep = 545. State = [[-0.20039605  0.13689499]]. Action = [[-0.18528065 -0.19388306 -0.19046536 -0.78337735]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 545 is [True, False, False, False, False, True]
Current timestep = 546. State = [[-0.20491238  0.11833327]]. Action = [[ 0.11718601 -0.05313367 -0.12275401 -0.00523949]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 546 is [True, False, False, False, False, True]
Current timestep = 547. State = [[-0.20354542  0.10963828]]. Action = [[-0.1115666  -0.04216656  0.22980398 -0.4704638 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 547 is [True, False, False, False, True, False]
Current timestep = 548. State = [[-0.21450727  0.10755464]]. Action = [[-0.09524816  0.05892876 -0.1629606  -0.07196218]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.2281459   0.10694175]]. Action = [[-0.11138451 -0.03963929 -0.2275195   0.9646642 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 549 is [True, False, False, False, True, False]
Current timestep = 550. State = [[-0.24110103  0.11573351]]. Action = [[ 0.04999661  0.22812504  0.12727529 -0.55471313]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.24169618  0.12607387]]. Action = [[ 0.1305055  -0.0370298  -0.14166245  0.5445552 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.23931506  0.11573101]]. Action = [[-0.11166431 -0.21322721  0.14516711  0.9531734 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 552 is [True, False, False, False, False, True]
Current timestep = 553. State = [[-0.24831001  0.11313743]]. Action = [[-0.19760095  0.1420241   0.16978669  0.7760217 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 553 is [True, False, False, False, True, False]
Current timestep = 554. State = [[-0.25797966  0.12350414]]. Action = [[0.01299435 0.06378341 0.19007581 0.33838618]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.25193664  0.11661135]]. Action = [[ 0.20212889 -0.19118091 -0.15568106  0.6057565 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 555 is [True, False, False, False, True, False]
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(1.2950e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of -1
Current timestep = 556. State = [[-0.24094579  0.1012363 ]]. Action = [[ 0.0322136  -0.02114698  0.18640172 -0.6890412 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.23521134  0.08991126]]. Action = [[ 0.07204357 -0.14668415 -0.19639352 -0.25569814]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.23626111  0.07487804]]. Action = [[-0.20952497 -0.08216226  0.0277423   0.31566787]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 558 is [True, False, False, False, True, False]
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.24345733  0.06197244]]. Action = [[ 0.17118743 -0.02484618 -0.02194913 -0.11958367]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 559 is [True, False, False, False, True, False]
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.2338551   0.04924544]]. Action = [[ 0.00725129 -0.17114176 -0.17926095  0.12709486]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.23631832  0.039868  ]]. Action = [[-0.16117711  0.05112731  0.22809479 -0.27943856]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.24324818  0.03665394]]. Action = [[ 0.00828254 -0.05446601 -0.02556284 -0.74919814]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 562 is [True, False, False, False, True, False]
Current timestep = 563. State = [[-0.25325468  0.04489896]]. Action = [[-0.15542153  0.20093131  0.04701179 -0.828134  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 563 is [True, False, False, False, True, False]
Current timestep = 564. State = [[-0.26412192  0.04234416]]. Action = [[ 0.0176633  -0.2359968  -0.21834971  0.13503695]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.26438287  0.02679099]]. Action = [[ 0.01284224 -0.06941107  0.20675808 -0.6483246 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-0.2629446  0.0126536]]. Action = [[ 0.07773763 -0.11392258  0.21693718 -0.8676504 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 566 is [True, False, False, False, True, False]
Current timestep = 567. State = [[-0.26070967  0.00232401]]. Action = [[-0.17407255  0.09294751 -0.16470003 -0.29982728]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.25922123 -0.01328334]]. Action = [[ 0.05204278 -0.20938678  0.10306263  0.10628068]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.25254294 -0.03540371]]. Action = [[ 0.07312718 -0.07848132  0.2211715  -0.71071297]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.24830608 -0.05005328]]. Action = [[ 0.04591388 -0.0693852   0.18715376  0.2986846 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 570 is [True, False, False, False, True, False]
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(3.8414e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of 0
Current timestep = 571. State = [[-0.24219105 -0.04837923]]. Action = [[ 0.06977311  0.17671311 -0.00222914  0.7197032 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 571 is [True, False, False, False, True, False]
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.23588747 -0.0443699 ]]. Action = [[-0.04361975 -0.17312597 -0.20298737  0.723572  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 572 is [True, False, False, False, True, False]
Current timestep = 573. State = [[-0.23221679 -0.06357665]]. Action = [[ 0.04739475 -0.15344529  0.19494736  0.40267038]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 573 is [True, False, False, False, True, False]
Current timestep = 574. State = [[-0.23165978 -0.06895838]]. Action = [[-0.05028766  0.151842   -0.24440223  0.7760961 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 574 is [True, False, False, False, True, False]
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(1.6657e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 0
Current timestep = 575. State = [[-0.23243028 -0.06326894]]. Action = [[ 0.10409808 -0.0289337   0.18053532  0.5041468 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 575 is [True, False, False, False, True, False]
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of 0
Current timestep = 576. State = [[-0.22730225 -0.06789529]]. Action = [[ 0.08710036 -0.07068902  0.18534037  0.5330571 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 576 is [True, False, False, False, True, False]
Current timestep = 577. State = [[-0.21212988 -0.08377679]]. Action = [[ 0.16433114 -0.22563292  0.21418542 -0.8638539 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.19802813 -0.10581969]]. Action = [[ 0.02547231 -0.09095728 -0.23730066  0.05813205]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.19144474 -0.10449507]]. Action = [[ 0.00928825  0.20512211 -0.09589666  0.13985956]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.18459411 -0.08246659]]. Action = [[ 0.07879356  0.20175827 -0.22210498  0.6695622 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 580 is [True, False, False, False, True, False]
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.17252131 -0.07084899]]. Action = [[ 0.14241272 -0.13990453 -0.24169698 -0.14819866]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.15997227 -0.09027511]]. Action = [[ 0.04649496 -0.23843034  0.1937148   0.7227416 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.14293377 -0.1059071 ]]. Action = [[ 1.9673097e-01  3.0395806e-02 -4.1648746e-04  4.8567343e-01]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 583 is [True, False, False, False, True, False]
Current timestep = 584. State = [[-0.13233688 -0.10470495]]. Action = [[-0.17423433  0.05993378  0.22376499  0.26166463]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 584 is [True, False, False, False, True, False]
Current timestep = 585. State = [[-0.13205685 -0.09349708]]. Action = [[ 0.087479    0.1665417   0.23125479 -0.06171811]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 585 is [True, False, False, False, True, False]
Current timestep = 586. State = [[-0.12500918 -0.07547181]]. Action = [[0.15148073 0.098378   0.06999815 0.60199356]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 586 is [True, False, False, False, True, False]
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of 1
Current timestep = 587. State = [[-0.11046906 -0.07489526]]. Action = [[ 0.08069247 -0.19006371 -0.09777339 -0.26553994]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.09577922 -0.0753806 ]]. Action = [[ 0.15858668  0.17499265 -0.04371512  0.8113072 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.08570376 -0.05739209]]. Action = [[-0.16510099  0.1741043   0.24742126  0.81046784]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 589 is [True, False, False, False, True, False]
Current timestep = 590. State = [[-0.08145733 -0.0366372 ]]. Action = [[ 0.21493569  0.11494228 -0.13631223 -0.8092564 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 590 is [True, False, False, False, True, False]
Scene graph at timestep 590 is [True, False, False, False, True, False]
State prediction error at timestep 590 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of 1
Current timestep = 591. State = [[-0.05879997 -0.0247805 ]]. Action = [[ 0.22552225 -0.02123983 -0.09126712 -0.09486938]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 591 is [True, False, False, False, True, False]
Current timestep = 592. State = [[-0.04775568 -0.01133436]]. Action = [[-0.16087759  0.22850543 -0.08505699 -0.80727625]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 592 is [True, False, False, False, True, False]
Current timestep = 593. State = [[-0.05397052  0.01642828]]. Action = [[-0.07054412  0.21755219 -0.22965865  0.8983958 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 593 is [False, True, False, False, True, False]
Current timestep = 594. State = [[-0.06029981  0.02955171]]. Action = [[-0.08071697 -0.10697827  0.1754503  -0.40706074]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.07054888  0.01594933]]. Action = [[-0.20801781 -0.16231613 -0.24121301 -0.8264939 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 595 is [True, False, False, False, True, False]
Current timestep = 596. State = [[-0.08422438  0.00484399]]. Action = [[ 0.03921241 -0.00086983  0.22558296 -0.7375115 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 596 is [True, False, False, False, True, False]
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of 1
Current timestep = 597. State = [[-0.08474588 -0.00412872]]. Action = [[ 0.11581299 -0.0994906   0.12495673  0.07369745]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 597 is [True, False, False, False, True, False]
Current timestep = 598. State = [[-0.08890031 -0.00792007]]. Action = [[-0.21773238  0.03588161  0.01825967 -0.42060268]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 598 is [True, False, False, False, True, False]
Current timestep = 599. State = [[-0.10323451  0.00098341]]. Action = [[-0.12502529  0.13247645 -0.18897747  0.4966612 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 599 is [True, False, False, False, True, False]
Current timestep = 600. State = [[-0.11171904  0.00274248]]. Action = [[ 0.16514963 -0.13253084 -0.02237426  0.5622891 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 600 is [True, False, False, False, True, False]
Current timestep = 601. State = [[-0.10487992  0.00312988]]. Action = [[0.11668199 0.13860428 0.10514218 0.3461156 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 601 is [True, False, False, False, True, False]
Current timestep = 602. State = [[-0.09988964  0.01486074]]. Action = [[-0.06646851  0.09293392  0.15802065 -0.9285892 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 602 is [True, False, False, False, True, False]
Current timestep = 603. State = [[-0.09881564  0.03200274]]. Action = [[ 0.06190628  0.17969954 -0.02303027 -0.5331186 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 603 is [True, False, False, False, True, False]
Current timestep = 604. State = [[-0.10360737  0.0546868 ]]. Action = [[-0.16448885  0.13275534  0.16339952 -0.32552242]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 604 is [True, False, False, False, True, False]
Current timestep = 605. State = [[-0.10270933  0.07965928]]. Action = [[ 0.1897139   0.20774615  0.12941909 -0.23734862]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-0.09145562  0.10762956]]. Action = [[0.08267859 0.20084015 0.11273855 0.9461963 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 606 is [True, False, False, False, True, False]
Current timestep = 607. State = [[-0.0736118   0.13554277]]. Action = [[ 0.23623654  0.15810323  0.00586599 -0.18577456]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.06022379  0.14019944]]. Action = [[-0.20798874 -0.22479519 -0.11063614 -0.10703266]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 608 is [True, False, False, False, False, True]
Current timestep = 609. State = [[-0.06774674  0.11759667]]. Action = [[-0.11069322 -0.20510392 -0.177962    0.9277239 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 609 is [True, False, False, False, False, True]
Current timestep = 610. State = [[-0.07643388  0.10366853]]. Action = [[-0.03072701  0.03966594 -0.02797732 -0.05221719]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.08032428  0.1016671 ]]. Action = [[-0.05532137 -0.01561804 -0.02268241  0.9490771 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 611 is [True, False, False, False, True, False]
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.08714105  0.09211588]]. Action = [[-0.01563954 -0.12329653 -0.11591634  0.28585005]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.09277226  0.0886288 ]]. Action = [[-0.10463598  0.06540972 -0.09956233 -0.92953676]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.09740046  0.09414167]]. Action = [[ 0.1611054   0.07180735 -0.15774162 -0.17724383]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.09186728  0.08536644]]. Action = [[ 0.10174349 -0.23053    -0.13018136  0.08264947]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 615 is [True, False, False, False, True, False]
Current timestep = 616. State = [[-0.08502276  0.08115218]]. Action = [[ 0.00633153  0.17604351 -0.1837595   0.6305376 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.07641733  0.10263432]]. Action = [[0.1479021  0.24083963 0.06728703 0.07132208]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 617 is [True, False, False, False, True, False]
Current timestep = 618. State = [[-0.064798    0.10953844]]. Action = [[-0.02288555 -0.21024501 -0.17449118 -0.31920385]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.06443478  0.11105104]]. Action = [[-0.04449397  0.1874862  -0.23632395 -0.026959  ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 619 is [True, False, False, False, True, False]
Current timestep = 620. State = [[-0.07113905  0.11965491]]. Action = [[-0.15394682 -0.03024103 -0.12077224 -0.88648313]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 620 is [True, False, False, False, True, False]
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of -1
Current timestep = 621. State = [[-0.07495572  0.13286042]]. Action = [[ 0.07782707  0.2041173   0.18924844 -0.42303336]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 621 is [True, False, False, False, True, False]
Current timestep = 622. State = [[-0.07645682  0.13535403]]. Action = [[ 0.03615585 -0.1671368  -0.03180604 -0.94866425]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 622 is [True, False, False, False, False, True]
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 0
Current timestep = 623. State = [[-0.07587697  0.1297946 ]]. Action = [[-0.11215976  0.00874555  0.19849375  0.57582843]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 623 is [True, False, False, False, False, True]
Current timestep = 624. State = [[-0.07116345  0.12132186]]. Action = [[ 0.21705711 -0.11716294  0.00666204 -0.43205398]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 624 is [True, False, False, False, False, True]
Current timestep = 625. State = [[-0.06172019  0.12061329]]. Action = [[0.12136495 0.13464323 0.02607748 0.04426193]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 625 is [True, False, False, False, True, False]
Current timestep = 626. State = [[-0.05822014  0.13223574]]. Action = [[-0.19772077  0.05370981  0.22605824  0.74063444]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 626 is [True, False, False, False, True, False]
Current timestep = 627. State = [[-0.05736408  0.12980375]]. Action = [[ 0.10693777 -0.11981273  0.03995162 -0.43971962]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 627 is [True, False, False, False, False, True]
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of 0
Current timestep = 628. State = [[-0.05495764  0.12381483]]. Action = [[ 0.03468353  0.01035765 -0.0622675  -0.13625139]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 628 is [True, False, False, False, False, True]
Current timestep = 629. State = [[-0.04717927  0.11917154]]. Action = [[ 0.18518344 -0.05160993  0.07069832 -0.8936237 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 629 is [True, False, False, False, True, False]
Current timestep = 630. State = [[-0.18075472  0.22324853]]. Action = [[ 0.11854878 -0.10833135 -0.03272459 -0.6442293 ]]. Reward = [100.]
Curr episode timestep = 122
Scene graph at timestep 630 is [False, True, False, False, True, False]
Scene graph at timestep 630 is [True, False, False, False, False, True]
State prediction error at timestep 630 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 0
Current timestep = 631. State = [[-0.16823725  0.2533402 ]]. Action = [[-0.22126532 -0.04957253  0.24052107 -0.60627633]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 631 is [True, False, False, False, False, True]
Current timestep = 632. State = [[-0.1665726   0.23798013]]. Action = [[ 0.22866368 -0.19989328  0.185009    0.43919384]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 632 is [True, False, False, False, False, True]
Scene graph at timestep 632 is [True, False, False, False, False, True]
State prediction error at timestep 632 is tensor(2.8236e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of 1
Current timestep = 633. State = [[-0.15430835  0.21625571]]. Action = [[ 0.12154788 -0.06393093  0.08995181 -0.9113784 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 633 is [True, False, False, False, False, True]
Current timestep = 634. State = [[-0.14234598  0.20083086]]. Action = [[ 0.09529114 -0.15883423 -0.19424552  0.02195382]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 634 is [True, False, False, False, False, True]
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of 1
Current timestep = 635. State = [[-0.13419414  0.17930876]]. Action = [[-0.2225314  -0.19911714 -0.10326932 -0.31696832]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 635 is [True, False, False, False, False, True]
Current timestep = 636. State = [[-0.13216758  0.17274342]]. Action = [[0.22465831 0.12162033 0.05154249 0.86204123]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 636 is [True, False, False, False, False, True]
Current timestep = 637. State = [[-0.13681786  0.19110541]]. Action = [[-0.20892033  0.23769292 -0.21150912 -0.8491319 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 637 is [True, False, False, False, False, True]
Current timestep = 638. State = [[-0.13811164  0.1962217 ]]. Action = [[ 0.23501706 -0.16471542 -0.21782444  0.47263825]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 638 is [True, False, False, False, False, True]
Current timestep = 639. State = [[-0.12590623  0.17433201]]. Action = [[ 0.09466133 -0.22247908  0.17656484  0.65889525]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 639 is [True, False, False, False, False, True]
Current timestep = 640. State = [[-0.11163283  0.15772364]]. Action = [[ 0.1128121  -0.00226644 -0.2029261   0.9713383 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 640 is [True, False, False, False, False, True]
Current timestep = 641. State = [[-0.09555262  0.16044472]]. Action = [[ 0.07884708  0.09723064 -0.20795064  0.2930832 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 641 is [True, False, False, False, False, True]
Current timestep = 642. State = [[-0.07919612  0.15329184]]. Action = [[ 0.13154483 -0.1949248   0.1906476  -0.24843872]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 642 is [True, False, False, False, False, True]
Current timestep = 643. State = [[-0.06967418  0.15513086]]. Action = [[ 0.0081155   0.2133323  -0.22100766 -0.4366569 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 643 is [True, False, False, False, False, True]
Current timestep = 644. State = [[-0.06390784  0.15803917]]. Action = [[-0.07001793 -0.15235509  0.06751767 -0.91431195]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 644 is [True, False, False, False, False, True]
Current timestep = 645. State = [[-0.06399329  0.1505033 ]]. Action = [[-0.11308227 -0.07522812 -0.17848475 -0.9053249 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 645 is [True, False, False, False, False, True]
Scene graph at timestep 645 is [True, False, False, False, False, True]
State prediction error at timestep 645 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of 1
Current timestep = 646. State = [[-0.06297171  0.13395661]]. Action = [[ 0.01987091 -0.1648565   0.02973938 -0.34029722]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 646 is [True, False, False, False, False, True]
Scene graph at timestep 646 is [True, False, False, False, False, True]
State prediction error at timestep 646 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of 1
Current timestep = 647. State = [[-0.06210308  0.10760283]]. Action = [[-0.01634532 -0.22726384  0.06373864  0.2858305 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 647 is [True, False, False, False, False, True]
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of 1
Current timestep = 648. State = [[-0.06758189  0.08592592]]. Action = [[ 0.05509901 -0.00864096 -0.23816103  0.07985198]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 648 is [True, False, False, False, True, False]
Current timestep = 649. State = [[-0.06824091  0.07823322]]. Action = [[-0.10830635 -0.11249945  0.03163502  0.9021399 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 649 is [True, False, False, False, True, False]
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of 1
Current timestep = 650. State = [[-0.06931101  0.06069923]]. Action = [[-0.0198846  -0.12751092  0.03071004  0.577024  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 650 is [True, False, False, False, True, False]
Current timestep = 651. State = [[-0.07391455  0.0589697 ]]. Action = [[-0.02358793  0.13448143 -0.06385627 -0.4882815 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 651 is [True, False, False, False, True, False]
Current timestep = 652. State = [[-0.07653962  0.06028149]]. Action = [[-0.00353995 -0.07084525 -0.21723716  0.35884273]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 652 is [True, False, False, False, True, False]
Current timestep = 653. State = [[-0.07325655  0.04638901]]. Action = [[ 0.09154457 -0.18281199  0.02750748  0.3567654 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 653 is [True, False, False, False, True, False]
Current timestep = 654. State = [[-0.06623196  0.02791543]]. Action = [[ 0.12322509 -0.06921959 -0.00535296 -0.58972925]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 654 is [True, False, False, False, True, False]
Current timestep = 655. State = [[-0.06220941  0.02493477]]. Action = [[0.06712925 0.11804533 0.13662028 0.6912141 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 655 is [True, False, False, False, True, False]
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of 1
Current timestep = 656. State = [[-0.06225483  0.03929903]]. Action = [[-0.1505014   0.15554851 -0.18795373 -0.6691477 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 656 is [True, False, False, False, True, False]
Current timestep = 657. State = [[-0.06072806  0.05889253]]. Action = [[0.21850133 0.14003909 0.2430749  0.13574648]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.05808502  0.07928401]]. Action = [[-0.20303862  0.14636186  0.14424568  0.9007046 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of -1
Current timestep = 659. State = [[-0.06931496  0.09214412]]. Action = [[-0.15105247 -0.08483163 -0.21885481  0.84105384]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 659 is [True, False, False, False, True, False]
Current timestep = 660. State = [[-0.0693851   0.08630285]]. Action = [[ 0.20465446 -0.0279486  -0.0616475  -0.42060113]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 660 is [True, False, False, False, True, False]
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 0
Current timestep = 661. State = [[-0.05909974  0.07634168]]. Action = [[ 0.19830751 -0.12933412  0.06583568  0.35842097]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 661 is [True, False, False, False, True, False]
Current timestep = 662. State = [[-0.05146414  0.0542135 ]]. Action = [[-0.22902168 -0.17869973 -0.11456674 -0.6829992 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 662 is [True, False, False, False, True, False]
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of 1
Current timestep = 663. State = [[-0.057306    0.04697543]]. Action = [[ 0.01579303  0.15017587  0.01446146 -0.7789418 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 663 is [True, False, False, False, True, False]
Current timestep = 664. State = [[-0.05746561  0.06393335]]. Action = [[ 0.11670783  0.15109795  0.11374059 -0.4637233 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 664 is [True, False, False, False, True, False]
Current timestep = 665. State = [[-0.04909823  0.08076909]]. Action = [[ 0.18492413  0.12476724 -0.11469412 -0.37877393]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 665 is [True, False, False, False, True, False]
Current timestep = 666. State = [[-0.03769124  0.0905925 ]]. Action = [[-0.13890415 -0.03710917 -0.07121277 -0.41205204]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 666 is [False, True, False, False, True, False]
Current timestep = 667. State = [[-0.04057156  0.09743933]]. Action = [[-0.04158053  0.08925134  0.08078653  0.40499198]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 667 is [False, True, False, False, True, False]
Current timestep = 668. State = [[-0.04747474  0.10619586]]. Action = [[-0.17937323  0.01127318 -0.14809565 -0.5748831 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 668 is [False, True, False, False, True, False]
Current timestep = 669. State = [[-0.05167743  0.09689302]]. Action = [[ 0.13516885 -0.21827094 -0.19358851 -0.31253517]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 669 is [False, True, False, False, True, False]
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of 0
Current timestep = 670. State = [[-0.0533888   0.07569649]]. Action = [[-0.19409199 -0.10085273  0.01250854 -0.4674405 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.06516208  0.07224227]]. Action = [[-0.00208394  0.10103378  0.04948118 -0.301816  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.06567174  0.07490351]]. Action = [[ 0.15618557 -0.02463458  0.22774729 -0.19946277]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 672 is [True, False, False, False, True, False]
Current timestep = 673. State = [[-0.05702732  0.07331934]]. Action = [[ 0.1840331  -0.0072047  -0.17838787 -0.5377625 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 673 is [True, False, False, False, True, False]
Current timestep = 674. State = [[-0.04392716  0.06387178]]. Action = [[ 0.08460563 -0.13835151  0.2349475  -0.8724349 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.17248096 -0.15544485]]. Action = [[-0.09643778 -0.09348272  0.06902161 -0.48854482]]. Reward = [100.]
Curr episode timestep = 44
Scene graph at timestep 675 is [False, True, False, False, True, False]
Current timestep = 676. State = [[-0.1504969  -0.16535129]]. Action = [[ 0.18286046  0.15532047 -0.00818098 -0.7145439 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 676 is [True, False, False, True, False, False]
Scene graph at timestep 676 is [True, False, False, True, False, False]
State prediction error at timestep 676 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 676 of 1
Current timestep = 677. State = [[-0.1310022  -0.14991324]]. Action = [[-0.09747404  0.13810045 -0.08691716 -0.06968164]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 677 is [True, False, False, True, False, False]
Current timestep = 678. State = [[-0.13459717 -0.1321807 ]]. Action = [[-0.08283301  0.14197123  0.10704741 -0.90602314]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 678 is [True, False, False, True, False, False]
Scene graph at timestep 678 is [True, False, False, True, False, False]
State prediction error at timestep 678 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.13401991 -0.12691696]]. Action = [[ 0.18754178 -0.16386585 -0.09194846  0.9020455 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 679 is [True, False, False, True, False, False]
Current timestep = 680. State = [[-0.12366977 -0.12474516]]. Action = [[ 0.16198856  0.15699399  0.16533726 -0.2152937 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 680 is [True, False, False, True, False, False]
Current timestep = 681. State = [[-0.10390197 -0.1084448 ]]. Action = [[ 0.14608526  0.1794216  -0.14277561  0.92969465]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 681 is [True, False, False, False, True, False]
Current timestep = 682. State = [[-0.0809212  -0.09538095]]. Action = [[0.18110293 0.00206172 0.15946233 0.2563653 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 682 is [True, False, False, False, True, False]
Current timestep = 683. State = [[-0.05377047 -0.09328101]]. Action = [[ 0.22538012 -0.01137894 -0.06744236 -0.14819187]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 683 is [True, False, False, False, True, False]
Current timestep = 684. State = [[-0.162077   -0.21573812]]. Action = [[-0.10674967  0.1827026   0.16218787  0.08550811]]. Reward = [100.]
Curr episode timestep = 8
Scene graph at timestep 684 is [True, False, False, False, True, False]
Current timestep = 685. State = [[-0.14379132 -0.2522679 ]]. Action = [[-0.20214736  0.03877065 -0.13164866  0.3780334 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 685 is [True, False, False, True, False, False]
Scene graph at timestep 685 is [True, False, False, True, False, False]
State prediction error at timestep 685 is tensor(0.0008, grad_fn=<MseLossBackward0>)
