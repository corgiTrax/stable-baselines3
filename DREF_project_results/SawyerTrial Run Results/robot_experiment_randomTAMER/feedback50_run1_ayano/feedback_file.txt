Current timestep = 0. State = [[-0.20675376  0.2294788 ]]. Action = [[ 0.06297028 -0.07997862  0.10210544  0.69326186]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.21229893  0.2363805 ]]. Action = [[-0.23294938  0.16463757 -0.17288275 -0.9050059 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, False, True]
Scene graph at timestep 1 is [True, False, False, False, False, True]
State prediction error at timestep 1 is tensor(0.0600, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.22153415  0.2385934 ]]. Action = [[ 0.11270288 -0.17563263  0.23866731 -0.8213406 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Scene graph at timestep 2 is [True, False, False, False, False, True]
State prediction error at timestep 2 is tensor(0.0392, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of 0
Current timestep = 3. State = [[-0.21581554  0.215326  ]]. Action = [[-0.15697455 -0.21786267 -0.08751446 -0.94051087]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, False, True]
Scene graph at timestep 3 is [True, False, False, False, False, True]
State prediction error at timestep 3 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of 0
Current timestep = 4. State = [[-0.22984542  0.19694191]]. Action = [[-0.22494343 -0.0803238  -0.01224574 -0.9660611 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, False, True]
Current timestep = 5. State = [[-0.2527636   0.20463009]]. Action = [[-0.15818407  0.22565243 -0.1780761  -0.8144088 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Current timestep = 6. State = [[-0.26992124  0.22059081]]. Action = [[-0.22981334 -0.23627879 -0.01330312 -0.9404658 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Current timestep = 7. State = [[-0.26451382  0.21069309]]. Action = [[ 0.24866575 -0.16088504  0.10769138 -0.4377445 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0186, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25615376  0.19819844]]. Action = [[-0.12282422 -0.00707684  0.23058265  0.691998  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Scene graph at timestep 8 is [True, False, False, False, False, True]
State prediction error at timestep 8 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.2590067   0.20752504]]. Action = [[0.11638898 0.2231611  0.18111795 0.40299213]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Scene graph at timestep 9 is [True, False, False, False, False, True]
State prediction error at timestep 9 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.25705546  0.21913661]]. Action = [[-0.1154967  -0.11649746 -0.12395468 -0.8622832 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.25411892  0.22088712]]. Action = [[ 0.09639394  0.01218113 -0.18695937 -0.25775957]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Current timestep = 12. State = [[-0.2407839  0.2214684]]. Action = [[ 0.13120094 -0.05176856  0.24404219 -0.14206684]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of 0
Current timestep = 13. State = [[-0.23026429  0.21694069]]. Action = [[-0.16043296 -0.0772499  -0.11024956 -0.09586716]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.23965941  0.21073452]]. Action = [[-0.1948788  -0.08970112  0.20015681 -0.14285451]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.2528793   0.20406924]]. Action = [[-0.23053473 -0.15109642 -0.22890939 -0.08265281]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.25555137  0.2047137 ]]. Action = [[ 0.02988482  0.05245471 -0.15917453 -0.69042975]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Scene graph at timestep 16 is [True, False, False, False, False, True]
State prediction error at timestep 16 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.25157097  0.21220991]]. Action = [[ 0.17601234  0.15844116  0.24486    -0.9515029 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.2417378   0.21618442]]. Action = [[ 0.09605157 -0.07710722  0.18926516  0.577683  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.22702591  0.2128392 ]]. Action = [[ 0.19603062 -0.04392283 -0.21405743  0.8010032 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Scene graph at timestep 19 is [True, False, False, False, False, True]
State prediction error at timestep 19 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.19809647  0.20706886]]. Action = [[ 0.19715497 -0.02018283 -0.20228624  0.00409985]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.17566092  0.21066317]]. Action = [[0.17293802 0.072833   0.24115604 0.71215963]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.16070385  0.20348729]]. Action = [[-0.10588528 -0.20917185  0.1015079   0.85525775]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Scene graph at timestep 22 is [True, False, False, False, False, True]
State prediction error at timestep 22 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.15865165  0.19049594]]. Action = [[ 0.02341968  0.05242613 -0.02491967 -0.9092844 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Scene graph at timestep 23 is [True, False, False, False, False, True]
State prediction error at timestep 23 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.1610135   0.18588495]]. Action = [[-0.14956585 -0.1441186  -0.01197198  0.40447593]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.16892512  0.16896586]]. Action = [[-0.11580712 -0.18439679  0.1669775   0.6858113 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Scene graph at timestep 25 is [True, False, False, False, False, True]
State prediction error at timestep 25 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.18488292  0.14114197]]. Action = [[-0.18497057 -0.17965198 -0.03084236 -0.28676426]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, False, True]
State prediction error at timestep 26 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.20749594  0.11660882]]. Action = [[-0.11439711 -0.14005344 -0.17456888  0.7042203 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.22767353  0.11062942]]. Action = [[-0.1935679   0.08352953  0.10463333  0.9692484 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.24446818  0.10937987]]. Action = [[-0.01994668 -0.07179871 -0.20162529 -0.18501735]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.25387037  0.10771559]]. Action = [[-0.13494813  0.02488905  0.20636916  0.46629786]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.26728955  0.09792655]]. Action = [[ 0.00268981 -0.1696655  -0.10758856 -0.9699385 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.26686436  0.0957541 ]]. Action = [[0.1447056  0.20421523 0.20660102 0.6810794 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.26550516  0.10245445]]. Action = [[-0.06545827 -0.17488025 -0.07722718 -0.51513356]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.26411873  0.09545415]]. Action = [[-0.05525461 -0.19106214  0.0124734  -0.16725749]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.26410255  0.0948599 ]]. Action = [[0.06660628 0.20718282 0.18542707 0.6509657 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.25646183  0.09412114]]. Action = [[ 0.18259865 -0.16975722  0.02361524 -0.79204744]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.2356361   0.09611673]]. Action = [[ 0.2300612   0.1685077  -0.10571416 -0.42865014]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.2171954   0.09246878]]. Action = [[-0.04597017 -0.21804081 -0.12066405 -0.6908584 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.20892662  0.07978638]]. Action = [[ 0.13551456 -0.01491544 -0.10230634  0.3191998 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.19894053  0.06571086]]. Action = [[ 0.08752471 -0.17396426 -0.12681985  0.49980378]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.1898907   0.04358505]]. Action = [[-0.04885158 -0.14615749  0.24577534 -0.21605396]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.1912481   0.02854722]]. Action = [[-0.131449   -0.01434618 -0.10266423 -0.6485638 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.19485119  0.03364178]]. Action = [[ 0.09716374  0.15052041 -0.15794088 -0.7770438 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 0
Current timestep = 44. State = [[-0.19655645  0.0524855 ]]. Action = [[-0.00445251  0.18055707  0.0457322  -0.8562316 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.20359056  0.07174557]]. Action = [[-0.14808904  0.04432666 -0.24293654 -0.4679861 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.21454401  0.08718678]]. Action = [[-0.15479623  0.1328103   0.14627117  0.2711128 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.23107755  0.10155804]]. Action = [[-0.13153164  0.02637088 -0.07701327  0.7068567 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of -1
Current timestep = 48. State = [[-0.23830561  0.09571633]]. Action = [[ 0.15391076 -0.14339283 -0.19965442  0.6217265 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.24135233  0.0841765 ]]. Action = [[-0.19826384 -0.05318272 -0.24025185 -0.70063156]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.24755348  0.07504378]]. Action = [[-0.04023093 -0.0264494   0.16772974 -0.58603984]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.25532475  0.07131689]]. Action = [[-0.23240174 -0.19659403  0.08620957  0.5376396 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.24941185  0.0586299 ]]. Action = [[ 0.16357568 -0.21479566 -0.22344208 -0.25281495]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.23769078  0.04740037]]. Action = [[0.1588611  0.13420117 0.18944812 0.8104564 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.22643906  0.04158523]]. Action = [[ 0.07285589 -0.21699832  0.14272448  0.0199616 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.22241919  0.04199129]]. Action = [[-0.05973603  0.23550788  0.00714278  0.07222414]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.212802    0.06143128]]. Action = [[0.23174173 0.1384067  0.22638202 0.6174549 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.19320533  0.06985256]]. Action = [[-0.04598081 -0.13384141  0.08371779 -0.89473045]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.18809326  0.0538593 ]]. Action = [[ 0.12417713 -0.19245711 -0.16977324  0.9273293 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.17975904  0.03264285]]. Action = [[ 0.02763861 -0.04382655 -0.02643728  0.9203427 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.1694519   0.03202584]]. Action = [[0.2146095  0.05625436 0.00956735 0.35110927]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.15898675  0.02754645]]. Action = [[-0.1615316  -0.13352731  0.08756745 -0.01623714]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-0.15315394  0.01086644]]. Action = [[ 0.23119316 -0.15395239 -0.01346993  0.4623202 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.14549182 -0.0067802 ]]. Action = [[-0.11455262 -0.06880702 -0.20160681  0.8897922 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.14675133 -0.01729779]]. Action = [[-0.03350288 -0.07226682  0.2063945  -0.48011035]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.14442995 -0.02358738]]. Action = [[ 0.15254045 -0.00523759 -0.2378371  -0.56581396]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.14536327 -0.03151942]]. Action = [[-0.15940075 -0.09254138 -0.23391786 -0.7769924 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.1567015  -0.04320714]]. Action = [[-0.20152308 -0.01801683 -0.22063683 -0.2816553 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.17471299 -0.03481041]]. Action = [[-0.11798334  0.23271367  0.11612731  0.87334967]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.18360952 -0.01625733]]. Action = [[ 0.20055366 -0.05082978  0.17689866  0.26615334]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.173911   -0.01418829]]. Action = [[ 0.04951864  0.08969873 -0.13358024 -0.09813821]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.17718269  0.00108369]]. Action = [[-0.2227798   0.18584284 -0.23865937  0.5597577 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.18004829  0.01331465]]. Action = [[ 0.20041978 -0.04484339 -0.13035287  0.68227756]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.1814585   0.00896462]]. Action = [[-0.15595107 -0.07748032 -0.06621313  0.18047333]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of 0
Current timestep = 74. State = [[-0.17954049  0.00207517]]. Action = [[ 0.16544127 -0.05090715 -0.134263    0.29893827]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.17067099 -0.00250909]]. Action = [[ 0.12636504 -0.00386262  0.03637156  0.9896952 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.1620806  -0.01648713]]. Action = [[-0.11368649 -0.23971608 -0.0525742   0.6360514 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.15838069 -0.03337404]]. Action = [[ 0.14064264  0.01005349 -0.1022684   0.07939827]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.14884697 -0.04179097]]. Action = [[ 0.12651461 -0.08605808 -0.09521119  0.43082356]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.13932773 -0.04466196]]. Action = [[-0.19134457  0.09395349  0.22458819 -0.24455518]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.14093749 -0.0514861 ]]. Action = [[ 0.12649256 -0.22263016  0.2317172  -0.7619516 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.1357595  -0.06607601]]. Action = [[ 0.08477733 -0.02804723  0.15775913 -0.41969085]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.12644856 -0.06735852]]. Action = [[0.09816968 0.07079238 0.23451573 0.14238942]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.1197313  -0.06389543]]. Action = [[-0.08639833 -0.00082009  0.24285778 -0.5816616 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.1259363 -0.0565959]]. Action = [[-0.21179529  0.14424154 -0.05502217 -0.5944514 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 0
Current timestep = 85. State = [[-0.13747506 -0.04959026]]. Action = [[ 0.08101058 -0.08693689  0.16208643 -0.44542414]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.13809393 -0.04447588]]. Action = [[-0.0213517   0.15784067 -0.17546275  0.2795664 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.14354587 -0.02676786]]. Action = [[-0.15741546  0.16850436 -0.22429167  0.3074422 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.15071093 -0.01501246]]. Action = [[ 0.08694431 -0.03811403  0.20759031 -0.2569611 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 0
Current timestep = 89. State = [[-0.1502508  -0.00821623]]. Action = [[ 0.02357712  0.11641055  0.08329156 -0.6158673 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 0
Current timestep = 90. State = [[-0.14301898 -0.00558308]]. Action = [[ 0.19288152 -0.11066073 -0.05253068 -0.9207579 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.13610741 -0.02010443]]. Action = [[-0.18162997 -0.14734392  0.08098817 -0.10761851]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.14025362 -0.03017234]]. Action = [[ 0.02264714 -0.03778329  0.11721411  0.9092132 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.13852336 -0.04463587]]. Action = [[ 0.11862782 -0.16623856  0.22079581  0.30041075]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 0
Current timestep = 94. State = [[-0.12799893 -0.05571447]]. Action = [[0.12694106 0.10626176 0.21311349 0.33983445]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.1206005 -0.0545667]]. Action = [[-0.15179478 -0.10327896 -0.12473749  0.8563627 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 0
Current timestep = 96. State = [[-0.13586576 -0.07554705]]. Action = [[-0.20095919 -0.20999306 -0.08214353 -0.662046  ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.1449141  -0.08632711]]. Action = [[ 0.11947644  0.05794016 -0.17621647 -0.5784203 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.14901419 -0.07494961]]. Action = [[-0.19410904  0.20757133 -0.22032477 -0.05749863]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.15886761 -0.054128  ]]. Action = [[-0.04869343  0.12219754  0.16539103 -0.42821312]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.17258587 -0.05148011]]. Action = [[-0.14816558 -0.13562346 -0.0332965  -0.37256718]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.18086255 -0.07101345]]. Action = [[ 0.24069217 -0.18736976 -0.17443238  0.67933285]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.17448506 -0.08243779]]. Action = [[-0.07212588 -0.0155998   0.14099619 -0.80371517]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.17654462 -0.09424116]]. Action = [[-0.02966709 -0.15245588 -0.19695029  0.143929  ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.16722062 -0.10520043]]. Action = [[ 0.24193984  0.00118771 -0.20854542 -0.74322873]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.14571527 -0.10415695]]. Action = [[ 0.14439374  0.04364175 -0.22257435 -0.7644946 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-0.13723661 -0.09113868]]. Action = [[-0.12930489  0.20654726 -0.05761319  0.08842719]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.13760445 -0.08770295]]. Action = [[ 0.07836384 -0.21663296 -0.23097752  0.33798194]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of 0
Current timestep = 108. State = [[-0.1346704  -0.10072581]]. Action = [[0.04145688 0.01994115 0.1962052  0.968591  ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0151, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 0
Current timestep = 109. State = [[-0.12978047 -0.10640837]]. Action = [[ 0.10243332 -0.10829228  0.1130299  -0.78720814]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
Current timestep = 110. State = [[-0.12687474 -0.12475489]]. Action = [[-0.11269102 -0.2028882  -0.19829586  0.04242897]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.11935687 -0.14421389]]. Action = [[ 0.20758206 -0.05003083  0.17042291 -0.90610296]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0191, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.10757966 -0.15740086]]. Action = [[-0.10510576 -0.08393916  0.05888137  0.24421203]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
Current timestep = 113. State = [[-0.09977067 -0.16921458]]. Action = [[ 0.24543598 -0.11574626 -0.21737261  0.6079476 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0252, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 0
Current timestep = 114. State = [[-0.07627796 -0.18563809]]. Action = [[ 0.16205528 -0.10564469 -0.12124047  0.6117351 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
Current timestep = 115. State = [[-0.0651833  -0.20237589]]. Action = [[-0.04080585 -0.13473108 -0.21850927  0.9417908 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.05390648 -0.20829047]]. Action = [[ 0.19462427  0.04750109 -0.24241191  0.02132976]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0280, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.03708189 -0.20374729]]. Action = [[-0.09456122  0.11789846  0.0436998   0.2729255 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
Current timestep = 118. State = [[-0.03684932 -0.19929086]]. Action = [[ 0.02178815  0.02822268  0.16133457 -0.0247196 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [False, True, False, True, False, False]
Current timestep = 119. State = [[-0.02799763 -0.20332417]]. Action = [[ 0.23845732 -0.1722359  -0.12156329  0.10952997]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [False, True, False, True, False, False]
Scene graph at timestep 119 is [False, True, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.00383384 -0.20342475]]. Action = [[0.07033941 0.16604191 0.11875281 0.8759477 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [False, True, False, True, False, False]
Current timestep = 121. State = [[ 0.00897671 -0.2058077 ]]. Action = [[ 0.20681167 -0.2260075   0.17042696  0.96659756]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [False, True, False, True, False, False]
Current timestep = 122. State = [[ 0.03217987 -0.22455011]]. Action = [[ 0.11686414 -0.1007576  -0.21948268  0.8940854 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [False, True, False, True, False, False]
Current timestep = 123. State = [[ 0.04147354 -0.24168542]]. Action = [[-0.09447052 -0.08304301  0.12388396 -0.4362682 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [False, True, False, True, False, False]
Current timestep = 124. State = [[ 0.03949853 -0.24764377]]. Action = [[ 0.15652704  0.20097056 -0.15289503  0.6090311 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [False, True, False, True, False, False]
Scene graph at timestep 124 is [False, True, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0460, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of -1
Current timestep = 125. State = [[ 0.03925081 -0.2485577 ]]. Action = [[ 0.21234247  0.20525795 -0.13068047 -0.9171189 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [False, True, False, True, False, False]
Scene graph at timestep 125 is [False, True, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0506, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of -1
Current timestep = 126. State = [[-0.24665487  0.01873713]]. Action = [[-0.23819414 -0.050855    0.0971384   0.31724048]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [False, True, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.23357634  0.02814839]]. Action = [[ 0.19225472  0.11105758  0.22442085 -0.71292186]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, False, True, False]
Current timestep = 128. State = [[-0.21359341  0.04094422]]. Action = [[0.10295907 0.0842523  0.1250441  0.35134006]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.20376404  0.05895649]]. Action = [[-0.00620091  0.16029713  0.01630726  0.69057226]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.18985112  0.0616191 ]]. Action = [[ 0.21063447 -0.17627744 -0.1426346   0.01225626]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 130 is [True, False, False, False, True, False]
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.17296508  0.04429774]]. Action = [[-0.14294283 -0.14810291 -0.02780455 -0.77367955]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 131 is [True, False, False, False, True, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.17530382  0.03090364]]. Action = [[ 4.4057965e-03 -6.2562525e-04  2.3545986e-01 -8.2150519e-01]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.17356059  0.02095889]]. Action = [[ 0.09935349 -0.16550867  0.2024175  -0.92786795]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.16491507 -0.00114705]]. Action = [[ 0.15492868 -0.20165758  0.05736661 -0.64204204]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 134 is [True, False, False, False, True, False]
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.1517241 -0.0193631]]. Action = [[ 0.00737205  0.0123305   0.01025346 -0.5465348 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 135 is [True, False, False, False, True, False]
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.14510302 -0.0115507 ]]. Action = [[0.1484533  0.1469391  0.16946954 0.24240148]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.12843451 -0.0049192 ]]. Action = [[ 0.06305745 -0.07137844 -0.06171218 -0.5112688 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.12401872 -0.0142694 ]]. Action = [[ 0.03363666 -0.11757067 -0.11326978  0.7946279 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.12127657 -0.01174387]]. Action = [[-0.12801588  0.18314135 -0.19030683 -0.72745085]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.12749763  0.01011151]]. Action = [[-0.09579539  0.2027398  -0.11950624 -0.5600505 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 140 is [True, False, False, False, True, False]
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.1301907   0.02509663]]. Action = [[ 0.13409233 -0.06678024  0.13209182 -0.48071373]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.12762487  0.01509161]]. Action = [[-0.02250984 -0.11621073 -0.1359491  -0.4325508 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 142 is [True, False, False, False, True, False]
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.13389547 -0.0028623 ]]. Action = [[-0.22818258 -0.13340597  0.03551456  0.29929113]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.14225422 -0.02269292]]. Action = [[ 0.00382146 -0.1432201  -0.01237091  0.40880716]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 144 is [True, False, False, False, True, False]
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 0
Current timestep = 145. State = [[-0.14147195 -0.02559996]]. Action = [[0.1871855  0.1781036  0.20639211 0.58317184]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 0
Current timestep = 146. State = [[-0.14100966 -0.02757456]]. Action = [[-0.1647377  -0.22274391  0.18458152  0.60629606]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.1450135  -0.04644198]]. Action = [[-0.03223202 -0.10640767 -0.16751577  0.1997652 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.15228666 -0.06676918]]. Action = [[-0.12094423 -0.1521456   0.11803824  0.79781556]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.162958   -0.09197748]]. Action = [[ 0.0168049  -0.15312225 -0.07703364  0.6523117 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 149 is [True, False, False, False, True, False]
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.1634186  -0.09215759]]. Action = [[0.11653167 0.22417665 0.19205412 0.5092902 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 150 is [True, False, False, False, True, False]
Current timestep = 151. State = [[-0.16639923 -0.06675812]]. Action = [[-0.21448363  0.23148441  0.16009802  0.7784225 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.17706315 -0.03557926]]. Action = [[-0.10941809  0.17830843  0.00446367 -0.77872187]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 152 is [True, False, False, False, True, False]
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.18409754 -0.00836447]]. Action = [[0.15131116 0.12340283 0.01667023 0.8673556 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.18503983 -0.00884338]]. Action = [[-0.14977935 -0.15853088 -0.20521441  0.78985476]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of 0
Current timestep = 155. State = [[-0.18960501 -0.02839518]]. Action = [[ 0.08145019 -0.15989484  0.05597031  0.8040612 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 0
Current timestep = 156. State = [[-0.17744076 -0.02852433]]. Action = [[ 0.24582547  0.19624239  0.23325664 -0.99631786]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.16436811 -0.02065426]]. Action = [[-0.14850393 -0.08494878  0.1070435   0.6479156 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.16354445 -0.01676977]]. Action = [[ 0.14172447  0.13259298  0.06128189 -0.8374532 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 0
Current timestep = 159. State = [[-0.15105534 -0.00701859]]. Action = [[ 0.23017126  0.03679082  0.00744146 -0.4101485 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.13117394 -0.00193837]]. Action = [[ 0.0138185   0.03636441 -0.17252213 -0.7582986 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 0
Current timestep = 161. State = [[-0.12746811  0.0131132 ]]. Action = [[ 0.00738209  0.1961757   0.13458169 -0.74053234]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 161 is [True, False, False, False, True, False]
Current timestep = 162. State = [[-0.13195838  0.02198704]]. Action = [[-0.1897417  -0.09319693 -0.24347553  0.6591034 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 162 is [True, False, False, False, True, False]
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.13311169  0.02767911]]. Action = [[ 0.1400671   0.13471356 -0.16915815  0.13944411]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.13558294  0.03894796]]. Action = [[-0.0516565   0.06896824  0.06277427 -0.6336768 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.13359077  0.05817686]]. Action = [[ 0.11262494  0.21090007 -0.21057267 -0.45585823]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.12677462  0.07148696]]. Action = [[-0.16360733 -0.11174148 -0.19446409 -0.37385392]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.12766509  0.06517147]]. Action = [[ 0.16925707  0.01558238  0.05699545 -0.0020718 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.12693909  0.06514331]]. Action = [[-0.08023214 -0.02915728 -0.00284116  0.6647949 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 0
Current timestep = 169. State = [[-0.12668249  0.07562048]]. Action = [[ 0.07641345  0.21282065 -0.15282054 -0.81603736]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 169 is [True, False, False, False, True, False]
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.11695451  0.0820125 ]]. Action = [[ 0.23094082 -0.12922324 -0.04117703  0.91713905]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.09939364  0.08563997]]. Action = [[-0.01362637  0.1361056   0.06960982  0.9046569 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.09297022  0.09681185]]. Action = [[ 0.07047459  0.10586163  0.13488835 -0.943111  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 172 is [True, False, False, False, True, False]
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.07747343  0.09558041]]. Action = [[ 0.14769232 -0.21231593 -0.11242437  0.69720674]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.06248578  0.07378846]]. Action = [[ 0.08370358 -0.16335662  0.16633096 -0.9521764 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 174 is [True, False, False, False, True, False]
Current timestep = 175. State = [[-0.05956229  0.0532828 ]]. Action = [[-0.22020659 -0.12696283 -0.11873877 -0.9037814 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 175 is [True, False, False, False, True, False]
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.06268843  0.04601843]]. Action = [[ 0.15482062  0.13067472  0.24529368 -0.6511129 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 176 is [True, False, False, False, True, False]
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 0
Current timestep = 177. State = [[-0.04885865  0.05113977]]. Action = [[ 0.19711792 -0.07212386  0.19288993 -0.11064178]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 177 is [True, False, False, False, True, False]
Scene graph at timestep 177 is [False, True, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.20405348 -0.08263722]]. Action = [[-0.05565995 -0.18210886  0.01119462 -0.61907697]]. Reward = [100.]
Curr episode timestep = 51
Scene graph at timestep 178 is [False, True, False, False, True, False]
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of 0
Current timestep = 179. State = [[-0.2001557  -0.09700623]]. Action = [[-0.10127768 -0.05439502 -0.21766487  0.6340395 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 179 is [True, False, False, False, True, False]
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of -1
Current timestep = 180. State = [[-0.20946476 -0.09701563]]. Action = [[-0.19482744  0.10990879  0.01144448  0.09039497]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.216705   -0.10374597]]. Action = [[ 0.0048224  -0.1810501   0.03931972  0.15131915]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 181 is [True, False, False, False, True, False]
Current timestep = 182. State = [[-0.22092532 -0.10590224]]. Action = [[-0.01778017  0.13584489 -0.08603105  0.1472218 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.23106183 -0.10734707]]. Action = [[-0.15078136 -0.1061061  -0.0058239  -0.6395728 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.24418898 -0.11418345]]. Action = [[-0.22075136 -0.03597471  0.20533508  0.7794564 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 184 is [True, False, False, False, True, False]
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.24175203 -0.10069974]]. Action = [[ 0.11753267  0.22618729 -0.09696659  0.8452921 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 185 is [True, False, False, False, True, False]
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.2372133  -0.08944665]]. Action = [[ 0.06028569 -0.1578931   0.04780158  0.4338801 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.22593929 -0.10570591]]. Action = [[ 0.16308951 -0.15983252  0.1603272   0.9912057 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 187 is [True, False, False, False, True, False]
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-0.20293266 -0.1302812 ]]. Action = [[ 0.16901639 -0.16243282 -0.10943003 -0.31401598]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 0
Current timestep = 189. State = [[-0.17985733 -0.13551919]]. Action = [[ 0.1751309   0.18780231 -0.23128456  0.15284038]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 189 is [True, False, False, True, False, False]
Scene graph at timestep 189 is [True, False, False, True, False, False]
State prediction error at timestep 189 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of 1
Current timestep = 190. State = [[-0.16345565 -0.13114111]]. Action = [[ 0.04747814 -0.15024844  0.03739741  0.9009228 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 190 is [True, False, False, True, False, False]
Scene graph at timestep 190 is [True, False, False, True, False, False]
State prediction error at timestep 190 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 0
Current timestep = 191. State = [[-0.1452958  -0.14432128]]. Action = [[ 0.24541327 -0.09741209  0.06147119  0.60637736]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 191 is [True, False, False, True, False, False]
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.1134326  -0.15214194]]. Action = [[ 0.13762608  0.02974942  0.01568067 -0.9543599 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 192 is [True, False, False, True, False, False]
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.09432884 -0.15668121]]. Action = [[ 0.10487285 -0.086557    0.05676475  0.6712886 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 193 is [True, False, False, True, False, False]
Current timestep = 194. State = [[-0.07848921 -0.15219992]]. Action = [[ 0.19414163  0.15853924  0.20165068 -0.7467615 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 194 is [True, False, False, True, False, False]
Scene graph at timestep 194 is [True, False, False, True, False, False]
State prediction error at timestep 194 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.05849874 -0.15310568]]. Action = [[-0.06405166 -0.13152204 -0.19504397 -0.92765725]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 195 is [True, False, False, True, False, False]
Current timestep = 196. State = [[-0.06279425 -0.16729172]]. Action = [[-0.11145402 -0.12788837  0.19390252  0.6797141 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 196 is [True, False, False, True, False, False]
Current timestep = 197. State = [[-0.06085306 -0.18602218]]. Action = [[ 0.23147267 -0.1714801  -0.14416806 -0.09841979]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 197 is [True, False, False, True, False, False]
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.04591857 -0.19060987]]. Action = [[ 0.00912309  0.21031326 -0.08732416 -0.69958806]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 198 is [True, False, False, True, False, False]
Current timestep = 199. State = [[-0.04909902 -0.17918941]]. Action = [[-0.23898841  0.04397902 -0.21931614 -0.3909223 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 199 is [False, True, False, True, False, False]
Current timestep = 200. State = [[-0.05258385 -0.18557674]]. Action = [[ 0.14584601 -0.19206856  0.0860219   0.3872949 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 200 is [False, True, False, True, False, False]
Current timestep = 201. State = [[-0.05244662 -0.20505838]]. Action = [[ 0.04967341 -0.20881201  0.20156324 -0.53257984]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 201 is [True, False, False, True, False, False]
Current timestep = 202. State = [[-0.05566138 -0.23440611]]. Action = [[-0.06062967 -0.20206097 -0.18409602  0.3038497 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 202 is [True, False, False, True, False, False]
Current timestep = 203. State = [[-0.05624384 -0.24124952]]. Action = [[0.00041834 0.18680006 0.05655354 0.26752508]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 203 is [True, False, False, True, False, False]
Scene graph at timestep 203 is [True, False, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.04652607 -0.22221586]]. Action = [[ 0.20738691  0.20460767 -0.20059517  0.8597317 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 204 is [True, False, False, True, False, False]
Scene graph at timestep 204 is [False, True, False, True, False, False]
State prediction error at timestep 204 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.03121627 -0.20602418]]. Action = [[-0.00792316 -0.05602378  0.01964355  0.04407966]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 205 is [False, True, False, True, False, False]
Scene graph at timestep 205 is [False, True, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 0
Current timestep = 206. State = [[-0.03170414 -0.20819241]]. Action = [[ 0.00189909 -0.02738769  0.00760955 -0.8074422 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 206 is [False, True, False, True, False, False]
Scene graph at timestep 206 is [False, True, False, True, False, False]
State prediction error at timestep 206 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 0
Current timestep = 207. State = [[-0.02552767 -0.20710026]]. Action = [[ 0.15232238  0.05110466 -0.21680336  0.7242154 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 207 is [False, True, False, True, False, False]
Scene graph at timestep 207 is [False, True, False, True, False, False]
State prediction error at timestep 207 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.00703162 -0.19737317]]. Action = [[ 0.08279809  0.13982609 -0.11375672 -0.9538181 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 208 is [False, True, False, True, False, False]
Scene graph at timestep 208 is [False, True, False, True, False, False]
State prediction error at timestep 208 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[ 0.00198731 -0.19373147]]. Action = [[-0.19113435 -0.0650143  -0.19210024 -0.9743647 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 209 is [False, True, False, True, False, False]
Current timestep = 210. State = [[-0.00424058 -0.2051073 ]]. Action = [[-0.08350366 -0.10429311  0.00897411 -0.41520548]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 210 is [False, True, False, True, False, False]
Scene graph at timestep 210 is [False, True, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.00961212 -0.22113545]]. Action = [[ 0.12003732 -0.14046493 -0.05892521  0.46822405]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 211 is [False, True, False, True, False, False]
Current timestep = 212. State = [[-0.0063722  -0.23862436]]. Action = [[ 0.15675837 -0.2348864  -0.17053226 -0.4500491 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 212 is [False, True, False, True, False, False]
Scene graph at timestep 212 is [False, True, False, True, False, False]
State prediction error at timestep 212 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[ 0.01535676 -0.2637462 ]]. Action = [[ 0.20337218 -0.07548711  0.18808693 -0.07764167]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 213 is [False, True, False, True, False, False]
Scene graph at timestep 213 is [False, True, False, True, False, False]
State prediction error at timestep 213 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[ 0.03645239 -0.27080816]]. Action = [[ 0.21890262 -0.00189342 -0.01380897 -0.43303144]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 214 is [False, True, False, True, False, False]
Current timestep = 215. State = [[ 0.03661765 -0.26198253]]. Action = [[-0.13592727  0.21862245  0.20372564 -0.83814895]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 215 is [False, True, False, True, False, False]
Current timestep = 216. State = [[ 0.03564897 -0.24612275]]. Action = [[-0.08748123  0.12014079  0.12579817  0.00992453]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 216 is [False, True, False, True, False, False]
Current timestep = 217. State = [[ 0.02098627 -0.25014865]]. Action = [[-0.16693148 -0.20915604 -0.10561839  0.03408599]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 217 is [False, True, False, True, False, False]
Current timestep = 218. State = [[ 0.01209726 -0.25296402]]. Action = [[-0.03999007  0.16549861  0.13851172 -0.27206397]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 218 is [False, True, False, True, False, False]
Scene graph at timestep 218 is [False, True, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[ 0.00199828 -0.24161698]]. Action = [[-0.01625182  0.00553668 -0.10032435 -0.85132194]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 219 is [False, True, False, True, False, False]
Scene graph at timestep 219 is [False, True, False, True, False, False]
State prediction error at timestep 219 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.00037338 -0.25216943]]. Action = [[ 0.0905852  -0.23386532  0.10334688  0.12634659]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 220 is [False, True, False, True, False, False]
Current timestep = 221. State = [[-0.00705477 -0.27439734]]. Action = [[-0.16735744 -0.11137009  0.09996685 -0.7744927 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 221 is [False, True, False, True, False, False]
Current timestep = 222. State = [[-0.01394316 -0.2842447 ]]. Action = [[-0.06869254 -0.23671715 -0.08829105  0.7866404 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 222 is [False, True, False, True, False, False]
Current timestep = 223. State = [[-0.0201053  -0.27655682]]. Action = [[-0.11386809  0.20378184  0.00193858 -0.5903059 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 223 is [False, True, False, True, False, False]
Current timestep = 224. State = [[-0.02558797 -0.25920075]]. Action = [[-4.3408573e-04  8.6966932e-02 -1.4841138e-01 -8.9785862e-01]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 224 is [False, True, False, True, False, False]
Scene graph at timestep 224 is [False, True, False, True, False, False]
State prediction error at timestep 224 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.02687364 -0.24575618]]. Action = [[-0.05099753  0.07892364  0.22150174  0.04013026]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 225 is [False, True, False, True, False, False]
Current timestep = 226. State = [[-0.03693585 -0.24922702]]. Action = [[-0.09473845 -0.15091346 -0.02433418 -0.04164773]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 226 is [False, True, False, True, False, False]
Current timestep = 227. State = [[-0.04408603 -0.26882792]]. Action = [[ 0.13281414 -0.22319227  0.18863451 -0.3249997 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 227 is [False, True, False, True, False, False]
Current timestep = 228. State = [[-0.04469686 -0.28851503]]. Action = [[ 0.03101978 -0.10354227 -0.0735704  -0.8932044 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 228 is [False, True, False, True, False, False]
Scene graph at timestep 228 is [False, True, False, True, False, False]
State prediction error at timestep 228 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.04955561 -0.29948202]]. Action = [[-0.17513995  0.04235387 -0.23641238  0.14383924]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 229 is [False, True, False, True, False, False]
Current timestep = 230. State = [[-0.05262635 -0.30168667]]. Action = [[ 0.19798574 -0.21023309 -0.07708424 -0.6857463 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 230 is [False, True, False, True, False, False]
Scene graph at timestep 230 is [True, False, False, True, False, False]
State prediction error at timestep 230 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.05289156 -0.30184826]]. Action = [[ 0.04874322 -0.02063127  0.00430572 -0.14896822]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 231 is [True, False, False, True, False, False]
Scene graph at timestep 231 is [True, False, False, True, False, False]
State prediction error at timestep 231 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.06036188 -0.2982897 ]]. Action = [[-0.21563482  0.11577472  0.13692725 -0.2288124 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 232 is [True, False, False, True, False, False]
Current timestep = 233. State = [[-0.07229705 -0.28085798]]. Action = [[-0.03003848  0.19490868  0.22399187  0.56523204]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 233 is [True, False, False, True, False, False]
Current timestep = 234. State = [[-0.07811641 -0.27193493]]. Action = [[ 0.08371985 -0.15453246  0.11107853 -0.2698536 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 234 is [True, False, False, True, False, False]
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.07928559 -0.28637713]]. Action = [[ 0.07585722 -0.17399509 -0.0934884  -0.8053155 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 235 is [True, False, False, True, False, False]
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.0792771  -0.29740867]]. Action = [[-0.2214999  -0.22239314  0.00367904 -0.27273464]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 236 is [True, False, False, True, False, False]
Current timestep = 237. State = [[-0.08245249 -0.29302925]]. Action = [[-0.15460838  0.14481348  0.0626778  -0.38159138]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 237 is [True, False, False, True, False, False]
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.078296  -0.2764172]]. Action = [[ 0.18781942  0.21735036 -0.19496566 -0.37388718]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 238 is [True, False, False, True, False, False]
Current timestep = 239. State = [[-0.07124618 -0.253225  ]]. Action = [[ 0.09270293  0.0311299  -0.01248883 -0.46811348]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 239 is [True, False, False, True, False, False]
Current timestep = 240. State = [[-0.06184429 -0.23908868]]. Action = [[ 0.1098941   0.11426431 -0.14392872  0.9724896 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 240 is [True, False, False, True, False, False]
Scene graph at timestep 240 is [True, False, False, True, False, False]
State prediction error at timestep 240 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.04806509 -0.22935134]]. Action = [[ 0.06348681 -0.0239594  -0.01714411  0.19365096]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 241 is [True, False, False, True, False, False]
Current timestep = 242. State = [[-0.0355873  -0.22201093]]. Action = [[ 0.24064475  0.10791725 -0.13975957 -0.12344968]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 242 is [False, True, False, True, False, False]
Current timestep = 243. State = [[-0.01180726 -0.20883055]]. Action = [[ 0.01403135  0.11017913 -0.08203469 -0.5465289 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 243 is [False, True, False, True, False, False]
Current timestep = 244. State = [[ 0.00181868 -0.19508824]]. Action = [[ 0.16861784  0.0640614   0.1466077  -0.66002786]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 244 is [False, True, False, True, False, False]
Scene graph at timestep 244 is [False, True, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[ 0.01799645 -0.17458278]]. Action = [[-0.14539741  0.22585142 -0.19324167 -0.93072677]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 245 is [False, True, False, True, False, False]
Current timestep = 246. State = [[ 0.01017871 -0.16405047]]. Action = [[-0.16625127 -0.02325761 -0.02822354 -0.30969357]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 246 is [False, True, False, True, False, False]
Scene graph at timestep 246 is [False, True, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.00710576 -0.1547889 ]]. Action = [[-0.10126828  0.12704659 -0.03987774  0.2714082 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 247 is [False, True, False, True, False, False]
Scene graph at timestep 247 is [False, True, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.02124572 -0.15384148]]. Action = [[ 0.0583733  -0.22128126  0.21365136 -0.97152424]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 248 is [False, True, False, True, False, False]
Current timestep = 249. State = [[-0.02343727 -0.1557204 ]]. Action = [[-0.1327275   0.2101717  -0.130471   -0.44397414]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 249 is [False, True, False, True, False, False]
Scene graph at timestep 249 is [False, True, False, True, False, False]
State prediction error at timestep 249 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 0
Current timestep = 250. State = [[-0.03005009 -0.15337838]]. Action = [[ 0.14038488 -0.19152567 -0.0624896  -0.4462496 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 250 is [False, True, False, True, False, False]
Scene graph at timestep 250 is [False, True, False, True, False, False]
State prediction error at timestep 250 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.0373197  -0.17288184]]. Action = [[-0.23649617 -0.12291874  0.22189921  0.89934087]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 251 is [False, True, False, True, False, False]
Current timestep = 252. State = [[-0.05538719 -0.19374195]]. Action = [[-0.1684923  -0.1553758   0.16338363 -0.5096293 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 252 is [False, True, False, True, False, False]
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.08194946 -0.22362383]]. Action = [[-0.17820539 -0.24218892  0.12704006 -0.24399412]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 253 is [True, False, False, True, False, False]
Scene graph at timestep 253 is [True, False, False, True, False, False]
State prediction error at timestep 253 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.11365585 -0.250719  ]]. Action = [[-0.19929306 -0.0803467   0.22064936  0.68953824]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 254 is [True, False, False, True, False, False]
Current timestep = 255. State = [[-0.1164034  -0.25117743]]. Action = [[ 0.2401954   0.10519075 -0.13347894  0.62969375]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.11963908 -0.25767002]]. Action = [[-0.21983352 -0.20466726  0.08908477 -0.5379763 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 256 is [True, False, False, True, False, False]
Scene graph at timestep 256 is [True, False, False, True, False, False]
State prediction error at timestep 256 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.13539025 -0.263067  ]]. Action = [[-0.12424222  0.1966914   0.19827789 -0.67037845]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 257 is [True, False, False, True, False, False]
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.13939403 -0.25649065]]. Action = [[ 0.19753736 -0.15577698 -0.06536806 -0.7597543 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 258 is [True, False, False, True, False, False]
Scene graph at timestep 258 is [True, False, False, True, False, False]
State prediction error at timestep 258 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.12921411 -0.2607151 ]]. Action = [[ 0.07732683  0.02272698 -0.16169286  0.4318452 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 259 is [True, False, False, True, False, False]
Current timestep = 260. State = [[-0.12115072 -0.24984041]]. Action = [[0.03923714 0.21814096 0.08568996 0.31695378]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 260 is [True, False, False, True, False, False]
Current timestep = 261. State = [[-0.11466275 -0.23288083]]. Action = [[0.04304016 0.05460632 0.10540456 0.08995152]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.10260062 -0.23399843]]. Action = [[ 0.20731312 -0.17986201  0.07068634  0.70318377]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 262 is [True, False, False, True, False, False]
Scene graph at timestep 262 is [True, False, False, True, False, False]
State prediction error at timestep 262 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of 1
Current timestep = 263. State = [[-0.08690476 -0.24295388]]. Action = [[-0.18264443  0.07757109 -0.15454672  0.01508093]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 263 is [True, False, False, True, False, False]
Scene graph at timestep 263 is [True, False, False, True, False, False]
State prediction error at timestep 263 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.10646439 -0.24938208]]. Action = [[-0.24217737 -0.09247044  0.18402395 -0.8315474 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 264 is [True, False, False, True, False, False]
Current timestep = 265. State = [[-0.12266555 -0.259956  ]]. Action = [[ 0.01249787 -0.12086859  0.17422643  0.9523964 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 265 is [True, False, False, True, False, False]
Current timestep = 266. State = [[-0.13208605 -0.25987417]]. Action = [[-0.14389151  0.15857172  0.13417146  0.7483196 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 266 is [True, False, False, True, False, False]
Current timestep = 267. State = [[-0.13076149 -0.24185786]]. Action = [[ 0.1775071   0.20549786 -0.2015105  -0.83577645]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 267 is [True, False, False, True, False, False]
Current timestep = 268. State = [[-0.11900073 -0.21607497]]. Action = [[0.16693324 0.10267958 0.10480148 0.20859158]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 268 is [True, False, False, True, False, False]
Current timestep = 269. State = [[-0.11796223 -0.20729718]]. Action = [[-0.1512788  -0.02472684 -0.16851433 -0.5180986 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 269 is [True, False, False, True, False, False]
Scene graph at timestep 269 is [True, False, False, True, False, False]
State prediction error at timestep 269 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.11520066 -0.21251024]]. Action = [[ 0.2119562  -0.14071374  0.17087996 -0.2315433 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 270 is [True, False, False, True, False, False]
Current timestep = 271. State = [[-0.10819228 -0.21836443]]. Action = [[-0.0309031   0.00245824  0.11575711 -0.00364405]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 271 is [True, False, False, True, False, False]
Scene graph at timestep 271 is [True, False, False, True, False, False]
State prediction error at timestep 271 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.10911842 -0.22698718]]. Action = [[-0.02810368 -0.13389273 -0.01494357 -0.9658076 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 272 is [True, False, False, True, False, False]
Current timestep = 273. State = [[-0.10986576 -0.24359691]]. Action = [[ 0.02879313 -0.10401699  0.18825644  0.8530297 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 273 is [True, False, False, True, False, False]
Current timestep = 274. State = [[-0.11152806 -0.26401234]]. Action = [[-0.01196489 -0.21207054  0.23619309 -0.5180362 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 274 is [True, False, False, True, False, False]
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.10713151 -0.2778544 ]]. Action = [[ 0.04120758  0.13393521  0.15572411 -0.64944464]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 275 is [True, False, False, True, False, False]
Scene graph at timestep 275 is [True, False, False, True, False, False]
State prediction error at timestep 275 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of -1
Current timestep = 276. State = [[-0.10660983 -0.26316273]]. Action = [[-0.1833024   0.19436395  0.07092959 -0.7262622 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 276 is [True, False, False, True, False, False]
Scene graph at timestep 276 is [True, False, False, True, False, False]
State prediction error at timestep 276 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 0
Current timestep = 277. State = [[-0.10837703 -0.24065682]]. Action = [[ 0.16396508  0.09464446  0.13500845 -0.80104005]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 277 is [True, False, False, True, False, False]
Current timestep = 278. State = [[-0.09063236 -0.22358787]]. Action = [[ 0.22551423  0.13624975 -0.21966466  0.39658403]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 278 is [True, False, False, True, False, False]
Scene graph at timestep 278 is [True, False, False, True, False, False]
State prediction error at timestep 278 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.07423516 -0.21602881]]. Action = [[-0.0519522  -0.10816289 -0.13619298 -0.96946937]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 279 is [True, False, False, True, False, False]
Current timestep = 280. State = [[-0.07091703 -0.2220439 ]]. Action = [[ 0.15438288 -0.05687705 -0.24733749 -0.30174947]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 280 is [True, False, False, True, False, False]
Current timestep = 281. State = [[-0.06463005 -0.22063757]]. Action = [[-0.06347804  0.12840462 -0.20426047  0.39947033]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 281 is [True, False, False, True, False, False]
Scene graph at timestep 281 is [True, False, False, True, False, False]
State prediction error at timestep 281 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 0
Current timestep = 282. State = [[-0.05814444 -0.22351195]]. Action = [[ 0.17094398 -0.16740812  0.01384184  0.10594785]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 282 is [True, False, False, True, False, False]
Current timestep = 283. State = [[-0.04272233 -0.22962736]]. Action = [[ 0.07126054  0.01885715 -0.09405527 -0.28381473]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 283 is [True, False, False, True, False, False]
Scene graph at timestep 283 is [False, True, False, True, False, False]
State prediction error at timestep 283 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.04148638 -0.2428252 ]]. Action = [[-0.15926464 -0.23219103  0.1369186   0.70707583]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 284 is [False, True, False, True, False, False]
Scene graph at timestep 284 is [False, True, False, True, False, False]
State prediction error at timestep 284 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.05039129 -0.25687   ]]. Action = [[-0.12764126  0.19468552 -0.13415012  0.7695993 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 285 is [False, True, False, True, False, False]
Current timestep = 286. State = [[-0.05090538 -0.24007332]]. Action = [[ 0.16921094  0.09073097 -0.08048008  0.00123954]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 286 is [True, False, False, True, False, False]
Current timestep = 287. State = [[-0.04272888 -0.2184349 ]]. Action = [[ 0.01837212  0.22441888 -0.23216446  0.8834574 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 287 is [True, False, False, True, False, False]
Scene graph at timestep 287 is [False, True, False, True, False, False]
State prediction error at timestep 287 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 1
Current timestep = 288. State = [[-0.04002499 -0.1913532 ]]. Action = [[ 0.01995286  0.09794241  0.24076289 -0.8015636 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 288 is [False, True, False, True, False, False]
Scene graph at timestep 288 is [False, True, False, True, False, False]
State prediction error at timestep 288 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[-0.03701177 -0.19091442]]. Action = [[ 0.10454002 -0.17169435  0.02353621  0.9324417 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 289 is [False, True, False, True, False, False]
Scene graph at timestep 289 is [False, True, False, True, False, False]
State prediction error at timestep 289 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.03173386 -0.21094826]]. Action = [[-0.02291551 -0.18403532 -0.05326487  0.36855388]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 290 is [False, True, False, True, False, False]
Scene graph at timestep 290 is [False, True, False, True, False, False]
State prediction error at timestep 290 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.02524629 -0.2146163 ]]. Action = [[ 0.08376262  0.24705961  0.10433382 -0.25060058]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 291 is [False, True, False, True, False, False]
Current timestep = 292. State = [[-0.01339494 -0.18723893]]. Action = [[ 0.06334263  0.22218055 -0.18251322  0.71125317]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 292 is [False, True, False, True, False, False]
Current timestep = 293. State = [[ 0.00247261 -0.15980959]]. Action = [[ 0.22423047  0.09549323 -0.2342156   0.07250834]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 293 is [False, True, False, True, False, False]
Scene graph at timestep 293 is [False, True, False, True, False, False]
State prediction error at timestep 293 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[ 0.02733359 -0.1419348 ]]. Action = [[ 0.10249564  0.05875501  0.02978373 -0.07468843]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 294 is [False, True, False, True, False, False]
Current timestep = 295. State = [[ 0.03115266 -0.12738809]]. Action = [[-0.22027072  0.20724916 -0.2054256  -0.46702862]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 295 is [False, True, False, True, False, False]
Current timestep = 296. State = [[ 0.01637645 -0.11889524]]. Action = [[-0.18355076 -0.04462497 -0.15062112  0.04578888]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 296 is [False, True, False, True, False, False]
Scene graph at timestep 296 is [False, True, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[ 0.00259823 -0.11636809]]. Action = [[ 0.1821469   0.02144897 -0.20116952  0.40321922]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 297 is [False, True, False, False, True, False]
Scene graph at timestep 297 is [False, True, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 0
Current timestep = 298. State = [[ 0.00826768 -0.10405833]]. Action = [[-0.02900261  0.13777262 -0.14686173 -0.7555503 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 298 is [False, True, False, False, True, False]
Scene graph at timestep 298 is [False, True, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[ 0.00787052 -0.10648137]]. Action = [[ 0.04959911 -0.23036562 -0.18981807 -0.14838314]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 299 is [False, True, False, False, True, False]
Current timestep = 300. State = [[ 0.00770863 -0.11198869]]. Action = [[-0.09752029  0.15053836  0.02298778 -0.7759318 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 300 is [False, True, False, False, True, False]
Current timestep = 301. State = [[ 0.01234988 -0.11739465]]. Action = [[ 0.22867331 -0.21844429  0.22683078 -0.86994284]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 301 is [False, True, False, False, True, False]
Current timestep = 302. State = [[ 0.03272992 -0.13508281]]. Action = [[ 0.1735509  -0.10065377 -0.22903495  0.39360225]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 302 is [False, True, False, False, True, False]
Current timestep = 303. State = [[ 0.04602558 -0.14358975]]. Action = [[-0.09985563  0.03306568  0.21698326 -0.83411074]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 303 is [False, True, False, True, False, False]
Current timestep = 304. State = [[ 0.04076291 -0.1376397 ]]. Action = [[-0.20893201  0.12862864 -0.02559878  0.62524605]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 304 is [False, True, False, True, False, False]
Scene graph at timestep 304 is [False, True, False, True, False, False]
State prediction error at timestep 304 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 0
Current timestep = 305. State = [[-0.19694804 -0.07563694]]. Action = [[ 0.08939147 -0.16282605 -0.17930245 -0.5217937 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 305 is [False, True, False, True, False, False]
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 0
Current timestep = 306. State = [[-0.17700338 -0.07300118]]. Action = [[ 0.24543816  0.23585609 -0.21387328  0.61174226]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 306 is [True, False, False, False, True, False]
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.14593734 -0.05978725]]. Action = [[ 0.18085846 -0.02311657  0.03583273  0.424492  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 307 is [True, False, False, False, True, False]
Current timestep = 308. State = [[-0.13144018 -0.05431604]]. Action = [[-0.02641049  0.10070059 -0.09630051  0.35009754]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 308 is [True, False, False, False, True, False]
Current timestep = 309. State = [[-0.12259287 -0.0497578 ]]. Action = [[ 0.16602886 -0.02709939  0.20152515 -0.6820412 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 309 is [True, False, False, False, True, False]
Current timestep = 310. State = [[-0.11461223 -0.03901209]]. Action = [[-0.13939387  0.19509047  0.23447993  0.89120793]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 310 is [True, False, False, False, True, False]
Current timestep = 311. State = [[-0.11834024 -0.03041843]]. Action = [[-0.04296024 -0.09246555  0.22696942 -0.2898618 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 311 is [True, False, False, False, True, False]
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.11252828 -0.02861994]]. Action = [[ 0.2227895   0.0999774  -0.05709353 -0.85613304]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 312 is [True, False, False, False, True, False]
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.10515861 -0.02191265]]. Action = [[-0.19079228  0.00485632 -0.24785198 -0.5581018 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 313 is [True, False, False, False, True, False]
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.11628693 -0.00902881]]. Action = [[-0.1703087   0.161039   -0.10009703  0.73303986]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 314 is [True, False, False, False, True, False]
Current timestep = 315. State = [[-0.11987428  0.01275222]]. Action = [[ 0.2191096   0.14312202 -0.12285474  0.06964326]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 315 is [True, False, False, False, True, False]
Current timestep = 316. State = [[-0.1117562   0.02381576]]. Action = [[ 0.15572149  0.0434857  -0.24372569  0.38369942]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 316 is [True, False, False, False, True, False]
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.09704137  0.03177598]]. Action = [[ 0.00727162  0.04332444 -0.07082354  0.609867  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 317 is [True, False, False, False, True, False]
Current timestep = 318. State = [[-0.09992763  0.0399554 ]]. Action = [[-0.12180692  0.06394055 -0.19130489  0.44806886]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 318 is [True, False, False, False, True, False]
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.10465114  0.05206575]]. Action = [[-0.006715    0.07634169 -0.05903073 -0.4930513 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 319 is [True, False, False, False, True, False]
Current timestep = 320. State = [[-0.10951633  0.05880608]]. Action = [[-0.13406704  0.01357251 -0.06027457  0.3503393 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.11116322  0.07444694]]. Action = [[ 0.19977528  0.22136593 -0.13791782 -0.96643424]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 321 is [True, False, False, False, True, False]
Current timestep = 322. State = [[-0.09608568  0.08182852]]. Action = [[ 0.2177583  -0.11446047  0.11151963  0.8377433 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.0833225   0.07317014]]. Action = [[-0.1473589  -0.10236344  0.04586738 -0.7789764 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 323 is [True, False, False, False, True, False]
Current timestep = 324. State = [[-0.07728402  0.06071937]]. Action = [[ 0.20708293 -0.07765698 -0.09448898  0.56954217]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 324 is [True, False, False, False, True, False]
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 0
Current timestep = 325. State = [[-0.06594475  0.06327929]]. Action = [[ 0.04988173  0.1895122  -0.17205945  0.47110736]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 325 is [True, False, False, False, True, False]
Current timestep = 326. State = [[-0.06340075  0.06182078]]. Action = [[-0.23114489 -0.22104025 -0.1808461  -0.49841952]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 326 is [True, False, False, False, True, False]
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 0
Current timestep = 327. State = [[-0.07256832  0.05131461]]. Action = [[-0.09394604  0.0493356   0.1743966   0.9647887 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 327 is [True, False, False, False, True, False]
Current timestep = 328. State = [[-0.07427787  0.04515905]]. Action = [[ 0.13972005 -0.1695964  -0.226432    0.65107656]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 328 is [True, False, False, False, True, False]
Current timestep = 329. State = [[-0.07117762  0.03945833]]. Action = [[ 0.10061169  0.06378374 -0.02370948 -0.9321298 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 329 is [True, False, False, False, True, False]
Current timestep = 330. State = [[-0.07367839  0.02597356]]. Action = [[-0.19941007 -0.24144231  0.07880002 -0.84364426]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 330 is [True, False, False, False, True, False]
Current timestep = 331. State = [[-0.07335017  0.00263135]]. Action = [[ 0.18854576 -0.14579062 -0.23105949 -0.68766475]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 331 is [True, False, False, False, True, False]
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.06760529 -0.0261314 ]]. Action = [[-0.03939518 -0.19667692 -0.13742676 -0.33581012]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 332 is [True, False, False, False, True, False]
Current timestep = 333. State = [[-0.06317192 -0.04218784]]. Action = [[ 0.11821616 -0.03829712  0.03683525 -0.17109239]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 333 is [True, False, False, False, True, False]
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of 0
Current timestep = 334. State = [[-0.05851827 -0.05607228]]. Action = [[-0.0308174  -0.12065142 -0.18757276  0.8597666 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 334 is [True, False, False, False, True, False]
Current timestep = 335. State = [[-0.05461336 -0.07566816]]. Action = [[ 0.11536384 -0.18824673  0.19516003  0.7860563 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 335 is [True, False, False, False, True, False]
Current timestep = 336. State = [[-0.04050341 -0.09581985]]. Action = [[ 0.21801054 -0.06417787 -0.12398812  0.5449848 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 336 is [True, False, False, False, True, False]
Scene graph at timestep 336 is [False, True, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.26069525 -0.15527608]]. Action = [[-0.07118902  0.19940513  0.20820966 -0.18206757]]. Reward = [100.]
Curr episode timestep = 31
Scene graph at timestep 337 is [False, True, False, False, True, False]
Current timestep = 338. State = [[-0.26202798 -0.17175862]]. Action = [[-0.2269941  -0.19252945  0.11038226  0.43724167]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 338 is [True, False, False, True, False, False]
Scene graph at timestep 338 is [True, False, False, True, False, False]
State prediction error at timestep 338 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.2621001  -0.17136808]]. Action = [[-0.0393476   0.05733064 -0.10762005 -0.956384  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 339 is [True, False, False, True, False, False]
Scene graph at timestep 339 is [True, False, False, True, False, False]
State prediction error at timestep 339 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.2620337  -0.17130096]]. Action = [[-0.12602209  0.18647343 -0.0285567  -0.42602742]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 340 is [True, False, False, True, False, False]
Current timestep = 341. State = [[-0.25841102 -0.1726523 ]]. Action = [[ 0.13831344 -0.03207368 -0.09816992  0.8728163 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 341 is [True, False, False, True, False, False]
Scene graph at timestep 341 is [True, False, False, True, False, False]
State prediction error at timestep 341 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[-0.24532309 -0.17585793]]. Action = [[ 0.14024544 -0.03174518  0.03399834 -0.4671011 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 342 is [True, False, False, True, False, False]
Scene graph at timestep 342 is [True, False, False, True, False, False]
State prediction error at timestep 342 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 1
Current timestep = 343. State = [[-0.23457652 -0.17919524]]. Action = [[-0.06379363  0.01152849  0.19804996 -0.8285586 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 343 is [True, False, False, True, False, False]
Scene graph at timestep 343 is [True, False, False, True, False, False]
State prediction error at timestep 343 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.23604591 -0.16706729]]. Action = [[-0.06910303  0.22155428 -0.13970102 -0.18576485]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 344 is [True, False, False, True, False, False]
Scene graph at timestep 344 is [True, False, False, True, False, False]
State prediction error at timestep 344 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.23152383 -0.14371453]]. Action = [[ 0.22187364  0.07714665 -0.15746608  0.2719561 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 345 is [True, False, False, True, False, False]
Current timestep = 346. State = [[-0.2217575  -0.12543139]]. Action = [[-0.02219301  0.17721045 -0.02796379 -0.5512813 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 346 is [True, False, False, True, False, False]
Current timestep = 347. State = [[-0.22149351 -0.11267398]]. Action = [[-0.03003749  0.00820243 -0.09473345  0.24001384]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 347 is [True, False, False, True, False, False]
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of 1
Current timestep = 348. State = [[-0.21603997 -0.10412923]]. Action = [[ 0.13618273  0.08167765 -0.03014995 -0.89973605]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 348 is [True, False, False, False, True, False]
Current timestep = 349. State = [[-0.19960806 -0.0937424 ]]. Action = [[ 0.23639327  0.07992181 -0.2291159   0.45129442]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 349 is [True, False, False, False, True, False]
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of 1
Current timestep = 350. State = [[-0.16469547 -0.07310707]]. Action = [[0.19222331 0.17898023 0.01091424 0.8976228 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 350 is [True, False, False, False, True, False]
Current timestep = 351. State = [[-0.14205216 -0.04539528]]. Action = [[ 0.1540026   0.24629545  0.03496924 -0.39416504]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 351 is [True, False, False, False, True, False]
Current timestep = 352. State = [[-0.12479357 -0.01856336]]. Action = [[ 0.04578048  0.10933518 -0.23880589  0.17148137]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 352 is [True, False, False, False, True, False]
Current timestep = 353. State = [[-0.11899702 -0.00753589]]. Action = [[-0.05327219  0.00110549  0.09065884  0.9384885 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.12496545  0.00022021]]. Action = [[-0.1793198   0.07493532  0.14750293 -0.15332782]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 354 is [True, False, False, False, True, False]
Current timestep = 355. State = [[-0.14006336 -0.00193223]]. Action = [[-0.21687585 -0.12714314  0.03707066  0.95816624]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 355 is [True, False, False, False, True, False]
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(5.4960e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.15280627  0.00119971]]. Action = [[0.22640967 0.22146362 0.04069492 0.07827783]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 356 is [True, False, False, False, True, False]
Current timestep = 357. State = [[-0.13673393  0.01957504]]. Action = [[0.20376581 0.06210005 0.18394983 0.8648808 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 357 is [True, False, False, False, True, False]
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.11720537  0.02161578]]. Action = [[ 0.01966029 -0.12252489 -0.010024   -0.27588284]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 358 is [True, False, False, False, True, False]
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 0
Current timestep = 359. State = [[-0.11624705  0.02312481]]. Action = [[ 0.02643713  0.13661736  0.1577026  -0.11808801]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 359 is [True, False, False, False, True, False]
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 0
Current timestep = 360. State = [[-0.11506066  0.03637847]]. Action = [[-0.14064589  0.0792439  -0.12467894  0.3501265 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 360 is [True, False, False, False, True, False]
Current timestep = 361. State = [[-0.11628574  0.03496801]]. Action = [[ 0.04732549 -0.14675537  0.05548504 -0.9084857 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 361 is [True, False, False, False, True, False]
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of 0
Current timestep = 362. State = [[-0.11411443  0.01095444]]. Action = [[ 0.04209501 -0.23844483 -0.06727108 -0.4102847 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 362 is [True, False, False, False, True, False]
Current timestep = 363. State = [[-0.11180522 -0.00776299]]. Action = [[-0.00642239 -0.07518168 -0.05027603  0.5655048 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 363 is [True, False, False, False, True, False]
Current timestep = 364. State = [[-0.11033583 -0.01773459]]. Action = [[ 0.05338916 -0.02769084 -0.09059384  0.8115336 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 364 is [True, False, False, False, True, False]
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 1
Current timestep = 365. State = [[-0.10854596 -0.01989601]]. Action = [[ 0.05627057  0.06805182  0.1280142  -0.45568013]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 365 is [True, False, False, False, True, False]
Current timestep = 366. State = [[-0.10695972 -0.01371395]]. Action = [[ 0.00631449  0.07473871 -0.01912978  0.77944195]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 366 is [True, False, False, False, True, False]
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(9.2895e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of 1
Current timestep = 367. State = [[-0.1064459   0.00644581]]. Action = [[-0.21948764  0.23356238 -0.17650698  0.37088704]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 367 is [True, False, False, False, True, False]
Current timestep = 368. State = [[-0.1069997   0.01651496]]. Action = [[ 0.2220507  -0.15469857  0.16045585 -0.21349859]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 368 is [True, False, False, False, True, False]
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 0
Current timestep = 369. State = [[-0.10427811  0.01557625]]. Action = [[-0.13290937  0.10054883  0.06503353  0.8761641 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 369 is [True, False, False, False, True, False]
Current timestep = 370. State = [[-0.10586394  0.02615003]]. Action = [[ 0.10175744  0.11609733 -0.1827394   0.10866988]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 370 is [True, False, False, False, True, False]
Current timestep = 371. State = [[-0.09604021  0.04313713]]. Action = [[ 0.2445839   0.15593827  0.09838945 -0.7644728 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 371 is [True, False, False, False, True, False]
Current timestep = 372. State = [[-0.06778085  0.06832905]]. Action = [[ 0.17993417  0.20311058 -0.11014882  0.24727046]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 372 is [True, False, False, False, True, False]
Current timestep = 373. State = [[-0.05049907  0.07341335]]. Action = [[-0.05840179 -0.19701144  0.24724066 -0.20075172]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 373 is [True, False, False, False, True, False]
Current timestep = 374. State = [[-0.05354193  0.06291658]]. Action = [[-0.18859136 -0.0474683  -0.20374483 -0.7928127 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 374 is [True, False, False, False, True, False]
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of 1
Current timestep = 375. State = [[-0.05823928  0.06726328]]. Action = [[ 0.08779937  0.21542299 -0.05536905  0.10944009]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 375 is [True, False, False, False, True, False]
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.05995734  0.07548541]]. Action = [[ 0.03361031 -0.15077235  0.06385139  0.54738975]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 376 is [True, False, False, False, True, False]
Current timestep = 377. State = [[-0.05988841  0.07798581]]. Action = [[-0.04574524  0.14208904 -0.08312693 -0.64785177]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 377 is [True, False, False, False, True, False]
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 0
Current timestep = 378. State = [[-0.06285014  0.0713247 ]]. Action = [[-0.10450232 -0.23272184 -0.17953612 -0.71576697]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 378 is [True, False, False, False, True, False]
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.067867    0.04328104]]. Action = [[-0.00731932 -0.20612064  0.08505705  0.75536203]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 379 is [True, False, False, False, True, False]
Current timestep = 380. State = [[-0.06293884  0.03861641]]. Action = [[0.19712317 0.155366   0.20916116 0.5600141 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 380 is [True, False, False, False, True, False]
Current timestep = 381. State = [[-0.04736925  0.05504764]]. Action = [[0.22262332 0.20625868 0.24312437 0.66445804]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 381 is [True, False, False, False, True, False]
Scene graph at timestep 381 is [False, True, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of 1
Current timestep = 382. State = [[-0.21545728  0.07982282]]. Action = [[-0.16186033 -0.0730834   0.10813075  0.7807281 ]]. Reward = [100.]
Curr episode timestep = 44
Scene graph at timestep 382 is [False, True, False, False, True, False]
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.2021397   0.09185857]]. Action = [[ 0.11086339  0.05971622  0.2062999  -0.3255093 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 383 is [True, False, False, False, True, False]
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(7.3207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of 1
Current timestep = 384. State = [[-0.19382727  0.1005199 ]]. Action = [[-0.09946637  0.02415282 -0.15384142  0.01766682]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 384 is [True, False, False, False, True, False]
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.20292862  0.10031755]]. Action = [[-0.20045486 -0.08247644 -0.0266856   0.9012952 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 385 is [True, False, False, False, True, False]
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.20978943  0.10422603]]. Action = [[ 0.21611664  0.16866136 -0.17292981  0.16739595]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 386 is [True, False, False, False, True, False]
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of 1
Current timestep = 387. State = [[-0.20171484  0.11408166]]. Action = [[-0.07141401 -0.03195113  0.23242146 -0.8429952 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 387 is [True, False, False, False, True, False]
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.20613648  0.11615392]]. Action = [[-0.12481551  0.01379982  0.21608746  0.9538014 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 388 is [True, False, False, False, True, False]
Current timestep = 389. State = [[-0.20561835  0.12152209]]. Action = [[ 0.21821666  0.09393445 -0.01346903  0.11948454]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 389 is [True, False, False, False, True, False]
Current timestep = 390. State = [[-0.20027536  0.12413375]]. Action = [[ 0.00420055 -0.03769696 -0.17758918 -0.69351065]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 390 is [True, False, False, False, True, False]
Current timestep = 391. State = [[-0.19341092  0.12691581]]. Action = [[ 0.10715446  0.08324176 -0.24400805 -0.8209001 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 391 is [True, False, False, False, True, False]
Current timestep = 392. State = [[-0.186363    0.12451195]]. Action = [[-0.10160276 -0.13751836  0.11659238  0.4464631 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 392 is [True, False, False, False, False, True]
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.18401633  0.11170475]]. Action = [[ 0.08830902 -0.1005187   0.03545189 -0.91258293]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 393 is [True, False, False, False, True, False]
Current timestep = 394. State = [[-0.1850786   0.10061461]]. Action = [[-0.16591094 -0.0786618   0.19447494 -0.8497979 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 394 is [True, False, False, False, True, False]
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of 0
Current timestep = 395. State = [[-0.19268858  0.09357454]]. Action = [[-0.07530783 -0.00168911 -0.13104394 -0.9114853 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 395 is [True, False, False, False, True, False]
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.19792312  0.08969209]]. Action = [[-0.04222338 -0.06560646 -0.05884588 -0.8425444 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 396 is [True, False, False, False, True, False]
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of 1
Current timestep = 397. State = [[-0.20922129  0.07166696]]. Action = [[-0.18171872 -0.22052632 -0.21883409  0.07879281]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 397 is [True, False, False, False, True, False]
Current timestep = 398. State = [[-0.22182375  0.06651583]]. Action = [[ 0.13238543  0.19185114 -0.16653672  0.58741474]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 398 is [True, False, False, False, True, False]
Current timestep = 399. State = [[-0.21877648  0.06567545]]. Action = [[ 0.05338854 -0.16407555 -0.13368012  0.1858387 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 399 is [True, False, False, False, True, False]
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(2.3465e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of 0
Current timestep = 400. State = [[-0.22161905  0.06096436]]. Action = [[-0.20480697  0.05484772  0.09540427 -0.35101104]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 400 is [True, False, False, False, True, False]
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.2232301   0.05313834]]. Action = [[ 0.18755776 -0.17697954  0.18872869 -0.7055672 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 401 is [True, False, False, False, True, False]
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of 1
Current timestep = 402. State = [[-0.21472423  0.05150715]]. Action = [[ 0.1061261   0.24451137  0.18991965 -0.25498068]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 402 is [True, False, False, False, True, False]
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of 1
Current timestep = 403. State = [[-0.20522226  0.08106907]]. Action = [[ 0.09575266  0.22884083  0.15046242 -0.45543742]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.20258866  0.09828331]]. Action = [[-0.14577927 -0.01065809 -0.20397161  0.9813596 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.2026229  0.1062623]]. Action = [[ 0.1455968   0.10932797  0.24161875 -0.6855946 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(7.4265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.19884083  0.11500875]]. Action = [[-0.06389067 -0.02319901  0.21161953  0.46221066]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 406 is [True, False, False, False, True, False]
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(8.0326e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.20182483  0.1138997 ]]. Action = [[-0.13948624 -0.04085019 -0.17502251 -0.24901211]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 407 is [True, False, False, False, True, False]
Current timestep = 408. State = [[-0.21698478  0.12206115]]. Action = [[-0.21619745  0.15055838 -0.06527953  0.8930099 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 408 is [True, False, False, False, True, False]
Current timestep = 409. State = [[-0.24312165  0.14029452]]. Action = [[-0.21423835  0.0774267  -0.13203894  0.3763249 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 409 is [True, False, False, False, True, False]
Scene graph at timestep 409 is [True, False, False, False, False, True]
State prediction error at timestep 409 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.26952085  0.15522295]]. Action = [[ 0.09372282  0.15482631 -0.09310484  0.16094577]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 410 is [True, False, False, False, False, True]
Current timestep = 411. State = [[-0.2611888   0.16489199]]. Action = [[ 0.19814175  0.04621026 -0.11232927  0.40253055]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 411 is [True, False, False, False, False, True]
Current timestep = 412. State = [[-0.23677321  0.16705729]]. Action = [[ 0.24602598 -0.07205099  0.14589253 -0.4746046 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 412 is [True, False, False, False, False, True]
Current timestep = 413. State = [[-0.20640615  0.15194634]]. Action = [[ 0.22301987 -0.20404668  0.07634822 -0.79244286]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 413 is [True, False, False, False, False, True]
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of 1
Current timestep = 414. State = [[-0.18751056  0.13414626]]. Action = [[-0.21969892 -0.04689232 -0.18173571  0.01980817]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 414 is [True, False, False, False, False, True]
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.2008496   0.14094462]]. Action = [[-0.1501443   0.19186962 -0.2421317  -0.6113893 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.2070715   0.14303647]]. Action = [[ 0.16638467 -0.23081854 -0.07369497 -0.8103495 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 416 is [True, False, False, False, False, True]
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(6.8798e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of 1
Current timestep = 417. State = [[-0.194627   0.1330526]]. Action = [[ 0.18440711  0.1443817   0.01999786 -0.9179711 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 417 is [True, False, False, False, False, True]
Scene graph at timestep 417 is [True, False, False, False, False, True]
State prediction error at timestep 417 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.18189995  0.12887393]]. Action = [[-0.08421808 -0.23330103 -0.14719348 -0.7669908 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 418 is [True, False, False, False, False, True]
Current timestep = 419. State = [[-0.1763767   0.12301907]]. Action = [[ 0.20694757  0.13939881 -0.02117158 -0.38743043]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 419 is [True, False, False, False, False, True]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.16629516  0.1253538 ]]. Action = [[-0.08267169 -0.09295784 -0.04847546 -0.4508226 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 420 is [True, False, False, False, True, False]
Scene graph at timestep 420 is [True, False, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 0
Current timestep = 421. State = [[-0.16681843  0.10533171]]. Action = [[-0.01344472 -0.2352817   0.14319873 -0.6987021 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 421 is [True, False, False, False, False, True]
Current timestep = 422. State = [[-0.15936239  0.08339547]]. Action = [[ 0.21401829 -0.0639326   0.06400949  0.7319124 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.14539789  0.06061061]]. Action = [[ 0.04987466 -0.21253645 -0.19520357 -0.17499644]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 423 is [True, False, False, False, True, False]
Current timestep = 424. State = [[-0.12934461  0.04727623]]. Action = [[ 0.2319532   0.05951017  0.03025109 -0.4414258 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 424 is [True, False, False, False, True, False]
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.11165114  0.04872074]]. Action = [[-0.15767002 -0.01988688 -0.04879883 -0.8282842 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 425 is [True, False, False, False, True, False]
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of 0
Current timestep = 426. State = [[-0.1198445   0.05888661]]. Action = [[-0.16259207  0.18192595 -0.19673644  0.86943173]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 426 is [True, False, False, False, True, False]
Current timestep = 427. State = [[-0.13999262  0.06823561]]. Action = [[-0.2330158  -0.03463191  0.17004406  0.15731573]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 427 is [True, False, False, False, True, False]
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.16318499  0.06745171]]. Action = [[0.10891888 0.00352299 0.07564425 0.48718488]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.152588    0.07211382]]. Action = [[ 0.24360678  0.09340107 -0.12965003  0.18334377]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.13216569  0.07400671]]. Action = [[ 0.12709397 -0.07538664  0.1455751   0.31890225]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.12100862  0.06352008]]. Action = [[-0.1142963  -0.13965575  0.02033824 -0.67691547]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 431 is [True, False, False, False, True, False]
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 0
Current timestep = 432. State = [[-0.12087247  0.06469402]]. Action = [[0.07889166 0.21033204 0.22221965 0.09779322]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.11011399  0.08942997]]. Action = [[ 0.21884519  0.23272574 -0.14912273 -0.15050459]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.08693095  0.1159343 ]]. Action = [[ 0.13776964  0.16010651 -0.02553421 -0.07949817]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 434 is [True, False, False, False, True, False]
Current timestep = 435. State = [[-0.07686189  0.12947188]]. Action = [[-0.19925308 -0.06046058 -0.18967594  0.8107321 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 435 is [True, False, False, False, True, False]
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.07459065  0.13322577]]. Action = [[0.18996537 0.06950933 0.18952876 0.40011883]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 436 is [True, False, False, False, False, True]
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of -1
Current timestep = 437. State = [[-0.06274612  0.14382552]]. Action = [[ 0.2264713   0.1381588  -0.23102307  0.26902616]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 437 is [True, False, False, False, False, True]
Current timestep = 438. State = [[-0.04829598  0.1716942 ]]. Action = [[-0.12534656  0.23528111 -0.02138424 -0.9175302 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 438 is [True, False, False, False, False, True]
Scene graph at timestep 438 is [False, True, False, False, False, True]
State prediction error at timestep 438 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.0575661   0.20189533]]. Action = [[-0.20186049  0.19204384 -0.03534999  0.08364439]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 439 is [False, True, False, False, False, True]
Current timestep = 440. State = [[-0.07465153  0.22566423]]. Action = [[-0.15772414  0.05984914 -0.0730169   0.40768337]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 440 is [True, False, False, False, False, True]
Current timestep = 441. State = [[-0.08610467  0.24488005]]. Action = [[ 0.10636616  0.221932    0.19739264 -0.278283  ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 441 is [True, False, False, False, False, True]
Current timestep = 442. State = [[-0.09421332  0.26479852]]. Action = [[-0.14571188  0.09028164 -0.10959393  0.03669488]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 442 is [True, False, False, False, False, True]
Current timestep = 443. State = [[-0.10006151  0.27329034]]. Action = [[-0.01503065 -0.06014332 -0.01637805  0.48211908]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 443 is [True, False, False, False, False, True]
Current timestep = 444. State = [[-0.09918398  0.2723173 ]]. Action = [[ 0.10008341  0.01526895 -0.10307869 -0.29164332]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 444 is [True, False, False, False, False, True]
Current timestep = 445. State = [[-0.09871962  0.27159524]]. Action = [[-0.16981667  0.20331746  0.1145066   0.928408  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 445 is [True, False, False, False, False, True]
Current timestep = 446. State = [[-0.09290409  0.2629348 ]]. Action = [[ 0.08533961 -0.146092   -0.03934485 -0.46069926]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 446 is [True, False, False, False, False, True]
Current timestep = 447. State = [[-0.08566525  0.259432  ]]. Action = [[0.09038773 0.09694704 0.15201765 0.737772  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 447 is [True, False, False, False, False, True]
Current timestep = 448. State = [[-0.07306714  0.27408338]]. Action = [[ 0.1667642   0.19417673  0.08214945 -0.12820804]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.0497749   0.29053503]]. Action = [[0.20432478 0.05413237 0.06846675 0.72933364]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.02969415  0.29115438]]. Action = [[-0.04411651 -0.1037202  -0.21107942 -0.93719983]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 450 is [False, True, False, False, False, True]
Current timestep = 451. State = [[-0.02163613  0.27960643]]. Action = [[ 0.1119689  -0.09571141 -0.07970038 -0.2787012 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 451 is [False, True, False, False, False, True]
Current timestep = 452. State = [[-0.00746499  0.27588162]]. Action = [[ 0.17568344  0.04792494 -0.19936563 -0.2657246 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 452 is [False, True, False, False, False, True]
Scene graph at timestep 452 is [False, True, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[0.01406737 0.27776352]]. Action = [[-0.21922457  0.18896016 -0.04494748  0.41767526]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 453 is [False, True, False, False, False, True]
Scene graph at timestep 453 is [False, True, False, False, False, True]
State prediction error at timestep 453 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[0.01730951 0.2663427 ]]. Action = [[-0.00787652 -0.2327037  -0.16295475  0.58195186]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 454 is [False, True, False, False, False, True]
Scene graph at timestep 454 is [False, True, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 0
Current timestep = 455. State = [[0.02811902 0.251315  ]]. Action = [[ 0.17043608  0.0270943  -0.09399711  0.58576536]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 455 is [False, True, False, False, False, True]
Current timestep = 456. State = [[0.03448954 0.25263023]]. Action = [[ 0.00439519  0.04122061  0.0603081  -0.25482184]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 456 is [False, True, False, False, False, True]
Current timestep = 457. State = [[0.03945564 0.261659  ]]. Action = [[ 0.12789437  0.113747   -0.18893188  0.86268115]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 457 is [False, True, False, False, False, True]
Current timestep = 458. State = [[0.05336925 0.26923126]]. Action = [[ 0.03778937  0.0101209  -0.13969195  0.7653755 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 458 is [False, True, False, False, False, True]
Current timestep = 459. State = [[0.06062487 0.2717171 ]]. Action = [[ 0.21270117  0.2057884  -0.16189928 -0.131266  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 459 is [False, False, True, False, False, True]
Scene graph at timestep 459 is [False, False, True, False, False, True]
State prediction error at timestep 459 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[0.06309061 0.26476616]]. Action = [[-0.11230308 -0.18189545  0.02772912  0.4764433 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 460 is [False, False, True, False, False, True]
Scene graph at timestep 460 is [False, False, True, False, False, True]
State prediction error at timestep 460 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[0.05897405 0.24860843]]. Action = [[-0.2140985  -0.10733238  0.01100874 -0.6593717 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 461 is [False, False, True, False, False, True]
Scene graph at timestep 461 is [False, False, True, False, False, True]
State prediction error at timestep 461 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of 1
Current timestep = 462. State = [[0.03870528 0.2236417 ]]. Action = [[-0.11828227 -0.23687358 -0.10513094  0.61743855]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 462 is [False, False, True, False, False, True]
Scene graph at timestep 462 is [False, True, False, False, False, True]
State prediction error at timestep 462 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of 1
Current timestep = 463. State = [[0.02257239 0.20967114]]. Action = [[-0.06575853  0.08291176  0.19294536  0.3313024 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 463 is [False, True, False, False, False, True]
Scene graph at timestep 463 is [False, True, False, False, False, True]
State prediction error at timestep 463 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[0.01818489 0.21965946]]. Action = [[ 0.20663518  0.09085435 -0.21482825 -0.39723796]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 464 is [False, True, False, False, False, True]
Current timestep = 465. State = [[0.02915257 0.22048882]]. Action = [[ 0.24094662 -0.05186172  0.19294167 -0.29592478]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 465 is [False, True, False, False, False, True]
Scene graph at timestep 465 is [False, True, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[0.05403869 0.21620528]]. Action = [[ 0.24677032 -0.08960266 -0.08706027  0.87027717]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 466 is [False, True, False, False, False, True]
Scene graph at timestep 466 is [False, False, True, False, False, True]
State prediction error at timestep 466 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[0.05357586 0.2215147 ]]. Action = [[-0.04307234  0.08569968 -0.04350822 -0.07288909]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 467 is [False, False, True, False, False, True]
Current timestep = 468. State = [[0.0564976  0.21721336]]. Action = [[ 0.02479416 -0.16940527  0.03786659 -0.88679814]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 468 is [False, False, True, False, False, True]
Current timestep = 469. State = [[0.06003447 0.20973402]]. Action = [[0.16541415 0.09761059 0.21465987 0.0201478 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 469 is [False, False, True, False, False, True]
Scene graph at timestep 469 is [False, False, True, False, False, True]
State prediction error at timestep 469 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[0.06066002 0.20839538]]. Action = [[ 0.20179194  0.10102239 -0.01888424 -0.38981807]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 470 is [False, False, True, False, False, True]
Scene graph at timestep 470 is [False, False, True, False, False, True]
State prediction error at timestep 470 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[0.06066002 0.20839538]]. Action = [[ 0.05267259 -0.16110657 -0.12488179 -0.33526373]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 471 is [False, False, True, False, False, True]
Current timestep = 472. State = [[0.06066002 0.20839538]]. Action = [[ 0.18981528  0.00336903 -0.17940013  0.95349836]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 472 is [False, False, True, False, False, True]
Scene graph at timestep 472 is [False, False, True, False, False, True]
State prediction error at timestep 472 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[0.05732655 0.20592435]]. Action = [[-0.17679077 -0.04045655 -0.06357458 -0.7441811 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 473 is [False, False, True, False, False, True]
Scene graph at timestep 473 is [False, False, True, False, False, True]
State prediction error at timestep 473 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[0.05058393 0.2020752 ]]. Action = [[ 0.20950091 -0.04522784 -0.05931202  0.56515586]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 474 is [False, False, True, False, False, True]
Scene graph at timestep 474 is [False, False, True, False, False, True]
State prediction error at timestep 474 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[0.05058393 0.2020752 ]]. Action = [[ 0.23095426  0.21850038 -0.00632703  0.94143045]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 475 is [False, False, True, False, False, True]
Current timestep = 476. State = [[0.04977151 0.1893983 ]]. Action = [[-0.00780453 -0.20589747 -0.22834176 -0.3595339 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 476 is [False, False, True, False, False, True]
Current timestep = 477. State = [[0.04904595 0.17355838]]. Action = [[ 0.02360559 -0.0622814  -0.07566342 -0.605876  ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 477 is [False, True, False, False, False, True]
Current timestep = 478. State = [[0.05124778 0.16092527]]. Action = [[ 0.08238193 -0.08601794 -0.14109823 -0.24930096]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 478 is [False, True, False, False, False, True]
Current timestep = 479. State = [[0.0547569  0.15212093]]. Action = [[ 0.03518921 -0.00650467  0.11599699  0.23990619]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 479 is [False, False, True, False, False, True]
Current timestep = 480. State = [[0.05678912 0.15547381]]. Action = [[ 0.03682548  0.10523164 -0.10773849  0.55737185]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 480 is [False, False, True, False, False, True]
Scene graph at timestep 480 is [False, False, True, False, False, True]
State prediction error at timestep 480 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of 0
Current timestep = 481. State = [[0.05518659 0.16711234]]. Action = [[-0.14457805  0.0720686  -0.07109919  0.95710325]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 481 is [False, False, True, False, False, True]
Scene graph at timestep 481 is [False, False, True, False, False, True]
State prediction error at timestep 481 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[0.05239712 0.17289433]]. Action = [[ 0.21455583  0.09143335 -0.05846488 -0.61283815]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 482 is [False, False, True, False, False, True]
Scene graph at timestep 482 is [False, False, True, False, False, True]
State prediction error at timestep 482 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[0.05195695 0.17251429]]. Action = [[ 0.23790258  0.22522548 -0.22914833 -0.9079255 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 483 is [False, False, True, False, False, True]
Current timestep = 484. State = [[0.04705065 0.16697732]]. Action = [[-0.15882002 -0.09939024 -0.19887395  0.15417504]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 484 is [False, False, True, False, False, True]
Current timestep = 485. State = [[0.03792775 0.16113077]]. Action = [[-0.01552834 -0.01917464 -0.14670339  0.96914995]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 485 is [False, True, False, False, False, True]
Current timestep = 486. State = [[0.03753147 0.15417586]]. Action = [[ 0.12210268 -0.06795985  0.12296909  0.49909687]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 486 is [False, True, False, False, False, True]
Scene graph at timestep 486 is [False, True, False, False, False, True]
State prediction error at timestep 486 is tensor(4.6282e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of 0
Current timestep = 487. State = [[0.03515318 0.13896567]]. Action = [[-0.21016216 -0.16851959  0.01796243  0.97130847]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 487 is [False, True, False, False, False, True]
Current timestep = 488. State = [[0.02211378 0.12533872]]. Action = [[ 0.22393543 -0.1255841   0.03607902  0.7262602 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 488 is [False, True, False, False, False, True]
Current timestep = 489. State = [[-0.15484494  0.09195473]]. Action = [[ 0.05279294 -0.12740634  0.12884247  0.62788606]]. Reward = [100.]
Curr episode timestep = 106
Scene graph at timestep 489 is [False, True, False, False, False, True]
Current timestep = 490. State = [[-0.13301733  0.11363673]]. Action = [[ 0.09341821  0.158441   -0.13839658 -0.6955689 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 490 is [True, False, False, False, True, False]
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.11486116  0.13620317]]. Action = [[ 0.20404154  0.15868375 -0.0709323   0.89717555]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 491 is [True, False, False, False, True, False]
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of -1
Current timestep = 492. State = [[-0.09364787  0.13990657]]. Action = [[-0.1541322  -0.21248932 -0.22955947  0.5504265 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 492 is [True, False, False, False, False, True]
Scene graph at timestep 492 is [True, False, False, False, False, True]
State prediction error at timestep 492 is tensor(5.7212e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of 1
Current timestep = 493. State = [[-0.10264463  0.1184104 ]]. Action = [[-0.19274326 -0.14126347  0.07702357  0.593452  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 493 is [True, False, False, False, False, True]
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of 0
Current timestep = 494. State = [[-0.11277202  0.10063162]]. Action = [[ 0.0789237  -0.11215015  0.06260571  0.25550795]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 494 is [True, False, False, False, True, False]
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of 1
Current timestep = 495. State = [[-0.1077962   0.09955081]]. Action = [[ 0.15591389  0.16416839  0.12721837 -0.97056526]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 495 is [True, False, False, False, True, False]
Current timestep = 496. State = [[-0.10603968  0.1047719 ]]. Action = [[-0.1253794  -0.0547599  -0.15686849  0.06539381]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.11252725  0.09516506]]. Action = [[-0.1666706  -0.17276873  0.20960617 -0.6515434 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 0
Current timestep = 498. State = [[-0.1220496   0.09212796]]. Action = [[0.14364862 0.17282224 0.01596561 0.31284595]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 498 is [True, False, False, False, True, False]
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.12488171  0.11339219]]. Action = [[-0.00478832  0.21100107 -0.17060903  0.6888933 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.13265833  0.12241966]]. Action = [[-0.22515182 -0.17304657 -0.02638729 -0.02538508]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 500 is [True, False, False, False, True, False]
Current timestep = 501. State = [[-0.14635864  0.12587361]]. Action = [[-0.22050133  0.1382398   0.24489895  0.30122447]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(2.3460e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.17113642  0.1267652 ]]. Action = [[-0.05710188 -0.15443914 -0.02176809  0.9725375 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 502 is [True, False, False, False, False, True]
Scene graph at timestep 502 is [True, False, False, False, False, True]
State prediction error at timestep 502 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.17672169  0.11502601]]. Action = [[ 0.05301729  0.07099849 -0.09256873 -0.06932634]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 503 is [True, False, False, False, False, True]
Current timestep = 504. State = [[-0.16925238  0.11730216]]. Action = [[ 0.205859   -0.03646685 -0.06677258  0.91627455]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 504 is [True, False, False, False, True, False]
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(4.0436e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of 0
Current timestep = 505. State = [[-0.16385111  0.12267092]]. Action = [[-0.01058947  0.1353957   0.19324875  0.20517349]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 505 is [True, False, False, False, True, False]
Current timestep = 506. State = [[-0.17033702  0.14007014]]. Action = [[-0.17833205  0.17148331  0.22073174  0.85132456]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 506 is [True, False, False, False, True, False]
Scene graph at timestep 506 is [True, False, False, False, False, True]
State prediction error at timestep 506 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.1741614   0.16594262]]. Action = [[ 0.22055227  0.18779308 -0.18520616  0.00147057]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 507 is [True, False, False, False, False, True]
Scene graph at timestep 507 is [True, False, False, False, False, True]
State prediction error at timestep 507 is tensor(5.2559e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of 0
Current timestep = 508. State = [[-0.14951737  0.18287267]]. Action = [[ 0.23980308  0.03763723 -0.10046571  0.76856315]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 508 is [True, False, False, False, False, True]
Scene graph at timestep 508 is [True, False, False, False, False, True]
State prediction error at timestep 508 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 0
Current timestep = 509. State = [[-0.12347986  0.18726751]]. Action = [[ 0.1850107  -0.02773392  0.11320207  0.03045213]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 509 is [True, False, False, False, False, True]
Scene graph at timestep 509 is [True, False, False, False, False, True]
State prediction error at timestep 509 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 0
Current timestep = 510. State = [[-0.09510244  0.19659877]]. Action = [[0.1955443  0.1750763  0.19384232 0.20085812]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 510 is [True, False, False, False, False, True]
Scene graph at timestep 510 is [True, False, False, False, False, True]
State prediction error at timestep 510 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.06159621  0.19788428]]. Action = [[ 0.2473824  -0.23015767 -0.14261527  0.18656635]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 511 is [True, False, False, False, False, True]
Scene graph at timestep 511 is [True, False, False, False, False, True]
State prediction error at timestep 511 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of 0
Current timestep = 512. State = [[-0.03587978  0.17045237]]. Action = [[-0.21155825 -0.21200012 -0.20165987 -0.3822999 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 512 is [True, False, False, False, False, True]
Current timestep = 513. State = [[-0.03249185  0.14909858]]. Action = [[ 0.1897664  -0.13048716  0.09632084 -0.12943971]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 513 is [False, True, False, False, False, True]
Scene graph at timestep 513 is [False, True, False, False, False, True]
State prediction error at timestep 513 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of 1
Current timestep = 514. State = [[-0.02762466  0.1381226 ]]. Action = [[ 0.05451721  0.08861479  0.11217812 -0.06462175]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 514 is [False, True, False, False, False, True]
Current timestep = 515. State = [[-0.03072634  0.13214692]]. Action = [[-0.23150322 -0.18282776 -0.16490297 -0.35104293]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 515 is [False, True, False, False, False, True]
Current timestep = 516. State = [[-0.03861312  0.12554714]]. Action = [[-0.1443426   0.01240155  0.24145517  0.08274996]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 516 is [False, True, False, False, False, True]
Scene graph at timestep 516 is [False, True, False, False, False, True]
State prediction error at timestep 516 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of 1
Current timestep = 517. State = [[-0.05984405  0.10855589]]. Action = [[-0.19845429 -0.24451312 -0.11292648  0.70194817]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 517 is [False, True, False, False, False, True]
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 0
Current timestep = 518. State = [[-0.08395989  0.07279691]]. Action = [[-0.15032376 -0.21857946  0.03468016  0.02922964]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 518 is [True, False, False, False, True, False]
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of 0
Current timestep = 519. State = [[-0.09946562  0.05607866]]. Action = [[ 0.2278471   0.12594813 -0.1964097  -0.18433523]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 519 is [True, False, False, False, True, False]
Current timestep = 520. State = [[-0.08510967  0.06756265]]. Action = [[ 0.21848184  0.07881328  0.04024014 -0.34428203]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.06532387  0.07764702]]. Action = [[0.16746125 0.02178136 0.01720524 0.25032544]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 521 is [True, False, False, False, True, False]
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(7.8167e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of 1
Current timestep = 522. State = [[-0.05010455  0.07447004]]. Action = [[-0.13732347 -0.1560227   0.12338057  0.59906304]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.04909887  0.0550171 ]]. Action = [[ 0.08110672 -0.18237998 -0.02546786 -0.5115784 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 523 is [True, False, False, False, True, False]
Current timestep = 524. State = [[-0.03964325  0.04612858]]. Action = [[0.18910074 0.12072825 0.06695801 0.9163172 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 524 is [False, True, False, False, True, False]
Current timestep = 525. State = [[-0.1762473  -0.20392841]]. Action = [[-0.13165413 -0.23318645  0.21322143  0.31992555]]. Reward = [100.]
Curr episode timestep = 35
Scene graph at timestep 525 is [False, True, False, False, True, False]
Current timestep = 526. State = [[-0.15323308 -0.2323264 ]]. Action = [[ 0.2209509  -0.09814692  0.22077966 -0.53234273]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 526 is [True, False, False, True, False, False]
Current timestep = 527. State = [[-0.12317592 -0.23006217]]. Action = [[ 0.2363058   0.18659347  0.17536587 -0.12158823]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 527 is [True, False, False, True, False, False]
Scene graph at timestep 527 is [True, False, False, True, False, False]
State prediction error at timestep 527 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 1
Current timestep = 528. State = [[-0.08878843 -0.22327441]]. Action = [[ 0.2099902  -0.02376781 -0.24451219 -0.9471624 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 528 is [True, False, False, True, False, False]
Scene graph at timestep 528 is [True, False, False, True, False, False]
State prediction error at timestep 528 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of 0
Current timestep = 529. State = [[-0.06456256 -0.2333208 ]]. Action = [[ 0.01813814 -0.13639714  0.19535547  0.9936075 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 529 is [True, False, False, True, False, False]
Scene graph at timestep 529 is [True, False, False, True, False, False]
State prediction error at timestep 529 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.06702054 -0.23876715]]. Action = [[-0.17595819  0.07236168  0.1291439   0.5931319 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 530 is [True, False, False, True, False, False]
Scene graph at timestep 530 is [True, False, False, True, False, False]
State prediction error at timestep 530 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.07400527 -0.23898664]]. Action = [[-0.12847888  0.01840696  0.07520065  0.871053  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 531 is [True, False, False, True, False, False]
Current timestep = 532. State = [[-0.08619802 -0.25033703]]. Action = [[-0.06680442 -0.17551458  0.09230477  0.75141287]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 532 is [True, False, False, True, False, False]
Current timestep = 533. State = [[-0.09430818 -0.2514283 ]]. Action = [[-0.04115459  0.14779174 -0.02196401 -0.77726626]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 533 is [True, False, False, True, False, False]
Scene graph at timestep 533 is [True, False, False, True, False, False]
State prediction error at timestep 533 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.09502025 -0.25209084]]. Action = [[ 0.2078259  -0.21109314 -0.22845432  0.7688986 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 534 is [True, False, False, True, False, False]
Current timestep = 535. State = [[-0.08536775 -0.26132405]]. Action = [[ 0.12709081 -0.02688664  0.24779004 -0.8079307 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 535 is [True, False, False, True, False, False]
Current timestep = 536. State = [[-0.07401787 -0.2674056 ]]. Action = [[ 0.04621279 -0.05634262 -0.1647886  -0.29523182]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 536 is [True, False, False, True, False, False]
Scene graph at timestep 536 is [True, False, False, True, False, False]
State prediction error at timestep 536 is tensor(5.3425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 0
Current timestep = 537. State = [[-0.05762906 -0.28421447]]. Action = [[ 0.21540493 -0.20710151 -0.08418405 -0.04086185]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 537 is [True, False, False, True, False, False]
Current timestep = 538. State = [[-0.02971232 -0.30555558]]. Action = [[ 0.2444182  -0.1303899   0.16604716 -0.76573074]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 538 is [True, False, False, True, False, False]
Current timestep = 539. State = [[ 0.00571863 -0.30681038]]. Action = [[ 0.21153533  0.22085276  0.14108351 -0.63695854]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 539 is [False, True, False, True, False, False]
Scene graph at timestep 539 is [False, True, False, True, False, False]
State prediction error at timestep 539 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 0
Current timestep = 540. State = [[ 0.0383048  -0.29910627]]. Action = [[ 0.22643262  0.24447405  0.05733928 -0.61065173]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 540 is [False, True, False, True, False, False]
Current timestep = 541. State = [[ 0.0351804  -0.29693264]]. Action = [[-0.2279876   0.09780651  0.05272758 -0.46343958]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 541 is [False, True, False, True, False, False]
Current timestep = 542. State = [[ 0.03046567 -0.28773466]]. Action = [[-0.02992156  0.08651263  0.12361372  0.90372944]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 542 is [False, True, False, True, False, False]
Scene graph at timestep 542 is [False, True, False, True, False, False]
State prediction error at timestep 542 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[ 0.01601177 -0.2730694 ]]. Action = [[-0.24382462  0.13124657  0.19823101  0.02992487]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 543 is [False, True, False, True, False, False]
Scene graph at timestep 543 is [False, True, False, True, False, False]
State prediction error at timestep 543 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of 0
Current timestep = 544. State = [[-0.0062315  -0.25625584]]. Action = [[0.10648578 0.03405353 0.01934326 0.7272785 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 544 is [False, True, False, True, False, False]
Scene graph at timestep 544 is [False, True, False, True, False, False]
State prediction error at timestep 544 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of 0
Current timestep = 545. State = [[-0.00511273 -0.26235482]]. Action = [[-0.02340595 -0.19635175 -0.04708551  0.87729657]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 545 is [False, True, False, True, False, False]
Current timestep = 546. State = [[-0.01657235 -0.2782168 ]]. Action = [[-0.23687556 -0.0366697   0.12916774  0.4704125 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 546 is [False, True, False, True, False, False]
Current timestep = 547. State = [[-0.02420987 -0.2925968 ]]. Action = [[ 0.1806553  -0.16909388 -0.22064267 -0.4924146 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 547 is [False, True, False, True, False, False]
Scene graph at timestep 547 is [False, True, False, True, False, False]
State prediction error at timestep 547 is tensor(2.0794e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.02253685 -0.30250496]]. Action = [[ 0.09186736 -0.13543478  0.178025   -0.8142168 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 548 is [False, True, False, True, False, False]
Current timestep = 549. State = [[-0.02244746 -0.30240178]]. Action = [[-0.05242313  0.04448998  0.08222386  0.56498826]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 549 is [False, True, False, True, False, False]
Scene graph at timestep 549 is [False, True, False, True, False, False]
State prediction error at timestep 549 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.02843253 -0.3023948 ]]. Action = [[-0.16152339  0.04254618 -0.11072081 -0.6074691 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 550 is [False, True, False, True, False, False]
Current timestep = 551. State = [[-0.03396479 -0.30297786]]. Action = [[-0.02598277 -0.23011872  0.16891795  0.50509083]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 551 is [False, True, False, True, False, False]
Current timestep = 552. State = [[-0.03949607 -0.30418894]]. Action = [[-0.11816677 -0.00200661  0.20114475  0.76358044]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 552 is [False, True, False, True, False, False]
Scene graph at timestep 552 is [False, True, False, True, False, False]
State prediction error at timestep 552 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of -1
Current timestep = 553. State = [[-0.0619178 -0.2977179]]. Action = [[-0.2207555   0.13465163 -0.22699441  0.49414206]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 553 is [False, True, False, True, False, False]
Current timestep = 554. State = [[-0.08277515 -0.28579885]]. Action = [[ 0.1298747  -0.20347737  0.09563708  0.36482596]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 554 is [True, False, False, True, False, False]
Current timestep = 555. State = [[-0.08508357 -0.2698677 ]]. Action = [[ 0.06021708  0.23169667  0.00254703 -0.16596991]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 555 is [True, False, False, True, False, False]
Current timestep = 556. State = [[-0.08876118 -0.2445277 ]]. Action = [[-0.19318168  0.12942472  0.22896492 -0.8345118 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 556 is [True, False, False, True, False, False]
Scene graph at timestep 556 is [True, False, False, True, False, False]
State prediction error at timestep 556 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of -1
Current timestep = 557. State = [[-0.11004967 -0.23151828]]. Action = [[ 0.05617562 -0.09703743  0.00452811  0.60959136]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 557 is [True, False, False, True, False, False]
Current timestep = 558. State = [[-0.11058079 -0.23396574]]. Action = [[-0.04660967  0.01008782 -0.12279001  0.5851741 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 558 is [True, False, False, True, False, False]
Current timestep = 559. State = [[-0.11044239 -0.23486203]]. Action = [[ 0.13905662 -0.03135341  0.1918348  -0.8470692 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 559 is [True, False, False, True, False, False]
Scene graph at timestep 559 is [True, False, False, True, False, False]
State prediction error at timestep 559 is tensor(2.0281e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of -1
Current timestep = 560. State = [[-0.09863155 -0.23768312]]. Action = [[ 0.2032837  -0.06520003 -0.10588893  0.848606  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 560 is [True, False, False, True, False, False]
Scene graph at timestep 560 is [True, False, False, True, False, False]
State prediction error at timestep 560 is tensor(2.8508e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of 0
Current timestep = 561. State = [[-0.08611938 -0.23799606]]. Action = [[-0.11097556  0.10668248 -0.12767777  0.22506738]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 561 is [True, False, False, True, False, False]
Scene graph at timestep 561 is [True, False, False, True, False, False]
State prediction error at timestep 561 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of 0
Current timestep = 562. State = [[-0.08242718 -0.23604743]]. Action = [[ 0.17964381 -0.05073953  0.04707068  0.26761174]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 562 is [True, False, False, True, False, False]
Current timestep = 563. State = [[-0.07351903 -0.23253298]]. Action = [[0.08751586 0.06778246 0.16257289 0.09871483]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 563 is [True, False, False, True, False, False]
Scene graph at timestep 563 is [True, False, False, True, False, False]
State prediction error at timestep 563 is tensor(3.8605e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of 1
Current timestep = 564. State = [[-0.05484137 -0.22411756]]. Action = [[ 0.17015654  0.07421431 -0.12435327 -0.81991744]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 564 is [True, False, False, True, False, False]
Current timestep = 565. State = [[-0.03718284 -0.2307782 ]]. Action = [[ 0.12749046 -0.21999048 -0.00310163 -0.28240514]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 565 is [True, False, False, True, False, False]
Current timestep = 566. State = [[-0.01163875 -0.22973259]]. Action = [[0.2428244  0.21057522 0.07627675 0.00709748]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 566 is [False, True, False, True, False, False]
Scene graph at timestep 566 is [False, True, False, True, False, False]
State prediction error at timestep 566 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of 0
Current timestep = 567. State = [[ 0.02844345 -0.20318688]]. Action = [[0.21305397 0.248611   0.11855501 0.9181535 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 567 is [False, True, False, True, False, False]
Current timestep = 568. State = [[ 0.04680752 -0.18678054]]. Action = [[ 0.22128147  0.09886652 -0.22134705 -0.53995126]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 568 is [False, True, False, True, False, False]
Current timestep = 569. State = [[ 0.04889731 -0.17205493]]. Action = [[-0.17219904  0.23144132  0.1896201   0.40409648]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 569 is [False, True, False, True, False, False]
Current timestep = 570. State = [[ 0.03589463 -0.16681892]]. Action = [[-0.24402852 -0.14159854  0.06456602  0.96735525]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 570 is [False, True, False, True, False, False]
Current timestep = 571. State = [[ 0.02108481 -0.17705055]]. Action = [[ 0.06669071 -0.10583483 -0.03740379 -0.7378575 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 571 is [False, True, False, True, False, False]
Scene graph at timestep 571 is [False, True, False, True, False, False]
State prediction error at timestep 571 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[ 0.02213328 -0.17502572]]. Action = [[ 0.19753438  0.15961266 -0.20340216 -0.92835593]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 572 is [False, True, False, True, False, False]
Current timestep = 573. State = [[ 0.02372514 -0.17118864]]. Action = [[-0.24768625 -0.077969   -0.1198433  -0.4249711 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 573 is [False, True, False, True, False, False]
Current timestep = 574. State = [[ 0.01475167 -0.16709483]]. Action = [[-0.08329354  0.14911634  0.06854159 -0.7724965 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 574 is [False, True, False, True, False, False]
Scene graph at timestep 574 is [False, True, False, True, False, False]
State prediction error at timestep 574 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[ 0.00207004 -0.14035015]]. Action = [[ 0.04984364  0.2173937   0.08803272 -0.00429839]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 575 is [False, True, False, True, False, False]
Current timestep = 576. State = [[ 0.00235991 -0.1141709 ]]. Action = [[0.03532013 0.14406013 0.21309674 0.8838935 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 576 is [False, True, False, True, False, False]
Current timestep = 577. State = [[-0.22169897  0.01312052]]. Action = [[-0.05518314  0.10857594 -0.0312299   0.401039  ]]. Reward = [100.]
Curr episode timestep = 51
Scene graph at timestep 577 is [False, True, False, False, True, False]
Scene graph at timestep 577 is [True, False, False, False, True, False]
State prediction error at timestep 577 is tensor(0.0337, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of -1
Current timestep = 578. State = [[-0.21723671  0.01162606]]. Action = [[-0.22126915 -0.11302008 -0.18587333  0.6606958 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 578 is [True, False, False, False, True, False]
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 578 of -1
Current timestep = 579. State = [[-0.23120598  0.01237327]]. Action = [[-0.07280794  0.11878473  0.15660515 -0.57046556]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.23252419  0.00874269]]. Action = [[ 0.16717818 -0.1893154   0.21860218  0.176512  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 580 is [True, False, False, False, True, False]
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of -1
Current timestep = 581. State = [[-0.2310904   0.00653513]]. Action = [[-0.00425771  0.18343255 -0.10442141 -0.97414005]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.23860407  0.02516424]]. Action = [[-0.19865508  0.13379994  0.07533276 -0.5150334 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.24427165  0.04309329]]. Action = [[0.10706529 0.10324752 0.22617933 0.15824234]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 583 is [True, False, False, False, True, False]
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.24099751  0.05023514]]. Action = [[ 0.08411855 -0.0531964   0.14419183  0.8036258 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 584 is [True, False, False, False, True, False]
Current timestep = 585. State = [[-0.23545285  0.04991842]]. Action = [[-0.22169457  0.06382388 -0.17486112  0.8428216 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 585 is [True, False, False, False, True, False]
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(2.3793e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 585 of 1
Current timestep = 586. State = [[-0.22395827  0.03577546]]. Action = [[ 0.18947339 -0.2179478   0.19938284 -0.83138764]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 586 is [True, False, False, False, True, False]
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of 1
Current timestep = 587. State = [[-0.19760355  0.00769074]]. Action = [[ 0.1763053  -0.18324599 -0.22481413 -0.24589252]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.18241435 -0.01899037]]. Action = [[ 0.02599582 -0.22770219  0.16121936  0.8664491 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 588 is [True, False, False, False, True, False]
Scene graph at timestep 588 is [True, False, False, False, True, False]
State prediction error at timestep 588 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of 1
Current timestep = 589. State = [[-0.17600805 -0.02850477]]. Action = [[-0.01579307  0.2142821   0.07831758 -0.29916722]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 589 is [True, False, False, False, True, False]
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(6.3665e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[-0.18228073 -0.00594144]]. Action = [[-0.1954269   0.1631644  -0.21042867 -0.7297666 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 590 is [True, False, False, False, True, False]
Current timestep = 591. State = [[-0.18652293  0.00473799]]. Action = [[ 0.1230644  -0.04638226  0.17513227 -0.7852972 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 591 is [True, False, False, False, True, False]
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(1.6107e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.17935427 -0.00873355]]. Action = [[ 0.1320787  -0.22688365  0.15358734 -0.85483867]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 592 is [True, False, False, False, True, False]
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of 1
Current timestep = 593. State = [[-0.17198443 -0.02966778]]. Action = [[-0.03588277 -0.07508001 -0.11472201  0.13700509]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 593 is [True, False, False, False, True, False]
Current timestep = 594. State = [[-0.1671953  -0.02808465]]. Action = [[0.14882684 0.15352309 0.03449684 0.9253218 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 594 is [True, False, False, False, True, False]
Scene graph at timestep 594 is [True, False, False, False, True, False]
State prediction error at timestep 594 is tensor(9.4804e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 594 of 1
Current timestep = 595. State = [[-0.15354872 -0.03403411]]. Action = [[-0.06657851 -0.22566105 -0.13359004 -0.7446758 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 595 is [True, False, False, False, True, False]
Current timestep = 596. State = [[-0.1596574  -0.04777717]]. Action = [[-0.15943907 -0.03272934 -0.17490242  0.851063  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 596 is [True, False, False, False, True, False]
Current timestep = 597. State = [[-0.17323993 -0.06815617]]. Action = [[-0.15655632 -0.22395532 -0.2415328  -0.13368481]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 597 is [True, False, False, False, True, False]
Scene graph at timestep 597 is [True, False, False, False, True, False]
State prediction error at timestep 597 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.19886915 -0.07619774]]. Action = [[-0.20372832  0.21799833 -0.12135784 -0.24522746]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 598 is [True, False, False, False, True, False]
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.21764551 -0.06003571]]. Action = [[ 0.0356124   0.0164516   0.08545381 -0.49980378]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 599 is [True, False, False, False, True, False]
Current timestep = 600. State = [[-0.21399    -0.06491146]]. Action = [[ 0.11089611 -0.13706505 -0.1284562  -0.8962807 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 600 is [True, False, False, False, True, False]
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(3.6334e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 0
Current timestep = 601. State = [[-0.21576251 -0.06967449]]. Action = [[-0.2089714   0.06183562 -0.0143429  -0.7344617 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 601 is [True, False, False, False, True, False]
Current timestep = 602. State = [[-0.23527841 -0.08469308]]. Action = [[-0.24530143 -0.24169977  0.02519783 -0.7075258 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 602 is [True, False, False, False, True, False]
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.2580461  -0.09311058]]. Action = [[ 0.15215287  0.19213092 -0.02834751  0.27790487]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 603 is [True, False, False, False, True, False]
Current timestep = 604. State = [[-0.24485332 -0.07107096]]. Action = [[ 0.18742996  0.15961096  0.1973069  -0.19876301]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 604 is [True, False, False, False, True, False]
Current timestep = 605. State = [[-0.22813305 -0.0617131 ]]. Action = [[ 0.11278954 -0.08280765 -0.06245118  0.8347821 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-0.21832429 -0.05227489]]. Action = [[6.7155063e-03 1.7495805e-01 3.6421418e-04 5.2084184e-01]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 606 is [True, False, False, False, True, False]
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of 1
Current timestep = 607. State = [[-0.2062171  -0.05382383]]. Action = [[ 0.15279445 -0.23472299  0.19437894  0.05476558]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.19055088 -0.07534833]]. Action = [[ 0.12276727 -0.1754433   0.11215776 -0.19275594]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 608 is [True, False, False, False, True, False]
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of 1
Current timestep = 609. State = [[-0.1762596  -0.08329736]]. Action = [[-0.02768305  0.1896424   0.16139227  0.9545126 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 0
Current timestep = 610. State = [[-0.17757939 -0.07167295]]. Action = [[-0.04513073  0.02305901 -0.03333712 -0.8871646 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.17952675 -0.06252997]]. Action = [[-0.05478056  0.11224687  0.1547113  -0.52762645]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 611 is [True, False, False, False, True, False]
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(1.5531e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of 1
Current timestep = 612. State = [[-0.17914061 -0.05775749]]. Action = [[ 0.12551498 -0.11255321  0.14497164  0.9094125 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.1680347  -0.05047882]]. Action = [[0.18566835 0.20466876 0.15863708 0.18260622]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 613 is [True, False, False, False, True, False]
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(3.9345e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of 1
Current timestep = 614. State = [[-0.13891648 -0.04548154]]. Action = [[ 0.24777022 -0.12921204  0.10319012 -0.8976109 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.12199793 -0.06389952]]. Action = [[-0.03801355 -0.21726185  0.20019189 -0.8236277 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 615 is [True, False, False, False, True, False]
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of 1
Current timestep = 616. State = [[-0.11102048 -0.09530047]]. Action = [[ 0.19749779 -0.2255363   0.13851252  0.85573936]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.0859181 -0.1001569]]. Action = [[0.16105881 0.21473953 0.18007505 0.50055504]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 617 is [True, False, False, False, True, False]
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.06531561 -0.10078104]]. Action = [[ 0.04525578 -0.20647967  0.05068281  0.34133816]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.06589733 -0.1184111 ]]. Action = [[-0.14799301 -0.08432028 -0.09390874  0.9482486 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 619 is [True, False, False, False, True, False]
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.0695001  -0.12712297]]. Action = [[-0.00082047  0.03462943 -0.23564005  0.47624612]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 620 is [True, False, False, False, True, False]
Scene graph at timestep 620 is [True, False, False, True, False, False]
State prediction error at timestep 620 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of -1
Current timestep = 621. State = [[-0.07039323 -0.13349031]]. Action = [[-0.01945123 -0.11794272 -0.10386208 -0.52433664]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 621 is [True, False, False, True, False, False]
Scene graph at timestep 621 is [True, False, False, True, False, False]
State prediction error at timestep 621 is tensor(2.3658e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.07355037 -0.15134487]]. Action = [[-0.01472427 -0.1717212   0.23017141  0.25954175]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 622 is [True, False, False, True, False, False]
Scene graph at timestep 622 is [True, False, False, True, False, False]
State prediction error at timestep 622 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.07625407 -0.16566417]]. Action = [[-0.08015457  0.03449079  0.20812124  0.20061123]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 623 is [True, False, False, True, False, False]
Scene graph at timestep 623 is [True, False, False, True, False, False]
State prediction error at timestep 623 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.07301225 -0.17596306]]. Action = [[ 0.23857832 -0.21121033 -0.06547451  0.42270064]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 624 is [True, False, False, True, False, False]
Current timestep = 625. State = [[-0.05861995 -0.17648658]]. Action = [[0.11612046 0.2255561  0.08547211 0.8233743 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 625 is [True, False, False, True, False, False]
Current timestep = 626. State = [[-0.04295259 -0.15996766]]. Action = [[ 0.07944548  0.10539401  0.17480993 -0.0712831 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 626 is [True, False, False, True, False, False]
Scene graph at timestep 626 is [False, True, False, True, False, False]
State prediction error at timestep 626 is tensor(8.0489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.03611413 -0.14847273]]. Action = [[-0.20220421  0.0410234   0.05019581  0.87967575]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 627 is [False, True, False, True, False, False]
Scene graph at timestep 627 is [False, True, False, True, False, False]
State prediction error at timestep 627 is tensor(7.2508e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of 0
Current timestep = 628. State = [[-0.049005   -0.15233853]]. Action = [[-0.18917777 -0.09894909 -0.074503    0.3967638 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 628 is [False, True, False, True, False, False]
Scene graph at timestep 628 is [False, True, False, True, False, False]
State prediction error at timestep 628 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[-0.06460527 -0.14820966]]. Action = [[ 0.07284829  0.17238426  0.02643132 -0.16966265]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 629 is [False, True, False, True, False, False]
Current timestep = 630. State = [[-0.06560443 -0.14781804]]. Action = [[-0.04820491 -0.20846327 -0.19558646  0.98060584]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 630 is [True, False, False, True, False, False]
Current timestep = 631. State = [[-0.07116659 -0.15964411]]. Action = [[-0.05658472 -0.03041536  0.1770894   0.9029896 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 631 is [True, False, False, True, False, False]
Current timestep = 632. State = [[-0.08304561 -0.16210127]]. Action = [[-0.17453691  0.05502933 -0.08375728  0.52424634]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 632 is [True, False, False, True, False, False]
Scene graph at timestep 632 is [True, False, False, True, False, False]
State prediction error at timestep 632 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.09683999 -0.1621362 ]]. Action = [[ 0.01371521 -0.0014137  -0.06786391  0.5190244 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 633 is [True, False, False, True, False, False]
Scene graph at timestep 633 is [True, False, False, True, False, False]
State prediction error at timestep 633 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.10103446 -0.15760629]]. Action = [[-0.11336569  0.07697362  0.1355181  -0.04363495]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 634 is [True, False, False, True, False, False]
Scene graph at timestep 634 is [True, False, False, True, False, False]
State prediction error at timestep 634 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.10284474 -0.13779151]]. Action = [[ 0.2246232   0.21719801  0.1394422  -0.7898395 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 635 is [True, False, False, True, False, False]
Scene graph at timestep 635 is [True, False, False, True, False, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of 1
Current timestep = 636. State = [[-0.09452543 -0.12496981]]. Action = [[ 0.13266131 -0.15149142 -0.22280541 -0.80813605]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 636 is [True, False, False, True, False, False]
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 0
Current timestep = 637. State = [[-0.08437899 -0.12919864]]. Action = [[-0.12161556  0.08438846  0.0367704   0.850384  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 637 is [True, False, False, False, True, False]
Scene graph at timestep 637 is [True, False, False, True, False, False]
State prediction error at timestep 637 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 0
Current timestep = 638. State = [[-0.07965288 -0.11655617]]. Action = [[0.1712296  0.17549014 0.06860965 0.7857809 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 638 is [True, False, False, True, False, False]
Current timestep = 639. State = [[-0.069754  -0.0910244]]. Action = [[ 0.12308291  0.21929908  0.12001368 -0.92096597]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 639 is [True, False, False, False, True, False]
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of 1
Current timestep = 640. State = [[-0.06246979 -0.08065645]]. Action = [[-0.1695335  -0.2019561   0.19984567  0.5932696 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 640 is [True, False, False, False, True, False]
Current timestep = 641. State = [[-0.07052899 -0.08236728]]. Action = [[-0.17770582  0.15625954 -0.16938534 -0.04953104]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 641 is [True, False, False, False, True, False]
Current timestep = 642. State = [[-0.07783069 -0.06147803]]. Action = [[ 0.22719723  0.23467985  0.24026468 -0.8946926 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 642 is [True, False, False, False, True, False]
Current timestep = 643. State = [[-0.06705878 -0.05130976]]. Action = [[ 0.19510415 -0.13872719 -0.07998571  0.9084902 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 643 is [True, False, False, False, True, False]
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(2.4291e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[-0.04609152 -0.05449177]]. Action = [[ 0.14617822 -0.00639333 -0.06941089  0.96776986]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-0.2544699 -0.172595 ]]. Action = [[-0.11994532 -0.10897607 -0.04473235  0.92090034]]. Reward = [100.]
Curr episode timestep = 67
Scene graph at timestep 645 is [False, True, False, False, True, False]
Scene graph at timestep 645 is [True, False, False, True, False, False]
State prediction error at timestep 645 is tensor(0.0331, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of 0
Current timestep = 646. State = [[-0.25265148 -0.19129051]]. Action = [[-0.22352795  0.07796139  0.19694036  0.9163301 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 646 is [True, False, False, True, False, False]
Current timestep = 647. State = [[-0.25273457 -0.18992126]]. Action = [[-0.07654768  0.08512107 -0.09315574 -0.18041903]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 647 is [True, False, False, True, False, False]
Current timestep = 648. State = [[-0.2527232 -0.1896854]]. Action = [[-0.24190299  0.15123254  0.19464976  0.25188613]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 648 is [True, False, False, True, False, False]
Scene graph at timestep 648 is [True, False, False, True, False, False]
State prediction error at timestep 648 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of 0
Current timestep = 649. State = [[-0.25270683 -0.17640395]]. Action = [[ 2.9909611e-04  2.3787779e-01  1.8798628e-01 -8.9967388e-01]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 649 is [True, False, False, True, False, False]
Scene graph at timestep 649 is [True, False, False, True, False, False]
State prediction error at timestep 649 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of 1
Current timestep = 650. State = [[-0.24946909 -0.16329472]]. Action = [[ 0.12332663 -0.16728632  0.0993461  -0.13423312]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 650 is [True, False, False, True, False, False]
Scene graph at timestep 650 is [True, False, False, True, False, False]
State prediction error at timestep 650 is tensor(2.5393e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.23917645 -0.16331466]]. Action = [[ 0.13764566  0.13958138  0.23181498 -0.82999504]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 651 is [True, False, False, True, False, False]
Scene graph at timestep 651 is [True, False, False, True, False, False]
State prediction error at timestep 651 is tensor(3.5991e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[-0.23582798 -0.15332787]]. Action = [[-0.22555996  0.08520249  0.07986644  0.88562775]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 652 is [True, False, False, True, False, False]
Current timestep = 653. State = [[-0.23541565 -0.1594042 ]]. Action = [[ 0.20235011 -0.2425213   0.09234628  0.984534  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 653 is [True, False, False, True, False, False]
Scene graph at timestep 653 is [True, False, False, True, False, False]
State prediction error at timestep 653 is tensor(4.7376e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 0
Current timestep = 654. State = [[-0.22789033 -0.162991  ]]. Action = [[ 0.07919914  0.16974899 -0.16531497  0.7273959 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 654 is [True, False, False, True, False, False]
Current timestep = 655. State = [[-0.21363987 -0.14551136]]. Action = [[0.16930377 0.17511952 0.12422726 0.24271023]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 655 is [True, False, False, True, False, False]
Scene graph at timestep 655 is [True, False, False, True, False, False]
State prediction error at timestep 655 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of 1
Current timestep = 656. State = [[-0.20285407 -0.13977157]]. Action = [[-0.16719516 -0.16905198 -0.13511123  0.19350886]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 656 is [True, False, False, True, False, False]
Scene graph at timestep 656 is [True, False, False, True, False, False]
State prediction error at timestep 656 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.20846109 -0.15877354]]. Action = [[ 0.08710068 -0.14770244 -0.13315865  0.08964276]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 657 is [True, False, False, True, False, False]
Current timestep = 658. State = [[-0.20078418 -0.16021936]]. Action = [[ 0.12864488  0.13043407  0.1780563  -0.2930436 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 658 is [True, False, False, True, False, False]
Scene graph at timestep 658 is [True, False, False, True, False, False]
State prediction error at timestep 658 is tensor(4.2784e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 0
Current timestep = 659. State = [[-0.19622931 -0.16480595]]. Action = [[-0.19865073 -0.16622151 -0.01639882 -0.15052319]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 659 is [True, False, False, True, False, False]
Scene graph at timestep 659 is [True, False, False, True, False, False]
State prediction error at timestep 659 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.20091636 -0.18697226]]. Action = [[ 0.15759063 -0.1751789   0.22139913  0.52761793]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 660 is [True, False, False, True, False, False]
Current timestep = 661. State = [[-0.19919996 -0.18842992]]. Action = [[-0.16239618  0.17880684 -0.13233325  0.6155298 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 661 is [True, False, False, True, False, False]
Scene graph at timestep 661 is [True, False, False, True, False, False]
State prediction error at timestep 661 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of -1
Current timestep = 662. State = [[-0.19482952 -0.18794785]]. Action = [[ 0.24066788 -0.14117464  0.06957212  0.71508145]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 662 is [True, False, False, True, False, False]
Current timestep = 663. State = [[-0.18344843 -0.18594298]]. Action = [[ 0.0258067   0.1291165  -0.02852471  0.83767724]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 663 is [True, False, False, True, False, False]
Scene graph at timestep 663 is [True, False, False, True, False, False]
State prediction error at timestep 663 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of 0
Current timestep = 664. State = [[-0.17989197 -0.18139707]]. Action = [[ 0.04488814 -0.01065665  0.17404038 -0.82873094]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 664 is [True, False, False, True, False, False]
Current timestep = 665. State = [[-0.16754726 -0.1763688 ]]. Action = [[ 0.23380154  0.08346939 -0.14357062  0.94697726]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 665 is [True, False, False, True, False, False]
Current timestep = 666. State = [[-0.15217131 -0.18239397]]. Action = [[-0.14969112 -0.18659548 -0.08170116  0.33153987]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 666 is [True, False, False, True, False, False]
Current timestep = 667. State = [[-0.14929478 -0.18381183]]. Action = [[0.16203985 0.13098526 0.10369411 0.45639884]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 667 is [True, False, False, True, False, False]
Scene graph at timestep 667 is [True, False, False, True, False, False]
State prediction error at timestep 667 is tensor(3.8520e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 0
Current timestep = 668. State = [[-0.13042997 -0.18620805]]. Action = [[ 0.18775249 -0.16075787  0.10909653 -0.20502222]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 668 is [True, False, False, True, False, False]
Current timestep = 669. State = [[-0.10370114 -0.19218273]]. Action = [[ 0.2220692   0.0413433   0.01132807 -0.8955531 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 669 is [True, False, False, True, False, False]
Scene graph at timestep 669 is [True, False, False, True, False, False]
State prediction error at timestep 669 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of 0
Current timestep = 670. State = [[-0.07210368 -0.18815938]]. Action = [[0.13911247 0.08962646 0.0524011  0.27232254]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 670 is [True, False, False, True, False, False]
Scene graph at timestep 670 is [True, False, False, True, False, False]
State prediction error at timestep 670 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.05117492 -0.16988014]]. Action = [[ 0.08240074  0.23545823 -0.2202867   0.47158337]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 671 is [True, False, False, True, False, False]
Current timestep = 672. State = [[-0.03872123 -0.15898244]]. Action = [[ 0.15885866 -0.13326634  0.17916727  0.04009175]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 672 is [True, False, False, True, False, False]
Current timestep = 673. State = [[-0.02864249 -0.17242311]]. Action = [[-0.07848302 -0.15523471  0.1526517  -0.9192542 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 673 is [False, True, False, True, False, False]
Scene graph at timestep 673 is [False, True, False, True, False, False]
State prediction error at timestep 673 is tensor(7.8330e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 0
Current timestep = 674. State = [[-0.024846   -0.18754728]]. Action = [[ 0.12448677 -0.06180024  0.17971992 -0.49490476]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 674 is [False, True, False, True, False, False]
Current timestep = 675. State = [[-0.00997605 -0.18114416]]. Action = [[ 0.11220199  0.19532031 -0.01917602  0.5036297 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 675 is [False, True, False, True, False, False]
Current timestep = 676. State = [[ 0.00795389 -0.17829746]]. Action = [[ 0.16397423 -0.12140909 -0.21687785  0.5455165 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 676 is [False, True, False, True, False, False]
Current timestep = 677. State = [[ 0.03361692 -0.17908101]]. Action = [[ 0.23311019  0.02510718 -0.11153743 -0.10422659]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 677 is [False, True, False, True, False, False]
Current timestep = 678. State = [[ 0.06242734 -0.18364263]]. Action = [[ 0.16912097 -0.08300346 -0.14567162  0.34886706]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 678 is [False, True, False, True, False, False]
Scene graph at timestep 678 is [False, False, True, True, False, False]
State prediction error at timestep 678 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[ 0.09100111 -0.19301991]]. Action = [[ 0.02348471  0.0076718  -0.0648711  -0.89642453]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 679 is [False, False, True, True, False, False]
Scene graph at timestep 679 is [False, False, True, True, False, False]
State prediction error at timestep 679 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of -1
Current timestep = 680. State = [[ 0.09100111 -0.19301991]]. Action = [[ 0.02774811  0.21701854 -0.21358512  0.24321091]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 680 is [False, False, True, True, False, False]
Current timestep = 681. State = [[ 0.091021   -0.19299722]]. Action = [[-0.00853637 -0.20131642 -0.1457701  -0.8946974 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 681 is [False, False, True, True, False, False]
Scene graph at timestep 681 is [False, False, True, True, False, False]
State prediction error at timestep 681 is tensor(5.3958e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of -1
Current timestep = 682. State = [[ 0.091021   -0.19299722]]. Action = [[-0.0419666  -0.08972985  0.16133744 -0.9651493 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 682 is [False, False, True, True, False, False]
Scene graph at timestep 682 is [False, False, True, True, False, False]
State prediction error at timestep 682 is tensor(1.0159e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[ 0.091021   -0.19299722]]. Action = [[ 0.08349526 -0.14601217 -0.01345672  0.6271095 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 683 is [False, False, True, True, False, False]
Scene graph at timestep 683 is [False, False, True, True, False, False]
State prediction error at timestep 683 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[ 0.091021   -0.19299722]]. Action = [[ 0.20845947  0.20926696 -0.0589367   0.6015897 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 684 is [False, False, True, True, False, False]
Scene graph at timestep 684 is [False, False, True, True, False, False]
State prediction error at timestep 684 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of -1
Current timestep = 685. State = [[ 0.091021   -0.19299722]]. Action = [[-0.02903128  0.11684322 -0.21775731 -0.32871258]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 685 is [False, False, True, True, False, False]
Scene graph at timestep 685 is [False, False, True, True, False, False]
State prediction error at timestep 685 is tensor(2.5249e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of -1
Current timestep = 686. State = [[ 0.09023582 -0.18659225]]. Action = [[-0.16510649  0.14466763 -0.13905491 -0.42123497]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 686 is [False, False, True, True, False, False]
Current timestep = 687. State = [[ 0.08821786 -0.18094175]]. Action = [[-0.0964354   0.1801982  -0.21179622 -0.7082228 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 687 is [False, False, True, True, False, False]
Scene graph at timestep 687 is [False, False, True, True, False, False]
State prediction error at timestep 687 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 687 of 1
Current timestep = 688. State = [[ 0.08767939 -0.17975913]]. Action = [[-0.00739281  0.233437    0.21189636  0.23811603]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 688 is [False, False, True, True, False, False]
Scene graph at timestep 688 is [False, False, True, True, False, False]
State prediction error at timestep 688 is tensor(6.2982e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of -1
Current timestep = 689. State = [[ 0.08476508 -0.18328215]]. Action = [[-0.11436978 -0.04919694  0.21013874 -0.9505772 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 689 is [False, False, True, True, False, False]
Current timestep = 690. State = [[ 0.08205748 -0.18812102]]. Action = [[ 0.10594031 -0.21539973  0.07441697  0.58020926]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 690 is [False, False, True, True, False, False]
Current timestep = 691. State = [[ 0.08160428 -0.18827052]]. Action = [[ 0.12083155  0.1074425  -0.04447857  0.3115343 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 691 is [False, False, True, True, False, False]
Scene graph at timestep 691 is [False, False, True, True, False, False]
State prediction error at timestep 691 is tensor(5.9195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[ 0.08147028 -0.18830696]]. Action = [[ 0.08181119  0.0577265   0.11828363 -0.6064341 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 692 is [False, False, True, True, False, False]
Current timestep = 693. State = [[ 0.07200149 -0.20148729]]. Action = [[-0.20010188 -0.19273864  0.19997516 -0.81471026]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 693 is [False, False, True, True, False, False]
Current timestep = 694. State = [[ 0.05235264 -0.2110963 ]]. Action = [[-0.15665345  0.06409678 -0.14708139  0.55252254]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 694 is [False, False, True, True, False, False]
Current timestep = 695. State = [[ 0.03642092 -0.20834841]]. Action = [[ 0.19523504  0.20982385 -0.21205983  0.32706237]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 695 is [False, False, True, True, False, False]
Current timestep = 696. State = [[ 0.02456094 -0.19597913]]. Action = [[-0.22573681  0.21172929  0.03930151 -0.2797194 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 696 is [False, True, False, True, False, False]
Current timestep = 697. State = [[ 0.01183874 -0.16693187]]. Action = [[0.13661331 0.17481935 0.1719273  0.40909207]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 697 is [False, True, False, True, False, False]
Current timestep = 698. State = [[ 0.00386355 -0.16696854]]. Action = [[-0.21885464 -0.23088138  0.21885183  0.05314279]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 698 is [False, True, False, True, False, False]
Scene graph at timestep 698 is [False, True, False, True, False, False]
State prediction error at timestep 698 is tensor(7.7964e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[-0.01121587 -0.177097  ]]. Action = [[ 0.19958675  0.07524952 -0.13572586  0.5459826 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 699 is [False, True, False, True, False, False]
Current timestep = 700. State = [[-0.00477605 -0.17198546]]. Action = [[ 0.08878881 -0.04282069  0.10021219  0.23332345]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 700 is [False, True, False, True, False, False]
Current timestep = 701. State = [[-0.00205516 -0.18515052]]. Action = [[ 0.04688618 -0.23340088 -0.15998797  0.41979074]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 701 is [False, True, False, True, False, False]
Current timestep = 702. State = [[ 0.01533107 -0.18703647]]. Action = [[ 0.238056    0.23420098 -0.01728907 -0.08681816]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 702 is [False, True, False, True, False, False]
Current timestep = 703. State = [[ 0.0434739  -0.17262718]]. Action = [[ 0.22726393  0.03716302  0.16688946 -0.7356856 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 703 is [False, True, False, True, False, False]
Scene graph at timestep 703 is [False, True, False, True, False, False]
State prediction error at timestep 703 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[ 0.0652595  -0.15447871]]. Action = [[-0.21962355  0.20087662  0.16976482 -0.04388547]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 704 is [False, True, False, True, False, False]
Scene graph at timestep 704 is [False, False, True, True, False, False]
State prediction error at timestep 704 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of 1
Current timestep = 705. State = [[ 0.06061993 -0.13882491]]. Action = [[ 0.02629057  0.04027769  0.23041433 -0.3376541 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 705 is [False, False, True, True, False, False]
Scene graph at timestep 705 is [False, False, True, True, False, False]
State prediction error at timestep 705 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of 1
Current timestep = 706. State = [[ 0.06054525 -0.13586567]]. Action = [[0.149221   0.12509307 0.10277671 0.60716677]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 706 is [False, False, True, True, False, False]
Current timestep = 707. State = [[ 0.06054525 -0.13586567]]. Action = [[ 0.04831031  0.0553298   0.16773456 -0.2952757 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 707 is [False, False, True, True, False, False]
Current timestep = 708. State = [[ 0.06054525 -0.13586567]]. Action = [[ 0.20106763 -0.09625393 -0.04566295  0.9366994 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 708 is [False, False, True, True, False, False]
Scene graph at timestep 708 is [False, False, True, True, False, False]
State prediction error at timestep 708 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[ 0.05617003 -0.12625347]]. Action = [[-0.19605061  0.16399485  0.07906243 -0.19770288]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 709 is [False, False, True, True, False, False]
Scene graph at timestep 709 is [False, False, True, True, False, False]
State prediction error at timestep 709 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[ 0.03906963 -0.10989928]]. Action = [[ 0.19271082  0.05011043 -0.08527763 -0.5717604 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 710 is [False, False, True, True, False, False]
Current timestep = 711. State = [[ 0.04258623 -0.09756865]]. Action = [[ 0.14862698  0.21103847 -0.17604864  0.97885585]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 711 is [False, True, False, False, True, False]
Current timestep = 712. State = [[ 0.04277694 -0.08159601]]. Action = [[ 0.22797137  0.0950546   0.10951877 -0.61912626]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 712 is [False, True, False, False, True, False]
Current timestep = 713. State = [[ 0.04253925 -0.07954228]]. Action = [[ 0.14957607  0.02628183 -0.10355411 -0.19778019]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 713 is [False, True, False, False, True, False]
Current timestep = 714. State = [[ 0.04485854 -0.08656237]]. Action = [[ 0.0952099  -0.15327711 -0.17951313  0.44343007]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 714 is [False, True, False, False, True, False]
Scene graph at timestep 714 is [False, True, False, False, True, False]
State prediction error at timestep 714 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[ 0.04673569 -0.09412251]]. Action = [[ 0.140652   -0.20117895 -0.10321689  0.15873802]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 715 is [False, True, False, False, True, False]
Scene graph at timestep 715 is [False, True, False, False, True, False]
State prediction error at timestep 715 is tensor(5.7285e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of -1
Current timestep = 716. State = [[ 0.04673569 -0.09412251]]. Action = [[ 0.21391362  0.01260325  0.05927432 -0.00225163]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 716 is [False, True, False, False, True, False]
Scene graph at timestep 716 is [False, True, False, False, True, False]
State prediction error at timestep 716 is tensor(2.8769e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of -1
Current timestep = 717. State = [[ 0.04662704 -0.09277694]]. Action = [[-0.05091843  0.04853272  0.04371229  0.02527237]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 717 is [False, True, False, False, True, False]
Scene graph at timestep 717 is [False, True, False, False, True, False]
State prediction error at timestep 717 is tensor(1.4811e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[ 0.04658575 -0.09223339]]. Action = [[ 0.14231703 -0.2250166   0.00730002 -0.24129963]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 718 is [False, True, False, False, True, False]
Scene graph at timestep 718 is [False, True, False, False, True, False]
State prediction error at timestep 718 is tensor(8.5281e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of -1
Current timestep = 719. State = [[ 0.04519187 -0.10350448]]. Action = [[-0.05114128 -0.19957213 -0.17086905 -0.45375896]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 719 is [False, True, False, False, True, False]
Scene graph at timestep 719 is [False, True, False, False, True, False]
State prediction error at timestep 719 is tensor(5.0476e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of -1
Current timestep = 720. State = [[ 0.04362825 -0.13042559]]. Action = [[-0.00768632 -0.23297411  0.01176581 -0.873732  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 720 is [False, True, False, False, True, False]
Scene graph at timestep 720 is [False, True, False, True, False, False]
State prediction error at timestep 720 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of -1
Current timestep = 721. State = [[ 0.04410602 -0.15051122]]. Action = [[ 0.17671797  0.16930687 -0.10080819  0.7538816 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 721 is [False, True, False, True, False, False]
Current timestep = 722. State = [[ 0.04147363 -0.16138116]]. Action = [[-0.0249874  -0.16328655 -0.0023293  -0.03811055]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 722 is [False, True, False, True, False, False]
Current timestep = 723. State = [[ 0.04028413 -0.17349324]]. Action = [[0.1487273  0.11839271 0.09761107 0.01321554]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 723 is [False, True, False, True, False, False]
Current timestep = 724. State = [[ 0.03339368 -0.18487072]]. Action = [[-0.150937   -0.12754953 -0.06008172  0.6229639 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 724 is [False, True, False, True, False, False]
Current timestep = 725. State = [[ 0.02189028 -0.20359766]]. Action = [[-0.05991431 -0.1614892   0.10559624 -0.8589212 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 725 is [False, True, False, True, False, False]
Current timestep = 726. State = [[ 0.02016501 -0.21596621]]. Action = [[ 0.18951619 -0.00308564 -0.20598401  0.7689018 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 726 is [False, True, False, True, False, False]
Current timestep = 727. State = [[ 0.03176028 -0.21722813]]. Action = [[ 0.18192372 -0.01852226 -0.07480218 -0.52587986]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 727 is [False, True, False, True, False, False]
Scene graph at timestep 727 is [False, True, False, True, False, False]
State prediction error at timestep 727 is tensor(8.2562e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of -1
Current timestep = 728. State = [[ 0.05214776 -0.21534929]]. Action = [[0.01317847 0.11058956 0.23501596 0.8028393 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 728 is [False, True, False, True, False, False]
Scene graph at timestep 728 is [False, False, True, True, False, False]
State prediction error at timestep 728 is tensor(1.7751e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of -1
Current timestep = 729. State = [[ 0.05699203 -0.19771501]]. Action = [[-0.09938908  0.23525661  0.14123642  0.8995286 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 729 is [False, False, True, True, False, False]
Current timestep = 730. State = [[ 0.05720053 -0.18495096]]. Action = [[ 0.01557389 -0.06162979  0.16716266  0.7805474 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 730 is [False, False, True, True, False, False]
Scene graph at timestep 730 is [False, False, True, True, False, False]
State prediction error at timestep 730 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of -1
Current timestep = 731. State = [[ 0.05794611 -0.1746769 ]]. Action = [[ 0.01606429  0.20629916  0.17352524 -0.32006872]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 731 is [False, False, True, True, False, False]
Current timestep = 732. State = [[ 0.05830204 -0.15959145]]. Action = [[ 0.20431769  0.19551167  0.00663561 -0.85326296]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 732 is [False, False, True, True, False, False]
Current timestep = 733. State = [[ 0.05407804 -0.17003569]]. Action = [[-0.05540805 -0.24075165 -0.20661403  0.86278844]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 733 is [False, False, True, True, False, False]
Scene graph at timestep 733 is [False, False, True, True, False, False]
State prediction error at timestep 733 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[ 0.04940319 -0.17851752]]. Action = [[ 0.02570567  0.13251257 -0.13211192 -0.91429406]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 734 is [False, False, True, True, False, False]
Current timestep = 735. State = [[ 0.04959495 -0.17379223]]. Action = [[ 0.17353132 -0.02172768 -0.18483038 -0.03109097]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 735 is [False, True, False, True, False, False]
Current timestep = 736. State = [[ 0.04961444 -0.17316887]]. Action = [[ 0.12211663 -0.13181561 -0.07807399 -0.6094069 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 736 is [False, True, False, True, False, False]
Scene graph at timestep 736 is [False, True, False, True, False, False]
State prediction error at timestep 736 is tensor(2.6878e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[ 0.04961444 -0.17316887]]. Action = [[ 0.21233684 -0.0636563  -0.14914954  0.8658893 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 737 is [False, True, False, True, False, False]
Scene graph at timestep 737 is [False, True, False, True, False, False]
State prediction error at timestep 737 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of -1
Current timestep = 738. State = [[ 0.04906118 -0.16657752]]. Action = [[-0.07402748  0.10022241 -0.04218854  0.06305552]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 738 is [False, True, False, True, False, False]
Scene graph at timestep 738 is [False, True, False, True, False, False]
State prediction error at timestep 738 is tensor(4.7833e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[ 0.04852912 -0.15903886]]. Action = [[ 0.1251244   0.21178028  0.1739294  -0.8568206 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 739 is [False, True, False, True, False, False]
Scene graph at timestep 739 is [False, True, False, True, False, False]
State prediction error at timestep 739 is tensor(9.3920e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 739 of -1
Current timestep = 740. State = [[ 0.04896379 -0.14531595]]. Action = [[-0.0050962   0.24067402  0.19190848 -0.8871902 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 740 is [False, True, False, True, False, False]
Current timestep = 741. State = [[ 0.04840994 -0.11556213]]. Action = [[0.07337916 0.14325231 0.04433727 0.4557302 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 741 is [False, True, False, True, False, False]
Current timestep = 742. State = [[ 0.04520432 -0.11230298]]. Action = [[-0.10173774 -0.16395003  0.06096956  0.21494985]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 742 is [False, True, False, False, True, False]
Scene graph at timestep 742 is [False, True, False, False, True, False]
State prediction error at timestep 742 is tensor(1.2813e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of 1
Current timestep = 743. State = [[ 0.0393173  -0.10626791]]. Action = [[-0.03221032  0.24616265  0.19987074  0.23618996]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 743 is [False, True, False, False, True, False]
Current timestep = 744. State = [[-0.15030663 -0.17602351]]. Action = [[-0.03810196 -0.21257149  0.13719848  0.98565364]]. Reward = [100.]
Curr episode timestep = 98
Scene graph at timestep 744 is [False, True, False, False, True, False]
Scene graph at timestep 744 is [True, False, False, True, False, False]
State prediction error at timestep 744 is tensor(0.0236, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of 0
Current timestep = 745. State = [[-0.12824166 -0.1850059 ]]. Action = [[ 0.12347007  0.20531577  0.1309835  -0.6279118 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 745 is [True, False, False, True, False, False]
Current timestep = 746. State = [[-0.10944197 -0.16849084]]. Action = [[ 0.17845058  0.11014602  0.2072308  -0.8338866 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 746 is [True, False, False, True, False, False]
Current timestep = 747. State = [[-0.08290707 -0.162881  ]]. Action = [[ 0.22150129 -0.05319801  0.13619143 -0.10176432]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 747 is [True, False, False, True, False, False]
Current timestep = 748. State = [[-0.06657334 -0.17003079]]. Action = [[-0.11399359 -0.09195535  0.08188593  0.02123189]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 748 is [True, False, False, True, False, False]
Scene graph at timestep 748 is [True, False, False, True, False, False]
State prediction error at timestep 748 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of 1
Current timestep = 749. State = [[-0.07436646 -0.16503808]]. Action = [[-0.24175224  0.203574   -0.07541028  0.16543794]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 749 is [True, False, False, True, False, False]
Scene graph at timestep 749 is [True, False, False, True, False, False]
State prediction error at timestep 749 is tensor(5.4676e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 0
Current timestep = 750. State = [[-0.08183578 -0.13861938]]. Action = [[ 0.20513123  0.22629735  0.06400791 -0.94684947]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 750 is [True, False, False, True, False, False]
Scene graph at timestep 750 is [True, False, False, True, False, False]
State prediction error at timestep 750 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.07255392 -0.1291159 ]]. Action = [[ 0.15316856 -0.21837284 -0.10201111  0.5414336 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 751 is [True, False, False, True, False, False]
Current timestep = 752. State = [[-0.06401169 -0.13677536]]. Action = [[-0.19719778  0.08933076  0.01530915 -0.7716092 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 752 is [True, False, False, True, False, False]
Scene graph at timestep 752 is [True, False, False, True, False, False]
State prediction error at timestep 752 is tensor(1.9395e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.06323421 -0.14249165]]. Action = [[ 0.15796489 -0.13120013 -0.04161915 -0.44762748]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 753 is [True, False, False, True, False, False]
Current timestep = 754. State = [[-0.05344386 -0.13452315]]. Action = [[ 0.18261057  0.22111821  0.13460812 -0.15745676]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 754 is [True, False, False, True, False, False]
Current timestep = 755. State = [[-0.04367065 -0.11424073]]. Action = [[-0.16638356  0.16843498 -0.02789257 -0.65979385]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 755 is [True, False, False, True, False, False]
Current timestep = 756. State = [[-0.04735345 -0.0889276 ]]. Action = [[-0.06367925  0.19001931  0.0531522   0.29764557]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 756 is [False, True, False, False, True, False]
Current timestep = 757. State = [[-0.05060938 -0.06368714]]. Action = [[0.07159907 0.13700098 0.01132467 0.5330392 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 757 is [False, True, False, False, True, False]
Scene graph at timestep 757 is [True, False, False, False, True, False]
State prediction error at timestep 757 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of 1
Current timestep = 758. State = [[-0.04849082 -0.04616259]]. Action = [[ 0.11393955  0.07213262 -0.058007    0.58211684]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 758 is [True, False, False, False, True, False]
Scene graph at timestep 758 is [False, True, False, False, True, False]
State prediction error at timestep 758 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.04040178 -0.03201897]]. Action = [[ 0.03996611  0.14033857 -0.15890153  0.5361192 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 759 is [False, True, False, False, True, False]
Scene graph at timestep 759 is [False, True, False, False, True, False]
State prediction error at timestep 759 is tensor(3.8848e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of 1
Current timestep = 760. State = [[-0.17678702  0.12566851]]. Action = [[ 0.09023997  0.02340633 -0.19818048 -0.39664996]]. Reward = [100.]
Curr episode timestep = 15
Scene graph at timestep 760 is [False, True, False, False, True, False]
Current timestep = 761. State = [[-0.15705827  0.15003325]]. Action = [[0.09605005 0.1508714  0.16727519 0.76427674]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 761 is [True, False, False, False, False, True]
Current timestep = 762. State = [[-0.13729015  0.15493175]]. Action = [[ 0.21220237 -0.09341457  0.12989244 -0.9650933 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 762 is [True, False, False, False, False, True]
Current timestep = 763. State = [[-0.11310405  0.14087982]]. Action = [[ 0.11926514 -0.19149347  0.21614021 -0.94954383]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 763 is [True, False, False, False, False, True]
Scene graph at timestep 763 is [True, False, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 0
Current timestep = 764. State = [[-0.08437803  0.11122338]]. Action = [[ 0.24447185 -0.2374038   0.24204189 -0.782879  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 764 is [True, False, False, False, False, True]
Current timestep = 765. State = [[-0.06037072  0.10903566]]. Action = [[ 0.18849766  0.23616374 -0.15922883 -0.3611405 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 765 is [True, False, False, False, True, False]
Current timestep = 766. State = [[-0.26273865 -0.0135814 ]]. Action = [[ 0.06031704 -0.0195408  -0.00072239 -0.10344386]]. Reward = [100.]
Curr episode timestep = 5
Scene graph at timestep 766 is [True, False, False, False, True, False]
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(0.0269, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 766 of 0
Current timestep = 767. State = [[-0.2644209  -0.03026791]]. Action = [[-0.04700017 -0.22953668 -0.14318767  0.33549726]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 767 is [True, False, False, False, True, False]
Current timestep = 768. State = [[-0.26611483 -0.05443838]]. Action = [[ 0.07923904 -0.15253259  0.00292522 -0.77998394]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 768 is [True, False, False, False, True, False]
Scene graph at timestep 768 is [True, False, False, False, True, False]
State prediction error at timestep 768 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of -1
Current timestep = 769. State = [[-0.26701775 -0.06901638]]. Action = [[-0.18906905  0.06803006  0.0340451   0.83724904]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 769 is [True, False, False, False, True, False]
Current timestep = 770. State = [[-0.26003703 -0.05965431]]. Action = [[ 0.16329837  0.19444394 -0.04889844  0.6542697 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 770 is [True, False, False, False, True, False]
Current timestep = 771. State = [[-0.23972045 -0.05101046]]. Action = [[ 0.22743148 -0.02340733  0.2260058  -0.31572843]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 771 is [True, False, False, False, True, False]
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(1.5336e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.21864899 -0.05287002]]. Action = [[-0.00141768 -0.07288858  0.07453117 -0.1657269 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 772 is [True, False, False, False, True, False]
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 0
Current timestep = 773. State = [[-0.21500875 -0.04375566]]. Action = [[ 0.06161207  0.20172781 -0.20545875 -0.71556795]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 773 is [True, False, False, False, True, False]
Current timestep = 774. State = [[-0.21689592 -0.01951417]]. Action = [[-0.21042325  0.24500567  0.04828006 -0.62952805]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 774 is [True, False, False, False, True, False]
Current timestep = 775. State = [[-0.2283754   0.01541973]]. Action = [[-0.08199278  0.23734179 -0.11506587 -0.2551555 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 775 is [True, False, False, False, True, False]
Scene graph at timestep 775 is [True, False, False, False, True, False]
State prediction error at timestep 775 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of -1
Current timestep = 776. State = [[-0.2400287   0.03117898]]. Action = [[-0.08024651 -0.17279719  0.14796722 -0.20103991]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 776 is [True, False, False, False, True, False]
Current timestep = 777. State = [[-0.24489741  0.02045619]]. Action = [[-0.03662488  0.00055301  0.09791327  0.0740869 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 777 is [True, False, False, False, True, False]
Current timestep = 778. State = [[-0.24326903  0.00950925]]. Action = [[ 0.1787687  -0.13469692  0.229276    0.06104422]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 778 is [True, False, False, False, True, False]
Scene graph at timestep 778 is [True, False, False, False, True, False]
State prediction error at timestep 778 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of 0
Current timestep = 779. State = [[-0.2306321   0.00357839]]. Action = [[ 0.17137966  0.09520301 -0.04458003  0.92309904]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 779 is [True, False, False, False, True, False]
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[-2.1002996e-01  2.8666520e-06]]. Action = [[ 0.139238   -0.16704725 -0.03311257 -0.3891474 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 780 is [True, False, False, False, True, False]
Current timestep = 781. State = [[-0.20342901 -0.01208074]]. Action = [[-0.10579982 -0.06245884 -0.11232445 -0.5896338 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 781 is [True, False, False, False, True, False]
Scene graph at timestep 781 is [True, False, False, False, True, False]
State prediction error at timestep 781 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 781 of 1
Current timestep = 782. State = [[-0.2098608  -0.02275318]]. Action = [[-0.16948327 -0.06024724  0.22301877  0.06746078]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 782 is [True, False, False, False, True, False]
Current timestep = 783. State = [[-0.21493182 -0.0427008 ]]. Action = [[ 0.09671885 -0.23363616 -0.23795629  0.95485616]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 783 is [True, False, False, False, True, False]
Current timestep = 784. State = [[-0.2178607  -0.07447474]]. Action = [[-0.10898644 -0.24740751 -0.05765456  0.20964932]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 784 is [True, False, False, False, True, False]
Scene graph at timestep 784 is [True, False, False, False, True, False]
State prediction error at timestep 784 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of -1
Current timestep = 785. State = [[-0.23230101 -0.11331337]]. Action = [[-0.14148696 -0.22876506 -0.10511646 -0.5427159 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 785 is [True, False, False, False, True, False]
Current timestep = 786. State = [[-0.23667514 -0.13545005]]. Action = [[ 0.10057136 -0.08516061  0.08257878 -0.13200206]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 786 is [True, False, False, False, True, False]
Current timestep = 787. State = [[-0.23190975 -0.14061283]]. Action = [[0.08585382 0.09555519 0.09051916 0.8342507 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 787 is [True, False, False, True, False, False]
Scene graph at timestep 787 is [True, False, False, True, False, False]
State prediction error at timestep 787 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of -1
Current timestep = 788. State = [[-0.22624855 -0.13287063]]. Action = [[ 0.07690853  0.07708505 -0.11711438  0.6847621 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 788 is [True, False, False, True, False, False]
Current timestep = 789. State = [[-0.22393002 -0.1275798 ]]. Action = [[-0.10570037  0.0050633   0.15336153  0.9267899 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 789 is [True, False, False, True, False, False]
Scene graph at timestep 789 is [True, False, False, True, False, False]
State prediction error at timestep 789 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of 0
Current timestep = 790. State = [[-0.2222553  -0.12282407]]. Action = [[0.09307402 0.05891892 0.24309143 0.8909285 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 790 is [True, False, False, True, False, False]
Current timestep = 791. State = [[-0.22638477 -0.10630014]]. Action = [[-0.21430896  0.23441792  0.19077188  0.81413984]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.23131599 -0.08401676]]. Action = [[0.04742211 0.0692111  0.18227458 0.16599393]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 792 is [True, False, False, False, True, False]
Current timestep = 793. State = [[-0.23258577 -0.0684778 ]]. Action = [[ 0.01589936  0.12911522 -0.05520749  0.58095145]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 793 is [True, False, False, False, True, False]
Current timestep = 794. State = [[-0.23038225 -0.04342392]]. Action = [[ 0.09270334  0.23008072 -0.1434284  -0.65595347]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 794 is [True, False, False, False, True, False]
Scene graph at timestep 794 is [True, False, False, False, True, False]
State prediction error at timestep 794 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[-0.21962358 -0.03338647]]. Action = [[ 0.19633096 -0.19838095 -0.17293735 -0.9414203 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 795 is [True, False, False, False, True, False]
Current timestep = 796. State = [[-0.19768493 -0.03360467]]. Action = [[ 0.13898301  0.15683007 -0.13985433 -0.5997705 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 796 is [True, False, False, False, True, False]
Scene graph at timestep 796 is [True, False, False, False, True, False]
State prediction error at timestep 796 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 796 of 1
Current timestep = 797. State = [[-0.18450792 -0.03129035]]. Action = [[-0.04607546 -0.08216614 -0.06385526 -0.6185792 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 797 is [True, False, False, False, True, False]
Current timestep = 798. State = [[-0.18507364 -0.04249227]]. Action = [[-0.04904129 -0.14360724  0.22651607  0.4968326 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 798 is [True, False, False, False, True, False]
Current timestep = 799. State = [[-0.17979696 -0.06486277]]. Action = [[ 0.21425658 -0.22452359 -0.22545637 -0.8961542 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 799 is [True, False, False, False, True, False]
Current timestep = 800. State = [[-0.17013414 -0.07578906]]. Action = [[-0.06723443  0.12587002 -0.0330624   0.24444962]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 800 is [True, False, False, False, True, False]
Scene graph at timestep 800 is [True, False, False, False, True, False]
State prediction error at timestep 800 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of 1
Current timestep = 801. State = [[-0.16672707 -0.06643572]]. Action = [[ 0.11974365  0.11364937 -0.02708665  0.41065025]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 801 is [True, False, False, False, True, False]
Scene graph at timestep 801 is [True, False, False, False, True, False]
State prediction error at timestep 801 is tensor(3.8788e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.15618668 -0.06022334]]. Action = [[ 0.06199729 -0.06191908 -0.20569436  0.03287315]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 802 is [True, False, False, False, True, False]
Current timestep = 803. State = [[-0.15456872 -0.05893946]]. Action = [[-0.17115134  0.06103441 -0.07982028  0.19724667]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 803 is [True, False, False, False, True, False]
Current timestep = 804. State = [[-0.15974136 -0.05299035]]. Action = [[-0.0951522   0.0642145  -0.21676071  0.8429482 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 804 is [True, False, False, False, True, False]
Scene graph at timestep 804 is [True, False, False, False, True, False]
State prediction error at timestep 804 is tensor(2.0518e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of 0
Current timestep = 805. State = [[-0.17655742 -0.05264775]]. Action = [[-0.19194797 -0.08173829 -0.06248206  0.796237  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 805 is [True, False, False, False, True, False]
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of -1
Current timestep = 806. State = [[-0.190849   -0.07380983]]. Action = [[ 0.05328065 -0.24136019  0.15851906  0.09724593]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 806 is [True, False, False, False, True, False]
Current timestep = 807. State = [[-0.18404771 -0.07891891]]. Action = [[ 0.22644109  0.14650708 -0.21524552 -0.53340906]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 807 is [True, False, False, False, True, False]
Scene graph at timestep 807 is [True, False, False, False, True, False]
State prediction error at timestep 807 is tensor(1.6687e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of -1
Current timestep = 808. State = [[-0.16680491 -0.06566544]]. Action = [[ 0.13183475  0.1144093   0.10351562 -0.00354046]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 808 is [True, False, False, False, True, False]
Current timestep = 809. State = [[-0.16412942 -0.05627311]]. Action = [[-0.241213    0.0380758  -0.11489719  0.12139964]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 809 is [True, False, False, False, True, False]
Current timestep = 810. State = [[-0.16374052 -0.05251542]]. Action = [[ 0.22026324 -0.00438577  0.20010304  0.95819545]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 810 is [True, False, False, False, True, False]
Scene graph at timestep 810 is [True, False, False, False, True, False]
State prediction error at timestep 810 is tensor(5.2548e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of 0
Current timestep = 811. State = [[-0.1618439  -0.06478378]]. Action = [[-0.14658336 -0.23570992 -0.14702012 -0.51373017]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 811 is [True, False, False, False, True, False]
Scene graph at timestep 811 is [True, False, False, False, True, False]
State prediction error at timestep 811 is tensor(5.9131e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[-0.1656559  -0.07844292]]. Action = [[-0.05578379  0.04926354 -0.04184793  0.4986713 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 812 is [True, False, False, False, True, False]
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[-0.16808857 -0.07367917]]. Action = [[ 0.1217294   0.07166874  0.10576367 -0.5998952 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 813 is [True, False, False, False, True, False]
Current timestep = 814. State = [[-0.1694914  -0.05615359]]. Action = [[-0.09911352  0.24302167 -0.2329     -0.44462073]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 814 is [True, False, False, False, True, False]
Current timestep = 815. State = [[-0.16387725 -0.049437  ]]. Action = [[ 0.22818291 -0.21852045  0.02748701  0.2375294 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 815 is [True, False, False, False, True, False]
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of 0
Current timestep = 816. State = [[-0.14485982 -0.0446129 ]]. Action = [[ 0.14545241  0.24687266  0.15260446 -0.11953866]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 816 is [True, False, False, False, True, False]
Current timestep = 817. State = [[-0.13363077 -0.02404406]]. Action = [[-0.02126862  0.13571143 -0.04287027 -0.9669282 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 817 is [True, False, False, False, True, False]
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of 1
Current timestep = 818. State = [[-0.12320844  0.00347537]]. Action = [[ 0.1430372   0.21593684 -0.07895367 -0.3387344 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 818 is [True, False, False, False, True, False]
Current timestep = 819. State = [[-0.11440526  0.00434898]]. Action = [[-0.05281594 -0.24786083  0.07212096  0.62993026]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 819 is [True, False, False, False, True, False]
Current timestep = 820. State = [[-0.10730318 -0.00702864]]. Action = [[ 0.18385258 -0.02732636  0.11683476  0.8814974 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 820 is [True, False, False, False, True, False]
Current timestep = 821. State = [[-1.0043499e-01  2.8372851e-05]]. Action = [[-0.11001199  0.19540873 -0.1668411  -0.84983706]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 821 is [True, False, False, False, True, False]
Scene graph at timestep 821 is [True, False, False, False, True, False]
State prediction error at timestep 821 is tensor(4.5483e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of 1
Current timestep = 822. State = [[-0.10383576  0.00113814]]. Action = [[-0.06479384 -0.18343607  0.05378088 -0.86692834]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 822 is [True, False, False, False, True, False]
Current timestep = 823. State = [[-0.1094385  -0.01516492]]. Action = [[-0.1208556  -0.10716462 -0.10283804 -0.22016561]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 823 is [True, False, False, False, True, False]
Current timestep = 824. State = [[-0.12211182 -0.03510914]]. Action = [[-0.1303599  -0.16297983  0.22947547  0.0156126 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 824 is [True, False, False, False, True, False]
Current timestep = 825. State = [[-0.13849926 -0.03448221]]. Action = [[-0.15025471  0.22927454  0.10269687 -0.03543091]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 825 is [True, False, False, False, True, False]
Scene graph at timestep 825 is [True, False, False, False, True, False]
State prediction error at timestep 825 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[-0.14592278 -0.02071512]]. Action = [[ 0.21873784 -0.03256759 -0.15855683  0.6649685 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 826 is [True, False, False, False, True, False]
Scene graph at timestep 826 is [True, False, False, False, True, False]
State prediction error at timestep 826 is tensor(9.5501e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of 0
Current timestep = 827. State = [[-0.13374524 -0.03239062]]. Action = [[ 0.2160421  -0.22294597 -0.13279128  0.19555175]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 827 is [True, False, False, False, True, False]
Current timestep = 828. State = [[-0.11200304 -0.05737382]]. Action = [[ 0.1048882  -0.1260741   0.01548466  0.47218966]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 828 is [True, False, False, False, True, False]
Current timestep = 829. State = [[-0.09491166 -0.06315843]]. Action = [[ 0.12812781  0.11063713 -0.1011771  -0.9943972 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 829 is [True, False, False, False, True, False]
Current timestep = 830. State = [[-0.08350476 -0.05828932]]. Action = [[-0.02650824  0.00821018  0.21322548 -0.04120129]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 830 is [True, False, False, False, True, False]
Current timestep = 831. State = [[-0.08244421 -0.05590361]]. Action = [[0.01285136 0.02192304 0.14966139 0.22277784]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 831 is [True, False, False, False, True, False]
Scene graph at timestep 831 is [True, False, False, False, True, False]
State prediction error at timestep 831 is tensor(1.6190e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of 0
Current timestep = 832. State = [[-0.08606555 -0.06804828]]. Action = [[-0.18929319 -0.22117947  0.22487783 -0.59839976]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 832 is [True, False, False, False, True, False]
Current timestep = 833. State = [[-0.08938493 -0.08696761]]. Action = [[ 0.08557373 -0.10294661 -0.05227315  0.89633274]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 833 is [True, False, False, False, True, False]
Scene graph at timestep 833 is [True, False, False, False, True, False]
State prediction error at timestep 833 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 833 of -1
Current timestep = 834. State = [[-0.09531472 -0.10045657]]. Action = [[-0.20484257 -0.01787391 -0.10004391  0.471519  ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 834 is [True, False, False, False, True, False]
Current timestep = 835. State = [[-0.10163759 -0.10314538]]. Action = [[1.1776149e-02 3.2991171e-05 2.0788306e-01 1.3379335e-01]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 835 is [True, False, False, False, True, False]
Scene graph at timestep 835 is [True, False, False, False, True, False]
State prediction error at timestep 835 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of -1
Current timestep = 836. State = [[-0.10084493 -0.10246992]]. Action = [[0.11031938 0.04408032 0.21145266 0.8625388 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 836 is [True, False, False, False, True, False]
Current timestep = 837. State = [[-0.10234095 -0.10190655]]. Action = [[-0.1432207  -0.02579626 -0.21766384 -0.94632363]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 837 is [True, False, False, False, True, False]
Scene graph at timestep 837 is [True, False, False, False, True, False]
State prediction error at timestep 837 is tensor(2.4458e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 0
Current timestep = 838. State = [[-0.10330797 -0.09488372]]. Action = [[ 0.10190237  0.14283112  0.24189717 -0.37127852]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 838 is [True, False, False, False, True, False]
Scene graph at timestep 838 is [True, False, False, False, True, False]
State prediction error at timestep 838 is tensor(9.2036e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 0
Current timestep = 839. State = [[-0.09314387 -0.072228  ]]. Action = [[ 0.22807264  0.19728267  0.20910913 -0.6284918 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 839 is [True, False, False, False, True, False]
Current timestep = 840. State = [[-0.07479619 -0.0492785 ]]. Action = [[ 0.15657595  0.15637779  0.09336627 -0.08297938]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 840 is [True, False, False, False, True, False]
Scene graph at timestep 840 is [True, False, False, False, True, False]
State prediction error at timestep 840 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of 1
Current timestep = 841. State = [[-0.06021287 -0.02371213]]. Action = [[-0.16755132  0.15258741  0.04423502 -0.40569538]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 841 is [True, False, False, False, True, False]
Current timestep = 842. State = [[-0.06748914 -0.00334721]]. Action = [[-0.11407307  0.11762023 -0.00615142  0.13142836]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 842 is [True, False, False, False, True, False]
Current timestep = 843. State = [[-0.07505029  0.00727883]]. Action = [[0.00814214 0.00119406 0.14505541 0.5024538 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.07278854 -0.00020874]]. Action = [[ 0.13323015 -0.17747556 -0.20991066  0.21936142]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 844 is [True, False, False, False, True, False]
Current timestep = 845. State = [[-0.0720528 -0.0118896]]. Action = [[-0.09789644 -0.04109201 -0.08669266 -0.59855163]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 845 is [True, False, False, False, True, False]
Current timestep = 846. State = [[-0.08034027 -0.02594859]]. Action = [[-0.11652859 -0.12575193  0.07282159  0.79800034]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 846 is [True, False, False, False, True, False]
Scene graph at timestep 846 is [True, False, False, False, True, False]
State prediction error at timestep 846 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 0
Current timestep = 847. State = [[-0.0941391  -0.02657103]]. Action = [[-0.06189069  0.21013573  0.14921662  0.61056805]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 847 is [True, False, False, False, True, False]
Scene graph at timestep 847 is [True, False, False, False, True, False]
State prediction error at timestep 847 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of 0
Current timestep = 848. State = [[-0.09767758 -0.02280984]]. Action = [[ 0.03346881 -0.22793776 -0.04542795 -0.48712438]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.09334608 -0.04549744]]. Action = [[ 0.11238247 -0.17088047  0.01197979 -0.8916493 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 849 is [True, False, False, False, True, False]
Current timestep = 850. State = [[-0.09592726 -0.07450668]]. Action = [[-0.11789073 -0.21256578 -0.13707753 -0.47655082]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 850 is [True, False, False, False, True, False]
Current timestep = 851. State = [[-0.09214227 -0.08094732]]. Action = [[0.16881561 0.19321585 0.24117577 0.8568742 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(1.0867e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 0
Current timestep = 852. State = [[-0.09015413 -0.06306685]]. Action = [[-0.12547623  0.1765314   0.18387848 -0.61460036]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of 0
Current timestep = 853. State = [[-0.09901552 -0.05680056]]. Action = [[-0.1186637  -0.15713839 -0.1406127   0.06597662]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.10374697 -0.0671138 ]]. Action = [[ 0.08956796 -0.06504861 -0.17622167  0.6470264 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.10198583 -0.06568649]]. Action = [[ 0.05969772  0.13503265 -0.0855754   0.81834936]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.1013282  -0.06287038]]. Action = [[-0.00836426 -0.03319418 -0.10157101 -0.8765503 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.09425464 -0.05949087]]. Action = [[ 0.15685225  0.06514627 -0.21146977 -0.33355933]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 857 is [True, False, False, False, True, False]
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(8.4924e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of 1
Current timestep = 858. State = [[-0.08228103 -0.06068832]]. Action = [[-0.04802623 -0.13171001 -0.23075801 -0.35450888]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 858 is [True, False, False, False, True, False]
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of 0
Current timestep = 859. State = [[-0.07805321 -0.07667276]]. Action = [[ 0.1422298  -0.13353688  0.05993468  0.8142288 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.0668545 -0.099038 ]]. Action = [[ 0.18083587 -0.2224851  -0.01959985 -0.1746304 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.04163373 -0.11157154]]. Action = [[ 0.1095528   0.07831624 -0.03393379  0.09888422]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 861 is [True, False, False, False, True, False]
Scene graph at timestep 861 is [False, True, False, False, True, False]
State prediction error at timestep 861 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of 1
Current timestep = 862. State = [[-0.14688012 -0.13300721]]. Action = [[-0.17450024  0.23002094 -0.10978952 -0.6825355 ]]. Reward = [100.]
Curr episode timestep = 95
Scene graph at timestep 862 is [False, True, False, False, True, False]
Current timestep = 863. State = [[-0.12146937 -0.16376255]]. Action = [[ 0.18826789 -0.23503935 -0.16012168 -0.1179899 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 863 is [True, False, False, True, False, False]
Current timestep = 864. State = [[-0.10692045 -0.17342517]]. Action = [[-0.03413184  0.12904876  0.22351986 -0.2130655 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 864 is [True, False, False, True, False, False]
Scene graph at timestep 864 is [True, False, False, True, False, False]
State prediction error at timestep 864 is tensor(3.7736e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of -1
Current timestep = 865. State = [[-0.10524556 -0.16880259]]. Action = [[-0.08166525  0.04876834  0.0420669   0.0041424 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 865 is [True, False, False, True, False, False]
Current timestep = 866. State = [[-0.10570452 -0.16482314]]. Action = [[-0.05495933  0.04449132  0.08305949  0.22303009]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 866 is [True, False, False, True, False, False]
Scene graph at timestep 866 is [True, False, False, True, False, False]
State prediction error at timestep 866 is tensor(4.6266e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of 1
Current timestep = 867. State = [[-0.11514038 -0.17129675]]. Action = [[-0.1590484 -0.1623195  0.0284448  0.5943215]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 867 is [True, False, False, True, False, False]
Scene graph at timestep 867 is [True, False, False, True, False, False]
State prediction error at timestep 867 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of -1
Current timestep = 868. State = [[-0.12826428 -0.18153888]]. Action = [[-2.0070374e-04  4.3426812e-02 -2.1662210e-01 -1.0048121e-01]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 868 is [True, False, False, True, False, False]
Scene graph at timestep 868 is [True, False, False, True, False, False]
State prediction error at timestep 868 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of 0
Current timestep = 869. State = [[-0.12294345 -0.16372083]]. Action = [[ 0.20042253  0.23793918 -0.15060236 -0.67350405]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 869 is [True, False, False, True, False, False]
Current timestep = 870. State = [[-0.10968698 -0.13236798]]. Action = [[0.22008231 0.19670713 0.21492079 0.98884153]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 870 is [True, False, False, True, False, False]
Current timestep = 871. State = [[-0.09366053 -0.10299681]]. Action = [[-0.00912388  0.20842505 -0.0753644   0.33680415]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 871 is [True, False, False, True, False, False]
Current timestep = 872. State = [[-0.09014994 -0.07847159]]. Action = [[ 0.10099339  0.11820632 -0.17257844  0.691854  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 872 is [True, False, False, False, True, False]
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.07625394 -0.07328653]]. Action = [[ 0.02991471 -0.15952438  0.18166831 -0.8261819 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 0
Current timestep = 874. State = [[-0.07438599 -0.06836695]]. Action = [[ 0.01125711  0.24412411 -0.16817574  0.66472244]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.06612585 -0.0619464 ]]. Action = [[ 0.14909351 -0.12764518  0.11972886 -0.88006586]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.05566999 -0.07672776]]. Action = [[-0.09313756 -0.17338769 -0.22406955 -0.87207943]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.04929025 -0.10029443]]. Action = [[ 0.2191686  -0.16927092  0.22000107  0.6106738 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.03614849 -0.12691355]]. Action = [[-0.04263982 -0.23383637  0.12753147  0.6173403 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 878 is [False, True, False, False, True, False]
Scene graph at timestep 878 is [False, True, False, True, False, False]
State prediction error at timestep 878 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of -1
Current timestep = 879. State = [[-0.03792007 -0.15029901]]. Action = [[-0.21545069  0.00539428  0.2273038  -0.75413203]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 879 is [False, True, False, True, False, False]
Scene graph at timestep 879 is [False, True, False, True, False, False]
State prediction error at timestep 879 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of -1
Current timestep = 880. State = [[-0.04289213 -0.1561708 ]]. Action = [[-0.00999956 -0.02257209 -0.13664502  0.9763591 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 880 is [False, True, False, True, False, False]
Scene graph at timestep 880 is [False, True, False, True, False, False]
State prediction error at timestep 880 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of -1
Current timestep = 881. State = [[-0.04363248 -0.15612398]]. Action = [[ 0.11736029  0.02016443  0.06813449 -0.62143034]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 881 is [False, True, False, True, False, False]
Scene graph at timestep 881 is [False, True, False, True, False, False]
State prediction error at timestep 881 is tensor(4.4326e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of 0
Current timestep = 882. State = [[-0.04307916 -0.14608632]]. Action = [[-0.01273178  0.14580372 -0.03825644  0.28306723]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 882 is [False, True, False, True, False, False]
Scene graph at timestep 882 is [False, True, False, True, False, False]
State prediction error at timestep 882 is tensor(6.1527e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 1
Current timestep = 883. State = [[-0.03746416 -0.11962868]]. Action = [[ 0.16423449  0.23027903 -0.20476548  0.62557757]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 883 is [False, True, False, True, False, False]
Current timestep = 884. State = [[-0.2585014   0.07600192]]. Action = [[-0.0173967   0.12798434  0.24240798 -0.77582854]]. Reward = [100.]
Curr episode timestep = 21
Scene graph at timestep 884 is [False, True, False, False, True, False]
Current timestep = 885. State = [[-0.25200438  0.08745287]]. Action = [[0.08976239 0.0668028  0.00411853 0.53540325]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.23595127  0.08181594]]. Action = [[ 0.13030207 -0.2126912  -0.1426126   0.9562429 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 886 is [True, False, False, False, True, False]
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.21864064  0.06776302]]. Action = [[ 0.13699806 -0.02224779 -0.12481081  0.51003623]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 887 is [True, False, False, False, True, False]
Scene graph at timestep 887 is [True, False, False, False, True, False]
State prediction error at timestep 887 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of 1
Current timestep = 888. State = [[-0.20370929  0.05398993]]. Action = [[ 0.04625654 -0.19471046 -0.17917933  0.8458772 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 888 is [True, False, False, False, True, False]
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of 1
Current timestep = 889. State = [[-0.1872126   0.02560573]]. Action = [[ 0.23961908 -0.23084041  0.20815599  0.46629298]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.1664241  0.0125287]]. Action = [[ 0.00758019  0.04943368  0.11889625 -0.33118063]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 890 is [True, False, False, False, True, False]
Current timestep = 891. State = [[-0.16591047  0.0211931 ]]. Action = [[-0.15875882  0.15854281  0.22195429 -0.81188846]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 891 is [True, False, False, False, True, False]
Scene graph at timestep 891 is [True, False, False, False, True, False]
State prediction error at timestep 891 is tensor(1.7128e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 891 of 1
Current timestep = 892. State = [[-0.17482463  0.02583218]]. Action = [[-0.14304353 -0.11985341 -0.15363286  0.44999647]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 892 is [True, False, False, False, True, False]
Scene graph at timestep 892 is [True, False, False, False, True, False]
State prediction error at timestep 892 is tensor(5.3948e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of -1
Current timestep = 893. State = [[-0.18217833  0.00603171]]. Action = [[ 0.04193681 -0.17609411  0.17902559 -0.12497103]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 893 is [True, False, False, False, True, False]
Current timestep = 894. State = [[-0.17953046  0.00148119]]. Action = [[ 0.15174872  0.12342924 -0.03758632 -0.82119143]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 894 is [True, False, False, False, True, False]
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(2.2176e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 1
Current timestep = 895. State = [[-0.16309497 -0.00278084]]. Action = [[ 0.22953758 -0.17072411 -0.21501622  0.62165284]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 895 is [True, False, False, False, True, False]
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of 1
Current timestep = 896. State = [[-0.14572011 -0.01876692]]. Action = [[-0.16772063 -0.07982859  0.08353451  0.38820255]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 896 is [True, False, False, False, True, False]
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of -1
Current timestep = 897. State = [[-0.15189509 -0.02435414]]. Action = [[-0.08919716  0.04043588  0.14362246  0.42745936]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 897 is [True, False, False, False, True, False]
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of -1
Current timestep = 898. State = [[-0.16609599 -0.01387172]]. Action = [[-0.15846713  0.1528616   0.18627292  0.5211222 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.18813625  0.01035882]]. Action = [[-0.22382261  0.22519213  0.04440224 -0.28396636]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 899 is [True, False, False, False, True, False]
Current timestep = 900. State = [[-0.20967571  0.04078372]]. Action = [[ 0.05718601  0.19364798 -0.17719665 -0.90975153]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.21668789  0.05012741]]. Action = [[-0.13185038 -0.11782894  0.18115503  0.26392627]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 901 is [True, False, False, False, True, False]
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(2.8699e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of -1
Current timestep = 902. State = [[-0.22601958  0.0361791 ]]. Action = [[-0.09872586 -0.16473246 -0.1411176   0.889001  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 902 is [True, False, False, False, True, False]
Current timestep = 903. State = [[-0.2285176  0.0355747]]. Action = [[ 0.22598714  0.20858118 -0.04587246  0.17931986]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.2137242   0.04402417]]. Action = [[ 0.19341958 -0.02253792  0.13962293 -0.7977461 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 904 is [True, False, False, False, True, False]
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of 0
Current timestep = 905. State = [[-0.19740024  0.03526822]]. Action = [[-0.02715899 -0.20542254  0.13999537 -0.49433637]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.19343875  0.00933288]]. Action = [[ 0.02578288 -0.2270991  -0.08573416 -0.7722813 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.19492532  0.0006533 ]]. Action = [[-0.14598767  0.16810912 -0.20282198 -0.3196919 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.20235978  0.01919606]]. Action = [[0.01563245 0.23039976 0.1365655  0.01649749]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 908 is [True, False, False, False, True, False]
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 908 of 0
Current timestep = 909. State = [[-0.20933886  0.02687155]]. Action = [[-0.05319777 -0.222822    0.17392701 -0.84867173]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.20466006  0.00383254]]. Action = [[ 0.18850201 -0.1814928   0.1256533   0.7662535 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 910 is [True, False, False, False, True, False]
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of 1
Current timestep = 911. State = [[-0.1961386  -0.01925314]]. Action = [[-0.07519069 -0.08466032 -0.22351797 -0.13735914]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.20504206 -0.01312551]]. Action = [[-0.21008262  0.22504866  0.09592581 -0.80841184]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 912 is [True, False, False, False, True, False]
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(3.0289e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of -1
Current timestep = 913. State = [[-0.21554974 -0.00703548]]. Action = [[ 0.17413431 -0.1492469  -0.2072649  -0.34168494]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.20233613 -0.00545206]]. Action = [[0.16788572 0.1786204  0.20992357 0.71735597]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 914 is [True, False, False, False, True, False]
Current timestep = 915. State = [[-0.19678375  0.00756988]]. Action = [[-0.17573969  0.08468592 -0.2279495  -0.96340466]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 915 is [True, False, False, False, True, False]
Scene graph at timestep 915 is [True, False, False, False, True, False]
State prediction error at timestep 915 is tensor(1.3923e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 915 of 0
Current timestep = 916. State = [[-0.20423812  0.0274037 ]]. Action = [[-0.09019428  0.18024048 -0.101171    0.07192492]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.20398426  0.05181715]]. Action = [[ 0.23181614  0.18999237 -0.15426339  0.7459432 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.18371709  0.07003362]]. Action = [[ 0.23887807  0.07128546 -0.0376749   0.9064944 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.16680227  0.07249296]]. Action = [[-0.03786437 -0.132069   -0.02624547  0.8563969 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.1631657   0.05470303]]. Action = [[ 0.00868687 -0.22452858  0.0871219   0.6185863 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.1633185  0.0415191]]. Action = [[-0.12927236  0.03482369 -0.12912592 -0.89736646]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.16328385  0.02736064]]. Action = [[ 0.0903855  -0.191461   -0.23433648 -0.97032136]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 922 is [True, False, False, False, True, False]
Current timestep = 923. State = [[-0.16816722  0.01726162]]. Action = [[-0.21087767  0.02751505  0.16349739 -0.27267873]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.18703893  0.01606076]]. Action = [[-0.16608168  0.02939495  0.05300087 -0.64239866]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.20658226  0.02724201]]. Action = [[-0.14230154  0.14107281 -0.23120786  0.8550414 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 925 is [True, False, False, False, True, False]
Scene graph at timestep 925 is [True, False, False, False, True, False]
State prediction error at timestep 925 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 925 of -1
Current timestep = 926. State = [[-0.21887343  0.02588898]]. Action = [[ 0.19688159 -0.21008734 -0.04125766 -0.81880444]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 926 is [True, False, False, False, True, False]
Scene graph at timestep 926 is [True, False, False, False, True, False]
State prediction error at timestep 926 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.21601929  0.0134208 ]]. Action = [[-0.18767524  0.05902767  0.2008605   0.43691766]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 927 is [True, False, False, False, True, False]
Current timestep = 928. State = [[-0.22508864  0.00470718]]. Action = [[-0.04203151 -0.17605568 -0.12190561 -0.05414844]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.23111007 -0.00735871]]. Action = [[ 0.01698178 -0.03527158 -0.06351301  0.8621005 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [True, False, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of -1
Current timestep = 930. State = [[-0.23154512 -0.02131489]]. Action = [[ 0.04676694 -0.13494043  0.17815429 -0.8049235 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 930 is [True, False, False, False, True, False]
Scene graph at timestep 930 is [True, False, False, False, True, False]
State prediction error at timestep 930 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 930 of -1
Current timestep = 931. State = [[-0.2251873  -0.03412113]]. Action = [[ 0.12621963 -0.02995394 -0.09581144  0.17296803]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 931 is [True, False, False, False, True, False]
Current timestep = 932. State = [[-0.21573378 -0.0364791 ]]. Action = [[ 0.1307216   0.03205934 -0.06839782 -0.60880333]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 932 is [True, False, False, False, True, False]
Scene graph at timestep 932 is [True, False, False, False, True, False]
State prediction error at timestep 932 is tensor(3.0429e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[-0.2054562  -0.02291158]]. Action = [[-0.04956844  0.23270798  0.1465759   0.15184486]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 933 is [True, False, False, False, True, False]
Scene graph at timestep 933 is [True, False, False, False, True, False]
State prediction error at timestep 933 is tensor(9.8521e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 0
Current timestep = 934. State = [[-0.20942701  0.00364802]]. Action = [[-0.05949241  0.16947126 -0.20949717 -0.21738386]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 934 is [True, False, False, False, True, False]
Current timestep = 935. State = [[-0.21061604  0.00636067]]. Action = [[ 0.00867724 -0.18776073 -0.11064069  0.48255658]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.20220058 -0.00590146]]. Action = [[ 0.21787965 -0.09231244  0.19567847 -0.3164885 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.18205684 -0.01825709]]. Action = [[ 0.1922152  -0.05948055  0.18251643  0.07753992]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 937 is [True, False, False, False, True, False]
Current timestep = 938. State = [[-0.15451692 -0.02736186]]. Action = [[ 0.23223472 -0.06820697  0.08527979 -0.06881958]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 938 is [True, False, False, False, True, False]
Current timestep = 939. State = [[-0.12600446 -0.02550934]]. Action = [[ 0.13615537  0.120774   -0.16704476  0.1950326 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 939 is [True, False, False, False, True, False]
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.10165832 -0.02743061]]. Action = [[ 0.14796191 -0.17478997  0.10557103 -0.31003034]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.08407658 -0.0492985 ]]. Action = [[ 0.12021273 -0.18830131 -0.01051673  0.75832117]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 941 is [True, False, False, False, True, False]
Scene graph at timestep 941 is [True, False, False, False, True, False]
State prediction error at timestep 941 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of 1
Current timestep = 942. State = [[-0.06958389 -0.07322129]]. Action = [[-0.04629397 -0.09440848 -0.21117108 -0.72939646]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 942 is [True, False, False, False, True, False]
Current timestep = 943. State = [[-0.07591713 -0.08296856]]. Action = [[-0.2241544  -0.01810008 -0.15149717 -0.42094207]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.08412379 -0.07935617]]. Action = [[-0.02996904  0.15553138  0.11666313  0.44894576]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 944 is [True, False, False, False, True, False]
Scene graph at timestep 944 is [True, False, False, False, True, False]
State prediction error at timestep 944 is tensor(9.5679e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of 0
Current timestep = 945. State = [[-0.08997466 -0.06234887]]. Action = [[ 0.00450036  0.07493466 -0.07576218  0.03002977]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 945 is [True, False, False, False, True, False]
Scene graph at timestep 945 is [True, False, False, False, True, False]
State prediction error at timestep 945 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 945 of 0
Current timestep = 946. State = [[-0.09593148 -0.06694987]]. Action = [[-0.10041153 -0.17647341  0.11170948  0.8778528 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.11509787 -0.08430829]]. Action = [[-0.23472793 -0.08039784  0.21828571 -0.40677536]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.1455182  -0.10334499]]. Action = [[-0.16154397 -0.13974752  0.10304156 -0.03212541]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.15292394 -0.11995099]]. Action = [[ 0.16676989 -0.13074996  0.23444092 -0.8244444 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 949 is [True, False, False, False, True, False]
Current timestep = 950. State = [[-0.15072109 -0.13919151]]. Action = [[-0.02694163 -0.16348392 -0.17463732 -0.06619477]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 950 is [True, False, False, False, True, False]
Current timestep = 951. State = [[-0.15731642 -0.14813699]]. Action = [[-0.17903887  0.12234575 -0.06358685 -0.03010374]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 951 is [True, False, False, True, False, False]
Current timestep = 952. State = [[-0.17147788 -0.14411707]]. Action = [[-0.1357901   0.03327531  0.22756743  0.6676624 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 952 is [True, False, False, True, False, False]
Current timestep = 953. State = [[-0.18950671 -0.14471415]]. Action = [[-0.14910476 -0.03136255 -0.143599    0.42435217]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 953 is [True, False, False, True, False, False]
Current timestep = 954. State = [[-0.19931692 -0.15058765]]. Action = [[ 0.0830878  -0.11059764 -0.13244714  0.61000097]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 954 is [True, False, False, True, False, False]
Scene graph at timestep 954 is [True, False, False, True, False, False]
State prediction error at timestep 954 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of -1
Current timestep = 955. State = [[-0.19598454 -0.1562665 ]]. Action = [[ 0.1491496   0.02366355 -0.08941942  0.39953744]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 955 is [True, False, False, True, False, False]
Current timestep = 956. State = [[-0.19748688 -0.1475408 ]]. Action = [[-0.1991479   0.15226626  0.09907675  0.8689523 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 956 is [True, False, False, True, False, False]
Scene graph at timestep 956 is [True, False, False, True, False, False]
State prediction error at timestep 956 is tensor(2.1449e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.21191983 -0.12338513]]. Action = [[-0.14099942  0.23334026  0.01473379  0.95479774]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 957 is [True, False, False, True, False, False]
Current timestep = 958. State = [[-0.21675749 -0.10710257]]. Action = [[ 0.18658972 -0.02650908 -0.02321675 -0.28131753]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 958 is [True, False, False, False, True, False]
Scene graph at timestep 958 is [True, False, False, False, True, False]
State prediction error at timestep 958 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 958 of -1
Current timestep = 959. State = [[-0.20447676 -0.10680653]]. Action = [[ 0.19298056 -0.06520008 -0.03069972  0.62373734]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 959 is [True, False, False, False, True, False]
Current timestep = 960. State = [[-0.19072062 -0.11803069]]. Action = [[ 0.10373688 -0.19369255  0.04695225 -0.3726784 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 960 is [True, False, False, False, True, False]
Current timestep = 961. State = [[-0.1795268  -0.12129245]]. Action = [[-0.04989435  0.2270577  -0.12861615  0.06570315]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 961 is [True, False, False, False, True, False]
Current timestep = 962. State = [[-0.17097522 -0.09889443]]. Action = [[ 0.14319873  0.20018461 -0.14469863 -0.6260663 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 962 is [True, False, False, False, True, False]
Current timestep = 963. State = [[-0.15186556 -0.08104084]]. Action = [[ 0.21884245  0.008706    0.20531937 -0.9262994 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 963 is [True, False, False, False, True, False]
Current timestep = 964. State = [[-0.1414579  -0.08675574]]. Action = [[-0.17437857 -0.1995728  -0.07126829  0.49747443]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 964 is [True, False, False, False, True, False]
Scene graph at timestep 964 is [True, False, False, False, True, False]
State prediction error at timestep 964 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 1
Current timestep = 965. State = [[-0.14206871 -0.0932576 ]]. Action = [[ 0.13517791  0.1198374  -0.19094153  0.64258313]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 965 is [True, False, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(4.3830e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of 1
Current timestep = 966. State = [[-0.12948261 -0.07342945]]. Action = [[ 0.07272285  0.24391103  0.01590145 -0.39982837]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 966 is [True, False, False, False, True, False]
Scene graph at timestep 966 is [True, False, False, False, True, False]
State prediction error at timestep 966 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.12053084 -0.06126241]]. Action = [[ 0.10177392 -0.15388633 -0.18881339  0.08463502]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 967 is [True, False, False, False, True, False]
Current timestep = 968. State = [[-0.10729299 -0.06535267]]. Action = [[ 0.15406829  0.02866432 -0.20515943  0.52434134]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 968 is [True, False, False, False, True, False]
Current timestep = 969. State = [[-0.09438604 -0.07532521]]. Action = [[ 0.00209123 -0.18284065 -0.01058741 -0.78040934]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 969 is [True, False, False, False, True, False]
Current timestep = 970. State = [[-0.08878317 -0.09183837]]. Action = [[ 0.0402565  -0.05594325  0.10541269 -0.16541219]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 970 is [True, False, False, False, True, False]
Scene graph at timestep 970 is [True, False, False, False, True, False]
State prediction error at timestep 970 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.07885653 -0.08947548]]. Action = [[ 0.13580132  0.15605903  0.17135721 -0.35694265]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 971 is [True, False, False, False, True, False]
Scene graph at timestep 971 is [True, False, False, False, True, False]
State prediction error at timestep 971 is tensor(2.8714e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.05953614 -0.07504263]]. Action = [[ 0.06520417  0.05524799 -0.22503333  0.16147733]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 972 is [True, False, False, False, True, False]
Current timestep = 973. State = [[-0.05432025 -0.08197522]]. Action = [[ 0.06251848 -0.18793356  0.0785026  -0.43943107]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 973 is [True, False, False, False, True, False]
Current timestep = 974. State = [[-0.04841801 -0.08280862]]. Action = [[-0.06046221  0.2018573  -0.16182931  0.9476259 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 974 is [True, False, False, False, True, False]
Scene graph at timestep 974 is [False, True, False, False, True, False]
State prediction error at timestep 974 is tensor(7.0618e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 974 of 1
Current timestep = 975. State = [[-0.05526902 -0.06198159]]. Action = [[-0.23397516  0.19434077  0.10773623  0.74521816]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 975 is [False, True, False, False, True, False]
Scene graph at timestep 975 is [True, False, False, False, True, False]
State prediction error at timestep 975 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[-0.07457782 -0.04984154]]. Action = [[-0.19967265 -0.07576941 -0.16763853  0.27632332]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 976 is [True, False, False, False, True, False]
Scene graph at timestep 976 is [True, False, False, False, True, False]
State prediction error at timestep 976 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of -1
Current timestep = 977. State = [[-0.09429308 -0.04119425]]. Action = [[ 0.05119225  0.23111987 -0.07160476 -0.84027046]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 977 is [True, False, False, False, True, False]
Current timestep = 978. State = [[-0.08808103 -0.01886502]]. Action = [[ 0.19139105  0.07772964 -0.08114547 -0.5483384 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 978 is [True, False, False, False, True, False]
Scene graph at timestep 978 is [True, False, False, False, True, False]
State prediction error at timestep 978 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of 0
Current timestep = 979. State = [[-0.07908738 -0.01930928]]. Action = [[-0.06475738 -0.21396706 -0.07355723 -0.1991666 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 979 is [True, False, False, False, True, False]
Scene graph at timestep 979 is [True, False, False, False, True, False]
State prediction error at timestep 979 is tensor(8.2399e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 979 of 0
Current timestep = 980. State = [[-0.07833737 -0.04648532]]. Action = [[ 0.09564781 -0.21818294  0.24066684  0.9280453 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 980 is [True, False, False, False, True, False]
Scene graph at timestep 980 is [True, False, False, False, True, False]
State prediction error at timestep 980 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 0
Current timestep = 981. State = [[-0.07719477 -0.0600723 ]]. Action = [[-0.15838015  0.12629658 -0.23638955 -0.74530476]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 981 is [True, False, False, False, True, False]
Current timestep = 982. State = [[-0.08375984 -0.0465744 ]]. Action = [[-0.16460325  0.14740688 -0.21452053 -0.70751685]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 982 is [True, False, False, False, True, False]
Scene graph at timestep 982 is [True, False, False, False, True, False]
State prediction error at timestep 982 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of -1
Current timestep = 983. State = [[-0.10526686 -0.04160864]]. Action = [[ 0.00735074 -0.184972    0.0839029  -0.8069386 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 983 is [True, False, False, False, True, False]
Scene graph at timestep 983 is [True, False, False, False, True, False]
State prediction error at timestep 983 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 983 of -1
Current timestep = 984. State = [[-0.11287865 -0.06689169]]. Action = [[ 0.00379282 -0.22936258 -0.15986964  0.66463137]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 984 is [True, False, False, False, True, False]
Current timestep = 985. State = [[-0.10298519 -0.07797316]]. Action = [[ 0.19584006  0.07293081  0.0112763  -0.7165311 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 985 is [True, False, False, False, True, False]
Current timestep = 986. State = [[-0.08998604 -0.06546912]]. Action = [[ 0.03673208  0.2215839   0.24555403 -0.38978666]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 986 is [True, False, False, False, True, False]
Current timestep = 987. State = [[-0.07958606 -0.03939972]]. Action = [[ 0.07006902  0.21904719  0.1995872  -0.0258646 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 987 is [True, False, False, False, True, False]
Scene graph at timestep 987 is [True, False, False, False, True, False]
State prediction error at timestep 987 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 0
Current timestep = 988. State = [[-0.06883724 -0.02919424]]. Action = [[ 0.20224398 -0.21386528  0.03306043  0.20129442]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 988 is [True, False, False, False, True, False]
Current timestep = 989. State = [[-0.05534011 -0.04565541]]. Action = [[-0.09350304 -0.06517701 -0.12441343  0.31987584]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [True, False, False, False, True, False]
State prediction error at timestep 989 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.05823632 -0.05134497]]. Action = [[-0.12042108  0.03772539  0.15435994 -0.49199355]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 990 is [True, False, False, False, True, False]
Scene graph at timestep 990 is [True, False, False, False, True, False]
State prediction error at timestep 990 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 990 of -1
Current timestep = 991. State = [[-0.06720491 -0.05621943]]. Action = [[-0.14584246 -0.0696207  -0.04066399  0.6630039 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 991 is [True, False, False, False, True, False]
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of -1
Current timestep = 992. State = [[-0.08676902 -0.05069255]]. Action = [[-0.21953791  0.21062651  0.07318711  0.6164682 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 992 is [True, False, False, False, True, False]
Scene graph at timestep 992 is [True, False, False, False, True, False]
State prediction error at timestep 992 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 992 of 0
Current timestep = 993. State = [[-0.10595678 -0.03422768]]. Action = [[ 0.19045049 -0.08140886  0.14854681  0.76889944]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 993 is [True, False, False, False, True, False]
Scene graph at timestep 993 is [True, False, False, False, True, False]
State prediction error at timestep 993 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 1
Current timestep = 994. State = [[-0.0979257  -0.02926169]]. Action = [[0.08151376 0.1546512  0.13778049 0.49463248]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 994 is [True, False, False, False, True, False]
Current timestep = 995. State = [[-0.08937051 -0.00847952]]. Action = [[-0.03719281  0.24461305  0.23634744 -0.8420772 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 995 is [True, False, False, False, True, False]
Scene graph at timestep 995 is [True, False, False, False, True, False]
State prediction error at timestep 995 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of 1
Current timestep = 996. State = [[-0.09695768  0.02301823]]. Action = [[-0.17504936  0.16135031  0.06709486 -0.18321735]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 996 is [True, False, False, False, True, False]
Scene graph at timestep 996 is [True, False, False, False, True, False]
State prediction error at timestep 996 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of -1
Current timestep = 997. State = [[-0.10360578  0.04295358]]. Action = [[ 0.08493251  0.06934956  0.10504234 -0.6560484 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 997 is [True, False, False, False, True, False]
Scene graph at timestep 997 is [True, False, False, False, True, False]
State prediction error at timestep 997 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 997 of -1
Current timestep = 998. State = [[-0.09663195  0.03577242]]. Action = [[ 0.2327506  -0.23752183  0.05844933  0.54733586]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 998 is [True, False, False, False, True, False]
Current timestep = 999. State = [[-0.07597002  0.0138182 ]]. Action = [[ 0.14357749 -0.11422312 -0.11538643 -0.43347788]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 999 is [True, False, False, False, True, False]
Current timestep = 1000. State = [[-0.05291538  0.01177096]]. Action = [[ 0.15161753  0.10832247  0.01377657 -0.5011544 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1000 is [True, False, False, False, True, False]
Scene graph at timestep 1000 is [True, False, False, False, True, False]
State prediction error at timestep 1000 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.26442423  0.06125813]]. Action = [[ 0.20582962 -0.20012888  0.07980627  0.2888224 ]]. Reward = [100.]
Curr episode timestep = 116
Scene graph at timestep 1001 is [True, False, False, False, True, False]
Current timestep = 1002. State = [[-0.25974995  0.08046039]]. Action = [[0.07812956 0.18568912 0.20192382 0.30661762]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1002 is [True, False, False, False, True, False]
Current timestep = 1003. State = [[-0.24979615  0.10131813]]. Action = [[ 0.10866225  0.13503739 -0.11619547  0.13151312]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1003 is [True, False, False, False, True, False]
Current timestep = 1004. State = [[-0.23038451  0.11588502]]. Action = [[ 0.18943903  0.0640057  -0.00300974  0.8028506 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1004 is [True, False, False, False, True, False]
Scene graph at timestep 1004 is [True, False, False, False, True, False]
State prediction error at timestep 1004 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of 0
Current timestep = 1005. State = [[-0.20096922  0.11138386]]. Action = [[ 0.15245697 -0.23260191  0.17583764 -0.41145635]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1005 is [True, False, False, False, True, False]
Scene graph at timestep 1005 is [True, False, False, False, True, False]
State prediction error at timestep 1005 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of 1
Current timestep = 1006. State = [[-0.1767943   0.08151694]]. Action = [[ 0.23920909 -0.22200051  0.07123578  0.69063246]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1006 is [True, False, False, False, True, False]
Scene graph at timestep 1006 is [True, False, False, False, True, False]
State prediction error at timestep 1006 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.14125556  0.05172199]]. Action = [[ 0.20875135 -0.21344803 -0.13717224 -0.04342222]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1007 is [True, False, False, False, True, False]
Scene graph at timestep 1007 is [True, False, False, False, True, False]
State prediction error at timestep 1007 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.11878639  0.03600508]]. Action = [[0.0889942  0.03953853 0.2147196  0.6011841 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1008 is [True, False, False, False, True, False]
Current timestep = 1009. State = [[-0.10572862  0.02362007]]. Action = [[ 0.07177737 -0.2373101  -0.05055252  0.55646837]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1009 is [True, False, False, False, True, False]
Scene graph at timestep 1009 is [True, False, False, False, True, False]
State prediction error at timestep 1009 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1009 of 1
Current timestep = 1010. State = [[-0.09119815  0.00556108]]. Action = [[ 0.14637566 -0.02668753  0.2208268  -0.04658949]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1010 is [True, False, False, False, True, False]
Current timestep = 1011. State = [[-0.08045518  0.00414108]]. Action = [[-0.06201595  0.00876719 -0.23350361  0.48292255]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1011 is [True, False, False, False, True, False]
Current timestep = 1012. State = [[-0.0748822   0.00438005]]. Action = [[ 0.16411948  0.01412031  0.13886148 -0.42183375]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1012 is [True, False, False, False, True, False]
Current timestep = 1013. State = [[-0.05442188  0.003495  ]]. Action = [[ 0.19909453 -0.02339754  0.17540881  0.3765608 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1013 is [True, False, False, False, True, False]
Scene graph at timestep 1013 is [True, False, False, False, True, False]
State prediction error at timestep 1013 is tensor(9.5182e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of 1
Current timestep = 1014. State = [[-0.25153768 -0.21836585]]. Action = [[-0.02624902 -0.04683788  0.10277709  0.8317599 ]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 1014 is [True, False, False, False, True, False]
Current timestep = 1015. State = [[-0.24815768 -0.24831651]]. Action = [[-0.08258179 -0.0413041   0.18766612 -0.7106951 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1015 is [True, False, False, True, False, False]
Scene graph at timestep 1015 is [True, False, False, True, False, False]
State prediction error at timestep 1015 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of 0
Current timestep = 1016. State = [[-0.25081697 -0.25061837]]. Action = [[ 0.09073132  0.06883061  0.03105149 -0.24009776]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1016 is [True, False, False, True, False, False]
Scene graph at timestep 1016 is [True, False, False, True, False, False]
State prediction error at timestep 1016 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[-0.23963407 -0.23111013]]. Action = [[ 0.1538581   0.24728298 -0.07077724 -0.61434436]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1017 is [True, False, False, True, False, False]
Current timestep = 1018. State = [[-0.22335675 -0.1994015 ]]. Action = [[0.1264959  0.22768635 0.21186194 0.95422995]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1018 is [True, False, False, True, False, False]
Scene graph at timestep 1018 is [True, False, False, True, False, False]
State prediction error at timestep 1018 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of 1
Current timestep = 1019. State = [[-0.20635702 -0.1732889 ]]. Action = [[ 0.1009497   0.07684827 -0.23711061 -0.35714018]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1019 is [True, False, False, True, False, False]
Current timestep = 1020. State = [[-0.1976191  -0.15338545]]. Action = [[-0.04522453  0.23653555 -0.2177387  -0.83618766]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1020 is [True, False, False, True, False, False]
Scene graph at timestep 1020 is [True, False, False, True, False, False]
State prediction error at timestep 1020 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of 1
Current timestep = 1021. State = [[-0.19792888 -0.12765272]]. Action = [[ 0.00596777  0.0831205  -0.02984652  0.16780281]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1021 is [True, False, False, True, False, False]
Scene graph at timestep 1021 is [True, False, False, True, False, False]
State prediction error at timestep 1021 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of 1
Current timestep = 1022. State = [[-0.1973292 -0.1091772]]. Action = [[ 0.01433834  0.17733419 -0.1813364  -0.2705984 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1022 is [True, False, False, True, False, False]
Current timestep = 1023. State = [[-0.2018493  -0.08514227]]. Action = [[-0.17204894  0.18473634  0.00968105  0.84526086]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1023 is [True, False, False, False, True, False]
Scene graph at timestep 1023 is [True, False, False, False, True, False]
State prediction error at timestep 1023 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of 1
Current timestep = 1024. State = [[-0.21791075 -0.05469232]]. Action = [[-0.1981725   0.18547773 -0.2116369   0.47377574]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1024 is [True, False, False, False, True, False]
Scene graph at timestep 1024 is [True, False, False, False, True, False]
State prediction error at timestep 1024 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of 0
Current timestep = 1025. State = [[-0.23397714 -0.0328184 ]]. Action = [[ 0.12106714  0.09796533  0.12173444 -0.6894341 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1025 is [True, False, False, False, True, False]
Current timestep = 1026. State = [[-0.23577799 -0.01596583]]. Action = [[-0.14356121  0.16099584  0.0754914  -0.25664794]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1026 is [True, False, False, False, True, False]
Scene graph at timestep 1026 is [True, False, False, False, True, False]
State prediction error at timestep 1026 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[-0.24241786  0.00277786]]. Action = [[-0.00435823  0.0456855   0.19289097 -0.13654524]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1027 is [True, False, False, False, True, False]
Current timestep = 1028. State = [[-0.24241851  0.00041981]]. Action = [[ 0.04408205 -0.11245802 -0.17239746  0.9331157 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1028 is [True, False, False, False, True, False]
Scene graph at timestep 1028 is [True, False, False, False, True, False]
State prediction error at timestep 1028 is tensor(6.3457e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of -1
Current timestep = 1029. State = [[-0.24192551 -0.00167077]]. Action = [[ 0.02786148  0.12135237 -0.1317534   0.8040737 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1029 is [True, False, False, False, True, False]
Scene graph at timestep 1029 is [True, False, False, False, True, False]
State prediction error at timestep 1029 is tensor(4.5530e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[-0.23360416  0.00697675]]. Action = [[ 0.1832993   0.02035993 -0.01889548 -0.39289653]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1030 is [True, False, False, False, True, False]
Scene graph at timestep 1030 is [True, False, False, False, True, False]
State prediction error at timestep 1030 is tensor(3.4936e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[-0.21615309  0.00023845]]. Action = [[ 0.10940608 -0.18120775 -0.13549279 -0.9174023 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1031 is [True, False, False, False, True, False]
Scene graph at timestep 1031 is [True, False, False, False, True, False]
State prediction error at timestep 1031 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of 1
Current timestep = 1032. State = [[-0.19191563 -0.019196  ]]. Action = [[ 0.22013229 -0.16121036 -0.05688608 -0.69217217]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1032 is [True, False, False, False, True, False]
Current timestep = 1033. State = [[-0.16615291 -0.02321678]]. Action = [[0.20474237 0.11480597 0.04799858 0.5834534 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1033 is [True, False, False, False, True, False]
Scene graph at timestep 1033 is [True, False, False, False, True, False]
State prediction error at timestep 1033 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of 1
Current timestep = 1034. State = [[-0.14377399 -0.0200818 ]]. Action = [[-0.0334323  -0.04619129  0.1006124  -0.02782184]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1034 is [True, False, False, False, True, False]
Scene graph at timestep 1034 is [True, False, False, False, True, False]
State prediction error at timestep 1034 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of 0
Current timestep = 1035. State = [[-0.14482324 -0.01851431]]. Action = [[-0.05780485  0.08100092  0.133854    0.87785256]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1035 is [True, False, False, False, True, False]
Current timestep = 1036. State = [[-0.14372678 -0.02628192]]. Action = [[ 0.05851126 -0.17721239 -0.11160064  0.5941446 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1036 is [True, False, False, False, True, False]
Scene graph at timestep 1036 is [True, False, False, False, True, False]
State prediction error at timestep 1036 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 0
Current timestep = 1037. State = [[-0.13501896 -0.03402784]]. Action = [[ 0.21547458  0.06501526 -0.05745777  0.7824813 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1037 is [True, False, False, False, True, False]
Scene graph at timestep 1037 is [True, False, False, False, True, False]
State prediction error at timestep 1037 is tensor(7.4249e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of 1
Current timestep = 1038. State = [[-0.11459962 -0.04297566]]. Action = [[-0.08068585 -0.22751917 -0.18135503 -0.23271549]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1038 is [True, False, False, False, True, False]
Scene graph at timestep 1038 is [True, False, False, False, True, False]
State prediction error at timestep 1038 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of 0
Current timestep = 1039. State = [[-0.11209037 -0.05989016]]. Action = [[ 0.15016013 -0.02040361  0.2038464  -0.67581755]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1039 is [True, False, False, False, True, False]
Scene graph at timestep 1039 is [True, False, False, False, True, False]
State prediction error at timestep 1039 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[-0.10594259 -0.06598888]]. Action = [[-0.0058139  -0.06657575  0.16292697  0.9929116 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1040 is [True, False, False, False, True, False]
Current timestep = 1041. State = [[-0.10049385 -0.08290563]]. Action = [[ 0.13038063 -0.20263763 -0.16943368  0.3461994 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1041 is [True, False, False, False, True, False]
Current timestep = 1042. State = [[-0.0913759  -0.09589482]]. Action = [[-0.07796174  0.0470736  -0.11801308 -0.4469453 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1042 is [True, False, False, False, True, False]
Current timestep = 1043. State = [[-0.09839071 -0.09342824]]. Action = [[-0.24290434  0.07954657  0.05217206  0.88433695]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1043 is [True, False, False, False, True, False]
Scene graph at timestep 1043 is [True, False, False, False, True, False]
State prediction error at timestep 1043 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[-0.11639175 -0.09951234]]. Action = [[-0.10320112 -0.14929843 -0.1907269   0.10399687]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1044 is [True, False, False, False, True, False]
Current timestep = 1045. State = [[-0.1272365  -0.12208295]]. Action = [[-0.06748378 -0.21160698  0.16469765  0.6553428 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1045 is [True, False, False, False, True, False]
Current timestep = 1046. State = [[-0.12938388 -0.12553681]]. Action = [[ 0.21602842  0.18256491 -0.13425393  0.63715816]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1046 is [True, False, False, False, True, False]
Scene graph at timestep 1046 is [True, False, False, True, False, False]
State prediction error at timestep 1046 is tensor(1.4512e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of -1
Current timestep = 1047. State = [[-0.12154303 -0.10573655]]. Action = [[-0.01106189  0.18849188  0.09353063 -0.22436255]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1047 is [True, False, False, True, False, False]
Current timestep = 1048. State = [[-0.12614559 -0.10292309]]. Action = [[-0.18182196 -0.16954826 -0.17552039 -0.92823637]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1048 is [True, False, False, False, True, False]
Current timestep = 1049. State = [[-0.13823333 -0.1227832 ]]. Action = [[-0.07208318 -0.20564012  0.09931797 -0.02025557]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1049 is [True, False, False, False, True, False]
Scene graph at timestep 1049 is [True, False, False, False, True, False]
State prediction error at timestep 1049 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.15215792 -0.14108403]]. Action = [[-0.12446213  0.00741926 -0.17398478  0.06341314]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1050 is [True, False, False, False, True, False]
Current timestep = 1051. State = [[-0.15717185 -0.12975729]]. Action = [[0.0771479  0.2279577  0.04538378 0.7652321 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1051 is [True, False, False, True, False, False]
Current timestep = 1052. State = [[-0.14822645 -0.10316736]]. Action = [[0.2277799  0.20941529 0.05419445 0.82871366]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1052 is [True, False, False, True, False, False]
Current timestep = 1053. State = [[-0.13009621 -0.09294862]]. Action = [[ 0.18701321 -0.16881257  0.19446066  0.7949176 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1053 is [True, False, False, False, True, False]
Scene graph at timestep 1053 is [True, False, False, False, True, False]
State prediction error at timestep 1053 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1053 of 1
Current timestep = 1054. State = [[-0.10718375 -0.0942003 ]]. Action = [[0.13298553 0.06891859 0.11492616 0.6799011 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1054 is [True, False, False, False, True, False]
Current timestep = 1055. State = [[-0.08812184 -0.09924485]]. Action = [[ 0.17967078 -0.18554409  0.2241112   0.7222955 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1055 is [True, False, False, False, True, False]
Scene graph at timestep 1055 is [True, False, False, False, True, False]
State prediction error at timestep 1055 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[-0.05911348 -0.11260269]]. Action = [[ 0.18652934  0.0071944  -0.13491479  0.43934464]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1056 is [True, False, False, False, True, False]
Scene graph at timestep 1056 is [True, False, False, False, True, False]
State prediction error at timestep 1056 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1056 of 1
Current timestep = 1057. State = [[-0.03515068 -0.11952338]]. Action = [[ 0.10101968 -0.11273152  0.05887902 -0.3602122 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Scene graph at timestep 1057 is [False, True, False, False, True, False]
State prediction error at timestep 1057 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of 1
Current timestep = 1058. State = [[-0.02737332 -0.1371362 ]]. Action = [[-0.09913388 -0.12612364  0.01551393  0.16284847]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1058 is [False, True, False, False, True, False]
Current timestep = 1059. State = [[-0.0280731  -0.15106626]]. Action = [[ 0.0851976  -0.08004558 -0.00151454 -0.7042495 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1059 is [False, True, False, True, False, False]
Scene graph at timestep 1059 is [False, True, False, True, False, False]
State prediction error at timestep 1059 is tensor(8.5233e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[-0.02208789 -0.15534152]]. Action = [[ 0.05335838  0.06592476 -0.22582506  0.3731203 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1060 is [False, True, False, True, False, False]
Current timestep = 1061. State = [[-0.01595535 -0.14625087]]. Action = [[-0.0318743   0.12532216 -0.24707386  0.87242484]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1061 is [False, True, False, True, False, False]
Scene graph at timestep 1061 is [False, True, False, True, False, False]
State prediction error at timestep 1061 is tensor(5.8659e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of 1
Current timestep = 1062. State = [[-0.02017999 -0.13568254]]. Action = [[-0.24023429  0.06520608  0.04839531  0.1460222 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1062 is [False, True, False, True, False, False]
Scene graph at timestep 1062 is [False, True, False, True, False, False]
State prediction error at timestep 1062 is tensor(7.3429e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of 0
Current timestep = 1063. State = [[-0.03292471 -0.11670985]]. Action = [[-0.00039576  0.23812646 -0.23637389  0.04667461]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1063 is [False, True, False, True, False, False]
Current timestep = 1064. State = [[-0.2276877 -0.2028178]]. Action = [[ 0.06800389 -0.06183577  0.13997835  0.07012749]]. Reward = [100.]
Curr episode timestep = 49
Scene graph at timestep 1064 is [False, True, False, False, True, False]
Scene graph at timestep 1064 is [True, False, False, True, False, False]
State prediction error at timestep 1064 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of -1
Current timestep = 1065. State = [[-0.21773636 -0.22606286]]. Action = [[ 0.08634889  0.0293476   0.07037348 -0.12848604]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1065 is [True, False, False, True, False, False]
Scene graph at timestep 1065 is [True, False, False, True, False, False]
State prediction error at timestep 1065 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of 1
Current timestep = 1066. State = [[-0.21316186 -0.21473405]]. Action = [[-0.1194725   0.2306152   0.02363759  0.41380668]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1066 is [True, False, False, True, False, False]
Current timestep = 1067. State = [[-0.21212102 -0.19367248]]. Action = [[ 0.10488212  0.12703592 -0.20169681  0.15975213]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1067 is [True, False, False, True, False, False]
Current timestep = 1068. State = [[-0.2146592  -0.17368814]]. Action = [[-0.17532666  0.12830931 -0.1606094  -0.63103336]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1068 is [True, False, False, True, False, False]
Scene graph at timestep 1068 is [True, False, False, True, False, False]
State prediction error at timestep 1068 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1068 of 1
Current timestep = 1069. State = [[-0.21209805 -0.17292736]]. Action = [[ 0.22818565 -0.23562384  0.16070223 -0.692568  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1069 is [True, False, False, True, False, False]
Current timestep = 1070. State = [[-0.19902895 -0.1772859 ]]. Action = [[ 0.1455574   0.12124684 -0.17359297 -0.54613334]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1070 is [True, False, False, True, False, False]
Current timestep = 1071. State = [[-0.19087781 -0.16315578]]. Action = [[-0.12799561  0.2284823  -0.03180376 -0.6975478 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1071 is [True, False, False, True, False, False]
Scene graph at timestep 1071 is [True, False, False, True, False, False]
State prediction error at timestep 1071 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1071 of 1
Current timestep = 1072. State = [[-0.19038518 -0.14184967]]. Action = [[ 0.10503572  0.03226477 -0.22198394  0.7586714 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1072 is [True, False, False, True, False, False]
Current timestep = 1073. State = [[-0.19117084 -0.14654309]]. Action = [[-0.13546821 -0.11768559 -0.12138283  0.19899237]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1073 is [True, False, False, True, False, False]
Scene graph at timestep 1073 is [True, False, False, True, False, False]
State prediction error at timestep 1073 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 0
Current timestep = 1074. State = [[-0.19650014 -0.15492092]]. Action = [[-0.06236351 -0.00868769  0.14351165  0.50844026]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1074 is [True, False, False, True, False, False]
Current timestep = 1075. State = [[-0.1964474  -0.14426424]]. Action = [[ 0.15178049  0.18288407 -0.12774342 -0.83327526]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1075 is [True, False, False, True, False, False]
Current timestep = 1076. State = [[-0.182701   -0.12894699]]. Action = [[ 0.22673535  0.04391786 -0.15098278 -0.27079886]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1076 is [True, False, False, True, False, False]
Current timestep = 1077. State = [[-0.17262197 -0.12742199]]. Action = [[-0.1960831  -0.05078711  0.22441244 -0.61741155]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1077 is [True, False, False, True, False, False]
Current timestep = 1078. State = [[-0.1703223  -0.14131653]]. Action = [[ 0.172701   -0.23381983 -0.14688763 -0.29711366]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1078 is [True, False, False, True, False, False]
Current timestep = 1079. State = [[-0.15449212 -0.16418979]]. Action = [[ 0.21444345 -0.15845561 -0.1456446   0.8949213 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1079 is [True, False, False, True, False, False]
Scene graph at timestep 1079 is [True, False, False, True, False, False]
State prediction error at timestep 1079 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of 1
Current timestep = 1080. State = [[-0.12640506 -0.17024076]]. Action = [[ 0.1636042   0.16951519 -0.05138101  0.30109143]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1080 is [True, False, False, True, False, False]
Scene graph at timestep 1080 is [True, False, False, True, False, False]
State prediction error at timestep 1080 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1080 of 1
Current timestep = 1081. State = [[-0.10192001 -0.16192932]]. Action = [[ 0.22300774 -0.00566034  0.22927335 -0.6337112 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1081 is [True, False, False, True, False, False]
Scene graph at timestep 1081 is [True, False, False, True, False, False]
State prediction error at timestep 1081 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1081 of 1
Current timestep = 1082. State = [[-0.07935525 -0.15011624]]. Action = [[-0.13323495  0.2077843   0.09995404  0.31815338]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1082 is [True, False, False, True, False, False]
Scene graph at timestep 1082 is [True, False, False, True, False, False]
State prediction error at timestep 1082 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1082 of 1
Current timestep = 1083. State = [[-0.07639837 -0.1453543 ]]. Action = [[ 0.20295173 -0.21821602 -0.15084638  0.91553164]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1083 is [True, False, False, True, False, False]
Scene graph at timestep 1083 is [True, False, False, True, False, False]
State prediction error at timestep 1083 is tensor(3.4005e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1083 of -1
Current timestep = 1084. State = [[-0.06557051 -0.15663287]]. Action = [[ 0.03617573  0.04109585 -0.1049532   0.63552916]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1084 is [True, False, False, True, False, False]
Scene graph at timestep 1084 is [True, False, False, True, False, False]
State prediction error at timestep 1084 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1084 of 0
Current timestep = 1085. State = [[-0.05774925 -0.14704426]]. Action = [[ 0.05779737  0.14886388 -0.2355572  -0.37651336]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1085 is [True, False, False, True, False, False]
Scene graph at timestep 1085 is [True, False, False, True, False, False]
State prediction error at timestep 1085 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[-0.05250346 -0.12828575]]. Action = [[0.08670866 0.12421879 0.15102553 0.80960417]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1086 is [True, False, False, True, False, False]
Scene graph at timestep 1086 is [True, False, False, True, False, False]
State prediction error at timestep 1086 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1086 of 1
Current timestep = 1087. State = [[-0.03580032 -0.13014422]]. Action = [[ 0.1368283  -0.21189348  0.1648818   0.9341965 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1087 is [True, False, False, True, False, False]
Current timestep = 1088. State = [[-0.01806764 -0.1438787 ]]. Action = [[ 0.13409072 -0.05606236 -0.12920217  0.8866612 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1088 is [False, True, False, True, False, False]
Scene graph at timestep 1088 is [False, True, False, True, False, False]
State prediction error at timestep 1088 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1088 of 0
Current timestep = 1089. State = [[-0.00431901 -0.1430874 ]]. Action = [[-0.20195006  0.17272961 -0.19825344 -0.88247263]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1089 is [False, True, False, True, False, False]
Scene graph at timestep 1089 is [False, True, False, True, False, False]
State prediction error at timestep 1089 is tensor(4.6328e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of 0
Current timestep = 1090. State = [[-0.00648237 -0.13138632]]. Action = [[0.167736   0.03579447 0.21080378 0.2062738 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1090 is [False, True, False, True, False, False]
Current timestep = 1091. State = [[-0.00410427 -0.13698632]]. Action = [[-0.06434971 -0.15298225  0.23375899  0.15651488]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1091 is [False, True, False, True, False, False]
Scene graph at timestep 1091 is [False, True, False, True, False, False]
State prediction error at timestep 1091 is tensor(6.5392e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of 0
Current timestep = 1092. State = [[ 0.0010865  -0.14481595]]. Action = [[ 0.20347023 -0.03520678  0.05972511  0.62402415]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1092 is [False, True, False, True, False, False]
Current timestep = 1093. State = [[ 0.01098096 -0.1527913 ]]. Action = [[-0.04070568 -0.07759723  0.10459912 -0.81255084]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1093 is [False, True, False, True, False, False]
Scene graph at timestep 1093 is [False, True, False, True, False, False]
State prediction error at timestep 1093 is tensor(6.0150e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 0
Current timestep = 1094. State = [[ 0.00912272 -0.15222163]]. Action = [[ 0.00899521  0.1529451   0.17199913 -0.9739828 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1094 is [False, True, False, True, False, False]
Scene graph at timestep 1094 is [False, True, False, True, False, False]
State prediction error at timestep 1094 is tensor(7.4113e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of 0
Current timestep = 1095. State = [[ 0.00948118 -0.14180174]]. Action = [[-0.22841449  0.04889661 -0.01642764 -0.33701277]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1095 is [False, True, False, True, False, False]
Current timestep = 1096. State = [[ 0.00699654 -0.13234966]]. Action = [[0.10410365 0.0957135  0.09687865 0.33164263]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1096 is [False, True, False, True, False, False]
Current timestep = 1097. State = [[ 0.00430088 -0.11192945]]. Action = [[-0.16869979  0.22228795  0.24502772 -0.6753027 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1097 is [False, True, False, True, False, False]
Current timestep = 1098. State = [[-0.15043058  0.00135352]]. Action = [[0.15544897 0.12373018 0.229805   0.7530267 ]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 1098 is [False, True, False, False, True, False]
Current timestep = 1099. State = [[-0.13400172  0.0093067 ]]. Action = [[ 0.00577089  0.16611055 -0.16906615  0.582597  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Current timestep = 1100. State = [[-0.12979802  0.00813297]]. Action = [[ 0.0640344  -0.18473978  0.12473208  0.26282263]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Scene graph at timestep 1100 is [True, False, False, False, True, False]
State prediction error at timestep 1100 is tensor(5.3171e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of 0
Current timestep = 1101. State = [[-0.12827833 -0.0035442 ]]. Action = [[-0.2214451  -0.02426998  0.10175604  0.5522522 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Scene graph at timestep 1101 is [True, False, False, False, True, False]
State prediction error at timestep 1101 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of -1
Current timestep = 1102. State = [[-0.13161972 -0.00693325]]. Action = [[ 0.05217153 -0.02304062 -0.09660012 -0.54975736]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Scene graph at timestep 1102 is [True, False, False, False, True, False]
State prediction error at timestep 1102 is tensor(4.7871e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of 0
Current timestep = 1103. State = [[-0.13182344 -0.00961858]]. Action = [[-0.01269461 -0.00846434 -0.17380637 -0.77752286]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Current timestep = 1104. State = [[-0.12471471 -0.01914687]]. Action = [[ 0.24052101 -0.13639909 -0.2409624   0.74151516]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Current timestep = 1105. State = [[-0.11921954 -0.04049262]]. Action = [[-0.13347773 -0.21199435  0.23945963 -0.9921794 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Current timestep = 1106. State = [[-0.11816412 -0.05328887]]. Action = [[ 0.13447753  0.05395317 -0.13187963 -0.86733884]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Current timestep = 1107. State = [[-0.11606207 -0.05209823]]. Action = [[-0.10289723  0.02590495  0.13607943  0.9035474 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Current timestep = 1108. State = [[-0.12282235 -0.03990239]]. Action = [[-0.16501044  0.20025182  0.02809215 -0.81135225]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Current timestep = 1109. State = [[-0.13069384 -0.03077736]]. Action = [[-0.04066624 -0.07356867 -0.01154983  0.71454215]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Scene graph at timestep 1109 is [True, False, False, False, True, False]
State prediction error at timestep 1109 is tensor(5.8039e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of -1
Current timestep = 1110. State = [[-0.13896233 -0.02111571]]. Action = [[-0.05499861  0.1929732  -0.19081132  0.7649505 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Current timestep = 1111. State = [[-0.14039269 -0.00560104]]. Action = [[ 0.15686262  0.04111779 -0.129808   -0.3390361 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Scene graph at timestep 1111 is [True, False, False, False, True, False]
State prediction error at timestep 1111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of 0
Current timestep = 1112. State = [[-0.12831509 -0.01044947]]. Action = [[ 0.23137176 -0.18227133  0.10300985 -0.28801292]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1112 is [True, False, False, False, True, False]
Current timestep = 1113. State = [[-0.11156518 -0.0066273 ]]. Action = [[ 0.01940253  0.2358658   0.03128377 -0.3427739 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1113 is [True, False, False, False, True, False]
Scene graph at timestep 1113 is [True, False, False, False, True, False]
State prediction error at timestep 1113 is tensor(8.7381e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of 1
Current timestep = 1114. State = [[-0.1036376   0.00048134]]. Action = [[ 0.12681371 -0.10451508  0.07949281 -0.12513196]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1114 is [True, False, False, False, True, False]
Scene graph at timestep 1114 is [True, False, False, False, True, False]
State prediction error at timestep 1114 is tensor(7.0497e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.09124927  0.00882588]]. Action = [[-0.01610307  0.22584182  0.15251529  0.14447546]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1115 is [True, False, False, False, True, False]
Current timestep = 1116. State = [[-0.09528127  0.02632956]]. Action = [[-0.06963158  0.07106325 -0.06027228 -0.52508193]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1116 is [True, False, False, False, True, False]
Scene graph at timestep 1116 is [True, False, False, False, True, False]
State prediction error at timestep 1116 is tensor(9.0999e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of 0
Current timestep = 1117. State = [[-0.09600198  0.03076178]]. Action = [[ 0.07077956 -0.08687904  0.23798674  0.8195145 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1117 is [True, False, False, False, True, False]
Current timestep = 1118. State = [[-0.09532074  0.01923281]]. Action = [[-0.11762065 -0.1308729   0.1952497   0.20646906]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1118 is [True, False, False, False, True, False]
Current timestep = 1119. State = [[-0.09398637 -0.00379928]]. Action = [[ 0.10430655 -0.216776    0.2369411   0.01036549]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1119 is [True, False, False, False, True, False]
Current timestep = 1120. State = [[-0.09649938 -0.01203331]]. Action = [[-0.19243993  0.12291461  0.1486255  -0.51713014]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1120 is [True, False, False, False, True, False]
Current timestep = 1121. State = [[-0.10092922 -0.00674911]]. Action = [[-0.03253068  0.04062471 -0.12849015 -0.57780534]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1121 is [True, False, False, False, True, False]
Current timestep = 1122. State = [[-0.10227693 -0.00318251]]. Action = [[ 7.8537166e-03  5.0663948e-06  2.3202106e-01 -7.8198272e-01]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1122 is [True, False, False, False, True, False]
Current timestep = 1123. State = [[-0.10746635 -0.01476111]]. Action = [[-0.11852133 -0.19487032  0.02850032  0.28011847]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1123 is [True, False, False, False, True, False]
Current timestep = 1124. State = [[-0.1216683  -0.03200332]]. Action = [[-0.15577668 -0.08657303 -0.06574133 -0.40901852]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Current timestep = 1125. State = [[-0.13084526 -0.04630435]]. Action = [[ 0.1247566  -0.10209727 -0.0564815  -0.20544791]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Scene graph at timestep 1125 is [True, False, False, False, True, False]
State prediction error at timestep 1125 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of 0
Current timestep = 1126. State = [[-0.12673286 -0.06364129]]. Action = [[ 0.10289675 -0.12917998 -0.11708993  0.07188475]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Current timestep = 1127. State = [[-0.11294304 -0.07456511]]. Action = [[ 0.24767399 -0.03056145  0.01824448 -0.9480187 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1127 is [True, False, False, False, True, False]
Current timestep = 1128. State = [[-0.08843252 -0.0888019 ]]. Action = [[ 0.17236474 -0.1837414   0.16312587  0.16802382]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1128 is [True, False, False, False, True, False]
Scene graph at timestep 1128 is [True, False, False, False, True, False]
State prediction error at timestep 1128 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1128 of 1
Current timestep = 1129. State = [[-0.06728376 -0.10313896]]. Action = [[ 0.09517813  0.0456838  -0.13609722  0.40531874]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1129 is [True, False, False, False, True, False]
Scene graph at timestep 1129 is [True, False, False, False, True, False]
State prediction error at timestep 1129 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[-0.05580142 -0.1123801 ]]. Action = [[-0.01466228 -0.18002261  0.16624337 -0.33455324]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1130 is [True, False, False, False, True, False]
Current timestep = 1131. State = [[-0.05563266 -0.11543204]]. Action = [[-0.01375921  0.15215471 -0.20732409 -0.48515075]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1131 is [True, False, False, False, True, False]
Scene graph at timestep 1131 is [True, False, False, False, True, False]
State prediction error at timestep 1131 is tensor(9.8720e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1131 of 0
Current timestep = 1132. State = [[-0.05571379 -0.10431755]]. Action = [[-0.00068143  0.08533704  0.15707278  0.47850883]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1132 is [True, False, False, False, True, False]
Scene graph at timestep 1132 is [True, False, False, False, True, False]
State prediction error at timestep 1132 is tensor(3.6227e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[-0.05608158 -0.0975833 ]]. Action = [[-0.03088105 -0.00328535  0.12942833  0.71684504]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1133 is [True, False, False, False, True, False]
Scene graph at timestep 1133 is [True, False, False, False, True, False]
State prediction error at timestep 1133 is tensor(5.5527e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 0
Current timestep = 1134. State = [[-0.04948554 -0.09731507]]. Action = [[ 0.20731705 -0.01483193  0.19802713 -0.92115754]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1134 is [True, False, False, False, True, False]
Current timestep = 1135. State = [[-0.03316484 -0.10677328]]. Action = [[ 0.0908944  -0.17544775  0.14973414  0.1018728 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1135 is [False, True, False, False, True, False]
Current timestep = 1136. State = [[-0.01535304 -0.12065426]]. Action = [[ 0.2068392  -0.06507775  0.11533988  0.46213913]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1136 is [False, True, False, False, True, False]
Scene graph at timestep 1136 is [False, True, False, False, True, False]
State prediction error at timestep 1136 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of 1
Current timestep = 1137. State = [[ 0.00824738 -0.119395  ]]. Action = [[-0.03967629  0.17477173  0.13893628  0.21660495]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1137 is [False, True, False, False, True, False]
Scene graph at timestep 1137 is [False, True, False, False, True, False]
State prediction error at timestep 1137 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of 1
Current timestep = 1138. State = [[ 0.00690967 -0.11271517]]. Action = [[-0.10339874 -0.06598642  0.11395013 -0.5870246 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1138 is [False, True, False, False, True, False]
Current timestep = 1139. State = [[ 0.00287312 -0.12322282]]. Action = [[-0.09798712 -0.10746023 -0.04605903  0.6713941 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1139 is [False, True, False, False, True, False]
Scene graph at timestep 1139 is [False, True, False, False, True, False]
State prediction error at timestep 1139 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of -1
Current timestep = 1140. State = [[-0.00979284 -0.14439069]]. Action = [[-0.20472725 -0.15157546 -0.03964154  0.02339923]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1140 is [False, True, False, False, True, False]
Current timestep = 1141. State = [[-0.0209148  -0.15598224]]. Action = [[ 0.00075123 -0.01697873  0.20233032 -0.21980244]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1141 is [False, True, False, True, False, False]
Scene graph at timestep 1141 is [False, True, False, True, False, False]
State prediction error at timestep 1141 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of -1
Current timestep = 1142. State = [[-0.02378224 -0.16339329]]. Action = [[ 0.07229435 -0.07560533  0.1457718   0.48723078]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1142 is [False, True, False, True, False, False]
Scene graph at timestep 1142 is [False, True, False, True, False, False]
State prediction error at timestep 1142 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1142 of -1
Current timestep = 1143. State = [[-0.02180689 -0.17768767]]. Action = [[ 0.16088831 -0.20085038  0.11729416  0.18572092]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1143 is [False, True, False, True, False, False]
Scene graph at timestep 1143 is [False, True, False, True, False, False]
State prediction error at timestep 1143 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1143 of -1
Current timestep = 1144. State = [[-0.01497608 -0.18255787]]. Action = [[ 0.04955736  0.21365851 -0.07205972  0.9835124 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1144 is [False, True, False, True, False, False]
Scene graph at timestep 1144 is [False, True, False, True, False, False]
State prediction error at timestep 1144 is tensor(1.5178e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of -1
Current timestep = 1145. State = [[-0.01061348 -0.17081702]]. Action = [[-0.07830274  0.00801331 -0.12216599  0.845032  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1145 is [False, True, False, True, False, False]
Current timestep = 1146. State = [[-0.0130908  -0.16242291]]. Action = [[-0.14641055  0.15068635  0.0125308  -0.20949334]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1146 is [False, True, False, True, False, False]
Current timestep = 1147. State = [[-0.01494443 -0.16060431]]. Action = [[ 0.15674835 -0.20155127 -0.20765251 -0.235098  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1147 is [False, True, False, True, False, False]
Current timestep = 1148. State = [[-0.0082432  -0.17969972]]. Action = [[ 0.19647771 -0.19714808 -0.20537354  0.42850387]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1148 is [False, True, False, True, False, False]
Scene graph at timestep 1148 is [False, True, False, True, False, False]
State prediction error at timestep 1148 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1148 of -1
Current timestep = 1149. State = [[ 0.00556527 -0.21159859]]. Action = [[ 0.0845297  -0.24759285  0.09333253  0.7633114 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1149 is [False, True, False, True, False, False]
Scene graph at timestep 1149 is [False, True, False, True, False, False]
State prediction error at timestep 1149 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of -1
Current timestep = 1150. State = [[ 0.02056428 -0.22355874]]. Action = [[0.03604925 0.1925937  0.1491813  0.91592026]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1150 is [False, True, False, True, False, False]
Scene graph at timestep 1150 is [False, True, False, True, False, False]
State prediction error at timestep 1150 is tensor(1.9991e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1150 of 1
Current timestep = 1151. State = [[ 0.03434652 -0.22352602]]. Action = [[ 0.18064672 -0.20660268 -0.05504742 -0.4195218 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1151 is [False, True, False, True, False, False]
Current timestep = 1152. State = [[ 0.05726638 -0.22134294]]. Action = [[0.17090315 0.22625518 0.24380043 0.8674555 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1152 is [False, True, False, True, False, False]
Current timestep = 1153. State = [[ 0.07266462 -0.20491515]]. Action = [[-0.13154839  0.13245404  0.14936846  0.9287751 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1153 is [False, False, True, True, False, False]
Scene graph at timestep 1153 is [False, False, True, True, False, False]
State prediction error at timestep 1153 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of -1
Current timestep = 1154. State = [[ 0.07279304 -0.19317767]]. Action = [[ 0.24212676  0.15428263 -0.01070246 -0.6566982 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1154 is [False, False, True, True, False, False]
Current timestep = 1155. State = [[ 0.07279304 -0.19317767]]. Action = [[ 0.15457293  0.06831294 -0.17011996 -0.28568876]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1155 is [False, False, True, True, False, False]
Scene graph at timestep 1155 is [False, False, True, True, False, False]
State prediction error at timestep 1155 is tensor(3.0522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of -1
Current timestep = 1156. State = [[ 0.07279304 -0.19317767]]. Action = [[ 0.21201068 -0.17053802  0.09220904 -0.33861518]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1156 is [False, False, True, True, False, False]
Scene graph at timestep 1156 is [False, False, True, True, False, False]
State prediction error at timestep 1156 is tensor(2.4468e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of -1
Current timestep = 1157. State = [[ 0.07279304 -0.19317767]]. Action = [[ 0.1651273   0.13518637  0.11813805 -0.15706444]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1157 is [False, False, True, True, False, False]
Scene graph at timestep 1157 is [False, False, True, True, False, False]
State prediction error at timestep 1157 is tensor(6.1768e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of -1
Current timestep = 1158. State = [[ 0.07279304 -0.19317767]]. Action = [[ 0.02654082 -0.21585579  0.2094154   0.42963123]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1158 is [False, False, True, True, False, False]
Current timestep = 1159. State = [[ 0.07323811 -0.18018472]]. Action = [[-0.0313351   0.21895355  0.05136096 -0.71074784]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1159 is [False, False, True, True, False, False]
Scene graph at timestep 1159 is [False, False, True, True, False, False]
State prediction error at timestep 1159 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of 1
Current timestep = 1160. State = [[ 0.0700074  -0.16149862]]. Action = [[-0.16251503  0.02079713  0.21474415  0.78711057]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1160 is [False, False, True, True, False, False]
Scene graph at timestep 1160 is [False, False, True, True, False, False]
State prediction error at timestep 1160 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[ 0.0604656  -0.15874901]]. Action = [[0.07983249 0.01840985 0.1303156  0.19110954]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1161 is [False, False, True, True, False, False]
Scene graph at timestep 1161 is [False, False, True, True, False, False]
State prediction error at timestep 1161 is tensor(1.0485e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1161 of -1
Current timestep = 1162. State = [[ 0.06037172 -0.15877622]]. Action = [[ 0.13385177  0.09208649 -0.20747538  0.1945442 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1162 is [False, False, True, True, False, False]
Current timestep = 1163. State = [[ 0.06027782 -0.15880343]]. Action = [[ 0.11926359  0.0162206  -0.14499618 -0.4174999 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1163 is [False, False, True, True, False, False]
Scene graph at timestep 1163 is [False, False, True, True, False, False]
State prediction error at timestep 1163 is tensor(1.0522e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1163 of -1
Current timestep = 1164. State = [[ 0.05425067 -0.17215998]]. Action = [[-0.07720095 -0.22225362  0.09596315 -0.11830294]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1164 is [False, False, True, True, False, False]
Current timestep = 1165. State = [[ 0.04582794 -0.18426451]]. Action = [[-0.11337909  0.02266455 -0.16143231 -0.293378  ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1165 is [False, False, True, True, False, False]
Scene graph at timestep 1165 is [False, True, False, True, False, False]
State prediction error at timestep 1165 is tensor(5.5234e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of 1
Current timestep = 1166. State = [[ 0.03570832 -0.19653209]]. Action = [[ 0.12082532 -0.17612833  0.09603143 -0.5732527 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1166 is [False, True, False, True, False, False]
Scene graph at timestep 1166 is [False, True, False, True, False, False]
State prediction error at timestep 1166 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of -1
Current timestep = 1167. State = [[ 0.03656505 -0.20689057]]. Action = [[ 0.19548738  0.17996472  0.03242904 -0.34984457]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1167 is [False, True, False, True, False, False]
Current timestep = 1168. State = [[ 0.03855042 -0.19441158]]. Action = [[0.04509628 0.23402193 0.21074104 0.01954591]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1168 is [False, True, False, True, False, False]
Scene graph at timestep 1168 is [False, True, False, True, False, False]
State prediction error at timestep 1168 is tensor(1.2347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[ 0.03934462 -0.17836694]]. Action = [[ 0.23377663 -0.05048236  0.24846023  0.7715937 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1169 is [False, True, False, True, False, False]
Scene graph at timestep 1169 is [False, True, False, True, False, False]
State prediction error at timestep 1169 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of -1
Current timestep = 1170. State = [[ 0.03632081 -0.18385443]]. Action = [[-0.15047614 -0.09004629 -0.14567229  0.39357555]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1170 is [False, True, False, True, False, False]
Scene graph at timestep 1170 is [False, True, False, True, False, False]
State prediction error at timestep 1170 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of 0
Current timestep = 1171. State = [[ 0.03317579 -0.19794938]]. Action = [[ 0.03135121 -0.1591268   0.21950316 -0.96464247]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1171 is [False, True, False, True, False, False]
Scene graph at timestep 1171 is [False, True, False, True, False, False]
State prediction error at timestep 1171 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of -1
Current timestep = 1172. State = [[ 0.02707786 -0.20123442]]. Action = [[-0.19701439  0.19622123  0.21171606 -0.6061484 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1172 is [False, True, False, True, False, False]
Scene graph at timestep 1172 is [False, True, False, True, False, False]
State prediction error at timestep 1172 is tensor(9.1614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 0
Current timestep = 1173. State = [[ 0.00617287 -0.18621124]]. Action = [[-0.240343    0.06642184  0.16335884 -0.5895831 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1173 is [False, True, False, True, False, False]
Scene graph at timestep 1173 is [False, True, False, True, False, False]
State prediction error at timestep 1173 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 0
Current timestep = 1174. State = [[-0.0177077  -0.17233281]]. Action = [[ 0.20256907  0.06846008 -0.01800478  0.27258706]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1174 is [False, True, False, True, False, False]
Scene graph at timestep 1174 is [False, True, False, True, False, False]
State prediction error at timestep 1174 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of 0
Current timestep = 1175. State = [[-0.01410923 -0.17675404]]. Action = [[ 0.03904906 -0.20406637 -0.13142255  0.15253162]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1175 is [False, True, False, True, False, False]
Current timestep = 1176. State = [[-0.01087244 -0.19162779]]. Action = [[ 0.08590692 -0.06955589  0.14869627 -0.8772906 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1176 is [False, True, False, True, False, False]
Scene graph at timestep 1176 is [False, True, False, True, False, False]
State prediction error at timestep 1176 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of -1
Current timestep = 1177. State = [[-0.00622199 -0.19874299]]. Action = [[-0.08433531  0.02589923  0.1088033  -0.19229686]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1177 is [False, True, False, True, False, False]
Scene graph at timestep 1177 is [False, True, False, True, False, False]
State prediction error at timestep 1177 is tensor(2.9891e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1177 of -1
Current timestep = 1178. State = [[-0.00646101 -0.20225301]]. Action = [[ 0.02356842 -0.07231341 -0.13356942 -0.0368759 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1178 is [False, True, False, True, False, False]
Current timestep = 1179. State = [[-0.01652105 -0.21900629]]. Action = [[-0.2238864  -0.19687119  0.08750752 -0.21084261]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1179 is [False, True, False, True, False, False]
Current timestep = 1180. State = [[-0.02039566 -0.23194602]]. Action = [[ 0.1120069   0.05138481  0.14597523 -0.7768906 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1180 is [False, True, False, True, False, False]
Current timestep = 1181. State = [[-0.01903769 -0.22097416]]. Action = [[-0.09284887  0.2022666  -0.07673486 -0.6788283 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1181 is [False, True, False, True, False, False]
Current timestep = 1182. State = [[-0.01746801 -0.2208481 ]]. Action = [[ 0.15616232 -0.22045659  0.03297919  0.05161166]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1182 is [False, True, False, True, False, False]
Scene graph at timestep 1182 is [False, True, False, True, False, False]
State prediction error at timestep 1182 is tensor(7.1669e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of -1
Current timestep = 1183. State = [[-0.01870882 -0.22907323]]. Action = [[-0.15112242  0.08252659 -0.10917097 -0.9829519 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1183 is [False, True, False, True, False, False]
Current timestep = 1184. State = [[-0.0203554  -0.23335916]]. Action = [[-0.0180226  -0.1037205   0.12637949  0.3950324 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1184 is [False, True, False, True, False, False]
Scene graph at timestep 1184 is [False, True, False, True, False, False]
State prediction error at timestep 1184 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1184 of -1
Current timestep = 1185. State = [[-0.03128619 -0.22878852]]. Action = [[-0.18112704  0.20233434 -0.22605383 -0.33084625]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1185 is [False, True, False, True, False, False]
Scene graph at timestep 1185 is [False, True, False, True, False, False]
State prediction error at timestep 1185 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of 0
Current timestep = 1186. State = [[-0.04829209 -0.21065605]]. Action = [[-0.12018409  0.07848406 -0.08615161  0.6301719 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1186 is [False, True, False, True, False, False]
Current timestep = 1187. State = [[-0.05315396 -0.19156522]]. Action = [[0.22821009 0.19078308 0.20127323 0.9784318 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1187 is [False, True, False, True, False, False]
Current timestep = 1188. State = [[-0.05025541 -0.16629383]]. Action = [[-0.00490274  0.12815934 -0.15844491  0.09366465]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1188 is [True, False, False, True, False, False]
Current timestep = 1189. State = [[-0.03965917 -0.14225763]]. Action = [[ 0.21359324  0.21265522 -0.08417958  0.6888952 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1189 is [True, False, False, True, False, False]
Current timestep = 1190. State = [[-0.01802807 -0.12581837]]. Action = [[ 0.23541015 -0.06327501  0.03357503 -0.63618296]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1190 is [False, True, False, True, False, False]
Current timestep = 1191. State = [[ 0.00745209 -0.11942841]]. Action = [[ 0.13286036  0.05978769  0.03384227 -0.06082541]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1191 is [False, True, False, True, False, False]
Scene graph at timestep 1191 is [False, True, False, False, True, False]
State prediction error at timestep 1191 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of 1
Current timestep = 1192. State = [[ 0.02632156 -0.11961836]]. Action = [[ 0.15978584 -0.09543321  0.23584658  0.7709727 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1192 is [False, True, False, False, True, False]
Scene graph at timestep 1192 is [False, True, False, False, True, False]
State prediction error at timestep 1192 is tensor(5.3995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of 0
Current timestep = 1193. State = [[ 0.04225794 -0.13284558]]. Action = [[-0.1953148  -0.06359941  0.0142726  -0.1035195 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1193 is [False, True, False, False, True, False]
Scene graph at timestep 1193 is [False, True, False, True, False, False]
State prediction error at timestep 1193 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of 0
Current timestep = 1194. State = [[ 0.03918087 -0.14742664]]. Action = [[ 0.12878826 -0.1271582   0.06147525  0.14229989]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1194 is [False, True, False, True, False, False]
Scene graph at timestep 1194 is [False, True, False, True, False, False]
State prediction error at timestep 1194 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of -1
Current timestep = 1195. State = [[ 0.0393463 -0.1435074]]. Action = [[-0.1106461   0.23116183  0.19526863  0.96978366]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1195 is [False, True, False, True, False, False]
Current timestep = 1196. State = [[ 0.03837072 -0.125871  ]]. Action = [[-0.04513842  0.09173563  0.10960588  0.3285054 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1196 is [False, True, False, True, False, False]
Current timestep = 1197. State = [[ 0.03717417 -0.11691525]]. Action = [[ 0.2377744  -0.12705699 -0.02976958 -0.8347626 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1197 is [False, True, False, True, False, False]
Current timestep = 1198. State = [[ 0.03498386 -0.12886724]]. Action = [[ 0.05068845 -0.24790004  0.2096582   0.8943968 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1198 is [False, True, False, False, True, False]
Current timestep = 1199. State = [[ 0.03481701 -0.15052618]]. Action = [[ 0.01514319 -0.12654737  0.12106931 -0.06962883]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1199 is [False, True, False, True, False, False]
Scene graph at timestep 1199 is [False, True, False, True, False, False]
State prediction error at timestep 1199 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of -1
Current timestep = 1200. State = [[ 0.03974481 -0.1565704 ]]. Action = [[ 0.15833116  0.12212798  0.10509208 -0.53334874]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1200 is [False, True, False, True, False, False]
Current timestep = 1201. State = [[ 0.04434293 -0.14552228]]. Action = [[-0.21980906  0.10478961  0.24200231  0.20625305]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1201 is [False, True, False, True, False, False]
Scene graph at timestep 1201 is [False, True, False, True, False, False]
State prediction error at timestep 1201 is tensor(1.9542e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1201 of 1
Current timestep = 1202. State = [[ 0.03702625 -0.13653463]]. Action = [[-0.10021976  0.02706835  0.20258144  0.29953933]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1202 is [False, True, False, True, False, False]
Current timestep = 1203. State = [[ 0.02766138 -0.14250305]]. Action = [[ 0.00344998 -0.15794168  0.01605773  0.27927387]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1203 is [False, True, False, True, False, False]
Current timestep = 1204. State = [[ 0.0210309  -0.14555493]]. Action = [[-0.09973581  0.09707111  0.23843604 -0.89800656]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1204 is [False, True, False, True, False, False]
Scene graph at timestep 1204 is [False, True, False, True, False, False]
State prediction error at timestep 1204 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 1
Current timestep = 1205. State = [[ 0.00970584 -0.1480457 ]]. Action = [[-0.03123528 -0.10821505  0.2028482  -0.94204074]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1205 is [False, True, False, True, False, False]
Scene graph at timestep 1205 is [False, True, False, True, False, False]
State prediction error at timestep 1205 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of 0
Current timestep = 1206. State = [[-0.00453814 -0.15590239]]. Action = [[-0.21616997  0.03322256 -0.21296978 -0.56021595]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1206 is [False, True, False, True, False, False]
Scene graph at timestep 1206 is [False, True, False, True, False, False]
State prediction error at timestep 1206 is tensor(9.6749e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of -1
Current timestep = 1207. State = [[-0.02407842 -0.16275759]]. Action = [[ 0.03905019 -0.17617014 -0.01399393  0.5514438 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1207 is [False, True, False, True, False, False]
Scene graph at timestep 1207 is [False, True, False, True, False, False]
State prediction error at timestep 1207 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1207 of -1
Current timestep = 1208. State = [[-0.02457704 -0.17186151]]. Action = [[0.01978621 0.0148184  0.05171412 0.67870426]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1208 is [False, True, False, True, False, False]
Scene graph at timestep 1208 is [False, True, False, True, False, False]
State prediction error at timestep 1208 is tensor(8.5061e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of -1
Current timestep = 1209. State = [[-0.02197592 -0.17493665]]. Action = [[ 0.13205373 -0.07169336  0.2100693   0.96353984]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1209 is [False, True, False, True, False, False]
Current timestep = 1210. State = [[-0.01640875 -0.18351743]]. Action = [[ 0.11771712 -0.0859912  -0.2037793   0.08273685]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1210 is [False, True, False, True, False, False]
Scene graph at timestep 1210 is [False, True, False, True, False, False]
State prediction error at timestep 1210 is tensor(9.8191e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of -1
Current timestep = 1211. State = [[-0.00393108 -0.18480505]]. Action = [[-0.08374175  0.15586767 -0.10864887  0.00292647]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1211 is [False, True, False, True, False, False]
Current timestep = 1212. State = [[-0.00495444 -0.16817902]]. Action = [[-0.08636183  0.17340285 -0.21574725 -0.83442694]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1212 is [False, True, False, True, False, False]
Current timestep = 1213. State = [[-0.00915248 -0.14827174]]. Action = [[-0.09424618  0.10027382  0.21195933  0.9651432 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1213 is [False, True, False, True, False, False]
Scene graph at timestep 1213 is [False, True, False, True, False, False]
State prediction error at timestep 1213 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1213 of 1
Current timestep = 1214. State = [[-0.02253138 -0.1386775 ]]. Action = [[-0.07126296 -0.00979225  0.22590649 -0.5448967 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1214 is [False, True, False, True, False, False]
Current timestep = 1215. State = [[-0.02383634 -0.12779279]]. Action = [[ 0.05957419  0.18521893 -0.01540577 -0.5295718 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1215 is [False, True, False, True, False, False]
Current timestep = 1216. State = [[-0.03152938 -0.12829691]]. Action = [[-0.16385503 -0.23544638  0.15815124 -0.622886  ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1216 is [False, True, False, True, False, False]
Current timestep = 1217. State = [[-0.04030489 -0.13050365]]. Action = [[ 0.13900101  0.15791678  0.02716091 -0.07138383]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1217 is [False, True, False, True, False, False]
Scene graph at timestep 1217 is [False, True, False, True, False, False]
State prediction error at timestep 1217 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of 0
Current timestep = 1218. State = [[-0.03831499 -0.12647575]]. Action = [[ 0.07692182 -0.10008425 -0.0158408   0.5315826 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1218 is [False, True, False, True, False, False]
Scene graph at timestep 1218 is [False, True, False, True, False, False]
State prediction error at timestep 1218 is tensor(3.4409e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of -1
Current timestep = 1219. State = [[-0.0251252  -0.12650426]]. Action = [[ 0.20260942  0.03278893 -0.08890407  0.38606572]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1219 is [False, True, False, True, False, False]
Scene graph at timestep 1219 is [False, True, False, True, False, False]
State prediction error at timestep 1219 is tensor(1.1246e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of 1
Current timestep = 1220. State = [[-0.00496101 -0.13459088]]. Action = [[ 0.16793317 -0.18029611  0.02517545 -0.44104004]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1220 is [False, True, False, True, False, False]
Current timestep = 1221. State = [[ 0.02230328 -0.15192221]]. Action = [[ 0.23494375 -0.07462782  0.03880972  0.53212166]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1221 is [False, True, False, True, False, False]
Current timestep = 1222. State = [[ 0.03866994 -0.15934631]]. Action = [[-0.22861433  0.05116659  0.02478418 -0.9342996 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1222 is [False, True, False, True, False, False]
Current timestep = 1223. State = [[ 0.03293976 -0.15238215]]. Action = [[-0.14396422  0.13923398 -0.17332701  0.5859778 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1223 is [False, True, False, True, False, False]
Scene graph at timestep 1223 is [False, True, False, True, False, False]
State prediction error at timestep 1223 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 0
Current timestep = 1224. State = [[ 0.02090207 -0.14539807]]. Action = [[ 0.17522457 -0.10179429  0.1848445   0.44094658]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1224 is [False, True, False, True, False, False]
Current timestep = 1225. State = [[-0.24784394  0.07626029]]. Action = [[-0.04048237  0.22591585  0.03365141  0.77879333]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1225 is [False, True, False, True, False, False]
Scene graph at timestep 1225 is [True, False, False, False, True, False]
State prediction error at timestep 1225 is tensor(0.0636, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of 0
Current timestep = 1226. State = [[-0.23996828  0.07477734]]. Action = [[-0.0655753  -0.22465263  0.088514   -0.28553808]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Current timestep = 1227. State = [[-0.23890889  0.06345357]]. Action = [[ 0.0981915   0.02371022 -0.10749075 -0.85925937]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Current timestep = 1228. State = [[-0.23890224  0.05771506]]. Action = [[-0.12619728 -0.07409862  0.18808284 -0.9741202 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Scene graph at timestep 1228 is [True, False, False, False, True, False]
State prediction error at timestep 1228 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1228 of 0
Current timestep = 1229. State = [[-0.24318801  0.05257078]]. Action = [[-0.08867976  0.02007502 -0.14892127  0.00018859]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1229 is [True, False, False, False, True, False]
Current timestep = 1230. State = [[-0.25329953  0.06606096]]. Action = [[-0.04214877  0.20932913  0.04467618  0.08220112]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Current timestep = 1231. State = [[-0.2546209   0.07784725]]. Action = [[ 0.17869714 -0.05510701  0.06939155  0.9234917 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Current timestep = 1232. State = [[-0.2484065   0.07514667]]. Action = [[ 0.04109386 -0.04430057 -0.14265805 -0.9375515 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.23731041  0.06968401]]. Action = [[ 0.14623487 -0.04325001 -0.18710153 -0.9669779 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.21336502  0.05606626]]. Action = [[ 0.22816736 -0.16144638  0.1758798   0.37477624]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.20069227  0.0520198 ]]. Action = [[-0.156436    0.11397833  0.09366146 -0.340746  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1235 is [True, False, False, False, True, False]
Scene graph at timestep 1235 is [True, False, False, False, True, False]
State prediction error at timestep 1235 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of 1
Current timestep = 1236. State = [[-0.20074786  0.05373814]]. Action = [[ 0.10335428 -0.04913768 -0.2064009  -0.9753005 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Scene graph at timestep 1236 is [True, False, False, False, True, False]
State prediction error at timestep 1236 is tensor(1.0537e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1236 of 1
Current timestep = 1237. State = [[-0.1870267   0.04694682]]. Action = [[ 0.19925094 -0.08293566 -0.14292663 -0.48345876]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Scene graph at timestep 1237 is [True, False, False, False, True, False]
State prediction error at timestep 1237 is tensor(7.0460e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of 1
Current timestep = 1238. State = [[-0.15869655  0.02855592]]. Action = [[ 0.14805505 -0.1641775  -0.17495954  0.07607329]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1238 is [True, False, False, False, True, False]
Scene graph at timestep 1238 is [True, False, False, False, True, False]
State prediction error at timestep 1238 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 1
Current timestep = 1239. State = [[-0.1363157   0.01294375]]. Action = [[ 0.23250559 -0.06884861  0.07958853 -0.9862519 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1239 is [True, False, False, False, True, False]
Scene graph at timestep 1239 is [True, False, False, False, True, False]
State prediction error at timestep 1239 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[-0.11292879 -0.00658513]]. Action = [[-0.01018743 -0.20268953  0.1855655  -0.0812369 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1240 is [True, False, False, False, True, False]
Scene graph at timestep 1240 is [True, False, False, False, True, False]
State prediction error at timestep 1240 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of 1
Current timestep = 1241. State = [[-0.10496058 -0.01895098]]. Action = [[ 0.14934337  0.07831842 -0.17677821  0.88703585]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1241 is [True, False, False, False, True, False]
Current timestep = 1242. State = [[-0.09740924 -0.01371638]]. Action = [[-0.20434904  0.03321809 -0.00078823  0.5968076 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1242 is [True, False, False, False, True, False]
Scene graph at timestep 1242 is [True, False, False, False, True, False]
State prediction error at timestep 1242 is tensor(3.1198e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1242 of 1
Current timestep = 1243. State = [[-0.10669119 -0.0051586 ]]. Action = [[-0.179187    0.08409017  0.24806309  0.87577355]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1243 is [True, False, False, False, True, False]
Scene graph at timestep 1243 is [True, False, False, False, True, False]
State prediction error at timestep 1243 is tensor(9.5709e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1243 of -1
Current timestep = 1244. State = [[-0.1330099   0.01327578]]. Action = [[-0.21935332  0.17161822  0.09845257 -0.8184718 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1244 is [True, False, False, False, True, False]
Scene graph at timestep 1244 is [True, False, False, False, True, False]
State prediction error at timestep 1244 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of -1
Current timestep = 1245. State = [[-0.15125094  0.02852955]]. Action = [[ 0.15348178  0.00753948 -0.19207664  0.919425  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1245 is [True, False, False, False, True, False]
Current timestep = 1246. State = [[-0.15034507  0.02911358]]. Action = [[-0.0602892   0.0078077   0.03200307 -0.8331966 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1246 is [True, False, False, False, True, False]
Scene graph at timestep 1246 is [True, False, False, False, True, False]
State prediction error at timestep 1246 is tensor(9.2134e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1246 of 0
Current timestep = 1247. State = [[-0.14906704  0.02476877]]. Action = [[ 0.05425707 -0.08959422  0.17994684  0.3581624 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1247 is [True, False, False, False, True, False]
Current timestep = 1248. State = [[-0.1497826   0.02596796]]. Action = [[-0.0245118   0.11074904  0.03838032  0.8731742 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1248 is [True, False, False, False, True, False]
Current timestep = 1249. State = [[-0.15054993  0.02883898]]. Action = [[ 0.00161725 -0.00361982  0.14059013 -0.32634592]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1249 is [True, False, False, False, True, False]
