Current timestep = 0. State = [[-0.22965097 -0.01858406]]. Action = [[-0.13565592  0.00675699  0.18866557 -0.05631596]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.22492681 -0.01760093]]. Action = [[0.2306419  0.04789957 0.24687213 0.5908774 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.21323107 -0.00237766]]. Action = [[ 0.0432092   0.21867192  0.18182561 -0.18628275]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.19820407  0.02618455]]. Action = [[0.23364931 0.23118639 0.0178239  0.14666975]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Current timestep = 4. State = [[-0.18289101  0.04268143]]. Action = [[-0.10863596 -0.03231508 -0.18620019  0.05609548]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.18509357  0.04699991]]. Action = [[-0.08203651  0.02032652  0.14625275  0.41066396]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.19384702  0.05267872]]. Action = [[-0.16613668  0.02428532  0.17820358  0.9277955 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.20978197  0.0610808 ]]. Action = [[-0.11634099  0.07294083  0.13745862  0.97219455]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.22114617  0.06290521]]. Action = [[ 0.08176962 -0.06011389  0.18047982  0.28969002]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Current timestep = 9. State = [[-0.21491553  0.04846502]]. Action = [[ 0.11475402 -0.16840668  0.22267565 -0.7321589 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Current timestep = 10. State = [[-0.21387681  0.0458656 ]]. Action = [[-0.05636783  0.15778404 -0.20094253 -0.46304524]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.20924525  0.03712903]]. Action = [[ 0.13160932 -0.24295096  0.02043861  0.2686906 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.20313518  0.0111737 ]]. Action = [[-0.05865118 -0.2211653   0.07113701 -0.44630414]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.21218827 -0.02117413]]. Action = [[-0.23629698 -0.23523359 -0.00662839  0.05992079]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Current timestep = 14. State = [[-0.22104861 -0.02946936]]. Action = [[-0.00443538  0.20325941 -0.10710773 -0.54546773]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.21939395 -0.0173584 ]]. Action = [[ 0.17449892  0.07579106 -0.01935583 -0.94450873]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.2040837 -0.0200776]]. Action = [[ 0.21935594 -0.17567617 -0.02608433  0.27522016]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Current timestep = 17. State = [[-0.19618776 -0.02915509]]. Action = [[-0.22884408 -0.00972088  0.12617737 -0.3648823 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.21280155 -0.02014212]]. Action = [[-0.24005601  0.21933043  0.07713771  0.275903  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Current timestep = 19. State = [[-0.24070086 -0.00758355]]. Action = [[-0.21430175 -0.03617281  0.22785354  0.31542385]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.2667524  -0.00768528]]. Action = [[-0.1269679  -0.14423712 -0.00281766  0.71073353]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
Current timestep = 21. State = [[-0.2667524  -0.00768528]]. Action = [[-0.11182851  0.1898095  -0.13730513  0.39072454]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
Current timestep = 22. State = [[-0.2667524  -0.00768528]]. Action = [[-0.14400755  0.24658895  0.15599275  0.8299577 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
Current timestep = 23. State = [[-0.2667524  -0.00768528]]. Action = [[-0.20274985 -0.16557184 -0.20851128  0.55982816]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
Current timestep = 24. State = [[-0.2667524  -0.00768528]]. Action = [[-0.17557687 -0.23031056  0.05521163  0.32924545]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
Current timestep = 25. State = [[-0.2599199  -0.00804885]]. Action = [[ 0.22056293 -0.02141756  0.18656486  0.40935254]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
Current timestep = 26. State = [[-0.24468815 -0.02099043]]. Action = [[ 0.16590995 -0.22465508 -0.11545643  0.2437588 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
Current timestep = 27. State = [[-0.22816555 -0.02544101]]. Action = [[0.05914709 0.22656333 0.19351256 0.4507805 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.22504522 -0.01681264]]. Action = [[-0.19105892 -0.04748008 -0.19843802 -0.46049815]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of 0
Current timestep = 29. State = [[-0.22338633 -0.02794105]]. Action = [[ 0.23697895 -0.17188443 -0.19105555 -0.60334706]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.20484318 -0.02705919]]. Action = [[ 0.23054427  0.19791803 -0.13651264  0.9576609 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.1915433  -0.01530112]]. Action = [[-0.11395772  0.06284755  0.13230342  0.44459057]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.20170732 -0.00071889]]. Action = [[-0.23985143  0.11016524 -0.11816683 -0.40029556]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.20936963  0.01999808]]. Action = [[0.17249405 0.20192814 0.12199101 0.82170296]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.19915105  0.04137018]]. Action = [[ 0.18996716  0.1297788  -0.24538366  0.620564  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.1861176   0.06491476]]. Action = [[-0.15945752  0.11213121 -0.02143295 -0.97598034]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.18911245  0.06580432]]. Action = [[ 0.01904011 -0.14133388 -0.03636694 -0.5595484 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 0
Current timestep = 37. State = [[-0.18777531  0.05834306]]. Action = [[ 0.11293641  0.02408051  0.1751898  -0.04713649]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.18017212  0.05121089]]. Action = [[ 0.11539364 -0.13991654 -0.02938342  0.09612536]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.16828969  0.0381363 ]]. Action = [[ 0.01128119 -0.07268515  0.06794026 -0.92027396]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.17145605  0.01993904]]. Action = [[-0.19074793 -0.21020451 -0.0504957  -0.77041715]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.17316103 -0.00450287]]. Action = [[ 0.06169027 -0.13825579  0.03719097 -0.7494572 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Current timestep = 42. State = [[-0.16939983 -0.01964506]]. Action = [[ 0.13127404 -0.05302335  0.05697495  0.67425513]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.15901107 -0.03156232]]. Action = [[ 0.13999194 -0.05680175  0.18707454 -0.24916881]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.14032175 -0.04181194]]. Action = [[ 0.12575793 -0.10118502  0.13212502 -0.44433963]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.11895745 -0.05843024]]. Action = [[ 0.22680792 -0.13526425  0.15825242  0.491907  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Current timestep = 46. State = [[-0.10633553 -0.06039959]]. Action = [[-0.2096954   0.14614809  0.06400582 -0.9471842 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Current timestep = 47. State = [[-0.10597452 -0.04688031]]. Action = [[ 0.15501499  0.14351311 -0.04863425 -0.3778572 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.10751421 -0.04233852]]. Action = [[-0.23089324 -0.09701505  0.0265694  -0.01297498]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.11922885 -0.05232215]]. Action = [[-0.08943889 -0.09406178  0.19513035 -0.91617864]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.11858167 -0.0460088 ]]. Action = [[ 0.22432917  0.24128306 -0.09294668  0.8710184 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.1077202  -0.03641852]]. Action = [[ 0.17029542 -0.15978959 -0.18644844 -0.10272944]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Current timestep = 52. State = [[-0.08371966 -0.04704484]]. Action = [[ 0.2383999  -0.06158061  0.20970434 -0.48644954]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.06422472 -0.06392901]]. Action = [[-0.20032081 -0.15497456 -0.0984704  -0.5318219 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.07754351 -0.06310181]]. Action = [[-0.2406557   0.2003411  -0.19424099  0.5975388 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.09282757 -0.03813123]]. Action = [[ 0.15388891  0.23708925 -0.04621147 -0.22137803]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.09464033 -0.01846903]]. Action = [[-0.1575199  -0.00050835  0.11494774 -0.43087363]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Current timestep = 57. State = [[-0.10077272 -0.00682676]]. Action = [[-0.07670641  0.15511692  0.05825773 -0.09600353]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.10175065  0.017148  ]]. Action = [[ 0.23769215  0.21556437  0.00474545 -0.60404223]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Current timestep = 59. State = [[-0.0983993   0.03258529]]. Action = [[-0.13800311 -0.01137751 -0.09528677 -0.5020531 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.09842152  0.0440013 ]]. Action = [[ 0.13033593  0.14689833  0.07805473 -0.07333338]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.0894971   0.05527106]]. Action = [[0.18968436 0.03494141 0.09379283 0.6111363 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-0.08062933  0.05369462]]. Action = [[-0.18971148 -0.1331386  -0.23311806  0.2938776 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.0804847   0.04647795]]. Action = [[ 0.11269617 -0.04124928  0.06329253 -0.54098   ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.08058175  0.04552241]]. Action = [[-0.18018726  0.04658869  0.05714613 -0.46347153]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.08391506  0.03261862]]. Action = [[ 0.01203582 -0.22716138  0.13925344  0.19638944]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.0898107   0.02780369]]. Action = [[-0.05699469  0.13969421 -0.21221828  0.94891596]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Current timestep = 67. State = [[-0.09046181  0.0210124 ]]. Action = [[ 0.07997268 -0.19356689 -0.03974538  0.3283838 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.08782219  0.00957903]]. Action = [[ 0.01132655 -0.01131311  0.24205351  0.38248944]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Current timestep = 69. State = [[-0.0964029   0.00380068]]. Action = [[-0.22596695 -0.05293861  0.22571853  0.07773066]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.12034319 -0.01563631]]. Action = [[-0.2212401  -0.18405081 -0.06699838 -0.67292994]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Current timestep = 71. State = [[-0.13635343 -0.0267351 ]]. Action = [[ 0.01455024  0.0113965   0.10327601 -0.61507714]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.13342422 -0.01543496]]. Action = [[ 0.1889692   0.23182446  0.19884223 -0.8784117 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.13430944 -0.00539318]]. Action = [[-0.15778813 -0.05201158 -0.20363127 -0.3713516 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.13067123 -0.0102013 ]]. Action = [[ 0.22074747 -0.10399865  0.24281672 -0.7240591 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.12799579 -0.01008514]]. Action = [[-0.08361766  0.10275322  0.14164937  0.9648435 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 0
Current timestep = 76. State = [[-0.13260344 -0.01251326]]. Action = [[-0.14297622 -0.11553267 -0.1968982   0.8545718 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.14358672 -0.03001619]]. Action = [[-0.12458055 -0.17175126 -0.12894532  0.085114  ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Current timestep = 78. State = [[-0.14739957 -0.05498455]]. Action = [[ 0.07751584 -0.21812241 -0.1849562  -0.83301073]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.14862919 -0.0814083 ]]. Action = [[ 0.02738491 -0.08842853  0.23652714 -0.09118998]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.1406788 -0.0957144]]. Action = [[ 0.22312537 -0.08853005 -0.17515041  0.4338827 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.12209471 -0.11179973]]. Action = [[ 0.18543679 -0.16188465 -0.02486131  0.03505051]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.09934936 -0.12976019]]. Action = [[ 0.1675787  -0.08284548  0.1897369  -0.66989213]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.08336818 -0.12823412]]. Action = [[-1.4717430e-03  1.6747996e-01 -2.9627979e-04 -8.3171952e-01]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
Current timestep = 84. State = [[-0.08187642 -0.12357462]]. Action = [[-0.09634531 -0.04265906  0.00815773 -0.20931423]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
Current timestep = 85. State = [[-0.07837036 -0.12875214]]. Action = [[ 0.16862062 -0.09599404  0.01771772 -0.6053748 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Current timestep = 86. State = [[-0.06656848 -0.12217318]]. Action = [[ 0.11547327  0.20578969  0.05657095 -0.59920007]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
Current timestep = 87. State = [[-0.0559076 -0.1194028]]. Action = [[-0.0445064  -0.14086907  0.14182821  0.32833517]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.06143809 -0.12614754]]. Action = [[-0.21981576 -0.00128371  0.08951703 -0.4761033 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Current timestep = 89. State = [[-0.07570284 -0.11810442]]. Action = [[-0.19336314  0.18825674  0.18555403  0.588521  ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.10053004 -0.0989158 ]]. Action = [[-0.11667752  0.08285269  0.2012518  -0.797525  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Current timestep = 91. State = [[-0.11468811 -0.10058323]]. Action = [[-0.11266939 -0.10345963 -0.04291448  0.27375233]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.12220045 -0.09591611]]. Action = [[ 0.11629742  0.16339803 -0.1650162   0.4039917 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.11356065 -0.07927234]]. Action = [[ 0.20914698  0.08969665 -0.03405932 -0.7708471 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.10413193 -0.07072866]]. Action = [[-0.09412453  0.00297111 -0.12999241  0.7428241 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.09891745 -0.07272848]]. Action = [[ 0.21345797 -0.07171217  0.24811888  0.31381226]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.07809458 -0.07872385]]. Action = [[ 0.24492437 -0.09181207  0.15696633  0.99322855]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.06139014 -0.08601097]]. Action = [[-0.05380601 -0.00628553 -0.05757838 -0.6804294 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.05925967 -0.09973692]]. Action = [[-0.02161664 -0.21073255 -0.05809502 -0.20518398]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.06711422 -0.12603031]]. Action = [[-0.23737904 -0.14932291  0.04163933 -0.4039817 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.0885154  -0.15016489]]. Action = [[-0.1800896  -0.14514911 -0.20897828 -0.68598413]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
Current timestep = 101. State = [[-0.10214165 -0.17217433]]. Action = [[ 0.04072559 -0.17433321 -0.16593285  0.803432  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
Current timestep = 102. State = [[-0.10604382 -0.17894687]]. Action = [[-0.12234962  0.16524172 -0.03533228 -0.6091843 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
Current timestep = 103. State = [[-0.10882796 -0.17062004]]. Action = [[0.02010322 0.05389458 0.19971734 0.8384036 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
Current timestep = 104. State = [[-0.10806929 -0.17495723]]. Action = [[ 0.143282   -0.18926264 -0.06262538  0.12324262]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
Current timestep = 105. State = [[-0.10751945 -0.18140951]]. Action = [[-0.07871807  0.04079548  0.19550437 -0.84353256]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
Current timestep = 106. State = [[-0.10008472 -0.17158493]]. Action = [[ 0.18812796  0.17472619 -0.03534108  0.1871916 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
Current timestep = 107. State = [[-0.10288451 -0.15653561]]. Action = [[-0.24176703  0.08365181  0.0584417  -0.24381334]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
Current timestep = 108. State = [[-0.10620037 -0.15771836]]. Action = [[ 0.03691497 -0.14720668 -0.21783991  0.5515938 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
Current timestep = 109. State = [[-0.11234488 -0.17027207]]. Action = [[-0.09316486 -0.10618478  0.21505296  0.5566186 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
Current timestep = 110. State = [[-0.11169463 -0.18143898]]. Action = [[ 0.24672565 -0.08547139 -0.13383755 -0.88952947]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
Current timestep = 111. State = [[-0.10358888 -0.20040181]]. Action = [[ 0.06462654 -0.2070299   0.1591464   0.9644108 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
Current timestep = 112. State = [[-0.08599018 -0.20815532]]. Action = [[ 0.14809614  0.14145231 -0.22989371  0.770509  ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.07134727 -0.21214248]]. Action = [[ 0.08849943 -0.18130776 -0.12951455  0.9544444 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0142, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.06423337 -0.22036175]]. Action = [[-0.22696856  0.1415016  -0.1549218   0.5584483 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
Current timestep = 115. State = [[-0.07113421 -0.22153084]]. Action = [[ 0.01123023 -0.12635733 -0.10577044 -0.7780275 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.07968544 -0.22589512]]. Action = [[-0.12372366  0.03522408 -0.17969917 -0.9733367 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
Current timestep = 117. State = [[-0.08253699 -0.23813471]]. Action = [[ 0.17551547 -0.24107507  0.16813862  0.7100917 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0158, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.08109641 -0.25114843]]. Action = [[ 0.01365069  0.05965626  0.23354024 -0.30902755]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0185, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.08079378 -0.23857296]]. Action = [[-0.14691147  0.23985687  0.14426565  0.9387772 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
Current timestep = 120. State = [[-0.08225928 -0.21969114]]. Action = [[ 0.06778324  0.03645295  0.235668   -0.9432017 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.07047991 -0.2012531 ]]. Action = [[ 0.21056402  0.19918382 -0.12556577  0.9731257 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
Current timestep = 122. State = [[-0.06954947 -0.1823132 ]]. Action = [[-0.2172223   0.07582191 -0.10632807  0.1856749 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
Current timestep = 123. State = [[-0.08123948 -0.17286664]]. Action = [[-0.14887778  0.04300576 -0.19283871 -0.96105796]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
Current timestep = 124. State = [[-0.08994315 -0.16891877]]. Action = [[ 0.05443448 -0.01523601 -0.11550465  0.46743178]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
Current timestep = 125. State = [[-0.10026595 -0.18102962]]. Action = [[-0.17884684 -0.21224989 -0.03698015  0.05482912]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
Current timestep = 126. State = [[-0.18862107 -0.16938716]]. Action = [[-0.21406496  0.21514857 -0.12109688  0.5272205 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 0
Current timestep = 127. State = [[-0.17965741 -0.17762312]]. Action = [[-0.06617206  0.2205435   0.10653019  0.27628446]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, True, False, False]
Current timestep = 128. State = [[-0.1799805  -0.16167255]]. Action = [[-0.01914798  0.09753901 -0.15813012 -0.2762277 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 128 is [True, False, False, True, False, False]
Current timestep = 129. State = [[-0.17216432 -0.14287405]]. Action = [[ 0.24439383  0.13717246  0.07871634 -0.370283  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 129 is [True, False, False, True, False, False]
Current timestep = 130. State = [[-0.16438815 -0.13189025]]. Action = [[-0.16071822  0.02041644 -0.16633987 -0.40059543]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 130 is [True, False, False, True, False, False]
Current timestep = 131. State = [[-0.1584504  -0.12113442]]. Action = [[ 0.2360909   0.10197234 -0.07598314  0.88065517]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.14110652 -0.09614028]]. Action = [[ 0.17186266  0.24426413  0.17939192 -0.7431592 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 132 is [True, False, False, False, True, False]
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.11324927 -0.0701358 ]]. Action = [[ 0.18035167  0.09699258 -0.0922786  -0.8753923 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 133 is [True, False, False, False, True, False]
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.08917139 -0.05904307]]. Action = [[ 0.1220414   0.05746692  0.12837982 -0.02764291]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 134 is [True, False, False, False, True, False]
Current timestep = 135. State = [[-0.08221849 -0.06105793]]. Action = [[-0.09905046 -0.11206686  0.04938003 -0.519293  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.0849314  -0.05246961]]. Action = [[-0.01051752  0.242805    0.1770612   0.5237578 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.09397981 -0.04209124]]. Action = [[-0.22209224 -0.11277211  0.12353876 -0.35422432]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 137 is [True, False, False, False, True, False]
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of 0
Current timestep = 138. State = [[-0.09822418 -0.05973824]]. Action = [[ 0.20194405 -0.1797727  -0.17425688  0.33886838]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.10020805 -0.0563144 ]]. Action = [[-0.24025448  0.23604399  0.08366472  0.6225567 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 139 is [True, False, False, False, True, False]
Current timestep = 140. State = [[-0.11302412 -0.03127428]]. Action = [[-0.11252674  0.19661999 -0.10657267 -0.6345361 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.12889943 -0.00681273]]. Action = [[-0.17516005  0.12858042 -0.16687733  0.5720556 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.13751063 -0.00465623]]. Action = [[ 0.2131297  -0.16887121  0.15819472  0.9572058 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.1411376  -0.02324223]]. Action = [[-0.21455134 -0.15756299  0.13028675  0.25691354]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.14914401 -0.03827778]]. Action = [[-0.00801893 -0.04432881  0.22059923 -0.431252  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.15853456 -0.05229218]]. Action = [[-0.1667236  -0.12091096  0.00978988  0.50568044]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.17185663 -0.0709578 ]]. Action = [[ 0.07742834 -0.13895535 -0.03827144  0.7752477 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 146 is [True, False, False, False, True, False]
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.1701363  -0.06969877]]. Action = [[ 0.01139009  0.21801805  0.02954316 -0.10955781]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.17044716 -0.06248044]]. Action = [[ 0.00636789 -0.09564158 -0.08557859  0.00663173]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 148 is [True, False, False, False, True, False]
Current timestep = 149. State = [[-0.16725443 -0.05625754]]. Action = [[ 0.11333734  0.14346105  0.18950361 -0.75585103]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.15814628 -0.0381422 ]]. Action = [[ 0.14026213  0.19306096 -0.15706217 -0.85539216]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 150 is [True, False, False, False, True, False]
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.14717239 -0.02784936]]. Action = [[-0.14852303 -0.15290579 -0.03580084 -0.46656847]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.15962274 -0.03596257]]. Action = [[-0.22867875  0.00443825  0.0535652  -0.23839772]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 152 is [True, False, False, False, True, False]
Current timestep = 153. State = [[-0.17921911 -0.03006518]]. Action = [[-0.08913082  0.12468097  0.13477144  0.27571678]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.18173052 -0.03435116]]. Action = [[ 0.1383039  -0.20471968  0.13138479 -0.37513065]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 154 is [True, False, False, False, True, False]
Current timestep = 155. State = [[-0.18153332 -0.03588832]]. Action = [[-0.06097481  0.14950997 -0.10936409 -0.1724127 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.19250426 -0.02459774]]. Action = [[-0.24470575  0.0815222  -0.20504174  0.4551208 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.21426931 -0.02715922]]. Action = [[-0.05158725 -0.17137536  0.0439814  -0.5910432 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 157 is [True, False, False, False, True, False]
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.23100981 -0.03328653]]. Action = [[-0.23041807  0.12901208 -0.23210704  0.30712545]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.2554158  -0.01210979]]. Action = [[ 0.1355873   0.2070727  -0.1080128   0.03544068]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 0
Current timestep = 160. State = [[-0.25289422  0.00543078]]. Action = [[-0.16523606  0.15038007 -0.13170187  0.3637358 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 160 is [True, False, False, False, True, False]
Current timestep = 161. State = [[-0.25206414  0.00458191]]. Action = [[-0.00987767 -0.05216007 -0.03062592  0.12958157]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 161 is [True, False, False, False, True, False]
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of 0
Current timestep = 162. State = [[-0.2519799   0.00372322]]. Action = [[-0.16074884  0.2045798   0.00371182 -0.6130427 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 162 is [True, False, False, False, True, False]
Current timestep = 163. State = [[-0.25236577  0.00535313]]. Action = [[ 0.00307807  0.06516945 -0.24676818  0.8804631 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.25044528 -0.00088147]]. Action = [[ 0.01276159 -0.15445639 -0.21042322 -0.23932004]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.24705522 -0.0143461 ]]. Action = [[ 0.03232044 -0.10493806 -0.10884199 -0.70185834]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 165 is [True, False, False, False, True, False]
Current timestep = 166. State = [[-0.24463944 -0.02149724]]. Action = [[-0.23568705  0.15362483  0.11632556  0.31927156]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 0
Current timestep = 167. State = [[-0.2511913  -0.03718011]]. Action = [[-0.15808842 -0.18828778  0.21637413  0.3303684 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 167 is [True, False, False, False, True, False]
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.2529623  -0.04245917]]. Action = [[ 0.18764961  0.21932256 -0.06514983  0.90400815]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.24850427 -0.02761065]]. Action = [[-0.18029974  0.01245201  0.08151817  0.7354307 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 169 is [True, False, False, False, True, False]
Current timestep = 170. State = [[-0.24540958 -0.02475155]]. Action = [[ 0.05844778  0.04462668  0.15624437 -0.61422646]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.24534263 -0.01457428]]. Action = [[-0.12704173  0.10147828  0.02088735 -0.821437  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.25163344 -0.01603514]]. Action = [[-0.09607044 -0.16141671  0.02224725  0.15724385]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 172 is [True, False, False, False, True, False]
Current timestep = 173. State = [[-0.25406757 -0.02640656]]. Action = [[ 0.0448415  -0.06052217 -0.21299213 -0.0571956 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.25359118 -0.02986584]]. Action = [[-0.22106597  0.20261657 -0.2015215  -0.22242308]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 174 is [True, False, False, False, True, False]
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of 0
Current timestep = 175. State = [[-0.25500938 -0.02627744]]. Action = [[ 0.00523952  0.12954289 -0.12231988  0.6868105 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 175 is [True, False, False, False, True, False]
Current timestep = 176. State = [[-0.25704584 -0.01839083]]. Action = [[-0.04386833  0.04465216  0.1796053  -0.8855651 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 176 is [True, False, False, False, True, False]
Current timestep = 177. State = [[-0.24979189 -0.02373925]]. Action = [[ 0.2312429  -0.18498531 -0.09999333  0.8456075 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.24093218 -0.04096878]]. Action = [[-0.03659111 -0.12497023 -0.0860967  -0.7000143 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.23353197 -0.05126664]]. Action = [[ 1.6506648e-01 -8.5934997e-05  1.6593587e-01 -8.1527239e-01]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 179 is [True, False, False, False, True, False]
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.21706507 -0.05083841]]. Action = [[ 0.11923862  0.04641739  0.13404006 -0.5693778 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 180 is [True, False, False, False, True, False]
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.19627386 -0.0402454 ]]. Action = [[ 0.20513359  0.16053179 -0.22291726  0.14969194]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 181 is [True, False, False, False, True, False]
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.1804691  -0.01427558]]. Action = [[-0.14126515  0.22179693  0.17115423 -0.71599686]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.18418077 -0.00061046]]. Action = [[ 0.01118788 -0.04138663  0.20524955 -0.6250873 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 183 is [True, False, False, False, True, False]
Current timestep = 184. State = [[-0.19306561  0.00580295]]. Action = [[-0.22595645  0.08968747 -0.10357684  0.10464931]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 184 is [True, False, False, False, True, False]
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.19963218  0.00464594]]. Action = [[ 0.24441218 -0.18172587  0.21609122 -0.30911458]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.18722889 -0.00150291]]. Action = [[ 0.11686927  0.09557199  0.19575244 -0.15953803]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.18021262 -0.00868097]]. Action = [[-0.11627276 -0.17934431  0.21500999  0.9275551 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 187 is [True, False, False, False, True, False]
Current timestep = 188. State = [[-0.18407601 -0.01697906]]. Action = [[-0.08361952  0.01314941  0.05734825 -0.87635535]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 188 is [True, False, False, False, True, False]
Current timestep = 189. State = [[-0.18403584 -0.00963549]]. Action = [[ 0.14281416  0.16807917 -0.20867267 -0.66870576]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.190193    0.00579332]]. Action = [[-0.23954862  0.11469951  0.23910278  0.57023907]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 190 is [True, False, False, False, True, False]
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.2116612   0.01151274]]. Action = [[-0.22470784 -0.11873221 -0.0529052  -0.53181463]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 191 is [True, False, False, False, True, False]
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.23622319  0.01435495]]. Action = [[-0.14361861  0.1999188   0.20831096 -0.89846987]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 192 is [True, False, False, False, True, False]
Current timestep = 193. State = [[-0.24119337  0.01557093]]. Action = [[ 0.11332908 -0.2148431   0.0316852  -0.40572655]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.23272824 -0.00255676]]. Action = [[ 0.18002254 -0.14313407  0.20209378 -0.50187737]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.21874301 -0.02271841]]. Action = [[ 0.15140456 -0.09545901 -0.10711972 -0.9224721 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.20270914 -0.04571927]]. Action = [[ 0.15659046 -0.24774386  0.17208424 -0.64373916]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.1945707  -0.06444109]]. Action = [[-0.2016927   0.01470852  0.23460582  0.5812713 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.19375204 -0.07298312]]. Action = [[ 0.18928373 -0.09763542  0.19056964 -0.9745786 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 198 is [True, False, False, False, True, False]
Current timestep = 199. State = [[-0.18024853 -0.0770994 ]]. Action = [[ 0.14893168  0.04618174 -0.08533728  0.41223836]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 199 is [True, False, False, False, True, False]
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.16726731 -0.08217607]]. Action = [[-0.05249782 -0.12580726  0.20349365 -0.97640854]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 200 is [True, False, False, False, True, False]
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 0
Current timestep = 201. State = [[-0.1683969  -0.08337675]]. Action = [[-0.07668251  0.17507023 -0.06154954  0.6500075 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.16655102 -0.08393309]]. Action = [[ 0.15492243 -0.16753794 -0.03682666 -0.27025366]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.16526592 -0.08186787]]. Action = [[-0.06927912  0.17282188 -0.07643116 -0.94613945]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 203 is [True, False, False, False, True, False]
Current timestep = 204. State = [[-0.16791335 -0.06209687]]. Action = [[-0.04435781  0.21740824 -0.0383189  -0.36704135]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.17528814 -0.03190747]]. Action = [[-0.15864436  0.2049888  -0.2093937  -0.8558986 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 205 is [True, False, False, False, True, False]
Current timestep = 206. State = [[-0.17972575 -0.00066612]]. Action = [[ 0.12542623  0.20882839 -0.1666469  -0.21460414]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.17544506  0.00817047]]. Action = [[ 0.10113063 -0.13449737  0.15284213  0.8676113 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 207 is [True, False, False, False, True, False]
Current timestep = 208. State = [[-0.1706495  -0.00023389]]. Action = [[-0.05430371 -0.09005406 -0.16933131  0.865538  ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 208 is [True, False, False, False, True, False]
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.1778612  -0.00847606]]. Action = [[-0.23966154 -0.0201796  -0.21932186  0.8568294 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.19395334 -0.0166794 ]]. Action = [[-0.09600478 -0.05671866  0.02725238 -0.65403396]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 210 is [True, False, False, False, True, False]
Current timestep = 211. State = [[-0.2142034  -0.02990901]]. Action = [[-0.2252635  -0.13264185  0.08615777 -0.74975395]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.22922015 -0.02689566]]. Action = [[0.07693636 0.2225112  0.22149345 0.7006111 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 212 is [True, False, False, False, True, False]
Current timestep = 213. State = [[-0.23657499 -0.00710702]]. Action = [[-0.14197715  0.09675983  0.06523293  0.51965666]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 213 is [True, False, False, False, True, False]
Current timestep = 214. State = [[-0.23687074 -0.00042209]]. Action = [[ 0.1698299  -0.06513985  0.17945617 -0.8818967 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 214 is [True, False, False, False, True, False]
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.23274677 -0.01184656]]. Action = [[-0.04096973 -0.16943453  0.1788491   0.4031825 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.24165739 -0.02099138]]. Action = [[-0.20591184  0.05147237 -0.21941888 -0.52021015]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 216 is [True, False, False, False, True, False]
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.26083133 -0.03546946]]. Action = [[-0.09010339 -0.20980994  0.20809346 -0.69870645]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 217 is [True, False, False, False, True, False]
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.2688068  -0.04022932]]. Action = [[ 0.15113562  0.22958133  0.06341496 -0.17903507]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 0
Current timestep = 219. State = [[-0.26406625 -0.02379798]]. Action = [[-0.22488868 -0.10666865 -0.19367102 -0.5536172 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 219 is [True, False, False, False, True, False]
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 0
Current timestep = 220. State = [[-0.264864   -0.01624435]]. Action = [[-0.02434216  0.1117146  -0.03315276 -0.8338695 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.26057583 -0.01128367]]. Action = [[ 0.14709532 -0.0577001  -0.06576127 -0.60080904]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.24313219 -0.01159836]]. Action = [[ 0.19421077 -0.00303696  0.22183496 -0.8953244 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 222 is [True, False, False, False, True, False]
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.21588027 -0.01533512]]. Action = [[ 0.18301213 -0.1017288   0.1146909  -0.3870306 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.20886269 -0.01236013]]. Action = [[-0.20333736  0.17530668 -0.14361952  0.96955895]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 224 is [True, False, False, False, True, False]
Current timestep = 225. State = [[-0.2133778  -0.00958816]]. Action = [[-0.01768509 -0.11074418 -0.21480106  0.39006722]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 225 is [True, False, False, False, True, False]
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.22367969 -0.01384985]]. Action = [[-0.13833715  0.01520029 -0.18765986 -0.19977081]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 226 is [True, False, False, False, True, False]
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of 1
Current timestep = 227. State = [[-0.2389566  -0.00769565]]. Action = [[-0.18984604  0.11611247  0.14897132  0.86220455]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.24931593 -0.00202941]]. Action = [[ 0.1768673  -0.05608566  0.03678408  0.83660936]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 228 is [True, False, False, False, True, False]
Current timestep = 229. State = [[-0.24049997  0.00791835]]. Action = [[ 0.11141288  0.21053046  0.10137141 -0.52862555]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.24089137  0.01185944]]. Action = [[-0.19879383 -0.13011049 -0.19134244  0.08323634]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 230 is [True, False, False, False, True, False]
Current timestep = 231. State = [[-0.24333213  0.01929992]]. Action = [[ 0.10892919  0.21329534 -0.01717018 -0.37320316]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of 0
Current timestep = 232. State = [[-0.24873264  0.04826536]]. Action = [[-0.17974973  0.24785364  0.12288228 -0.55204207]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.24940316  0.0587872 ]]. Action = [[ 0.20367175 -0.17411985 -0.235318    0.6500803 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 233 is [True, False, False, False, True, False]
Current timestep = 234. State = [[-0.24059963  0.06123689]]. Action = [[0.05764908 0.16378492 0.03672802 0.3132975 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 234 is [True, False, False, False, True, False]
Current timestep = 235. State = [[-0.24045937  0.06431596]]. Action = [[-0.13937746 -0.08276936  0.10219634  0.93536687]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 235 is [True, False, False, False, True, False]
Current timestep = 236. State = [[-0.25237256  0.06826288]]. Action = [[-0.19261718  0.09472406 -0.06236759 -0.85183626]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 236 is [True, False, False, False, True, False]
Current timestep = 237. State = [[-0.2646776   0.07317645]]. Action = [[-0.02331549 -0.02190661 -0.15130144  0.07891405]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 237 is [True, False, False, False, True, False]
Current timestep = 238. State = [[-0.2685333   0.07257082]]. Action = [[-0.08489558 -0.21338063  0.02320391 -0.9746852 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 238 is [True, False, False, False, True, False]
Current timestep = 239. State = [[-0.2639417   0.08176918]]. Action = [[ 0.17622691  0.18562338 -0.0330625   0.6799567 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.24972075  0.0811748 ]]. Action = [[ 0.23462206 -0.21224481  0.0070751   0.14238858]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 240 is [True, False, False, False, True, False]
Current timestep = 241. State = [[-0.23000787  0.07902171]]. Action = [[ 0.08475375  0.12857777 -0.03859791  0.44290924]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 241 is [True, False, False, False, True, False]
Current timestep = 242. State = [[-0.22370063  0.09689546]]. Action = [[-0.11358367  0.20485237  0.08369792  0.32717538]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 242 is [True, False, False, False, True, False]
Current timestep = 243. State = [[-0.22999729  0.10247893]]. Action = [[-0.09876704 -0.16788124 -0.12754574 -0.5165717 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 243 is [True, False, False, False, True, False]
Current timestep = 244. State = [[-0.22684066  0.10624585]]. Action = [[ 0.18889225  0.19339812 -0.01464705 -0.685337  ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 244 is [True, False, False, False, True, False]
Current timestep = 245. State = [[-0.2103765   0.12667362]]. Action = [[ 0.18657562  0.1652329  -0.23568815 -0.4460683 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 245 is [True, False, False, False, True, False]
Current timestep = 246. State = [[-0.19463453  0.13152511]]. Action = [[ 0.06440213 -0.16184008  0.08346906 -0.9612945 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 246 is [True, False, False, False, False, True]
Current timestep = 247. State = [[-0.18269378  0.11814123]]. Action = [[ 0.13088691 -0.10642684 -0.03214061  0.9279827 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 247 is [True, False, False, False, False, True]
Current timestep = 248. State = [[-0.17375799  0.11254443]]. Action = [[-0.11826327  0.0416958  -0.11545783 -0.6546456 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 248 is [True, False, False, False, True, False]
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.16480556  0.12499937]]. Action = [[ 0.23332798  0.19584161 -0.16099559  0.93440104]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 249 is [True, False, False, False, True, False]
Current timestep = 250. State = [[-0.15683478  0.13309321]]. Action = [[-0.19186649 -0.1176008  -0.04295839 -0.6749596 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 250 is [True, False, False, False, True, False]
Current timestep = 251. State = [[-0.15650794  0.14091872]]. Action = [[ 0.12144589  0.20574874  0.00099683 -0.36483663]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 251 is [True, False, False, False, False, True]
Current timestep = 252. State = [[-0.15317702  0.16223681]]. Action = [[ 0.02785009  0.18782589  0.12809253 -0.8607976 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 252 is [True, False, False, False, False, True]
Current timestep = 253. State = [[-0.14690837 -0.00295387]]. Action = [[-0.02338451 -0.22210824 -0.13999161 -0.60573494]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 253 is [True, False, False, False, False, True]
Current timestep = 254. State = [[-0.13221921 -0.01790681]]. Action = [[-0.04111031 -0.20048207 -0.19722886 -0.34925354]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 254 is [True, False, False, False, True, False]
Current timestep = 255. State = [[-0.13343912 -0.02796655]]. Action = [[-0.07163198  0.03361157  0.18741882  0.9296799 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 255 is [True, False, False, False, True, False]
Current timestep = 256. State = [[-0.14168663 -0.02459512]]. Action = [[-0.1896206   0.0692127  -0.00278641  0.5056908 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 256 is [True, False, False, False, True, False]
Current timestep = 257. State = [[-0.15029474 -0.01086244]]. Action = [[ 0.10501629  0.19071132 -0.15754756  0.23709011]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 257 is [True, False, False, False, True, False]
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 0
Current timestep = 258. State = [[-0.15456663  0.01083879]]. Action = [[-0.00523256  0.11094147 -0.05585954  0.42210782]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 258 is [True, False, False, False, True, False]
Current timestep = 259. State = [[-0.15846923  0.02634006]]. Action = [[-0.06832035  0.11260796  0.0931277   0.66239405]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 259 is [True, False, False, False, True, False]
Current timestep = 260. State = [[-0.16867389  0.03550481]]. Action = [[-0.24611345 -0.03218794  0.17891055 -0.710894  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 260 is [True, False, False, False, True, False]
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 0
Current timestep = 261. State = [[-0.1859191   0.04112773]]. Action = [[-0.08911753  0.05798122 -0.21588957  0.6588993 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 261 is [True, False, False, False, True, False]
Current timestep = 262. State = [[-0.2015754   0.05211833]]. Action = [[-0.15169555  0.10926339  0.23343867  0.5049412 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 262 is [True, False, False, False, True, False]
Current timestep = 263. State = [[-0.22363569  0.05254137]]. Action = [[-0.2087518  -0.12727493 -0.0278322  -0.23246175]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 263 is [True, False, False, False, True, False]
Current timestep = 264. State = [[-0.23714833  0.03105452]]. Action = [[ 0.11746949 -0.22052403 -0.0446481  -0.33081955]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 264 is [True, False, False, False, True, False]
Current timestep = 265. State = [[-0.24229744  0.00990187]]. Action = [[-0.15252341 -0.09763838  0.18808013  0.7006855 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 265 is [True, False, False, False, True, False]
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.25591615 -0.0135888 ]]. Action = [[-0.03516901 -0.16755767  0.04516989  0.4338212 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 266 is [True, False, False, False, True, False]
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.25899094 -0.02893931]]. Action = [[-0.11265445  0.21207261  0.12128741 -0.7871075 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.26018405 -0.02507891]]. Action = [[-0.01500079  0.07439953  0.06531066  0.8534677 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 268 is [True, False, False, False, True, False]
Current timestep = 269. State = [[-0.26100668 -0.02253663]]. Action = [[-0.1756585   0.06873435  0.19129789 -0.90346265]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 269 is [True, False, False, False, True, False]
Current timestep = 270. State = [[-0.25879043 -0.01033357]]. Action = [[ 0.13094395  0.1935617   0.04909146 -0.86006033]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.24561593  0.00439218]]. Action = [[ 0.23011017  0.0069313  -0.18942365  0.34712243]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 271 is [True, False, False, False, True, False]
Current timestep = 272. State = [[-0.22456187 -0.00270715]]. Action = [[ 0.13693339 -0.19679348 -0.03996003  0.42433083]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 272 is [True, False, False, False, True, False]
Current timestep = 273. State = [[-0.21005468 -0.02566739]]. Action = [[ 0.03893861 -0.17542611 -0.11481051  0.92519975]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 273 is [True, False, False, False, True, False]
Current timestep = 274. State = [[-0.20952235 -0.04037087]]. Action = [[-0.15116158 -0.00724004  0.14101392 -0.45696974]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 274 is [True, False, False, False, True, False]
Current timestep = 275. State = [[-0.2106678  -0.04087974]]. Action = [[0.09925723 0.07300043 0.09615326 0.2076981 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 275 is [True, False, False, False, True, False]
Current timestep = 276. State = [[-0.20484647 -0.04684082]]. Action = [[ 0.11113492 -0.14965372 -0.00617296  0.74245906]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 276 is [True, False, False, False, True, False]
Current timestep = 277. State = [[-0.19960538 -0.04869179]]. Action = [[0.01868856 0.11458573 0.18208092 0.23996925]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.19797744 -0.04869756]]. Action = [[-0.14003834 -0.07730691 -0.14524578 -0.9588373 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 278 is [True, False, False, False, True, False]
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.19445333 -0.05452839]]. Action = [[ 0.21401542 -0.05059066 -0.17680234 -0.83726054]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 279 is [True, False, False, False, True, False]
Current timestep = 280. State = [[-0.1849556  -0.06801762]]. Action = [[-0.02361932 -0.1844709  -0.15553968  0.58050466]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.18347839 -0.08102868]]. Action = [[-0.08647016  0.00143126  0.20377144 -0.08190274]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.1850988  -0.08176903]]. Action = [[-0.01184128  0.08099893 -0.15958944 -0.67327255]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 282 is [True, False, False, False, True, False]
Current timestep = 283. State = [[-0.19635132 -0.0670983 ]]. Action = [[-0.20665546  0.20866922  0.08004478 -0.8391723 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 283 is [True, False, False, False, True, False]
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.21327482 -0.03373103]]. Action = [[ 0.03294528  0.21842688 -0.07130897 -0.36372   ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 284 is [True, False, False, False, True, False]
Current timestep = 285. State = [[-0.21564212 -0.02565484]]. Action = [[-0.04983956 -0.12254408  0.2284837   0.7186153 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 285 is [True, False, False, False, True, False]
Current timestep = 286. State = [[-0.21784335 -0.03412364]]. Action = [[-0.0419693  -0.08693033 -0.02267507 -0.26441324]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.22479504 -0.04022966]]. Action = [[-0.1231844   0.02227613  0.10627139  0.8198519 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 287 is [True, False, False, False, True, False]
Current timestep = 288. State = [[-0.24290007 -0.05208728]]. Action = [[-0.18930092 -0.17831802  0.1191172   0.6009203 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.25625575 -0.06472601]]. Action = [[-0.21921729  0.01353729  0.02437741 -0.77840656]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 289 is [True, False, False, False, True, False]
Current timestep = 290. State = [[-0.26365444 -0.06153508]]. Action = [[-0.05785599  0.08192769  0.21757239 -0.9090941 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 290 is [True, False, False, False, True, False]
Current timestep = 291. State = [[-0.26039034 -0.06918189]]. Action = [[ 0.24017012 -0.21013314  0.14122796 -0.7337272 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 291 is [True, False, False, False, True, False]
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.24532832 -0.07244883]]. Action = [[0.15202475 0.20622963 0.11720777 0.6323174 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 292 is [True, False, False, False, True, False]
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.22281402 -0.06526645]]. Action = [[ 0.21228772 -0.13026583 -0.21721685  0.7901156 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 293 is [True, False, False, False, True, False]
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(8.2997e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[-0.20640841 -0.0651131 ]]. Action = [[-0.05918175  0.12349334  0.13022304  0.30335164]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 294 is [True, False, False, False, True, False]
Current timestep = 295. State = [[-0.21040517 -0.07422244]]. Action = [[-0.10999265 -0.23278317 -0.03819636 -0.1614151 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 295 is [True, False, False, False, True, False]
Current timestep = 296. State = [[-0.2088947  -0.08164767]]. Action = [[ 0.21414253  0.06954622  0.1146782  -0.4129653 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 296 is [True, False, False, False, True, False]
Current timestep = 297. State = [[-0.19986169 -0.07726575]]. Action = [[ 0.04292095  0.07099903 -0.10028552 -0.9123684 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 297 is [True, False, False, False, True, False]
Current timestep = 298. State = [[-0.18517761 -0.07400771]]. Action = [[ 0.22570008 -0.02341969 -0.16414066 -0.39686418]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 298 is [True, False, False, False, True, False]
Current timestep = 299. State = [[-0.17039275 -0.07314295]]. Action = [[-0.11303051  0.00844035 -0.02505341 -0.9460965 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 299 is [True, False, False, False, True, False]
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(1.8366e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.1662372  -0.07642521]]. Action = [[ 0.14068806 -0.08141476  0.13141945 -0.59529465]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 300 is [True, False, False, False, True, False]
Current timestep = 301. State = [[-0.16156311 -0.07428533]]. Action = [[-0.0499955   0.11322823  0.18343401 -0.11761934]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 301 is [True, False, False, False, True, False]
Current timestep = 302. State = [[-0.16534396 -0.08168803]]. Action = [[-0.14523973 -0.17757726  0.03071222  0.19391179]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 302 is [True, False, False, False, True, False]
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(9.2974e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.18317685 -0.10294316]]. Action = [[-0.22861454 -0.15635478 -0.0696905   0.50061953]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 303 is [True, False, False, False, True, False]
Current timestep = 304. State = [[-0.19231097 -0.1084214 ]]. Action = [[ 0.08295763  0.10519707  0.20896578 -0.30159986]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 304 is [True, False, False, False, True, False]
Current timestep = 305. State = [[-0.1832003  -0.11276313]]. Action = [[ 0.23964539 -0.1474067  -0.07058635 -0.45546412]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 305 is [True, False, False, False, True, False]
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(4.3387e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.17249952 -0.1322217 ]]. Action = [[-0.04291508 -0.20523459 -0.21318516 -0.1772067 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 306 is [True, False, False, False, True, False]
Scene graph at timestep 306 is [True, False, False, True, False, False]
State prediction error at timestep 306 is tensor(6.0057e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.16493978 -0.1555346 ]]. Action = [[ 0.19931448 -0.088992    0.22213328 -0.6245669 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 307 is [True, False, False, True, False, False]
Current timestep = 308. State = [[-0.14516525 -0.15105455]]. Action = [[ 0.11354268  0.1856651  -0.04437509 -0.8854869 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 308 is [True, False, False, True, False, False]
Current timestep = 309. State = [[-0.1379989  -0.14154032]]. Action = [[-0.09890831  0.04338819  0.22762546 -0.9182125 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 309 is [True, False, False, True, False, False]
Current timestep = 310. State = [[-0.13367687 -0.12802339]]. Action = [[0.14249516 0.14724782 0.05151954 0.9485376 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 310 is [True, False, False, True, False, False]
Current timestep = 311. State = [[-0.13319963 -0.12881981]]. Action = [[-0.20309824 -0.1794685  -0.035088   -0.6851337 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 311 is [True, False, False, True, False, False]
Current timestep = 312. State = [[-0.13363054 -0.13691556]]. Action = [[ 0.14733589 -0.04498467  0.07371199  0.6959233 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 312 is [True, False, False, True, False, False]
Current timestep = 313. State = [[-0.12489279 -0.15017311]]. Action = [[ 0.18290997 -0.1817607   0.16517639 -0.24685156]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 313 is [True, False, False, True, False, False]
Current timestep = 314. State = [[-0.11475376 -0.15731263]]. Action = [[-0.11650522  0.13848561 -0.11805172 -0.0160467 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 314 is [True, False, False, True, False, False]
Current timestep = 315. State = [[-0.12112287 -0.16734277]]. Action = [[-0.12656994 -0.21163592 -0.05212176  0.48143995]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 315 is [True, False, False, True, False, False]
Scene graph at timestep 315 is [True, False, False, True, False, False]
State prediction error at timestep 315 is tensor(1.0770e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of 1
Current timestep = 316. State = [[-0.12323655 -0.18322328]]. Action = [[ 0.20305216 -0.06391877  0.07217294  0.84871244]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 316 is [True, False, False, True, False, False]
Current timestep = 317. State = [[-0.10689479 -0.1905991 ]]. Action = [[ 0.19834703 -0.08245182  0.12427795  0.7286763 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 317 is [True, False, False, True, False, False]
Current timestep = 318. State = [[-0.08703335 -0.19200794]]. Action = [[ 0.01453388  0.11673504 -0.0256485  -0.2330659 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 318 is [True, False, False, True, False, False]
Current timestep = 319. State = [[-0.07277992 -0.19658525]]. Action = [[ 0.23236606 -0.15906328 -0.05897807  0.28909028]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 319 is [True, False, False, True, False, False]
Current timestep = 320. State = [[-0.05123277 -0.2154279 ]]. Action = [[ 0.05347145 -0.2060956   0.05538651 -0.6828776 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 320 is [True, False, False, True, False, False]
Current timestep = 321. State = [[-0.03042407 -0.2237827 ]]. Action = [[ 0.21197036  0.13565212 -0.15367639 -0.6258089 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 321 is [True, False, False, True, False, False]
Current timestep = 322. State = [[-0.01573299 -0.20815241]]. Action = [[-0.15800567  0.22792664 -0.21017991 -0.28062546]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 322 is [False, True, False, True, False, False]
Current timestep = 323. State = [[-0.01261477 -0.19943543]]. Action = [[ 0.17519265 -0.12978809  0.15984178  0.33233976]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 323 is [False, True, False, True, False, False]
Scene graph at timestep 323 is [False, True, False, True, False, False]
State prediction error at timestep 323 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.00643271 -0.21067506]]. Action = [[-0.01447137 -0.13739648  0.06220007  0.10964894]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 324 is [False, True, False, True, False, False]
Current timestep = 325. State = [[-0.00735937 -0.21824567]]. Action = [[-0.0968637   0.03368363 -0.01924035  0.8149593 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 325 is [False, True, False, True, False, False]
Current timestep = 326. State = [[-0.01386547 -0.21597694]]. Action = [[-0.21089397  0.1091305  -0.00180036  0.96786046]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 326 is [False, True, False, True, False, False]
Scene graph at timestep 326 is [False, True, False, True, False, False]
State prediction error at timestep 326 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.0267674  -0.21514215]]. Action = [[-0.02665326 -0.05289185  0.09082371  0.04037273]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 327 is [False, True, False, True, False, False]
Current timestep = 328. State = [[-0.02750089 -0.22344841]]. Action = [[ 0.13150495 -0.15389623 -0.18524311 -0.68508244]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 328 is [False, True, False, True, False, False]
Scene graph at timestep 328 is [False, True, False, True, False, False]
State prediction error at timestep 328 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.02418062 -0.22381994]]. Action = [[0.08483386 0.17424574 0.07255408 0.6378255 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 329 is [False, True, False, True, False, False]
Current timestep = 330. State = [[-0.02230637 -0.2211772 ]]. Action = [[ 0.02118433 -0.12691954  0.00708458  0.13814712]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 330 is [False, True, False, True, False, False]
Current timestep = 331. State = [[-0.01590815 -0.23464228]]. Action = [[ 0.22482777 -0.2301672   0.17981595  0.83819354]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 331 is [False, True, False, True, False, False]
Scene graph at timestep 331 is [False, True, False, True, False, False]
State prediction error at timestep 331 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[ 0.01645263 -0.26647204]]. Action = [[ 0.16022766 -0.16753921  0.17003974 -0.8775424 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 332 is [False, True, False, True, False, False]
Current timestep = 333. State = [[ 0.02793016 -0.28330767]]. Action = [[-0.24025817  0.00389752 -0.01066081  0.7886226 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 333 is [False, True, False, True, False, False]
Scene graph at timestep 333 is [False, True, False, True, False, False]
State prediction error at timestep 333 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[ 0.02321738 -0.28933963]]. Action = [[ 0.24517155 -0.24129134  0.05522561  0.5498712 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 334 is [False, True, False, True, False, False]
Current timestep = 335. State = [[ 0.02909258 -0.27868772]]. Action = [[ 0.20767963  0.16892838  0.1870662  -0.90302694]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 335 is [False, True, False, True, False, False]
Current timestep = 336. State = [[ 0.03324924 -0.26724708]]. Action = [[ 0.2157659  -0.16473384  0.20682257 -0.92056227]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 336 is [False, True, False, True, False, False]
Current timestep = 337. State = [[ 0.03348575 -0.26561937]]. Action = [[ 0.22853082 -0.11113335  0.11285898  0.5143511 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 337 is [False, True, False, True, False, False]
Current timestep = 338. State = [[ 0.03436201 -0.25794148]]. Action = [[-0.08857581  0.12200516  0.09341615 -0.8669089 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 338 is [False, True, False, True, False, False]
Current timestep = 339. State = [[ 0.03550627 -0.24890032]]. Action = [[ 0.05980736  0.02338901 -0.04597181 -0.52035785]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 339 is [False, True, False, True, False, False]
Current timestep = 340. State = [[ 0.035001   -0.24428843]]. Action = [[-0.12471551  0.04142806 -0.097321    0.65673757]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 340 is [False, True, False, True, False, False]
Current timestep = 341. State = [[ 0.02463502 -0.24986543]]. Action = [[-0.18741705 -0.11251909  0.23842588  0.75682414]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 341 is [False, True, False, True, False, False]
Scene graph at timestep 341 is [False, True, False, True, False, False]
State prediction error at timestep 341 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[ 0.01377823 -0.2645704 ]]. Action = [[ 0.18843612 -0.14968999 -0.18940924 -0.48741633]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 342 is [False, True, False, True, False, False]
Current timestep = 343. State = [[ 0.02416461 -0.26796448]]. Action = [[0.19302538 0.00166991 0.16638893 0.43503737]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 343 is [False, True, False, True, False, False]
Scene graph at timestep 343 is [False, True, False, True, False, False]
State prediction error at timestep 343 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[ 0.03881668 -0.2662516 ]]. Action = [[-0.11467254  0.0824042  -0.10340136  0.2051208 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 344 is [False, True, False, True, False, False]
Current timestep = 345. State = [[ 0.03444103 -0.27111736]]. Action = [[-0.13987213 -0.08777085 -0.12573622  0.7464652 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 345 is [False, True, False, True, False, False]
Current timestep = 346. State = [[ 0.02790617 -0.27715948]]. Action = [[ 0.18467188 -0.2431377   0.1620326   0.8733864 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 346 is [False, True, False, True, False, False]
Current timestep = 347. State = [[ 0.02228197 -0.28138754]]. Action = [[-0.06153791 -0.0455915   0.09465528 -0.46593922]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 347 is [False, True, False, True, False, False]
Current timestep = 348. State = [[ 0.02213654 -0.2734759 ]]. Action = [[ 0.13412938  0.15630418 -0.12071541 -0.96411633]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 348 is [False, True, False, True, False, False]
Current timestep = 349. State = [[ 0.02432735 -0.27050745]]. Action = [[ 0.06107026 -0.1393879   0.23580629  0.3537916 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 349 is [False, True, False, True, False, False]
Current timestep = 350. State = [[ 0.02370584 -0.2850959 ]]. Action = [[-0.03736772 -0.18979976  0.18967596  0.8600378 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 350 is [False, True, False, True, False, False]
Current timestep = 351. State = [[ 0.02056445 -0.29228666]]. Action = [[-0.14724354  0.1923356  -0.10789531 -0.8177355 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 351 is [False, True, False, True, False, False]
Current timestep = 352. State = [[ 0.01975698 -0.28774282]]. Action = [[ 0.13793266 -0.06217709  0.04327914  0.0143646 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 352 is [False, True, False, True, False, False]
Current timestep = 353. State = [[ 0.01993517 -0.28775078]]. Action = [[ 0.11795577 -0.17814076 -0.24491853 -0.6031549 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 353 is [False, True, False, True, False, False]
Current timestep = 354. State = [[ 0.01988902 -0.28773496]]. Action = [[ 0.03065139 -0.12380427  0.0841617   0.8491616 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 354 is [False, True, False, True, False, False]
Current timestep = 355. State = [[ 0.0308335  -0.28447345]]. Action = [[ 0.2328507   0.02661631  0.05502793 -0.61153316]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 355 is [False, True, False, True, False, False]
Current timestep = 356. State = [[ 0.04252804 -0.2789023 ]]. Action = [[-0.18780953  0.07961869 -0.19297294 -0.21626699]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 356 is [False, True, False, True, False, False]
Current timestep = 357. State = [[ 0.03835943 -0.27628523]]. Action = [[-0.12061334  0.05084997  0.05720282 -0.65243953]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 357 is [False, True, False, True, False, False]
Current timestep = 358. State = [[ 0.0329396 -0.2728605]]. Action = [[ 0.17945242  0.10016412 -0.10287464 -0.04250914]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 358 is [False, True, False, True, False, False]
Current timestep = 359. State = [[ 0.03477638 -0.263319  ]]. Action = [[ 0.13448465  0.10739607  0.24557728 -0.24464512]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 359 is [False, True, False, True, False, False]
Current timestep = 360. State = [[ 0.04246572 -0.24486472]]. Action = [[ 0.13933825  0.14494663 -0.14150706  0.68236756]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 360 is [False, True, False, True, False, False]
Current timestep = 361. State = [[ 0.04499787 -0.2321061 ]]. Action = [[ 0.21430051  0.21629441 -0.0470852   0.24622977]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 361 is [False, True, False, True, False, False]
Current timestep = 362. State = [[ 0.04531684 -0.23016392]]. Action = [[ 0.21389031 -0.03298426 -0.20105134  0.17873931]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 362 is [False, True, False, True, False, False]
Current timestep = 363. State = [[ 0.04212156 -0.23723331]]. Action = [[-0.1677632  -0.13695082  0.0707947  -0.7658262 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 363 is [False, True, False, True, False, False]
Current timestep = 364. State = [[ 0.0400013  -0.24469358]]. Action = [[ 0.24672532 -0.068197   -0.09384023 -0.7011898 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 364 is [False, True, False, True, False, False]
Current timestep = 365. State = [[ 0.03990189 -0.23425873]]. Action = [[-0.05981797  0.2414886  -0.19391555 -0.38284123]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 365 is [False, True, False, True, False, False]
Current timestep = 366. State = [[ 0.0392593  -0.22050491]]. Action = [[ 0.1687755  -0.03700221  0.17005885 -0.8420602 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 366 is [False, True, False, True, False, False]
Current timestep = 367. State = [[ 0.04032298 -0.21860093]]. Action = [[ 0.15315017 -0.03411514  0.18221691  0.29359186]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 367 is [False, True, False, True, False, False]
Current timestep = 368. State = [[ 0.04086978 -0.20812882]]. Action = [[-0.1331111   0.1913175   0.11254579 -0.85002863]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 368 is [False, True, False, True, False, False]
Current timestep = 369. State = [[ 0.03481901 -0.18611449]]. Action = [[-0.18712476  0.18734455  0.09212789  0.9582596 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 369 is [False, True, False, True, False, False]
Current timestep = 370. State = [[ 0.01568339 -0.16781387]]. Action = [[ 0.18936878  0.03070635 -0.01819625 -0.07227874]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 370 is [False, True, False, True, False, False]
Current timestep = 371. State = [[ 0.00538488 -0.15331322]]. Action = [[-0.18644251  0.21387812 -0.14205962  0.07620788]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 371 is [False, True, False, True, False, False]
Current timestep = 372. State = [[-0.00556609 -0.12348449]]. Action = [[ 0.15356305  0.15944284  0.2207908  -0.5072502 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 372 is [False, True, False, True, False, False]
Scene graph at timestep 372 is [False, True, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.00112179 -0.10882105]]. Action = [[ 0.14853609 -0.04877675 -0.14985693 -0.8083447 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 373 is [False, True, False, False, True, False]
Current timestep = 374. State = [[-0.00074606 -0.11081346]]. Action = [[-0.1858271  -0.00361748  0.12823218 -0.48306042]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 374 is [False, True, False, False, True, False]
Current timestep = 375. State = [[-0.01200433 -0.1165023 ]]. Action = [[-0.19672291 -0.02103697 -0.16823526  0.9746492 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 375 is [False, True, False, False, True, False]
Scene graph at timestep 375 is [False, True, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.02651058 -0.1301298 ]]. Action = [[ 0.12986785 -0.22721891 -0.0934388   0.3841746 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 376 is [False, True, False, False, True, False]
Scene graph at timestep 376 is [False, True, False, True, False, False]
State prediction error at timestep 376 is tensor(3.3599e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of -1
Current timestep = 377. State = [[-0.02441149 -0.14386563]]. Action = [[-0.00565293  0.05416644 -0.23532319  0.5628457 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 377 is [False, True, False, True, False, False]
Scene graph at timestep 377 is [False, True, False, True, False, False]
State prediction error at timestep 377 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 1
Current timestep = 378. State = [[-0.0217779  -0.13823119]]. Action = [[ 0.0827736   0.06526932  0.07860938 -0.35646904]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 378 is [False, True, False, True, False, False]
Current timestep = 379. State = [[-0.01193621 -0.13984244]]. Action = [[ 0.20341623 -0.12364356  0.15107691  0.5312736 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 379 is [False, True, False, True, False, False]
Current timestep = 380. State = [[-0.23230529  0.02926495]]. Action = [[ 0.06497085  0.05302584  0.11548346 -0.3279971 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 380 is [False, True, False, True, False, False]
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0434, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of 0
Current timestep = 381. State = [[-0.2237869   0.04858807]]. Action = [[ 0.07972515  0.23495895 -0.06881988  0.35599065]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 381 is [True, False, False, False, True, False]
Current timestep = 382. State = [[-0.22332211  0.07702079]]. Action = [[-0.15151621  0.19514772  0.1985006  -0.52464163]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 382 is [True, False, False, False, True, False]
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.24020645  0.10526669]]. Action = [[-0.17330132  0.08336011  0.14297208  0.06852221]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 383 is [True, False, False, False, True, False]
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.2482414   0.11475194]]. Action = [[ 0.00519338  0.01091066  0.20644706 -0.6332972 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 384 is [True, False, False, False, True, False]
Current timestep = 385. State = [[-0.23935167  0.10719033]]. Action = [[ 0.23710245 -0.12054953  0.00174361  0.79747295]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 385 is [True, False, False, False, True, False]
Current timestep = 386. State = [[-0.23430637  0.11297555]]. Action = [[-0.09461705  0.22700572 -0.19224337  0.8012676 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 386 is [True, False, False, False, True, False]
Current timestep = 387. State = [[-0.23586638  0.1346484 ]]. Action = [[ 0.08437431  0.17880619 -0.06106016 -0.55117893]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 387 is [True, False, False, False, True, False]
Current timestep = 388. State = [[-0.23630096  0.15355733]]. Action = [[-0.17698485  0.01347834  0.03293064  0.9765618 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 388 is [True, False, False, False, False, True]
Current timestep = 389. State = [[-0.24191993  0.15725705]]. Action = [[ 0.02246678 -0.01773149  0.11559644 -0.42175603]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 389 is [True, False, False, False, False, True]
Current timestep = 390. State = [[-0.24196799  0.15706933]]. Action = [[-0.22441056 -0.07601616 -0.05672181  0.70345783]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.24411511  0.15284972]]. Action = [[-0.10050336 -0.09235561  0.01879841 -0.1965121 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 391 is [True, False, False, False, False, True]
Scene graph at timestep 391 is [True, False, False, False, False, True]
State prediction error at timestep 391 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of 0
Current timestep = 392. State = [[-0.2508222   0.14959645]]. Action = [[-0.05012123  0.05827373 -0.19081868 -0.22266155]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 392 is [True, False, False, False, False, True]
Scene graph at timestep 392 is [True, False, False, False, False, True]
State prediction error at timestep 392 is tensor(7.0938e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of 0
Current timestep = 393. State = [[-0.2557341   0.15479666]]. Action = [[-0.22794414 -0.08604498  0.08224976 -0.4806    ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.25025353  0.16200964]]. Action = [[ 0.21250811  0.1770244  -0.04179023 -0.6873191 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 394 is [True, False, False, False, False, True]
Current timestep = 395. State = [[-0.23215762  0.18013111]]. Action = [[ 0.20235646  0.16058093 -0.10906997  0.48125672]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 395 is [True, False, False, False, False, True]
Scene graph at timestep 395 is [True, False, False, False, False, True]
State prediction error at timestep 395 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of 0
Current timestep = 396. State = [[-0.20499717  0.19144283]]. Action = [[ 0.09625518 -0.11563677 -0.08226955  0.8264899 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 396 is [True, False, False, False, False, True]
Scene graph at timestep 396 is [True, False, False, False, False, True]
State prediction error at timestep 396 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of 1
Current timestep = 397. State = [[-0.19211973  0.19087182]]. Action = [[ 0.16088188  0.16370255  0.11598754 -0.28359723]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 397 is [True, False, False, False, False, True]
Current timestep = 398. State = [[-0.17349982  0.19607653]]. Action = [[ 0.03920159 -0.11872374 -0.02577734  0.7650707 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 398 is [True, False, False, False, False, True]
Current timestep = 399. State = [[-0.1628952   0.17897798]]. Action = [[ 0.08777207 -0.21418665 -0.11139697 -0.35671127]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 399 is [True, False, False, False, False, True]
Current timestep = 400. State = [[-0.15410054  0.15398176]]. Action = [[-0.05465496 -0.19566925  0.2133221   0.71681166]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.15299064  0.15040565]]. Action = [[ 0.06627306  0.21589506 -0.11909837 -0.88066125]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 401 is [True, False, False, False, False, True]
Current timestep = 402. State = [[-0.15203662  0.16270417]]. Action = [[ 0.06769374  0.09951818 -0.09140629  0.04963887]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 402 is [True, False, False, False, False, True]
Current timestep = 403. State = [[-0.13368727  0.17918462]]. Action = [[0.23100635 0.13475347 0.03439575 0.15469623]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 403 is [True, False, False, False, False, True]
Current timestep = 404. State = [[-0.113925    0.17904685]]. Action = [[-0.04625721 -0.22311074 -0.0024492   0.7545345 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 404 is [True, False, False, False, False, True]
Current timestep = 405. State = [[-0.10824372  0.18084556]]. Action = [[ 0.12425444  0.21723384 -0.22435498  0.5361204 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 405 is [True, False, False, False, False, True]
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[-0.10124074  0.20610388]]. Action = [[-0.18785208  0.16257998 -0.20823386 -0.6052172 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 406 is [True, False, False, False, False, True]
Current timestep = 407. State = [[-0.11676423  0.22966808]]. Action = [[-0.06149292  0.16526246 -0.24694853  0.48745513]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 407 is [True, False, False, False, False, True]
Current timestep = 408. State = [[-0.12699433  0.24545015]]. Action = [[-0.08280404  0.01349795  0.15332198  0.80190384]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 408 is [True, False, False, False, False, True]
Current timestep = 409. State = [[-0.12781829  0.2522616 ]]. Action = [[ 0.1607151   0.10017949 -0.11882229  0.34519398]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 409 is [True, False, False, False, False, True]
Current timestep = 410. State = [[-0.1231277  0.2565046]]. Action = [[0.00141644 0.00903961 0.15290877 0.36864996]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 410 is [True, False, False, False, False, True]
Current timestep = 411. State = [[-0.11660848  0.24783081]]. Action = [[ 0.01356429 -0.21886012 -0.03680596  0.5559349 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 411 is [True, False, False, False, False, True]
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.10369993  0.22933576]]. Action = [[ 0.24593824 -0.07639226  0.21430695 -0.86558115]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 412 is [True, False, False, False, False, True]
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of 1
Current timestep = 413. State = [[-0.09031981  0.22858837]]. Action = [[-0.1972819   0.0739176   0.00852126  0.57420564]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 413 is [True, False, False, False, False, True]
Current timestep = 414. State = [[-0.09303193  0.22153687]]. Action = [[-0.0120396  -0.23007867 -0.06372648  0.2780329 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 414 is [True, False, False, False, False, True]
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.09218334  0.20675574]]. Action = [[ 0.03939     0.05796826  0.09533501 -0.46312326]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of 1
Current timestep = 416. State = [[-0.09016397  0.2148496 ]]. Action = [[ 0.16174373  0.15450549 -0.14421923 -0.90486246]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 416 is [True, False, False, False, False, True]
Current timestep = 417. State = [[-0.07677826  0.22409563]]. Action = [[ 0.16222295 -0.01119469 -0.04337829 -0.52167124]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 417 is [True, False, False, False, False, True]
Current timestep = 418. State = [[-0.06638284  0.22337767]]. Action = [[-0.12124136 -0.11353964  0.06352335  0.614171  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 418 is [True, False, False, False, False, True]
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.05984249  0.21994108]]. Action = [[ 0.2406542   0.09512216 -0.10334684 -0.7610252 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 419 is [True, False, False, False, False, True]
Current timestep = 420. State = [[-0.03803924  0.2237861 ]]. Action = [[ 0.21667469 -0.02966465 -0.16354804 -0.5943443 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 420 is [True, False, False, False, False, True]
Scene graph at timestep 420 is [False, True, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.00825395  0.23338233]]. Action = [[ 0.03283411  0.15241879  0.03905648 -0.729974  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 421 is [False, True, False, False, False, True]
Current timestep = 422. State = [[-0.00886358  0.25062582]]. Action = [[-0.1313425   0.14178365 -0.15124397 -0.8400576 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 422 is [False, True, False, False, False, True]
Current timestep = 423. State = [[-0.02239055  0.2715048 ]]. Action = [[-0.15871099  0.11679775  0.04889002 -0.93089795]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 423 is [False, True, False, False, False, True]
Current timestep = 424. State = [[-0.02819093  0.28579193]]. Action = [[ 0.15599725  0.09551209  0.03711638 -0.11044621]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 424 is [False, True, False, False, False, True]
Current timestep = 425. State = [[-0.01568875  0.29818913]]. Action = [[ 0.23926756  0.09185746 -0.16937083 -0.17598146]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 425 is [False, True, False, False, False, True]
Current timestep = 426. State = [[0.00194842 0.30822283]]. Action = [[0.18143174 0.09068543 0.07533303 0.81936646]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 426 is [False, True, False, False, False, True]
Current timestep = 427. State = [[0.01756681 0.3028201 ]]. Action = [[ 0.24490112 -0.12369949  0.18855467 -0.7044446 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 427 is [False, True, False, False, False, True]
Scene graph at timestep 427 is [False, True, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[0.04378388 0.29117385]]. Action = [[-0.00226751  0.03657484 -0.23392427  0.8162496 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 428 is [False, True, False, False, False, True]
Current timestep = 429. State = [[0.04294201 0.28847876]]. Action = [[-0.19311993 -0.1962529  -0.19086918  0.960739  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 429 is [False, True, False, False, False, True]
Current timestep = 430. State = [[0.04297685 0.28394488]]. Action = [[-0.13110974  0.22339863  0.18809295  0.7581558 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 430 is [False, True, False, False, False, True]
Current timestep = 431. State = [[0.04360707 0.2822058 ]]. Action = [[ 0.22597694  0.17759538 -0.21258208 -0.7388153 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 431 is [False, True, False, False, False, True]
Current timestep = 432. State = [[0.04360653 0.28214306]]. Action = [[0.11601499 0.14210081 0.21551955 0.36663854]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 432 is [False, True, False, False, False, True]
Current timestep = 433. State = [[0.0462056  0.27314213]]. Action = [[ 0.01957121 -0.15583959 -0.05241832  0.9268892 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 433 is [False, True, False, False, False, True]
Current timestep = 434. State = [[0.04568478 0.25897878]]. Action = [[-0.07493739 -0.07877707 -0.21162057 -0.19424582]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 434 is [False, True, False, False, False, True]
Current timestep = 435. State = [[0.04214876 0.24810877]]. Action = [[-0.08403742 -0.06586185  0.02641672  0.5714185 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 435 is [False, True, False, False, False, True]
Current timestep = 436. State = [[0.02840214 0.2484769 ]]. Action = [[-0.21223031  0.09240565 -0.10658634  0.05617476]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 436 is [False, True, False, False, False, True]
Current timestep = 437. State = [[0.00229088 0.25157365]]. Action = [[-0.21048652 -0.07463291  0.22391444  0.16674674]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 437 is [False, True, False, False, False, True]
Current timestep = 438. State = [[-0.01566501  0.25003758]]. Action = [[ 0.13738209  0.08404613 -0.06241393  0.88030446]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 438 is [False, True, False, False, False, True]
Current timestep = 439. State = [[-0.01547594  0.2537778 ]]. Action = [[ 0.05775958  0.07328168  0.17651907 -0.05247343]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 439 is [False, True, False, False, False, True]
Current timestep = 440. State = [[-0.01479907  0.26476282]]. Action = [[ 0.09480876  0.17471087 -0.08872548 -0.48667252]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 440 is [False, True, False, False, False, True]
Current timestep = 441. State = [[-0.0072679  0.2694194]]. Action = [[ 0.15796584 -0.08333093 -0.19266844  0.05286944]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 441 is [False, True, False, False, False, True]
Current timestep = 442. State = [[0.00427787 0.2612661 ]]. Action = [[ 0.09889117 -0.09853749 -0.16206561 -0.627563  ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 442 is [False, True, False, False, False, True]
Scene graph at timestep 442 is [False, True, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[0.01550574 0.2552715 ]]. Action = [[-0.13736615 -0.01546419 -0.17672041  0.32361126]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 443 is [False, True, False, False, False, True]
Current timestep = 444. State = [[0.01093537 0.2646015 ]]. Action = [[-0.02330905  0.16464308  0.20251632 -0.6976975 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 444 is [False, True, False, False, False, True]
Current timestep = 445. State = [[0.00960418 0.262615  ]]. Action = [[-0.01988623 -0.21642365  0.09921831  0.92036915]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 445 is [False, True, False, False, False, True]
Scene graph at timestep 445 is [False, True, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of 1
Current timestep = 446. State = [[0.00366565 0.25147456]]. Action = [[-0.22981793 -0.03616565  0.02242133  0.61790454]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 446 is [False, True, False, False, False, True]
Scene graph at timestep 446 is [False, True, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of 0
Current timestep = 447. State = [[-0.01519563  0.25893667]]. Action = [[-0.05461575  0.21013221 -0.15188198  0.5636411 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 447 is [False, True, False, False, False, True]
Current timestep = 448. State = [[-0.02890994  0.2742506 ]]. Action = [[-0.1465758  -0.02076107  0.13352185 -0.57012653]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 448 is [False, True, False, False, False, True]
Current timestep = 449. State = [[-0.04170718  0.2762046 ]]. Action = [[-0.15563288 -0.06431952  0.1058856   0.16350615]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 449 is [False, True, False, False, False, True]
Scene graph at timestep 449 is [False, True, False, False, False, True]
State prediction error at timestep 449 is tensor(4.0227e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.06285623  0.26857373]]. Action = [[-0.23260339  0.22431344  0.14336711  0.9315089 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 450 is [False, True, False, False, False, True]
Current timestep = 451. State = [[-0.06287932  0.2685265 ]]. Action = [[0.17611235 0.22630957 0.13996798 0.60920835]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.06648088  0.25780848]]. Action = [[-0.12461685 -0.18577419  0.14776528 -0.49636728]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 452 is [True, False, False, False, False, True]
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of 1
Current timestep = 453. State = [[-0.0926585   0.24803647]]. Action = [[-0.14241661  0.19634598 -0.12789986 -0.01838601]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 453 is [True, False, False, False, False, True]
Current timestep = 454. State = [[-0.10023797  0.25411108]]. Action = [[ 0.18666309 -0.12061927 -0.16762894  0.13225377]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 0
Current timestep = 455. State = [[-0.09887327  0.25552168]]. Action = [[-0.08437288  0.14165825 -0.1589492  -0.9239189 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 455 is [True, False, False, False, False, True]
Scene graph at timestep 455 is [True, False, False, False, False, True]
State prediction error at timestep 455 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.09614855  0.2503233 ]]. Action = [[ 0.1779542  -0.21378131 -0.18362114 -0.6086518 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 456 is [True, False, False, False, False, True]
Scene graph at timestep 456 is [True, False, False, False, False, True]
State prediction error at timestep 456 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of 1
Current timestep = 457. State = [[-0.08693352  0.23550697]]. Action = [[ 0.01054233  0.03179815 -0.1756138   0.53902376]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.08063094  0.23848896]]. Action = [[ 0.18384558  0.06079099 -0.17827328 -0.47975338]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.07860036  0.25018302]]. Action = [[-0.19888017  0.12742043  0.23624179 -0.9334821 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 459 is [True, False, False, False, False, True]
Current timestep = 460. State = [[-0.08826686  0.26480624]]. Action = [[-0.09392753  0.08068824  0.10149026  0.4583075 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 460 is [True, False, False, False, False, True]
Current timestep = 461. State = [[-0.08833382  0.2791255 ]]. Action = [[ 0.21805274  0.13330689 -0.24076425  0.85770965]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.08307447  0.28868252]]. Action = [[-0.06705663  0.23714018  0.08944935  0.19779015]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.07865585  0.28317678]]. Action = [[ 0.01387221 -0.15858583 -0.09807512  0.6388252 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 463 is [True, False, False, False, False, True]
Current timestep = 464. State = [[-0.07674551  0.2753397 ]]. Action = [[-0.18384907 -0.00982888  0.02071971 -0.8419292 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 464 is [True, False, False, False, False, True]
Scene graph at timestep 464 is [True, False, False, False, False, True]
State prediction error at timestep 464 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of 0
Current timestep = 465. State = [[-0.07555228  0.26224372]]. Action = [[ 0.19579625 -0.16873634  0.10129228 -0.90601605]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 465 is [True, False, False, False, False, True]
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.06117984  0.24238192]]. Action = [[ 0.14717788 -0.06794196 -0.2007107  -0.84509516]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 466 is [True, False, False, False, False, True]
Current timestep = 467. State = [[-0.05400969  0.23877123]]. Action = [[ 0.05873111  0.0819068  -0.03670935  0.5866947 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 467 is [True, False, False, False, False, True]
Current timestep = 468. State = [[-0.04585176  0.23995349]]. Action = [[ 0.09057114 -0.03841774 -0.23367536  0.8723717 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 468 is [True, False, False, False, False, True]
Current timestep = 469. State = [[-0.0377177   0.24573459]]. Action = [[-0.021256    0.13341641 -0.23876879 -0.20580578]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 469 is [False, True, False, False, False, True]
Current timestep = 470. State = [[-0.04288644  0.26318306]]. Action = [[-0.22277586  0.14928311  0.15763336 -0.4291944 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 470 is [False, True, False, False, False, True]
Current timestep = 471. State = [[-0.04840213  0.28600743]]. Action = [[ 0.20509946  0.1808987   0.09883204 -0.58277464]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 471 is [False, True, False, False, False, True]
Current timestep = 472. State = [[-0.04267693  0.29305092]]. Action = [[-0.09263164 -0.16514865 -0.00636148 -0.45583242]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 472 is [False, True, False, False, False, True]
Scene graph at timestep 472 is [False, True, False, False, False, True]
State prediction error at timestep 472 is tensor(1.1237e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of 1
Current timestep = 473. State = [[-0.04213762  0.28930938]]. Action = [[-0.09238666  0.09235892  0.22712857 -0.5358746 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 473 is [False, True, False, False, False, True]
Current timestep = 474. State = [[-0.04637247  0.28911465]]. Action = [[-0.16797832 -0.00533615  0.18596661 -0.7884191 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 474 is [False, True, False, False, False, True]
Current timestep = 475. State = [[-0.05275752  0.2873472 ]]. Action = [[ 0.1093868   0.18599105  0.13495317 -0.09169108]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 475 is [False, True, False, False, False, True]
Current timestep = 476. State = [[-0.04632412  0.2755654 ]]. Action = [[ 0.22545505 -0.19948325  0.01726252  0.98265135]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 476 is [True, False, False, False, False, True]
Scene graph at timestep 476 is [False, True, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of 0
Current timestep = 477. State = [[-0.03041163  0.24643402]]. Action = [[ 0.16788608 -0.19492942 -0.0831324   0.05157435]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 477 is [False, True, False, False, False, True]
Scene graph at timestep 477 is [False, True, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of 1
Current timestep = 478. State = [[-0.02458002  0.23473585]]. Action = [[-0.23950475  0.1253722  -0.03364958 -0.12651962]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 478 is [False, True, False, False, False, True]
Current timestep = 479. State = [[-0.0267461   0.22946692]]. Action = [[ 0.1725204  -0.22625153  0.22381186  0.6982299 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 479 is [False, True, False, False, False, True]
Current timestep = 480. State = [[-0.01429821  0.20855327]]. Action = [[ 0.20563704 -0.10241723  0.0093556   0.07329166]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 480 is [False, True, False, False, False, True]
Current timestep = 481. State = [[0.00115736 0.20896432]]. Action = [[ 0.06199971  0.194166   -0.14901389  0.8314291 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 481 is [False, True, False, False, False, True]
Current timestep = 482. State = [[0.01157091 0.23246425]]. Action = [[ 0.12538362  0.19577947 -0.02474657  0.17001092]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 482 is [False, True, False, False, False, True]
Scene graph at timestep 482 is [False, True, False, False, False, True]
State prediction error at timestep 482 is tensor(2.0715e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[0.02605778 0.24810317]]. Action = [[-0.16828805 -0.07784349  0.21474314 -0.60007364]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 483 is [False, True, False, False, False, True]
Scene graph at timestep 483 is [False, True, False, False, False, True]
State prediction error at timestep 483 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 1
Current timestep = 484. State = [[0.02365103 0.24884614]]. Action = [[0.01199239 0.08877778 0.17201471 0.24562979]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 484 is [False, True, False, False, False, True]
Current timestep = 485. State = [[0.01857074 0.2642426 ]]. Action = [[-0.04570989  0.20867598  0.17141065 -0.21854842]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 485 is [False, True, False, False, False, True]
Current timestep = 486. State = [[0.00973672 0.26953512]]. Action = [[-0.13043119 -0.17103846  0.15338701  0.7428591 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 486 is [False, True, False, False, False, True]
Current timestep = 487. State = [[0.00811515 0.26856253]]. Action = [[0.11078918 0.10184538 0.05726796 0.39331555]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 487 is [False, True, False, False, False, True]
Current timestep = 488. State = [[0.01380514 0.25783098]]. Action = [[ 0.15356416 -0.2424011  -0.13701278 -0.3088674 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 488 is [False, True, False, False, False, True]
Scene graph at timestep 488 is [False, True, False, False, False, True]
State prediction error at timestep 488 is tensor(4.3106e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 0
Current timestep = 489. State = [[0.02749899 0.25142264]]. Action = [[ 0.1261186   0.19332945 -0.15036009  0.40222418]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 489 is [False, True, False, False, False, True]
Current timestep = 490. State = [[0.03360605 0.26215872]]. Action = [[ 0.23164457  0.11810696 -0.16274528 -0.32872736]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 490 is [False, True, False, False, False, True]
Current timestep = 491. State = [[0.03440288 0.26403165]]. Action = [[0.22807193 0.14223403 0.16630012 0.68808365]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 491 is [False, True, False, False, False, True]
Current timestep = 492. State = [[0.03848079 0.25324616]]. Action = [[ 0.07340646 -0.22408761 -0.14425962 -0.43673038]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 492 is [False, True, False, False, False, True]
Current timestep = 493. State = [[0.05031956 0.2515293 ]]. Action = [[ 0.13535827  0.2445373   0.00445831 -0.9425501 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 493 is [False, True, False, False, False, True]
Current timestep = 494. State = [[0.05510275 0.2761272 ]]. Action = [[-0.22095785  0.1952216   0.23128617  0.35469317]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 494 is [False, False, True, False, False, True]
Current timestep = 495. State = [[0.04005579 0.30011383]]. Action = [[-0.16801408  0.09402162  0.05650741  0.14162493]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 495 is [False, False, True, False, False, True]
Current timestep = 496. State = [[0.0320279 0.3119437]]. Action = [[-0.11383812  0.17952138 -0.07769334 -0.8619828 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 496 is [False, True, False, False, False, True]
Scene graph at timestep 496 is [False, True, False, False, False, True]
State prediction error at timestep 496 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[0.02827298 0.3095487 ]]. Action = [[-0.11878598 -0.09906459 -0.16371259  0.00499129]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 497 is [False, True, False, False, False, True]
Current timestep = 498. State = [[0.018962  0.2917534]]. Action = [[-0.16816537 -0.24262689 -0.24748297  0.6231047 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 498 is [False, True, False, False, False, True]
Current timestep = 499. State = [[-0.00336805  0.26462972]]. Action = [[-0.15904647 -0.1773814   0.22250378  0.89831924]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 499 is [False, True, False, False, False, True]
Scene graph at timestep 499 is [False, True, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.03159317  0.24949324]]. Action = [[-0.06729031  0.12253058  0.13312072 -0.05112141]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 500 is [False, True, False, False, False, True]
Current timestep = 501. State = [[-0.04221331  0.24924491]]. Action = [[-0.19444549 -0.17180115 -0.11382017 -0.7545433 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 501 is [False, True, False, False, False, True]
Current timestep = 502. State = [[-0.06504388  0.2431523 ]]. Action = [[-0.09693635  0.05673963 -0.14479885  0.5928894 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 502 is [False, True, False, False, False, True]
Current timestep = 503. State = [[-0.07452516  0.23681995]]. Action = [[ 0.13787878 -0.09966373 -0.07474299 -0.751905  ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 503 is [True, False, False, False, False, True]
Current timestep = 504. State = [[-0.07205536  0.2176344 ]]. Action = [[ 0.00037262 -0.21444009  0.08881497 -0.34019142]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 504 is [True, False, False, False, False, True]
Current timestep = 505. State = [[-0.06726497  0.1885545 ]]. Action = [[ 0.1775975  -0.19311906 -0.1496724   0.91355693]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 505 is [True, False, False, False, False, True]
Scene graph at timestep 505 is [True, False, False, False, False, True]
State prediction error at timestep 505 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 505 of 1
Current timestep = 506. State = [[-0.05767659  0.16428448]]. Action = [[-0.17158799 -0.02325732  0.1923624  -0.45005858]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 506 is [True, False, False, False, False, True]
Current timestep = 507. State = [[-0.24639288  0.12211854]]. Action = [[-0.1697236   0.02522734  0.08174068  0.20529485]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 507 is [True, False, False, False, False, True]
Current timestep = 508. State = [[-0.23401254  0.1503582 ]]. Action = [[ 0.17757577  0.24513316 -0.18885416 -0.38256884]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 508 is [True, False, False, False, True, False]
Current timestep = 509. State = [[-0.21233764  0.17007022]]. Action = [[ 0.13299316  0.03028113 -0.04330294 -0.6627854 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 509 is [True, False, False, False, False, True]
Scene graph at timestep 509 is [True, False, False, False, False, True]
State prediction error at timestep 509 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 0
Current timestep = 510. State = [[-0.19462618  0.16622241]]. Action = [[-0.0286001  -0.21528943  0.00912669  0.00792313]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 510 is [True, False, False, False, False, True]
Current timestep = 511. State = [[-0.19650704  0.16631204]]. Action = [[ 0.00827324  0.2139881   0.14513391 -0.92099035]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 511 is [True, False, False, False, False, True]
Current timestep = 512. State = [[-0.19943912  0.16631411]]. Action = [[-0.17857061 -0.21376196 -0.12769996 -0.22678864]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 512 is [True, False, False, False, False, True]
Current timestep = 513. State = [[-0.21117724  0.1684382 ]]. Action = [[-0.13342719  0.16187882  0.13836429  0.1092875 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 513 is [True, False, False, False, False, True]
Scene graph at timestep 513 is [True, False, False, False, False, True]
State prediction error at timestep 513 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.23578204  0.16601749]]. Action = [[-0.21673463 -0.22233252  0.15129471  0.82108545]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 514 is [True, False, False, False, False, True]
Current timestep = 515. State = [[-0.2461743   0.15832555]]. Action = [[ 0.13353863  0.14048538 -0.20333509 -0.05887455]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 515 is [True, False, False, False, False, True]
Current timestep = 516. State = [[-0.2424119   0.15403657]]. Action = [[ 0.06467238 -0.15295054  0.03631639  0.8418914 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 516 is [True, False, False, False, False, True]
Current timestep = 517. State = [[-0.23664133  0.1396197 ]]. Action = [[ 0.03551698 -0.10730606  0.1829431  -0.71040803]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 517 is [True, False, False, False, False, True]
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.22394645  0.14016934]]. Action = [[ 0.23375115  0.21557558  0.23805523 -0.41472512]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 518 is [True, False, False, False, False, True]
Current timestep = 519. State = [[-0.20597184  0.1637053 ]]. Action = [[ 0.07349154  0.20219612  0.22189066 -0.84558105]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 519 is [True, False, False, False, False, True]
Current timestep = 520. State = [[-0.19878218  0.18691997]]. Action = [[ 0.00514057  0.1356636  -0.06963545  0.49855077]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 520 is [True, False, False, False, False, True]
Current timestep = 521. State = [[-0.18580809  0.19384739]]. Action = [[ 0.20786616 -0.09324421 -0.2197838  -0.51123035]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 521 is [True, False, False, False, False, True]
Current timestep = 522. State = [[-0.17331548  0.1960109 ]]. Action = [[-0.04033205  0.06611592  0.13299844  0.11507785]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 522 is [True, False, False, False, False, True]
Current timestep = 523. State = [[-0.17477663  0.19512795]]. Action = [[-0.12544636 -0.09977883  0.08285779  0.155509  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 523 is [True, False, False, False, False, True]
Scene graph at timestep 523 is [True, False, False, False, False, True]
State prediction error at timestep 523 is tensor(1.0887e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 1
Current timestep = 524. State = [[-0.17161562  0.19559133]]. Action = [[ 0.2000278   0.10540622 -0.1731797   0.63963056]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 524 is [True, False, False, False, False, True]
Current timestep = 525. State = [[-0.15314351  0.18571141]]. Action = [[ 0.1895496  -0.22947565 -0.20413615  0.23790157]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 525 is [True, False, False, False, False, True]
Scene graph at timestep 525 is [True, False, False, False, False, True]
State prediction error at timestep 525 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of 1
Current timestep = 526. State = [[-0.14324561  0.17854486]]. Action = [[-0.1152498   0.13186342  0.24384701  0.19938433]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 526 is [True, False, False, False, False, True]
Current timestep = 527. State = [[-0.14061403  0.1871925 ]]. Action = [[ 0.14627248  0.05229282 -0.02965048  0.5301671 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 527 is [True, False, False, False, False, True]
Current timestep = 528. State = [[-0.13765779  0.20197594]]. Action = [[-0.1524207   0.14572194  0.06430107 -0.3003081 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 528 is [True, False, False, False, False, True]
Current timestep = 529. State = [[-0.145862    0.21212995]]. Action = [[-0.10413691 -0.07005244 -0.23259538 -0.95833296]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 529 is [True, False, False, False, False, True]
Current timestep = 530. State = [[-0.14538331  0.19950253]]. Action = [[ 0.02343133 -0.21967581 -0.10272221 -0.49439514]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 530 is [True, False, False, False, False, True]
Current timestep = 531. State = [[-0.15166335  0.19109967]]. Action = [[-0.18959226  0.05658832 -0.08415116 -0.09803468]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 531 is [True, False, False, False, False, True]
Scene graph at timestep 531 is [True, False, False, False, False, True]
State prediction error at timestep 531 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 531 of -1
Current timestep = 532. State = [[-0.16185133  0.18601404]]. Action = [[ 0.05941772 -0.09181267 -0.04936191  0.8683722 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 532 is [True, False, False, False, False, True]
Scene graph at timestep 532 is [True, False, False, False, False, True]
State prediction error at timestep 532 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of 1
Current timestep = 533. State = [[-0.16542369  0.19027874]]. Action = [[-0.02776527  0.20719802  0.17109856 -0.9251352 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 533 is [True, False, False, False, False, True]
Current timestep = 534. State = [[-0.16361076  0.1899203 ]]. Action = [[ 0.18287909 -0.18134701  0.23122078  0.91932964]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 534 is [True, False, False, False, False, True]
Current timestep = 535. State = [[-0.15756187  0.17123191]]. Action = [[-0.13481766 -0.17928614  0.11112934 -0.56430537]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 535 is [True, False, False, False, False, True]
Current timestep = 536. State = [[-0.1547771   0.14919807]]. Action = [[ 0.10175169 -0.14943972  0.04695755  0.08863318]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 536 is [True, False, False, False, False, True]
Current timestep = 537. State = [[-0.15673171  0.15075544]]. Action = [[-0.13992615  0.24787897  0.13733381 -0.15051925]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 537 is [True, False, False, False, False, True]
Current timestep = 538. State = [[-0.16397221  0.16464284]]. Action = [[ 0.06353691  0.05683002 -0.08829574  0.4870422 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 538 is [True, False, False, False, False, True]
Current timestep = 539. State = [[-0.16657408  0.16237925]]. Action = [[-0.12046123 -0.162903   -0.09678394  0.44878197]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 539 is [True, False, False, False, False, True]
Current timestep = 540. State = [[-0.17132905  0.1659145 ]]. Action = [[-0.01404132  0.17263854 -0.22140738  0.41105485]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 540 is [True, False, False, False, False, True]
Current timestep = 541. State = [[-0.18328732  0.17361622]]. Action = [[-0.2030665  -0.04445243 -0.08600387  0.5606742 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 541 is [True, False, False, False, False, True]
Current timestep = 542. State = [[-0.20273231  0.17022873]]. Action = [[-0.15528487 -0.06941307  0.13784736 -0.44135797]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 542 is [True, False, False, False, False, True]
Current timestep = 543. State = [[-0.21183668  0.1707196 ]]. Action = [[ 0.20260233  0.15214264 -0.19065997 -0.18097126]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 543 is [True, False, False, False, False, True]
Scene graph at timestep 543 is [True, False, False, False, False, True]
State prediction error at timestep 543 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.20808187  0.16558531]]. Action = [[-0.05296817 -0.22482902  0.03981212  0.11600924]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 544 is [True, False, False, False, False, True]
Current timestep = 545. State = [[-0.21443048  0.14424586]]. Action = [[-0.1840313  -0.19413446 -0.1958179  -0.7798824 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 545 is [True, False, False, False, False, True]
Current timestep = 546. State = [[-0.21841732  0.12558101]]. Action = [[ 0.11434773 -0.05278723 -0.13101548 -0.01320815]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 546 is [True, False, False, False, False, True]
Current timestep = 547. State = [[-0.22076184  0.11607604]]. Action = [[-0.11085004 -0.04171346  0.22948584 -0.47070265]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 547 is [True, False, False, False, False, True]
Current timestep = 548. State = [[-0.22823517  0.11683815]]. Action = [[-0.09471239  0.05979416 -0.16952042 -0.07881016]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.2368901   0.11789214]]. Action = [[-0.11056189 -0.0392974  -0.22992717  0.96218264]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 549 is [True, False, False, False, True, False]
Current timestep = 550. State = [[-0.25123805  0.12524925]]. Action = [[ 0.04834539  0.22852805  0.12257582 -0.5548823 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.25178447  0.13612072]]. Action = [[ 0.1279763  -0.03674187 -0.14891487  0.5319164 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 551 is [True, False, False, False, False, True]
Current timestep = 552. State = [[-0.2502473   0.12517549]]. Action = [[-0.110673   -0.21359293  0.14164895  0.94972587]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 552 is [True, False, False, False, False, True]
Current timestep = 553. State = [[-0.24944405  0.11440851]]. Action = [[-0.19603378  0.14314413  0.16734236  0.765537  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 553 is [True, False, False, False, False, True]
Current timestep = 554. State = [[-0.250023    0.11388919]]. Action = [[0.01158834 0.06537735 0.18872434 0.32200205]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.2421845   0.10351943]]. Action = [[ 0.19943184 -0.191114   -0.16130237  0.5893905 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 555 is [True, False, False, False, True, False]
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(3.5370e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of -1
Current timestep = 556. State = [[-0.22950561  0.08886051]]. Action = [[ 0.02992079 -0.01912379  0.1855774  -0.6907983 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.22354582  0.07785238]]. Action = [[ 0.06927013 -0.14621516 -0.20013493 -0.26631248]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.2238751   0.06155043]]. Action = [[-0.20795734 -0.08130437  0.02259839  0.2984357 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 558 is [True, False, False, False, True, False]
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of 1
Current timestep = 559. State = [[-0.2310535   0.04931582]]. Action = [[ 0.16822654 -0.02375348 -0.02802858 -0.13226247]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 559 is [True, False, False, False, True, False]
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.22250496  0.03616437]]. Action = [[ 0.00629517 -0.17165372 -0.1839001   0.11173987]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.2249416   0.02658659]]. Action = [[-0.15933307  0.0520013   0.22806323 -0.28843158]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.22788322  0.02380063]]. Action = [[ 0.00705764 -0.05480176 -0.03219059 -0.74990165]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 562 is [True, False, False, False, True, False]
Current timestep = 563. State = [[-0.23554903  0.03235679]]. Action = [[-0.15388931  0.201729    0.04132757 -0.828058  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 563 is [True, False, False, False, True, False]
Current timestep = 564. State = [[-0.24991646  0.03010939]]. Action = [[ 0.01542142 -0.23642664 -0.22093283  0.12139654]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.252987    0.01388262]]. Action = [[ 0.01036438 -0.07033595  0.20582414 -0.65049756]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-2.4878228e-01 -2.0023198e-04]]. Action = [[ 0.07390228 -0.11535057  0.21621448 -0.8670217 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 566 is [True, False, False, False, True, False]
Current timestep = 567. State = [[-0.24534078 -0.01020359]]. Action = [[-0.17332841  0.09282309 -0.16995896 -0.3057956 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.24213186 -0.02548533]]. Action = [[ 0.04777047 -0.21049304  0.09798357  0.09700525]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.23722668 -0.04871302]]. Action = [[ 0.0682773  -0.08061162  0.22033864 -0.71099293]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.23367502 -0.0624176 ]]. Action = [[ 0.04112464 -0.07204646  0.18481606  0.29113662]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 570 is [True, False, False, False, True, False]
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(2.5611e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of 1
Current timestep = 571. State = [[-0.2283636  -0.06064681]]. Action = [[ 0.06452501  0.17570531 -0.01001225  0.7154461 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 571 is [True, False, False, False, True, False]
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.22301666 -0.05900434]]. Action = [[-0.04773279 -0.17570697 -0.20632075  0.7203355 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 572 is [True, False, False, False, True, False]
Current timestep = 573. State = [[-0.22023803 -0.07680243]]. Action = [[ 0.04125372 -0.15647447  0.19286281  0.40062892]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 573 is [True, False, False, False, True, False]
Current timestep = 574. State = [[-0.21941733 -0.08334312]]. Action = [[-0.05508272  0.14914411 -0.24487922  0.77503395]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 574 is [True, False, False, False, True, False]
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(1.2239e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 0
Current timestep = 575. State = [[-0.2201546  -0.07823922]]. Action = [[ 0.0977827  -0.03381741  0.17774814  0.50536895]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 575 is [True, False, False, False, True, False]
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of 0
Current timestep = 576. State = [[-0.2160492 -0.0827755]]. Action = [[ 0.08060727 -0.07521227  0.18273121  0.53503203]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 576 is [True, False, False, False, True, False]
Current timestep = 577. State = [[-0.20562655 -0.09991782]]. Action = [[ 0.15940881 -0.22659586  0.21273246 -0.8604973 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.19188121 -0.12244924]]. Action = [[ 0.01978752 -0.09500405 -0.23822373  0.06553352]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.18663444 -0.12083455]]. Action = [[ 0.00426695  0.20372343 -0.10240704  0.14721382]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.17945826 -0.09969002]]. Action = [[ 0.07387897  0.20041785 -0.22400373  0.67176175]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 580 is [True, False, False, False, True, False]
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.16899471 -0.08842058]]. Action = [[ 0.13809907 -0.1423212  -0.24234757 -0.14199138]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.15741628 -0.10765491]]. Action = [[ 0.04196444 -0.23872472  0.19147414  0.72241235]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.14154705 -0.12291486]]. Action = [[ 0.1939626   0.02779332 -0.00763945  0.4863522 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 583 is [True, False, False, False, True, False]
Current timestep = 584. State = [[-0.13108392 -0.12245159]]. Action = [[-0.17431937  0.05750877  0.22280937  0.2631564 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 584 is [True, False, False, False, True, False]
Current timestep = 585. State = [[-0.13107307 -0.11183581]]. Action = [[ 0.0837689   0.1654064   0.23064166 -0.05943531]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 585 is [True, False, False, False, True, False]
Current timestep = 586. State = [[-0.12487139 -0.09467858]]. Action = [[0.1480174  0.09682706 0.06425563 0.59910893]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 586 is [True, False, False, False, True, False]
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of 1
Current timestep = 587. State = [[-0.11019068 -0.0947767 ]]. Action = [[ 0.07673952 -0.19081739 -0.10339543 -0.26460052]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.09712999 -0.09472822]]. Action = [[ 0.15502942  0.17436141 -0.04917756  0.8076248 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.08749936 -0.0770431 ]]. Action = [[-0.16560414  0.17338488  0.24740875  0.8064097 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 589 is [True, False, False, False, True, False]
Current timestep = 590. State = [[-0.08317649 -0.05659897]]. Action = [[ 0.21276861  0.1139828  -0.13989918 -0.8067737 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 590 is [True, False, False, False, True, False]
Scene graph at timestep 590 is [True, False, False, False, True, False]
State prediction error at timestep 590 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of 1
Current timestep = 591. State = [[-0.06293206 -0.04438947]]. Action = [[ 0.22373682 -0.02207406 -0.09483677 -0.09782928]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 591 is [True, False, False, False, True, False]
Current timestep = 592. State = [[-0.05082542 -0.03040764]]. Action = [[-0.16187513  0.22829318 -0.087975   -0.80459404]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 592 is [True, False, False, False, True, False]
Current timestep = 593. State = [[-0.05645794 -0.00345951]]. Action = [[-0.07391062  0.21724093 -0.23040925  0.89413   ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 593 is [True, False, False, False, True, False]
Current timestep = 594. State = [[-0.06294335  0.00895127]]. Action = [[-0.08409753 -0.10701409  0.17561093 -0.40685737]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.07473674 -0.00413205]]. Action = [[-0.20809007 -0.16203886 -0.24154624 -0.82324046]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 595 is [True, False, False, False, True, False]
Current timestep = 596. State = [[-0.08731111 -0.01555782]]. Action = [[ 0.03343663 -0.00113389  0.22582722 -0.73442   ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 596 is [True, False, False, False, True, False]
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of 1
Current timestep = 597. State = [[-0.08832381 -0.02407508]]. Action = [[ 0.11016682 -0.09975985  0.12432709  0.06942236]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 597 is [True, False, False, False, True, False]
Current timestep = 598. State = [[-0.09273272 -0.028102  ]]. Action = [[-0.21759725  0.03477812  0.0165005  -0.41956425]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 598 is [True, False, False, False, True, False]
Current timestep = 599. State = [[-0.10634927 -0.02029572]]. Action = [[-0.12675208  0.13122532 -0.19041361  0.49168622]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 599 is [True, False, False, False, True, False]
Current timestep = 600. State = [[-0.11507107 -0.01846715]]. Action = [[ 0.16080302 -0.13309239 -0.02480751  0.5578246 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 600 is [True, False, False, False, True, False]
Current timestep = 601. State = [[-0.10848617 -0.01637554]]. Action = [[0.11198622 0.13722691 0.10349324 0.3427179 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 601 is [True, False, False, False, True, False]
Current timestep = 602. State = [[-0.10680263 -0.00610478]]. Action = [[-0.0686551   0.09109706  0.15695465 -0.9276757 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 602 is [True, False, False, False, True, False]
Current timestep = 603. State = [[-0.10590609  0.0112422 ]]. Action = [[ 0.05766213  0.17852461 -0.02574971 -0.5312749 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 603 is [True, False, False, False, True, False]
Current timestep = 604. State = [[-0.10894327  0.03301023]]. Action = [[-0.16447419  0.13138777  0.16260707 -0.3237983 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 604 is [True, False, False, False, True, False]
Current timestep = 605. State = [[-0.1078302  0.0588474]]. Action = [[ 0.18631083  0.20711619  0.12840623 -0.23565006]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-0.09842937  0.08671776]]. Action = [[0.07801157 0.20056331 0.1119374  0.9443929 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 606 is [True, False, False, False, True, False]
Current timestep = 607. State = [[-0.08211187  0.11386678]]. Action = [[ 0.23481542  0.15810463  0.0046204  -0.18748033]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.07011532  0.11841948]]. Action = [[-0.20732418 -0.22432077 -0.11185122 -0.11095095]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 608 is [True, False, False, False, True, False]
Current timestep = 609. State = [[-0.07639856  0.09694701]]. Action = [[-0.1121822  -0.20425002 -0.17914037  0.923825  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 609 is [True, False, False, False, True, False]
Current timestep = 610. State = [[-0.08432021  0.08336265]]. Action = [[-0.03408988  0.04081556 -0.02971192 -0.05764329]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.08923156  0.08080318]]. Action = [[-0.05787711 -0.01437674 -0.02526757  0.94656444]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 611 is [True, False, False, False, True, False]
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.09637516  0.07101025]]. Action = [[-0.01922461 -0.12190971 -0.11906147  0.2757275 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.10324032  0.0677143 ]]. Action = [[-0.10585415  0.06700692 -0.10343236 -0.929094  ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.10603435  0.07320384]]. Action = [[ 0.15657085  0.07383963 -0.16123596 -0.18792379]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.09948578  0.06428276]]. Action = [[ 0.09661111 -0.23002997 -0.1343057   0.06688321]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 615 is [True, False, False, False, True, False]
Current timestep = 616. State = [[-0.09451982  0.06097002]]. Action = [[ 0.00242674  0.17768875 -0.18688607  0.6178286 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.08753995  0.08133122]]. Action = [[0.14308792 0.24105841 0.06322393 0.05276906]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 617 is [True, False, False, False, True, False]
Current timestep = 618. State = [[-0.07787156  0.08806405]]. Action = [[-0.0266019  -0.20846826 -0.17765942 -0.33692986]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.07880378  0.09127497]]. Action = [[-0.04794951  0.18981612 -0.23715447 -0.0498144 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 619 is [True, False, False, False, True, False]
Current timestep = 620. State = [[-0.08572143  0.09983215]]. Action = [[-0.15421626 -0.02368113 -0.12457053 -0.8894379 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 620 is [True, False, False, False, True, False]
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of 0
Current timestep = 621. State = [[-0.08963533  0.1134302 ]]. Action = [[ 0.07177988  0.20594579  0.1887986  -0.4397024 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 621 is [True, False, False, False, True, False]
Current timestep = 622. State = [[-0.09124076  0.11732663]]. Action = [[ 0.03040954 -0.1624571  -0.03536594 -0.9495072 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 622 is [True, False, False, False, True, False]
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 0
Current timestep = 623. State = [[-0.09145564  0.1130357 ]]. Action = [[-0.11372231  0.01548094  0.19839874  0.556298  ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 623 is [True, False, False, False, True, False]
Current timestep = 624. State = [[-0.08749486  0.10544683]]. Action = [[ 0.21359724 -0.11156878  0.0032841  -0.44598043]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 624 is [True, False, False, False, True, False]
Current timestep = 625. State = [[-0.07754115  0.10469196]]. Action = [[0.11504132 0.13751993 0.02274579 0.02413428]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 625 is [True, False, False, False, True, False]
Current timestep = 626. State = [[-0.07522228  0.11568052]]. Action = [[-0.19707258  0.05765042  0.22610897  0.7280544 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 626 is [True, False, False, False, True, False]
Current timestep = 627. State = [[-0.07555603  0.11491491]]. Action = [[ 0.10014802 -0.11557081  0.03749925 -0.4492948 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 627 is [True, False, False, False, True, False]
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of 0
Current timestep = 628. State = [[-0.07337796  0.10949859]]. Action = [[ 0.02861196  0.01312324 -0.06571542 -0.14990515]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 628 is [True, False, False, False, True, False]
Current timestep = 629. State = [[-0.06632674  0.10576823]]. Action = [[ 0.17993104 -0.04922688  0.06838858 -0.8930517 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 629 is [True, False, False, False, True, False]
Current timestep = 630. State = [[-0.05058592  0.09444716]]. Action = [[ 0.1116015  -0.10640529 -0.03589195 -0.6460091 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 630 is [True, False, False, False, True, False]
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.03788198  0.0848961 ]]. Action = [[-0.22250523 -0.03395617  0.24132025 -0.60896844]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 631 is [True, False, False, False, True, False]
Current timestep = 632. State = [[-0.03667336  0.0707407 ]]. Action = [[ 0.22481677 -0.19157462  0.18982291  0.41467535]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 632 is [False, True, False, False, True, False]
Scene graph at timestep 632 is [False, True, False, False, True, False]
State prediction error at timestep 632 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of 1
Current timestep = 633. State = [[-0.20970085 -0.01449579]]. Action = [[ 0.10808828 -0.0461887   0.0980297  -0.9083118 ]]. Reward = [100.]
Curr episode timestep = 125
Scene graph at timestep 633 is [False, True, False, False, True, False]
Current timestep = 634. State = [[-0.20038058 -0.02533952]]. Action = [[ 0.08143321 -0.13898188 -0.18928653  0.00332451]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 634 is [True, False, False, False, True, False]
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(5.3467e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of 0
Current timestep = 635. State = [[-0.19724683 -0.04888744]]. Action = [[-0.22504528 -0.18637751 -0.09344308 -0.3265673 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 635 is [True, False, False, False, True, False]
Current timestep = 636. State = [[-0.19840111 -0.05570906]]. Action = [[0.22117367 0.13761869 0.06157789 0.8505331 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 636 is [True, False, False, False, True, False]
Current timestep = 637. State = [[-0.20019741 -0.03731461]]. Action = [[-0.21251589  0.23907277 -0.20802172 -0.846138  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 637 is [True, False, False, False, True, False]
Current timestep = 638. State = [[-0.19792266 -0.0268882 ]]. Action = [[ 0.232799   -0.1464549  -0.21456389  0.4510721 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 638 is [True, False, False, False, True, False]
Current timestep = 639. State = [[-0.18371643 -0.04386543]]. Action = [[ 0.08076513 -0.2145831   0.18199757  0.6393486 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 639 is [True, False, False, False, True, False]
Current timestep = 640. State = [[-0.17481633 -0.05804731]]. Action = [[ 0.10009301  0.02311015 -0.19854313  0.96750975]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 640 is [True, False, False, False, True, False]
Current timestep = 641. State = [[-0.16115752 -0.0536473 ]]. Action = [[ 0.06519932  0.11587855 -0.20417956  0.27486122]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 641 is [True, False, False, False, True, False]
Current timestep = 642. State = [[-0.14890626 -0.05933381]]. Action = [[ 0.11991143 -0.18124491  0.19495833 -0.25491738]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 642 is [True, False, False, False, True, False]
Current timestep = 643. State = [[-0.14002042 -0.05549762]]. Action = [[-0.00661805  0.21840507 -0.21795999 -0.43844104]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 643 is [True, False, False, False, True, False]
Current timestep = 644. State = [[-0.13823149 -0.05252242]]. Action = [[-0.08327445 -0.13051075  0.07896537 -0.91028804]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-0.14255323 -0.05979516]]. Action = [[-0.12437898 -0.0457454  -0.17118636 -0.9011128 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 645 is [True, False, False, False, True, False]
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(9.1993e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of 1
Current timestep = 646. State = [[-0.14669868 -0.07498018]]. Action = [[ 0.00486967 -0.14344522  0.04284108 -0.34571874]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 646 is [True, False, False, False, True, False]
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(7.1383e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[-0.15080377 -0.10048994]]. Action = [[-0.0304288  -0.2196156   0.0754838   0.26598513]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 647 is [True, False, False, False, True, False]
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.15341255 -0.11628257]]. Action = [[ 0.04214147  0.01948607 -0.23692685  0.0621711 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 648 is [True, False, False, False, True, False]
Current timestep = 649. State = [[-0.15613346 -0.122054  ]]. Action = [[-0.11764625 -0.08916862  0.04152659  0.89467967]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 649 is [True, False, False, False, True, False]
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(1.8424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.16313162 -0.13572481]]. Action = [[-0.03112453 -0.10612622  0.04013681  0.5587734 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 650 is [True, False, False, False, True, False]
Current timestep = 651. State = [[-0.16791636 -0.13523217]]. Action = [[-0.03352925  0.14875245 -0.05581406 -0.49987912]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 651 is [True, False, False, True, False, False]
Current timestep = 652. State = [[-0.17039509 -0.13187596]]. Action = [[-0.01268789 -0.04815347 -0.21529858  0.33599472]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 652 is [True, False, False, True, False, False]
Current timestep = 653. State = [[-0.16995552 -0.14216483]]. Action = [[ 0.08429605 -0.16965027  0.0350835   0.3321314 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 653 is [True, False, False, True, False, False]
Current timestep = 654. State = [[-0.16414429 -0.15485553]]. Action = [[ 0.11779153 -0.04767267  0.00144362 -0.6058721 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 654 is [True, False, False, True, False, False]
Current timestep = 655. State = [[-0.15868226 -0.1521472 ]]. Action = [[0.06164598 0.13217765 0.140858   0.67439306]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 655 is [True, False, False, True, False, False]
Scene graph at timestep 655 is [True, False, False, True, False, False]
State prediction error at timestep 655 is tensor(8.8268e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of 1
Current timestep = 656. State = [[-0.15497017 -0.13536836]]. Action = [[-0.15375525  0.16578549 -0.18586433 -0.6858377 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 656 is [True, False, False, True, False, False]
Current timestep = 657. State = [[-0.15106344 -0.1150452 ]]. Action = [[0.21748632 0.1518366  0.24344748 0.10283446]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 657 is [True, False, False, True, False, False]
Current timestep = 658. State = [[-0.14835867 -0.09422183]]. Action = [[-0.20451221  0.15783921  0.14843214  0.8939121 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.15867738 -0.08218118]]. Action = [[-0.15359645 -0.06295094 -0.21768053  0.8300996 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 659 is [True, False, False, False, True, False]
Current timestep = 660. State = [[-0.1643601  -0.08486956]]. Action = [[ 0.20350355 -0.00449322 -0.05564976 -0.45233107]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 660 is [True, False, False, False, True, False]
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(1.5147e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 0
Current timestep = 661. State = [[-0.15200812 -0.09031812]]. Action = [[ 0.1972239  -0.11006644  0.07173067  0.3252982 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 661 is [True, False, False, False, True, False]
Current timestep = 662. State = [[-0.14687453 -0.10783889]]. Action = [[-0.2295017  -0.16507733 -0.11064222 -0.703093  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 662 is [True, False, False, False, True, False]
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(5.0686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of -1
Current timestep = 663. State = [[-0.15314028 -0.11437217]]. Action = [[ 0.01310059  0.15938762  0.01951069 -0.7933756 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 663 is [True, False, False, False, True, False]
Current timestep = 664. State = [[-0.15199734 -0.09632205]]. Action = [[ 0.11461854  0.15954685  0.11784577 -0.48913813]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 664 is [True, False, False, False, True, False]
Current timestep = 665. State = [[-0.14272428 -0.07497961]]. Action = [[ 0.18341425  0.13556325 -0.11120582 -0.40328783]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 665 is [True, False, False, False, True, False]
Current timestep = 666. State = [[-0.13211247 -0.06503613]]. Action = [[-0.14214753 -0.0170258  -0.06591371 -0.4327188 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 666 is [True, False, False, False, True, False]
Current timestep = 667. State = [[-0.13386507 -0.05700478]]. Action = [[-0.04708077  0.10338426  0.08747244  0.3901049 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 667 is [True, False, False, False, True, False]
Current timestep = 668. State = [[-0.14440832 -0.04710292]]. Action = [[-0.18276888  0.03032947 -0.14392693 -0.5860396 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.15225074 -0.05563928]]. Action = [[ 0.13026541 -0.20966794 -0.19103682 -0.3238355 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 669 is [True, False, False, False, True, False]
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(2.9939e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of 1
Current timestep = 670. State = [[-0.15896404 -0.07469736]]. Action = [[-0.19768645 -0.08067828  0.01953986 -0.4745375 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.16548981 -0.07578181]]. Action = [[-0.01024309  0.11257678  0.05527794 -0.30553955]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.16429195 -0.0712463 ]]. Action = [[ 0.15148425 -0.00698395  0.22895521 -0.19886208]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 672 is [True, False, False, False, True, False]
Current timestep = 673. State = [[-0.15452328 -0.06923073]]. Action = [[ 0.18039098  0.0098384  -0.17635733 -0.53582036]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 673 is [True, False, False, False, True, False]
Current timestep = 674. State = [[-0.1422305  -0.07430287]]. Action = [[ 0.07633555 -0.12219492  0.23581964 -0.87176037]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.13657528 -0.08492545]]. Action = [[-0.10572818 -0.07587296  0.07438153 -0.48152035]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.13107654 -0.0823448 ]]. Action = [[ 0.18108958  0.14795607 -0.01824659 -0.70880663]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 676 is [True, False, False, False, True, False]
Scene graph at timestep 676 is [True, False, False, False, True, False]
State prediction error at timestep 676 is tensor(3.4616e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 676 of 1
Current timestep = 677. State = [[-0.12518445 -0.06787603]]. Action = [[-0.10009228  0.12933922 -0.09664062 -0.04334897]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 677 is [True, False, False, False, True, False]
Current timestep = 678. State = [[-0.12878953 -0.05076276]]. Action = [[-0.08534801  0.13349864  0.09960669 -0.90482783]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 678 is [True, False, False, False, True, False]
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.12806256 -0.0464078 ]]. Action = [[ 0.1860902  -0.16729742 -0.10114387  0.9096124 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 679 is [True, False, False, False, True, False]
Current timestep = 680. State = [[-0.11604698 -0.04661947]]. Action = [[ 0.16025868  0.14962777  0.16093132 -0.19540507]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.09462586 -0.02918728]]. Action = [[ 0.14435875  0.17333469 -0.1502213   0.9347111 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 681 is [True, False, False, False, True, False]
Current timestep = 682. State = [[-0.07222258 -0.0183273 ]]. Action = [[ 0.17965299 -0.00685044  0.1553542   0.2750913 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 682 is [True, False, False, False, True, False]
Current timestep = 683. State = [[-0.04554657 -0.01554486]]. Action = [[ 0.22459817 -0.01994559 -0.07611749 -0.13535786]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 683 is [True, False, False, False, True, False]
Current timestep = 684. State = [[-0.16256016  0.04594867]]. Action = [[-0.10642625  0.17683637  0.1589483   0.09619415]]. Reward = [100.]
Curr episode timestep = 50
Scene graph at timestep 684 is [False, True, False, False, True, False]
Current timestep = 685. State = [[-0.15515941  0.05571119]]. Action = [[-0.19886176  0.01446992 -0.14574453  0.397403  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 685 is [True, False, False, False, True, False]
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 0
Current timestep = 686. State = [[-0.15736207  0.06346019]]. Action = [[ 0.16405314  0.10298836 -0.20600113 -0.04805481]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 686 is [True, False, False, False, True, False]
Current timestep = 687. State = [[-0.14643662  0.05762672]]. Action = [[ 0.1569781  -0.1971729   0.09112868  0.38585424]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.12146692  0.04316472]]. Action = [[ 0.21294674 -0.1000309  -0.17840235 -0.17260277]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 688 is [True, False, False, False, True, False]
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[-0.10250416  0.04495913]]. Action = [[-0.08495635  0.20548084  0.07158002 -0.52363044]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 689 is [True, False, False, False, True, False]
Current timestep = 690. State = [[-0.10546254  0.06894682]]. Action = [[ 0.07331377  0.23407215 -0.01516743 -0.15922284]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 690 is [True, False, False, False, True, False]
Current timestep = 691. State = [[-0.10214949  0.07695702]]. Action = [[-0.03330664 -0.189729    0.09076247  0.7926047 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 691 is [True, False, False, False, True, False]
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(7.5461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.10481259  0.0795167 ]]. Action = [[-0.02917284  0.19675067 -0.21419603  0.8279302 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.11361486  0.10253806]]. Action = [[-0.14691707  0.17686754  0.00885847 -0.7828139 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.12151973  0.11239069]]. Action = [[-0.10697995 -0.10928915  0.2478823  -0.3844893 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 694 is [True, False, False, False, True, False]
Current timestep = 695. State = [[-0.12859787  0.12287949]]. Action = [[ 0.02197391  0.23935124  0.04588044 -0.12554574]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 695 is [True, False, False, False, True, False]
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of -1
Current timestep = 696. State = [[-0.13817379  0.14228204]]. Action = [[-0.06349573  0.03479698 -0.03737068 -0.8580117 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.14423771  0.13565496]]. Action = [[-0.19385716 -0.18980196  0.01744977  0.86077094]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 697 is [True, False, False, False, False, True]
Current timestep = 698. State = [[-0.15117651  0.11500122]]. Action = [[-0.02983734 -0.1808953   0.15220475 -0.7251364 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 698 is [True, False, False, False, False, True]
Current timestep = 699. State = [[-0.1592944   0.08920471]]. Action = [[-0.11019492 -0.21155189  0.01443386  0.8242055 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 699 is [True, False, False, False, True, False]
Current timestep = 700. State = [[-0.1715817   0.06895116]]. Action = [[-0.09794855 -0.0377779   0.06802747 -0.14236397]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.18561587  0.07105043]]. Action = [[-0.0346431   0.13910499  0.2088522  -0.35504025]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.19052894  0.06406841]]. Action = [[-0.03463833 -0.24827176 -0.15814131 -0.5784977 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.18693036  0.04133727]]. Action = [[ 0.20282137 -0.13550484 -0.13676699  0.418756  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 703 is [True, False, False, False, True, False]
Current timestep = 704. State = [[-0.18158923  0.03777496]]. Action = [[ 0.0114764   0.16912109 -0.20796424 -0.5404838 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 704 is [True, False, False, False, True, False]
Current timestep = 705. State = [[-0.19018066  0.04521514]]. Action = [[-0.24657243  0.01208091  0.19537583 -0.7294677 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 705 is [True, False, False, False, True, False]
Current timestep = 706. State = [[-0.202553    0.04397054]]. Action = [[-0.0788065  -0.09824342 -0.08954725  0.6276897 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 706 is [True, False, False, False, True, False]
Current timestep = 707. State = [[-0.21037707  0.03796579]]. Action = [[-0.0908675  -0.00545412 -0.11967511 -0.12652779]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.23068836  0.04276919]]. Action = [[-0.16569333  0.09000033  0.13364232 -0.6651772 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 708 is [True, False, False, False, True, False]
Current timestep = 709. State = [[-0.23807639  0.06019337]]. Action = [[0.24257559 0.22362971 0.059946   0.90125155]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 709 is [True, False, False, False, True, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 0
Current timestep = 710. State = [[-0.23933662  0.08789462]]. Action = [[-0.1863698   0.1547215   0.17997631  0.15675855]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 710 is [True, False, False, False, True, False]
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of -1
Current timestep = 711. State = [[-0.24534577  0.09389517]]. Action = [[ 0.05207437 -0.16998592 -0.05142504 -0.08190298]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.24413769  0.08504091]]. Action = [[-0.22740772  0.10189417 -0.09138095 -0.4652505 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 712 is [True, False, False, False, True, False]
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[-0.2465669   0.07897148]]. Action = [[-0.10571136 -0.06839767  0.04511541  0.60628986]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 713 is [True, False, False, False, True, False]
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(8.2249e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of -1
Current timestep = 714. State = [[-0.25919825  0.07875956]]. Action = [[-0.06882368  0.13291842 -0.22893228  0.3492868 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 714 is [True, False, False, False, True, False]
Current timestep = 715. State = [[-0.26300037  0.0965443 ]]. Action = [[ 0.11643302  0.18425417 -0.021726   -0.7071715 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.2629816   0.10618433]]. Action = [[-0.09407693  0.20186296  0.08852905 -0.5028814 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 716 is [True, False, False, False, True, False]
Current timestep = 717. State = [[-0.2557897   0.10514478]]. Action = [[ 0.15613294 -0.07796364  0.15809673  0.31002748]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 717 is [True, False, False, False, True, False]
Current timestep = 718. State = [[-0.24786438  0.10978805]]. Action = [[0.0171392  0.13974965 0.11156142 0.6915662 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 718 is [True, False, False, False, True, False]
Current timestep = 719. State = [[-0.24300699  0.11814453]]. Action = [[-0.00438555  0.04279855 -0.0526156  -0.03136921]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 719 is [True, False, False, False, True, False]
Scene graph at timestep 719 is [True, False, False, False, True, False]
State prediction error at timestep 719 is tensor(9.0214e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of -1
Current timestep = 720. State = [[-0.24170035  0.13237701]]. Action = [[ 0.07316601  0.11884302 -0.1780508   0.40506232]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 720 is [True, False, False, False, True, False]
Current timestep = 721. State = [[-0.22888464  0.1335765 ]]. Action = [[ 0.16514361 -0.10797513 -0.18208651 -0.6142771 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 721 is [True, False, False, False, False, True]
Current timestep = 722. State = [[-0.20483345  0.12264536]]. Action = [[ 0.20050561 -0.1408746   0.12615883 -0.29101634]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 722 is [True, False, False, False, False, True]
Scene graph at timestep 722 is [True, False, False, False, True, False]
State prediction error at timestep 722 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.17631204  0.10115669]]. Action = [[ 0.20951945 -0.17574963  0.20057577 -0.909079  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 723 is [True, False, False, False, True, False]
Current timestep = 724. State = [[-0.15545847  0.086601  ]]. Action = [[ 0.07559139 -0.01621844  0.06989402  0.52751184]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 724 is [True, False, False, False, True, False]
Current timestep = 725. State = [[-0.14494409  0.06962606]]. Action = [[-0.00609331 -0.22840671 -0.23578964 -0.49647886]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 725 is [True, False, False, False, True, False]
Current timestep = 726. State = [[-0.13647898  0.04419164]]. Action = [[ 0.07596171 -0.14473182  0.24354619  0.48415208]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.1240788   0.01789789]]. Action = [[ 0.19151539 -0.21260376 -0.10286865  0.14549553]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 727 is [True, False, False, False, True, False]
Current timestep = 728. State = [[-0.11278613 -0.00563045]]. Action = [[-0.15575066 -0.10324275 -0.13178958  0.37819052]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 728 is [True, False, False, False, True, False]
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.11892824 -0.02683832]]. Action = [[-0.15189698 -0.14111659 -0.12764718  0.72537863]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 729 is [True, False, False, False, True, False]
Current timestep = 730. State = [[-0.12422246 -0.04905573]]. Action = [[ 0.08652622 -0.18508859 -0.02254747  0.9336699 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 730 is [True, False, False, False, True, False]
Current timestep = 731. State = [[-0.12574695 -0.05478228]]. Action = [[-0.13516456  0.1539686  -0.0144729   0.54183626]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 731 is [True, False, False, False, True, False]
Current timestep = 732. State = [[-0.13326006 -0.05614422]]. Action = [[-0.09323174 -0.1044182  -0.18866692  0.389848  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 732 is [True, False, False, False, True, False]
Current timestep = 733. State = [[-0.13907754 -0.05821397]]. Action = [[ 0.12203783  0.07251576  0.00059927 -0.09423244]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 733 is [True, False, False, False, True, False]
Current timestep = 734. State = [[-0.14029287 -0.05747931]]. Action = [[-0.10824075 -0.04042308  0.01242417  0.85968983]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 734 is [True, False, False, False, True, False]
Scene graph at timestep 734 is [True, False, False, False, True, False]
State prediction error at timestep 734 is tensor(1.4317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of -1
Current timestep = 735. State = [[-0.15057033 -0.06624109]]. Action = [[-0.18358067 -0.12958822  0.06075794 -0.87147784]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 735 is [True, False, False, False, True, False]
Current timestep = 736. State = [[-0.1620186  -0.07794999]]. Action = [[ 0.00191817 -0.03518073 -0.2014895  -0.03502017]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 736 is [True, False, False, False, True, False]
Current timestep = 737. State = [[-0.1640464  -0.07212457]]. Action = [[ 0.11077803  0.18417427  0.02341768 -0.5019055 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 737 is [True, False, False, False, True, False]
Current timestep = 738. State = [[-0.15452354 -0.07142317]]. Action = [[ 0.19927126 -0.20522663  0.07630461 -0.74739623]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 738 is [True, False, False, False, True, False]
Current timestep = 739. State = [[-0.13637613 -0.07816247]]. Action = [[ 0.22046858  0.05239993 -0.20508458  0.57768846]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 739 is [True, False, False, False, True, False]
Current timestep = 740. State = [[-0.11572675 -0.08914423]]. Action = [[ 0.07019287 -0.21463892  0.04144853  0.82741594]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 740 is [True, False, False, False, True, False]
Scene graph at timestep 740 is [True, False, False, False, True, False]
State prediction error at timestep 740 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.09900913 -0.11074838]]. Action = [[ 0.23409632 -0.07314092  0.07670537  0.68531203]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 741 is [True, False, False, False, True, False]
Scene graph at timestep 741 is [True, False, False, False, True, False]
State prediction error at timestep 741 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.07244901 -0.10607045]]. Action = [[-0.02406876  0.21816653  0.08028346  0.12371159]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 742 is [True, False, False, False, True, False]
Current timestep = 743. State = [[-0.06387465 -0.10571182]]. Action = [[ 0.23912251 -0.2197706  -0.1824261  -0.7452606 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 743 is [True, False, False, False, True, False]
Current timestep = 744. State = [[-0.0469566  -0.10490391]]. Action = [[-0.04945177  0.1971618  -0.129276   -0.91018325]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 744 is [True, False, False, False, True, False]
Current timestep = 745. State = [[-0.04404932 -0.10593727]]. Action = [[ 0.06386572 -0.17731184 -0.09955941  0.60674953]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 745 is [False, True, False, False, True, False]
Current timestep = 746. State = [[-0.03532656 -0.1158009 ]]. Action = [[ 0.13266093 -0.05293238 -0.18005344 -0.7405711 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 746 is [False, True, False, False, True, False]
Current timestep = 747. State = [[-0.01505669 -0.12710382]]. Action = [[ 0.18348673 -0.09808651  0.03395352 -0.9520573 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 747 is [False, True, False, False, True, False]
Current timestep = 748. State = [[ 0.01098346 -0.13959853]]. Action = [[ 0.16100058 -0.0840424   0.22055238  0.9338926 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 748 is [False, True, False, True, False, False]
Current timestep = 749. State = [[ 0.02778404 -0.1579092 ]]. Action = [[-0.01989335 -0.1733393  -0.24497063  0.5283836 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 749 is [False, True, False, True, False, False]
Current timestep = 750. State = [[ 0.02803776 -0.17182803]]. Action = [[-0.18052526  0.01721016 -0.22961968 -0.7939853 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 750 is [False, True, False, True, False, False]
Current timestep = 751. State = [[ 0.01938766 -0.16837937]]. Action = [[-0.21189761  0.14010426  0.1539599  -0.953377  ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 751 is [False, True, False, True, False, False]
Current timestep = 752. State = [[ 0.00104932 -0.17321487]]. Action = [[-0.1657008  -0.16588348 -0.14423107  0.96249294]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 752 is [False, True, False, True, False, False]
Scene graph at timestep 752 is [False, True, False, True, False, False]
State prediction error at timestep 752 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.02530505 -0.19088994]]. Action = [[-0.17278104 -0.06522682  0.21608117  0.12185168]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 753 is [False, True, False, True, False, False]
Current timestep = 754. State = [[-0.04439585 -0.18329711]]. Action = [[-1.6437343e-01  2.2455838e-01  2.1613210e-01 -9.6142292e-05]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 754 is [False, True, False, True, False, False]
Current timestep = 755. State = [[-0.05767372 -0.15543371]]. Action = [[ 0.07487297  0.15763992 -0.19357012 -0.7231565 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 755 is [False, True, False, True, False, False]
Scene graph at timestep 755 is [True, False, False, True, False, False]
State prediction error at timestep 755 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of 1
Current timestep = 756. State = [[-0.05047239 -0.125734  ]]. Action = [[ 0.22889638  0.22245991  0.04695114 -0.5757237 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 756 is [True, False, False, True, False, False]
Current timestep = 757. State = [[-0.05393835 -0.12083448]]. Action = [[-0.22149381 -0.20832185 -0.15016082  0.52453256]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 757 is [True, False, False, True, False, False]
Current timestep = 758. State = [[-0.06015151 -0.12087182]]. Action = [[-0.09124896  0.15914273 -0.11832806  0.73047256]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 758 is [True, False, False, False, True, False]
Current timestep = 759. State = [[-0.06481313 -0.10735653]]. Action = [[0.12872928 0.08176765 0.16441485 0.30853713]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 759 is [True, False, False, False, True, False]
Scene graph at timestep 759 is [True, False, False, False, True, False]
State prediction error at timestep 759 is tensor(4.8421e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of 1
Current timestep = 760. State = [[-0.05970167 -0.10142436]]. Action = [[ 0.15734714 -0.05916612  0.13627255 -0.6829658 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 760 is [True, False, False, False, True, False]
Current timestep = 761. State = [[-0.05516882 -0.094653  ]]. Action = [[ 0.00326487  0.14164689 -0.18335208 -0.68178844]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 761 is [True, False, False, False, True, False]
Scene graph at timestep 761 is [True, False, False, False, True, False]
State prediction error at timestep 761 is tensor(4.0792e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[-0.04977151 -0.08861076]]. Action = [[ 0.0778273  -0.05327377  0.17942092 -0.1767385 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 762 is [True, False, False, False, True, False]
Current timestep = 763. State = [[-0.04318968 -0.09273487]]. Action = [[ 0.11388656 -0.05593452 -0.04135253  0.89700246]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 763 is [False, True, False, False, True, False]
Current timestep = 764. State = [[-0.02465924 -0.10858079]]. Action = [[ 0.24281886 -0.23986973 -0.04024807  0.3389492 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 764 is [False, True, False, False, True, False]
Current timestep = 765. State = [[ 0.00429334 -0.12101009]]. Action = [[ 0.12118745  0.09373271 -0.03258318 -0.3217225 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 765 is [False, True, False, False, True, False]
Current timestep = 766. State = [[ 0.0166202  -0.10705098]]. Action = [[-0.07089901  0.21114028  0.18844381  0.70396113]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 766 is [False, True, False, False, True, False]
Current timestep = 767. State = [[-0.2160253  -0.18617325]]. Action = [[-0.22270536  0.22659823 -0.01927987  0.6293328 ]]. Reward = [100.]
Curr episode timestep = 82
Scene graph at timestep 767 is [False, True, False, False, True, False]
Current timestep = 768. State = [[-0.195983   -0.21630257]]. Action = [[ 0.23478284 -0.14969851  0.14821523 -0.82522154]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 768 is [True, False, False, True, False, False]
Current timestep = 769. State = [[-0.18457563 -0.22704676]]. Action = [[-0.18826449  0.02309614 -0.15758222 -0.5689811 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 769 is [True, False, False, True, False, False]
Current timestep = 770. State = [[-0.18269773 -0.21784724]]. Action = [[ 0.18862939  0.19330588  0.03184402 -0.53769714]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 770 is [True, False, False, True, False, False]
Current timestep = 771. State = [[-0.16742498 -0.19792706]]. Action = [[ 0.20531052  0.13284403 -0.00061144 -0.1001969 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 771 is [True, False, False, True, False, False]
Scene graph at timestep 771 is [True, False, False, True, False, False]
State prediction error at timestep 771 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.13944437 -0.18748954]]. Action = [[ 0.17964256 -0.08049968  0.16805115  0.66068625]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 772 is [True, False, False, True, False, False]
Current timestep = 773. State = [[-0.12614617 -0.19463453]]. Action = [[-0.07234812 -0.03363904 -0.15437217 -0.7175126 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 773 is [True, False, False, True, False, False]
Scene graph at timestep 773 is [True, False, False, True, False, False]
State prediction error at timestep 773 is tensor(2.6252e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 773 of 1
Current timestep = 774. State = [[-0.13292333 -0.1917275 ]]. Action = [[-0.20236795  0.13180691 -0.15017894 -0.75099134]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 774 is [True, False, False, True, False, False]
Current timestep = 775. State = [[-0.14268221 -0.17497566]]. Action = [[-0.11923552  0.21177623  0.0820587  -0.6203955 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 775 is [True, False, False, True, False, False]
Current timestep = 776. State = [[-0.14489335 -0.16909362]]. Action = [[ 0.22508788 -0.20326278  0.22033298 -0.9005394 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 776 is [True, False, False, True, False, False]
Current timestep = 777. State = [[-0.13899998 -0.1854336 ]]. Action = [[ 0.06388557 -0.18745115  0.14135936  0.8112447 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 777 is [True, False, False, True, False, False]
Scene graph at timestep 777 is [True, False, False, True, False, False]
State prediction error at timestep 777 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of -1
Current timestep = 778. State = [[-0.13573337 -0.19205372]]. Action = [[-0.09145775  0.20434985  0.09018698  0.82718086]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 778 is [True, False, False, True, False, False]
Current timestep = 779. State = [[-0.13522682 -0.18342198]]. Action = [[ 0.03534442 -0.00549132 -0.08963981  0.287122  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 779 is [True, False, False, True, False, False]
Current timestep = 780. State = [[-0.12536621 -0.1735303 ]]. Action = [[ 0.24676394  0.10378203 -0.00201999 -0.8477132 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 780 is [True, False, False, True, False, False]
Current timestep = 781. State = [[-0.10482301 -0.17461449]]. Action = [[ 0.12307754 -0.16026054  0.16437173 -0.7148339 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 781 is [True, False, False, True, False, False]
Current timestep = 782. State = [[-0.09629729 -0.17948705]]. Action = [[-0.17539681  0.07156894  0.02619827  0.7615192 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 782 is [True, False, False, True, False, False]
Current timestep = 783. State = [[-0.10605486 -0.19034833]]. Action = [[-0.1761174  -0.16528368  0.23037979  0.00081372]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 783 is [True, False, False, True, False, False]
Current timestep = 784. State = [[-0.11063606 -0.18873298]]. Action = [[ 0.14484268  0.20606464 -0.04294208  0.780957  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 784 is [True, False, False, True, False, False]
Scene graph at timestep 784 is [True, False, False, True, False, False]
State prediction error at timestep 784 is tensor(2.9833e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of 1
Current timestep = 785. State = [[-0.10386444 -0.17121209]]. Action = [[ 0.13931328  0.03302497  0.06987402 -0.01707578]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 785 is [True, False, False, True, False, False]
Current timestep = 786. State = [[-0.10558417 -0.17236786]]. Action = [[-0.23485832 -0.05527365 -0.19471624  0.5824635 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 786 is [True, False, False, True, False, False]
Current timestep = 787. State = [[-0.10367987 -0.17054673]]. Action = [[0.23795205 0.06899938 0.21615374 0.27307093]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 787 is [True, False, False, True, False, False]
Current timestep = 788. State = [[-0.09830425 -0.15334578]]. Action = [[0.03377795 0.19896334 0.19066197 0.21849656]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 788 is [True, False, False, True, False, False]
Current timestep = 789. State = [[-0.09819888 -0.12889011]]. Action = [[-0.16099675  0.17114538 -0.16485167  0.39854407]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 789 is [True, False, False, True, False, False]
Current timestep = 790. State = [[-0.10281492 -0.1227689 ]]. Action = [[-0.0581952  -0.12243968 -0.22483033 -0.9576468 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 790 is [True, False, False, True, False, False]
Scene graph at timestep 790 is [True, False, False, False, True, False]
State prediction error at timestep 790 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.11226097 -0.1442914 ]]. Action = [[-0.10346022 -0.23156962 -0.05512848  0.9696013 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.12407897 -0.14852421]]. Action = [[-0.15734357  0.16276377 -0.01820411 -0.3794409 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 792 is [True, False, False, True, False, False]
Current timestep = 793. State = [[-0.13716716 -0.13712665]]. Action = [[-0.01658523  0.08484507 -0.02447602 -0.5505641 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 793 is [True, False, False, True, False, False]
Current timestep = 794. State = [[-0.13428836 -0.13482885]]. Action = [[ 0.21135569 -0.10939592 -0.24480274  0.8753071 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 794 is [True, False, False, True, False, False]
Current timestep = 795. State = [[-0.13135868 -0.14088477]]. Action = [[-0.07503824 -0.05940513  0.12014374  0.91378427]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 795 is [True, False, False, True, False, False]
Scene graph at timestep 795 is [True, False, False, True, False, False]
State prediction error at timestep 795 is tensor(2.9769e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of -1
Current timestep = 796. State = [[-0.13127527 -0.1391468 ]]. Action = [[ 0.10673994  0.14959681  0.1088084  -0.9199881 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 796 is [True, False, False, True, False, False]
Current timestep = 797. State = [[-0.1305101  -0.13358207]]. Action = [[-0.0374105  -0.03330961  0.04238009 -0.5605792 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 797 is [True, False, False, True, False, False]
Current timestep = 798. State = [[-0.13583767 -0.12455066]]. Action = [[-0.20674218  0.16585398 -0.22957684 -0.14268988]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 798 is [True, False, False, True, False, False]
Scene graph at timestep 798 is [True, False, False, False, True, False]
State prediction error at timestep 798 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.14781417 -0.12059739]]. Action = [[-0.14107187 -0.1332294  -0.13597356  0.96086   ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 799 is [True, False, False, False, True, False]
Scene graph at timestep 799 is [True, False, False, False, True, False]
State prediction error at timestep 799 is tensor(3.6860e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of -1
Current timestep = 800. State = [[-0.15910359 -0.12295052]]. Action = [[ 0.13693315  0.14384562 -0.07256937  0.2774899 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 800 is [True, False, False, False, True, False]
Current timestep = 801. State = [[-0.16009381 -0.11229212]]. Action = [[-0.15750168  0.0063284   0.19883138 -0.19614851]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 801 is [True, False, False, False, True, False]
Current timestep = 802. State = [[-0.16778447 -0.10203214]]. Action = [[-0.07362419  0.11747658  0.1324935   0.5947466 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 802 is [True, False, False, False, True, False]
Current timestep = 803. State = [[-0.17753798 -0.08131803]]. Action = [[-0.12807386  0.1860748   0.20385933 -0.502834  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 803 is [True, False, False, False, True, False]
Current timestep = 804. State = [[-0.19452071 -0.06520346]]. Action = [[-0.06218758  0.0229446  -0.15325625  0.4361117 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 804 is [True, False, False, False, True, False]
Current timestep = 805. State = [[-0.20838305 -0.04864818]]. Action = [[-0.22353992  0.21928596  0.02841318  0.2671095 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 805 is [True, False, False, False, True, False]
Current timestep = 806. State = [[-0.22259292 -0.04184308]]. Action = [[ 0.15705812 -0.18697928  0.22781432  0.2440536 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 806 is [True, False, False, False, True, False]
Scene graph at timestep 806 is [True, False, False, False, True, False]
State prediction error at timestep 806 is tensor(8.9831e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of -1
Current timestep = 807. State = [[-0.22637677 -0.05657489]]. Action = [[-0.14250916 -0.07079723  0.151708    0.94512784]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 807 is [True, False, False, False, True, False]
Current timestep = 808. State = [[-0.23098888 -0.05375691]]. Action = [[0.00560591 0.14288506 0.11649323 0.15594232]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 808 is [True, False, False, False, True, False]
Current timestep = 809. State = [[-0.22546431 -0.0493412 ]]. Action = [[ 0.2173819  -0.07070962  0.13194677  0.7219353 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 809 is [True, False, False, False, True, False]
Current timestep = 810. State = [[-0.22120386 -0.0456134 ]]. Action = [[-0.01028478  0.09806746  0.10292733  0.86045074]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 810 is [True, False, False, False, True, False]
Current timestep = 811. State = [[-0.21877645 -0.02793018]]. Action = [[ 0.03596249  0.23805374 -0.15488319 -0.8016021 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 811 is [True, False, False, False, True, False]
Current timestep = 812. State = [[-0.22127318 -0.00368342]]. Action = [[-0.10578054  0.10186234 -0.09793878  0.9097395 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 812 is [True, False, False, False, True, False]
Current timestep = 813. State = [[-0.23326232 -0.00217722]]. Action = [[-0.22920115 -0.1532246  -0.22013615 -0.88368577]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 813 is [True, False, False, False, True, False]
Current timestep = 814. State = [[-0.24052371 -0.00849062]]. Action = [[ 0.11068076  0.00114191 -0.03902954  0.35814083]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 814 is [True, False, False, False, True, False]
Current timestep = 815. State = [[-0.23943111 -0.00868226]]. Action = [[-0.00407542  0.00819474  0.02040258 -0.78300446]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 815 is [True, False, False, False, True, False]
Current timestep = 816. State = [[-0.23913929 -0.0081041 ]]. Action = [[ 0.04787609  0.04236794  0.20930296 -0.9193515 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 816 is [True, False, False, False, True, False]
Scene graph at timestep 816 is [True, False, False, False, True, False]
State prediction error at timestep 816 is tensor(4.4271e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of -1
Current timestep = 817. State = [[-0.22801626 -0.0041434 ]]. Action = [[ 0.2180857   0.04352912 -0.13956445 -0.09126788]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 817 is [True, False, False, False, True, False]
Current timestep = 818. State = [[-0.21015385 -0.0063001 ]]. Action = [[ 0.13448238 -0.12441409 -0.00957355  0.93435144]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 818 is [True, False, False, False, True, False]
Current timestep = 819. State = [[-0.20574287 -0.01997619]]. Action = [[-0.1976275  -0.14355575  0.06952232 -0.25734985]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 819 is [True, False, False, False, True, False]
Current timestep = 820. State = [[-0.21484943 -0.02929018]]. Action = [[-0.11586854  0.01746923  0.04966041 -0.0964455 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 820 is [True, False, False, False, True, False]
Scene graph at timestep 820 is [True, False, False, False, True, False]
State prediction error at timestep 820 is tensor(7.3746e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of 1
Current timestep = 821. State = [[-0.21923983 -0.04507827]]. Action = [[ 0.10001045 -0.20650412  0.14725614 -0.6763175 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 821 is [True, False, False, False, True, False]
Current timestep = 822. State = [[-0.21324047 -0.06763972]]. Action = [[ 0.09153542 -0.17081377 -0.12998554  0.3696332 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 822 is [True, False, False, False, True, False]
Current timestep = 823. State = [[-0.20063904 -0.08923509]]. Action = [[ 0.23138312 -0.09483764 -0.23020081 -0.6569875 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 823 is [True, False, False, False, True, False]
Scene graph at timestep 823 is [True, False, False, False, True, False]
State prediction error at timestep 823 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of 1
Current timestep = 824. State = [[-0.18343529 -0.09513628]]. Action = [[-0.10590157  0.09493035 -0.13381112  0.46487808]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 824 is [True, False, False, False, True, False]
Current timestep = 825. State = [[-0.19065534 -0.08615242]]. Action = [[-0.17833024  0.0954532  -0.15663166 -0.6093265 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 825 is [True, False, False, False, True, False]
Current timestep = 826. State = [[-0.20267193 -0.083001  ]]. Action = [[-0.03182061 -0.08256954  0.04018143 -0.67769563]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 826 is [True, False, False, False, True, False]
Current timestep = 827. State = [[-0.20137538 -0.08947749]]. Action = [[ 0.19574225 -0.07889518  0.01925364 -0.09923995]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 827 is [True, False, False, False, True, False]
Current timestep = 828. State = [[-0.20310657 -0.10947441]]. Action = [[-0.14587313 -0.23274933 -0.14124376 -0.28806412]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 828 is [True, False, False, False, True, False]
Scene graph at timestep 828 is [True, False, False, False, True, False]
State prediction error at timestep 828 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 828 of -1
Current timestep = 829. State = [[-0.21005802 -0.13131519]]. Action = [[-0.09808531 -0.02211334 -0.18863438  0.36096478]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 829 is [True, False, False, False, True, False]
Scene graph at timestep 829 is [True, False, False, True, False, False]
State prediction error at timestep 829 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of -1
Current timestep = 830. State = [[-0.2239729  -0.15095362]]. Action = [[-0.1549997  -0.21241412  0.19073582 -0.42036307]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 830 is [True, False, False, True, False, False]
Current timestep = 831. State = [[-0.24521089 -0.17806034]]. Action = [[-0.12239121 -0.20031857  0.04313904  0.29215288]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 831 is [True, False, False, True, False, False]
Current timestep = 832. State = [[-0.24892198 -0.1915409 ]]. Action = [[0.13888192 0.07204732 0.2178747  0.8846643 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 832 is [True, False, False, True, False, False]
Current timestep = 833. State = [[-0.24943441 -0.18928815]]. Action = [[-0.13055459  0.05508405  0.2265532   0.78174806]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 833 is [True, False, False, True, False, False]
Current timestep = 834. State = [[-0.25638714 -0.19192156]]. Action = [[-0.08436278 -0.07859308  0.20406237  0.6167171 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 834 is [True, False, False, True, False, False]
Current timestep = 835. State = [[-0.26541126 -0.19478   ]]. Action = [[-0.16755201  0.01610821 -0.01281305 -0.3658967 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 835 is [True, False, False, True, False, False]
Current timestep = 836. State = [[-0.2647938  -0.18830323]]. Action = [[ 0.10872579  0.11871421 -0.1998121   0.38285363]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 836 is [True, False, False, True, False, False]
Current timestep = 837. State = [[-0.26383108 -0.18163426]]. Action = [[-0.17503616  0.1506918  -0.18303828  0.9009013 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 837 is [True, False, False, True, False, False]
Current timestep = 838. State = [[-0.26150715 -0.18617173]]. Action = [[ 0.05327949 -0.13063857 -0.05639097 -0.75617933]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 838 is [True, False, False, True, False, False]
Current timestep = 839. State = [[-0.26095992 -0.20146547]]. Action = [[-0.06740141 -0.1580759   0.05350402  0.56169355]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 839 is [True, False, False, True, False, False]
Current timestep = 840. State = [[-0.26318   -0.2063698]]. Action = [[-0.0524672   0.17700738  0.19733718  0.2164098 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 840 is [True, False, False, True, False, False]
Current timestep = 841. State = [[-0.26402774 -0.2006646 ]]. Action = [[-0.14465484 -0.18899746  0.12495041 -0.5414346 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 841 is [True, False, False, True, False, False]
Scene graph at timestep 841 is [True, False, False, True, False, False]
State prediction error at timestep 841 is tensor(8.2637e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of -1
Current timestep = 842. State = [[-0.2620726  -0.18881914]]. Action = [[ 0.0728144   0.17513832 -0.22635044 -0.49210048]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 842 is [True, False, False, True, False, False]
Current timestep = 843. State = [[-0.25891432 -0.18299344]]. Action = [[ 0.06630313 -0.17092933 -0.00923312  0.22464764]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 843 is [True, False, False, True, False, False]
Current timestep = 844. State = [[-0.2573139  -0.18819685]]. Action = [[ 0.0330244  -0.02012065  0.06516752  0.8636899 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 844 is [True, False, False, True, False, False]
Current timestep = 845. State = [[-0.25587195 -0.18972522]]. Action = [[-0.19799289  0.18087196 -0.07745479 -0.31699115]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 845 is [True, False, False, True, False, False]
Scene graph at timestep 845 is [True, False, False, True, False, False]
State prediction error at timestep 845 is tensor(4.8853e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of -1
Current timestep = 846. State = [[-0.24198301 -0.182465  ]]. Action = [[0.22229439 0.15922391 0.2334224  0.29998994]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 846 is [True, False, False, True, False, False]
Current timestep = 847. State = [[-0.2316027  -0.18273728]]. Action = [[-0.15557429 -0.13730732 -0.23348992 -0.06439173]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 847 is [True, False, False, True, False, False]
Current timestep = 848. State = [[-0.22552383 -0.1763765 ]]. Action = [[ 0.19838315  0.21208781  0.04529241 -0.3834558 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 848 is [True, False, False, True, False, False]
Scene graph at timestep 848 is [True, False, False, True, False, False]
State prediction error at timestep 848 is tensor(6.2692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of 1
Current timestep = 849. State = [[-0.2050554  -0.15482251]]. Action = [[ 0.2102392   0.08711874 -0.10460141  0.11092079]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 849 is [True, False, False, True, False, False]
Current timestep = 850. State = [[-0.19156659 -0.14522496]]. Action = [[-0.03144832  0.06590497 -0.146144   -0.5355796 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 850 is [True, False, False, True, False, False]
Current timestep = 851. State = [[-0.17937583 -0.13750418]]. Action = [[ 0.21541789  0.03087425 -0.23275477  0.6397413 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 851 is [True, False, False, True, False, False]
Current timestep = 852. State = [[-0.1602847  -0.13696004]]. Action = [[ 0.08359993 -0.07848629 -0.09691578 -0.737763  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 852 is [True, False, False, True, False, False]
Current timestep = 853. State = [[-0.14645275 -0.14195871]]. Action = [[ 0.09080976 -0.06654042  0.04203895  0.5118978 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 853 is [True, False, False, True, False, False]
Current timestep = 854. State = [[-0.12706135 -0.13645093]]. Action = [[ 0.23028255  0.17260933 -0.11407161 -0.02321547]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 854 is [True, False, False, True, False, False]
Current timestep = 855. State = [[-0.11097829 -0.12005778]]. Action = [[-0.14219682  0.16725397 -0.14589006  0.64030933]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 855 is [True, False, False, True, False, False]
Current timestep = 856. State = [[-0.10820646 -0.11291956]]. Action = [[ 0.16172078 -0.12923387 -0.17063226  0.6779616 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.09488623 -0.10422834]]. Action = [[ 0.10515252  0.22189778 -0.02928618  0.22962832]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.07320044 -0.08708136]]. Action = [[ 0.20405567  0.07362175 -0.141957    0.35920548]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 858 is [True, False, False, False, True, False]
Current timestep = 859. State = [[-0.04439913 -0.07062223]]. Action = [[0.23134494 0.12336963 0.23653805 0.84296954]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 859 is [True, False, False, False, True, False]
Scene graph at timestep 859 is [False, True, False, False, True, False]
State prediction error at timestep 859 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of 1
Current timestep = 860. State = [[-0.23155236  0.08332523]]. Action = [[0.23354867 0.08482939 0.12035161 0.87171865]]. Reward = [100.]
Curr episode timestep = 92
Scene graph at timestep 860 is [False, True, False, False, True, False]
Current timestep = 861. State = [[-0.21598408  0.09718785]]. Action = [[0.24397337 0.11373958 0.19193894 0.2866875 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 861 is [True, False, False, False, True, False]
Scene graph at timestep 861 is [True, False, False, False, True, False]
State prediction error at timestep 861 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of 0
Current timestep = 862. State = [[-0.18016504  0.09729713]]. Action = [[ 0.22198743 -0.21437179  0.08904621  0.59738183]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.15525797  0.08186048]]. Action = [[ 0.15132195 -0.06948441 -0.22817093  0.96164834]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 863 is [True, False, False, False, True, False]
Scene graph at timestep 863 is [True, False, False, False, True, False]
State prediction error at timestep 863 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of 1
Current timestep = 864. State = [[-0.14090736  0.08296014]]. Action = [[-0.07411999  0.16505527  0.19275835  0.14303184]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 864 is [True, False, False, False, True, False]
Current timestep = 865. State = [[-0.14388083  0.08970629]]. Action = [[ 0.01523954 -0.0384129  -0.18230423  0.64474726]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 865 is [True, False, False, False, True, False]
Current timestep = 866. State = [[-0.14024754  0.09921439]]. Action = [[0.09941655 0.17791522 0.21725374 0.9211676 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 866 is [True, False, False, False, True, False]
Current timestep = 867. State = [[-0.13610296  0.12366155]]. Action = [[-0.07713068  0.21272454  0.01708731  0.40243363]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 867 is [True, False, False, False, True, False]
Current timestep = 868. State = [[-0.1452442   0.13873444]]. Action = [[-0.20156625 -0.08354652  0.04340881 -0.70769376]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 868 is [True, False, False, False, True, False]
Current timestep = 869. State = [[-0.14950383  0.14036982]]. Action = [[ 0.1279895   0.05928776  0.16764718 -0.9676342 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 869 is [True, False, False, False, False, True]
Current timestep = 870. State = [[-0.14690927  0.1362175 ]]. Action = [[ 0.06174567 -0.09764503  0.02041706  0.5287378 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 870 is [True, False, False, False, False, True]
Current timestep = 871. State = [[-0.14758691  0.13876152]]. Action = [[-0.04996897  0.1267398  -0.14763331  0.45095134]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 871 is [True, False, False, False, False, True]
Current timestep = 872. State = [[-0.14226139  0.13266206]]. Action = [[ 0.15212482 -0.20262362 -0.08452606  0.3639983 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 872 is [True, False, False, False, False, True]
Current timestep = 873. State = [[-0.12052409  0.10862692]]. Action = [[ 0.20159936 -0.22493595 -0.10208476 -0.20664483]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 873 is [True, False, False, False, False, True]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 0
Current timestep = 874. State = [[-0.10334267  0.09766586]]. Action = [[-0.07314673  0.15174079 -0.05469206  0.6889272 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 874 is [True, False, False, False, True, False]
Scene graph at timestep 874 is [True, False, False, False, True, False]
State prediction error at timestep 874 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of -1
Current timestep = 875. State = [[-0.10558552  0.10502215]]. Action = [[ 0.06574655 -0.03370772 -0.22178672  0.968194  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.10202604  0.09596194]]. Action = [[ 0.04067364 -0.13011369 -0.17416418 -0.1223098 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 876 is [True, False, False, False, True, False]
Current timestep = 877. State = [[-0.09567486  0.0969435 ]]. Action = [[0.05108517 0.1596381  0.21804482 0.7631614 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.08437791  0.1031232 ]]. Action = [[ 0.0583055  -0.02944624  0.20539743 -0.42382944]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 878 is [True, False, False, False, True, False]
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(3.1298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.07894918  0.09262212]]. Action = [[-0.16425768 -0.219152    0.05783743 -0.5816164 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 879 is [True, False, False, False, True, False]
Current timestep = 880. State = [[-0.08095698  0.07792629]]. Action = [[-0.02141988 -0.03443022 -0.07792193  0.2801783 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.07748185  0.06478404]]. Action = [[ 0.20569381 -0.10731396 -0.01433137  0.92409205]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 881 is [True, False, False, False, True, False]
Current timestep = 882. State = [[-0.06555634  0.04606374]]. Action = [[ 0.18503204 -0.16259919  0.0807063  -0.10337937]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 882 is [True, False, False, False, True, False]
Current timestep = 883. State = [[-0.05437914  0.02882853]]. Action = [[-0.188696   -0.07065618 -0.18841511 -0.9841649 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 883 is [True, False, False, False, True, False]
Current timestep = 884. State = [[-0.0522754   0.01491151]]. Action = [[ 0.15807319 -0.12044434  0.03306144 -0.11440575]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.04723942  0.01475215]]. Action = [[0.07721689 0.18264318 0.21928272 0.2935866 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.04313035  0.03090456]]. Action = [[ 0.023913    0.15476045  0.0678739  -0.7668849 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 886 is [False, True, False, False, True, False]
Scene graph at timestep 886 is [False, True, False, False, True, False]
State prediction error at timestep 886 is tensor(8.3444e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.26140076 -0.08492574]]. Action = [[ 0.08802247  0.21303135 -0.20230459 -0.7947164 ]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 887 is [False, True, False, False, True, False]
Current timestep = 888. State = [[-0.26020524 -0.09432583]]. Action = [[-0.19354051  0.01989949 -0.1734508  -0.8961477 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 888 is [True, False, False, False, True, False]
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(8.1844e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of 0
Current timestep = 889. State = [[-0.2593846  -0.08871932]]. Action = [[ 0.05786961  0.13067758  0.14393589 -0.7196294 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.25733247 -0.07102823]]. Action = [[ 0.01011688  0.19979596 -0.1186111  -0.11173671]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 890 is [True, False, False, False, True, False]
Current timestep = 891. State = [[-0.245958   -0.06123755]]. Action = [[ 0.19493663 -0.06323186 -0.19881737  0.9348011 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 891 is [True, False, False, False, True, False]
Current timestep = 892. State = [[-0.2287009 -0.0489915]]. Action = [[0.05904216 0.20471025 0.24019706 0.9041461 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 892 is [True, False, False, False, True, False]
Scene graph at timestep 892 is [True, False, False, False, True, False]
State prediction error at timestep 892 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of 1
Current timestep = 893. State = [[-0.21636957 -0.03193517]]. Action = [[ 0.09281352  0.00571248 -0.24501605 -0.69056493]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 893 is [True, False, False, False, True, False]
Scene graph at timestep 893 is [True, False, False, False, True, False]
State prediction error at timestep 893 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of 1
Current timestep = 894. State = [[-0.2078211  -0.04057394]]. Action = [[ 0.01416314 -0.18875952 -0.19576667  0.8441769 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 894 is [True, False, False, False, True, False]
Current timestep = 895. State = [[-0.19679947 -0.06287036]]. Action = [[ 0.16925839 -0.20722432 -0.19981238  0.881827  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 895 is [True, False, False, False, True, False]
Current timestep = 896. State = [[-0.1756194  -0.06523806]]. Action = [[0.14400196 0.23733652 0.1872024  0.4382105 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 896 is [True, False, False, False, True, False]
Current timestep = 897. State = [[-0.16816942 -0.06855547]]. Action = [[-0.1477992  -0.20186928  0.09594581  0.17948008]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 897 is [True, False, False, False, True, False]
Current timestep = 898. State = [[-0.17345092 -0.08647172]]. Action = [[-0.08092789 -0.13814737 -0.09654093  0.27809727]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.18617216 -0.09567617]]. Action = [[-0.21335748  0.04916313 -0.17314522 -0.8427414 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 899 is [True, False, False, False, True, False]
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(9.2749e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of 0
Current timestep = 900. State = [[-0.21240222 -0.10002745]]. Action = [[-0.24302326 -0.02885076  0.01238507 -0.02128351]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.22747657 -0.08743834]]. Action = [[ 0.13169199  0.23686719 -0.22555378 -0.7335087 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 901 is [True, False, False, False, True, False]
Current timestep = 902. State = [[-0.21927553 -0.05884597]]. Action = [[ 0.2038672   0.18166068  0.09370402 -0.5103924 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 902 is [True, False, False, False, True, False]
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 902 of 0
Current timestep = 903. State = [[-0.20755275 -0.0280579 ]]. Action = [[0.03567648 0.19735241 0.00432137 0.29570913]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 903 is [True, False, False, False, True, False]
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 903 of 1
Current timestep = 904. State = [[-0.194456   -0.01411229]]. Action = [[ 0.19243205 -0.04513751  0.2047022  -0.8614325 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 904 is [True, False, False, False, True, False]
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of 1
Current timestep = 905. State = [[-0.17197528 -0.01106634]]. Action = [[ 0.14912176  0.08754119 -0.03764713 -0.6673944 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 905 is [True, False, False, False, True, False]
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 905 of 1
Current timestep = 906. State = [[-0.16295676  0.00509185]]. Action = [[-0.15689853  0.17915887  0.13152424 -0.28722453]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.17774767  0.02332946]]. Action = [[-0.23932678  0.0467979  -0.03658748  0.87385607]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.18503174  0.03423078]]. Action = [[0.20029142 0.08103892 0.22125772 0.8583697 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 908 is [True, False, False, False, True, False]
Current timestep = 909. State = [[-0.19025466  0.03636327]]. Action = [[-0.23812671 -0.04806305  0.08340526  0.46278036]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.20767921  0.04747768]]. Action = [[-0.23679507  0.1592575  -0.06863272 -0.122908  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 910 is [True, False, False, False, True, False]
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of -1
Current timestep = 911. State = [[-0.2367055   0.07012733]]. Action = [[-0.05224195  0.14866716  0.23395571  0.07224035]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.24410792  0.08393953]]. Action = [[-0.05239546  0.04863873  0.06192887 -0.4817524 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 912 is [True, False, False, False, True, False]
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(6.5680e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of -1
Current timestep = 913. State = [[-0.25040862  0.09364974]]. Action = [[ 0.02895856  0.07477498  0.07113969 -0.6054104 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 913 is [True, False, False, False, True, False]
Scene graph at timestep 913 is [True, False, False, False, True, False]
State prediction error at timestep 913 is tensor(3.1863e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 913 of -1
Current timestep = 914. State = [[-0.24360615  0.08832309]]. Action = [[ 0.16837683 -0.18065721  0.03480047 -0.42610162]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 914 is [True, False, False, False, True, False]
Scene graph at timestep 914 is [True, False, False, False, True, False]
State prediction error at timestep 914 is tensor(1.8980e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 914 of 1
Current timestep = 915. State = [[-0.2367234   0.06897088]]. Action = [[-0.04822141 -0.11439101  0.18630296 -0.9079149 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 915 is [True, False, False, False, True, False]
Current timestep = 916. State = [[-0.22993022  0.06089451]]. Action = [[ 0.19019306 -0.016314   -0.1886163   0.5330353 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.22088918  0.06935041]]. Action = [[ 0.00556105  0.21218371 -0.16132517  0.54838204]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.20791885  0.09400168]]. Action = [[ 0.20381755  0.24761164 -0.18267462 -0.86505705]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 918 is [True, False, False, False, True, False]
Scene graph at timestep 918 is [True, False, False, False, True, False]
State prediction error at timestep 918 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 918 of 1
Current timestep = 919. State = [[-0.18019739  0.1276884 ]]. Action = [[0.22446018 0.1994757  0.1966222  0.33930647]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 919 is [True, False, False, False, True, False]
Scene graph at timestep 919 is [True, False, False, False, False, True]
State prediction error at timestep 919 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of 1
Current timestep = 920. State = [[-0.15548268  0.1416158 ]]. Action = [[-0.02860624 -0.09681183 -0.19108827 -0.3844341 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 920 is [True, False, False, False, False, True]
Current timestep = 921. State = [[-0.15300398  0.13181552]]. Action = [[ 0.0326544  -0.10814852 -0.12307918  0.3003503 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 921 is [True, False, False, False, False, True]
Current timestep = 922. State = [[-0.14729317  0.11269072]]. Action = [[ 0.08093634 -0.19344845  0.11757722 -0.81043404]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 922 is [True, False, False, False, False, True]
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[-0.13667862  0.10512986]]. Action = [[ 0.15762001  0.1905517  -0.22888671  0.5673125 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.11807948  0.10774583]]. Action = [[ 0.08580792 -0.13357784 -0.02132384  0.56281805]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.10668493  0.11152446]]. Action = [[ 0.09725618  0.13899007 -0.23249172  0.705933  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.09301474  0.13032351]]. Action = [[0.08293527 0.2142514  0.06077534 0.93111515]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 926 is [True, False, False, False, True, False]
Scene graph at timestep 926 is [True, False, False, False, False, True]
State prediction error at timestep 926 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.07996438  0.14405699]]. Action = [[-0.02339336 -0.10496429 -0.23223598 -0.8817319 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 927 is [True, False, False, False, False, True]
Current timestep = 928. State = [[-0.07910397  0.14667808]]. Action = [[ 0.07184625  0.13394335 -0.1482792  -0.6099195 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 928 is [True, False, False, False, False, True]
Current timestep = 929. State = [[-0.069168    0.14667954]]. Action = [[ 0.12120125 -0.09667696 -0.2398431   0.9690535 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 929 is [True, False, False, False, False, True]
Scene graph at timestep 929 is [True, False, False, False, False, True]
State prediction error at timestep 929 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of 0
Current timestep = 930. State = [[-0.05888052  0.13259517]]. Action = [[-0.2275448  -0.23639572 -0.02603784  0.6511829 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 930 is [True, False, False, False, False, True]
Current timestep = 931. State = [[-0.06347656  0.1247444 ]]. Action = [[-0.05513196  0.10459474 -0.24165301 -0.94669527]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 931 is [True, False, False, False, False, True]
Current timestep = 932. State = [[-0.0652752   0.12188081]]. Action = [[ 0.10069746 -0.10974652  0.05436596  0.98943985]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 932 is [True, False, False, False, True, False]
Current timestep = 933. State = [[-0.0672165   0.12591754]]. Action = [[ 0.01683539  0.19317803 -0.201093    0.6931505 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 933 is [True, False, False, False, True, False]
Current timestep = 934. State = [[-0.0635073   0.12141668]]. Action = [[ 0.16430211 -0.21968594 -0.23705454  0.9362502 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 934 is [True, False, False, False, False, True]
Scene graph at timestep 934 is [True, False, False, False, True, False]
State prediction error at timestep 934 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[-0.05722927  0.1174492 ]]. Action = [[0.00792921 0.2123546  0.16194424 0.05093086]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.04923579  0.12194724]]. Action = [[ 0.08740494 -0.14355733  0.2340551  -0.16396713]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 936 is [True, False, False, False, True, False]
Scene graph at timestep 936 is [False, True, False, False, True, False]
State prediction error at timestep 936 is tensor(2.3507e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of 1
Current timestep = 937. State = [[-0.04650624  0.11610581]]. Action = [[-0.16138482 -0.01825096  0.11162373 -0.8203638 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 937 is [False, True, False, False, True, False]
Current timestep = 938. State = [[-0.04996615  0.11157858]]. Action = [[-0.0925857  -0.07484254  0.02239799  0.1828984 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 938 is [False, True, False, False, True, False]
Current timestep = 939. State = [[-0.06081894  0.12067524]]. Action = [[-0.24295191  0.23365247  0.15506828  0.9376861 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 939 is [False, True, False, False, True, False]
Current timestep = 940. State = [[-0.07810843  0.14605536]]. Action = [[0.11207423 0.14163703 0.23472828 0.07259333]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.07286061  0.1553622 ]]. Action = [[ 0.2423771  -0.0007634   0.05166996  0.1735338 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 941 is [True, False, False, False, False, True]
Current timestep = 942. State = [[-0.0700177   0.14464201]]. Action = [[-0.23875032 -0.23478179  0.02298048  0.823779  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 942 is [True, False, False, False, False, True]
Current timestep = 943. State = [[-0.06429369  0.11755239]]. Action = [[ 0.18843657 -0.23178339  0.08898902 -0.6828677 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 943 is [True, False, False, False, False, True]
Scene graph at timestep 943 is [True, False, False, False, True, False]
State prediction error at timestep 943 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of 1
Current timestep = 944. State = [[-0.06268037  0.08537147]]. Action = [[-0.21921621 -0.15010063 -0.12971927 -0.28961885]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 944 is [True, False, False, False, True, False]
Current timestep = 945. State = [[-0.07016519  0.08119064]]. Action = [[ 0.14673942  0.1422143   0.119147   -0.24255341]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 945 is [True, False, False, False, True, False]
Current timestep = 946. State = [[-0.07059388  0.08278992]]. Action = [[-0.11607134 -0.06629485  0.06802124 -0.6682651 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.07082062  0.08307383]]. Action = [[0.06005576 0.04852742 0.12695739 0.2672299 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 947 is [True, False, False, False, True, False]
Current timestep = 948. State = [[-0.06755903  0.09258512]]. Action = [[ 0.15078712  0.14636493  0.22167897 -0.37891412]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.06556781  0.09327829]]. Action = [[-0.15600024 -0.16451523 -0.2305701  -0.9667858 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 949 is [True, False, False, False, True, False]
Scene graph at timestep 949 is [True, False, False, False, True, False]
State prediction error at timestep 949 is tensor(5.6037e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 949 of 0
Current timestep = 950. State = [[-0.06880977  0.08482941]]. Action = [[ 0.00397125 -0.04225497  0.12613147 -0.6004533 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 950 is [True, False, False, False, True, False]
Scene graph at timestep 950 is [True, False, False, False, True, False]
State prediction error at timestep 950 is tensor(8.1727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of 1
Current timestep = 951. State = [[-0.07086512  0.09369368]]. Action = [[0.05541933 0.24594495 0.13232216 0.7645581 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 951 is [True, False, False, False, True, False]
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(2.6380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[-0.06634635  0.10061025]]. Action = [[ 0.22185749 -0.12603483 -0.05604315  0.9489622 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.05046848  0.10904436]]. Action = [[ 0.02052006  0.23893556  0.21233416 -0.84938145]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.04957718  0.12278549]]. Action = [[-0.21964194 -0.02711646 -0.03670008 -0.804319  ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 954 is [True, False, False, False, True, False]
Scene graph at timestep 954 is [False, True, False, False, True, False]
State prediction error at timestep 954 is tensor(4.4746e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of 1
Current timestep = 955. State = [[-0.05026521  0.12019826]]. Action = [[ 0.09612191 -0.12732537 -0.15823737 -0.61612195]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 955 is [False, True, False, False, True, False]
Current timestep = 956. State = [[-0.04649857  0.11058725]]. Action = [[ 0.03920749 -0.05456796  0.21269867 -0.6226327 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 956 is [True, False, False, False, True, False]
Scene graph at timestep 956 is [False, True, False, False, True, False]
State prediction error at timestep 956 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of 1
Current timestep = 957. State = [[-0.03919739  0.11376709]]. Action = [[ 0.19652599  0.2014167  -0.16053988  0.85726905]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 957 is [False, True, False, False, True, False]
Current timestep = 958. State = [[-0.03318865  0.12224934]]. Action = [[-0.20008652 -0.0495674  -0.06963557 -0.6332209 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 958 is [False, True, False, False, True, False]
Current timestep = 959. State = [[-0.16477111 -0.06268555]]. Action = [[ 0.00118038 -0.11994505  0.11300865 -0.70208377]]. Reward = [100.]
Curr episode timestep = 71
Scene graph at timestep 959 is [False, True, False, False, True, False]
Current timestep = 960. State = [[-0.1456917  -0.08025802]]. Action = [[ 0.11669922 -0.15995485 -0.20414011  0.8671844 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 960 is [True, False, False, False, True, False]
Current timestep = 961. State = [[-0.12795725 -0.10450236]]. Action = [[ 0.16427895 -0.20373505 -0.21524863  0.79708314]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 961 is [True, False, False, False, True, False]
Scene graph at timestep 961 is [True, False, False, False, True, False]
State prediction error at timestep 961 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 961 of 1
Current timestep = 962. State = [[-0.11107349 -0.1189253 ]]. Action = [[-0.09375373  0.10909566  0.10703543 -0.8706697 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 962 is [True, False, False, False, True, False]
Current timestep = 963. State = [[-0.12069647 -0.10810017]]. Action = [[-0.24113707  0.1278095  -0.15680991  0.93639994]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 963 is [True, False, False, False, True, False]
Current timestep = 964. State = [[-0.130726   -0.09040673]]. Action = [[ 0.04021734  0.12314633 -0.12263435 -0.5028568 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 964 is [True, False, False, False, True, False]
Current timestep = 965. State = [[-0.13843454 -0.07710393]]. Action = [[-0.15148725  0.0399459  -0.15629192 -0.47475052]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 965 is [True, False, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of -1
Current timestep = 966. State = [[-0.15169035 -0.06240147]]. Action = [[0.0120275  0.18015838 0.21027362 0.44789457]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 966 is [True, False, False, False, True, False]
Current timestep = 967. State = [[-0.15142848 -0.05003478]]. Action = [[ 0.12843132 -0.02722722 -0.06022432 -0.75854367]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 967 is [True, False, False, False, True, False]
Current timestep = 968. State = [[-0.15079552 -0.03608461]]. Action = [[-0.01392835  0.21892828  0.11670321 -0.7269624 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 968 is [True, False, False, False, True, False]
Scene graph at timestep 968 is [True, False, False, False, True, False]
State prediction error at timestep 968 is tensor(5.7392e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of 1
Current timestep = 969. State = [[-0.1485412  -0.00820864]]. Action = [[0.11077136 0.18092555 0.03790134 0.33730292]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 969 is [True, False, False, False, True, False]
Current timestep = 970. State = [[-0.13346285  0.01110969]]. Action = [[ 0.12571979  0.09472463 -0.04679742  0.37603748]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 970 is [True, False, False, False, True, False]
Scene graph at timestep 970 is [True, False, False, False, True, False]
State prediction error at timestep 970 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.11759377  0.03306601]]. Action = [[0.11489204 0.19450843 0.16440138 0.64475906]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 971 is [True, False, False, False, True, False]
Current timestep = 972. State = [[-0.11455724  0.05650362]]. Action = [[-0.20389618  0.13685665 -0.01286575  0.86036277]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 972 is [True, False, False, False, True, False]
Current timestep = 973. State = [[-0.12450549  0.08234449]]. Action = [[-0.03705178  0.18496877 -0.13324594  0.9875202 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 973 is [True, False, False, False, True, False]
Current timestep = 974. State = [[-0.12443645  0.10553041]]. Action = [[ 0.18813425  0.14858472 -0.09732687 -0.6444495 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 974 is [True, False, False, False, True, False]
Current timestep = 975. State = [[-0.12363481  0.12695076]]. Action = [[-0.21909821  0.1387876   0.14700335 -0.45983934]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 975 is [True, False, False, False, True, False]
Scene graph at timestep 975 is [True, False, False, False, False, True]
State prediction error at timestep 975 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of -1
Current timestep = 976. State = [[-0.14213069  0.15821834]]. Action = [[-0.15878998  0.23980337  0.1832465   0.9117198 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 976 is [True, False, False, False, False, True]
Current timestep = 977. State = [[-0.15260136  0.17034753]]. Action = [[ 0.00644168 -0.10602529 -0.15538208 -0.9305917 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 977 is [True, False, False, False, False, True]
Current timestep = 978. State = [[-0.15978122  0.17540836]]. Action = [[-0.1883308   0.08926862  0.1680316  -0.6197342 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 978 is [True, False, False, False, False, True]
Scene graph at timestep 978 is [True, False, False, False, False, True]
State prediction error at timestep 978 is tensor(2.1956e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of -1
Current timestep = 979. State = [[-0.17978685  0.19126272]]. Action = [[-0.06138286  0.16605198 -0.12501617 -0.00453937]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 979 is [True, False, False, False, False, True]
Current timestep = 980. State = [[-0.18231097  0.1895258 ]]. Action = [[ 0.06306076 -0.22114289 -0.23683716 -0.68738246]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 980 is [True, False, False, False, False, True]
Current timestep = 981. State = [[-0.1744143   0.17721076]]. Action = [[ 0.18661493  0.00600222 -0.17768048  0.7066566 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 981 is [True, False, False, False, False, True]
Current timestep = 982. State = [[-0.15992723  0.17906868]]. Action = [[0.22742802 0.09798351 0.21407837 0.38083696]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 982 is [True, False, False, False, False, True]
Current timestep = 983. State = [[-0.14045067  0.17331825]]. Action = [[-0.02343155 -0.18828802 -0.01354951  0.36061084]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 983 is [True, False, False, False, False, True]
Current timestep = 984. State = [[-0.12799278  0.15380263]]. Action = [[ 0.20744154 -0.1360344  -0.11148779 -0.80264133]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 984 is [True, False, False, False, False, True]
Current timestep = 985. State = [[-0.10158858  0.13072738]]. Action = [[ 0.21837497 -0.1839806   0.24036908  0.1795764 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 985 is [True, False, False, False, False, True]
Current timestep = 986. State = [[-0.08206709  0.10211843]]. Action = [[-0.07815745 -0.22471753 -0.23040392  0.83760357]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 986 is [True, False, False, False, False, True]
Current timestep = 987. State = [[-0.08289368  0.09938781]]. Action = [[-0.06660756  0.23345107  0.10722643 -0.7506142 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 987 is [True, False, False, False, True, False]
Scene graph at timestep 987 is [True, False, False, False, True, False]
State prediction error at timestep 987 is tensor(4.1596e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 1
Current timestep = 988. State = [[-0.080704    0.12434977]]. Action = [[ 0.21717167  0.24241015 -0.17743993  0.9714458 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 988 is [True, False, False, False, True, False]
Current timestep = 989. State = [[-0.0696233  0.1325972]]. Action = [[-0.10367353 -0.15659766  0.19253308 -0.14099145]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [True, False, False, False, False, True]
State prediction error at timestep 989 is tensor(9.5043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.0675301   0.12994511]]. Action = [[ 0.12988743  0.07524955 -0.09677388 -0.19118035]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 990 is [True, False, False, False, False, True]
Scene graph at timestep 990 is [True, False, False, False, False, True]
State prediction error at timestep 990 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 990 of 0
Current timestep = 991. State = [[-0.0552595  0.125043 ]]. Action = [[ 0.12072247 -0.14653456  0.03182027 -0.6810441 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 991 is [True, False, False, False, False, True]
Scene graph at timestep 991 is [True, False, False, False, False, True]
State prediction error at timestep 991 is tensor(7.0276e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.04965388  0.1273467 ]]. Action = [[-0.19330217  0.16859466  0.2404958   0.9422157 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 992 is [True, False, False, False, False, True]
Current timestep = 993. State = [[-0.05107353  0.14046964]]. Action = [[ 0.20931125  0.08540577  0.00479186 -0.612164  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 993 is [False, True, False, False, False, True]
Current timestep = 994. State = [[-0.0307941  0.1407461]]. Action = [[ 0.21761209 -0.11821024 -0.21171954 -0.89434564]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 994 is [True, False, False, False, False, True]
Current timestep = 995. State = [[-0.018105    0.14701407]]. Action = [[-0.15705404  0.15637988  0.07432055  0.9147483 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 995 is [False, True, False, False, False, True]
Current timestep = 996. State = [[-0.02687333  0.16680595]]. Action = [[-0.15683018  0.15301836  0.23669529 -0.11713552]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 996 is [False, True, False, False, False, True]
Current timestep = 997. State = [[-0.04197657  0.19350934]]. Action = [[-0.22263928  0.19436014 -0.14676462 -0.87607133]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 997 is [False, True, False, False, False, True]
Current timestep = 998. State = [[-0.05184513  0.20597091]]. Action = [[ 0.08458364 -0.0834455   0.16563869 -0.7442338 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 998 is [False, True, False, False, False, True]
Current timestep = 999. State = [[-0.05223129  0.20716217]]. Action = [[ 0.04781011  0.07410836 -0.02378105  0.497401  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 999 is [True, False, False, False, False, True]
Current timestep = 1000. State = [[-0.05374809  0.22097367]]. Action = [[ 0.06530625  0.23780757 -0.05449498 -0.8501986 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1000 is [True, False, False, False, False, True]
Current timestep = 1001. State = [[-0.05161595  0.24254738]]. Action = [[0.06771955 0.11302039 0.04449132 0.40010786]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1001 is [True, False, False, False, False, True]
Current timestep = 1002. State = [[-0.03952073  0.2570723 ]]. Action = [[ 0.12831223  0.06333879 -0.21934779  0.4879048 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1002 is [True, False, False, False, False, True]
Scene graph at timestep 1002 is [False, True, False, False, False, True]
State prediction error at timestep 1002 is tensor(7.6917e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of -1
Current timestep = 1003. State = [[-0.02172682  0.26052684]]. Action = [[-0.01832813 -0.17034684  0.2186504  -0.6131636 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1003 is [False, True, False, False, False, True]
Current timestep = 1004. State = [[-0.0147327   0.24942447]]. Action = [[0.19711363 0.01376072 0.0082472  0.48659837]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1004 is [False, True, False, False, False, True]
Scene graph at timestep 1004 is [False, True, False, False, False, True]
State prediction error at timestep 1004 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of 0
Current timestep = 1005. State = [[-0.00119848  0.24724428]]. Action = [[-0.17707872 -0.04253525 -0.1831962  -0.9932243 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1005 is [False, True, False, False, False, True]
Current timestep = 1006. State = [[0.00225712 0.23505928]]. Action = [[ 0.12320927 -0.19102149 -0.05209848 -0.2758364 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1006 is [False, True, False, False, False, True]
Scene graph at timestep 1006 is [False, True, False, False, False, True]
State prediction error at timestep 1006 is tensor(4.8166e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[0.01193346 0.21122837]]. Action = [[ 0.0835366  -0.1542209   0.13700676  0.6011472 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1007 is [False, True, False, False, False, True]
Scene graph at timestep 1007 is [False, True, False, False, False, True]
State prediction error at timestep 1007 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[0.02340033 0.19573256]]. Action = [[ 0.24425554  0.02821007 -0.19989085  0.703668  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1008 is [False, True, False, False, False, True]
Current timestep = 1009. State = [[0.03914895 0.1818242 ]]. Action = [[ 0.04934347 -0.23995805  0.02078989  0.37539077]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1009 is [False, True, False, False, False, True]
Current timestep = 1010. State = [[0.05004471 0.15149747]]. Action = [[-0.03419651 -0.23071218  0.01704651  0.349833  ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1010 is [False, True, False, False, False, True]
Current timestep = 1011. State = [[0.05407744 0.12310541]]. Action = [[-0.13529532 -0.19097729  0.11665222  0.944484  ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1011 is [False, False, True, False, False, True]
Current timestep = 1012. State = [[0.04711435 0.10943728]]. Action = [[-0.16617054  0.00801468 -0.00533326 -0.02875167]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1012 is [False, False, True, False, True, False]
Scene graph at timestep 1012 is [False, True, False, False, True, False]
State prediction error at timestep 1012 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1012 of 1
Current timestep = 1013. State = [[-0.16086403  0.11618332]]. Action = [[ 0.17472857 -0.11580959  0.00905454 -0.00976831]]. Reward = [100.]
Curr episode timestep = 53
Scene graph at timestep 1013 is [False, True, False, False, True, False]
Current timestep = 1014. State = [[-0.13953583  0.11693441]]. Action = [[ 0.05966437 -0.23521939 -0.13811503 -0.6541978 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1014 is [True, False, False, False, True, False]
Current timestep = 1015. State = [[-0.14121298  0.09276967]]. Action = [[-0.23731412 -0.20683461 -0.2050946   0.861285  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1015 is [True, False, False, False, True, False]
Current timestep = 1016. State = [[-0.14909387  0.0716003 ]]. Action = [[-0.12107512 -0.10381424  0.04429418  0.19208133]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1016 is [True, False, False, False, True, False]
Scene graph at timestep 1016 is [True, False, False, False, True, False]
State prediction error at timestep 1016 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[-0.16534573  0.04499122]]. Action = [[-0.12812272 -0.21922329  0.2171219   0.26271725]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1017 is [True, False, False, False, True, False]
Scene graph at timestep 1017 is [True, False, False, False, True, False]
State prediction error at timestep 1017 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of -1
Current timestep = 1018. State = [[-0.179988    0.03155494]]. Action = [[-0.0396495   0.08802885  0.14571604  0.10560155]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1018 is [True, False, False, False, True, False]
Current timestep = 1019. State = [[-0.17698313  0.02312229]]. Action = [[ 0.19130775 -0.24443926 -0.05056939  0.9593383 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1019 is [True, False, False, False, True, False]
Scene graph at timestep 1019 is [True, False, False, False, True, False]
State prediction error at timestep 1019 is tensor(1.9358e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1019 of 0
Current timestep = 1020. State = [[-0.17475224 -0.00662069]]. Action = [[-0.16032079 -0.20613757 -0.16525082  0.74765384]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1020 is [True, False, False, False, True, False]
Current timestep = 1021. State = [[-0.17917396 -0.01731651]]. Action = [[ 0.1141274   0.09478551 -0.10635495  0.16311479]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1021 is [True, False, False, False, True, False]
Current timestep = 1022. State = [[-0.1820039  -0.02138863]]. Action = [[-0.15812698 -0.1097129   0.08046889  0.23933375]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1022 is [True, False, False, False, True, False]
Current timestep = 1023. State = [[-0.18886265 -0.03432129]]. Action = [[-0.07869036 -0.11267364 -0.09429535  0.32117867]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1023 is [True, False, False, False, True, False]
Current timestep = 1024. State = [[-0.20311517 -0.04829047]]. Action = [[-0.13009314 -0.05508514 -0.18298033  0.94908917]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1024 is [True, False, False, False, True, False]
Current timestep = 1025. State = [[-0.20944718 -0.06009979]]. Action = [[ 0.07676843 -0.11468899  0.11756334  0.6155015 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1025 is [True, False, False, False, True, False]
Current timestep = 1026. State = [[-0.20692408 -0.07521277]]. Action = [[ 0.00192559 -0.10239342 -0.24770269  0.8271766 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1026 is [True, False, False, False, True, False]
Current timestep = 1027. State = [[-0.2172908  -0.09740625]]. Action = [[-0.18288258 -0.18062074  0.02362376 -0.8808981 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1027 is [True, False, False, False, True, False]
Scene graph at timestep 1027 is [True, False, False, False, True, False]
State prediction error at timestep 1027 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of -1
Current timestep = 1028. State = [[-0.2314599  -0.10422067]]. Action = [[ 0.02058855  0.22822693 -0.20564385  0.72807455]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1028 is [True, False, False, False, True, False]
Scene graph at timestep 1028 is [True, False, False, False, True, False]
State prediction error at timestep 1028 is tensor(7.9853e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[-0.22339648 -0.07484385]]. Action = [[ 0.24564382  0.21949819 -0.18330647 -0.7927462 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1029 is [True, False, False, False, True, False]
Current timestep = 1030. State = [[-0.20704664 -0.0602676 ]]. Action = [[ 0.16185772 -0.0572117   0.00293252 -0.35356623]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1030 is [True, False, False, False, True, False]
Current timestep = 1031. State = [[-0.19499291 -0.04823255]]. Action = [[-0.0110957   0.19266692  0.20443723  0.73207736]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1031 is [True, False, False, False, True, False]
Current timestep = 1032. State = [[-0.18890548 -0.0475063 ]]. Action = [[ 0.07728687 -0.1918451   0.09301543 -0.6847322 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1032 is [True, False, False, False, True, False]
Current timestep = 1033. State = [[-0.17494752 -0.04970697]]. Action = [[ 0.2128604   0.0958339  -0.01945481  0.7287419 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1033 is [True, False, False, False, True, False]
Current timestep = 1034. State = [[-0.14774959 -0.04721056]]. Action = [[ 0.20246077 -0.01245491 -0.09723979  0.8873416 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1034 is [True, False, False, False, True, False]
Current timestep = 1035. State = [[-0.12235858 -0.03352776]]. Action = [[ 0.14084929  0.2075071  -0.06533358  0.84312654]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1035 is [True, False, False, False, True, False]
Current timestep = 1036. State = [[-0.11167546 -0.02278516]]. Action = [[-0.10262972 -0.00823809 -0.03740618  0.17137027]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1036 is [True, False, False, False, True, False]
Scene graph at timestep 1036 is [True, False, False, False, True, False]
State prediction error at timestep 1036 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 1
Current timestep = 1037. State = [[-0.11241093 -0.03327044]]. Action = [[-0.04483351 -0.21827781  0.17343545  0.6883404 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1037 is [True, False, False, False, True, False]
Scene graph at timestep 1037 is [True, False, False, False, True, False]
State prediction error at timestep 1037 is tensor(7.7398e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of 1
Current timestep = 1038. State = [[-0.11521777 -0.05702271]]. Action = [[-0.04861915 -0.18727805  0.20562601 -0.503009  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1038 is [True, False, False, False, True, False]
Current timestep = 1039. State = [[-0.11669648 -0.06774848]]. Action = [[ 0.13064823  0.02802983  0.1832628  -0.43724716]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1039 is [True, False, False, False, True, False]
Current timestep = 1040. State = [[-0.11041557 -0.07507325]]. Action = [[ 0.10352001 -0.09720358 -0.15596394  0.06737483]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1040 is [True, False, False, False, True, False]
Current timestep = 1041. State = [[-0.10528702 -0.07375421]]. Action = [[-0.2014281   0.15303016 -0.17383854 -0.8350491 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1041 is [True, False, False, False, True, False]
Current timestep = 1042. State = [[-0.11694856 -0.06060288]]. Action = [[-0.21214275  0.1093694   0.12710309 -0.71263677]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1042 is [True, False, False, False, True, False]
Current timestep = 1043. State = [[-0.14198801 -0.06511799]]. Action = [[-0.23484233 -0.19732097 -0.12021589 -0.5269918 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1043 is [True, False, False, False, True, False]
Current timestep = 1044. State = [[-0.15696542 -0.08132557]]. Action = [[ 0.15859604 -0.10199136  0.02941114 -0.7525223 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1044 is [True, False, False, False, True, False]
Current timestep = 1045. State = [[-0.16397671 -0.10362532]]. Action = [[-0.15963767 -0.23008381  0.11947116 -0.40142047]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1045 is [True, False, False, False, True, False]
Current timestep = 1046. State = [[-0.18372136 -0.13312967]]. Action = [[-0.2116886  -0.16435751  0.14064494  0.0504899 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1046 is [True, False, False, False, True, False]
Current timestep = 1047. State = [[-0.19207054 -0.13640895]]. Action = [[0.19638759 0.19648272 0.00401631 0.58068097]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1047 is [True, False, False, True, False, False]
Current timestep = 1048. State = [[-0.1932002  -0.13778865]]. Action = [[-0.14944984 -0.1467991   0.15281025 -0.03522778]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1048 is [True, False, False, True, False, False]
Current timestep = 1049. State = [[-0.20305796 -0.13718882]]. Action = [[-0.16895343  0.11600253 -0.12793833 -0.43176687]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1049 is [True, False, False, True, False, False]
Current timestep = 1050. State = [[-0.21052048 -0.11975054]]. Action = [[0.1063019  0.23119158 0.08775795 0.70862985]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1050 is [True, False, False, True, False, False]
Current timestep = 1051. State = [[-0.21310996 -0.10488571]]. Action = [[-0.10385689 -0.04223108 -0.13114816  0.10143971]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1051 is [True, False, False, False, True, False]
Current timestep = 1052. State = [[-0.22718549 -0.1098901 ]]. Action = [[-0.19442655 -0.07817996  0.22638202 -0.00894719]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1052 is [True, False, False, False, True, False]
Current timestep = 1053. State = [[-0.23786768 -0.10313162]]. Action = [[ 0.11317387  0.20245367  0.2274819  -0.715147  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1053 is [True, False, False, False, True, False]
Current timestep = 1054. State = [[-0.239364   -0.07827181]]. Action = [[-0.04325038  0.18569744  0.0521265   0.6630156 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1054 is [True, False, False, False, True, False]
Current timestep = 1055. State = [[-0.24452372 -0.06231419]]. Action = [[-0.0742542  -0.00155571  0.21418199  0.7899051 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1055 is [True, False, False, False, True, False]
Current timestep = 1056. State = [[-0.23961243 -0.04627487]]. Action = [[0.22761953 0.2104693  0.10474676 0.1312306 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1056 is [True, False, False, False, True, False]
Current timestep = 1057. State = [[-0.23777726 -0.02667969]]. Action = [[-0.09376101  0.08992457  0.10189211  0.91643023]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Current timestep = 1058. State = [[-0.24096332 -0.01381866]]. Action = [[-0.03728022  0.05449003  0.09089336 -0.67724735]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Current timestep = 1059. State = [[-0.23458733 -0.00563265]]. Action = [[ 0.1916756   0.03716487 -0.11788785  0.47172117]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Current timestep = 1060. State = [[-0.21685976 -0.01219997]]. Action = [[ 0.23726517 -0.21692704  0.07333821 -0.56724447]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Current timestep = 1061. State = [[-0.20088834 -0.03526963]]. Action = [[-0.0798901  -0.20139678 -0.0282338  -0.3407097 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Current timestep = 1062. State = [[-0.19040638 -0.05829713]]. Action = [[ 0.2095232  -0.10609356 -0.11515157  0.04327321]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1062 is [True, False, False, False, True, False]
Current timestep = 1063. State = [[-0.18038952 -0.0573281 ]]. Action = [[-0.05147606  0.20467979 -0.12005922  0.16345882]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1063 is [True, False, False, False, True, False]
Current timestep = 1064. State = [[-0.17066476 -0.05999913]]. Action = [[ 0.24376497 -0.225768    0.10390332  0.9059069 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1064 is [True, False, False, False, True, False]
Scene graph at timestep 1064 is [True, False, False, False, True, False]
State prediction error at timestep 1064 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of -1
Current timestep = 1065. State = [[-0.14439139 -0.0766606 ]]. Action = [[ 0.15253729 -0.07734221 -0.15018356  0.88365555]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1065 is [True, False, False, False, True, False]
Current timestep = 1066. State = [[-0.12117489 -0.08035266]]. Action = [[ 0.194843    0.02005085 -0.22763965  0.775192  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1066 is [True, False, False, False, True, False]
Current timestep = 1067. State = [[-0.0991009  -0.07671949]]. Action = [[ 0.0651592   0.07303122 -0.14684828 -0.29305136]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1067 is [True, False, False, False, True, False]
Current timestep = 1068. State = [[-0.08551246 -0.0792012 ]]. Action = [[ 0.10886627 -0.11548975  0.049741    0.48356438]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1068 is [True, False, False, False, True, False]
Scene graph at timestep 1068 is [True, False, False, False, True, False]
State prediction error at timestep 1068 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1068 of 1
Current timestep = 1069. State = [[-0.06468635 -0.09590974]]. Action = [[ 0.18860406 -0.16706015  0.06181997 -0.03144485]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1069 is [True, False, False, False, True, False]
Current timestep = 1070. State = [[-0.03803581 -0.10245517]]. Action = [[ 0.2036717   0.08032042 -0.03119795  0.46396923]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1070 is [True, False, False, False, True, False]
Current timestep = 1071. State = [[-0.01585239 -0.10428637]]. Action = [[ 0.07589716 -0.08302101  0.23191017  0.80082655]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1071 is [False, True, False, False, True, False]
Current timestep = 1072. State = [[-0.19114478 -0.13794772]]. Action = [[-0.15842651  0.17581204 -0.07495394  0.04500425]]. Reward = [100.]
Curr episode timestep = 58
Scene graph at timestep 1072 is [False, True, False, False, True, False]
Current timestep = 1073. State = [[-0.18589965 -0.15953447]]. Action = [[-0.19011351 -0.06979458 -0.08327784 -0.9398364 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1073 is [True, False, False, True, False, False]
Current timestep = 1074. State = [[-0.19678067 -0.16360016]]. Action = [[-0.11143431  0.08501345 -0.06473267  0.29019105]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1074 is [True, False, False, True, False, False]
Scene graph at timestep 1074 is [True, False, False, True, False, False]
State prediction error at timestep 1074 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1074 of -1
Current timestep = 1075. State = [[-0.20521168 -0.14935283]]. Action = [[0.09918022 0.16771692 0.20092061 0.6190176 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1075 is [True, False, False, True, False, False]
Current timestep = 1076. State = [[-0.20620224 -0.14215408]]. Action = [[-0.08291714 -0.06962352  0.02540371 -0.37278628]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1076 is [True, False, False, True, False, False]
Scene graph at timestep 1076 is [True, False, False, True, False, False]
State prediction error at timestep 1076 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1076 of 1
Current timestep = 1077. State = [[-0.20836896 -0.15216444]]. Action = [[ 0.00884545 -0.13948071 -0.12638088  0.61881065]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1077 is [True, False, False, True, False, False]
Current timestep = 1078. State = [[-0.20697466 -0.14615081]]. Action = [[0.11920252 0.23933503 0.18029618 0.40219915]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1078 is [True, False, False, True, False, False]
Current timestep = 1079. State = [[-0.20277534 -0.12743828]]. Action = [[-0.00105216  0.09384936  0.16543785  0.57873917]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1079 is [True, False, False, True, False, False]
Current timestep = 1080. State = [[-0.20334648 -0.1264449 ]]. Action = [[-0.05462554 -0.13883857 -0.00581701  0.8960861 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1080 is [True, False, False, True, False, False]
Current timestep = 1081. State = [[-0.20702203 -0.13509095]]. Action = [[-0.10064812 -0.05144879  0.00551984 -0.07940215]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1081 is [True, False, False, True, False, False]
Current timestep = 1082. State = [[-0.21333277 -0.14225191]]. Action = [[-0.06109901 -0.01427087  0.00950539  0.02246141]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1082 is [True, False, False, True, False, False]
Current timestep = 1083. State = [[-0.22375011 -0.14970998]]. Action = [[-0.1258107  -0.07052612  0.17098373 -0.5276349 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1083 is [True, False, False, True, False, False]
Current timestep = 1084. State = [[-0.22934483 -0.14864673]]. Action = [[ 0.01966453  0.12061328 -0.0348061  -0.80695957]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1084 is [True, False, False, True, False, False]
Current timestep = 1085. State = [[-0.2293008  -0.13073449]]. Action = [[ 0.03311941  0.20925084  0.09203556 -0.2063992 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1085 is [True, False, False, True, False, False]
Current timestep = 1086. State = [[-0.23446691 -0.10610744]]. Action = [[-0.13077322  0.11585316  0.09335625  0.705796  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1086 is [True, False, False, True, False, False]
Current timestep = 1087. State = [[-0.2527464  -0.08642752]]. Action = [[-0.13929893  0.13357788  0.16894352  0.0071454 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Current timestep = 1088. State = [[-0.26400626 -0.07366447]]. Action = [[-0.0736455   0.03183776  0.22751924  0.3900169 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Current timestep = 1089. State = [[-0.27049014 -0.06900605]]. Action = [[-0.09227553  0.00385916  0.08008653 -0.31443512]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Current timestep = 1090. State = [[-0.27279162 -0.06025487]]. Action = [[ 0.04619017  0.14026833 -0.16526942 -0.60460067]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Current timestep = 1091. State = [[-0.2730063  -0.03710201]]. Action = [[0.03940144 0.19876796 0.20285773 0.426579  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Scene graph at timestep 1091 is [True, False, False, False, True, False]
State prediction error at timestep 1091 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of -1
Current timestep = 1092. State = [[-0.27114037 -0.01924763]]. Action = [[0.05705529 0.00281462 0.05298117 0.89663696]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1092 is [True, False, False, False, True, False]
Current timestep = 1093. State = [[-0.26931104 -0.01893285]]. Action = [[-0.1619347   0.19445169 -0.21854806 -0.91039217]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1093 is [True, False, False, False, True, False]
Current timestep = 1094. State = [[-0.2681488  -0.02146577]]. Action = [[ 0.00376827 -0.08204456 -0.01753327 -0.60398537]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1094 is [True, False, False, False, True, False]
Scene graph at timestep 1094 is [True, False, False, False, True, False]
State prediction error at timestep 1094 is tensor(1.1325e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of 0
Current timestep = 1095. State = [[-0.25962615 -0.01384892]]. Action = [[ 0.14620638  0.22327167 -0.22584589 -0.61092204]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Current timestep = 1096. State = [[-0.246065    0.00327484]]. Action = [[ 0.18730634  0.08932513 -0.10846581 -0.03593028]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Current timestep = 1097. State = [[-0.21895461  0.01560595]]. Action = [[ 0.22921693  0.06443554 -0.05216794 -0.8243446 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Current timestep = 1098. State = [[-0.19750436  0.03401776]]. Action = [[ 0.04030383  0.15766782  0.01886773 -0.92681724]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Current timestep = 1099. State = [[-0.18056129  0.03566309]]. Action = [[ 0.18383846 -0.18335348  0.01700124  0.37691963]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Current timestep = 1100. State = [[-0.1728048   0.03600019]]. Action = [[-0.1948127   0.10536101 -0.11333571 -0.18462205]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Current timestep = 1101. State = [[-0.1742926  0.0441145]]. Action = [[ 0.10956994  0.08731526  0.11208618 -0.25877976]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Current timestep = 1102. State = [[-0.17401102  0.04331807]]. Action = [[-0.13006313 -0.11766647  0.10402068  0.03045535]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Current timestep = 1103. State = [[-0.18130101  0.03647028]]. Action = [[-0.14860237 -0.02891511 -0.19305421 -0.42778623]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Scene graph at timestep 1103 is [True, False, False, False, True, False]
State prediction error at timestep 1103 is tensor(9.4383e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of 1
Current timestep = 1104. State = [[-0.19246204  0.02555786]]. Action = [[ 0.12167373 -0.1191749  -0.22290349 -0.58629197]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Current timestep = 1105. State = [[-0.19644701  0.01602399]]. Action = [[-0.1749828  -0.05384134 -0.22483878  0.96656656]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Current timestep = 1106. State = [[-0.20578177  0.02287051]]. Action = [[-0.09869215  0.23514616 -0.00716312 -0.9927639 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Current timestep = 1107. State = [[-0.2104585   0.04334125]]. Action = [[ 0.16221255  0.09400654 -0.15063877  0.7058673 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Current timestep = 1108. State = [[-0.20639129  0.05375944]]. Action = [[ 0.0658353   0.07645953 -0.01245882 -0.47321177]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Current timestep = 1109. State = [[-0.20485072  0.06156929]]. Action = [[-0.02007855  0.03049296 -0.1292306  -0.65863746]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Current timestep = 1110. State = [[-0.1943979   0.06532477]]. Action = [[ 0.22594988 -0.0105973  -0.06262462  0.10803246]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Scene graph at timestep 1110 is [True, False, False, False, True, False]
State prediction error at timestep 1110 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1110 of 1
Current timestep = 1111. State = [[-0.16793379  0.07737242]]. Action = [[0.1851033  0.16450414 0.16678679 0.07848024]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Current timestep = 1112. State = [[-0.1488314   0.09067252]]. Action = [[ 0.02818006  0.06542957 -0.02116376  0.71688795]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1112 is [True, False, False, False, True, False]
Current timestep = 1113. State = [[-0.13887034  0.09008662]]. Action = [[ 0.08115065 -0.13981554  0.12974954 -0.45300305]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1113 is [True, False, False, False, True, False]
Current timestep = 1114. State = [[-0.13452823  0.09669159]]. Action = [[ 0.00519675  0.20179296 -0.08749568 -0.8483823 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1114 is [True, False, False, False, True, False]
Current timestep = 1115. State = [[-0.13423614  0.10997052]]. Action = [[-0.14714402  0.02871999  0.11299664 -0.45303273]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1115 is [True, False, False, False, True, False]
Scene graph at timestep 1115 is [True, False, False, False, True, False]
State prediction error at timestep 1115 is tensor(2.9324e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1115 of 1
Current timestep = 1116. State = [[-0.13534938  0.11017308]]. Action = [[ 0.10653833 -0.10789958  0.02645656  0.02772343]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1116 is [True, False, False, False, True, False]
Current timestep = 1117. State = [[-0.13750602  0.11546705]]. Action = [[-0.05824882  0.18314907  0.14899987  0.12620938]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1117 is [True, False, False, False, True, False]
Current timestep = 1118. State = [[-0.14146157  0.11510563]]. Action = [[-0.1107709  -0.17079781  0.11857224  0.23205495]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1118 is [True, False, False, False, True, False]
Current timestep = 1119. State = [[-0.14948335  0.09435957]]. Action = [[-0.16303934 -0.21710421 -0.06323436  0.3874371 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1119 is [True, False, False, False, True, False]
Current timestep = 1120. State = [[-0.15374343  0.07176201]]. Action = [[ 0.16906175 -0.1063188  -0.21536374  0.83114684]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1120 is [True, False, False, False, True, False]
Scene graph at timestep 1120 is [True, False, False, False, True, False]
State prediction error at timestep 1120 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1120 of 1
Current timestep = 1121. State = [[-0.15123348  0.05628373]]. Action = [[-0.05658501 -0.07435778  0.08616707 -0.02655137]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1121 is [True, False, False, False, True, False]
Current timestep = 1122. State = [[-0.15404287  0.05298254]]. Action = [[-0.03604563  0.03827718  0.14213353 -0.928398  ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1122 is [True, False, False, False, True, False]
Scene graph at timestep 1122 is [True, False, False, False, True, False]
State prediction error at timestep 1122 is tensor(4.1711e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1122 of 0
Current timestep = 1123. State = [[-0.1553435   0.06545012]]. Action = [[ 0.08600113  0.23093033  0.15645957 -0.9032796 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1123 is [True, False, False, False, True, False]
Current timestep = 1124. State = [[-0.1519092   0.08168478]]. Action = [[ 0.12964654  0.07625765  0.03599867 -0.36367565]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Scene graph at timestep 1124 is [True, False, False, False, True, False]
State prediction error at timestep 1124 is tensor(7.2767e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of 1
Current timestep = 1125. State = [[-0.14554358  0.10260093]]. Action = [[-0.1284817   0.17635572  0.22344893  0.84226465]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Current timestep = 1126. State = [[-0.14837247  0.11581048]]. Action = [[ 0.12041727  0.01173332 -0.15913452  0.13624096]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Current timestep = 1127. State = [[-0.14414288  0.11407202]]. Action = [[-0.0567117  -0.12065776  0.17192012  0.606158  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1127 is [True, False, False, False, True, False]
Current timestep = 1128. State = [[-0.14526218  0.1111917 ]]. Action = [[-0.06627788  0.00740996 -0.21976528  0.45307708]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1128 is [True, False, False, False, True, False]
Current timestep = 1129. State = [[-0.14627129  0.09908602]]. Action = [[-0.00267988 -0.18729416 -0.17054287 -0.6987831 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1129 is [True, False, False, False, True, False]
Current timestep = 1130. State = [[-0.14791064  0.07730323]]. Action = [[-0.04786432 -0.16475055 -0.04641423 -0.1351043 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1130 is [True, False, False, False, True, False]
Current timestep = 1131. State = [[-0.15852106  0.05946678]]. Action = [[-0.14483416 -0.07196474 -0.07446353 -0.9492614 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1131 is [True, False, False, False, True, False]
Current timestep = 1132. State = [[-0.1616747   0.05080115]]. Action = [[ 0.14930078 -0.02599239 -0.15112819  0.15877223]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1132 is [True, False, False, False, True, False]
Current timestep = 1133. State = [[-0.16676567  0.03712216]]. Action = [[-0.22170496 -0.14528711  0.05090737 -0.33411264]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1133 is [True, False, False, False, True, False]
Current timestep = 1134. State = [[-0.16904737  0.01745187]]. Action = [[ 0.20113322 -0.13724345 -0.08688816  0.33020556]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1134 is [True, False, False, False, True, False]
Current timestep = 1135. State = [[-0.16229053  0.00258954]]. Action = [[ 0.04655212 -0.03962749  0.19394892 -0.7931072 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1135 is [True, False, False, False, True, False]
Scene graph at timestep 1135 is [True, False, False, False, True, False]
State prediction error at timestep 1135 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of 1
Current timestep = 1136. State = [[-0.15524748 -0.0144709 ]]. Action = [[ 0.08021206 -0.19382818 -0.10573509  0.5455129 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1136 is [True, False, False, False, True, False]
Current timestep = 1137. State = [[-0.15077548 -0.03194652]]. Action = [[ 0.00698346 -0.03564668  0.12585062 -0.46632075]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1137 is [True, False, False, False, True, False]
Scene graph at timestep 1137 is [True, False, False, False, True, False]
State prediction error at timestep 1137 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of 1
Current timestep = 1138. State = [[-0.14528136 -0.04433688]]. Action = [[ 0.0965617  -0.11093777  0.05037114 -0.16196704]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1138 is [True, False, False, False, True, False]
Current timestep = 1139. State = [[-0.14042464 -0.05774752]]. Action = [[-0.11671272 -0.080948    0.20304021  0.07703304]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1139 is [True, False, False, False, True, False]
Current timestep = 1140. State = [[-0.14772402 -0.07970508]]. Action = [[-0.12215477 -0.22404467  0.05304402 -0.28370655]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1140 is [True, False, False, False, True, False]
Scene graph at timestep 1140 is [True, False, False, False, True, False]
State prediction error at timestep 1140 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1140 of -1
Current timestep = 1141. State = [[-0.16308694 -0.09641254]]. Action = [[-0.231815    0.08493122 -0.11145121 -0.50245893]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1141 is [True, False, False, False, True, False]
Current timestep = 1142. State = [[-0.18479845 -0.098027  ]]. Action = [[-0.16534698 -0.04611485  0.04953659  0.71261024]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1142 is [True, False, False, False, True, False]
Current timestep = 1143. State = [[-0.19653933 -0.09948265]]. Action = [[ 0.1174227   0.02704898 -0.20109478  0.93728757]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1143 is [True, False, False, False, True, False]
Current timestep = 1144. State = [[-0.19778731 -0.09356447]]. Action = [[-0.07676864  0.06593603  0.12041798  0.8441844 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1144 is [True, False, False, False, True, False]
Current timestep = 1145. State = [[-0.20110612 -0.08246168]]. Action = [[-0.03409515  0.09771061  0.07520592 -0.14896828]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1145 is [True, False, False, False, True, False]
Scene graph at timestep 1145 is [True, False, False, False, True, False]
State prediction error at timestep 1145 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1145 of -1
Current timestep = 1146. State = [[-0.19884312 -0.06408161]]. Action = [[ 0.18266404  0.14064068 -0.06094977  0.71255565]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1146 is [True, False, False, False, True, False]
Current timestep = 1147. State = [[-0.19224042 -0.052723  ]]. Action = [[ 0.11134377  0.04984969 -0.12542003  0.53259766]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1147 is [True, False, False, False, True, False]
Scene graph at timestep 1147 is [True, False, False, False, True, False]
State prediction error at timestep 1147 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1147 of 1
Current timestep = 1148. State = [[-0.18441592 -0.03509524]]. Action = [[-0.04562303  0.16050482 -0.23529957 -0.12042743]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1148 is [True, False, False, False, True, False]
Scene graph at timestep 1148 is [True, False, False, False, True, False]
State prediction error at timestep 1148 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1148 of 1
Current timestep = 1149. State = [[-0.18698187 -0.01790247]]. Action = [[-0.03948577  0.06800574  0.01831159  0.05078626]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1149 is [True, False, False, False, True, False]
Current timestep = 1150. State = [[-0.19153862 -0.0251178 ]]. Action = [[-0.09229925 -0.21035613 -0.23260993  0.2715757 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1150 is [True, False, False, False, True, False]
Current timestep = 1151. State = [[-0.20082144 -0.02129778]]. Action = [[-0.1648969   0.24775061  0.05191925  0.6407175 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1151 is [True, False, False, False, True, False]
Scene graph at timestep 1151 is [True, False, False, False, True, False]
State prediction error at timestep 1151 is tensor(5.1086e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of -1
Current timestep = 1152. State = [[-0.22243471 -0.00481098]]. Action = [[-0.19309062 -0.00988273  0.17341202 -0.07139367]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1152 is [True, False, False, False, True, False]
Scene graph at timestep 1152 is [True, False, False, False, True, False]
State prediction error at timestep 1152 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1152 of -1
Current timestep = 1153. State = [[-0.2318551  -0.00186325]]. Action = [[ 0.23944688  0.06525061  0.06093523 -0.4866873 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1153 is [True, False, False, False, True, False]
Current timestep = 1154. State = [[-0.22708996  0.0062087 ]]. Action = [[-0.03101052  0.09317502  0.21292156 -0.1750313 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1154 is [True, False, False, False, True, False]
Current timestep = 1155. State = [[-0.22759368  0.02402704]]. Action = [[ 0.00034606  0.19398117 -0.11651038 -0.2527089 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1155 is [True, False, False, False, True, False]
Scene graph at timestep 1155 is [True, False, False, False, True, False]
State prediction error at timestep 1155 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 1
Current timestep = 1156. State = [[-0.22330761  0.04292234]]. Action = [[ 0.11984104  0.00967818 -0.11693658  0.6102698 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1156 is [True, False, False, False, True, False]
Current timestep = 1157. State = [[-0.20815495  0.05584104]]. Action = [[ 0.14033788  0.1795628   0.24548095 -0.6187019 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1157 is [True, False, False, False, True, False]
Current timestep = 1158. State = [[-0.18833579  0.07156833]]. Action = [[0.16596255 0.04664057 0.21167853 0.7501445 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1158 is [True, False, False, False, True, False]
Current timestep = 1159. State = [[-0.1659395   0.08900002]]. Action = [[0.19579548 0.17652565 0.18325743 0.40268242]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1159 is [True, False, False, False, True, False]
Current timestep = 1160. State = [[-0.14908026  0.10757713]]. Action = [[-0.03013462  0.07628047 -0.06383634  0.34364235]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1160 is [True, False, False, False, True, False]
Scene graph at timestep 1160 is [True, False, False, False, True, False]
State prediction error at timestep 1160 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.14102046  0.12122481]]. Action = [[0.12889889 0.111249   0.21909219 0.5851722 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1161 is [True, False, False, False, True, False]
Current timestep = 1162. State = [[-0.13045505  0.13010617]]. Action = [[ 0.00041142 -0.05180676 -0.22592613  0.15143287]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Scene graph at timestep 1162 is [True, False, False, False, False, True]
State prediction error at timestep 1162 is tensor(5.1219e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1162 of 0
Current timestep = 1163. State = [[-0.12091306  0.1384575 ]]. Action = [[0.21041375 0.15814975 0.10540608 0.8050511 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1163 is [True, False, False, False, False, True]
Scene graph at timestep 1163 is [True, False, False, False, False, True]
State prediction error at timestep 1163 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1163 of 1
Current timestep = 1164. State = [[-0.09727177  0.14806795]]. Action = [[ 0.08081388 -0.09972835  0.17090726 -0.07138646]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1164 is [True, False, False, False, False, True]
Current timestep = 1165. State = [[-0.08166293  0.14234279]]. Action = [[ 0.21230474  0.02391988 -0.22386931 -0.50265986]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1165 is [True, False, False, False, False, True]
Current timestep = 1166. State = [[-0.05804105  0.15013424]]. Action = [[0.08111697 0.10112625 0.00497457 0.12203491]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1166 is [True, False, False, False, False, True]
Current timestep = 1167. State = [[-0.04174324  0.14848015]]. Action = [[ 0.13146901 -0.13213427  0.05973411  0.44490075]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1167 is [True, False, False, False, False, True]
Scene graph at timestep 1167 is [False, True, False, False, False, True]
State prediction error at timestep 1167 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of 1
Current timestep = 1168. State = [[-0.01875681  0.13476226]]. Action = [[ 0.1595332  -0.09877628 -0.1841154   0.18944573]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1168 is [False, True, False, False, False, True]
Scene graph at timestep 1168 is [False, True, False, False, False, True]
State prediction error at timestep 1168 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[-0.18699305 -0.00036877]]. Action = [[ 0.23142487 -0.21856116  0.05588073 -0.59561634]]. Reward = [100.]
Curr episode timestep = 96
Scene graph at timestep 1169 is [False, True, False, False, False, True]
Current timestep = 1170. State = [[-0.17796302 -0.00272326]]. Action = [[-0.07444152 -0.04139525 -0.22982875 -0.49407363]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Current timestep = 1171. State = [[-0.17786326 -0.01741048]]. Action = [[ 0.01831681 -0.18123572 -0.22074878  0.84839153]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Current timestep = 1172. State = [[-0.17161202 -0.01971225]]. Action = [[ 0.19339651  0.16364413  0.08739933 -0.00667381]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Current timestep = 1173. State = [[-0.16266726 -0.00473377]]. Action = [[-0.06356746  0.16043514 -0.1060441  -0.02390498]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Current timestep = 1174. State = [[-0.15761882  0.01863347]]. Action = [[0.15935218 0.18656504 0.24360472 0.24700642]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Current timestep = 1175. State = [[-0.13935119  0.0459316 ]]. Action = [[ 0.16582549  0.22691798  0.04004943 -0.13503408]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Current timestep = 1176. State = [[-0.12439739  0.06341474]]. Action = [[-0.08295391 -0.0375609  -0.04801817 -0.24669385]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Current timestep = 1177. State = [[-0.12557548  0.06182355]]. Action = [[-0.12736323 -0.06898002  0.12271526  0.0078069 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Current timestep = 1178. State = [[-0.13622184  0.05501743]]. Action = [[-0.22398409 -0.07441929 -0.1877346  -0.01266146]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Scene graph at timestep 1178 is [True, False, False, False, True, False]
State prediction error at timestep 1178 is tensor(8.5467e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1178 of 1
Current timestep = 1179. State = [[-0.15313637  0.05681838]]. Action = [[ 0.1758402   0.15968645 -0.17596169 -0.5699228 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Current timestep = 1180. State = [[-0.14825933  0.06893723]]. Action = [[ 0.09691903  0.08360425  0.21871984 -0.8891654 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Current timestep = 1181. State = [[-0.14316031  0.0789272 ]]. Action = [[-0.02575304  0.0475274   0.22310615  0.29256248]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(8.1757e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of -1
Current timestep = 1182. State = [[-0.13390662  0.08017862]]. Action = [[ 0.1784592  -0.09239745 -0.21170366 -0.78308135]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Current timestep = 1183. State = [[-0.11322253  0.06694207]]. Action = [[ 0.1534046  -0.1685448  -0.15429172 -0.6038356 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Current timestep = 1184. State = [[-0.0901418  0.0661977]]. Action = [[ 0.21935791  0.19399887  0.16474822 -0.3666464 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.05798672  0.06079786]]. Action = [[ 0.23473448 -0.2338984   0.23281264 -0.63777286]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Current timestep = 1186. State = [[-0.16840698 -0.0718234 ]]. Action = [[ 0.09747097 -0.06178658 -0.02860491  0.86291504]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.15553184 -0.07786788]]. Action = [[-0.0112168   0.04658553 -0.17399852 -0.80753213]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Current timestep = 1188. State = [[-0.15337548 -0.06465922]]. Action = [[ 0.06968829  0.21357602 -0.22417831  0.7756022 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Current timestep = 1189. State = [[-0.15189135 -0.05436892]]. Action = [[-0.11334445 -0.05660805  0.1835748  -0.9310621 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Scene graph at timestep 1189 is [True, False, False, False, True, False]
State prediction error at timestep 1189 is tensor(3.5638e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1189 of 1
Current timestep = 1190. State = [[-0.15670389 -0.0485177 ]]. Action = [[-0.10336232  0.09261853 -0.13044041 -0.63471854]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Current timestep = 1191. State = [[-0.15899764 -0.04758002]]. Action = [[ 0.1084384  -0.09608775  0.20318824 -0.64062864]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Current timestep = 1192. State = [[-0.1486762  -0.03836615]]. Action = [[0.23129538 0.22091424 0.04451907 0.27530348]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Current timestep = 1193. State = [[-0.13085738 -0.01673128]]. Action = [[0.05542496 0.16378713 0.22975308 0.7804053 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1193 is [True, False, False, False, True, False]
Scene graph at timestep 1193 is [True, False, False, False, True, False]
State prediction error at timestep 1193 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of 1
Current timestep = 1194. State = [[-0.12144201  0.00263509]]. Action = [[-0.00524776  0.05820656 -0.09011829 -0.73033196]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Scene graph at timestep 1194 is [True, False, False, False, True, False]
State prediction error at timestep 1194 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of 1
Current timestep = 1195. State = [[-0.11244512  0.0118706 ]]. Action = [[ 0.21512717  0.07811612  0.0631727  -0.302871  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Current timestep = 1196. State = [[-0.08766741  0.01857194]]. Action = [[ 0.1370582   0.03970489 -0.06762773  0.37942588]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Current timestep = 1197. State = [[-0.06918109  0.01106901]]. Action = [[ 0.04481024 -0.21508244 -0.0796883  -0.5185605 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Scene graph at timestep 1197 is [True, False, False, False, True, False]
State prediction error at timestep 1197 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1197 of 1
Current timestep = 1198. State = [[-0.06749786  0.01121663]]. Action = [[-0.07073313  0.19717422 -0.15320608  0.8729477 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Current timestep = 1199. State = [[-0.06140191  0.02379996]]. Action = [[0.24720445 0.06953686 0.21472275 0.42825937]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Current timestep = 1200. State = [[-0.04371474  0.03979862]]. Action = [[-0.0286888   0.1418587   0.19867676  0.16213584]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1200 is [True, False, False, False, True, False]
Scene graph at timestep 1200 is [False, True, False, False, True, False]
State prediction error at timestep 1200 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of 1
Current timestep = 1201. State = [[-0.0433424   0.05217202]]. Action = [[-0.0875317  -0.01214412  0.23015794 -0.51480985]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1201 is [False, True, False, False, True, False]
Current timestep = 1202. State = [[-0.04281129  0.05556112]]. Action = [[ 0.12734812  0.07164511  0.13559279 -0.7228475 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1202 is [False, True, False, False, True, False]
Current timestep = 1203. State = [[-0.1586483  -0.11128205]]. Action = [[0.15014267 0.07579988 0.18622947 0.15175128]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1203 is [False, True, False, False, True, False]
Current timestep = 1204. State = [[-0.14217722 -0.12066261]]. Action = [[-0.04347175  0.07224375 -0.01642919 -0.5834354 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1204 is [True, False, False, False, True, False]
Current timestep = 1205. State = [[-0.13581766 -0.11985845]]. Action = [[ 0.18597957 -0.02547051  0.08187625 -0.5630738 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1205 is [True, False, False, False, True, False]
Current timestep = 1206. State = [[-0.13048887 -0.12848638]]. Action = [[-0.18499391 -0.1239996   0.19112787  0.5437827 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1206 is [True, False, False, False, True, False]
Scene graph at timestep 1206 is [True, False, False, True, False, False]
State prediction error at timestep 1206 is tensor(9.8517e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of -1
Current timestep = 1207. State = [[-0.14216013 -0.12737724]]. Action = [[-0.16631255  0.22304702 -0.2063326  -0.4726423 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1207 is [True, False, False, True, False, False]
Current timestep = 1208. State = [[-0.14662509 -0.11819217]]. Action = [[ 0.05725181 -0.08759634 -0.06661457  0.64207697]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1208 is [True, False, False, True, False, False]
Scene graph at timestep 1208 is [True, False, False, False, True, False]
State prediction error at timestep 1208 is tensor(3.6888e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of 1
Current timestep = 1209. State = [[-0.14701746 -0.11648725]]. Action = [[-0.00382015  0.07900071  0.16783899  0.5295038 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Current timestep = 1210. State = [[-0.14050351 -0.10065359]]. Action = [[ 0.21387261  0.18390968  0.20103961 -0.23426259]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Scene graph at timestep 1210 is [True, False, False, False, True, False]
State prediction error at timestep 1210 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of 1
Current timestep = 1211. State = [[-0.13461816 -0.09411559]]. Action = [[-0.00200212 -0.18886958 -0.13589251  0.9286542 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Current timestep = 1212. State = [[-0.13331869 -0.09222215]]. Action = [[-0.00718111  0.20437303 -0.17102738 -0.1542477 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Current timestep = 1213. State = [[-0.13011312 -0.07351596]]. Action = [[0.08712658 0.15931249 0.03868708 0.43828237]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Current timestep = 1214. State = [[-0.12822099 -0.06351785]]. Action = [[-0.13921258 -0.02308665 -0.01705196  0.85658073]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Current timestep = 1215. State = [[-0.12437453 -0.05194718]]. Action = [[ 0.21461982  0.18501115 -0.22838381  0.96026015]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.10659283 -0.04793221]]. Action = [[ 0.17900604 -0.17632864  0.19287577  0.6954849 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Current timestep = 1217. State = [[-0.09365378 -0.06770355]]. Action = [[-0.14407174 -0.20778826  0.15508589  0.12102342]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1217 is [True, False, False, False, True, False]
Scene graph at timestep 1217 is [True, False, False, False, True, False]
State prediction error at timestep 1217 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of 1
Current timestep = 1218. State = [[-0.09360487 -0.07898875]]. Action = [[0.15908396 0.12819284 0.16546154 0.8012936 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1218 is [True, False, False, False, True, False]
Current timestep = 1219. State = [[-0.08854117 -0.08279698]]. Action = [[-0.11360958 -0.15191747 -0.08383512  0.90554714]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1219 is [True, False, False, False, True, False]
Scene graph at timestep 1219 is [True, False, False, False, True, False]
State prediction error at timestep 1219 is tensor(2.2303e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of 0
Current timestep = 1220. State = [[-0.08531782 -0.07657703]]. Action = [[0.1562025  0.21254653 0.01986742 0.84379494]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1220 is [True, False, False, False, True, False]
Current timestep = 1221. State = [[-0.07629791 -0.06411009]]. Action = [[ 0.05225971  0.03973353  0.11263239 -0.7682177 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1221 is [True, False, False, False, True, False]
Scene graph at timestep 1221 is [True, False, False, False, True, False]
State prediction error at timestep 1221 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of 1
Current timestep = 1222. State = [[-0.06650169 -0.06321212]]. Action = [[ 0.07219145 -0.10807648  0.01243436 -0.0971874 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1222 is [True, False, False, False, True, False]
Scene graph at timestep 1222 is [True, False, False, False, True, False]
State prediction error at timestep 1222 is tensor(1.6245e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of 1
Current timestep = 1223. State = [[-0.05086677 -0.0669978 ]]. Action = [[ 0.21979976  0.04125625 -0.12768091 -0.3126598 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1223 is [True, False, False, False, True, False]
Scene graph at timestep 1223 is [True, False, False, False, True, False]
State prediction error at timestep 1223 is tensor(6.3956e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[-0.2414777  -0.12517995]]. Action = [[-0.10058206  0.04913303  0.09715909  0.3479638 ]]. Reward = [100.]
Curr episode timestep = 20
Scene graph at timestep 1224 is [True, False, False, False, True, False]
Current timestep = 1225. State = [[-0.23266567 -0.1368109 ]]. Action = [[ 0.11505407  0.07269219 -0.02770558  0.742357  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1225 is [True, False, False, True, False, False]
Current timestep = 1226. State = [[-0.21777883 -0.12286232]]. Action = [[ 0.1272471   0.18642819 -0.15924717 -0.4750285 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1226 is [True, False, False, True, False, False]
Current timestep = 1227. State = [[-0.21063735 -0.10326444]]. Action = [[-0.09499627  0.1335547   0.08272919  0.38293064]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Current timestep = 1228. State = [[-0.21278621 -0.08587369]]. Action = [[0.00292706 0.09570777 0.02596223 0.73828363]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Current timestep = 1229. State = [[-0.21002981 -0.08703643]]. Action = [[ 0.09623152 -0.18781605 -0.12736726 -0.85275555]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1229 is [True, False, False, False, True, False]
Current timestep = 1230. State = [[-0.19852857 -0.09848603]]. Action = [[ 0.11137235 -0.06615469  0.11901763 -0.1022374 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Current timestep = 1231. State = [[-0.1826618  -0.10785118]]. Action = [[ 0.14657253 -0.06816405  0.10646096 -0.3776914 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Scene graph at timestep 1231 is [True, False, False, False, True, False]
State prediction error at timestep 1231 is tensor(9.0622e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of 1
Current timestep = 1232. State = [[-0.16881755 -0.11313788]]. Action = [[0.02139047 0.03642184 0.13364753 0.992394  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.1557727  -0.11643406]]. Action = [[ 0.21227896 -0.08084804  0.16473454  0.02809429]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.14284717 -0.13076171]]. Action = [[-0.17598866 -0.13441971  0.00941956 -0.50462097]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.14765377 -0.152732  ]]. Action = [[ 0.03924933 -0.20157404 -0.12273186 -0.6139858 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1235 is [True, False, False, True, False, False]
Scene graph at timestep 1235 is [True, False, False, True, False, False]
State prediction error at timestep 1235 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of -1
Current timestep = 1236. State = [[-0.14729409 -0.1833685 ]]. Action = [[ 0.05378875 -0.21007188 -0.06408772 -0.5398574 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1236 is [True, False, False, True, False, False]
Current timestep = 1237. State = [[-0.1418129  -0.19397874]]. Action = [[0.02592903 0.09288666 0.20788151 0.6011319 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1237 is [True, False, False, True, False, False]
Current timestep = 1238. State = [[-0.1381629  -0.19913156]]. Action = [[ 0.02365464 -0.13128226  0.18080845  0.49147224]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1238 is [True, False, False, True, False, False]
Current timestep = 1239. State = [[-0.13381773 -0.19287843]]. Action = [[ 0.08109871  0.23032618 -0.10326147  0.8331058 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1239 is [True, False, False, True, False, False]
Scene graph at timestep 1239 is [True, False, False, True, False, False]
State prediction error at timestep 1239 is tensor(9.1441e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 0
Current timestep = 1240. State = [[-0.11624827 -0.19113486]]. Action = [[ 0.23783547 -0.21438746  0.07466871  0.28788567]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1240 is [True, False, False, True, False, False]
Current timestep = 1241. State = [[-0.08729899 -0.18870947]]. Action = [[0.13718832 0.23719883 0.03444466 0.02038491]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1241 is [True, False, False, True, False, False]
Current timestep = 1242. State = [[-0.06614917 -0.18958074]]. Action = [[ 0.14726067 -0.19491914  0.13075203  0.33137763]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1242 is [True, False, False, True, False, False]
Current timestep = 1243. State = [[-0.05335841 -0.19015773]]. Action = [[-0.03573799  0.14465466  0.03665814 -0.4571182 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1243 is [True, False, False, True, False, False]
Current timestep = 1244. State = [[-0.05009732 -0.1807197 ]]. Action = [[ 0.0945794   0.05735523  0.02120319 -0.47140825]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1244 is [True, False, False, True, False, False]
Current timestep = 1245. State = [[-0.03422173 -0.17809977]]. Action = [[ 0.19125235 -0.03550035 -0.10758235 -0.10282582]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1245 is [True, False, False, True, False, False]
Current timestep = 1246. State = [[-0.01261807 -0.18133488]]. Action = [[ 0.08956578 -0.03850906  0.2177915  -0.5038483 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1246 is [False, True, False, True, False, False]
Current timestep = 1247. State = [[-0.00569916 -0.19099514]]. Action = [[-0.13653499 -0.11689749  0.10540846  0.5702218 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1247 is [False, True, False, True, False, False]
Current timestep = 1248. State = [[-0.01110724 -0.19273447]]. Action = [[-0.19472165  0.14486444 -0.05737291  0.72615504]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1248 is [False, True, False, True, False, False]
Current timestep = 1249. State = [[-0.01420255 -0.18234234]]. Action = [[-0.0077403   0.09948707 -0.22358255  0.55163026]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1249 is [False, True, False, True, False, False]
