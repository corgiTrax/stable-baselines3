Current timestep = 0. State = [[-0.22286992  0.00824421]]. Action = [[-0.068102   -0.18123451 -0.22207429  0.4303794 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.21477403  0.00474094]]. Action = [[ 0.23785585  0.1088565  -0.19512375 -0.06585735]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.20144354  0.0035504 ]]. Action = [[-0.02720083 -0.09777875  0.11793792  0.03991866]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.19054608  0.00368287]]. Action = [[ 0.23987103  0.09150901  0.18607908 -0.47691786]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Current timestep = 4. State = [[-0.17256549  0.00378255]]. Action = [[-0.05971523 -0.0777951   0.23937994 -0.39599174]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.1737691  -0.00833902]]. Action = [[-0.11792172 -0.15843748 -0.13533045  0.77168846]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.17769906 -0.00759885]]. Action = [[-0.00968787  0.21500498  0.15943491  0.74961853]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.18555233  0.00659872]]. Action = [[-0.13217701  0.04042852  0.02170107  0.22967792]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.19568911  0.02480079]]. Action = [[-0.05432864  0.20773679  0.0979591   0.6013918 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.21137264  0.03339032]]. Action = [[-0.19508572 -0.15952803 -0.23573805 -0.82946575]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Current timestep = 10. State = [[-0.22608423  0.02593079]]. Action = [[-0.10135086  0.02916756 -0.22614713  0.22444534]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.24374485  0.03779693]]. Action = [[-0.09706455  0.20715237  0.00767446 -0.04390508]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.25706697  0.06386608]]. Action = [[-0.03522183  0.17808193  0.02435628  0.791548  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.25569138  0.09013251]]. Action = [[ 0.2374877   0.21503273 -0.13965721 -0.7415381 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Current timestep = 14. State = [[-0.24592264  0.10407692]]. Action = [[-0.14330015 -0.17541522  0.00971857  0.21188486]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.24971944  0.11964813]]. Action = [[-0.09748507  0.18201226 -0.19788183 -0.6566722 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.25762954  0.13601957]]. Action = [[-0.00552553  0.05779186 -0.21174906 -0.04305321]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Current timestep = 17. State = [[-0.25958383  0.14158496]]. Action = [[-0.231388   -0.0003777  -0.02759881 -0.01367652]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.25460136  0.15347026]]. Action = [[0.13450012 0.18160814 0.08649772 0.3104    ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Scene graph at timestep 18 is [True, False, False, False, False, True]
State prediction error at timestep 18 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.24188519  0.16736373]]. Action = [[-0.0408996  -0.08523744 -0.04977548  0.8664503 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Current timestep = 20. State = [[-0.2456028   0.16935985]]. Action = [[-0.10526156  0.06328022 -0.19594672 -0.5288597 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.2571923   0.17961545]]. Action = [[-0.15486793  0.07241291  0.14825332  0.57149625]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Scene graph at timestep 21 is [True, False, False, False, False, True]
State prediction error at timestep 21 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.26995355  0.1973361 ]]. Action = [[ 0.11073768  0.22495499 -0.10901442 -0.7718727 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.27174133  0.20619439]]. Action = [[-0.20755687  0.19762051  0.19234067  0.89657974]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Current timestep = 24. State = [[-0.27214006  0.20770249]]. Action = [[-0.10829604 -0.20677899 -0.14592099  0.936368  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.26135448  0.20201512]]. Action = [[ 0.2022292  -0.12814194  0.19143572 -0.89889085]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Scene graph at timestep 25 is [True, False, False, False, False, True]
State prediction error at timestep 25 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.2451562   0.18682286]]. Action = [[-0.15846486 -0.20786859  0.02137589 -0.04285091]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Current timestep = 27. State = [[-0.2430672   0.18145859]]. Action = [[ 0.16459343  0.15501866 -0.14098622 -0.8712537 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Current timestep = 28. State = [[-0.24357672  0.17941903]]. Action = [[-0.15221661 -0.14942598  0.09198493  0.43834198]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, False, True]
Current timestep = 29. State = [[-0.24500152  0.17076963]]. Action = [[-0.03288765 -0.0740684  -0.01027229  0.42931533]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, False, True]
Current timestep = 30. State = [[-0.24226378  0.1537948 ]]. Action = [[ 0.0541136  -0.18580616  0.11067617 -0.8873709 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, False, True]
Scene graph at timestep 30 is [True, False, False, False, False, True]
State prediction error at timestep 30 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.2348477   0.14416979]]. Action = [[0.23749357 0.16557682 0.10627031 0.1054498 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, False, True]
Current timestep = 32. State = [[-0.23349707  0.15093775]]. Action = [[-0.22902842 -0.04106973 -0.07646361  0.5484508 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, False, True]
Current timestep = 33. State = [[-0.24397108  0.16199917]]. Action = [[-0.11846824  0.16272846 -0.11904308 -0.8991466 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, False, True]
Current timestep = 34. State = [[-0.25135452  0.16977976]]. Action = [[ 0.05374038 -0.07447386  0.15968663 -0.21853226]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, False, True]
Scene graph at timestep 34 is [True, False, False, False, False, True]
State prediction error at timestep 34 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.2523885   0.16226695]]. Action = [[-0.07519579 -0.11393222 -0.17640491  0.86827135]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, False, True]
Current timestep = 36. State = [[-0.25802645  0.15004346]]. Action = [[-0.0568863  -0.06135055 -0.18911693 -0.8366105 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, False, True]
Current timestep = 37. State = [[-0.26422015  0.1448158 ]]. Action = [[-0.12070301 -0.08951584  0.20176363  0.71526074]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, False, True]
Current timestep = 38. State = [[-0.26028544  0.13290602]]. Action = [[ 0.17403632 -0.14984217 -0.22740775  0.6137222 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, False, True]
Scene graph at timestep 38 is [True, False, False, False, False, True]
State prediction error at timestep 38 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.25186136  0.12052307]]. Action = [[ 0.07430619  0.02427626 -0.08022591 -0.49786794]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, False, True]
Current timestep = 40. State = [[-0.2496899   0.12202538]]. Action = [[-0.18144532  0.01849735  0.07610071  0.8454337 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.24861969  0.12252367]]. Action = [[-0.17104785 -0.16090311  0.16633439  0.36072123]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Current timestep = 42. State = [[-0.24244522  0.12528752]]. Action = [[0.13805294 0.07575205 0.11899367 0.55993366]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Current timestep = 43. State = [[-0.23268554  0.11637425]]. Action = [[-0.08500332 -0.23348726  0.09616598  0.6766434 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, False, True]
Current timestep = 44. State = [[-0.2301697   0.11108691]]. Action = [[0.11052489 0.16014749 0.15785411 0.0248363 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Current timestep = 45. State = [[-0.2288357   0.12761268]]. Action = [[-0.00326489  0.19263011 -0.04898919  0.74414897]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Current timestep = 46. State = [[-0.21710703  0.13379933]]. Action = [[ 0.21748847 -0.08696462 -0.04491922  0.6716639 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, False, True]
Current timestep = 47. State = [[-0.19358352  0.14551456]]. Action = [[ 0.16177565  0.20835039 -0.05251899 -0.03607893]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, False, True]
Scene graph at timestep 47 is [True, False, False, False, False, True]
State prediction error at timestep 47 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.16591161  0.1656219 ]]. Action = [[ 0.18905723  0.08939755 -0.09344721  0.6916058 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, False, True]
Current timestep = 49. State = [[-0.14460261  0.16446082]]. Action = [[ 0.07465911 -0.15969494  0.19262141 -0.41689563]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, False, True]
Current timestep = 50. State = [[-0.13247518  0.1417667 ]]. Action = [[ 0.10903868 -0.22666837  0.07881552  0.7264564 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, False, True]
Current timestep = 51. State = [[-0.12904748  0.13369426]]. Action = [[-0.17282704  0.13866836 -0.19673789 -0.8783917 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, False, True]
Current timestep = 52. State = [[-0.12643354  0.12617977]]. Action = [[ 0.14510441 -0.2062674  -0.04806133  0.6711862 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, False, True]
Current timestep = 53. State = [[-0.12021684  0.11145993]]. Action = [[-0.06104983 -0.0766442  -0.10502174  0.7346842 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, False, True]
Current timestep = 54. State = [[-0.11281559  0.11557537]]. Action = [[ 0.23309427  0.19830137 -0.2328748   0.740139  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.09036878  0.11265279]]. Action = [[ 0.15025365 -0.21917129  0.13634333  0.47394228]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.07771116  0.10381687]]. Action = [[0.00941756 0.07176507 0.03607556 0.5126238 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Current timestep = 57. State = [[-0.07290523  0.09260529]]. Action = [[ 0.04690292 -0.2248826  -0.03811273 -0.10919064]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.0623682  0.0863525]]. Action = [[ 0.15739661  0.15469187  0.02409342 -0.22589976]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.03800479  0.08836648]]. Action = [[ 0.09468392 -0.12682356  0.20207214 -0.82429785]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.15039906  0.20735252]]. Action = [[-0.12717852  0.00656    -0.17956674 -0.15019172]]. Reward = [100.]
Curr episode timestep = 60
Scene graph at timestep 60 is [False, True, False, False, True, False]
Current timestep = 61. State = [[-0.12956189  0.23435539]]. Action = [[-0.09841082 -0.02699928  0.1757974  -0.10287941]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 61 is [True, False, False, False, False, True]
Current timestep = 62. State = [[-0.14011821  0.23472323]]. Action = [[-0.24216972 -0.06270885  0.04949757  0.7959771 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 62 is [True, False, False, False, False, True]
Current timestep = 63. State = [[-0.15686965  0.24528119]]. Action = [[-0.09314919  0.16574919 -0.08012572 -0.7569742 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 63 is [True, False, False, False, False, True]
Current timestep = 64. State = [[-0.17728607  0.26341027]]. Action = [[-0.17299199  0.07692435 -0.10032728  0.5999875 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 64 is [True, False, False, False, False, True]
Current timestep = 65. State = [[-0.20060621  0.28496715]]. Action = [[-0.15463613  0.23013836 -0.19202848  0.781466  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 65 is [True, False, False, False, False, True]
Current timestep = 66. State = [[-0.21102557  0.29532078]]. Action = [[ 0.15185094 -0.0736894   0.15476555 -0.96684855]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 66 is [True, False, False, False, False, True]
Scene graph at timestep 66 is [True, False, False, False, False, True]
State prediction error at timestep 66 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.20788684  0.29039463]]. Action = [[ 0.21561259  0.11730611  0.01470384 -0.87931013]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 67 is [True, False, False, False, False, True]
Current timestep = 68. State = [[-0.19746794  0.27720627]]. Action = [[ 0.22564822 -0.18245178  0.12125221 -0.5800589 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 68 is [True, False, False, False, False, True]
Current timestep = 69. State = [[-0.18760203  0.2581526 ]]. Action = [[-0.12289023 -0.13835773  0.23524743  0.67129385]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 69 is [True, False, False, False, False, True]
Current timestep = 70. State = [[-0.18468155  0.24853866]]. Action = [[ 0.02771157 -0.0145587  -0.02326457  0.6901603 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 70 is [True, False, False, False, False, True]
Current timestep = 71. State = [[-0.19201653  0.2517497 ]]. Action = [[-0.19068253  0.08274645 -0.13442641  0.04228866]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 71 is [True, False, False, False, False, True]
Current timestep = 72. State = [[-0.20982867  0.24907914]]. Action = [[-0.24374391 -0.19235413 -0.20127773 -0.9291143 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 72 is [True, False, False, False, False, True]
Current timestep = 73. State = [[-0.24192826  0.23227093]]. Action = [[-0.20795664 -0.06986462  0.07793134 -0.09338343]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 73 is [True, False, False, False, False, True]
Current timestep = 74. State = [[-0.25680453  0.23290113]]. Action = [[ 0.17039686  0.2056889  -0.23628011 -0.5961228 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 74 is [True, False, False, False, False, True]
Scene graph at timestep 74 is [True, False, False, False, False, True]
State prediction error at timestep 74 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.24932572  0.23572625]]. Action = [[ 0.18863308 -0.06627174  0.05984896 -0.41557115]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 75 is [True, False, False, False, False, True]
Current timestep = 76. State = [[-0.24158461  0.22787714]]. Action = [[-0.09394054 -0.14311144 -0.17564306 -0.04996711]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 76 is [True, False, False, False, False, True]
Current timestep = 77. State = [[-0.2368795  0.2177127]]. Action = [[ 0.10777798 -0.01316682  0.0698626  -0.7062794 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 77 is [True, False, False, False, False, True]
Scene graph at timestep 77 is [True, False, False, False, False, True]
State prediction error at timestep 77 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.23384042  0.2035501 ]]. Action = [[-0.09890747 -0.19648732 -0.12141754  0.38750553]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 78 is [True, False, False, False, False, True]
Current timestep = 79. State = [[-0.24329047  0.19959103]]. Action = [[-0.2355239   0.08642912  0.03648174 -0.318883  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 79 is [True, False, False, False, False, True]
Current timestep = 80. State = [[-0.25587097  0.2105259 ]]. Action = [[0.0601238  0.15584633 0.1413325  0.50642395]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 80 is [True, False, False, False, False, True]
Current timestep = 81. State = [[-0.25012407  0.21634167]]. Action = [[ 0.24124658 -0.01472893 -0.07121029  0.655422  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 81 is [True, False, False, False, False, True]
Current timestep = 82. State = [[-0.2342893  0.2105511]]. Action = [[ 0.13692415 -0.08803514 -0.21372235  0.5713148 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 82 is [True, False, False, False, False, True]
Current timestep = 83. State = [[-0.22309774  0.1901848 ]]. Action = [[-0.06372839 -0.24405524  0.16667807 -0.6478939 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 83 is [True, False, False, False, False, True]
Current timestep = 84. State = [[-0.22790833  0.18678549]]. Action = [[-0.19041604  0.18831107 -0.21478818 -0.3338614 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 84 is [True, False, False, False, False, True]
Current timestep = 85. State = [[-0.23302492  0.18870148]]. Action = [[ 0.04129601 -0.14178643 -0.02917661  0.1535021 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 85 is [True, False, False, False, False, True]
Scene graph at timestep 85 is [True, False, False, False, False, True]
State prediction error at timestep 85 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-0.2231631   0.17058216]]. Action = [[ 0.2356394  -0.1560152  -0.0410641  -0.85631055]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 86 is [True, False, False, False, False, True]
Current timestep = 87. State = [[-0.21805333  0.16607702]]. Action = [[-0.11644131  0.17382139  0.02430433  0.5630307 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 87 is [True, False, False, False, False, True]
Scene graph at timestep 87 is [True, False, False, False, False, True]
State prediction error at timestep 87 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.22341801  0.17698748]]. Action = [[ 0.00655279  0.06465495 -0.22249015 -0.30105233]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 88 is [True, False, False, False, False, True]
Scene graph at timestep 88 is [True, False, False, False, False, True]
State prediction error at timestep 88 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.21537133  0.1821473 ]]. Action = [[ 0.23979753 -0.00162499 -0.01348175  0.9314531 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 89 is [True, False, False, False, False, True]
Scene graph at timestep 89 is [True, False, False, False, False, True]
State prediction error at timestep 89 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.19937995  0.18395327]]. Action = [[ 0.08751714  0.02454159  0.22591901 -0.74156284]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 90 is [True, False, False, False, False, True]
Current timestep = 91. State = [[-0.19847558  0.19819544]]. Action = [[-0.1874027   0.19326925  0.12904775 -0.16711259]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 91 is [True, False, False, False, False, True]
Current timestep = 92. State = [[-0.19639921  0.20134522]]. Action = [[ 0.24243695 -0.12982266  0.063346    0.24812531]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 92 is [True, False, False, False, False, True]
Current timestep = 93. State = [[-0.18237367  0.18990783]]. Action = [[-0.04305273 -0.1428023   0.2236656  -0.49682915]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 93 is [True, False, False, False, False, True]
Current timestep = 94. State = [[-0.17649248  0.18680148]]. Action = [[0.12535608 0.12868786 0.24151623 0.8038714 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 94 is [True, False, False, False, False, True]
Current timestep = 95. State = [[-0.16950922  0.18001069]]. Action = [[-0.09514157 -0.2244222   0.12707865  0.73861325]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 95 is [True, False, False, False, False, True]
Scene graph at timestep 95 is [True, False, False, False, False, True]
State prediction error at timestep 95 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.17385618  0.15745044]]. Action = [[-0.17183872 -0.19824156 -0.09487896  0.4939561 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 96 is [True, False, False, False, False, True]
Current timestep = 97. State = [[-0.18457301  0.15216939]]. Action = [[ 0.04789314  0.19665262  0.09291226 -0.72419524]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 97 is [True, False, False, False, False, True]
Scene graph at timestep 97 is [True, False, False, False, False, True]
State prediction error at timestep 97 is tensor(6.5008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.19194816  0.16904853]]. Action = [[-0.07292685  0.15226674  0.17525512  0.8584064 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 98 is [True, False, False, False, False, True]
Current timestep = 99. State = [[-0.18972056  0.17278828]]. Action = [[ 0.18858433 -0.12162453 -0.07299656  0.8143952 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 99 is [True, False, False, False, False, True]
Current timestep = 100. State = [[-0.18024664  0.1656755 ]]. Action = [[ 0.08105651 -0.00963631  0.03892303 -0.62952346]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 100 is [True, False, False, False, False, True]
Current timestep = 101. State = [[-0.16763347  0.16877843]]. Action = [[0.1336872  0.08804744 0.06641346 0.74181783]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 101 is [True, False, False, False, False, True]
Current timestep = 102. State = [[-0.14769682  0.16604921]]. Action = [[ 0.1604257  -0.14757212 -0.04929058  0.22533917]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 102 is [True, False, False, False, False, True]
Scene graph at timestep 102 is [True, False, False, False, False, True]
State prediction error at timestep 102 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.13419084  0.15401807]]. Action = [[-0.15589856 -0.05116406 -0.07931271 -0.8935576 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 103 is [True, False, False, False, False, True]
Scene graph at timestep 103 is [True, False, False, False, False, True]
State prediction error at timestep 103 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.13296737  0.14156091]]. Action = [[ 0.07125792 -0.18073893 -0.2071235   0.29636705]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 104 is [True, False, False, False, False, True]
Current timestep = 105. State = [[-0.13413689  0.1378987 ]]. Action = [[-0.13885333  0.1490806   0.23092651 -0.14655685]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 105 is [True, False, False, False, False, True]
Scene graph at timestep 105 is [True, False, False, False, False, True]
State prediction error at timestep 105 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.14492793  0.15523325]]. Action = [[-0.11857975  0.17802817 -0.174279    0.65568805]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 106 is [True, False, False, False, False, True]
Current timestep = 107. State = [[-0.1644085   0.17264953]]. Action = [[-0.19534956  0.02534366  0.00666893 -0.34080172]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 107 is [True, False, False, False, False, True]
Current timestep = 108. State = [[-0.17669998  0.18468747]]. Action = [[ 0.13639435  0.17086947 -0.2014407   0.16480637]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 108 is [True, False, False, False, False, True]
Scene graph at timestep 108 is [True, False, False, False, False, True]
State prediction error at timestep 108 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.18346433  0.20043372]]. Action = [[-0.07764857  0.10442811  0.18512803 -0.8856788 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 109 is [True, False, False, False, False, True]
Current timestep = 110. State = [[-0.19406134  0.20056395]]. Action = [[-0.24608676 -0.1977117   0.24071532  0.35914207]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 110 is [True, False, False, False, False, True]
Current timestep = 111. State = [[-0.21642695  0.18562068]]. Action = [[-0.21458398 -0.11975875  0.09119388 -0.8504246 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 111 is [True, False, False, False, False, True]
Current timestep = 112. State = [[-0.23054327  0.16622142]]. Action = [[ 0.03585255 -0.16091049  0.23620564  0.6399108 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 112 is [True, False, False, False, False, True]
Current timestep = 113. State = [[-0.22570807  0.1490993 ]]. Action = [[ 0.18901893 -0.06324401 -0.21438374 -0.34760553]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 113 is [True, False, False, False, False, True]
Current timestep = 114. State = [[-0.21602902  0.13716523]]. Action = [[ 0.06874207 -0.07195622  0.2310189  -0.00985491]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 114 is [True, False, False, False, False, True]
Current timestep = 115. State = [[-0.2099404   0.11726535]]. Action = [[-0.01097497 -0.20396352  0.00915891 -0.6630974 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 115 is [True, False, False, False, False, True]
Current timestep = 116. State = [[-0.19932133  0.08975492]]. Action = [[ 0.15806827 -0.19740379  0.10327265  0.97762704]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 116 is [True, False, False, False, True, False]
Current timestep = 117. State = [[-0.19757125  0.05862094]]. Action = [[-0.20496613 -0.19024426 -0.12574483  0.03333783]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 117 is [True, False, False, False, True, False]
Current timestep = 118. State = [[-0.20659196  0.0368594 ]]. Action = [[-0.05324736 -0.10656081  0.09092408 -0.839833  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 118 is [True, False, False, False, True, False]
Current timestep = 119. State = [[-0.20999672  0.03119956]]. Action = [[0.12322849 0.12313879 0.17088705 0.5612383 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 119 is [True, False, False, False, True, False]
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.21621741  0.03091087]]. Action = [[-0.24340335 -0.07446152  0.07311362  0.28109884]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.2201301   0.02540044]]. Action = [[ 0.18429208 -0.03677763  0.03782421  0.97079945]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 121 is [True, False, False, False, True, False]
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.22244719  0.00800942]]. Action = [[-0.1585538  -0.23604706 -0.09880012 -0.89198744]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 122 is [True, False, False, False, True, False]
Current timestep = 123. State = [[-0.22130832  0.00026482]]. Action = [[0.24541703 0.14080656 0.15933162 0.5421243 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 123 is [True, False, False, False, True, False]
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.22028486 -0.00047686]]. Action = [[-0.18353719 -0.09906387 -0.07494059  0.22058499]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 124 is [True, False, False, False, True, False]
Current timestep = 125. State = [[-0.21883315  0.00184145]]. Action = [[ 0.16096383  0.14235604 -0.13395797 -0.6183615 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 125 is [True, False, False, False, True, False]
Current timestep = 126. State = [[-0.21202512  0.0094224 ]]. Action = [[ 0.11043516  0.03792801  0.20170385 -0.5774936 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 126 is [True, False, False, False, True, False]
Current timestep = 127. State = [[-0.19697107  0.02507411]]. Action = [[ 0.11360866  0.18821499  0.14005423 -0.9979311 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 127 is [True, False, False, False, True, False]
Current timestep = 128. State = [[-0.1915692  0.0393573]]. Action = [[-0.0837386   0.02662528  0.01438653  0.1439004 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.19696563  0.05450548]]. Action = [[-0.12732255  0.174258   -0.1983533   0.92661834]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.20749666  0.07344835]]. Action = [[-0.10544136  0.05693156  0.05098075  0.6857927 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 130 is [True, False, False, False, True, False]
Current timestep = 131. State = [[-0.21174948  0.0764967 ]]. Action = [[ 0.04801172 -0.07880582  0.14845812  0.78715515]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 131 is [True, False, False, False, True, False]
Current timestep = 132. State = [[-0.20474915  0.06666493]]. Action = [[ 0.21546364 -0.10398006 -0.1120782   0.5350158 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.18747483  0.05072785]]. Action = [[ 0.18333602 -0.11453883  0.15524921 -0.26163697]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.16060197  0.03915443]]. Action = [[ 0.22255054 -0.02780032  0.15759295  0.06443095]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 134 is [True, False, False, False, True, False]
Current timestep = 135. State = [[-0.14288221  0.04120406]]. Action = [[-0.04050612  0.10264423 -0.09847283 -0.42486602]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.14684978  0.05155355]]. Action = [[-0.18659092  0.07689938  0.0094007  -0.94635135]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 136 is [True, False, False, False, True, False]
Current timestep = 137. State = [[-0.15149136  0.05276496]]. Action = [[-0.00646289 -0.11599658  0.15769413 -0.9645614 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.15249525  0.04376676]]. Action = [[-0.01208861 -0.07969531  0.18056479  0.5948138 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.15533897  0.04248875]]. Action = [[-0.06147005  0.09255856 -0.08581877 -0.16694164]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 139 is [True, False, False, False, True, False]
Current timestep = 140. State = [[-0.15418616  0.03992034]]. Action = [[ 0.15772524 -0.09073356  0.15302643  0.5636718 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.15360998  0.02539378]]. Action = [[-0.09385091 -0.18160573  0.17721185 -0.88284874]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 141 is [True, False, False, False, True, False]
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.15730926  0.00884828]]. Action = [[-0.0241641  -0.01815206  0.21482134 -0.9058559 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.15318494  0.0132476 ]]. Action = [[ 0.18730545  0.12667975 -0.14283146  0.56748617]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.14101921  0.00880239]]. Action = [[ 0.14877534 -0.1795173  -0.14221786  0.05076182]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.13487273 -0.01233036]]. Action = [[-0.18100436 -0.17555545 -0.03143932 -0.5464252 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 145 is [True, False, False, False, True, False]
Current timestep = 146. State = [[-0.12987155 -0.02740379]]. Action = [[ 0.21495953 -0.03013435  0.09115845 -0.75890684]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.12517896 -0.03815916]]. Action = [[-0.06435628 -0.09630978  0.17509496  0.2564739 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.12416271 -0.04349776]]. Action = [[ 0.03759629  0.05471528 -0.06273398 -0.74780774]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 148 is [True, False, False, False, True, False]
Current timestep = 149. State = [[-0.13057633 -0.05146093]]. Action = [[-0.21570377 -0.15697357  0.04671609 -0.4202708 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.14574268 -0.07075199]]. Action = [[-0.1987758  -0.15185983 -0.20300041 -0.32780188]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 150 is [True, False, False, False, True, False]
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.17501935 -0.09553935]]. Action = [[-0.1702725  -0.13231283 -0.03016531  0.5039041 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.17930967 -0.10805432]]. Action = [[ 0.1756683  -0.05710846 -0.05726835 -0.57921314]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 152 is [True, False, False, False, True, False]
Current timestep = 153. State = [[-0.1856331 -0.125634 ]]. Action = [[-0.22808327 -0.1731961  -0.17117986  0.6785629 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.202975   -0.12881842]]. Action = [[-0.15361701  0.18745115 -0.2000133   0.5825572 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 154 is [True, False, False, True, False, False]
Current timestep = 155. State = [[-0.21120358 -0.10832718]]. Action = [[ 0.05733532  0.2304787   0.03512359 -0.02136099]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 155 is [True, False, False, True, False, False]
Current timestep = 156. State = [[-0.20479012 -0.09984149]]. Action = [[ 0.21750644 -0.19619064  0.15828058 -0.9136788 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.1967732 -0.0962607]]. Action = [[-0.02691808  0.2326056  -0.0770517  -0.5819927 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.20523414 -0.09778538]]. Action = [[-0.20826335 -0.24934074  0.20309299 -0.6083155 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 158 is [True, False, False, False, True, False]
Current timestep = 159. State = [[-0.22225975 -0.10205766]]. Action = [[-0.1636498   0.12909788  0.08205944 -0.02720678]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 159 is [True, False, False, False, True, False]
Current timestep = 160. State = [[-0.2377062  -0.09648832]]. Action = [[-0.12384751  0.03888863 -0.14186764 -0.650939  ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.25651354 -0.10365369]]. Action = [[-0.08378676 -0.16671883 -0.02937776  0.6135843 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 161 is [True, False, False, False, True, False]
Current timestep = 162. State = [[-0.26732352 -0.11141615]]. Action = [[-0.1854693   0.18213242 -0.03256062  0.10099208]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 162 is [True, False, False, False, True, False]
Current timestep = 163. State = [[-0.2661819  -0.11490006]]. Action = [[ 0.12856096 -0.04011331 -0.12561679 -0.31992108]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.2636052  -0.11682721]]. Action = [[-0.20441203 -0.17959401 -0.17739455 -0.91826904]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.2635083  -0.11687634]]. Action = [[-0.21801355 -0.09218161 -0.09192277  0.43863714]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.26186466 -0.11647033]]. Action = [[ 0.05862167  0.02628556 -0.16565877 -0.27654612]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 166 is [True, False, False, False, True, False]
Current timestep = 167. State = [[-0.25766045 -0.11651599]]. Action = [[ 0.05618688 -0.02311994  0.17789271 -0.3130362 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.25473177 -0.11681129]]. Action = [[-0.212095    0.1949532   0.23556036 -0.82307047]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.25133857 -0.11142654]]. Action = [[ 0.02040106  0.12842354 -0.24362716 -0.65425366]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 169 is [True, False, False, False, True, False]
Current timestep = 170. State = [[-0.24871545 -0.10619531]]. Action = [[-0.18404825  0.17709917  0.13957357  0.29722714]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.25036544 -0.09881902]]. Action = [[-0.12735267  0.10174218 -0.20037493  0.7584889 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 171 is [True, False, False, False, True, False]
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of 1
Current timestep = 172. State = [[-0.25333354 -0.09099764]]. Action = [[-0.15712613  0.06377923 -0.03348447  0.06213093]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 172 is [True, False, False, False, True, False]
Current timestep = 173. State = [[-0.254591  -0.0912112]]. Action = [[-0.03400287 -0.00286242 -0.17146735 -0.68099236]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.25611192 -0.08799536]]. Action = [[ 0.04721591  0.05258274 -0.11719789  0.9718504 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 174 is [True, False, False, False, True, False]
Current timestep = 175. State = [[-0.25656074 -0.08423702]]. Action = [[-0.1379696   0.07381624 -0.00995566 -0.6869479 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 175 is [True, False, False, False, True, False]
Current timestep = 176. State = [[-0.2566733  -0.08391981]]. Action = [[-0.20416197 -0.16445178  0.1339921  -0.08664149]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 176 is [True, False, False, False, True, False]
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.2570658  -0.09554213]]. Action = [[-0.00542183 -0.23295027  0.2058526   0.30799603]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.25137934 -0.10746694]]. Action = [[0.16673577 0.00794512 0.0348148  0.7451916 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 178 is [True, False, False, False, True, False]
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.24903694 -0.10204482]]. Action = [[-0.14861159  0.15793425  0.12130111  0.802773  ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.2518896  -0.09510639]]. Action = [[-0.20448518  0.19585681 -0.12506284 -0.816648  ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.25215137 -0.08560339]]. Action = [[0.04918128 0.12892169 0.06592873 0.8729173 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 181 is [True, False, False, False, True, False]
Current timestep = 182. State = [[-0.2510276  -0.07740266]]. Action = [[ 0.05436629 -0.02047901  0.11821824  0.4527284 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.2400211  -0.08100854]]. Action = [[ 0.183779   -0.09084691  0.07223076 -0.22604537]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 183 is [True, False, False, False, True, False]
Current timestep = 184. State = [[-0.2219918  -0.09315038]]. Action = [[ 0.14987376 -0.1729111   0.19935161 -0.5147052 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.20830753 -0.11101786]]. Action = [[-0.03853033 -0.0927112  -0.12077829 -0.01576376]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.20437914 -0.1066493 ]]. Action = [[ 0.02921486  0.239606   -0.12963413  0.7312207 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 186 is [True, False, False, False, True, False]
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.21042484 -0.16853248]]. Action = [[ 0.03944397 -0.21461834  0.10327232  0.41021943]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 187 is [True, False, False, False, True, False]
Current timestep = 188. State = [[-0.20034449 -0.1966673 ]]. Action = [[ 0.08974174 -0.17051189 -0.1995925   0.63321114]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 188 is [True, False, False, True, False, False]
Current timestep = 189. State = [[-0.18121867 -0.21042937]]. Action = [[ 0.22977778 -0.0141167  -0.1783676   0.7277454 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 189 is [True, False, False, True, False, False]
Current timestep = 190. State = [[-0.15350926 -0.21841098]]. Action = [[ 0.21384376 -0.04609475  0.20461667  0.641176  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 190 is [True, False, False, True, False, False]
Current timestep = 191. State = [[-0.12166512 -0.22017193]]. Action = [[ 0.22166416  0.05634174  0.1307481  -0.15309197]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 191 is [True, False, False, True, False, False]
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.09893528 -0.20755823]]. Action = [[-0.1507301   0.206869    0.0203591  -0.74562883]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 192 is [True, False, False, True, False, False]
Current timestep = 193. State = [[-0.10812328 -0.21040875]]. Action = [[-0.18178603 -0.22959481 -0.05757578  0.56009924]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 193 is [True, False, False, True, False, False]
Current timestep = 194. State = [[-0.11912587 -0.23679039]]. Action = [[ 0.03959212 -0.24499762 -0.1512728  -0.62047654]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 194 is [True, False, False, True, False, False]
Current timestep = 195. State = [[-0.12224933 -0.24376601]]. Action = [[-0.05977784  0.20852089 -0.0987021   0.6818104 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 195 is [True, False, False, True, False, False]
Current timestep = 196. State = [[-0.1231924  -0.24476993]]. Action = [[-0.00376491 -0.1414632  -0.0344716   0.39770222]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 196 is [True, False, False, True, False, False]
Current timestep = 197. State = [[-0.11951839 -0.24228433]]. Action = [[ 0.19347501  0.09050646  0.11511824 -0.14213371]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 197 is [True, False, False, True, False, False]
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.10740872 -0.22249125]]. Action = [[ 0.14609897  0.21675342 -0.13424833 -0.2818299 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 198 is [True, False, False, True, False, False]
Current timestep = 199. State = [[-0.10475272 -0.22210507]]. Action = [[-0.18764725 -0.21122558  0.18864316  0.9621742 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 199 is [True, False, False, True, False, False]
Current timestep = 200. State = [[-0.10973597 -0.23086676]]. Action = [[-0.02902406  0.02736166  0.10945019  0.7697344 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 200 is [True, False, False, True, False, False]
Scene graph at timestep 200 is [True, False, False, True, False, False]
State prediction error at timestep 200 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.10533252 -0.24073145]]. Action = [[ 0.24027002 -0.19908357  0.09693605 -0.10667264]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 201 is [True, False, False, True, False, False]
Scene graph at timestep 201 is [True, False, False, True, False, False]
State prediction error at timestep 201 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.09752346 -0.26773745]]. Action = [[-0.20587566 -0.18340296  0.10478407 -0.7710748 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 202 is [True, False, False, True, False, False]
Current timestep = 203. State = [[-0.10425739 -0.28561404]]. Action = [[ 0.07543865 -0.06236061  0.14135998  0.54085207]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 203 is [True, False, False, True, False, False]
Current timestep = 204. State = [[-0.10824846 -0.2983624 ]]. Action = [[-0.08624908 -0.09043235 -0.03884828 -0.01515216]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 204 is [True, False, False, True, False, False]
Scene graph at timestep 204 is [True, False, False, True, False, False]
State prediction error at timestep 204 is tensor(0.0151, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.10347307 -0.2987613 ]]. Action = [[ 0.2366333   0.13970771 -0.1154485  -0.94788074]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 205 is [True, False, False, True, False, False]
Current timestep = 206. State = [[-0.09684663 -0.29021582]]. Action = [[-0.06542543 -0.01162875 -0.08781588 -0.47375768]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 206 is [True, False, False, True, False, False]
Scene graph at timestep 206 is [True, False, False, True, False, False]
State prediction error at timestep 206 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.10058106 -0.29771438]]. Action = [[-0.13643323 -0.10010791 -0.1978823   0.35094213]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 207 is [True, False, False, True, False, False]
Current timestep = 208. State = [[-0.10068201 -0.29111004]]. Action = [[ 0.0487102   0.23358023 -0.00889939  0.8014085 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 208 is [True, False, False, True, False, False]
Current timestep = 209. State = [[-0.09855912 -0.28123105]]. Action = [[ 0.1964359  -0.12627459 -0.02759363 -0.76080877]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 209 is [True, False, False, True, False, False]
Scene graph at timestep 209 is [True, False, False, True, False, False]
State prediction error at timestep 209 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.10867839 -0.2879139 ]]. Action = [[-0.24612315 -0.12983863  0.00190482 -0.71409124]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 210 is [True, False, False, True, False, False]
Current timestep = 211. State = [[-0.129814   -0.29692212]]. Action = [[-0.16431864 -0.01706015 -0.05824327 -0.8637178 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 211 is [True, False, False, True, False, False]
Current timestep = 212. State = [[-0.14393893 -0.2995466 ]]. Action = [[ 0.22947502 -0.1446731  -0.10553353 -0.7255437 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 212 is [True, False, False, True, False, False]
Current timestep = 213. State = [[-0.13829939 -0.29455552]]. Action = [[ 0.23318857  0.05419883 -0.1466793   0.6644037 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 213 is [True, False, False, True, False, False]
Current timestep = 214. State = [[-0.1279237  -0.29256344]]. Action = [[ 0.12286863 -0.07911991 -0.13809416 -0.33341938]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 214 is [True, False, False, True, False, False]
Current timestep = 215. State = [[-0.12414414 -0.28955048]]. Action = [[-0.17034534  0.1408884   0.22828364  0.87956464]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 215 is [True, False, False, True, False, False]
Current timestep = 216. State = [[-0.12305639 -0.278891  ]]. Action = [[ 0.1311174   0.06644568 -0.17130093  0.12915146]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 216 is [True, False, False, True, False, False]
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.11079919 -0.27250728]]. Action = [[ 0.22066614 -0.05333945 -0.04331782  0.41911888]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 217 is [True, False, False, True, False, False]
Current timestep = 218. State = [[-0.08922308 -0.2609026 ]]. Action = [[ 0.13989025  0.20430315 -0.15279399  0.83641195]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 218 is [True, False, False, True, False, False]
Scene graph at timestep 218 is [True, False, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.07847981 -0.24196054]]. Action = [[-0.15265714  0.08718827 -0.09579611  0.7721772 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 219 is [True, False, False, True, False, False]
Current timestep = 220. State = [[-0.08794098 -0.25199017]]. Action = [[-1.4876814e-01 -2.2360425e-01 -1.4276206e-01  7.6055527e-05]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 220 is [True, False, False, True, False, False]
Current timestep = 221. State = [[-0.09593212 -0.25173014]]. Action = [[-0.00297774  0.22104546 -0.05655664 -0.04802775]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 221 is [True, False, False, True, False, False]
Current timestep = 222. State = [[-0.09101472 -0.23394997]]. Action = [[0.19150275 0.08479485 0.01033884 0.3945638 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 222 is [True, False, False, True, False, False]
Current timestep = 223. State = [[-0.08980027 -0.23435345]]. Action = [[-0.05172563 -0.1811482   0.03298736 -0.4446746 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 223 is [True, False, False, True, False, False]
Current timestep = 224. State = [[-0.08467671 -0.25157177]]. Action = [[ 0.17541462 -0.1862173   0.11501065 -0.06990659]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 224 is [True, False, False, True, False, False]
Current timestep = 225. State = [[-0.08123709 -0.26993057]]. Action = [[-0.21643579 -0.03868377 -0.15986496 -0.99147797]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 225 is [True, False, False, True, False, False]
Current timestep = 226. State = [[-0.08215685 -0.26622984]]. Action = [[ 0.1484851   0.18311256 -0.01602197  0.19574833]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 226 is [True, False, False, True, False, False]
Scene graph at timestep 226 is [True, False, False, True, False, False]
State prediction error at timestep 226 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.0789125  -0.26465786]]. Action = [[ 0.01437104 -0.20832424 -0.24538885  0.9781089 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 227 is [True, False, False, True, False, False]
Current timestep = 228. State = [[-0.07645646 -0.2627578 ]]. Action = [[ 0.01242834  0.23675758 -0.06902444 -0.39067113]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 228 is [True, False, False, True, False, False]
Current timestep = 229. State = [[-0.073494   -0.24597225]]. Action = [[-0.04449971  0.113821   -0.23191567  0.7530761 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 229 is [True, False, False, True, False, False]
Current timestep = 230. State = [[-0.07572094 -0.24827188]]. Action = [[-0.08777744 -0.18426852 -0.23748921 -0.66369927]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 230 is [True, False, False, True, False, False]
Current timestep = 231. State = [[-0.0765183  -0.26618305]]. Action = [[ 0.17461064 -0.20454778 -0.07739592 -0.6401094 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 231 is [True, False, False, True, False, False]
Current timestep = 232. State = [[-0.08220192 -0.2945297 ]]. Action = [[-0.21629854 -0.20103578 -0.01689069 -0.4712026 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 232 is [True, False, False, True, False, False]
Current timestep = 233. State = [[-0.09047037 -0.301542  ]]. Action = [[-0.07244578  0.22648299  0.19362566 -0.8921345 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 233 is [True, False, False, True, False, False]
Current timestep = 234. State = [[-0.09157857 -0.29584563]]. Action = [[-0.1908972  -0.13583477 -0.13241792 -0.4390973 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 234 is [True, False, False, True, False, False]
Current timestep = 235. State = [[-0.09158568 -0.29521722]]. Action = [[ 0.05997568 -0.16115567 -0.11639154 -0.72360235]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 235 is [True, False, False, True, False, False]
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.08918597 -0.2954436 ]]. Action = [[ 0.1660229  -0.07115865  0.1900295  -0.9071117 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 236 is [True, False, False, True, False, False]
Current timestep = 237. State = [[-0.08857698 -0.29623094]]. Action = [[-0.07929358 -0.01343317  0.12007189 -0.22710407]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 237 is [True, False, False, True, False, False]
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.08901583 -0.29754657]]. Action = [[ 0.23925847 -0.23787208  0.17447597 -0.7959362 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 238 is [True, False, False, True, False, False]
Scene graph at timestep 238 is [True, False, False, True, False, False]
State prediction error at timestep 238 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.08901583 -0.29754657]]. Action = [[-0.09385261 -0.08426763  0.23366344 -0.29058033]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 239 is [True, False, False, True, False, False]
Scene graph at timestep 239 is [True, False, False, True, False, False]
State prediction error at timestep 239 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of 1
Current timestep = 240. State = [[-0.09129734 -0.29728365]]. Action = [[-0.11039418  0.04266751  0.19023222  0.13391197]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 240 is [True, False, False, True, False, False]
Current timestep = 241. State = [[-0.09002542 -0.29740798]]. Action = [[ 0.1810134  -0.04421823 -0.22345951  0.59710526]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 241 is [True, False, False, True, False, False]
Current timestep = 242. State = [[-0.08903071 -0.2968807 ]]. Action = [[ 0.04586115 -0.16231783  0.0876596   0.624398  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 242 is [True, False, False, True, False, False]
Current timestep = 243. State = [[-0.08880028 -0.29660103]]. Action = [[-0.21337247 -0.22622877 -0.04700722  0.64868975]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 243 is [True, False, False, True, False, False]
Current timestep = 244. State = [[-0.08887987 -0.29646358]]. Action = [[-0.10244164  0.03705165  0.14552644  0.07866824]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 244 is [True, False, False, True, False, False]
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 0
Current timestep = 245. State = [[-0.08885729 -0.2964795 ]]. Action = [[-0.05252188 -0.19017291 -0.23231974  0.95366204]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 245 is [True, False, False, True, False, False]
Current timestep = 246. State = [[-0.08885729 -0.2964795 ]]. Action = [[-0.13137144 -0.07629237  0.02980879 -0.63547903]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 246 is [True, False, False, True, False, False]
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 0
Current timestep = 247. State = [[-0.09378731 -0.29395267]]. Action = [[-0.14644153  0.07594028 -0.04909714  0.05484068]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 247 is [True, False, False, True, False, False]
Current timestep = 248. State = [[-0.11226898 -0.29221433]]. Action = [[-0.2116875  -0.01602301  0.19392496  0.649384  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 248 is [True, False, False, True, False, False]
Current timestep = 249. State = [[-0.13823782 -0.27799645]]. Action = [[-0.17819335  0.23183286 -0.13769837  0.43639374]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 249 is [True, False, False, True, False, False]
Current timestep = 250. State = [[-0.15872982 -0.26559368]]. Action = [[-0.07058147 -0.04223244  0.20997    -0.09263092]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 250 is [True, False, False, True, False, False]
Current timestep = 251. State = [[-0.15541264 -0.2523322 ]]. Action = [[0.23998064 0.20130801 0.21936509 0.9449434 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 251 is [True, False, False, True, False, False]
Current timestep = 252. State = [[-0.15008944 -0.22542885]]. Action = [[-0.11196977  0.18987334  0.09519809 -0.03080863]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 252 is [True, False, False, True, False, False]
Current timestep = 253. State = [[-0.1592426  -0.22412364]]. Action = [[-0.13932289 -0.23143259 -0.1268337  -0.02102762]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 253 is [True, False, False, True, False, False]
Current timestep = 254. State = [[-0.1776541  -0.22884806]]. Action = [[-0.18254623  0.11778772  0.06310904  0.53937614]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 254 is [True, False, False, True, False, False]
Current timestep = 255. State = [[-0.19806674 -0.21164137]]. Action = [[-0.08542591  0.20757645 -0.01209238  0.86572754]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.20320745 -0.20662287]]. Action = [[ 0.16376936 -0.22531044 -0.14541773  0.43496168]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 256 is [True, False, False, True, False, False]
Scene graph at timestep 256 is [True, False, False, True, False, False]
State prediction error at timestep 256 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.19117996 -0.21143901]]. Action = [[ 0.21064216  0.0628109  -0.21164297  0.8478961 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 257 is [True, False, False, True, False, False]
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.1852809 -0.2221373]]. Action = [[-0.15444343 -0.23096676  0.03058717 -0.93744653]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 258 is [True, False, False, True, False, False]
Current timestep = 259. State = [[-0.19143556 -0.23937853]]. Action = [[-0.08670443 -0.01946206 -0.14560604  0.6512066 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 259 is [True, False, False, True, False, False]
Current timestep = 260. State = [[-0.19366278 -0.25592336]]. Action = [[ 0.13831878 -0.22184007  0.127087    0.3481605 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 260 is [True, False, False, True, False, False]
Current timestep = 261. State = [[-0.1870783  -0.27156433]]. Action = [[ 0.11800185 -0.02753323  0.0131309  -0.6105741 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.18465507 -0.27437106]]. Action = [[-0.18924671  0.07234266 -0.18511562  0.5328921 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 262 is [True, False, False, True, False, False]
Current timestep = 263. State = [[-0.18146352 -0.25941035]]. Action = [[0.1720398  0.23705578 0.23233527 0.10436094]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 263 is [True, False, False, True, False, False]
Scene graph at timestep 263 is [True, False, False, True, False, False]
State prediction error at timestep 263 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.17815126 -0.2511343 ]]. Action = [[-0.0579997  -0.22334331 -0.20729479  0.53885055]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 264 is [True, False, False, True, False, False]
Scene graph at timestep 264 is [True, False, False, True, False, False]
State prediction error at timestep 264 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.17195149 -0.2667388 ]]. Action = [[ 0.22540814 -0.02018936  0.03122404  0.5940063 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 265 is [True, False, False, True, False, False]
Current timestep = 266. State = [[-0.16103335 -0.26646277]]. Action = [[-0.09541465  0.08286816 -0.13861366 -0.332412  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 266 is [True, False, False, True, False, False]
Current timestep = 267. State = [[-0.17202842 -0.27479303]]. Action = [[-0.22621742 -0.15694615 -0.24114066 -0.86583453]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 267 is [True, False, False, True, False, False]
Current timestep = 268. State = [[-0.18592815 -0.2743675 ]]. Action = [[-0.12473501  0.19012368  0.20792633  0.65839696]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 268 is [True, False, False, True, False, False]
Current timestep = 269. State = [[-0.20895949 -0.27050573]]. Action = [[-0.19682844 -0.07324505  0.21959668  0.5974114 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 269 is [True, False, False, True, False, False]
Scene graph at timestep 269 is [True, False, False, True, False, False]
State prediction error at timestep 269 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.2297758 -0.2737893]]. Action = [[-0.01970355 -0.24883087 -0.16465913 -0.26450944]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 270 is [True, False, False, True, False, False]
Current timestep = 271. State = [[-0.2289524  -0.27216095]]. Action = [[ 0.09182632  0.01373318  0.05701286 -0.29894483]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 271 is [True, False, False, True, False, False]
Current timestep = 272. State = [[-0.22370763 -0.25748613]]. Action = [[ 0.0119493   0.24789733  0.01150274 -0.7723437 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 272 is [True, False, False, True, False, False]
Scene graph at timestep 272 is [True, False, False, True, False, False]
State prediction error at timestep 272 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.21457626 -0.23169997]]. Action = [[ 0.13392651  0.04804689 -0.20989896  0.33999276]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 273 is [True, False, False, True, False, False]
Current timestep = 274. State = [[-0.20973857 -0.22253758]]. Action = [[ 0.02771389  0.07151386 -0.10049032  0.9032657 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 274 is [True, False, False, True, False, False]
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.19690667 -0.215723  ]]. Action = [[ 0.18408513 -0.05626388  0.19833988 -0.5135288 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 275 is [True, False, False, True, False, False]
Current timestep = 276. State = [[-0.184169   -0.21402167]]. Action = [[ 0.02231666  0.0831368   0.00350082 -0.48141444]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 276 is [True, False, False, True, False, False]
Scene graph at timestep 276 is [True, False, False, True, False, False]
State prediction error at timestep 276 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.17102474 -0.21684642]]. Action = [[ 0.17621458 -0.1982171  -0.21399643  0.7318108 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 277 is [True, False, False, True, False, False]
Current timestep = 278. State = [[-0.16615352 -0.24072085]]. Action = [[-0.24209507 -0.17824663  0.13437068  0.01606429]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 278 is [True, False, False, True, False, False]
Current timestep = 279. State = [[-0.17883204 -0.2556697 ]]. Action = [[-0.08289577  0.03057557 -0.02710673  0.7007508 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 279 is [True, False, False, True, False, False]
Current timestep = 280. State = [[-0.18966767 -0.26784343]]. Action = [[-0.07147156 -0.14535639 -0.05652401  0.5502844 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 280 is [True, False, False, True, False, False]
Current timestep = 281. State = [[-0.19570458 -0.27642554]]. Action = [[-0.2086894  -0.24146122  0.23819286 -0.83636737]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 281 is [True, False, False, True, False, False]
Current timestep = 282. State = [[-0.19210152 -0.28696176]]. Action = [[ 0.19269475 -0.17825931  0.04281992 -0.5731229 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 282 is [True, False, False, True, False, False]
Current timestep = 283. State = [[-0.18108834 -0.2928884 ]]. Action = [[ 0.09885877  0.08945733 -0.11591437  0.9869373 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 283 is [True, False, False, True, False, False]
Current timestep = 284. State = [[-0.16631642 -0.27879488]]. Action = [[ 0.12824994  0.19528216 -0.01223852  0.7986026 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 284 is [True, False, False, True, False, False]
Current timestep = 285. State = [[-0.147176   -0.27001792]]. Action = [[ 0.20280313 -0.10797998  0.12004417 -0.06630754]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 285 is [True, False, False, True, False, False]
Scene graph at timestep 285 is [True, False, False, True, False, False]
State prediction error at timestep 285 is tensor(5.2419e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.12794212 -0.27808195]]. Action = [[-0.1053108  -0.02107322 -0.2134967  -0.27173293]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 286 is [True, False, False, True, False, False]
Scene graph at timestep 286 is [True, False, False, True, False, False]
State prediction error at timestep 286 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.12168494 -0.2834472 ]]. Action = [[ 0.2327354  -0.10010672 -0.21824388 -0.5718854 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 287 is [True, False, False, True, False, False]
Current timestep = 288. State = [[-0.10374787 -0.2804874 ]]. Action = [[0.06629694 0.17648283 0.07030419 0.5708077 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 288 is [True, False, False, True, False, False]
Current timestep = 289. State = [[-0.09894983 -0.2763802 ]]. Action = [[-0.07495132 -0.04819679 -0.21600285  0.93920374]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 289 is [True, False, False, True, False, False]
Current timestep = 290. State = [[-0.09997083 -0.26850432]]. Action = [[-0.09392203  0.18586206 -0.20907108 -0.917778  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 290 is [True, False, False, True, False, False]
Scene graph at timestep 290 is [True, False, False, True, False, False]
State prediction error at timestep 290 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.09813125 -0.24947968]]. Action = [[ 0.10904202  0.12489513 -0.02199459 -0.8499452 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 291 is [True, False, False, True, False, False]
Current timestep = 292. State = [[-0.08930097 -0.22395474]]. Action = [[0.14054751 0.21544391 0.09468818 0.6471133 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 292 is [True, False, False, True, False, False]
Current timestep = 293. State = [[-0.08302454 -0.1943355 ]]. Action = [[ 0.00909838  0.18819383  0.139377   -0.04760003]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 293 is [True, False, False, True, False, False]
Current timestep = 294. State = [[-0.07305246 -0.16580042]]. Action = [[ 0.17014286  0.17423356 -0.17575923 -0.44226545]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 294 is [True, False, False, True, False, False]
Scene graph at timestep 294 is [True, False, False, True, False, False]
State prediction error at timestep 294 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of 1
Current timestep = 295. State = [[-0.0590147  -0.15951976]]. Action = [[-0.05536461 -0.1863062   0.14251512 -0.47199303]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 295 is [True, False, False, True, False, False]
Current timestep = 296. State = [[-0.06016087 -0.15882425]]. Action = [[-0.0372432   0.17401254  0.17380893  0.9355173 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 296 is [True, False, False, True, False, False]
Current timestep = 297. State = [[-0.06061069 -0.14226265]]. Action = [[-0.03011648  0.1807825  -0.06485102  0.37448776]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 297 is [True, False, False, True, False, False]
Current timestep = 298. State = [[-0.06065835 -0.11406377]]. Action = [[ 0.07332292  0.21735632 -0.17495744 -0.93890315]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 298 is [True, False, False, True, False, False]
Current timestep = 299. State = [[-0.05696718 -0.0797701 ]]. Action = [[0.10923314 0.24133283 0.12186214 0.49615383]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 299 is [True, False, False, False, True, False]
Current timestep = 300. State = [[-0.04235956 -0.06822201]]. Action = [[ 0.17589295 -0.15758598  0.1705479   0.81661975]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 300 is [True, False, False, False, True, False]
Current timestep = 301. State = [[-0.2606869  -0.17582092]]. Action = [[ 0.08153313 -0.01048647 -0.16892685  0.09732842]]. Reward = [100.]
Curr episode timestep = 113
Scene graph at timestep 301 is [False, True, False, False, True, False]
Scene graph at timestep 301 is [True, False, False, True, False, False]
State prediction error at timestep 301 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.25401473 -0.20434867]]. Action = [[ 0.12877786 -0.129194    0.06159788 -0.42704904]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 302 is [True, False, False, True, False, False]
Current timestep = 303. State = [[-0.24222544 -0.2127476 ]]. Action = [[ 0.01524782  0.0388734  -0.12524837  0.3956728 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 303 is [True, False, False, True, False, False]
Current timestep = 304. State = [[-0.2405263  -0.20889987]]. Action = [[-0.04170164  0.10060227 -0.0719465  -0.759896  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 304 is [True, False, False, True, False, False]
Scene graph at timestep 304 is [True, False, False, True, False, False]
State prediction error at timestep 304 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.23894042 -0.20497298]]. Action = [[-0.23313858  0.11586568 -0.00395894 -0.9654042 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 305 is [True, False, False, True, False, False]
Current timestep = 306. State = [[-0.23833385 -0.21026433]]. Action = [[ 0.05873138 -0.14226067 -0.20759115 -0.76825774]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 306 is [True, False, False, True, False, False]
Current timestep = 307. State = [[-0.23017694 -0.21582623]]. Action = [[ 0.17620617 -0.02093895  0.23775434  0.5188229 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 307 is [True, False, False, True, False, False]
Current timestep = 308. State = [[-0.21485202 -0.23460679]]. Action = [[ 0.01170811 -0.24393582 -0.2324406   0.45696998]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 308 is [True, False, False, True, False, False]
Current timestep = 309. State = [[-0.21842457 -0.24080446]]. Action = [[-0.18555379  0.21479493  0.11464149 -0.6464197 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 309 is [True, False, False, True, False, False]
Scene graph at timestep 309 is [True, False, False, True, False, False]
State prediction error at timestep 309 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.22057056 -0.24230145]]. Action = [[ 0.12151143 -0.16483632  0.21576974  0.12214899]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 310 is [True, False, False, True, False, False]
Current timestep = 311. State = [[-0.22820345 -0.25642583]]. Action = [[-0.22609718 -0.11896524 -0.10914177  0.38221133]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 311 is [True, False, False, True, False, False]
Current timestep = 312. State = [[-0.22968146 -0.27759045]]. Action = [[ 0.22312245 -0.18648621 -0.13386132  0.86802876]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 312 is [True, False, False, True, False, False]
Current timestep = 313. State = [[-0.22562559 -0.29281983]]. Action = [[-0.12389812 -0.02222618 -0.15569447 -0.07386184]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 313 is [True, False, False, True, False, False]
Current timestep = 314. State = [[-0.22927329 -0.2979907 ]]. Action = [[-0.1446685  -0.21207163 -0.10842243  0.5212493 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 314 is [True, False, False, True, False, False]
Current timestep = 315. State = [[-0.22988807 -0.29899508]]. Action = [[-0.23142049 -0.18890728 -0.0889799   0.23386407]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 315 is [True, False, False, True, False, False]
Current timestep = 316. State = [[-0.22712249 -0.29262266]]. Action = [[0.08114389 0.13874692 0.22632062 0.43007052]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 316 is [True, False, False, True, False, False]
Current timestep = 317. State = [[-0.21550499 -0.28037986]]. Action = [[ 0.19304198  0.07402483  0.05419594 -0.60183126]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 317 is [True, False, False, True, False, False]
Current timestep = 318. State = [[-0.19645144 -0.263413  ]]. Action = [[ 0.11126775  0.15453416  0.16763586 -0.61006624]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 318 is [True, False, False, True, False, False]
Current timestep = 319. State = [[-0.1883035  -0.24109283]]. Action = [[-0.10975692  0.22211707 -0.21030472  0.1285342 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 319 is [True, False, False, True, False, False]
Scene graph at timestep 319 is [True, False, False, True, False, False]
State prediction error at timestep 319 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.18478481 -0.22269532]]. Action = [[ 0.1379891  -0.04443471 -0.2000394   0.58995247]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 320 is [True, False, False, True, False, False]
Scene graph at timestep 320 is [True, False, False, True, False, False]
State prediction error at timestep 320 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.1730512  -0.23056279]]. Action = [[ 0.16970909 -0.16823518 -0.05056703 -0.4792229 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 321 is [True, False, False, True, False, False]
Current timestep = 322. State = [[-0.1608078  -0.23069306]]. Action = [[-0.09423822  0.18603495 -0.16769168 -0.81658804]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 322 is [True, False, False, True, False, False]
Scene graph at timestep 322 is [True, False, False, True, False, False]
State prediction error at timestep 322 is tensor(1.9591e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.16364425 -0.21457115]]. Action = [[-0.10610196  0.16249508  0.06164995 -0.7382568 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 323 is [True, False, False, True, False, False]
Current timestep = 324. State = [[-0.17741708 -0.20863877]]. Action = [[-0.22316818 -0.06321999  0.1950686  -0.19150305]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 324 is [True, False, False, True, False, False]
Scene graph at timestep 324 is [True, False, False, True, False, False]
State prediction error at timestep 324 is tensor(5.7748e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.1908517  -0.20461857]]. Action = [[ 0.21692589  0.0592581   0.07052466 -0.70463187]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 325 is [True, False, False, True, False, False]
Scene graph at timestep 325 is [True, False, False, True, False, False]
State prediction error at timestep 325 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.19224623 -0.20265278]]. Action = [[-0.2403398  -0.04743037  0.03529334  0.28823316]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 326 is [True, False, False, True, False, False]
Current timestep = 327. State = [[-0.19679308 -0.2166094 ]]. Action = [[ 0.14146572 -0.19724114  0.08624154  0.6784332 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 327 is [True, False, False, True, False, False]
Scene graph at timestep 327 is [True, False, False, True, False, False]
State prediction error at timestep 327 is tensor(5.3379e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.19002682 -0.23573461]]. Action = [[ 0.12616944 -0.15297611 -0.04641952  0.768661  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 328 is [True, False, False, True, False, False]
Current timestep = 329. State = [[-0.1888894  -0.26205918]]. Action = [[-0.1764582  -0.2308975   0.1115447  -0.75896466]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 329 is [True, False, False, True, False, False]
Current timestep = 330. State = [[-0.19905587 -0.2884608 ]]. Action = [[-0.08710676 -0.0905388  -0.17933334  0.41084862]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 330 is [True, False, False, True, False, False]
Current timestep = 331. State = [[-0.19933102 -0.2961194 ]]. Action = [[ 0.17011532  0.03735268  0.17474884 -0.3592503 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 331 is [True, False, False, True, False, False]
Current timestep = 332. State = [[-0.19523275 -0.2954808 ]]. Action = [[-0.22506508 -0.23546842  0.02964085 -0.7637995 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 332 is [True, False, False, True, False, False]
Current timestep = 333. State = [[-0.19476692 -0.2950943 ]]. Action = [[-0.2239024  -0.2277384  -0.18642598  0.7145014 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 333 is [True, False, False, True, False, False]
Current timestep = 334. State = [[-0.19475204 -0.2950417 ]]. Action = [[-0.04305565 -0.18964225  0.18304163 -0.47121203]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 334 is [True, False, False, True, False, False]
Current timestep = 335. State = [[-0.1822419 -0.2850623]]. Action = [[ 0.22376618  0.15158868 -0.24582742  0.69226635]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 335 is [True, False, False, True, False, False]
Current timestep = 336. State = [[-0.17108716 -0.27868432]]. Action = [[-0.17344071 -0.01896118 -0.12212132 -0.9614602 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 336 is [True, False, False, True, False, False]
Scene graph at timestep 336 is [True, False, False, True, False, False]
State prediction error at timestep 336 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.17242576 -0.27912864]]. Action = [[-0.16064088 -0.22288238 -0.14684083  0.96081376]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 337 is [True, False, False, True, False, False]
Current timestep = 338. State = [[-0.17242576 -0.27912864]]. Action = [[ 0.19934154 -0.18246226 -0.04449654 -0.25581622]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 338 is [True, False, False, True, False, False]
Current timestep = 339. State = [[-0.17864378 -0.2759368 ]]. Action = [[-0.14726134  0.08718634 -0.20829651 -0.8448135 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 339 is [True, False, False, True, False, False]
Current timestep = 340. State = [[-0.18624459 -0.2724718 ]]. Action = [[-0.12449789 -0.21342328 -0.17336918 -0.9861037 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 340 is [True, False, False, True, False, False]
Current timestep = 341. State = [[-0.18608132 -0.25893804]]. Action = [[ 0.01441407  0.2457577  -0.03536138  0.32375562]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 341 is [True, False, False, True, False, False]
Current timestep = 342. State = [[-0.1858407  -0.22913717]]. Action = [[-0.02643193  0.17314655  0.2054106   0.55179095]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 342 is [True, False, False, True, False, False]
Current timestep = 343. State = [[-0.18503669 -0.22148941]]. Action = [[ 0.1837067  -0.19509009  0.04713747 -0.9195806 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 343 is [True, False, False, True, False, False]
Current timestep = 344. State = [[-0.17013009 -0.21317129]]. Action = [[ 0.19698781  0.22648329 -0.0748259   0.7707921 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 344 is [True, False, False, True, False, False]
Current timestep = 345. State = [[-0.16595376 -0.19265093]]. Action = [[-0.23218541  0.18396086 -0.15629315  0.33500063]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 345 is [True, False, False, True, False, False]
Current timestep = 346. State = [[-0.16532706 -0.18700078]]. Action = [[ 0.1953603  -0.17725468 -0.01357654  0.5818701 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 346 is [True, False, False, True, False, False]
Scene graph at timestep 346 is [True, False, False, True, False, False]
State prediction error at timestep 346 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 346 of 1
Current timestep = 347. State = [[-0.1524563 -0.1925655]]. Action = [[ 0.1485048  -0.02858476  0.17661723  0.30386186]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 347 is [True, False, False, True, False, False]
Current timestep = 348. State = [[-0.13254465 -0.20309782]]. Action = [[ 0.14775333 -0.12019403 -0.02323399  0.43481374]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 348 is [True, False, False, True, False, False]
Current timestep = 349. State = [[-0.11380059 -0.22006993]]. Action = [[ 0.12201357 -0.15068123  0.04875201  0.7120594 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 349 is [True, False, False, True, False, False]
Current timestep = 350. State = [[-0.09808028 -0.23041615]]. Action = [[0.06147194 0.02715749 0.12752813 0.4302057 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 350 is [True, False, False, True, False, False]
Current timestep = 351. State = [[-0.07916207 -0.234625  ]]. Action = [[ 0.23061407 -0.06108773  0.03244665 -0.71512645]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 351 is [True, False, False, True, False, False]
Current timestep = 352. State = [[-0.05695005 -0.24928336]]. Action = [[ 0.08346105 -0.17768797  0.02908844  0.12331188]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 352 is [True, False, False, True, False, False]
Current timestep = 353. State = [[-0.04426382 -0.2588137 ]]. Action = [[0.04655221 0.08562386 0.15945637 0.8033588 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 353 is [True, False, False, True, False, False]
Scene graph at timestep 353 is [False, True, False, True, False, False]
State prediction error at timestep 353 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.04156967 -0.26773465]]. Action = [[-0.16734287 -0.11723819 -0.1964326   0.82745564]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 354 is [False, True, False, True, False, False]
Current timestep = 355. State = [[-0.03881045 -0.27227214]]. Action = [[ 0.23924512  0.01408011 -0.14201242 -0.14357758]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 355 is [False, True, False, True, False, False]
Scene graph at timestep 355 is [False, True, False, True, False, False]
State prediction error at timestep 355 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.0233107  -0.26798412]]. Action = [[-0.06464559  0.11939409  0.01294768 -0.14541864]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 356 is [False, True, False, True, False, False]
Current timestep = 357. State = [[-0.02654037 -0.25152656]]. Action = [[-0.19485676  0.22823325 -0.10528725  0.29485154]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 357 is [False, True, False, True, False, False]
Scene graph at timestep 357 is [False, True, False, True, False, False]
State prediction error at timestep 357 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.03583107 -0.22987741]]. Action = [[ 0.1400482   0.01165941  0.06001115 -0.1586008 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 358 is [False, True, False, True, False, False]
Current timestep = 359. State = [[-0.03076064 -0.23472399]]. Action = [[ 0.10660288 -0.15641874 -0.23901269  0.9254861 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 359 is [False, True, False, True, False, False]
Current timestep = 360. State = [[-0.03073807 -0.24353962]]. Action = [[-0.12556228 -0.03839469  0.19496757 -0.62890375]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 360 is [False, True, False, True, False, False]
Current timestep = 361. State = [[-0.0400915  -0.24935329]]. Action = [[-0.20687167  0.01486185 -0.04639134 -0.32073218]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 361 is [False, True, False, True, False, False]
Scene graph at timestep 361 is [False, True, False, True, False, False]
State prediction error at timestep 361 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.04804518 -0.2416633 ]]. Action = [[0.08933872 0.22961825 0.00793159 0.6818669 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 362 is [False, True, False, True, False, False]
Current timestep = 363. State = [[-0.05541072 -0.23990344]]. Action = [[-0.23413064 -0.24282776 -0.1917582   0.61239314]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 363 is [False, True, False, True, False, False]
Scene graph at timestep 363 is [True, False, False, True, False, False]
State prediction error at timestep 363 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.07071038 -0.2549737 ]]. Action = [[-0.00227863  0.00844276  0.18322724 -0.3770144 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 364 is [True, False, False, True, False, False]
Current timestep = 365. State = [[-0.06520683 -0.2496299 ]]. Action = [[ 0.23292106  0.05021352 -0.13203204 -0.5601368 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 365 is [True, False, False, True, False, False]
Scene graph at timestep 365 is [True, False, False, True, False, False]
State prediction error at timestep 365 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.06163481 -0.25390112]]. Action = [[-0.09481058 -0.16021991 -0.05420633 -0.76179826]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 366 is [True, False, False, True, False, False]
Scene graph at timestep 366 is [True, False, False, True, False, False]
State prediction error at timestep 366 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.06403019 -0.2710199 ]]. Action = [[ 0.02683085 -0.13758256  0.20113984 -0.5090532 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 367 is [True, False, False, True, False, False]
Current timestep = 368. State = [[-0.05936384 -0.29127544]]. Action = [[ 0.18776286 -0.2027079   0.01590014  0.45236003]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 368 is [True, False, False, True, False, False]
Scene graph at timestep 368 is [True, False, False, True, False, False]
State prediction error at timestep 368 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.04722308 -0.30787122]]. Action = [[-0.1113766   0.05944425 -0.04198568 -0.8403655 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 369 is [True, False, False, True, False, False]
Current timestep = 370. State = [[-0.05474019 -0.30796355]]. Action = [[-0.1923002   0.02670407  0.07739884  0.93555963]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 370 is [False, True, False, True, False, False]
Current timestep = 371. State = [[-0.0708949  -0.30320862]]. Action = [[-0.1792845   0.11037189  0.22882307  0.6516986 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 371 is [True, False, False, True, False, False]
Current timestep = 372. State = [[-0.08245146 -0.28992805]]. Action = [[ 0.0936138   0.10288322 -0.22091195 -0.27950484]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 372 is [True, False, False, True, False, False]
Scene graph at timestep 372 is [True, False, False, True, False, False]
State prediction error at timestep 372 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.08025327 -0.27996597]]. Action = [[-0.14527513 -0.23282602 -0.16899754  0.06972384]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 373 is [True, False, False, True, False, False]
Scene graph at timestep 373 is [True, False, False, True, False, False]
State prediction error at timestep 373 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of 1
Current timestep = 374. State = [[-0.08576752 -0.28527102]]. Action = [[-0.13821867 -0.08833921  0.19629377 -0.8240317 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 374 is [True, False, False, True, False, False]
Current timestep = 375. State = [[-0.10211807 -0.27734792]]. Action = [[-0.1901122   0.2357828   0.10743457  0.9516337 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 375 is [True, False, False, True, False, False]
Current timestep = 376. State = [[-0.12081917 -0.25915188]]. Action = [[-0.00109836  0.04400629 -0.01963706  0.54338217]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 376 is [True, False, False, True, False, False]
Current timestep = 377. State = [[-0.12972993 -0.2501183 ]]. Action = [[-0.2134748   0.0923647   0.16462958  0.33110785]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 377 is [True, False, False, True, False, False]
Current timestep = 378. State = [[-0.15630262 -0.25292903]]. Action = [[-0.06962039 -0.22208093  0.07716563  0.9707215 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 378 is [True, False, False, True, False, False]
Current timestep = 379. State = [[-0.16900307 -0.27182037]]. Action = [[-0.01720102 -0.1371069  -0.22317496 -0.5585225 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 379 is [True, False, False, True, False, False]
Scene graph at timestep 379 is [True, False, False, True, False, False]
State prediction error at timestep 379 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.18019539 -0.2787529 ]]. Action = [[-0.19638188  0.14953446 -0.15439285 -0.43345952]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 380 is [True, False, False, True, False, False]
Scene graph at timestep 380 is [True, False, False, True, False, False]
State prediction error at timestep 380 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.19506465 -0.2632978 ]]. Action = [[ 0.02549624  0.16303676 -0.24087656  0.51111937]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 381 is [True, False, False, True, False, False]
Scene graph at timestep 381 is [True, False, False, True, False, False]
State prediction error at timestep 381 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of 1
Current timestep = 382. State = [[-0.19494194 -0.256968  ]]. Action = [[-0.00978278 -0.17756881 -0.22395907 -0.11445737]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 382 is [True, False, False, True, False, False]
Current timestep = 383. State = [[-0.19220892 -0.27127817]]. Action = [[ 0.18504924 -0.14004448  0.02380928 -0.02287757]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 383 is [True, False, False, True, False, False]
Current timestep = 384. State = [[-0.17848927 -0.28352076]]. Action = [[ 0.23558706 -0.09215352 -0.13419737 -0.1419968 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 384 is [True, False, False, True, False, False]
Current timestep = 385. State = [[-0.16231872 -0.2979216 ]]. Action = [[ 0.00395048 -0.11353338  0.16945368 -0.9464054 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 385 is [True, False, False, True, False, False]
Current timestep = 386. State = [[-0.15477367 -0.30677968]]. Action = [[ 0.10064    -0.00954555 -0.03398624  0.9536762 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 386 is [True, False, False, True, False, False]
Current timestep = 387. State = [[-0.14588296 -0.3094338 ]]. Action = [[ 0.23896962 -0.07617742  0.12135199 -0.5826085 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 387 is [True, False, False, True, False, False]
Current timestep = 388. State = [[-0.14526036 -0.3097168 ]]. Action = [[-0.13604943 -0.21026914 -0.18280786 -0.7425474 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 388 is [True, False, False, True, False, False]
Current timestep = 389. State = [[-0.13845803 -0.30855525]]. Action = [[0.13348556 0.02993673 0.09883213 0.07052267]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 389 is [True, False, False, True, False, False]
Scene graph at timestep 389 is [True, False, False, True, False, False]
State prediction error at timestep 389 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.13158867 -0.3001707 ]]. Action = [[-0.23127355  0.20251554 -0.06749153  0.67959905]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 390 is [True, False, False, True, False, False]
Current timestep = 391. State = [[-0.13024789 -0.27692208]]. Action = [[ 0.22506589  0.21159908  0.13562185 -0.6213732 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 391 is [True, False, False, True, False, False]
Current timestep = 392. State = [[-0.11857802 -0.26532844]]. Action = [[ 0.14133626 -0.17702132  0.05806616 -0.42665893]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 392 is [True, False, False, True, False, False]
Current timestep = 393. State = [[-0.09844578 -0.26665577]]. Action = [[0.19081032 0.04750779 0.00069305 0.10605919]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 393 is [True, False, False, True, False, False]
Current timestep = 394. State = [[-0.07611807 -0.2673659 ]]. Action = [[ 0.14514324 -0.04585314 -0.23532228  0.25401282]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 394 is [True, False, False, True, False, False]
Current timestep = 395. State = [[-0.0621912  -0.25800905]]. Action = [[-0.08517003  0.24395105 -0.23814227  0.20520091]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 395 is [True, False, False, True, False, False]
Current timestep = 396. State = [[-0.05560512 -0.24845952]]. Action = [[ 0.17745918 -0.07770315 -0.09243447 -0.22233379]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 396 is [True, False, False, True, False, False]
Current timestep = 397. State = [[-0.0396778  -0.25307047]]. Action = [[ 0.14006644 -0.0969187  -0.24949832 -0.8623433 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 397 is [True, False, False, True, False, False]
Scene graph at timestep 397 is [False, True, False, True, False, False]
State prediction error at timestep 397 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of 1
Current timestep = 398. State = [[-0.01482977 -0.2506402 ]]. Action = [[0.18647724 0.15637583 0.11606213 0.9063387 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 398 is [False, True, False, True, False, False]
Current timestep = 399. State = [[-0.0023346  -0.23677292]]. Action = [[-0.20434028  0.15883982 -0.19290286  0.6495671 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 399 is [False, True, False, True, False, False]
Scene graph at timestep 399 is [False, True, False, True, False, False]
State prediction error at timestep 399 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of 1
Current timestep = 400. State = [[-0.00897307 -0.23163149]]. Action = [[-0.12007749 -0.09368093 -0.2234041   0.03066254]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 400 is [False, True, False, True, False, False]
Scene graph at timestep 400 is [False, True, False, True, False, False]
State prediction error at timestep 400 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.01599672 -0.2372617 ]]. Action = [[ 0.06829971  0.00730443 -0.01747686 -0.27500242]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 401 is [False, True, False, True, False, False]
Current timestep = 402. State = [[-0.0165308  -0.24265924]]. Action = [[-0.01351905 -0.11359154 -0.01734181  0.6919284 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 402 is [False, True, False, True, False, False]
Current timestep = 403. State = [[-0.01209607 -0.25550786]]. Action = [[ 0.19835997 -0.17067046  0.01004824  0.9052594 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 403 is [False, True, False, True, False, False]
Scene graph at timestep 403 is [False, True, False, True, False, False]
State prediction error at timestep 403 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.00650766 -0.27637205]]. Action = [[ 0.05977505 -0.17245083 -0.04721874  0.15915954]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 404 is [False, True, False, True, False, False]
Current timestep = 405. State = [[-1.2076142e-04 -2.8367463e-01]]. Action = [[-0.12488686  0.1545749  -0.2393313   0.3950429 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 405 is [False, True, False, True, False, False]
Current timestep = 406. State = [[ 0.00405746 -0.28164652]]. Action = [[ 0.17064935 -0.04888913  0.13577408 -0.45882148]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 406 is [False, True, False, True, False, False]
Scene graph at timestep 406 is [False, True, False, True, False, False]
State prediction error at timestep 406 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[ 0.00869038 -0.28899693]]. Action = [[ 0.01827848 -0.14619516  0.05609271 -0.61032593]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 407 is [False, True, False, True, False, False]
Current timestep = 408. State = [[ 0.01020778 -0.2921345 ]]. Action = [[-0.12805176  0.13408339 -0.00843894 -0.31512392]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 408 is [False, True, False, True, False, False]
Current timestep = 409. State = [[ 0.0102622 -0.2791765]]. Action = [[-0.07105395  0.22092843 -0.03574318 -0.13319367]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 409 is [False, True, False, True, False, False]
Scene graph at timestep 409 is [False, True, False, True, False, False]
State prediction error at timestep 409 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of 1
Current timestep = 410. State = [[ 0.00981978 -0.26770777]]. Action = [[ 0.02581343 -0.141728   -0.11891471  0.20082855]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 410 is [False, True, False, True, False, False]
Current timestep = 411. State = [[ 0.00338065 -0.26391378]]. Action = [[-0.17800342  0.19063932 -0.05943173  0.4689616 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 411 is [False, True, False, True, False, False]
Current timestep = 412. State = [[-0.00935053 -0.24383701]]. Action = [[-0.08613512  0.195463    0.03911439  0.7559376 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 412 is [False, True, False, True, False, False]
Current timestep = 413. State = [[-0.01490804 -0.23070799]]. Action = [[ 0.2065646  -0.09956127  0.20077282 -0.47362047]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 413 is [False, True, False, True, False, False]
Current timestep = 414. State = [[-0.01215231 -0.2212841 ]]. Action = [[-0.02514721  0.18320638  0.2322135   0.6533822 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 414 is [False, True, False, True, False, False]
Scene graph at timestep 414 is [False, True, False, True, False, False]
State prediction error at timestep 414 is tensor(7.3396e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.01380772 -0.2197293 ]]. Action = [[-0.09986082 -0.18386878 -0.0742244   0.69598615]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 415 is [False, True, False, True, False, False]
Current timestep = 416. State = [[-0.01592973 -0.22710223]]. Action = [[0.0311653  0.04280064 0.23254523 0.9513737 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 416 is [False, True, False, True, False, False]
Current timestep = 417. State = [[-0.011697   -0.23686455]]. Action = [[ 0.20035014 -0.21714486 -0.00211911  0.26216817]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 417 is [False, True, False, True, False, False]
Scene graph at timestep 417 is [False, True, False, True, False, False]
State prediction error at timestep 417 is tensor(1.5479e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[ 0.00528429 -0.2507478 ]]. Action = [[ 0.20033771 -0.02076797  0.11377674  0.9090644 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 418 is [False, True, False, True, False, False]
Current timestep = 419. State = [[ 0.02644156 -0.2579915 ]]. Action = [[ 0.1262109  -0.08483213  0.21182156 -0.02803528]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 419 is [False, True, False, True, False, False]
Current timestep = 420. State = [[ 0.03912397 -0.26725313]]. Action = [[-0.05222267 -0.02052763 -0.03801584 -0.06680924]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 420 is [False, True, False, True, False, False]
Current timestep = 421. State = [[ 0.03846586 -0.26096043]]. Action = [[-0.18024935  0.21568087  0.09005624  0.63278055]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 421 is [False, True, False, True, False, False]
Current timestep = 422. State = [[ 0.03014566 -0.24179173]]. Action = [[-0.22754946  0.20878917  0.15065235 -0.24472213]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 422 is [False, True, False, True, False, False]
Current timestep = 423. State = [[ 0.00884739 -0.22298554]]. Action = [[ 0.24348935 -0.18615124  0.13841808 -0.19888794]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 423 is [False, True, False, True, False, False]
Current timestep = 424. State = [[-0.00429632 -0.21965311]]. Action = [[-0.23642738  0.03905791 -0.139759   -0.5735201 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 424 is [False, True, False, True, False, False]
Scene graph at timestep 424 is [False, True, False, True, False, False]
State prediction error at timestep 424 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.02660459 -0.2197567 ]]. Action = [[ 0.07035512 -0.12284195  0.05828705 -0.94640815]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 425 is [False, True, False, True, False, False]
Scene graph at timestep 425 is [False, True, False, True, False, False]
State prediction error at timestep 425 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.02216598 -0.23198396]]. Action = [[ 0.17454356 -0.15306762 -0.23088168 -0.3270101 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 426 is [False, True, False, True, False, False]
Scene graph at timestep 426 is [False, True, False, True, False, False]
State prediction error at timestep 426 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.01447881 -0.25465715]]. Action = [[ 0.14079738 -0.22151823  0.02934378 -0.4715762 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 427 is [False, True, False, True, False, False]
Current timestep = 428. State = [[-0.26619494  0.0316033 ]]. Action = [[ 0.1379354   0.05189478 -0.23018378  0.64080715]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 428 is [False, True, False, True, False, False]
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0745, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.25851527  0.04817296]]. Action = [[ 0.17998302  0.21993124 -0.21647443 -0.29909146]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.24762651  0.06270802]]. Action = [[-0.2203889  -0.08054768 -0.05859934 -0.2873521 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 430 is [True, False, False, False, True, False]
Current timestep = 431. State = [[-0.24721672  0.06449097]]. Action = [[ 0.01803711  0.01832047 -0.14782733 -0.3382886 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 431 is [True, False, False, False, True, False]
Current timestep = 432. State = [[-0.24625914  0.06555746]]. Action = [[-0.18966952  0.17029119 -0.04274584  0.9400873 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.24327928  0.06627977]]. Action = [[ 0.02966601  0.00551835  0.06486666 -0.11025435]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 433 is [True, False, False, False, True, False]
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.23073883  0.06861002]]. Action = [[ 0.19721994  0.02317965 -0.2424295  -0.9552507 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 434 is [True, False, False, False, True, False]
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(5.8332e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.21131878  0.08453344]]. Action = [[-0.02069354  0.20945156  0.19184655  0.8516185 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 435 is [True, False, False, False, True, False]
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.21980219  0.10555571]]. Action = [[-0.2036177   0.00113052 -0.08523908  0.16201532]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 436 is [True, False, False, False, True, False]
Current timestep = 437. State = [[-0.23016632  0.12123489]]. Action = [[-0.0472665   0.19769162  0.11515349 -0.02620214]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 437 is [True, False, False, False, True, False]
Current timestep = 438. State = [[-0.23497857  0.12599495]]. Action = [[ 0.07906443 -0.14951348  0.10420525 -0.9743977 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 438 is [True, False, False, False, True, False]
Current timestep = 439. State = [[-0.23938082  0.13146232]]. Action = [[-0.10066748  0.24027205 -0.01847817 -0.0252251 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 439 is [True, False, False, False, False, True]
Current timestep = 440. State = [[-0.2479177   0.14566116]]. Action = [[-0.24253556  0.04535699  0.17252642 -0.47740674]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 440 is [True, False, False, False, False, True]
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.24179393  0.15280262]]. Action = [[ 0.16223615  0.11105224 -0.21551885 -0.91972435]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 441 is [True, False, False, False, False, True]
Current timestep = 442. State = [[-0.23221494  0.15617429]]. Action = [[-0.06851777 -0.11943048  0.01152819  0.9155073 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 442 is [True, False, False, False, False, True]
Current timestep = 443. State = [[-0.23274928  0.15360294]]. Action = [[-0.07803664 -0.00389354 -0.13213229 -0.8417647 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 443 is [True, False, False, False, False, True]
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.22720887  0.15170835]]. Action = [[ 2.3195037e-01 -1.6969442e-04  6.4769953e-02  7.0407438e-01]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 444 is [True, False, False, False, False, True]
Current timestep = 445. State = [[-0.22609477  0.15544133]]. Action = [[-0.19599341  0.05606177 -0.11094117  0.8989868 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 445 is [True, False, False, False, False, True]
Current timestep = 446. State = [[-0.2328214   0.16451047]]. Action = [[0.04008281 0.11362284 0.12998837 0.27715695]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 446 is [True, False, False, False, False, True]
Current timestep = 447. State = [[-0.23275019  0.17021501]]. Action = [[ 0.08430523  0.03371584 -0.21864697 -0.84741265]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 447 is [True, False, False, False, False, True]
Current timestep = 448. State = [[-0.22474253  0.17076032]]. Action = [[ 0.04527992 -0.07819927  0.1667031  -0.55101573]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.21905829  0.15860678]]. Action = [[ 0.02622896 -0.16698544 -0.18872674 -0.65475774]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.22180943  0.15707812]]. Action = [[-0.14033484  0.15644461 -0.21744722  0.05782449]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 450 is [True, False, False, False, False, True]
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of -1
Current timestep = 451. State = [[-0.22971058  0.16829836]]. Action = [[-0.06318435  0.03284666  0.19379759 -0.07987028]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.2431494   0.18016274]]. Action = [[-0.2131703   0.09944344 -0.09078023 -0.3671986 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 452 is [True, False, False, False, False, True]
Current timestep = 453. State = [[-0.2610582   0.19869491]]. Action = [[ 0.00907215  0.178695   -0.21067008 -0.78580254]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 453 is [True, False, False, False, False, True]
Current timestep = 454. State = [[-0.26809645  0.20887887]]. Action = [[-0.1633189  -0.06736778 -0.12973759  0.8590256 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 454 is [True, False, False, False, False, True]
Current timestep = 455. State = [[-0.26343206  0.21563414]]. Action = [[ 0.14656276  0.11736369  0.07264656 -0.40316552]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 455 is [True, False, False, False, False, True]
Current timestep = 456. State = [[-0.24940552  0.21518819]]. Action = [[ 0.1065889  -0.15081003  0.00789514  0.13483334]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 456 is [True, False, False, False, False, True]
Current timestep = 457. State = [[-0.24340917  0.20950419]]. Action = [[-0.1861929  -0.23041078 -0.20142004 -0.44302714]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.23682672  0.21107504]]. Action = [[ 0.1109879   0.05047649 -0.21136159 -0.8105626 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 458 is [True, False, False, False, False, True]
Scene graph at timestep 458 is [True, False, False, False, False, True]
State prediction error at timestep 458 is tensor(8.0826e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.22731772  0.21398993]]. Action = [[-0.10668458 -0.04883865 -0.03309217 -0.92724806]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 459 is [True, False, False, False, False, True]
Scene graph at timestep 459 is [True, False, False, False, False, True]
State prediction error at timestep 459 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.23778199  0.22124976]]. Action = [[-0.19951266  0.1330257  -0.02390701 -0.2949865 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 460 is [True, False, False, False, False, True]
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.24667387  0.23792773]]. Action = [[ 0.19035912  0.15766773 -0.12968084 -0.3975259 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.23436491  0.25828892]]. Action = [[0.12300956 0.23795289 0.00458258 0.60685706]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.2342105  0.2889124]]. Action = [[-0.16901042  0.1934256   0.03926429 -0.37955523]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 463 is [True, False, False, False, False, True]
Current timestep = 464. State = [[-0.24338132  0.30695516]]. Action = [[-0.14179975  0.18491277  0.01234734 -0.22848374]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 464 is [True, False, False, False, False, True]
Scene graph at timestep 464 is [True, False, False, False, False, True]
State prediction error at timestep 464 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.244078   0.3100306]]. Action = [[-0.11887509  0.12532187  0.20882148 -0.28378022]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 465 is [True, False, False, False, False, True]
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.244078   0.3100306]]. Action = [[-0.21997038  0.14092359 -0.10788384 -0.43913108]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 466 is [True, False, False, False, False, True]
Current timestep = 467. State = [[-0.244078   0.3100306]]. Action = [[ 0.02281553 -0.02440748 -0.0572519  -0.8275207 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 467 is [True, False, False, False, False, True]
Current timestep = 468. State = [[-0.24452393  0.31011027]]. Action = [[-0.0567825 -0.0367059 -0.17022    0.631636 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 468 is [True, False, False, False, False, True]
Current timestep = 469. State = [[-0.24493252  0.3101061 ]]. Action = [[-0.22918911  0.06449968  0.15869763  0.32558155]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 469 is [True, False, False, False, False, True]
Current timestep = 470. State = [[-0.24495533  0.3100935 ]]. Action = [[-0.14306791  0.01690584 -0.09651315  0.05461669]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 470 is [True, False, False, False, False, True]
Scene graph at timestep 470 is [True, False, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.24495533  0.3100935 ]]. Action = [[0.24372622 0.04413497 0.15539181 0.5890615 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 471 is [True, False, False, False, False, True]
Current timestep = 472. State = [[-0.24495533  0.3100935 ]]. Action = [[-0.19707355 -0.07323876 -0.2156351   0.8060527 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 472 is [True, False, False, False, False, True]
Current timestep = 473. State = [[-0.24495533  0.3100935 ]]. Action = [[-0.23444767  0.00904948  0.06859416 -0.3847947 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 473 is [True, False, False, False, False, True]
Current timestep = 474. State = [[-0.24495533  0.3100935 ]]. Action = [[0.24776638 0.12912828 0.1934135  0.96906304]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 474 is [True, False, False, False, False, True]
Current timestep = 475. State = [[-0.24958865  0.30418724]]. Action = [[-0.16663362 -0.15868886  0.07710195 -0.0098415 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 475 is [True, False, False, False, False, True]
Current timestep = 476. State = [[-0.24698706  0.283062  ]]. Action = [[ 0.22759598 -0.19044183  0.02396351  0.5034478 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 476 is [True, False, False, False, False, True]
Current timestep = 477. State = [[-0.23937523  0.26691455]]. Action = [[0.2200703  0.19910347 0.17813057 0.1668024 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 477 is [True, False, False, False, False, True]
Current timestep = 478. State = [[-0.23703934  0.26181445]]. Action = [[-0.01491472 -0.06880924 -0.17371708  0.9352796 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 478 is [True, False, False, False, False, True]
Current timestep = 479. State = [[-0.24435729  0.2674538 ]]. Action = [[-0.18590662  0.16623968 -0.02082561  0.05440438]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 479 is [True, False, False, False, False, True]
Current timestep = 480. State = [[-0.2531559   0.27999818]]. Action = [[ 0.11749813  0.10005566 -0.08139572 -0.56268674]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 480 is [True, False, False, False, False, True]
Current timestep = 481. State = [[-0.2543361   0.28316358]]. Action = [[-0.24595572  0.09660974 -0.19291173 -0.8525075 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 481 is [True, False, False, False, False, True]
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(1.8318e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.25443915  0.28326258]]. Action = [[-0.07029623 -0.08314571  0.18938732 -0.5170191 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 482 is [True, False, False, False, False, True]
Scene graph at timestep 482 is [True, False, False, False, False, True]
State prediction error at timestep 482 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.2540435   0.28244346]]. Action = [[-0.19358851  0.04746532 -0.22900644 -0.8694909 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 483 is [True, False, False, False, False, True]
Current timestep = 484. State = [[-0.2540435   0.28244346]]. Action = [[0.18566829 0.16223198 0.1295703  0.52081704]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 484 is [True, False, False, False, False, True]
Current timestep = 485. State = [[-0.2540435   0.28244346]]. Action = [[-0.01453769  0.2030707  -0.11373958  0.5894394 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 485 is [True, False, False, False, False, True]
Current timestep = 486. State = [[-0.25867268  0.28745306]]. Action = [[-0.0714062   0.09731844 -0.2166656  -0.4886822 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 486 is [True, False, False, False, False, True]
Current timestep = 487. State = [[-0.26246753  0.2807587 ]]. Action = [[-0.03734572 -0.23877953 -0.19908203 -0.96241975]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 487 is [True, False, False, False, False, True]
Current timestep = 488. State = [[-0.26059708  0.27032825]]. Action = [[-0.23010439 -0.13013504  0.16703224 -0.76115334]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 488 is [True, False, False, False, False, True]
Current timestep = 489. State = [[-0.26473582  0.2734488 ]]. Action = [[-0.01500995  0.14069238 -0.11505252  0.30095267]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 489 is [True, False, False, False, False, True]
Current timestep = 490. State = [[-0.26920727  0.27877954]]. Action = [[-0.23922384  0.12015283  0.20056039  0.7238116 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 490 is [True, False, False, False, False, True]
Scene graph at timestep 490 is [True, False, False, False, False, True]
State prediction error at timestep 490 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.26993582  0.2794571 ]]. Action = [[-0.14410923 -0.16522996 -0.24125805 -0.7311514 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 491 is [True, False, False, False, False, True]
Current timestep = 492. State = [[-0.2649016   0.27415845]]. Action = [[ 0.13399765 -0.09234448  0.07461593  0.761796  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 492 is [True, False, False, False, False, True]
Current timestep = 493. State = [[-0.26064986  0.26966017]]. Action = [[-0.21490067 -0.01758313  0.18610084 -0.636463  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 493 is [True, False, False, False, False, True]
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.25962302  0.26825634]]. Action = [[-0.14092478 -0.22118905  0.18507141  0.50242853]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 494 is [True, False, False, False, False, True]
Current timestep = 495. State = [[-0.25409764  0.2585787 ]]. Action = [[ 0.04005033 -0.17385794 -0.24205555  0.9651649 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 495 is [True, False, False, False, False, True]
Scene graph at timestep 495 is [True, False, False, False, False, True]
State prediction error at timestep 495 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.24860345  0.24696381]]. Action = [[-0.00789569  0.0312264   0.13543671  0.2899475 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 496 is [True, False, False, False, False, True]
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(8.5285e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.25081998  0.24972288]]. Action = [[-0.03862955  0.04993823 -0.07387388  0.9929042 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 497 is [True, False, False, False, False, True]
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(8.5119e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[-0.25680655  0.25407758]]. Action = [[-0.11016111 -0.00132495 -0.20845044 -0.90471834]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 498 is [True, False, False, False, False, True]
Scene graph at timestep 498 is [True, False, False, False, False, True]
State prediction error at timestep 498 is tensor(8.2798e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.2659325  0.2663921]]. Action = [[0.07040367 0.24392465 0.13223839 0.922626  ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 499 is [True, False, False, False, False, True]
Current timestep = 500. State = [[-0.26653162  0.27817234]]. Action = [[-0.00163031 -0.00828949  0.13232589 -0.6578182 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 500 is [True, False, False, False, False, True]
Current timestep = 501. State = [[-0.26700395  0.27968258]]. Action = [[-0.00168149 -0.0596332  -0.16855153  0.27828002]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 501 is [True, False, False, False, False, True]
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.2668379   0.27939042]]. Action = [[-0.0103806  -0.0146209  -0.09053798 -0.9350632 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 502 is [True, False, False, False, False, True]
Scene graph at timestep 502 is [True, False, False, False, False, True]
State prediction error at timestep 502 is tensor(1.8965e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.26500025  0.280292  ]]. Action = [[ 0.08580363  0.0585359  -0.19361454  0.17816162]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 503 is [True, False, False, False, False, True]
Current timestep = 504. State = [[-0.26399124  0.28068373]]. Action = [[-0.08044371 -0.13377231 -0.08683538 -0.9752632 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 504 is [True, False, False, False, False, True]
Current timestep = 505. State = [[-0.25758982  0.27136666]]. Action = [[ 0.05273888 -0.18119493 -0.22779585  0.507869  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 505 is [True, False, False, False, False, True]
Current timestep = 506. State = [[-0.2543302   0.26836225]]. Action = [[0.0101316  0.14552581 0.05140239 0.4110067 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 506 is [True, False, False, False, False, True]
Scene graph at timestep 506 is [True, False, False, False, False, True]
State prediction error at timestep 506 is tensor(2.1427e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.25430435  0.27213213]]. Action = [[-0.17580819 -0.13824199 -0.11304757  0.8076868 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 507 is [True, False, False, False, False, True]
Current timestep = 508. State = [[-0.2413857  0.25805  ]]. Action = [[ 0.24492964 -0.2403043  -0.19755676 -0.7381827 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 508 is [True, False, False, False, False, True]
Scene graph at timestep 508 is [True, False, False, False, False, True]
State prediction error at timestep 508 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.21581951  0.23422076]]. Action = [[ 1.5637279e-04 -1.7112008e-01  1.8140388e-01 -3.7668669e-01]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 509 is [True, False, False, False, False, True]
Current timestep = 510. State = [[-0.21194275  0.2125156 ]]. Action = [[-0.0822961  -0.22558136 -0.14504771  0.47050858]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 510 is [True, False, False, False, False, True]
Current timestep = 511. State = [[-0.21557133  0.20385207]]. Action = [[-0.01426554  0.16507071 -0.10628572  0.8254869 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 511 is [True, False, False, False, False, True]
Current timestep = 512. State = [[-0.20998904  0.1952947 ]]. Action = [[ 0.17372477 -0.21784699 -0.18350288  0.8305321 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 512 is [True, False, False, False, False, True]
Current timestep = 513. State = [[-0.1982643   0.16921103]]. Action = [[-0.02845331 -0.23949198 -0.19505143  0.40722454]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 513 is [True, False, False, False, False, True]
Current timestep = 514. State = [[-0.18511492  0.14849375]]. Action = [[ 0.23712927 -0.01517148 -0.23479716 -0.74194634]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 514 is [True, False, False, False, False, True]
Current timestep = 515. State = [[-0.16654852  0.14995612]]. Action = [[ 0.22523189  0.18328226 -0.1495163   0.4059521 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 515 is [True, False, False, False, False, True]
Current timestep = 516. State = [[-0.14289728  0.15877476]]. Action = [[ 0.08945408 -0.03448752  0.14702383  0.90448236]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 516 is [True, False, False, False, False, True]
Scene graph at timestep 516 is [True, False, False, False, False, True]
State prediction error at timestep 516 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of 1
Current timestep = 517. State = [[-0.1317271   0.15993112]]. Action = [[ 0.00563437 -0.01988733 -0.02483103 -0.8924497 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 517 is [True, False, False, False, False, True]
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.12655632  0.14864539]]. Action = [[ 0.06992978 -0.14803876  0.10181534 -0.27941155]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 518 is [True, False, False, False, False, True]
Current timestep = 519. State = [[-0.11543161  0.14402878]]. Action = [[ 0.16143912  0.13971713 -0.00306152  0.3390565 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 519 is [True, False, False, False, False, True]
Current timestep = 520. State = [[-0.10949893  0.16026644]]. Action = [[-0.23459919  0.10244706  0.1001333  -0.6051022 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 520 is [True, False, False, False, False, True]
Current timestep = 521. State = [[-0.11412499  0.16857624]]. Action = [[ 0.05534205  0.02331463 -0.09325367 -0.29164457]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 521 is [True, False, False, False, False, True]
Current timestep = 522. State = [[-0.10994723  0.1609147 ]]. Action = [[ 0.12700641 -0.16605149  0.15678221 -0.5124487 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 522 is [True, False, False, False, False, True]
Scene graph at timestep 522 is [True, False, False, False, False, True]
State prediction error at timestep 522 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 522 of 1
Current timestep = 523. State = [[-0.10464346  0.15075836]]. Action = [[-0.04401201 -0.02026527  0.04494447 -0.05248213]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 523 is [True, False, False, False, False, True]
Current timestep = 524. State = [[-0.11040962  0.15369363]]. Action = [[-0.21105829  0.03574821  0.02848095 -0.9037936 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 524 is [True, False, False, False, False, True]
Scene graph at timestep 524 is [True, False, False, False, False, True]
State prediction error at timestep 524 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.11466363  0.14891025]]. Action = [[-0.02670035 -0.13735604 -0.03684483 -0.7166192 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 525 is [True, False, False, False, False, True]
Current timestep = 526. State = [[-0.11832187  0.14385115]]. Action = [[ 0.01668048  0.0791415  -0.22584634 -0.8394597 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 526 is [True, False, False, False, False, True]
Current timestep = 527. State = [[-0.12650356  0.13646431]]. Action = [[-0.18603002 -0.18243618 -0.13548195 -0.44691217]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 527 is [True, False, False, False, False, True]
Current timestep = 528. State = [[-0.13421588  0.10942689]]. Action = [[ 0.18658137 -0.23710799  0.04353192 -0.46425706]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 528 is [True, False, False, False, False, True]
Current timestep = 529. State = [[-0.12075299  0.07737745]]. Action = [[ 0.16993105 -0.19439885 -0.04656105  0.30361664]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 529 is [True, False, False, False, True, False]
Current timestep = 530. State = [[-0.11166272  0.05521012]]. Action = [[ 0.013212   -0.03027073  0.14361691  0.45123863]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 530 is [True, False, False, False, True, False]
Current timestep = 531. State = [[-0.11345695  0.05989472]]. Action = [[-0.14830163  0.16866812 -0.23705237  0.06055236]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 531 is [True, False, False, False, True, False]
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 531 of 1
Current timestep = 532. State = [[-0.12132458  0.07803593]]. Action = [[-0.09696078  0.0838002  -0.03013857 -0.5286476 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 532 is [True, False, False, False, True, False]
Current timestep = 533. State = [[-0.12166025  0.09242093]]. Action = [[ 0.1975267   0.1563375  -0.08106896  0.90820515]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 533 is [True, False, False, False, True, False]
Current timestep = 534. State = [[-0.1217588   0.11522867]]. Action = [[-0.10286048  0.2188462  -0.11278409 -0.08144957]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.13119419  0.12224855]]. Action = [[-0.17748171 -0.21067597  0.17946616  0.79731345]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.13457349  0.11409336]]. Action = [[ 0.14725578 -0.00532793 -0.19968499 -0.04686558]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 536 is [True, False, False, False, True, False]
Current timestep = 537. State = [[-0.13987939  0.125283  ]]. Action = [[-0.21337564  0.23045391 -0.21735886 -0.31199253]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 537 is [True, False, False, False, True, False]
Scene graph at timestep 537 is [True, False, False, False, False, True]
State prediction error at timestep 537 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of -1
Current timestep = 538. State = [[-0.1532643  0.1352102]]. Action = [[-0.08618635 -0.15628448  0.0645135  -0.7440756 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 538 is [True, False, False, False, False, True]
Current timestep = 539. State = [[-0.15799797  0.12912855]]. Action = [[ 0.01243699  0.06064579  0.22248688 -0.7412057 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 539 is [True, False, False, False, False, True]
Current timestep = 540. State = [[-0.16826694  0.12040189]]. Action = [[-0.24094379 -0.18184803 -0.09387511  0.2225666 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 540 is [True, False, False, False, False, True]
Current timestep = 541. State = [[-0.19565043  0.09458829]]. Action = [[-0.18334286 -0.21713082  0.19922036  0.32932627]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 541 is [True, False, False, False, True, False]
Current timestep = 542. State = [[-0.22359875  0.0687274 ]]. Action = [[-0.17018569 -0.13975778 -0.15572105 -0.1375134 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 542 is [True, False, False, False, True, False]
Current timestep = 543. State = [[-0.24798636  0.05002657]]. Action = [[-0.09789768 -0.07107879 -0.09473816  0.1267538 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 543 is [True, False, False, False, True, False]
Current timestep = 544. State = [[-0.2567641   0.04481439]]. Action = [[ 0.03189778  0.01971266 -0.06558987 -0.0239315 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 544 is [True, False, False, False, True, False]
Current timestep = 545. State = [[-0.25839648  0.04398609]]. Action = [[-0.21197876 -0.12515177  0.12652463  0.421888  ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.2549242   0.02877226]]. Action = [[ 0.07861832 -0.23725264 -0.15020798  0.54074955]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.25162235  0.01286445]]. Action = [[-0.17280006 -0.02211325 -0.04479255  0.93500125]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 547 is [True, False, False, False, True, False]
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.25091004  0.00992754]]. Action = [[-0.23200153 -0.20016561 -0.22388561 -0.16613293]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.25091004  0.00992754]]. Action = [[-0.22114599  0.15331382 -0.00646257  0.92996454]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 549 is [True, False, False, False, True, False]
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(2.6456e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.25670028 -0.00636441]]. Action = [[-0.12456086 -0.2299795  -0.04405792 -0.55684024]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.2633204  -0.01372329]]. Action = [[ 0.11545458  0.18049666  0.04013491 -0.9651004 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.25424242 -0.00086768]]. Action = [[0.15757823 0.12043968 0.15572584 0.38304663]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 552 is [True, False, False, False, True, False]
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of -1
Current timestep = 553. State = [[-0.23997222  0.00077408]]. Action = [[ 0.06102106 -0.20179288  0.22041893 -0.3020664 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 553 is [True, False, False, False, True, False]
Current timestep = 554. State = [[-2.3707084e-01 -8.2024017e-05]]. Action = [[-0.0426988   0.23262125 -0.23578505  0.7902036 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 554 is [True, False, False, False, True, False]
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(8.0779e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 554 of -1
Current timestep = 555. State = [[-0.22108084 -0.0249154 ]]. Action = [[ 0.23882806  0.02511355  0.11397058 -0.21347177]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 555 is [True, False, False, False, True, False]
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 1
Current timestep = 556. State = [[-0.21272632 -0.04276419]]. Action = [[ 0.02218407 -0.22604784 -0.11706406 -0.8475417 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.20576443 -0.05263824]]. Action = [[ 0.09978479  0.11353499 -0.14517522 -0.87300116]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.1971752  -0.05869757]]. Action = [[ 0.03551087 -0.1416148   0.03891081 -0.7927557 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 558 is [True, False, False, False, True, False]
Current timestep = 559. State = [[-0.19270967 -0.0520643 ]]. Action = [[ 0.00538298  0.22744352 -0.13163003  0.07730591]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 559 is [True, False, False, False, True, False]
Current timestep = 560. State = [[-0.1907317  -0.04307784]]. Action = [[ 0.01868352 -0.01187699 -0.2123619  -0.19987506]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.19171931 -0.0456384 ]]. Action = [[-0.13533595 -0.09941402 -0.02890685  0.9084798 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.19336867 -0.03996939]]. Action = [[ 0.08745089  0.18010151  0.17629969 -0.2631241 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 562 is [True, False, False, False, True, False]
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 562 of -1
Current timestep = 563. State = [[-0.19550234 -0.0396914 ]]. Action = [[-0.12246659 -0.1831925  -0.24393481  0.01851904]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 563 is [True, False, False, False, True, False]
Current timestep = 564. State = [[-0.19763955 -0.05448149]]. Action = [[ 0.04448751 -0.08563748 -0.12136059  0.7387483 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.19634622 -0.05083163]]. Action = [[ 0.08389756  0.1861245  -0.20074716  0.8508071 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-0.18494742 -0.04758923]]. Action = [[ 0.17573121 -0.07687795 -0.06988175 -0.06190562]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 566 is [True, False, False, False, True, False]
Current timestep = 567. State = [[-0.16898961 -0.05572416]]. Action = [[ 0.03545204 -0.1153048  -0.09480806 -0.92906994]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.16989143 -0.05008738]]. Action = [[-0.24077171  0.23803091 -0.17599282 -0.07577264]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.17153478 -0.03971543]]. Action = [[ 0.16942221 -0.05870235  0.13257682  0.59754014]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.16438381 -0.05488434]]. Action = [[ 0.11328587 -0.24905357  0.08834386 -0.7910506 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 570 is [True, False, False, False, True, False]
Current timestep = 571. State = [[-0.15895507 -0.08250624]]. Action = [[-0.07417661 -0.16809805 -0.1368285  -0.872242  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 571 is [True, False, False, False, True, False]
Current timestep = 572. State = [[-0.1608734  -0.10077724]]. Action = [[-0.07470353 -0.06717266 -0.20322761 -0.8967909 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 572 is [True, False, False, False, True, False]
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.15711096 -0.10531068]]. Action = [[ 0.2481153   0.08592224  0.00745916 -0.49256778]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 573 is [True, False, False, False, True, False]
Current timestep = 574. State = [[-0.14845549 -0.10066711]]. Action = [[-0.04170717  0.01694223  0.00911722 -0.5440497 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 574 is [True, False, False, False, True, False]
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(3.8786e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[-0.14712441 -0.09131937]]. Action = [[ 0.02188838  0.1171875  -0.12569195 -0.46584982]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 575 is [True, False, False, False, True, False]
Current timestep = 576. State = [[-0.13697053 -0.08246368]]. Action = [[ 0.19583899  0.01866573  0.10134777 -0.6639707 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 576 is [True, False, False, False, True, False]
Current timestep = 577. State = [[-0.11142472 -0.08255196]]. Action = [[ 0.18230447 -0.03870785  0.16023374  0.92806244]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.09327447 -0.09680275]]. Action = [[-0.01009534 -0.21552478 -0.14032614 -0.00214374]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.08267478 -0.10730382]]. Action = [[0.2029028  0.02156112 0.19621566 0.72932935]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.06189419 -0.10537287]]. Action = [[ 0.07057208  0.08230913 -0.09942463  0.5531156 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 580 is [True, False, False, False, True, False]
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.04741902 -0.11442598]]. Action = [[ 0.07571834 -0.23378305 -0.22457504  0.21669674]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.04240393 -0.14223719]]. Action = [[-0.07599396 -0.22457884 -0.05014095  0.39554238]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 582 is [False, True, False, False, True, False]
Current timestep = 583. State = [[-0.04047464 -0.1577652 ]]. Action = [[ 0.13609987  0.03423294 -0.07827362 -0.24465418]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 583 is [False, True, False, True, False, False]
Current timestep = 584. State = [[-0.02723852 -0.1709397 ]]. Action = [[ 0.11533827 -0.19949089  0.2343427   0.797184  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 584 is [False, True, False, True, False, False]
Current timestep = 585. State = [[-0.01245381 -0.18160379]]. Action = [[ 0.0527094   0.06697041 -0.16818103 -0.19957858]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 585 is [False, True, False, True, False, False]
Current timestep = 586. State = [[-0.0080138  -0.17243463]]. Action = [[-0.13253097  0.16708711 -0.02206153  0.23209703]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 586 is [False, True, False, True, False, False]
Current timestep = 587. State = [[-0.00791942 -0.16203813]]. Action = [[ 0.09960935  0.01058871 -0.05665138 -0.44502354]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 587 is [False, True, False, True, False, False]
Current timestep = 588. State = [[-0.0077639  -0.15098283]]. Action = [[-0.08913931  0.1337533  -0.02409366 -0.19955367]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 588 is [False, True, False, True, False, False]
Current timestep = 589. State = [[-0.00603938 -0.15100463]]. Action = [[ 0.14212099 -0.18029307 -0.19419749 -0.09039527]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 589 is [False, True, False, True, False, False]
Scene graph at timestep 589 is [False, True, False, True, False, False]
State prediction error at timestep 589 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.00219213 -0.1670608 ]]. Action = [[-0.1630705  -0.08882767  0.05933744  0.14699781]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 590 is [False, True, False, True, False, False]
Current timestep = 591. State = [[-0.00956377 -0.1846352 ]]. Action = [[-0.11028785 -0.15110426 -0.16480921 -0.46170342]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 591 is [False, True, False, True, False, False]
Current timestep = 592. State = [[-0.01604647 -0.18746735]]. Action = [[-0.0649519   0.16430259 -0.01445808  0.3763144 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 592 is [False, True, False, True, False, False]
Current timestep = 593. State = [[-0.02824505 -0.19187677]]. Action = [[-0.14501226 -0.17053865 -0.11524385  0.24027336]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 593 is [False, True, False, True, False, False]
Scene graph at timestep 593 is [False, True, False, True, False, False]
State prediction error at timestep 593 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.0409812  -0.20717406]]. Action = [[ 0.07973179 -0.09538743  0.2400594  -0.14701247]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 594 is [False, True, False, True, False, False]
Current timestep = 595. State = [[-0.04236808 -0.20953807]]. Action = [[-0.1250301   0.06735349  0.22099444  0.8090775 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 595 is [False, True, False, True, False, False]
Scene graph at timestep 595 is [False, True, False, True, False, False]
State prediction error at timestep 595 is tensor(5.0870e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.04606301 -0.1979374 ]]. Action = [[ 0.15720451  0.14499432 -0.18856534  0.57679343]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 596 is [False, True, False, True, False, False]
Current timestep = 597. State = [[-0.03971513 -0.19916356]]. Action = [[ 0.15293348 -0.21893153 -0.17668103 -0.94595134]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 597 is [False, True, False, True, False, False]
Current timestep = 598. State = [[-0.0312071 -0.2026522]]. Action = [[ 0.00115082  0.13675559  0.1577096  -0.21227199]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 598 is [False, True, False, True, False, False]
Current timestep = 599. State = [[-0.03030499 -0.20696467]]. Action = [[ 0.00936171 -0.16427639 -0.2266178   0.6624119 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 599 is [False, True, False, True, False, False]
Current timestep = 600. State = [[-0.01948136 -0.20140143]]. Action = [[ 0.21874845  0.21891573  0.09849182 -0.9899921 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 600 is [False, True, False, True, False, False]
Current timestep = 601. State = [[-0.00765747 -0.1823335 ]]. Action = [[-0.21556637  0.16566205 -0.20272186 -0.5005361 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 601 is [False, True, False, True, False, False]
Current timestep = 602. State = [[-0.00644965 -0.17207827]]. Action = [[ 0.16841698 -0.05906481 -0.02564836  0.88496983]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 602 is [False, True, False, True, False, False]
Current timestep = 603. State = [[-0.00693426 -0.17544767]]. Action = [[-1.3355443e-01 -5.2823544e-02  1.8838048e-04  2.7459431e-01]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 603 is [False, True, False, True, False, False]
Scene graph at timestep 603 is [False, True, False, True, False, False]
State prediction error at timestep 603 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.0059042  -0.17756921]]. Action = [[ 0.17557672  0.00171235  0.2142216  -0.02860302]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 604 is [False, True, False, True, False, False]
Scene graph at timestep 604 is [False, True, False, True, False, False]
State prediction error at timestep 604 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of 1
Current timestep = 605. State = [[ 0.00420716 -0.18300685]]. Action = [[ 0.10374072 -0.10191332  0.09778044 -0.7674807 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 605 is [False, True, False, True, False, False]
Current timestep = 606. State = [[ 0.01019858 -0.17925283]]. Action = [[-0.18272087  0.18839574 -0.12240438  0.4027791 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 606 is [False, True, False, True, False, False]
Current timestep = 607. State = [[ 0.01564023 -0.15886591]]. Action = [[ 0.23147309  0.19830596 -0.23239659 -0.74025023]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 607 is [False, True, False, True, False, False]
Current timestep = 608. State = [[ 0.0203677  -0.15297282]]. Action = [[-0.0324035  -0.18457243 -0.01121937  0.04506922]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 608 is [False, True, False, True, False, False]
Current timestep = 609. State = [[ 0.0193022 -0.1657925]]. Action = [[-0.1056478  -0.08474773  0.07202384 -0.9441686 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 609 is [False, True, False, True, False, False]
Current timestep = 610. State = [[ 0.01161717 -0.18514366]]. Action = [[-0.19512133 -0.14982593 -0.00786561  0.1757592 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 610 is [False, True, False, True, False, False]
Current timestep = 611. State = [[-0.00618859 -0.1981616 ]]. Action = [[-0.20076287  0.00618568  0.05244526  0.9166062 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 611 is [False, True, False, True, False, False]
Current timestep = 612. State = [[-0.02846918 -0.21295549]]. Action = [[-0.12353253 -0.1882793  -0.11741334  0.35821056]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 612 is [False, True, False, True, False, False]
Current timestep = 613. State = [[-0.03788757 -0.23625062]]. Action = [[ 0.12735766 -0.20369984 -0.20266226  0.0499047 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 613 is [False, True, False, True, False, False]
Current timestep = 614. State = [[-0.03888372 -0.25106615]]. Action = [[-0.01042806  0.01582935  0.06417847  0.4340489 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 614 is [False, True, False, True, False, False]
Current timestep = 615. State = [[-0.03392284 -0.25307006]]. Action = [[ 0.17945063 -0.04122007 -0.20919047 -0.886554  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 615 is [False, True, False, True, False, False]
Current timestep = 616. State = [[-0.01757967 -0.24221602]]. Action = [[ 0.21450031  0.21346638 -0.24020821  0.9844942 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 616 is [False, True, False, True, False, False]
Current timestep = 617. State = [[ 0.00837346 -0.22604893]]. Action = [[ 0.21334302  0.08006561  0.1769146  -0.8943243 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 617 is [False, True, False, True, False, False]
Current timestep = 618. State = [[ 0.02389214 -0.2323759 ]]. Action = [[-0.09455344 -0.2213343  -0.21850254  0.5066595 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 618 is [False, True, False, True, False, False]
Current timestep = 619. State = [[ 0.02525599 -0.24532862]]. Action = [[ 0.08304819 -0.05113639 -0.11439329 -0.24113238]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 619 is [False, True, False, True, False, False]
Current timestep = 620. State = [[ 0.02897209 -0.2529997 ]]. Action = [[-0.04360035 -0.03657693 -0.03046384  0.21964037]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 620 is [False, True, False, True, False, False]
Current timestep = 621. State = [[ 0.0293281 -0.2481628]]. Action = [[-0.0148766   0.17604661  0.2095018  -0.68864596]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 621 is [False, True, False, True, False, False]
Scene graph at timestep 621 is [False, True, False, True, False, False]
State prediction error at timestep 621 is tensor(5.1030e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[ 0.03481684 -0.23993422]]. Action = [[ 0.17035878 -0.05172969  0.20054892 -0.29798394]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 622 is [False, True, False, True, False, False]
Current timestep = 623. State = [[ 0.04485755 -0.24987637]]. Action = [[-0.04148123 -0.1357857   0.23498031  0.3911065 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 623 is [False, True, False, True, False, False]
Current timestep = 624. State = [[ 0.04437233 -0.2526964 ]]. Action = [[-0.05535212  0.10828543 -0.2299516   0.16756165]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 624 is [False, True, False, True, False, False]
Current timestep = 625. State = [[ 0.03622178 -0.26650336]]. Action = [[-0.22064056 -0.2308301   0.08061782  0.06648564]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 625 is [False, True, False, True, False, False]
Current timestep = 626. State = [[ 0.02807752 -0.27534077]]. Action = [[-0.08388257  0.11618003  0.15833884 -0.53996265]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 626 is [False, True, False, True, False, False]
Current timestep = 627. State = [[ 0.0234945 -0.2737748]]. Action = [[ 0.02241737 -0.19446954  0.19163102 -0.49655408]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 627 is [False, True, False, True, False, False]
Current timestep = 628. State = [[ 0.01455108 -0.2747892 ]]. Action = [[-0.1897586   0.00772902 -0.20365016  0.01164317]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 628 is [False, True, False, True, False, False]
Scene graph at timestep 628 is [False, True, False, True, False, False]
State prediction error at timestep 628 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[-0.01170128 -0.2728594 ]]. Action = [[-0.17020111  0.04177466 -0.18887125  0.17154598]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 629 is [False, True, False, True, False, False]
Current timestep = 630. State = [[-0.03363521 -0.25627398]]. Action = [[-0.19559804  0.22079057 -0.20576829  0.89840245]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 630 is [False, True, False, True, False, False]
Current timestep = 631. State = [[-0.05696158 -0.25142112]]. Action = [[-0.00660622 -0.24230982 -0.14643015 -0.42661798]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 631 is [False, True, False, True, False, False]
Current timestep = 632. State = [[-0.06359302 -0.25495115]]. Action = [[-0.03709197  0.14434034  0.13121814  0.02137065]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 632 is [True, False, False, True, False, False]
Scene graph at timestep 632 is [True, False, False, True, False, False]
State prediction error at timestep 632 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.07070293 -0.24168727]]. Action = [[-0.17773224  0.15190887  0.01820192 -0.04272121]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 633 is [True, False, False, True, False, False]
Current timestep = 634. State = [[-0.08529095 -0.23370068]]. Action = [[ 0.15207121 -0.12040636  0.05914587  0.65097404]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 634 is [True, False, False, True, False, False]
Current timestep = 635. State = [[-0.09349532 -0.25211552]]. Action = [[-0.17661569 -0.24062806  0.14394009 -0.01277041]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 635 is [True, False, False, True, False, False]
Scene graph at timestep 635 is [True, False, False, True, False, False]
State prediction error at timestep 635 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.09973395 -0.26644275]]. Action = [[0.18643165 0.09065944 0.23701295 0.57935905]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 636 is [True, False, False, True, False, False]
Current timestep = 637. State = [[-0.09507398 -0.26868567]]. Action = [[ 0.04283434 -0.13843967 -0.14308888 -0.98354805]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 637 is [True, False, False, True, False, False]
Current timestep = 638. State = [[-0.08531526 -0.2803276 ]]. Action = [[ 0.17553675 -0.11929888 -0.20193885 -0.7817353 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 638 is [True, False, False, True, False, False]
Current timestep = 639. State = [[-0.0742078  -0.29246682]]. Action = [[-0.04738735 -0.04388487 -0.23086686  0.08452606]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 639 is [True, False, False, True, False, False]
Scene graph at timestep 639 is [True, False, False, True, False, False]
State prediction error at timestep 639 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.06378431 -0.2945494 ]]. Action = [[ 0.24693504  0.03377581 -0.0497067  -0.7358036 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 640 is [True, False, False, True, False, False]
Scene graph at timestep 640 is [True, False, False, True, False, False]
State prediction error at timestep 640 is tensor(8.9637e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of 1
Current timestep = 641. State = [[-0.04437283 -0.28637594]]. Action = [[-0.17896391  0.20583495 -0.04195657 -0.6021474 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 641 is [True, False, False, True, False, False]
Current timestep = 642. State = [[-0.04584424 -0.26372567]]. Action = [[-0.07212552  0.23850614 -0.10986017 -0.71877295]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 642 is [False, True, False, True, False, False]
Current timestep = 643. State = [[-0.06450706 -0.25678772]]. Action = [[-0.20852701 -0.20997597  0.01792967 -0.7462169 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 643 is [False, True, False, True, False, False]
Current timestep = 644. State = [[-0.0727726  -0.25439265]]. Action = [[ 0.24743283  0.1389314   0.19754124 -0.38390625]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 644 is [True, False, False, True, False, False]
Current timestep = 645. State = [[-0.06196324 -0.25505495]]. Action = [[ 0.16864693 -0.17482828 -0.13758932 -0.3954708 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 645 is [True, False, False, True, False, False]
Current timestep = 646. State = [[-0.05010628 -0.25269765]]. Action = [[-0.11528076  0.2053954  -0.14214864 -0.63484603]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 646 is [True, False, False, True, False, False]
Current timestep = 647. State = [[-0.0496088 -0.2510478]]. Action = [[ 0.01967755 -0.13413674  0.04962665  0.48268378]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 647 is [True, False, False, True, False, False]
Current timestep = 648. State = [[-0.04968414 -0.24752924]]. Action = [[-0.05934675  0.15583289  0.13829547 -0.31236947]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 648 is [False, True, False, True, False, False]
Current timestep = 649. State = [[-0.04856285 -0.23931107]]. Action = [[ 0.00547436  0.04197904  0.09077552 -0.3246622 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 649 is [False, True, False, True, False, False]
Current timestep = 650. State = [[-0.0548293  -0.22275403]]. Action = [[-0.19256149  0.23396039  0.15393856 -0.6057068 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 650 is [False, True, False, True, False, False]
Current timestep = 651. State = [[-0.0649621  -0.21047376]]. Action = [[ 0.19197822 -0.14015856 -0.19270882 -0.96946466]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 651 is [True, False, False, True, False, False]
Current timestep = 652. State = [[-0.06388009 -0.20720322]]. Action = [[-0.00235613  0.10347292 -0.22596107  0.8616035 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 652 is [True, False, False, True, False, False]
Scene graph at timestep 652 is [True, False, False, True, False, False]
State prediction error at timestep 652 is tensor(5.5946e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 652 of 1
Current timestep = 653. State = [[-0.05759314 -0.21378687]]. Action = [[ 0.16202351 -0.23767938 -0.18683223  0.13562524]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 653 is [True, False, False, True, False, False]
Scene graph at timestep 653 is [True, False, False, True, False, False]
State prediction error at timestep 653 is tensor(6.3394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 1
Current timestep = 654. State = [[-0.04070051 -0.22902668]]. Action = [[ 0.11567846  0.01420641 -0.02127707 -0.46880966]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 654 is [True, False, False, True, False, False]
Scene graph at timestep 654 is [False, True, False, True, False, False]
State prediction error at timestep 654 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 654 of 1
Current timestep = 655. State = [[-0.02731862 -0.21497725]]. Action = [[ 0.09405521  0.23144352  0.17969    -0.5109575 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 655 is [False, True, False, True, False, False]
Current timestep = 656. State = [[-0.02049797 -0.19371909]]. Action = [[-0.04761291  0.13780493 -0.17518282 -0.9177435 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 656 is [False, True, False, True, False, False]
Scene graph at timestep 656 is [False, True, False, True, False, False]
State prediction error at timestep 656 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of 1
Current timestep = 657. State = [[-0.02239994 -0.17720126]]. Action = [[-0.18708313  0.06967741 -0.01966636 -0.8615777 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 657 is [False, True, False, True, False, False]
Current timestep = 658. State = [[-0.03599783 -0.16440167]]. Action = [[-0.22917426  0.14497739  0.24084899 -0.89502054]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 658 is [False, True, False, True, False, False]
Current timestep = 659. State = [[-0.05014766 -0.144356  ]]. Action = [[0.08205381 0.12586346 0.14987195 0.5050771 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 659 is [False, True, False, True, False, False]
Current timestep = 660. State = [[-0.05068593 -0.12218191]]. Action = [[-0.03671914  0.21537971 -0.19647048  0.11537564]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 660 is [True, False, False, True, False, False]
Current timestep = 661. State = [[-0.05187508 -0.10903065]]. Action = [[ 0.04057103 -0.11525238 -0.1271654  -0.487095  ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 661 is [True, False, False, False, True, False]
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of 1
Current timestep = 662. State = [[-0.04722483 -0.1165656 ]]. Action = [[ 0.21177858 -0.10080665 -0.19983028 -0.36863875]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 662 is [True, False, False, False, True, False]
Scene graph at timestep 662 is [False, True, False, False, True, False]
State prediction error at timestep 662 is tensor(5.0447e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of 1
Current timestep = 663. State = [[-0.02965392 -0.12162712]]. Action = [[0.21367228 0.04170406 0.10259357 0.4794134 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 663 is [False, True, False, False, True, False]
Current timestep = 664. State = [[-0.01854767 -0.11480545]]. Action = [[-0.1465215   0.09539685  0.18680894  0.381778  ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 664 is [False, True, False, False, True, False]
Current timestep = 665. State = [[-0.01956416 -0.10607645]]. Action = [[-0.07662451  0.06820533  0.22160316 -0.1811738 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 665 is [False, True, False, False, True, False]
Current timestep = 666. State = [[-0.25229216  0.06279313]]. Action = [[ 0.1316759   0.2288355  -0.01407579  0.4247651 ]]. Reward = [100.]
Curr episode timestep = 110
Scene graph at timestep 666 is [False, True, False, False, True, False]
Current timestep = 667. State = [[-0.2414836   0.06937316]]. Action = [[ 0.22102216 -0.00489867 -0.07233031 -0.6592644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 667 is [True, False, False, False, True, False]
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.21675147  0.0665332 ]]. Action = [[ 0.00288296 -0.11451252 -0.05605395 -0.02813953]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.21849234  0.05217671]]. Action = [[-0.1319559  -0.15066403 -0.00485308 -0.5834738 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.21614131  0.03139513]]. Action = [[ 0.16203094 -0.14522554 -0.0855682  -0.8193149 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.21222395  0.02730363]]. Action = [[ 0.03077871  0.14906412 -0.03389269  0.64000416]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.20198429  0.03066632]]. Action = [[ 0.1584548  -0.02480343 -0.23244365 -0.00025851]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 672 is [True, False, False, False, True, False]
Current timestep = 673. State = [[-0.18650506  0.03312344]]. Action = [[0.02820152 0.04859817 0.14882937 0.778551  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 673 is [True, False, False, False, True, False]
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.17934556  0.02591467]]. Action = [[ 0.00299442 -0.19875744 -0.22766933  0.37587845]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.17746237  0.00730539]]. Action = [[-0.01766258 -0.1385907   0.09569338 -0.0503847 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.17705777  0.00823049]]. Action = [[0.04999071 0.20686904 0.08164328 0.9181323 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.17711699  0.01336311]]. Action = [[-0.06615263 -0.03762572  0.12202349  0.81085324]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 677 is [True, False, False, False, True, False]
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(2.5365e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 677 of -1
Current timestep = 678. State = [[-0.16949934  0.00120223]]. Action = [[ 0.1999375  -0.21674523 -0.14212938 -0.66924244]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 678 is [True, False, False, False, True, False]
Current timestep = 679. State = [[-0.14874807 -0.01670758]]. Action = [[ 0.21264142 -0.07651721 -0.05981228  0.43051744]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 679 is [True, False, False, False, True, False]
Scene graph at timestep 679 is [True, False, False, False, True, False]
State prediction error at timestep 679 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.12708542 -0.02585376]]. Action = [[-0.13278143 -0.03436661 -0.15311286 -0.7236759 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.12813316 -0.02114576]]. Action = [[0.10581329 0.15608835 0.06857005 0.14281511]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 681 is [True, False, False, False, True, False]
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.1274788   0.00183487]]. Action = [[ 0.03070739  0.23318356 -0.03983738  0.80649495]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 682 is [True, False, False, False, True, False]
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.12688033  0.02030791]]. Action = [[-0.22306769 -0.02613038  0.21613815 -0.50828576]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 683 is [True, False, False, False, True, False]
Current timestep = 684. State = [[-0.1340251   0.01679911]]. Action = [[-0.04668663 -0.05639932  0.01907441 -0.10695434]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 684 is [True, False, False, False, True, False]
Current timestep = 685. State = [[-0.13480437  0.02087111]]. Action = [[ 0.2174922   0.1428116  -0.13543354  0.95059   ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 685 is [True, False, False, False, True, False]
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(2.2184e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of -1
Current timestep = 686. State = [[-0.12055365  0.02244342]]. Action = [[ 0.17287782 -0.13840775 -0.10128506 -0.90331244]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 686 is [True, False, False, False, True, False]
Current timestep = 687. State = [[-0.11125674  0.02273358]]. Action = [[-0.05189091  0.11684978  0.23993409  0.74626637]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.10271573  0.0261598 ]]. Action = [[ 0.19840214 -0.00240459 -0.06519583 -0.04229838]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 688 is [True, False, False, False, True, False]
Current timestep = 689. State = [[-0.09102999  0.03673556]]. Action = [[-0.07130185  0.15365577 -0.18224636 -0.5429018 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 689 is [True, False, False, False, True, False]
Current timestep = 690. State = [[-0.09108898  0.06237392]]. Action = [[ 0.05736229  0.2440843  -0.24051905 -0.86707014]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 690 is [True, False, False, False, True, False]
Current timestep = 691. State = [[-0.09217434  0.06816575]]. Action = [[-0.21298996 -0.20124505 -0.16701145  0.8738427 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 691 is [True, False, False, False, True, False]
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of -1
Current timestep = 692. State = [[-0.10450942  0.06886274]]. Action = [[-0.21280092  0.14213502 -0.01162954 -0.28337198]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.11388546  0.06657775]]. Action = [[ 0.16579604 -0.1957597  -0.00853425 -0.5016322 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.10430257  0.05099209]]. Action = [[ 0.20702827 -0.07858029  0.21866453 -0.9358006 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 694 is [True, False, False, False, True, False]
Scene graph at timestep 694 is [True, False, False, False, True, False]
State prediction error at timestep 694 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 694 of 1
Current timestep = 695. State = [[-0.08739378  0.04597933]]. Action = [[ 0.1380912   0.09012166  0.06175017 -0.1114127 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 695 is [True, False, False, False, True, False]
Current timestep = 696. State = [[-0.07841181  0.05831883]]. Action = [[-0.05454406  0.14016789 -0.2115774  -0.18745208]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 696 is [True, False, False, False, True, False]
Scene graph at timestep 696 is [True, False, False, False, True, False]
State prediction error at timestep 696 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 696 of 1
Current timestep = 697. State = [[-0.08007619  0.06413617]]. Action = [[-0.1726918  -0.12629594 -0.18628758  0.14879513]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.07923465  0.04449478]]. Action = [[ 0.08084467 -0.23930825  0.07050872  0.5774368 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 698 is [True, False, False, False, True, False]
Current timestep = 699. State = [[-0.08080203  0.03124062]]. Action = [[-0.119899    0.0410707   0.01832342  0.030743  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 699 is [True, False, False, False, True, False]
Scene graph at timestep 699 is [True, False, False, False, True, False]
State prediction error at timestep 699 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 699 of -1
Current timestep = 700. State = [[-0.08896102  0.03556467]]. Action = [[0.11201698 0.14576852 0.17578262 0.74047256]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.0927616   0.05642875]]. Action = [[-0.13264884  0.21275914  0.13445991 -0.9391338 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.09176192  0.07299173]]. Action = [[ 0.22183192  0.00396255 -0.08162044  0.4123981 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 702 is [True, False, False, False, True, False]
Scene graph at timestep 702 is [True, False, False, False, True, False]
State prediction error at timestep 702 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 702 of -1
Current timestep = 703. State = [[-0.07955889  0.074712  ]]. Action = [[ 0.10955495 -0.03168112  0.08306897  0.11988914]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 703 is [True, False, False, False, True, False]
Current timestep = 704. State = [[-0.06034094  0.08384928]]. Action = [[ 0.24870455  0.1699791   0.18886727 -0.6878876 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 704 is [True, False, False, False, True, False]
Current timestep = 705. State = [[-0.27360636  0.15402758]]. Action = [[ 0.02770975  0.19156599 -0.17208266 -0.25492674]]. Reward = [100.]
Curr episode timestep = 38
Scene graph at timestep 705 is [True, False, False, False, True, False]
Current timestep = 706. State = [[-0.25993916  0.16309018]]. Action = [[ 0.21866107 -0.14638051 -0.0065112  -0.70051575]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 706 is [True, False, False, False, False, True]
Current timestep = 707. State = [[-0.2325481  0.1505873]]. Action = [[ 0.20341516 -0.09562716 -0.22527716  0.78349423]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 707 is [True, False, False, False, False, True]
Current timestep = 708. State = [[-0.20594154  0.12988414]]. Action = [[ 0.14799798 -0.22898181 -0.17035626  0.30061722]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 708 is [True, False, False, False, False, True]
Current timestep = 709. State = [[-0.19556497  0.11076158]]. Action = [[-0.14941381 -0.07972825 -0.06353529  0.05416441]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 709 is [True, False, False, False, False, True]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[-0.20609915  0.10457656]]. Action = [[-0.24076365  0.03700858 -0.21339239 -0.09149808]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 710 is [True, False, False, False, True, False]
Current timestep = 711. State = [[-0.2132108   0.10482333]]. Action = [[ 0.21676517 -0.01748262 -0.23218113 -0.02641976]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.21294339  0.10689051]]. Action = [[-0.11866161  0.05474338 -0.05188933 -0.6252306 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 712 is [True, False, False, False, True, False]
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(1.6206e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[-0.22249739  0.10804885]]. Action = [[-0.19335055 -0.04643214 -0.15621553 -0.9655723 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 713 is [True, False, False, False, True, False]
Current timestep = 714. State = [[-0.22756523  0.11167723]]. Action = [[0.19352692 0.13132519 0.15697289 0.19495356]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 714 is [True, False, False, False, True, False]
Current timestep = 715. State = [[-0.23025182  0.12904651]]. Action = [[-0.06965759  0.20718455 -0.08009128 -0.18051815]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.22792068  0.15449645]]. Action = [[ 0.17353821  0.18031001  0.10712215 -0.7168533 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 716 is [True, False, False, False, False, True]
Current timestep = 717. State = [[-0.20672762  0.1749421 ]]. Action = [[0.20178813 0.11739796 0.03793496 0.8641641 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 717 is [True, False, False, False, False, True]
Scene graph at timestep 717 is [True, False, False, False, False, True]
State prediction error at timestep 717 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of -1
Current timestep = 718. State = [[-0.1884728   0.20259997]]. Action = [[-0.0117244   0.21124583  0.042732    0.44098258]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 718 is [True, False, False, False, False, True]
Current timestep = 719. State = [[-0.19418944  0.2290047 ]]. Action = [[-0.0683075   0.20288628  0.17712528  0.37133062]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 719 is [True, False, False, False, False, True]
Current timestep = 720. State = [[-0.1895137   0.24701723]]. Action = [[ 0.14699832  0.02116373 -0.05218753 -0.78182954]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 720 is [True, False, False, False, False, True]
Scene graph at timestep 720 is [True, False, False, False, False, True]
State prediction error at timestep 720 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of -1
Current timestep = 721. State = [[-0.17372967  0.24622102]]. Action = [[-0.04220271 -0.17343323 -0.24519464 -0.6438064 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 721 is [True, False, False, False, False, True]
Current timestep = 722. State = [[-0.18027364  0.24119443]]. Action = [[-0.24058746 -0.00094055 -0.19940647  0.80019355]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 722 is [True, False, False, False, False, True]
Scene graph at timestep 722 is [True, False, False, False, False, True]
State prediction error at timestep 722 is tensor(5.1489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.19410248  0.24424152]]. Action = [[ 0.1231609   0.11384428 -0.13034357  0.31139243]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 723 is [True, False, False, False, False, True]
Current timestep = 724. State = [[-0.183456    0.24609998]]. Action = [[ 0.24391896 -0.00548244  0.19007382 -0.06714559]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 724 is [True, False, False, False, False, True]
Current timestep = 725. State = [[-0.17494352  0.2528534 ]]. Action = [[-0.22544326  0.008811    0.01523235 -0.88960826]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 725 is [True, False, False, False, False, True]
Current timestep = 726. State = [[-0.18372591  0.26293704]]. Action = [[-0.08929145  0.12253881  0.23712814 -0.27576005]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 726 is [True, False, False, False, False, True]
Current timestep = 727. State = [[-0.19792421  0.26162842]]. Action = [[-0.21568252 -0.21691844  0.05975038 -0.17482322]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 727 is [True, False, False, False, False, True]
Current timestep = 728. State = [[-0.21206337  0.25577065]]. Action = [[ 0.10541871  0.14128524  0.07570615 -0.7765473 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 728 is [True, False, False, False, False, True]
Current timestep = 729. State = [[-0.20525539  0.25295398]]. Action = [[ 0.20269299 -0.08948807 -0.21734767  0.1322608 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 729 is [True, False, False, False, False, True]
Current timestep = 730. State = [[-0.18707985  0.253276  ]]. Action = [[0.23973686 0.11672932 0.21681559 0.287004  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 730 is [True, False, False, False, False, True]
Current timestep = 731. State = [[-0.17243102  0.2589826 ]]. Action = [[-0.14853074 -0.06324853  0.08740938  0.78110695]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 731 is [True, False, False, False, False, True]
Scene graph at timestep 731 is [True, False, False, False, False, True]
State prediction error at timestep 731 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of -1
Current timestep = 732. State = [[-0.16636865  0.24795051]]. Action = [[ 0.17998576 -0.10630509  0.02969176 -0.13478315]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 732 is [True, False, False, False, False, True]
Scene graph at timestep 732 is [True, False, False, False, False, True]
State prediction error at timestep 732 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of 1
Current timestep = 733. State = [[-0.15105054  0.24051899]]. Action = [[ 0.15544641  0.03941789 -0.19711292  0.1082375 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 733 is [True, False, False, False, False, True]
Current timestep = 734. State = [[-0.13086443  0.23077284]]. Action = [[ 0.16945073 -0.19364098 -0.15848169  0.43514442]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 734 is [True, False, False, False, False, True]
Current timestep = 735. State = [[-0.1149041   0.20842864]]. Action = [[-0.16503897 -0.22027816 -0.20851572  0.33578682]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 735 is [True, False, False, False, False, True]
Current timestep = 736. State = [[-0.1177536   0.18402058]]. Action = [[-0.14095578 -0.18692298 -0.08603534 -0.18966186]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 736 is [True, False, False, False, False, True]
Scene graph at timestep 736 is [True, False, False, False, False, True]
State prediction error at timestep 736 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.12139151  0.15764284]]. Action = [[ 0.12524933 -0.12297016 -0.20155782  0.040416  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 737 is [True, False, False, False, False, True]
Current timestep = 738. State = [[-0.11732605  0.1396524 ]]. Action = [[-0.04497404 -0.15375137  0.07525611 -0.7920877 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 738 is [True, False, False, False, False, True]
Current timestep = 739. State = [[-0.11587025  0.13241003]]. Action = [[ 0.07630447  0.10965741  0.06717873 -0.8918981 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 739 is [True, False, False, False, False, True]
Current timestep = 740. State = [[-0.11420885  0.13825046]]. Action = [[ 0.09464476  0.10255331 -0.13648438 -0.69206494]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 740 is [True, False, False, False, False, True]
Scene graph at timestep 740 is [True, False, False, False, False, True]
State prediction error at timestep 740 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.1047628   0.14919433]]. Action = [[0.19587728 0.09635422 0.10022587 0.0094564 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 741 is [True, False, False, False, False, True]
Scene graph at timestep 741 is [True, False, False, False, False, True]
State prediction error at timestep 741 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.07920374  0.16305551]]. Action = [[ 0.24383926  0.0911454  -0.01441468  0.39392614]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 742 is [True, False, False, False, False, True]
Current timestep = 743. State = [[-0.06048129  0.16447866]]. Action = [[-0.07867843 -0.14446568 -0.10430565  0.14870155]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 743 is [True, False, False, False, False, True]
Current timestep = 744. State = [[-0.05620469  0.14703041]]. Action = [[-0.01445401 -0.20144576  0.1454992   0.86305475]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 744 is [True, False, False, False, False, True]
Scene graph at timestep 744 is [True, False, False, False, False, True]
State prediction error at timestep 744 is tensor(6.3393e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of 1
Current timestep = 745. State = [[-0.05176779  0.1266941 ]]. Action = [[-0.06327975 -0.06493044  0.1604209  -0.4150263 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 745 is [True, False, False, False, False, True]
Current timestep = 746. State = [[-0.05223692  0.10797407]]. Action = [[ 0.01339027 -0.21003811 -0.19324112  0.8865609 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 746 is [True, False, False, False, False, True]
Current timestep = 747. State = [[-0.04761452  0.080955  ]]. Action = [[ 0.10904419 -0.19167805  0.13118076  0.59888864]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 747 is [True, False, False, False, True, False]
Current timestep = 748. State = [[-0.03969745  0.06174201]]. Action = [[ 0.13155627  0.01685628  0.18938404 -0.7154137 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 748 is [False, True, False, False, True, False]
Current timestep = 749. State = [[-0.04199489  0.06385193]]. Action = [[-0.22861725  0.07576013 -0.11876002 -0.46273983]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 749 is [False, True, False, False, True, False]
Scene graph at timestep 749 is [False, True, False, False, True, False]
State prediction error at timestep 749 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[-0.04368203  0.06857809]]. Action = [[ 0.14997864  0.02456218 -0.18247133 -0.57306516]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 750 is [False, True, False, False, True, False]
Current timestep = 751. State = [[-0.04084612  0.0621655 ]]. Action = [[ 0.05592376 -0.11201701 -0.00931592 -0.84838104]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 751 is [False, True, False, False, True, False]
Scene graph at timestep 751 is [False, True, False, False, True, False]
State prediction error at timestep 751 is tensor(7.3733e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of 1
Current timestep = 752. State = [[-0.25636438  0.21487075]]. Action = [[0.15539533 0.23027825 0.0660978  0.78747416]]. Reward = [100.]
Curr episode timestep = 46
Scene graph at timestep 752 is [False, True, False, False, True, False]
Current timestep = 753. State = [[-0.23938294  0.23302908]]. Action = [[ 0.24740747 -0.10529035 -0.10935912 -0.55797195]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 753 is [True, False, False, False, False, True]
Current timestep = 754. State = [[-0.2210364  0.2236819]]. Action = [[-0.05363511 -0.12003061 -0.13808799  0.6576601 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 754 is [True, False, False, False, False, True]
Current timestep = 755. State = [[-0.20960873  0.21831334]]. Action = [[ 0.23251978  0.06895348 -0.03927101 -0.42407256]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 755 is [True, False, False, False, False, True]
Current timestep = 756. State = [[-0.1871915   0.21492118]]. Action = [[ 0.09206611 -0.11441998 -0.01332989  0.92559314]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 756 is [True, False, False, False, False, True]
Current timestep = 757. State = [[-0.17610651  0.21570772]]. Action = [[ 0.06436563  0.16160166 -0.18143179  0.09378386]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 757 is [True, False, False, False, False, True]
Current timestep = 758. State = [[-0.16728945  0.21386121]]. Action = [[-0.083783   -0.21148236  0.07402173 -0.9223972 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 758 is [True, False, False, False, False, True]
Current timestep = 759. State = [[-0.17553289  0.2061443 ]]. Action = [[-0.23013528 -0.01573424  0.21301687 -0.5310264 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 759 is [True, False, False, False, False, True]
Current timestep = 760. State = [[-0.18123862  0.1925957 ]]. Action = [[ 0.0642108  -0.18776864 -0.2326461  -0.3074031 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 760 is [True, False, False, False, False, True]
Current timestep = 761. State = [[-0.18647218  0.18970463]]. Action = [[-0.14867687  0.14350784 -0.24144515  0.15640664]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 761 is [True, False, False, False, False, True]
Current timestep = 762. State = [[-0.19525017  0.18503636]]. Action = [[-0.05084801 -0.21114054  0.00259987  0.9695337 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 762 is [True, False, False, False, False, True]
Current timestep = 763. State = [[-0.201754    0.17960152]]. Action = [[ 0.09325176  0.15947104  0.13019347 -0.75660855]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 763 is [True, False, False, False, False, True]
Scene graph at timestep 763 is [True, False, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.2078792   0.19115175]]. Action = [[-0.10773663  0.08886096  0.11917609 -0.9130115 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 764 is [True, False, False, False, False, True]
Current timestep = 765. State = [[-0.21203676  0.19723137]]. Action = [[ 0.00072074 -0.03753695 -0.17633091  0.4428122 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 765 is [True, False, False, False, False, True]
Current timestep = 766. State = [[-0.21213907  0.20627044]]. Action = [[ 0.101163    0.18835166 -0.16653048 -0.30628586]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 766 is [True, False, False, False, False, True]
Current timestep = 767. State = [[-0.20963375  0.20673129]]. Action = [[ 0.00902531 -0.14960977  0.02080598 -0.22795367]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 767 is [True, False, False, False, False, True]
Scene graph at timestep 767 is [True, False, False, False, False, True]
State prediction error at timestep 767 is tensor(1.9434e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of -1
Current timestep = 768. State = [[-0.20280503  0.20654885]]. Action = [[0.14133048 0.1286307  0.13124722 0.62371063]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 768 is [True, False, False, False, False, True]
Current timestep = 769. State = [[-0.18846057  0.22282603]]. Action = [[0.17214435 0.20340198 0.23219213 0.1442455 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 769 is [True, False, False, False, False, True]
Current timestep = 770. State = [[-0.16669281  0.23333345]]. Action = [[ 0.12303272 -0.06775779 -0.20817637  0.9841938 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 770 is [True, False, False, False, False, True]
Current timestep = 771. State = [[-0.15088417  0.22020967]]. Action = [[ 0.09382483 -0.24553819 -0.14926498  0.59659946]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 771 is [True, False, False, False, False, True]
Current timestep = 772. State = [[-0.13622308  0.19074494]]. Action = [[ 0.09123707 -0.21938772 -0.00736988 -0.03840286]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 772 is [True, False, False, False, False, True]
Current timestep = 773. State = [[-0.13154332  0.1847642 ]]. Action = [[-0.0384361   0.20635712  0.19681346 -0.8353649 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 773 is [True, False, False, False, False, True]
Current timestep = 774. State = [[-0.14232995  0.20561   ]]. Action = [[-0.21388528  0.22626746 -0.10066447  0.88297415]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 774 is [True, False, False, False, False, True]
Current timestep = 775. State = [[-0.15259066  0.2152076 ]]. Action = [[-0.08224002 -0.20616497 -0.05784345 -0.92069644]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 775 is [True, False, False, False, False, True]
Current timestep = 776. State = [[-0.14904352  0.2055041 ]]. Action = [[ 0.19695663 -0.02453758  0.01964703 -0.7704361 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 776 is [True, False, False, False, False, True]
Current timestep = 777. State = [[-0.146668    0.20232448]]. Action = [[-0.06631595  0.02724782  0.18334538 -0.13489282]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 777 is [True, False, False, False, False, True]
Current timestep = 778. State = [[-0.14120042  0.20593102]]. Action = [[0.1941467  0.08217445 0.07112557 0.65724933]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 778 is [True, False, False, False, False, True]
Current timestep = 779. State = [[-0.12483744  0.22132546]]. Action = [[ 0.1774301   0.22651815 -0.05529188 -0.78923357]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 779 is [True, False, False, False, False, True]
Scene graph at timestep 779 is [True, False, False, False, False, True]
State prediction error at timestep 779 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[-0.09049839  0.23483503]]. Action = [[ 0.1977886  -0.11874767  0.15899295  0.98539126]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 780 is [True, False, False, False, False, True]
Current timestep = 781. State = [[-0.07490112  0.22532037]]. Action = [[ 0.00279346 -0.09090962  0.18646404 -0.93150884]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 781 is [True, False, False, False, False, True]
Current timestep = 782. State = [[-0.07632763  0.22599754]]. Action = [[-0.12116686  0.10853246  0.14815319  0.36468506]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 782 is [True, False, False, False, False, True]
Scene graph at timestep 782 is [True, False, False, False, False, True]
State prediction error at timestep 782 is tensor(2.5053e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[-0.07428577  0.21914227]]. Action = [[ 0.0902251  -0.21988831  0.08709037 -0.03590047]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 783 is [True, False, False, False, False, True]
Current timestep = 784. State = [[-0.06118253  0.19540359]]. Action = [[ 0.21283454 -0.17127362  0.22247952 -0.08001268]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 784 is [True, False, False, False, False, True]
Current timestep = 785. State = [[-0.04935809  0.19175293]]. Action = [[-0.02453324  0.2213462  -0.23423296 -0.50783235]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 785 is [True, False, False, False, False, True]
Current timestep = 786. State = [[-0.05391092  0.21620601]]. Action = [[-0.20371068  0.22811389  0.15066147  0.19836831]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 786 is [False, True, False, False, False, True]
Scene graph at timestep 786 is [True, False, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of -1
Current timestep = 787. State = [[-0.06591975  0.24930513]]. Action = [[0.11972636 0.18503848 0.17636275 0.9684727 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 787 is [True, False, False, False, False, True]
Scene graph at timestep 787 is [True, False, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of -1
Current timestep = 788. State = [[-0.04844462  0.25427502]]. Action = [[ 0.10484341 -0.19068457 -0.23015058  0.41327667]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 788 is [True, False, False, False, False, True]
Current timestep = 789. State = [[-0.04297486  0.24180244]]. Action = [[-0.09969802 -0.08248244 -0.09379579  0.95812416]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 789 is [False, True, False, False, False, True]
Current timestep = 790. State = [[-0.03458089  0.22385009]]. Action = [[ 0.23561847 -0.18184698 -0.16974136  0.39767635]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 790 is [False, True, False, False, False, True]
Current timestep = 791. State = [[-0.02328104  0.20488724]]. Action = [[ 0.04982477 -0.0182465  -0.09451447  0.7755666 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 791 is [False, True, False, False, False, True]
Current timestep = 792. State = [[-0.02244287  0.2040242 ]]. Action = [[-0.05542888  0.07424992 -0.08670838 -0.54579365]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 792 is [False, True, False, False, False, True]
Scene graph at timestep 792 is [False, True, False, False, False, True]
State prediction error at timestep 792 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.01650669  0.2029588 ]]. Action = [[ 0.17394996 -0.05497164 -0.20010734  0.1826191 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 793 is [False, True, False, False, False, True]
Current timestep = 794. State = [[0.00286051 0.21316671]]. Action = [[ 0.15772822  0.20441842 -0.14882655  0.6119299 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 794 is [False, True, False, False, False, True]
Scene graph at timestep 794 is [False, True, False, False, False, True]
State prediction error at timestep 794 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of -1
Current timestep = 795. State = [[0.02252918 0.2443161 ]]. Action = [[-0.05123129  0.21572691  0.13614929  0.67591584]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 795 is [False, True, False, False, False, True]
Scene graph at timestep 795 is [False, True, False, False, False, True]
State prediction error at timestep 795 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of -1
Current timestep = 796. State = [[0.01769293 0.27514443]]. Action = [[-0.09567574  0.22848427 -0.01077516  0.844445  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 796 is [False, True, False, False, False, True]
Scene graph at timestep 796 is [False, True, False, False, False, True]
State prediction error at timestep 796 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 796 of -1
Current timestep = 797. State = [[0.00838461 0.29453418]]. Action = [[-0.12644815  0.2310622   0.00715882 -0.3685031 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 797 is [False, True, False, False, False, True]
Current timestep = 798. State = [[0.01086126 0.2872092 ]]. Action = [[-0.14060624 -0.21159628 -0.1593937   0.23896539]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 798 is [False, True, False, False, False, True]
Scene graph at timestep 798 is [False, True, False, False, False, True]
State prediction error at timestep 798 is tensor(6.9918e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[0.0052381 0.2853145]]. Action = [[-0.24563475  0.04034656  0.20379776 -0.18238795]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 799 is [False, True, False, False, False, True]
Scene graph at timestep 799 is [False, True, False, False, False, True]
State prediction error at timestep 799 is tensor(6.3921e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of 1
Current timestep = 800. State = [[-0.01207147  0.27873653]]. Action = [[-0.07315767 -0.20586254  0.07931641  0.5745069 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 800 is [False, True, False, False, False, True]
Current timestep = 801. State = [[-0.01641385  0.25681758]]. Action = [[ 0.06635588 -0.12017977 -0.16463664 -0.4767335 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 801 is [False, True, False, False, False, True]
Current timestep = 802. State = [[-0.01863099  0.23347396]]. Action = [[-0.17236549 -0.24825466 -0.02890274  0.75201917]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 802 is [False, True, False, False, False, True]
Current timestep = 803. State = [[-0.02218892  0.20130125]]. Action = [[ 0.24166572 -0.14010571  0.07132792  0.27864516]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 803 is [False, True, False, False, False, True]
Scene graph at timestep 803 is [False, True, False, False, False, True]
State prediction error at timestep 803 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of 1
Current timestep = 804. State = [[-0.01156806  0.17761351]]. Action = [[ 0.06465453 -0.06290963  0.10238206  0.5794151 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 804 is [False, True, False, False, False, True]
Scene graph at timestep 804 is [False, True, False, False, False, True]
State prediction error at timestep 804 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of 1
Current timestep = 805. State = [[-0.00589524  0.15965085]]. Action = [[ 0.04925472 -0.17849477  0.08127052  0.682431  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 805 is [False, True, False, False, False, True]
Current timestep = 806. State = [[-0.00119785  0.15477781]]. Action = [[ 0.09854507  0.1942214  -0.18103172  0.6680827 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 806 is [False, True, False, False, False, True]
Current timestep = 807. State = [[0.00605686 0.15552758]]. Action = [[ 0.20420963 -0.0516752  -0.1106002   0.85573626]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 807 is [False, True, False, False, False, True]
Scene graph at timestep 807 is [False, True, False, False, False, True]
State prediction error at timestep 807 is tensor(1.8346e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of 1
Current timestep = 808. State = [[0.01989073 0.15546548]]. Action = [[-0.12862948 -0.03428258  0.12877917 -0.753078  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 808 is [False, True, False, False, False, True]
Scene graph at timestep 808 is [False, True, False, False, False, True]
State prediction error at timestep 808 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of 1
Current timestep = 809. State = [[0.02417462 0.14836036]]. Action = [[ 0.22951591 -0.05315417  0.01596782  0.42202234]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 809 is [False, True, False, False, False, True]
Current timestep = 810. State = [[0.03531482 0.15198098]]. Action = [[ 0.06992456  0.16405076  0.03168458 -0.7874151 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 810 is [False, True, False, False, False, True]
Current timestep = 811. State = [[0.04755893 0.14806151]]. Action = [[ 0.14191028 -0.2224764   0.2040849  -0.3154403 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 811 is [False, True, False, False, False, True]
Current timestep = 812. State = [[0.05530914 0.14849867]]. Action = [[-0.21766002  0.22156674  0.18944263 -0.18444973]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 812 is [False, True, False, False, False, True]
Current timestep = 813. State = [[0.04912711 0.16197664]]. Action = [[ 0.18522924 -0.10395832 -0.15329371 -0.7345165 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 813 is [False, False, True, False, False, True]
Current timestep = 814. State = [[0.04488338 0.17140773]]. Action = [[-0.08913559  0.09562477  0.05217588 -0.7061137 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 814 is [False, True, False, False, False, True]
Scene graph at timestep 814 is [False, True, False, False, False, True]
State prediction error at timestep 814 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of -1
Current timestep = 815. State = [[0.04049012 0.18067625]]. Action = [[ 0.20176682 -0.17635836 -0.06828445  0.12260222]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 815 is [False, True, False, False, False, True]
Current timestep = 816. State = [[0.03555485 0.1906388 ]]. Action = [[-0.1321622   0.13869375 -0.07699811  0.40517497]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 816 is [False, True, False, False, False, True]
Current timestep = 817. State = [[0.02203647 0.2153281 ]]. Action = [[-0.22832528  0.21413088 -0.02754286  0.64655685]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 817 is [False, True, False, False, False, True]
Current timestep = 818. State = [[0.00775338 0.23047753]]. Action = [[ 0.02518949 -0.09994024 -0.0973857   0.60036945]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 818 is [False, True, False, False, False, True]
Current timestep = 819. State = [[0.0020721  0.23593952]]. Action = [[-0.03497198  0.16024345 -0.02251591  0.3412273 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 819 is [False, True, False, False, False, True]
Current timestep = 820. State = [[-0.01000521  0.2366654 ]]. Action = [[-0.23241024 -0.1875128  -0.14086246  0.40225005]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 820 is [False, True, False, False, False, True]
Current timestep = 821. State = [[-0.03326164  0.23483339]]. Action = [[-0.10950574  0.11567709  0.04447076 -0.17219359]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 821 is [False, True, False, False, False, True]
Scene graph at timestep 821 is [False, True, False, False, False, True]
State prediction error at timestep 821 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[-0.0546419   0.24412005]]. Action = [[-0.15898217 -0.00271484 -0.20471366  0.79466057]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 822 is [False, True, False, False, False, True]
Current timestep = 823. State = [[-0.07366524  0.25020576]]. Action = [[-0.08661927  0.09881476 -0.24057598 -0.71313304]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 823 is [True, False, False, False, False, True]
Scene graph at timestep 823 is [True, False, False, False, False, True]
State prediction error at timestep 823 is tensor(2.6067e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[-0.09298    0.2665342]]. Action = [[-0.20857558  0.13844964  0.18444976  0.13749504]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 824 is [True, False, False, False, False, True]
Scene graph at timestep 824 is [True, False, False, False, False, True]
State prediction error at timestep 824 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of -1
Current timestep = 825. State = [[-0.1272614  0.2675181]]. Action = [[-0.20717485 -0.1838101  -0.19658028 -0.5347526 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 825 is [True, False, False, False, False, True]
Current timestep = 826. State = [[-0.14386284  0.24804693]]. Action = [[ 0.06827858 -0.0866169   0.13351595  0.3192885 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 826 is [True, False, False, False, False, True]
Current timestep = 827. State = [[-0.14942114  0.24567513]]. Action = [[-0.12015116  0.10033828 -0.19486065  0.40065384]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 827 is [True, False, False, False, False, True]
Current timestep = 828. State = [[-0.16185123  0.25189248]]. Action = [[-0.09397209  0.00588486  0.15419093  0.3400489 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 828 is [True, False, False, False, False, True]
Current timestep = 829. State = [[-0.17226434  0.2421958 ]]. Action = [[-0.02655677 -0.16073568 -0.15811026  0.85366344]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 829 is [True, False, False, False, False, True]
Current timestep = 830. State = [[-0.18114278  0.222223  ]]. Action = [[-0.09224732 -0.11865894 -0.14355487 -0.90102196]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 830 is [True, False, False, False, False, True]
Current timestep = 831. State = [[-0.19621913  0.22542806]]. Action = [[-0.10701168  0.22568244  0.04496482 -0.19373804]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 831 is [True, False, False, False, False, True]
Scene graph at timestep 831 is [True, False, False, False, False, True]
State prediction error at timestep 831 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[-0.20994847  0.25055933]]. Action = [[ 0.15227926  0.24060002 -0.22309938 -0.8120915 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 832 is [True, False, False, False, False, True]
Current timestep = 833. State = [[-0.2119912   0.26071092]]. Action = [[-0.03529198  0.01924059 -0.01890621  0.6166816 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 833 is [True, False, False, False, False, True]
Current timestep = 834. State = [[-0.2124818   0.25877374]]. Action = [[-0.03583714 -0.170141    0.02172089  0.11465335]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 834 is [True, False, False, False, False, True]
Current timestep = 835. State = [[-0.20595105  0.23982789]]. Action = [[ 0.16852063 -0.23139611  0.1098479  -0.03607929]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 835 is [True, False, False, False, False, True]
Scene graph at timestep 835 is [True, False, False, False, False, True]
State prediction error at timestep 835 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[-0.19093794  0.22949646]]. Action = [[ 0.01710394  0.24125561 -0.11033414 -0.86051863]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 836 is [True, False, False, False, False, True]
Scene graph at timestep 836 is [True, False, False, False, False, True]
State prediction error at timestep 836 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of 1
Current timestep = 837. State = [[-0.19638911  0.2488703 ]]. Action = [[ 0.00695738  0.13616568  0.23607987 -0.8898342 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 837 is [True, False, False, False, False, True]
Current timestep = 838. State = [[-0.1942756   0.24811073]]. Action = [[-0.02111836 -0.22251827 -0.15592106 -0.1616779 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 838 is [True, False, False, False, False, True]
Current timestep = 839. State = [[-0.19734243  0.22979005]]. Action = [[-0.14697485 -0.18644628  0.08584693  0.99202156]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 839 is [True, False, False, False, False, True]
Scene graph at timestep 839 is [True, False, False, False, False, True]
State prediction error at timestep 839 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[-0.21241543  0.2007836 ]]. Action = [[-0.23451225 -0.20930453 -0.16192038  0.9278681 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 840 is [True, False, False, False, False, True]
Current timestep = 841. State = [[-0.2383745   0.18666065]]. Action = [[-0.16642724  0.0163582   0.073807    0.04483879]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 841 is [True, False, False, False, False, True]
Current timestep = 842. State = [[-0.26506278  0.18831694]]. Action = [[-0.17450963  0.06377158  0.21864486  0.6486366 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 842 is [True, False, False, False, False, True]
Current timestep = 843. State = [[-0.27551964  0.18669198]]. Action = [[ 0.12534225 -0.08759412 -0.07113576  0.05926919]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 843 is [True, False, False, False, False, True]
Current timestep = 844. State = [[-0.26765436  0.1709071 ]]. Action = [[ 0.14362264 -0.18025719 -0.05529028 -0.12317121]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 844 is [True, False, False, False, False, True]
Scene graph at timestep 844 is [True, False, False, False, False, True]
State prediction error at timestep 844 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of 1
Current timestep = 845. State = [[-0.26062354  0.15120083]]. Action = [[-0.18195345  0.07736167 -0.12556373  0.3147601 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 845 is [True, False, False, False, False, True]
Current timestep = 846. State = [[-0.26062354  0.15120083]]. Action = [[-0.1428275  -0.00466065  0.0464972   0.35691893]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 846 is [True, False, False, False, False, True]
Scene graph at timestep 846 is [True, False, False, False, False, True]
State prediction error at timestep 846 is tensor(3.1604e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 0
Current timestep = 847. State = [[-0.25323036  0.14135003]]. Action = [[ 0.15645704 -0.15203099  0.2307662   0.51193404]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 847 is [True, False, False, False, False, True]
Current timestep = 848. State = [[-0.24367775  0.12661777]]. Action = [[-0.19482699 -0.15769508 -0.20002542  0.38036096]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 848 is [True, False, False, False, False, True]
Current timestep = 849. State = [[-0.24001965  0.12645078]]. Action = [[ 0.07194695  0.04668739 -0.18938756 -0.67005223]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 849 is [True, False, False, False, False, True]
Current timestep = 850. State = [[-0.23225589  0.12422657]]. Action = [[ 0.12797388 -0.04328448 -0.10864711 -0.39712572]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 850 is [True, False, False, False, False, True]
Current timestep = 851. State = [[-0.2217566   0.11073076]]. Action = [[-0.05308519 -0.1960322  -0.15673573 -0.6438086 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 851 is [True, False, False, False, True, False]
Current timestep = 852. State = [[-0.22299081  0.09749511]]. Action = [[-0.13428733 -0.03563312 -0.07318833 -0.47158694]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 852 is [True, False, False, False, True, False]
Current timestep = 853. State = [[-0.22516413  0.09578346]]. Action = [[0.0881809  0.06034815 0.20187992 0.32136226]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.22347073  0.10419511]]. Action = [[ 0.0413298   0.14921796 -0.20252836  0.52153397]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.22874407  0.10112961]]. Action = [[-0.19223511 -0.21848872  0.17041838 -0.82531846]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.22989585  0.09775146]]. Action = [[ 0.12510198  0.09608033 -0.1550308   0.92773867]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.221674    0.08707345]]. Action = [[ 0.153276   -0.23528765 -0.01774664  0.11278021]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.20703451  0.06954718]]. Action = [[0.11800763 0.00288868 0.01398787 0.58940625]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 858 is [True, False, False, False, True, False]
Current timestep = 859. State = [[-0.20144872  0.05599624]]. Action = [[-0.04387799 -0.19821087 -0.11521766 -0.18113959]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.19527803  0.03282721]]. Action = [[ 0.014429   -0.15956987  0.05763185 -0.40047485]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.18442015  0.01758809]]. Action = [[ 0.19877195 -0.01206623 -0.22988428  0.66324055]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.17569767  0.00484558]]. Action = [[ 0.00278944 -0.13680749  0.18530941  0.9587791 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.17203306  0.00704892]]. Action = [[-0.02581006  0.2401348  -0.08949259 -0.00251508]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 863 is [True, False, False, False, True, False]
Current timestep = 864. State = [[-0.16431528  0.02207717]]. Action = [[0.14597565 0.03935218 0.09282261 0.86795926]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 864 is [True, False, False, False, True, False]
Current timestep = 865. State = [[-0.16104354  0.01944485]]. Action = [[-0.10490084 -0.17034917  0.06249687 -0.5118231 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 865 is [True, False, False, False, True, False]
Scene graph at timestep 865 is [True, False, False, False, True, False]
State prediction error at timestep 865 is tensor(1.2603e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 865 of 1
Current timestep = 866. State = [[-0.1652784   0.01018785]]. Action = [[-0.1258228  -0.00743745 -0.07463631  0.5844265 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 866 is [True, False, False, False, True, False]
Scene graph at timestep 866 is [True, False, False, False, True, False]
State prediction error at timestep 866 is tensor(2.9658e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of -1
Current timestep = 867. State = [[-0.17074724  0.01051693]]. Action = [[ 0.08911037  0.09501681  0.24813282 -0.59908247]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 867 is [True, False, False, False, True, False]
Current timestep = 868. State = [[-0.16417651  0.02247084]]. Action = [[ 0.08541837  0.12068248  0.04907599 -0.40478086]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 868 is [True, False, False, False, True, False]
Current timestep = 869. State = [[-0.14951351  0.04299351]]. Action = [[ 0.23433036  0.18462867 -0.09819141 -0.77980363]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 869 is [True, False, False, False, True, False]
Current timestep = 870. State = [[-0.13458124  0.05925831]]. Action = [[-0.06217128  0.01879719  0.22824052 -0.68775946]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 870 is [True, False, False, False, True, False]
Scene graph at timestep 870 is [True, False, False, False, True, False]
State prediction error at timestep 870 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of 1
Current timestep = 871. State = [[-0.13236009  0.06150549]]. Action = [[ 0.05831558 -0.04429039  0.13682342 -0.44048488]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 871 is [True, False, False, False, True, False]
Current timestep = 872. State = [[-0.13632159  0.06858274]]. Action = [[-0.20927788  0.12316152  0.23666263  0.6482469 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 872 is [True, False, False, False, True, False]
Current timestep = 873. State = [[-0.14549601  0.08802422]]. Action = [[-0.06857131  0.20109093 -0.12241057  0.6671816 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of -1
Current timestep = 874. State = [[-0.1476356   0.10812023]]. Action = [[0.23898834 0.02823251 0.05673295 0.37638676]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.13573056  0.10079627]]. Action = [[ 0.07522595 -0.17992428  0.08916295 -0.2842313 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.12107398  0.07846819]]. Action = [[ 0.16523126 -0.17315237 -0.01157181 -0.27211893]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.10210524  0.07260688]]. Action = [[ 0.01714006  0.16964975 -0.20104086  0.8535775 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.09017807  0.07752838]]. Action = [[ 0.19202065 -0.06674072 -0.09164339 -0.23834616]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 878 is [True, False, False, False, True, False]
Current timestep = 879. State = [[-0.18968809 -0.13217188]]. Action = [[-0.21917167  0.1336413  -0.05105644 -0.30803406]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 879 is [True, False, False, False, True, False]
Scene graph at timestep 879 is [True, False, False, True, False, False]
State prediction error at timestep 879 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of -1
Current timestep = 880. State = [[-0.17602125 -0.13492163]]. Action = [[ 0.05930525  0.23672193 -0.14495097  0.73615456]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 880 is [True, False, False, True, False, False]
Scene graph at timestep 880 is [True, False, False, True, False, False]
State prediction error at timestep 880 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of 1
Current timestep = 881. State = [[-0.16593559 -0.12636104]]. Action = [[ 0.12888783 -0.12776683 -0.22133544 -0.9849097 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 881 is [True, False, False, True, False, False]
Current timestep = 882. State = [[-0.15412144 -0.13970806]]. Action = [[ 0.00694016 -0.11477895  0.14325392  0.5927582 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 882 is [True, False, False, True, False, False]
Current timestep = 883. State = [[-0.1428049  -0.14825007]]. Action = [[ 0.15306103 -0.02424073  0.18447977 -0.36646897]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 883 is [True, False, False, True, False, False]
Current timestep = 884. State = [[-0.12580818 -0.14680476]]. Action = [[0.11255017 0.10480532 0.2046139  0.6386466 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 884 is [True, False, False, True, False, False]
Current timestep = 885. State = [[-0.10840333 -0.13637915]]. Action = [[ 0.10778773  0.11828199  0.16286677 -0.03366542]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 885 is [True, False, False, True, False, False]
Current timestep = 886. State = [[-0.10443457 -0.13440788]]. Action = [[-0.20760065 -0.08333224 -0.20668574 -0.5488789 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 886 is [True, False, False, True, False, False]
Current timestep = 887. State = [[-0.10331178 -0.12917972]]. Action = [[ 0.19353926  0.13216591  0.1421966  -0.71244824]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 887 is [True, False, False, True, False, False]
Current timestep = 888. State = [[-0.10100035 -0.13445516]]. Action = [[-0.07786384 -0.2131284   0.17244774  0.12479889]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 888 is [True, False, False, True, False, False]
Scene graph at timestep 888 is [True, False, False, True, False, False]
State prediction error at timestep 888 is tensor(1.1299e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of 1
Current timestep = 889. State = [[-0.09507461 -0.1323056 ]]. Action = [[ 0.22488108  0.23926598 -0.02552402 -0.53611964]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 889 is [True, False, False, True, False, False]
Current timestep = 890. State = [[-0.07538806 -0.1120021 ]]. Action = [[ 0.10110155  0.11868441 -0.09973712  0.7162992 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 890 is [True, False, False, True, False, False]
Current timestep = 891. State = [[-0.05685154 -0.09853118]]. Action = [[ 0.18686861  0.0372023  -0.04037692  0.77313554]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 891 is [True, False, False, False, True, False]
Current timestep = 892. State = [[-0.04338425 -0.10835543]]. Action = [[-0.11015391 -0.234562   -0.24800366 -0.67082036]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 892 is [True, False, False, False, True, False]
Current timestep = 893. State = [[-0.04790493 -0.1143847 ]]. Action = [[-0.15798709  0.12374559 -0.10901034  0.48705745]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 893 is [False, True, False, False, True, False]
Scene graph at timestep 893 is [False, True, False, False, True, False]
State prediction error at timestep 893 is tensor(4.6822e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of -1
Current timestep = 894. State = [[-0.04953729 -0.1110507 ]]. Action = [[ 0.02419242  0.00585175 -0.14076284  0.98774195]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 894 is [False, True, False, False, True, False]
Current timestep = 895. State = [[-0.05102763 -0.11941922]]. Action = [[-0.04668684 -0.1453827  -0.21905833  0.31507564]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 895 is [False, True, False, False, True, False]
Current timestep = 896. State = [[-0.05759675 -0.11581862]]. Action = [[-0.10269013  0.19760856  0.00878045 -0.97432554]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 896 is [True, False, False, False, True, False]
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(7.6978e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of 1
Current timestep = 897. State = [[-0.06111991 -0.0888587 ]]. Action = [[ 0.21970922  0.22547781 -0.20951921 -0.9607203 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 897 is [True, False, False, False, True, False]
Current timestep = 898. State = [[-0.04718133 -0.06750935]]. Action = [[ 0.2324712   0.06921861 -0.14993727  0.73961616]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.27221316  0.04639774]]. Action = [[ 0.22026739  0.15256232 -0.0144553   0.3510704 ]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 899 is [False, True, False, False, True, False]
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of -1
Current timestep = 900. State = [[-0.2734292   0.05049432]]. Action = [[-0.09179293 -0.23450007 -0.1146438   0.20030951]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.2703587   0.06167582]]. Action = [[ 0.07679969  0.1754444  -0.06311288 -0.800915  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 901 is [True, False, False, False, True, False]
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(1.4800e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of 1
Current timestep = 902. State = [[-0.26244614  0.06752242]]. Action = [[-0.0498234  -0.17434667  0.05874828 -0.40947986]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 902 is [True, False, False, False, True, False]
Current timestep = 903. State = [[-0.2607527   0.05080843]]. Action = [[ 0.00272989 -0.15981647  0.04884684 -0.36098933]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.25314057  0.04250631]]. Action = [[0.23639846 0.11310774 0.07075685 0.6154748 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 904 is [True, False, False, False, True, False]
Current timestep = 905. State = [[-0.23679316  0.05380992]]. Action = [[ 0.08215016  0.1435242  -0.22914684  0.37087584]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.21944499  0.0493515 ]]. Action = [[ 0.14982185 -0.24467702 -0.17192599 -0.37493324]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.19902302  0.04007401]]. Action = [[ 0.1475774   0.0122734  -0.23192777 -0.62965804]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.17590126  0.05078328]]. Action = [[ 0.2214325   0.22704843 -0.1834667  -0.28817546]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 908 is [True, False, False, False, True, False]
Current timestep = 909. State = [[-0.15016273  0.06762777]]. Action = [[ 0.09627771  0.08851334 -0.1302212  -0.71200883]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.14206642  0.06797013]]. Action = [[-0.17675005 -0.16198349  0.03089708 -0.223629  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 910 is [True, False, False, False, True, False]
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(1.9371e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of 1
Current timestep = 911. State = [[-0.13921286  0.05038306]]. Action = [[ 0.16739672 -0.154818   -0.2063678   0.7555766 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.14203103  0.03053105]]. Action = [[-0.23085432 -0.15251696 -0.18095997  0.06239831]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 912 is [True, False, False, False, True, False]
Current timestep = 913. State = [[-0.14493345  0.00648927]]. Action = [[ 0.09982246 -0.18218213  0.15799275 -0.5304588 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.13874966 -0.02010575]]. Action = [[ 0.17518246 -0.21691757 -0.10734922 -0.9589918 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 914 is [True, False, False, False, True, False]
Current timestep = 915. State = [[-0.13424312 -0.03966695]]. Action = [[-0.19030565 -0.02583797  0.10769886 -0.07880133]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 915 is [True, False, False, False, True, False]
Current timestep = 916. State = [[-0.13597663 -0.05272567]]. Action = [[ 0.10076863 -0.10050428 -0.1900033   0.07836652]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.12683447 -0.05791841]]. Action = [[ 0.19859985  0.02589181  0.07157406 -0.41627574]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.11510163 -0.06265014]]. Action = [[-0.00796415 -0.07227743 -0.0395378   0.59110594]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.11157656 -0.0598188 ]]. Action = [[ 0.05798492  0.1469807  -0.10522731 -0.92174035]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.10278355 -0.05359599]]. Action = [[ 0.1637432   0.00329071  0.08510008 -0.34725398]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.09025902 -0.04989601]]. Action = [[-0.15790759  0.02724931 -0.17653933  0.7371752 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.08815379 -0.04191289]]. Action = [[ 0.15881765  0.09189498 -0.13282703 -0.56369483]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 922 is [True, False, False, False, True, False]
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(9.1814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[-0.08054918 -0.04489395]]. Action = [[ 0.04673421 -0.19807151  0.1412079   0.18726206]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.08004496 -0.0458479 ]]. Action = [[-0.15603161  0.17843306 -0.11054979 -0.7075975 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.07615498 -0.05040601]]. Action = [[ 0.22838008 -0.21585871  0.04347664 -0.5209804 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.06726927 -0.07243057]]. Action = [[-0.05265886 -0.16875975  0.01976979 -0.63206875]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 926 is [True, False, False, False, True, False]
Current timestep = 927. State = [[-0.06196731 -0.09906282]]. Action = [[ 0.15558326 -0.21944608 -0.16724402  0.9555048 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 927 is [True, False, False, False, True, False]
Scene graph at timestep 927 is [True, False, False, False, True, False]
State prediction error at timestep 927 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of -1
Current timestep = 928. State = [[-0.05510591 -0.11083639]]. Action = [[-0.2121986   0.17816293  0.0390532   0.05113852]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.05698432 -0.10380069]]. Action = [[ 0.01318422 -0.01160304 -0.10438353 -0.5172416 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 929 is [True, False, False, False, True, False]
Current timestep = 930. State = [[-0.06554722 -0.10050872]]. Action = [[-0.18430921  0.04333588 -0.02284896 -0.36332458]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 930 is [True, False, False, False, True, False]
Current timestep = 931. State = [[-0.07100924 -0.09536129]]. Action = [[ 0.22890505  0.03230765  0.05877921 -0.95639014]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 931 is [True, False, False, False, True, False]
Scene graph at timestep 931 is [True, False, False, False, True, False]
State prediction error at timestep 931 is tensor(5.6142e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 931 of 1
Current timestep = 932. State = [[-0.06535117 -0.07971077]]. Action = [[ 0.07487258  0.19237709 -0.23682609  0.23932111]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 932 is [True, False, False, False, True, False]
Current timestep = 933. State = [[-0.0510905  -0.07571286]]. Action = [[ 0.23419732 -0.1707258  -0.01241842 -0.84809786]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 933 is [True, False, False, False, True, False]
Current timestep = 934. State = [[-0.16876699  0.14143619]]. Action = [[ 0.0468114   0.14902395  0.22240537 -0.7506545 ]]. Reward = [100.]
Curr episode timestep = 34
Scene graph at timestep 934 is [True, False, False, False, True, False]
Current timestep = 935. State = [[-0.14106548  0.14601216]]. Action = [[ 0.24155825 -0.20384154 -0.24340871  0.21587789]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 935 is [True, False, False, False, False, True]
Current timestep = 936. State = [[-0.12701775  0.14287062]]. Action = [[-0.08819655  0.1201793   0.03456086  0.7270447 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 936 is [True, False, False, False, False, True]
Current timestep = 937. State = [[-0.13376756  0.14677565]]. Action = [[-0.24034308 -0.04849032 -0.04642347 -0.4967168 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 937 is [True, False, False, False, False, True]
Current timestep = 938. State = [[-0.14382008  0.15753625]]. Action = [[ 0.01156485  0.16005781 -0.0956395  -0.1672526 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 938 is [True, False, False, False, False, True]
Scene graph at timestep 938 is [True, False, False, False, False, True]
State prediction error at timestep 938 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of 1
Current timestep = 939. State = [[-0.14367041  0.1723451 ]]. Action = [[ 0.20401782  0.1151545   0.08731386 -0.61217546]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 939 is [True, False, False, False, False, True]
Scene graph at timestep 939 is [True, False, False, False, False, True]
State prediction error at timestep 939 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.12448458  0.18413167]]. Action = [[ 0.23297432  0.10965535 -0.1538268   0.4317422 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 940 is [True, False, False, False, False, True]
Current timestep = 941. State = [[-0.09428764  0.18957087]]. Action = [[ 0.2010946  -0.0659093  -0.19398127  0.2226764 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 941 is [True, False, False, False, False, True]
Current timestep = 942. State = [[-0.06357832  0.17553256]]. Action = [[ 0.21084917 -0.22675355  0.24343449 -0.28615737]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 942 is [True, False, False, False, False, True]
Current timestep = 943. State = [[-0.04394361  0.1587266 ]]. Action = [[-0.05750114 -0.07073948  0.07513687  0.92705023]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 943 is [True, False, False, False, False, True]
Current timestep = 944. State = [[-0.04448071  0.1623807 ]]. Action = [[-0.01111096  0.18889332  0.04807392 -0.8617132 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 944 is [False, True, False, False, False, True]
Current timestep = 945. State = [[-0.03918447  0.17658187]]. Action = [[ 0.17085415  0.12078777 -0.06358351 -0.8095567 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 945 is [False, True, False, False, False, True]
Current timestep = 946. State = [[-0.02220726  0.18240133]]. Action = [[-0.01596677 -0.10656703 -0.20128764  0.22259891]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 946 is [False, True, False, False, False, True]
Scene graph at timestep 946 is [False, True, False, False, False, True]
State prediction error at timestep 946 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 946 of 1
Current timestep = 947. State = [[-0.02091469  0.19137125]]. Action = [[ 0.05086419  0.22710925 -0.1762853   0.9133189 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 947 is [False, True, False, False, False, True]
Current timestep = 948. State = [[-0.00738348  0.20646031]]. Action = [[ 0.20099205  0.03588679 -0.01310065 -0.41925335]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 948 is [False, True, False, False, False, True]
Scene graph at timestep 948 is [False, True, False, False, False, True]
State prediction error at timestep 948 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of -1
Current timestep = 949. State = [[0.02641107 0.20388891]]. Action = [[ 0.12014303 -0.20755213 -0.16082548 -0.4031036 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 949 is [False, True, False, False, False, True]
Current timestep = 950. State = [[0.03444739 0.18209785]]. Action = [[-0.16061991 -0.20244335 -0.03947936 -0.12750053]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 950 is [False, True, False, False, False, True]
Current timestep = 951. State = [[0.03622216 0.17815469]]. Action = [[ 0.15973723  0.19611639 -0.16828546 -0.18072748]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 951 is [False, True, False, False, False, True]
Scene graph at timestep 951 is [False, True, False, False, False, True]
State prediction error at timestep 951 is tensor(3.4401e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[0.03702659 0.18368186]]. Action = [[ 0.20297146  0.1938895  -0.05333056 -0.62909156]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 952 is [False, True, False, False, False, True]
Current timestep = 953. State = [[0.03679911 0.19662511]]. Action = [[0.05246669 0.23590231 0.16026789 0.8725395 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 953 is [False, True, False, False, False, True]
Current timestep = 954. State = [[0.04017329 0.20522384]]. Action = [[-0.23492591 -0.20246759 -0.12900046  0.9491298 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 954 is [False, True, False, False, False, True]
Current timestep = 955. State = [[0.03977451 0.20067826]]. Action = [[0.2323142  0.2298803  0.21370137 0.2545923 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 955 is [False, True, False, False, False, True]
Current timestep = 956. State = [[0.04048777 0.19031198]]. Action = [[-0.06349576 -0.19471778  0.06340638 -0.8408889 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 956 is [False, True, False, False, False, True]
Scene graph at timestep 956 is [False, True, False, False, False, True]
State prediction error at timestep 956 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[0.03527862 0.17610532]]. Action = [[ 0.17284632  0.0829137  -0.06255081  0.83667946]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 957 is [False, True, False, False, False, True]
Current timestep = 958. State = [[0.03861233 0.18138906]]. Action = [[ 0.13228899  0.08617681 -0.06467769 -0.266626  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 958 is [False, True, False, False, False, True]
Current timestep = 959. State = [[0.04611951 0.18766665]]. Action = [[ 0.19812122  0.22211266 -0.1848193   0.79003644]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 959 is [False, True, False, False, False, True]
Scene graph at timestep 959 is [False, True, False, False, False, True]
State prediction error at timestep 959 is tensor(2.5247e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[0.04848714 0.1868106 ]]. Action = [[-0.14872015 -0.0879443  -0.08386642  0.80224776]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 960 is [False, True, False, False, False, True]
Current timestep = 961. State = [[0.04824038 0.18665354]]. Action = [[ 0.0198167   0.05145842  0.08368188 -0.61716163]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 961 is [False, True, False, False, False, True]
Current timestep = 962. State = [[0.04775528 0.17888522]]. Action = [[-0.1136966  -0.16975817 -0.02925543 -0.58913356]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 962 is [False, True, False, False, False, True]
Current timestep = 963. State = [[0.04550947 0.15879628]]. Action = [[-0.05530906 -0.20920843  0.19610983  0.8209864 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 963 is [False, True, False, False, False, True]
Scene graph at timestep 963 is [False, True, False, False, False, True]
State prediction error at timestep 963 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 963 of 1
Current timestep = 964. State = [[0.03539775 0.14011188]]. Action = [[-0.06396866  0.01630035  0.06861451  0.791451  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 964 is [False, True, False, False, False, True]
Current timestep = 965. State = [[0.03416073 0.14060004]]. Action = [[ 0.19643289  0.03739992 -0.09063922  0.49834764]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 965 is [False, True, False, False, False, True]
Current timestep = 966. State = [[0.03044881 0.14900146]]. Action = [[ 0.09553963  0.19745213 -0.22237845 -0.8336898 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 966 is [False, True, False, False, False, True]
Current timestep = 967. State = [[0.02468579 0.1651626 ]]. Action = [[-0.11844409  0.03889242  0.16546357  0.37798035]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 967 is [False, True, False, False, False, True]
Current timestep = 968. State = [[0.02180371 0.17177598]]. Action = [[ 0.24075481  0.18778911  0.06912243 -0.567899  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 968 is [False, True, False, False, False, True]
Scene graph at timestep 968 is [False, True, False, False, False, True]
State prediction error at timestep 968 is tensor(6.5571e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of -1
Current timestep = 969. State = [[0.02146016 0.16513528]]. Action = [[-0.01882619 -0.14360213  0.1678794  -0.2525434 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 969 is [False, True, False, False, False, True]
Current timestep = 970. State = [[0.0177964  0.16694836]]. Action = [[ 0.02955899  0.17280662 -0.1637855   0.499959  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 970 is [False, True, False, False, False, True]
Current timestep = 971. State = [[0.01089032 0.16587375]]. Action = [[-0.24304791 -0.16665684 -0.04405619  0.45792758]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 971 is [False, True, False, False, False, True]
Current timestep = 972. State = [[0.00340426 0.15522464]]. Action = [[ 0.2414962  -0.02547312  0.07606992 -0.8898861 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 972 is [False, True, False, False, False, True]
Current timestep = 973. State = [[0.00967504 0.15663977]]. Action = [[ 0.16102016  0.12216341  0.07271314 -0.11978209]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 973 is [False, True, False, False, False, True]
Current timestep = 974. State = [[0.01163133 0.16624704]]. Action = [[-0.12920776  0.06902665 -0.03522348  0.05684626]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 974 is [False, True, False, False, False, True]
Current timestep = 975. State = [[0.0100312 0.1769053]]. Action = [[ 0.11094779  0.13517714 -0.01923233  0.2755692 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 975 is [False, True, False, False, False, True]
Scene graph at timestep 975 is [False, True, False, False, False, True]
State prediction error at timestep 975 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of -1
Current timestep = 976. State = [[0.01957178 0.19284606]]. Action = [[0.20424664 0.13228652 0.15050685 0.55748725]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 976 is [False, True, False, False, False, True]
Scene graph at timestep 976 is [False, True, False, False, False, True]
State prediction error at timestep 976 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of -1
Current timestep = 977. State = [[0.03710705 0.20993808]]. Action = [[ 0.1939587   0.1426763  -0.20415422 -0.7104587 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 977 is [False, True, False, False, False, True]
Current timestep = 978. State = [[0.03988983 0.20949988]]. Action = [[ 0.09017837  0.00050008 -0.09206554  0.03542197]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 978 is [False, True, False, False, False, True]
Current timestep = 979. State = [[0.04535192 0.20751   ]]. Action = [[-0.14134644 -0.08389902 -0.19802783 -0.48985207]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 979 is [False, True, False, False, False, True]
Current timestep = 980. State = [[0.03985983 0.21646756]]. Action = [[-0.14914511  0.18035364 -0.22337073 -0.19724804]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 980 is [False, True, False, False, False, True]
Scene graph at timestep 980 is [False, True, False, False, False, True]
State prediction error at timestep 980 is tensor(3.9751e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of -1
Current timestep = 981. State = [[0.03373485 0.23315056]]. Action = [[ 0.13306868  0.11737853 -0.08407377 -0.01471865]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 981 is [False, True, False, False, False, True]
Current timestep = 982. State = [[0.03802164 0.2273835 ]]. Action = [[ 0.02130634 -0.20322125 -0.21472582  0.30035043]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 982 is [False, True, False, False, False, True]
Current timestep = 983. State = [[0.03948445 0.2163358 ]]. Action = [[-0.18302722 -0.06876054  0.09419435  0.67224765]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 983 is [False, True, False, False, False, True]
Current timestep = 984. State = [[0.03813935 0.21201605]]. Action = [[ 0.24046582 -0.05736727 -0.02301508 -0.3337493 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 984 is [False, True, False, False, False, True]
Current timestep = 985. State = [[0.0342448  0.19801861]]. Action = [[-0.15158585 -0.24681826 -0.00542487  0.07489872]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 985 is [False, True, False, False, False, True]
Current timestep = 986. State = [[0.02200822 0.18315788]]. Action = [[ 0.01346219  0.05442327  0.00533304 -0.5879005 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 986 is [False, True, False, False, False, True]
Current timestep = 987. State = [[0.0168568  0.19343214]]. Action = [[-0.04510155  0.17902681 -0.02829741 -0.20566261]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 987 is [False, True, False, False, False, True]
Current timestep = 988. State = [[0.00315765 0.2144535 ]]. Action = [[-0.22434777  0.11234042 -0.00635183 -0.20292097]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 988 is [False, True, False, False, False, True]
Scene graph at timestep 988 is [False, True, False, False, False, True]
State prediction error at timestep 988 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 988 of -1
Current timestep = 989. State = [[-0.0106676   0.21907194]]. Action = [[ 0.14511627 -0.14794052  0.09779114  0.4079386 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 989 is [False, True, False, False, False, True]
Current timestep = 990. State = [[-0.00424845  0.2180432 ]]. Action = [[ 0.21780401  0.1672731  -0.02449536 -0.1634326 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 990 is [False, True, False, False, False, True]
Current timestep = 991. State = [[0.00465542 0.21881486]]. Action = [[ 0.16816866 -0.03950989 -0.04881449 -0.8856325 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 991 is [False, True, False, False, False, True]
Current timestep = 992. State = [[0.02179766 0.22719198]]. Action = [[ 0.0811488   0.20790166 -0.16636959  0.27670646]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 992 is [False, True, False, False, False, True]
Scene graph at timestep 992 is [False, True, False, False, False, True]
State prediction error at timestep 992 is tensor(2.8874e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 992 of -1
Current timestep = 993. State = [[0.03290885 0.25088775]]. Action = [[0.06254274 0.13252014 0.09804392 0.09674537]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 993 is [False, True, False, False, False, True]
Scene graph at timestep 993 is [False, True, False, False, False, True]
State prediction error at timestep 993 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of -1
Current timestep = 994. State = [[0.04046484 0.26342046]]. Action = [[ 0.15270877 -0.02025647  0.19025254 -0.16987586]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 994 is [False, True, False, False, False, True]
Current timestep = 995. State = [[0.04302719 0.27162257]]. Action = [[ 0.11403525  0.16016167  0.11130923 -0.58102584]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 995 is [False, True, False, False, False, True]
Scene graph at timestep 995 is [False, True, False, False, False, True]
State prediction error at timestep 995 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of -1
Current timestep = 996. State = [[0.04895256 0.28305796]]. Action = [[-0.21083827 -0.04386966 -0.1668158  -0.04035848]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 996 is [False, True, False, False, False, True]
Scene graph at timestep 996 is [False, True, False, False, False, True]
State prediction error at timestep 996 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of -1
Current timestep = 997. State = [[0.05280625 0.2724423 ]]. Action = [[ 0.10811743 -0.21466042 -0.11855949 -0.39795387]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 997 is [False, True, False, False, False, True]
Current timestep = 998. State = [[0.05984095 0.25732598]]. Action = [[-0.05681947 -0.0844762  -0.00756352  0.96043646]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 998 is [False, False, True, False, False, True]
Current timestep = 999. State = [[0.06578568 0.24124566]]. Action = [[-0.00185172 -0.21128805  0.05032605  0.10739005]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 999 is [False, False, True, False, False, True]
Current timestep = 1000. State = [[0.06982621 0.22922882]]. Action = [[-0.08590001  0.01006007 -0.20336165 -0.1474008 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1000 is [False, False, True, False, False, True]
Current timestep = 1001. State = [[0.06835083 0.22882445]]. Action = [[0.21616653 0.17912996 0.16755694 0.17186236]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1001 is [False, False, True, False, False, True]
Current timestep = 1002. State = [[0.06835083 0.22882445]]. Action = [[ 0.02318063  0.12159115  0.04860145 -0.33228886]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1002 is [False, False, True, False, False, True]
Scene graph at timestep 1002 is [False, False, True, False, False, True]
State prediction error at timestep 1002 is tensor(3.5158e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of 1
Current timestep = 1003. State = [[0.06266546 0.24069317]]. Action = [[-0.08969867  0.21728352  0.0745067  -0.53656596]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1003 is [False, False, True, False, False, True]
Current timestep = 1004. State = [[0.05571419 0.25398168]]. Action = [[ 0.12762406  0.06183541 -0.17073971 -0.01739526]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1004 is [False, False, True, False, False, True]
Scene graph at timestep 1004 is [False, False, True, False, False, True]
State prediction error at timestep 1004 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of -1
Current timestep = 1005. State = [[0.05114188 0.2615783 ]]. Action = [[-0.08125858  0.05017823 -0.13845812 -0.8133079 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1005 is [False, False, True, False, False, True]
Current timestep = 1006. State = [[0.04811775 0.26643425]]. Action = [[ 0.11093262 -0.23679589 -0.0721823   0.20149946]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1006 is [False, False, True, False, False, True]
Current timestep = 1007. State = [[0.04762072 0.2672485 ]]. Action = [[ 0.20874184  0.10072917  0.17840257 -0.6999486 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1007 is [False, True, False, False, False, True]
Current timestep = 1008. State = [[0.04824093 0.26622584]]. Action = [[ 0.08355555 -0.01014908  0.0887771  -0.1332125 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1008 is [False, True, False, False, False, True]
Current timestep = 1009. State = [[0.04871172 0.26536936]]. Action = [[ 0.07010794  0.02935103 -0.00012562  0.01014745]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1009 is [False, True, False, False, False, True]
Current timestep = 1010. State = [[0.05079372 0.258963  ]]. Action = [[-0.19662091 -0.21841846 -0.0497271   0.72550607]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1010 is [False, True, False, False, False, True]
Current timestep = 1011. State = [[0.05284378 0.25345492]]. Action = [[ 0.22569251 -0.18261753 -0.18840116 -0.01915938]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1011 is [False, False, True, False, False, True]
Current timestep = 1012. State = [[0.05321783 0.25235525]]. Action = [[ 0.09194732  0.03638214  0.17897674 -0.7201736 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1012 is [False, False, True, False, False, True]
Current timestep = 1013. State = [[0.05206273 0.251935  ]]. Action = [[-0.11672077 -0.04639579  0.06028205 -0.1060878 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1013 is [False, False, True, False, False, True]
Current timestep = 1014. State = [[0.05138103 0.25177854]]. Action = [[ 0.10055515 -0.21101369 -0.05101182 -0.18048805]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1014 is [False, False, True, False, False, True]
Current timestep = 1015. State = [[0.05017356 0.24853303]]. Action = [[-0.13653435 -0.12720609 -0.22141515  0.532516  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1015 is [False, False, True, False, False, True]
Current timestep = 1016. State = [[0.04963925 0.24579418]]. Action = [[ 0.11676139 -0.24175464 -0.10800876  0.527946  ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1016 is [False, False, True, False, False, True]
Current timestep = 1017. State = [[0.04546219 0.25400087]]. Action = [[-0.04790679  0.18689647  0.07361943  0.53380036]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1017 is [False, True, False, False, False, True]
Current timestep = 1018. State = [[0.04022811 0.2633355 ]]. Action = [[ 0.15210146 -0.13959564 -0.05198994  0.41679144]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1018 is [False, True, False, False, False, True]
Current timestep = 1019. State = [[0.03672713 0.2707761 ]]. Action = [[ 0.11293462  0.16744548 -0.11241674 -0.8452451 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1019 is [False, True, False, False, False, True]
Current timestep = 1020. State = [[0.02896538 0.28452674]]. Action = [[-0.19403659  0.04352167 -0.24408421 -0.69342285]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1020 is [False, True, False, False, False, True]
Current timestep = 1021. State = [[0.01650395 0.30384442]]. Action = [[-0.1742529   0.06277919  0.15370935 -0.2641362 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1021 is [False, True, False, False, False, True]
Current timestep = 1022. State = [[0.01628947 0.29804185]]. Action = [[ 0.22523934 -0.16244191  0.22581786  0.83199525]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1022 is [False, True, False, False, False, True]
Current timestep = 1023. State = [[-0.00623163  0.25780222]]. Action = [[-0.11165059 -0.13956837  0.0156863  -0.77815926]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1023 is [False, True, False, False, False, True]
Current timestep = 1024. State = [[-0.03982799  0.24104257]]. Action = [[-0.18584579  0.07553732 -0.00053589  0.4205973 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1024 is [False, True, False, False, False, True]
Current timestep = 1025. State = [[-0.06545744  0.25259343]]. Action = [[-0.12100632  0.16422588  0.01131982  0.91776955]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1025 is [False, True, False, False, False, True]
Current timestep = 1026. State = [[-0.07968182  0.26925984]]. Action = [[ 0.03545126  0.09950551 -0.17794645  0.50905824]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1026 is [True, False, False, False, False, True]
Scene graph at timestep 1026 is [True, False, False, False, False, True]
State prediction error at timestep 1026 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[-0.08909217  0.27142376]]. Action = [[-0.1591102  -0.10059443  0.03943172  0.45633996]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1027 is [True, False, False, False, False, True]
Current timestep = 1028. State = [[-0.10355391  0.24988528]]. Action = [[-0.07652977 -0.24642275 -0.02966903  0.03703904]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1028 is [True, False, False, False, False, True]
Current timestep = 1029. State = [[-0.11489587  0.22841087]]. Action = [[ 0.02705631 -0.01264347  0.17660934  0.5589752 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1029 is [True, False, False, False, False, True]
Current timestep = 1030. State = [[-0.11934403  0.21532786]]. Action = [[-0.10744692 -0.16099091 -0.23116907 -0.69680285]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1030 is [True, False, False, False, False, True]
Scene graph at timestep 1030 is [True, False, False, False, False, True]
State prediction error at timestep 1030 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[-0.13196039  0.19384743]]. Action = [[-0.0169532  -0.04746236  0.21540362  0.90783906]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1031 is [True, False, False, False, False, True]
Scene graph at timestep 1031 is [True, False, False, False, False, True]
State prediction error at timestep 1031 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[-0.14210004  0.20021276]]. Action = [[-0.18529692  0.15060255 -0.19275747 -0.6286062 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1032 is [True, False, False, False, False, True]
Scene graph at timestep 1032 is [True, False, False, False, False, True]
State prediction error at timestep 1032 is tensor(6.4733e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1032 of -1
Current timestep = 1033. State = [[-0.16935045  0.20974216]]. Action = [[-0.1657886  -0.04146899  0.18065071  0.09796464]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1033 is [True, False, False, False, False, True]
Current timestep = 1034. State = [[-0.19806653  0.21548459]]. Action = [[-0.2305058   0.15499896 -0.06526464 -0.8111537 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1034 is [True, False, False, False, False, True]
Current timestep = 1035. State = [[-0.22647698  0.21917988]]. Action = [[-0.11937642 -0.0909126  -0.10808447 -0.60610306]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1035 is [True, False, False, False, False, True]
Current timestep = 1036. State = [[-0.24000289  0.21865651]]. Action = [[0.04474592 0.10733706 0.20748451 0.6756761 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1036 is [True, False, False, False, False, True]
Current timestep = 1037. State = [[-0.2368794   0.21936122]]. Action = [[ 0.16670191 -0.04083976 -0.17742063  0.349746  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1037 is [True, False, False, False, False, True]
Current timestep = 1038. State = [[-0.24090254  0.22957274]]. Action = [[-0.20364866  0.18379468 -0.15695262  0.20681763]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1038 is [True, False, False, False, False, True]
Current timestep = 1039. State = [[-0.2480151  0.2388337]]. Action = [[0.07887644 0.00394809 0.05325979 0.22966921]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1039 is [True, False, False, False, False, True]
Scene graph at timestep 1039 is [True, False, False, False, False, True]
State prediction error at timestep 1039 is tensor(6.9149e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of -1
Current timestep = 1040. State = [[-0.23754363  0.25000507]]. Action = [[ 0.24264362  0.19422734 -0.19263811  0.37795985]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1040 is [True, False, False, False, False, True]
Current timestep = 1041. State = [[-0.21774593  0.2536254 ]]. Action = [[ 0.13170469 -0.18667169 -0.18504213 -0.48640513]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1041 is [True, False, False, False, False, True]
Current timestep = 1042. State = [[-0.20253783  0.24973233]]. Action = [[ 0.09155491  0.11343953  0.08816022 -0.15249884]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1042 is [True, False, False, False, False, True]
Current timestep = 1043. State = [[-0.19613937  0.24533755]]. Action = [[-0.11884636 -0.23065282 -0.12296914 -0.14371991]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1043 is [True, False, False, False, False, True]
Current timestep = 1044. State = [[-0.19657819  0.24134779]]. Action = [[-0.09183216  0.07188168 -0.06052797  0.7620959 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1044 is [True, False, False, False, False, True]
Current timestep = 1045. State = [[-0.20525187  0.23912434]]. Action = [[-0.1398469  -0.06992289 -0.21335723 -0.14049625]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1045 is [True, False, False, False, False, True]
Current timestep = 1046. State = [[-0.21406654  0.22595006]]. Action = [[ 0.05320695 -0.12935658 -0.04870774 -0.24345225]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1046 is [True, False, False, False, False, True]
Current timestep = 1047. State = [[-0.21737528  0.22483346]]. Action = [[-0.05663487  0.12568963 -0.07763596 -0.94143355]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1047 is [True, False, False, False, False, True]
Current timestep = 1048. State = [[-0.21572241  0.22320738]]. Action = [[ 0.1042918  -0.10717848  0.12428498 -0.52853835]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1048 is [True, False, False, False, False, True]
Current timestep = 1049. State = [[-0.21495941  0.21641427]]. Action = [[-0.06036891 -0.05798155 -0.00490634 -0.96134114]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1049 is [True, False, False, False, False, True]
Current timestep = 1050. State = [[-0.21940301  0.19633013]]. Action = [[-0.06313355 -0.22902696 -0.1281578  -0.64000046]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1050 is [True, False, False, False, False, True]
Current timestep = 1051. State = [[-0.23033479  0.1902897 ]]. Action = [[-0.20058794  0.14413601 -0.2293073   0.16931033]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1051 is [True, False, False, False, False, True]
Current timestep = 1052. State = [[-0.25486705  0.18616578]]. Action = [[-0.16845672 -0.12292004  0.14532954  0.06930566]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1052 is [True, False, False, False, False, True]
Current timestep = 1053. State = [[-0.26889622  0.16398236]]. Action = [[ 0.04778516 -0.24173175 -0.20846663 -0.41365665]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1053 is [True, False, False, False, False, True]
Current timestep = 1054. State = [[-0.26975116  0.14590408]]. Action = [[ 0.06783512  0.00429627  0.19235647 -0.28214246]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1054 is [True, False, False, False, False, True]
Current timestep = 1055. State = [[-0.2631306   0.13445096]]. Action = [[ 0.04013467 -0.12492025  0.2226603   0.39885545]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1055 is [True, False, False, False, False, True]
Current timestep = 1056. State = [[-0.25013     0.11720335]]. Action = [[ 0.24256092 -0.09427428  0.04622406 -0.26922572]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1056 is [True, False, False, False, False, True]
Current timestep = 1057. State = [[-0.2402095   0.10346902]]. Action = [[-0.11502174 -0.05211687  0.01201573  0.56336784]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Scene graph at timestep 1057 is [True, False, False, False, True, False]
State prediction error at timestep 1057 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of 1
Current timestep = 1058. State = [[-0.24869241  0.08989722]]. Action = [[-0.11147094 -0.14055797  0.01715147  0.19254613]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Scene graph at timestep 1058 is [True, False, False, False, True, False]
State prediction error at timestep 1058 is tensor(5.8742e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1058 of 1
Current timestep = 1059. State = [[-0.24605274  0.08692214]]. Action = [[ 0.23629618  0.20522717  0.19863272 -0.2778167 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Scene graph at timestep 1059 is [True, False, False, False, True, False]
State prediction error at timestep 1059 is tensor(5.8342e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of 1
Current timestep = 1060. State = [[-0.23706502  0.08714136]]. Action = [[-0.10684612 -0.22961056 -0.14969648 -0.29700994]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Current timestep = 1061. State = [[-0.2051511 -0.1526385]]. Action = [[ 0.02606675  0.12990269  0.05273631 -0.2018072 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Scene graph at timestep 1061 is [True, False, False, True, False, False]
State prediction error at timestep 1061 is tensor(0.0286, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.18783122 -0.1705587 ]]. Action = [[ 0.21996427 -0.02911115 -0.18230225  0.09142256]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1062 is [True, False, False, True, False, False]
Current timestep = 1063. State = [[-0.16370279 -0.1636166 ]]. Action = [[ 0.12439919  0.20030943 -0.11653212 -0.9090475 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1063 is [True, False, False, True, False, False]
Current timestep = 1064. State = [[-0.14949664 -0.15950173]]. Action = [[ 0.03040624 -0.10924579  0.06254628 -0.78017586]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1064 is [True, False, False, True, False, False]
Current timestep = 1065. State = [[-0.14067589 -0.15139422]]. Action = [[ 0.1440444   0.1876741  -0.22840945  0.692652  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1065 is [True, False, False, True, False, False]
Current timestep = 1066. State = [[-0.11661903 -0.13935159]]. Action = [[ 0.23454225  0.04390925 -0.10145479 -0.4071529 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1066 is [True, False, False, True, False, False]
Current timestep = 1067. State = [[-0.08410172 -0.12544471]]. Action = [[ 0.2124184   0.15354985  0.0316357  -0.7898277 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1067 is [True, False, False, True, False, False]
Current timestep = 1068. State = [[-0.06033974 -0.11528649]]. Action = [[ 0.05789539 -0.00699954 -0.13442148  0.9335309 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1068 is [True, False, False, True, False, False]
Current timestep = 1069. State = [[-0.04621289 -0.10303009]]. Action = [[ 0.09403133  0.16241744 -0.22334161 -0.3406406 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1069 is [True, False, False, False, True, False]
Current timestep = 1070. State = [[-0.1481809  0.1394983]]. Action = [[ 0.16186911 -0.15847595 -0.22230099  0.05135095]]. Reward = [100.]
Curr episode timestep = 8
Scene graph at timestep 1070 is [False, True, False, False, True, False]
Current timestep = 1071. State = [[-0.11793055  0.15061583]]. Action = [[ 0.21547228 -0.10785091  0.02671695  0.01504862]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1071 is [True, False, False, False, False, True]
Current timestep = 1072. State = [[-0.10192087  0.152293  ]]. Action = [[-0.00202684  0.10980058  0.01562306 -0.10535663]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1072 is [True, False, False, False, False, True]
Current timestep = 1073. State = [[-0.09744719  0.1473761 ]]. Action = [[-0.08854854 -0.17807181  0.16497213  0.6161039 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1073 is [True, False, False, False, False, True]
Current timestep = 1074. State = [[-0.09403594  0.12510997]]. Action = [[ 0.0289298  -0.22951274  0.02660039  0.08158624]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1074 is [True, False, False, False, False, True]
Current timestep = 1075. State = [[-0.08907773  0.10287987]]. Action = [[ 0.1446372  -0.08159143 -0.03639179  0.60590994]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1075 is [True, False, False, False, False, True]
Scene graph at timestep 1075 is [True, False, False, False, True, False]
State prediction error at timestep 1075 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of 1
Current timestep = 1076. State = [[-0.0847583   0.08942676]]. Action = [[-0.1040865  -0.06780688 -0.00324117  0.27352273]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1076 is [True, False, False, False, True, False]
Current timestep = 1077. State = [[-0.09302443  0.08986047]]. Action = [[-0.20364437  0.06998497 -0.19239338  0.47741652]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1077 is [True, False, False, False, True, False]
Current timestep = 1078. State = [[-0.10079606  0.10499177]]. Action = [[ 0.12075609  0.21139687 -0.08546239  0.90941787]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1078 is [True, False, False, False, True, False]
Scene graph at timestep 1078 is [True, False, False, False, True, False]
State prediction error at timestep 1078 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1078 of -1
Current timestep = 1079. State = [[-0.10653666  0.12470438]]. Action = [[ 0.00335595  0.10019252 -0.20528898 -0.08906913]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1079 is [True, False, False, False, True, False]
Current timestep = 1080. State = [[-0.10318022  0.12779817]]. Action = [[ 0.09145114 -0.07277757  0.18614358 -0.62141156]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1080 is [True, False, False, False, True, False]
Current timestep = 1081. State = [[-0.10067427  0.12838209]]. Action = [[-0.0829034   0.03215119 -0.169866   -0.03254014]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1081 is [True, False, False, False, False, True]
Current timestep = 1082. State = [[-0.1003001   0.12577657]]. Action = [[ 0.01133168 -0.07648179  0.11407983  0.5006558 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1082 is [True, False, False, False, False, True]
Current timestep = 1083. State = [[-0.0957743   0.10815109]]. Action = [[ 0.09394142 -0.2136405  -0.05364676  0.4416169 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1083 is [True, False, False, False, False, True]
Current timestep = 1084. State = [[-0.08982218  0.08840501]]. Action = [[-0.00642303 -0.10015401 -0.0886109   0.11584067]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1084 is [True, False, False, False, True, False]
Current timestep = 1085. State = [[-0.0845216   0.07051499]]. Action = [[ 0.10716099 -0.12114602  0.10001653 -0.74304444]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1085 is [True, False, False, False, True, False]
Scene graph at timestep 1085 is [True, False, False, False, True, False]
State prediction error at timestep 1085 is tensor(7.0219e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[-0.07733829  0.04584015]]. Action = [[ 0.02284849 -0.20464016 -0.24074724 -0.06130522]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1086 is [True, False, False, False, True, False]
Scene graph at timestep 1086 is [True, False, False, False, True, False]
State prediction error at timestep 1086 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1086 of 1
Current timestep = 1087. State = [[-0.07447456  0.03479609]]. Action = [[-0.00069255  0.12475273 -0.08505434  0.13906062]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Current timestep = 1088. State = [[-0.07507238  0.03806513]]. Action = [[-0.09125814 -0.04322359  0.15925655 -0.12591612]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Current timestep = 1089. State = [[-0.07085381  0.04888254]]. Action = [[ 0.2226618   0.20312482 -0.14264117  0.36395514]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Current timestep = 1090. State = [[-0.05263514  0.05402436]]. Action = [[ 0.15700263 -0.1268279  -0.01337408 -0.44085252]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Current timestep = 1091. State = [[-0.04494875  0.06219782]]. Action = [[-0.1387124   0.19937918 -0.18287443  0.56758356]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Current timestep = 1092. State = [[-0.04365512  0.07876099]]. Action = [[0.14878905 0.10158321 0.09661072 0.83951235]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1092 is [False, True, False, False, True, False]
Current timestep = 1093. State = [[-0.14826891  0.04745456]]. Action = [[ 0.20295984 -0.07391717 -0.17143749 -0.64370245]]. Reward = [100.]
Curr episode timestep = 22
Scene graph at timestep 1093 is [False, True, False, False, True, False]
Scene graph at timestep 1093 is [True, False, False, False, True, False]
State prediction error at timestep 1093 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 1
Current timestep = 1094. State = [[-0.11964725  0.04800707]]. Action = [[ 0.24475086 -0.15164971 -0.23575605  0.04842055]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1094 is [True, False, False, False, True, False]
Current timestep = 1095. State = [[-0.09623244  0.04093231]]. Action = [[ 0.05265272  0.00487098  0.14328203 -0.58398   ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Current timestep = 1096. State = [[-0.07933416  0.0454498 ]]. Action = [[ 0.2334984   0.14276189 -0.20829993 -0.9030691 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Current timestep = 1097. State = [[-0.06391067  0.04225545]]. Action = [[-0.20546998 -0.18846804  0.11601019 -0.34690273]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Scene graph at timestep 1097 is [True, False, False, False, True, False]
State prediction error at timestep 1097 is tensor(7.3078e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of 1
Current timestep = 1098. State = [[-0.06344109  0.04088055]]. Action = [[ 0.13538438  0.160568   -0.11923896 -0.9320268 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Scene graph at timestep 1098 is [True, False, False, False, True, False]
State prediction error at timestep 1098 is tensor(2.3080e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1098 of 1
Current timestep = 1099. State = [[-0.06772275  0.06451626]]. Action = [[-0.19449078  0.21101698  0.05735388 -0.211618  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Scene graph at timestep 1099 is [True, False, False, False, True, False]
State prediction error at timestep 1099 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1099 of -1
Current timestep = 1100. State = [[-0.08129372  0.09377071]]. Action = [[-0.1333593   0.13595617  0.00527483 -0.9798495 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Current timestep = 1101. State = [[-0.09109277  0.11419388]]. Action = [[ 0.01720262  0.14873344 -0.04890595  0.00479078]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Current timestep = 1102. State = [[-0.10129173  0.1358989 ]]. Action = [[-0.17382817  0.16355267 -0.18639933  0.06063509]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Current timestep = 1103. State = [[-0.11529746  0.15145949]]. Action = [[-0.12986207 -0.03612286  0.06469876 -0.22145516]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1103 is [True, False, False, False, False, True]
Current timestep = 1104. State = [[-0.1192515   0.15114014]]. Action = [[ 0.16465974 -0.00187615  0.228176   -0.47086167]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1104 is [True, False, False, False, False, True]
Current timestep = 1105. State = [[-0.12557966  0.16048934]]. Action = [[-0.23826514  0.14900404 -0.06846061  0.8043659 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1105 is [True, False, False, False, False, True]
Current timestep = 1106. State = [[-0.13982472  0.18528935]]. Action = [[0.03131983 0.22698453 0.18938857 0.5791817 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1106 is [True, False, False, False, False, True]
Current timestep = 1107. State = [[-0.14940664  0.20122123]]. Action = [[-0.11977606 -0.00867942  0.05931282  0.23211789]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1107 is [True, False, False, False, False, True]
Current timestep = 1108. State = [[-0.16073073  0.21166825]]. Action = [[-0.14927617  0.0618107  -0.22654395  0.17299259]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1108 is [True, False, False, False, False, True]
Current timestep = 1109. State = [[-0.17020619  0.2260968 ]]. Action = [[ 0.13875782  0.20227236  0.05691448 -0.6494686 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1109 is [True, False, False, False, False, True]
Scene graph at timestep 1109 is [True, False, False, False, False, True]
State prediction error at timestep 1109 is tensor(9.2321e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of -1
Current timestep = 1110. State = [[-0.1710989  0.229377 ]]. Action = [[-0.05823821 -0.17816325 -0.01676959  0.627087  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1110 is [True, False, False, False, False, True]
Current timestep = 1111. State = [[-0.16257845  0.20828976]]. Action = [[ 0.14529294 -0.18961139 -0.20506173 -0.07401162]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1111 is [True, False, False, False, False, True]
Scene graph at timestep 1111 is [True, False, False, False, False, True]
State prediction error at timestep 1111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of 1
Current timestep = 1112. State = [[-0.15574498  0.19327447]]. Action = [[ 0.02440676  0.03509653  0.17054665 -0.31023347]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1112 is [True, False, False, False, False, True]
Scene graph at timestep 1112 is [True, False, False, False, False, True]
State prediction error at timestep 1112 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1112 of 1
Current timestep = 1113. State = [[-0.16308825  0.20664652]]. Action = [[-0.1855228   0.20825547  0.11250597  0.78079414]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1113 is [True, False, False, False, False, True]
Scene graph at timestep 1113 is [True, False, False, False, False, True]
State prediction error at timestep 1113 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of -1
Current timestep = 1114. State = [[-0.16867371  0.22609158]]. Action = [[ 0.20061857  0.07628828 -0.15542838 -0.11316556]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1114 is [True, False, False, False, False, True]
Scene graph at timestep 1114 is [True, False, False, False, False, True]
State prediction error at timestep 1114 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of -1
Current timestep = 1115. State = [[-0.16517438  0.23971485]]. Action = [[-0.080293    0.16364202  0.02908677 -0.56300086]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1115 is [True, False, False, False, False, True]
Scene graph at timestep 1115 is [True, False, False, False, False, True]
State prediction error at timestep 1115 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1115 of -1
Current timestep = 1116. State = [[-0.16573772  0.2611383 ]]. Action = [[ 0.14651522  0.1939987  -0.16741183 -0.43071806]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1116 is [True, False, False, False, False, True]
Scene graph at timestep 1116 is [True, False, False, False, False, True]
State prediction error at timestep 1116 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of -1
Current timestep = 1117. State = [[-0.15121427  0.28524676]]. Action = [[-0.06117171  0.07649937  0.02791929 -0.8927784 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1117 is [True, False, False, False, False, True]
Current timestep = 1118. State = [[-0.15229242  0.29051587]]. Action = [[ 0.10861349 -0.00904697  0.09185421 -0.72103006]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1118 is [True, False, False, False, False, True]
Current timestep = 1119. State = [[-0.14766735  0.29447103]]. Action = [[ 0.00085261  0.06581038 -0.2083793   0.8119625 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1119 is [True, False, False, False, False, True]
Scene graph at timestep 1119 is [True, False, False, False, False, True]
State prediction error at timestep 1119 is tensor(7.1419e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of -1
Current timestep = 1120. State = [[-0.1410358   0.30020526]]. Action = [[ 0.16081461  0.11379516 -0.15083274 -0.18799233]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1120 is [True, False, False, False, False, True]
Current timestep = 1121. State = [[-0.1410358   0.30020526]]. Action = [[-0.16919452  0.130876   -0.00121552  0.36435235]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1121 is [True, False, False, False, False, True]
Current timestep = 1122. State = [[-0.14108239  0.3002639 ]]. Action = [[ 0.22805184  0.04866365 -0.19023353  0.27084303]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1122 is [True, False, False, False, False, True]
Current timestep = 1123. State = [[-0.13579956  0.29556176]]. Action = [[ 0.13781965 -0.08168587 -0.12861797  0.5230212 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1123 is [True, False, False, False, False, True]
Current timestep = 1124. State = [[-0.12574665  0.29422417]]. Action = [[0.10039419 0.19675556 0.23650992 0.13480258]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1124 is [True, False, False, False, False, True]
Current timestep = 1125. State = [[-0.12055926  0.29397076]]. Action = [[ 0.00241572  0.06638303  0.23608217 -0.17363364]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1125 is [True, False, False, False, False, True]
Current timestep = 1126. State = [[-0.11484116  0.28918365]]. Action = [[ 0.13213244 -0.04828924 -0.08981335 -0.4809981 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1126 is [True, False, False, False, False, True]
Scene graph at timestep 1126 is [True, False, False, False, False, True]
State prediction error at timestep 1126 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1126 of 1
Current timestep = 1127. State = [[-0.10257227  0.28954408]]. Action = [[-0.08902176  0.11183959  0.01270223  0.03920043]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1127 is [True, False, False, False, False, True]
Current timestep = 1128. State = [[-0.09182516  0.27711135]]. Action = [[ 0.22015154 -0.16657001 -0.13813794  0.3868128 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1128 is [True, False, False, False, False, True]
Current timestep = 1129. State = [[-0.07388947  0.26733503]]. Action = [[-0.09943682  0.24685958  0.00639316  0.44195354]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1129 is [True, False, False, False, False, True]
Scene graph at timestep 1129 is [True, False, False, False, False, True]
State prediction error at timestep 1129 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[-0.064794    0.25234267]]. Action = [[ 0.10936499 -0.20996487  0.00231576 -0.52064526]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1130 is [True, False, False, False, False, True]
Scene graph at timestep 1130 is [True, False, False, False, False, True]
State prediction error at timestep 1130 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of 1
Current timestep = 1131. State = [[-0.04857977  0.24009733]]. Action = [[ 0.18358976  0.11286241 -0.06626225 -0.18391287]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1131 is [True, False, False, False, False, True]
Current timestep = 1132. State = [[-0.03929773  0.25670218]]. Action = [[-0.18553093  0.11402214 -0.1597818  -0.4199096 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1132 is [False, True, False, False, False, True]
Current timestep = 1133. State = [[-0.04225169  0.2749896 ]]. Action = [[0.13782865 0.21122271 0.22292861 0.45192528]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1133 is [False, True, False, False, False, True]
Scene graph at timestep 1133 is [False, True, False, False, False, True]
State prediction error at timestep 1133 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of -1
Current timestep = 1134. State = [[-0.02549458  0.29548115]]. Action = [[0.08865032 0.00845918 0.07336584 0.85717463]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1134 is [False, True, False, False, False, True]
Current timestep = 1135. State = [[-0.01996897  0.28972137]]. Action = [[-0.2297011  -0.23587169 -0.10481468  0.12763548]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1135 is [False, True, False, False, False, True]
Current timestep = 1136. State = [[-0.01868356  0.28057876]]. Action = [[ 0.10030404  0.22225931  0.21543342 -0.80618924]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1136 is [False, True, False, False, False, True]
Scene graph at timestep 1136 is [False, True, False, False, False, True]
State prediction error at timestep 1136 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of 1
Current timestep = 1137. State = [[-0.01664292  0.27601546]]. Action = [[ 0.15432435  0.00602713 -0.06701213  0.7300457 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1137 is [False, True, False, False, False, True]
Current timestep = 1138. State = [[-0.01608224  0.2755158 ]]. Action = [[ 4.8989773e-02  5.4704219e-02 -3.3487380e-04 -6.1350918e-01]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1138 is [False, True, False, False, False, True]
Current timestep = 1139. State = [[-0.0184704   0.28168365]]. Action = [[-0.02922203  0.10511836 -0.04614434  0.5122138 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1139 is [False, True, False, False, False, True]
Current timestep = 1140. State = [[-0.01840643  0.28121668]]. Action = [[-0.1627822  -0.18781596 -0.14166352  0.09910786]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1140 is [False, True, False, False, False, True]
Current timestep = 1141. State = [[-0.01980933  0.26364854]]. Action = [[-0.10750961 -0.23410644 -0.0693993  -0.5388707 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1141 is [False, True, False, False, False, True]
Current timestep = 1142. State = [[-0.02819545  0.24784207]]. Action = [[-0.011493    0.03082877  0.20301485 -0.42278528]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1142 is [False, True, False, False, False, True]
Current timestep = 1143. State = [[-0.03451449  0.25553232]]. Action = [[-0.0044608   0.17473751  0.21420175  0.08904123]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1143 is [False, True, False, False, False, True]
Current timestep = 1144. State = [[-0.04485552  0.26706126]]. Action = [[-0.15870626 -0.01555961 -0.03993593  0.5518974 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1144 is [False, True, False, False, False, True]
Current timestep = 1145. State = [[-0.05082938  0.27308372]]. Action = [[ 0.17301792  0.12018645 -0.09286112 -0.5868847 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1145 is [False, True, False, False, False, True]
Current timestep = 1146. State = [[-0.05611723  0.2836384 ]]. Action = [[-0.16270132  0.07554805 -0.0035201  -0.08375847]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1146 is [True, False, False, False, False, True]
Scene graph at timestep 1146 is [True, False, False, False, False, True]
State prediction error at timestep 1146 is tensor(3.7401e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1146 of -1
Current timestep = 1147. State = [[-0.06549981  0.2967986 ]]. Action = [[-0.10409707  0.00479627 -0.24689712  0.3386451 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1147 is [True, False, False, False, False, True]
Scene graph at timestep 1147 is [True, False, False, False, False, True]
State prediction error at timestep 1147 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1147 of -1
Current timestep = 1148. State = [[-0.0611545  0.2875347]]. Action = [[ 0.20774588 -0.20241566 -0.0506067   0.02425265]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1148 is [True, False, False, False, False, True]
Current timestep = 1149. State = [[-0.04640429  0.26045507]]. Action = [[ 0.17373955 -0.20845462 -0.17460288  0.30255055]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1149 is [True, False, False, False, False, True]
Current timestep = 1150. State = [[-0.03886282  0.23963964]]. Action = [[-0.19091482 -0.0424038   0.1099624   0.68569946]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1150 is [False, True, False, False, False, True]
Current timestep = 1151. State = [[-0.04082417  0.22767586]]. Action = [[ 0.06247115 -0.08940163 -0.12064619 -0.8215617 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1151 is [False, True, False, False, False, True]
Scene graph at timestep 1151 is [False, True, False, False, False, True]
State prediction error at timestep 1151 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of 1
Current timestep = 1152. State = [[-0.03349327  0.21367393]]. Action = [[ 0.17099768 -0.09880164 -0.02137823 -0.7635518 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1152 is [False, True, False, False, False, True]
Current timestep = 1153. State = [[-0.0289471   0.20482653]]. Action = [[-0.15489048  0.00052771 -0.08320808 -0.06448168]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1153 is [False, True, False, False, False, True]
Current timestep = 1154. State = [[-0.02499389  0.19739442]]. Action = [[ 0.19425368 -0.09170106 -0.22979441 -0.16133606]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1154 is [False, True, False, False, False, True]
Current timestep = 1155. State = [[-0.01705437  0.19261482]]. Action = [[0.1694726  0.11089277 0.1681673  0.6553372 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1155 is [False, True, False, False, False, True]
Current timestep = 1156. State = [[-0.00935279  0.18976106]]. Action = [[ 0.00261185 -0.1116814   0.16880143 -0.94864094]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1156 is [False, True, False, False, False, True]
Current timestep = 1157. State = [[-0.00983443  0.1780945 ]]. Action = [[-0.21090908 -0.11200932 -0.18709822  0.0272913 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1157 is [False, True, False, False, False, True]
Current timestep = 1158. State = [[-0.00511023  0.15933642]]. Action = [[ 0.17349762 -0.19705527  0.18977606  0.75643206]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1158 is [False, True, False, False, False, True]
Scene graph at timestep 1158 is [False, True, False, False, False, True]
State prediction error at timestep 1158 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of 1
Current timestep = 1159. State = [[0.00655286 0.12870936]]. Action = [[ 0.09729892 -0.21454453  0.22301584 -0.11490852]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1159 is [False, True, False, False, False, True]
Current timestep = 1160. State = [[-0.2519913  0.1608207]]. Action = [[-0.08851543  0.21843415  0.03797197 -0.65227926]]. Reward = [100.]
Curr episode timestep = 66
Scene graph at timestep 1160 is [False, True, False, False, False, True]
Scene graph at timestep 1160 is [True, False, False, False, False, True]
State prediction error at timestep 1160 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of -1
Current timestep = 1161. State = [[-0.24787712  0.17973916]]. Action = [[-0.2150278   0.07766095 -0.14693776  0.03773093]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1161 is [True, False, False, False, False, True]
Current timestep = 1162. State = [[-0.2429442  0.1678538]]. Action = [[ 0.06546471 -0.21980062  0.05577332  0.9127619 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1162 is [True, False, False, False, False, True]
Current timestep = 1163. State = [[-0.23869996  0.15139796]]. Action = [[-0.02677584 -0.07262799 -0.11882374  0.6605694 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1163 is [True, False, False, False, False, True]
Current timestep = 1164. State = [[-0.23701479  0.14379294]]. Action = [[ 0.03822771 -0.00934908 -0.03613672 -0.27110088]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1164 is [True, False, False, False, False, True]
Scene graph at timestep 1164 is [True, False, False, False, False, True]
State prediction error at timestep 1164 is tensor(4.9614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1164 of 1
Current timestep = 1165. State = [[-0.24015173  0.1277822 ]]. Action = [[-0.1888358  -0.23996867 -0.23209539  0.63302016]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1165 is [True, False, False, False, False, True]
Current timestep = 1166. State = [[-0.239838    0.11735043]]. Action = [[0.18161085 0.10549313 0.10355169 0.8974372 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1166 is [True, False, False, False, False, True]
Current timestep = 1167. State = [[-0.239044    0.12112053]]. Action = [[-0.05996382  0.04940924  0.06608054 -0.7829137 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Current timestep = 1168. State = [[-0.23709482  0.11387564]]. Action = [[-6.1444938e-04 -1.9773217e-01  5.9067637e-02  9.1266918e-01]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1168 is [True, False, False, False, True, False]
Scene graph at timestep 1168 is [True, False, False, False, True, False]
State prediction error at timestep 1168 is tensor(1.5926e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[-0.23420194  0.0980226 ]]. Action = [[ 0.0217216  -0.07489827  0.18438914  0.91738164]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Current timestep = 1170. State = [[-0.23495717  0.08778028]]. Action = [[-0.068147   -0.10697415  0.2247017  -0.07528585]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Current timestep = 1171. State = [[-0.23613054  0.08235314]]. Action = [[0.05912066 0.07347888 0.03378171 0.11204362]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Current timestep = 1172. State = [[-0.22798504  0.08530491]]. Action = [[ 0.20019448  0.02442735  0.1003055  -0.41510904]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Scene graph at timestep 1172 is [True, False, False, False, True, False]
State prediction error at timestep 1172 is tensor(2.9864e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.21670148  0.09876652]]. Action = [[-0.04445641  0.18321985  0.1317198  -0.40091115]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of -1
Current timestep = 1174. State = [[-0.21811238  0.11794112]]. Action = [[ 0.05958685  0.14084852 -0.12639247  0.38390553]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Current timestep = 1175. State = [[-0.21518704  0.12184399]]. Action = [[-0.05089492 -0.12010643  0.11105418 -0.48332113]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Scene graph at timestep 1175 is [True, False, False, False, True, False]
State prediction error at timestep 1175 is tensor(2.6662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.21518359  0.10716424]]. Action = [[-0.07565176 -0.20551634 -0.1429069   0.7617941 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Current timestep = 1177. State = [[-0.2228167   0.08368191]]. Action = [[-0.16585237 -0.19031227  0.20407236  0.7698839 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Current timestep = 1178. State = [[-0.24095625  0.05632057]]. Action = [[-0.22377789 -0.19169953  0.23021042 -0.02004665]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Current timestep = 1179. State = [[-0.2644743   0.03332596]]. Action = [[-0.06353898 -0.10612044  0.22935021  0.8341608 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Scene graph at timestep 1179 is [True, False, False, False, True, False]
State prediction error at timestep 1179 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1179 of -1
Current timestep = 1180. State = [[-0.26542246  0.01206998]]. Action = [[ 0.20837009 -0.13694362 -0.19468176  0.77368236]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Current timestep = 1181. State = [[-0.25764915  0.00139715]]. Action = [[-0.14570561 -0.11238481  0.08995169 -0.5342501 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(3.2127e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of -1
Current timestep = 1182. State = [[-0.25530928  0.00039628]]. Action = [[ 0.0516434   0.06843579 -0.11644119 -0.38225245]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Current timestep = 1183. State = [[-0.24342449  0.00939665]]. Action = [[ 0.21175256  0.09955069 -0.0542143   0.41957414]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Current timestep = 1184. State = [[-0.22175705  0.02846594]]. Action = [[ 0.13706493  0.21517038  0.17953128 -0.61908966]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.1995547   0.05921993]]. Action = [[ 0.19834563  0.23930103  0.10792089 -0.7668527 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Current timestep = 1186. State = [[-0.17421551  0.07808162]]. Action = [[ 1.7481309e-01 -6.3836575e-04  1.2644380e-01  6.6834402e-01]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.15119535  0.07001288]]. Action = [[ 0.10008693 -0.22159122  0.08697471 -0.4007709 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Current timestep = 1188. State = [[-0.13942641  0.06816859]]. Action = [[ 0.04021877  0.18536377  0.03180298 -0.70289934]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Current timestep = 1189. State = [[-0.12383743  0.08819981]]. Action = [[ 0.23191914  0.2114191  -0.21256717 -0.00189668]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Scene graph at timestep 1189 is [True, False, False, False, True, False]
State prediction error at timestep 1189 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1189 of 1
Current timestep = 1190. State = [[-0.10567868  0.12282701]]. Action = [[-0.19233961  0.24238321  0.03861123 -0.7018545 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Scene graph at timestep 1190 is [True, False, False, False, True, False]
State prediction error at timestep 1190 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1190 of -1
Current timestep = 1191. State = [[-0.12178497  0.13635212]]. Action = [[-0.20496313 -0.17143945 -0.24058466  0.2123903 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Current timestep = 1192. State = [[-0.12344476  0.12570806]]. Action = [[ 0.2476848  -0.01475768 -0.04804227 -0.17773318]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1192 is [True, False, False, False, False, True]
Current timestep = 1193. State = [[-0.11821736  0.11424015]]. Action = [[-0.02943704 -0.13948345 -0.21102016 -0.89304584]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1193 is [True, False, False, False, False, True]
Current timestep = 1194. State = [[-0.11410731  0.09892065]]. Action = [[ 0.05477133 -0.09798765  0.08113939 -0.8734249 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Current timestep = 1195. State = [[-0.11923733  0.10472789]]. Action = [[-0.242314    0.23079914 -0.21772824 -0.8610816 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Current timestep = 1196. State = [[-0.13594179  0.11068621]]. Action = [[-0.22143595 -0.15110528  0.10144484  0.5963317 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Current timestep = 1197. State = [[-0.1482271   0.10564253]]. Action = [[ 0.2074255   0.07325947 -0.14513455  0.23245037]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Current timestep = 1198. State = [[-0.13880607  0.10714969]]. Action = [[ 0.19315785  0.00297445 -0.11878598  0.713411  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Scene graph at timestep 1198 is [True, False, False, False, True, False]
State prediction error at timestep 1198 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1198 of 1
Current timestep = 1199. State = [[-0.13131873  0.12312935]]. Action = [[-0.23320264  0.22285101  0.23368055 -0.68537176]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Current timestep = 1200. State = [[-0.13820155  0.13714659]]. Action = [[ 0.06536853 -0.01153784 -0.05981652 -0.82681227]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1200 is [True, False, False, False, True, False]
Scene graph at timestep 1200 is [True, False, False, False, False, True]
State prediction error at timestep 1200 is tensor(8.5240e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of -1
Current timestep = 1201. State = [[-0.13196382  0.13992988]]. Action = [[ 0.20778817  0.04044023  0.19842839 -0.47988838]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1201 is [True, False, False, False, False, True]
Current timestep = 1202. State = [[-0.11786298  0.13829075]]. Action = [[ 0.09972009 -0.0929748   0.2298736  -0.849696  ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1202 is [True, False, False, False, False, True]
Current timestep = 1203. State = [[-0.10793073  0.13450302]]. Action = [[ 0.01130402 -0.01202901  0.22546399  0.8927138 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1203 is [True, False, False, False, False, True]
Scene graph at timestep 1203 is [True, False, False, False, False, True]
State prediction error at timestep 1203 is tensor(4.0503e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1203 of 1
Current timestep = 1204. State = [[-0.1061828   0.12284575]]. Action = [[-0.09567513 -0.16367164  0.10614112 -0.01276118]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1204 is [True, False, False, False, False, True]
Current timestep = 1205. State = [[-0.10992688  0.11098714]]. Action = [[-0.14947394 -0.03671046 -0.11859128 -0.1341722 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1205 is [True, False, False, False, True, False]
Current timestep = 1206. State = [[-0.10847611  0.09796171]]. Action = [[ 0.15512782 -0.15565626 -0.19507743  0.7511655 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1206 is [True, False, False, False, True, False]
Current timestep = 1207. State = [[-0.10793976  0.07724289]]. Action = [[-0.13175473 -0.1627851   0.11943999 -0.09383512]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1207 is [True, False, False, False, True, False]
Current timestep = 1208. State = [[-0.11607799  0.05216143]]. Action = [[-0.07152903 -0.17398529 -0.10806417  0.8259556 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1208 is [True, False, False, False, True, False]
Current timestep = 1209. State = [[-0.11491355  0.03566787]]. Action = [[ 0.21337727 -0.0381522   0.0197663   0.611532  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Scene graph at timestep 1209 is [True, False, False, False, True, False]
State prediction error at timestep 1209 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1209 of 1
Current timestep = 1210. State = [[-0.10389759  0.02089438]]. Action = [[ 0.10766068 -0.08154395 -0.01508462 -0.02603573]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Current timestep = 1211. State = [[-0.10146677  0.01495162]]. Action = [[-0.03874965  0.01454028  0.13773462 -0.05580604]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Scene graph at timestep 1211 is [True, False, False, False, True, False]
State prediction error at timestep 1211 is tensor(5.3946e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of 1
Current timestep = 1212. State = [[-0.1000895   0.02404437]]. Action = [[ 0.05655903  0.17822057  0.20672077 -0.9359863 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Current timestep = 1213. State = [[-0.09663481  0.02637863]]. Action = [[ 0.01633099 -0.13540351  0.00903898 -0.24756908]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Current timestep = 1214. State = [[-0.08502429  0.02190388]]. Action = [[ 0.23087475  0.0055514   0.17282653 -0.90039337]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Current timestep = 1215. State = [[-0.05730795  0.02120596]]. Action = [[ 0.19320202 -0.01636322 -0.0853357  -0.62107575]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.2683535  -0.23054221]]. Action = [[ 0.11351049 -0.12385067 -0.24322513  0.92959046]]. Reward = [100.]
Curr episode timestep = 55
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Current timestep = 1217. State = [[-0.26617542 -0.25653154]]. Action = [[ 0.01068851 -0.02140449  0.11483681  0.55686283]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1217 is [True, False, False, True, False, False]
Current timestep = 1218. State = [[-0.26044446 -0.2632442 ]]. Action = [[ 0.09970403 -0.08914727 -0.17885362  0.40136302]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1218 is [True, False, False, True, False, False]
Scene graph at timestep 1218 is [True, False, False, True, False, False]
State prediction error at timestep 1218 is tensor(7.8547e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of 1
Current timestep = 1219. State = [[-0.24828261 -0.26567423]]. Action = [[ 0.10998967  0.11331832 -0.05243467 -0.2722543 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1219 is [True, False, False, True, False, False]
Current timestep = 1220. State = [[-0.2345153  -0.25256208]]. Action = [[ 0.1303997   0.14089942 -0.24716483  0.18048728]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1220 is [True, False, False, True, False, False]
Current timestep = 1221. State = [[-0.21381061 -0.23125225]]. Action = [[0.16982669 0.19408655 0.06853694 0.7872324 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1221 is [True, False, False, True, False, False]
Current timestep = 1222. State = [[-0.1890962  -0.20307939]]. Action = [[ 0.17719966  0.24379525  0.00896057 -0.9379275 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1222 is [True, False, False, True, False, False]
Current timestep = 1223. State = [[-0.17969343 -0.19005471]]. Action = [[-0.15820137 -0.09910181  0.12282473  0.6284021 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1223 is [True, False, False, True, False, False]
Scene graph at timestep 1223 is [True, False, False, True, False, False]
State prediction error at timestep 1223 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[-0.18778351 -0.18688951]]. Action = [[-0.15113191  0.11592382  0.085527    0.9777365 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1224 is [True, False, False, True, False, False]
Current timestep = 1225. State = [[-0.19327283 -0.18303691]]. Action = [[ 0.06158715 -0.04218632  0.18441772 -0.08676326]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1225 is [True, False, False, True, False, False]
Scene graph at timestep 1225 is [True, False, False, True, False, False]
State prediction error at timestep 1225 is tensor(6.1751e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of -1
Current timestep = 1226. State = [[-0.1897376  -0.17613308]]. Action = [[ 0.12349534  0.10435137  0.21549934 -0.4915229 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1226 is [True, False, False, True, False, False]
Current timestep = 1227. State = [[-0.18216929 -0.15858856]]. Action = [[ 0.11851016  0.17027932  0.21584827 -0.0779326 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1227 is [True, False, False, True, False, False]
Current timestep = 1228. State = [[-0.17318694 -0.14468308]]. Action = [[ 0.02710983  0.01700908  0.15115032 -0.20417404]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1228 is [True, False, False, True, False, False]
Scene graph at timestep 1228 is [True, False, False, True, False, False]
State prediction error at timestep 1228 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1228 of 1
Current timestep = 1229. State = [[-0.16006868 -0.13244963]]. Action = [[ 0.19188052  0.13329244 -0.24394292 -0.47791576]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1229 is [True, False, False, True, False, False]
Current timestep = 1230. State = [[-0.14658365 -0.13300358]]. Action = [[-0.09052089 -0.16609724 -0.18776317 -0.06658906]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1230 is [True, False, False, True, False, False]
Scene graph at timestep 1230 is [True, False, False, True, False, False]
State prediction error at timestep 1230 is tensor(1.1265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1230 of -1
Current timestep = 1231. State = [[-0.14982523 -0.13489436]]. Action = [[-0.08972558  0.11353636  0.12057498 -0.5506952 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1231 is [True, False, False, True, False, False]
Scene graph at timestep 1231 is [True, False, False, True, False, False]
State prediction error at timestep 1231 is tensor(3.8583e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of 1
Current timestep = 1232. State = [[-0.15112318 -0.13055493]]. Action = [[0.02610222 0.00108632 0.01837903 0.06457353]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1232 is [True, False, False, True, False, False]
Current timestep = 1233. State = [[-0.1497965  -0.11608855]]. Action = [[ 0.0525218   0.22251198 -0.23971227 -0.39095002]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1233 is [True, False, False, True, False, False]
Current timestep = 1234. State = [[-0.1458716 -0.0900398]]. Action = [[0.12036487 0.14271018 0.08154738 0.6306505 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.13433021 -0.08999804]]. Action = [[ 0.13521335 -0.23438673  0.05847025 -0.95806915]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1235 is [True, False, False, False, True, False]
Current timestep = 1236. State = [[-0.11508983 -0.09723593]]. Action = [[0.07885662 0.05245271 0.05057466 0.41170228]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Current timestep = 1237. State = [[-0.10954494 -0.09831335]]. Action = [[-0.18428439  0.00187442 -0.23557743  0.70337224]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Scene graph at timestep 1237 is [True, False, False, False, True, False]
State prediction error at timestep 1237 is tensor(2.4475e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of 1
Current timestep = 1238. State = [[-0.11297043 -0.08883437]]. Action = [[ 0.00624213  0.17093486  0.16829023 -0.08182162]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1238 is [True, False, False, False, True, False]
Current timestep = 1239. State = [[-0.11066714 -0.06741333]]. Action = [[ 0.1749034   0.15590805 -0.18355718  0.6378486 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1239 is [True, False, False, False, True, False]
Scene graph at timestep 1239 is [True, False, False, False, True, False]
State prediction error at timestep 1239 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[-0.0973895  -0.05949041]]. Action = [[ 0.16539887 -0.12275855  0.15094179  0.8643768 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1240 is [True, False, False, False, True, False]
Current timestep = 1241. State = [[-0.08027324 -0.07091764]]. Action = [[ 0.06076658 -0.11759105 -0.22473817  0.01739025]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1241 is [True, False, False, False, True, False]
Scene graph at timestep 1241 is [True, False, False, False, True, False]
State prediction error at timestep 1241 is tensor(3.4899e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of 1
Current timestep = 1242. State = [[-0.06537844 -0.08700141]]. Action = [[ 0.1687637  -0.09452215  0.07142565  0.6073834 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1242 is [True, False, False, False, True, False]
Current timestep = 1243. State = [[-0.04343205 -0.09178463]]. Action = [[ 0.14582089  0.03654495 -0.15180609  0.26626408]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1243 is [True, False, False, False, True, False]
Current timestep = 1244. State = [[-0.21057425  0.12753208]]. Action = [[-0.11589506  0.13924944 -0.12279779 -0.3409397 ]]. Reward = [100.]
Curr episode timestep = 27
Scene graph at timestep 1244 is [False, True, False, False, True, False]
Current timestep = 1245. State = [[-0.18635325  0.13135125]]. Action = [[ 0.23701894 -0.19438785  0.09813094  0.00217867]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1245 is [True, False, False, False, False, True]
Current timestep = 1246. State = [[-0.1622001   0.11179913]]. Action = [[ 0.1343534  -0.1542463  -0.03260627  0.17870128]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1246 is [True, False, False, False, False, True]
Current timestep = 1247. State = [[-0.15764299  0.11067565]]. Action = [[-0.22032554  0.17007056 -0.1008423  -0.52382463]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1247 is [True, False, False, False, True, False]
Current timestep = 1248. State = [[-0.16007699  0.11888219]]. Action = [[ 0.14621675  0.01707038 -0.17651342  0.9648528 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1248 is [True, False, False, False, True, False]
Current timestep = 1249. State = [[-0.15125103  0.12179895]]. Action = [[ 0.12710392  0.03573975 -0.0648056  -0.5942122 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1249 is [True, False, False, False, True, False]
