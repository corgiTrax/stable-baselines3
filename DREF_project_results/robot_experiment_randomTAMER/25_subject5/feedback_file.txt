Current timestep = 0. State = [[-0.18843077  0.12172774]]. Action = [[-0.22671764  0.22066998 -0.04355229  0.34637058]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0246, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.20081975  0.13928695]]. Action = [[ 0.05896631 -0.10201581  0.03424671 -0.12099397]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.18939473  0.11926848]]. Action = [[ 0.22276515 -0.21430774 -0.05969171 -0.2751137 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Current timestep = 3. State = [[-0.1812028   0.10203166]]. Action = [[-0.1502553  -0.04862022 -0.15158084  0.36483502]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Current timestep = 4. State = [[-0.18553136  0.10330359]]. Action = [[-0.09956092  0.10473228  0.16567981 -0.9410883 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.19223842  0.1149696 ]]. Action = [[-0.03286012  0.10425466  0.08611661 -0.46024847]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.20203964  0.13480288]]. Action = [[-0.05526994  0.17853564  0.24572635 -0.5616687 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Scene graph at timestep 6 is [True, False, False, False, False, True]
State prediction error at timestep 6 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.20726337  0.13485508]]. Action = [[ 0.00137934 -0.24957219 -0.03979251  0.5482707 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Current timestep = 8. State = [[-0.20014828  0.10817318]]. Action = [[ 0.15586007 -0.19847858  0.20316282 -0.33498323]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Current timestep = 9. State = [[-0.18582039  0.10456024]]. Action = [[0.22996166 0.24501726 0.15886295 0.708617  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.15870106  0.1121484 ]]. Action = [[ 0.14856339 -0.09235926  0.09073377 -0.9461258 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.14116865  0.10817963]]. Action = [[ 0.16659838 -0.00880079  0.18691838  0.37193322]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.13390535  0.11950054]]. Action = [[-0.20342161  0.21130368  0.0206984   0.15325308]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.1410613  0.1383395]]. Action = [[0.05968344 0.12590888 0.0957056  0.7304621 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Current timestep = 14. State = [[-0.13535996  0.13413863]]. Action = [[ 0.09213102 -0.23906428 -0.12561852 -0.5027117 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.12232594  0.12223648]]. Action = [[ 0.17429566 -0.00808503  0.20965701 -0.49337554]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.09663274  0.11886844]]. Action = [[ 0.18935215 -0.01958446  0.1980465  -0.78160304]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Current timestep = 17. State = [[-0.08472967  0.13094166]]. Action = [[-0.08619198  0.22749814  0.09795129  0.6718075 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.08650941  0.15084025]]. Action = [[ 0.04186094  0.14522287 -0.21266149  0.37002802]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.07685807  0.15797937]]. Action = [[ 0.21625692 -0.05268359  0.055296   -0.28686166]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Current timestep = 20. State = [[-0.06223124  0.1689733 ]]. Action = [[-0.16371483  0.10665327  0.16425613 -0.73848444]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.05708725  0.16352506]]. Action = [[ 0.21700358 -0.17815971 -0.05599724  0.9074366 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Scene graph at timestep 21 is [True, False, False, False, False, True]
State prediction error at timestep 21 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.04174338  0.14272054]]. Action = [[-0.21390547 -0.21999003  0.14933094  0.6461501 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.05256164  0.14057685]]. Action = [[-0.23934837  0.1566078   0.15989524 -0.01623911]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [False, True, False, False, False, True]
Current timestep = 24. State = [[-0.06832627  0.15209845]]. Action = [[0.01338693 0.03590164 0.11937347 0.7714747 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.08130192  0.16917974]]. Action = [[-0.22006918  0.239146   -0.04618838  0.62686765]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Scene graph at timestep 25 is [True, False, False, False, False, True]
State prediction error at timestep 25 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.10349765  0.19823073]]. Action = [[ 0.16193289  0.14598584 -0.07427329  0.8320601 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Current timestep = 27. State = [[-0.09620995  0.21527965]]. Action = [[0.22697574 0.24544424 0.06739506 0.5815027 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Current timestep = 28. State = [[-0.07709158  0.24248655]]. Action = [[ 0.13706541  0.18629873  0.1045554  -0.46674848]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, False, True]
Current timestep = 29. State = [[-0.05634886  0.25078556]]. Action = [[ 0.0623582  -0.17739153 -0.11859758  0.92736197]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, False, True]
Current timestep = 30. State = [[-0.0457838   0.23987916]]. Action = [[ 0.13661116 -0.08590928 -0.1770125  -0.8838094 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, False, True]
Current timestep = 31. State = [[-0.02914411  0.23074985]]. Action = [[ 0.11587656 -0.01837131  0.20526409  0.21930778]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [False, True, False, False, False, True]
Current timestep = 32. State = [[-0.00804534  0.21914463]]. Action = [[ 0.23697448 -0.19057573 -0.17538685  0.73586166]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [False, True, False, False, False, True]
Current timestep = 33. State = [[0.01197255 0.20369434]]. Action = [[-0.14794928 -0.04820293  0.07696539  0.40776587]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [False, True, False, False, False, True]
Current timestep = 34. State = [[0.01310957 0.19722275]]. Action = [[ 0.02487579 -0.04438815  0.23349217 -0.3338194 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [False, True, False, False, False, True]
Current timestep = 35. State = [[0.01654729 0.1898858 ]]. Action = [[ 0.09999269 -0.03386603 -0.12363833 -0.8939464 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [False, True, False, False, False, True]
Current timestep = 36. State = [[0.01801789 0.18268462]]. Action = [[-0.23763864 -0.10141008 -0.12531085  0.3620572 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [False, True, False, False, False, True]
Current timestep = 37. State = [[0.01547076 0.16931853]]. Action = [[-0.09711477 -0.1535168   0.07436472 -0.29905605]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [False, True, False, False, False, True]
Scene graph at timestep 37 is [False, True, False, False, False, True]
State prediction error at timestep 37 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[0.00367546 0.14816134]]. Action = [[-0.15569599 -0.13258062  0.04183757  0.02736747]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [False, True, False, False, False, True]
Scene graph at timestep 38 is [False, True, False, False, False, True]
State prediction error at timestep 38 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.01383889  0.13386758]]. Action = [[ 0.02924553  0.02386206 -0.06348938  0.25832772]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [False, True, False, False, False, True]
Scene graph at timestep 39 is [False, True, False, False, False, True]
State prediction error at timestep 39 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.0158551  0.1283737]]. Action = [[-0.1004481  -0.12975478 -0.18636695  0.41600513]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [False, True, False, False, False, True]
Current timestep = 41. State = [[-0.22397527 -0.09702055]]. Action = [[-0.05621368 -0.21712328 -0.07245359  0.492244  ]]. Reward = [100.]
Curr episode timestep = 41
Scene graph at timestep 41 is [False, True, False, False, False, True]
Current timestep = 42. State = [[-0.20957737 -0.11838572]]. Action = [[ 0.16137323 -0.19548766 -0.22149368 -0.85485756]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 42 is [True, False, False, False, True, False]
Current timestep = 43. State = [[-0.20056897 -0.13084282]]. Action = [[-0.15210007  0.05473405 -0.06592438  0.5533217 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 43 is [True, False, False, False, True, False]
Current timestep = 44. State = [[-0.19684954 -0.13075282]]. Action = [[0.20752159 0.01069722 0.11566275 0.03565276]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 44 is [True, False, False, True, False, False]
Current timestep = 45. State = [[-0.17922698 -0.12456577]]. Action = [[0.20689034 0.09676847 0.23201999 0.43347192]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 45 is [True, False, False, True, False, False]
Current timestep = 46. State = [[-0.16788732 -0.11101432]]. Action = [[-0.20232493  0.16069585 -0.11002342  0.63492966]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 46 is [True, False, False, False, True, False]
Current timestep = 47. State = [[-0.17033598 -0.09343235]]. Action = [[0.07233137 0.10308129 0.1918214  0.06679296]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.17455827 -0.08256996]]. Action = [[-0.18328625  0.02902856 -0.02347098  0.29027116]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.1901658  -0.06815761]]. Action = [[-0.21379778  0.16884637  0.14343077  0.39867592]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.20608366 -0.0695352 ]]. Action = [[ 0.01081023 -0.23768055  0.08200821  0.4953934 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.22035415 -0.07635865]]. Action = [[-0.22438826  0.10121468  0.0169791  -0.77092326]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 51 is [True, False, False, False, True, False]
Current timestep = 52. State = [[-0.23419431 -0.06660647]]. Action = [[ 0.08753449  0.1208992   0.15809613 -0.9705068 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.22706144 -0.06312478]]. Action = [[ 0.19701123 -0.11555505  0.02641967 -0.6284366 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.2239448  -0.05701225]]. Action = [[-0.15517548  0.1687463  -0.23962893  0.13313532]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.23019458 -0.04607021]]. Action = [[-0.0962449   0.04355696 -0.13653302 -0.8766257 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 55 is [True, False, False, False, True, False]
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0187, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.23441477 -0.04728532]]. Action = [[ 0.06667268 -0.12085792  0.06394777  0.8111768 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 56 is [True, False, False, False, True, False]
Current timestep = 57. State = [[-0.23523825 -0.04634183]]. Action = [[ 0.02117714  0.12862486 -0.02652939  0.02996445]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.23708847 -0.03273177]]. Action = [[-0.07443827  0.15373397 -0.16167246 -0.4077921 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 58 is [True, False, False, False, True, False]
Current timestep = 59. State = [[-0.24538127 -0.02730148]]. Action = [[-0.15562859 -0.12004372 -0.1784011  -0.57262766]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.2519855  -0.04346874]]. Action = [[ 0.10503852 -0.19795427 -0.18609035  0.8949951 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.2448183  -0.06987302]]. Action = [[ 0.177282   -0.24045074 -0.23220044  0.04772925]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 61 is [True, False, False, False, True, False]
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0167, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.23546754 -0.08716467]]. Action = [[-0.07308224  0.134401    0.21125269 -0.53100914]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.23469907 -0.07043836]]. Action = [[ 0.08659071  0.19302607  0.14995736 -0.3039031 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.23192084 -0.06448117]]. Action = [[ 0.01729006 -0.17175902  0.23534971 -0.27841496]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.22422607 -0.06200763]]. Action = [[ 0.12098905  0.15064204 -0.16357762 -0.4399295 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 65 is [True, False, False, False, True, False]
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.21236873 -0.05279562]]. Action = [[ 0.09332529 -0.01072434  0.07696423  0.69275   ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 66 is [True, False, False, False, True, False]
Current timestep = 67. State = [[-0.19726178 -0.06151996]]. Action = [[ 0.14088038 -0.14567372 -0.09252369  0.27181995]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 67 is [True, False, False, False, True, False]
Current timestep = 68. State = [[-0.1890339  -0.06475491]]. Action = [[-0.1541716   0.1001702   0.05368337 -0.22661787]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 68 is [True, False, False, False, True, False]
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0092, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.19612478 -0.07297947]]. Action = [[-0.1264244  -0.1818031  -0.1456133   0.50795484]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 69 is [True, False, False, False, True, False]
Current timestep = 70. State = [[-0.20061365 -0.0818608 ]]. Action = [[0.09823993 0.01696131 0.1794967  0.08516324]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 70 is [True, False, False, False, True, False]
Current timestep = 71. State = [[-0.20073348 -0.07921994]]. Action = [[-0.05624148  0.08000252 -0.05889478  0.6308682 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 71 is [True, False, False, False, True, False]
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.19880323 -0.07466088]]. Action = [[0.1118663  0.01517484 0.20111758 0.57818484]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.19455168 -0.06295692]]. Action = [[ 0.07209295  0.16215438 -0.12551592 -0.78445673]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.19331846 -0.05384741]]. Action = [[-0.16945481 -0.02537988 -0.22733462  0.30463243]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.20683016 -0.0660978 ]]. Action = [[-0.24108209 -0.2065551  -0.07230502 -0.42562342]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 75 is [True, False, False, False, True, False]
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.22044688 -0.08912324]]. Action = [[ 0.2413305  -0.14328104 -0.22451206  0.47960663]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.20277417 -0.11071496]]. Action = [[ 0.2011413  -0.2427762  -0.10695079  0.04491138]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 77 is [True, False, False, False, True, False]
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.17512128 -0.12079661]]. Action = [[ 0.20349336  0.20645195 -0.16060528  0.2314955 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 78 is [True, False, False, False, True, False]
Current timestep = 79. State = [[-0.15234578 -0.09689087]]. Action = [[ 0.13631162  0.2429092  -0.03836499  0.5943327 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 79 is [True, False, False, False, True, False]
Current timestep = 80. State = [[-0.1405137  -0.09088749]]. Action = [[-0.03089145 -0.22990707  0.24398041 -0.5369919 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.13808621 -0.09789326]]. Action = [[0.0608446  0.01493049 0.10063016 0.6007309 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.13383079 -0.100618  ]]. Action = [[ 0.05658615 -0.04262848  0.1677582  -0.33137572]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.12410334 -0.10532867]]. Action = [[ 0.06252623 -0.04389116 -0.04227786 -0.04480803]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.10579676 -0.12238083]]. Action = [[ 0.22688633 -0.21522366  0.20809889 -0.17568374]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 84 is [True, False, False, False, True, False]
Current timestep = 85. State = [[-0.0767687  -0.12589686]]. Action = [[ 0.16863823  0.23575264 -0.01181106 -0.78422487]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.06164679 -0.11231505]]. Action = [[-0.23824382  0.06730378 -0.19056354 -0.7793904 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 86 is [True, False, False, True, False, False]
Current timestep = 87. State = [[-0.07504561 -0.09681176]]. Action = [[-0.21770203  0.18644959 -0.07605553  0.3158474 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 87 is [True, False, False, False, True, False]
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.1006449  -0.09361002]]. Action = [[-0.084153   -0.24055678 -0.11155298  0.12588346]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.10517342 -0.12200066]]. Action = [[ 0.13655603 -0.1834166  -0.0630836   0.12816715]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 89 is [True, False, False, False, True, False]
Current timestep = 90. State = [[-0.10680615 -0.12678729]]. Action = [[-0.18207684  0.12990242 -0.11546612 -0.10775495]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 90 is [True, False, False, False, True, False]
Current timestep = 91. State = [[-0.12035234 -0.1250866 ]]. Action = [[-0.1743001  -0.012391    0.11953357  0.35677874]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 91 is [True, False, False, True, False, False]
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of -1
Current timestep = 92. State = [[-0.13677311 -0.11800703]]. Action = [[ 0.18192369  0.09304717 -0.0489035  -0.8747313 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 92 is [True, False, False, True, False, False]
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.12060507 -0.0987168 ]]. Action = [[ 0.21956617  0.18784869 -0.1241563  -0.16508245]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 93 is [True, False, False, False, True, False]
Current timestep = 94. State = [[-0.11093806 -0.07192351]]. Action = [[-0.14868525  0.24102321  0.23600379  0.43837476]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 94 is [True, False, False, False, True, False]
Current timestep = 95. State = [[-0.1168746  -0.04090681]]. Action = [[-0.08791879  0.1706698   0.22999123  0.72048426]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.12976015 -0.03539062]]. Action = [[-0.19840723 -0.1608273   0.13452369  0.6453407 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.14708064 -0.04042618]]. Action = [[-0.11082655  0.036562   -0.15429476  0.9236169 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.14890003 -0.0418805 ]]. Action = [[ 0.22731489 -0.0634304   0.14013448 -0.896046  ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.13601696 -0.03030724]]. Action = [[0.23695177 0.24586445 0.13134778 0.60665774]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.12631899 -0.00491237]]. Action = [[-0.23439573  0.22373301  0.21368352  0.819319  ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 100 is [True, False, False, False, True, False]
Current timestep = 101. State = [[-0.1310607   0.01105825]]. Action = [[ 0.13676971 -0.10223961 -0.20459883 -0.07081109]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.13125542  0.01664893]]. Action = [[-0.02139607  0.14412928 -0.14946859  0.34196138]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.13504127  0.01158853]]. Action = [[-0.1542666  -0.22682977 -0.16055842 -0.25982642]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.13714197 -0.00046518]]. Action = [[ 0.03387412 -0.01629668 -0.03468975  0.8334172 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 104 is [True, False, False, False, True, False]
Current timestep = 105. State = [[-0.1375887  -0.00261861]]. Action = [[-0.05086634 -0.00136708 -0.11015984  0.68712497]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-1.3934414e-01  8.7994849e-05]]. Action = [[-8.6039305e-05  8.6503625e-02 -1.7155369e-01  1.7607391e-01]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 106 is [True, False, False, False, True, False]
Current timestep = 107. State = [[-0.13860774  0.0008428 ]]. Action = [[ 0.10784733 -0.06168078 -0.14690858  0.28040862]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.1356759 -0.0149994]]. Action = [[-0.00107656 -0.24113742 -0.22713122 -0.14047515]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 108 is [True, False, False, False, True, False]
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.13430384 -0.02521728]]. Action = [[ 0.00612164  0.18539074 -0.15609968  0.22163475]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 109 is [True, False, False, False, True, False]
Current timestep = 110. State = [[-0.13417786 -0.02533217]]. Action = [[-0.02958408 -0.16480964 -0.14325505  0.24134195]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.13223647 -0.04111795]]. Action = [[ 0.08044696 -0.17456518  0.03009754  0.87281036]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.12664369 -0.0498706 ]]. Action = [[0.11694199 0.08443072 0.09295541 0.64059854]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 112 is [True, False, False, False, True, False]
Current timestep = 113. State = [[-0.11499356 -0.04467637]]. Action = [[0.13048244 0.07185838 0.12998602 0.47144198]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 113 is [True, False, False, False, True, False]
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.10503855 -0.03780379]]. Action = [[-0.20650779  0.01678827  0.09865323 -0.60559934]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 114 is [True, False, False, False, True, False]
Current timestep = 115. State = [[-0.11563053 -0.04206683]]. Action = [[-0.17620887 -0.07950661  0.02959466  0.8418906 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 115 is [True, False, False, False, True, False]
Current timestep = 116. State = [[-0.12407592 -0.0439304 ]]. Action = [[ 0.22167891  0.05882418 -0.18403888 -0.62264454]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 116 is [True, False, False, False, True, False]
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of 0
Current timestep = 117. State = [[-0.11424602 -0.03600125]]. Action = [[ 0.16261497  0.09175682 -0.18471928  0.49845743]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 117 is [True, False, False, False, True, False]
Current timestep = 118. State = [[-0.0968853  -0.03984578]]. Action = [[ 0.11883193 -0.18331003  0.16230747 -0.17373717]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 118 is [True, False, False, False, True, False]
Current timestep = 119. State = [[-0.09369949 -0.04948188]]. Action = [[-0.2123474  -0.03742641 -0.11008622  0.26119244]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 119 is [True, False, False, False, True, False]
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.10591152 -0.0544378 ]]. Action = [[-0.20112652  0.02398518  0.14303759 -0.71932876]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.11669761 -0.04645845]]. Action = [[ 0.02739862  0.14016002 -0.09250034 -0.23392606]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 121 is [True, False, False, False, True, False]
Current timestep = 122. State = [[-0.12921615 -0.02622986]]. Action = [[-0.23938781  0.17701018  0.21118566  0.9678614 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 122 is [True, False, False, False, True, False]
Current timestep = 123. State = [[-0.14797771 -0.02003834]]. Action = [[-0.05145621 -0.1480038  -0.17566761  0.87199974]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 123 is [True, False, False, False, True, False]
Current timestep = 124. State = [[-0.16458273 -0.03277167]]. Action = [[-0.20750488 -0.08271046  0.11596182  0.37079108]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 124 is [True, False, False, False, True, False]
Current timestep = 125. State = [[-0.18394604 -0.05129472]]. Action = [[-0.03517938 -0.18336976 -0.22528554  0.7225914 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 125 is [True, False, False, False, True, False]
Current timestep = 126. State = [[-0.18252787 -0.06987876]]. Action = [[ 0.23924625 -0.11389531  0.23117536  0.7742393 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 126 is [True, False, False, False, True, False]
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.18196748 -0.08255947]]. Action = [[-0.23101893  0.00474286  0.2152791  -0.51160103]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 127 is [True, False, False, False, True, False]
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.19258468 -0.08582175]]. Action = [[ 0.12535667  0.00487244 -0.02908099 -0.8500181 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.19197987 -0.08536115]]. Action = [[-0.04382035  0.02182731 -0.15949383 -0.84111005]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.19690564 -0.09324638]]. Action = [[-0.14561617 -0.16373092  0.10143733  0.12808442]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 130 is [True, False, False, False, True, False]
Current timestep = 131. State = [[-0.19777502 -0.0900116 ]]. Action = [[0.18441975 0.21541572 0.14692551 0.09916604]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 131 is [True, False, False, False, True, False]
Current timestep = 132. State = [[-0.19678472 -0.0695726 ]]. Action = [[-0.13440937  0.1649383  -0.06197929 -0.4613012 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.2070849 -0.0663253]]. Action = [[-0.18458761 -0.17711402 -0.17638224 -0.60625666]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.21389423 -0.06849686]]. Action = [[ 0.04040706  0.12407267 -0.24597818  0.60312057]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 134 is [True, False, False, False, True, False]
Current timestep = 135. State = [[-0.21000153 -0.05447696]]. Action = [[0.16366047 0.12349832 0.19733879 0.72344375]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.20085281 -0.04786804]]. Action = [[ 0.16722032 -0.05270837  0.11359477 -0.7184149 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 136 is [True, False, False, False, True, False]
Current timestep = 137. State = [[-0.1846562  -0.03508526]]. Action = [[ 0.16459352  0.20718443 -0.01476881 -0.6365332 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.15900299 -0.01142223]]. Action = [[ 0.23269886  0.16648215 -0.10070539  0.4737593 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.13413115 -0.00900258]]. Action = [[ 0.09348261 -0.21074794 -0.10856873 -0.31465924]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 139 is [True, False, False, False, True, False]
Current timestep = 140. State = [[-0.118185   -0.02553073]]. Action = [[ 0.15893099 -0.14008737 -0.09140667  0.4513675 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.10773052 -0.03027444]]. Action = [[-0.12022553  0.13078707  0.11892676 -0.50109214]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 141 is [True, False, False, False, True, False]
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.09884346 -0.0135857 ]]. Action = [[0.23474133 0.20498753 0.20128724 0.22008193]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.07526987 -0.01093669]]. Action = [[ 0.19750214 -0.2075616   0.21138713 -0.19734788]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.0623677  -0.01557924]]. Action = [[-0.12554301  0.0656524  -0.16059154  0.10288393]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.06711406 -0.02266158]]. Action = [[-0.18183878 -0.12280081  0.09776932 -0.6933418 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.07952385 -0.02026508]]. Action = [[-0.07876723  0.21352607  0.07632065  0.01156211]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.08052736  0.00687802]]. Action = [[ 0.22104043  0.20871252  0.10827279 -0.8935958 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 147 is [True, False, False, False, True, False]
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of 0
Current timestep = 148. State = [[-0.06805242  0.01701912]]. Action = [[ 0.18601766 -0.18909939  0.14188504  0.9412389 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of 1
Current timestep = 149. State = [[-0.04413658  0.01381044]]. Action = [[ 0.23067343  0.16328359 -0.21859929  0.06203973]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.24771729  0.22370885]]. Action = [[ 0.14827585  0.08453879 -0.20245436  0.05781496]]. Reward = [100.]
Curr episode timestep = 108
Scene graph at timestep 150 is [False, True, False, False, True, False]
Current timestep = 151. State = [[-0.23365924  0.23973851]]. Action = [[ 0.08262318 -0.20583816 -0.03854719 -0.16513395]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 151 is [True, False, False, False, False, True]
Scene graph at timestep 151 is [True, False, False, False, False, True]
State prediction error at timestep 151 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.22009389  0.2328242 ]]. Action = [[ 0.17928773  0.1772396   0.01737165 -0.21720749]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 152 is [True, False, False, False, False, True]
Current timestep = 153. State = [[-0.1987977   0.23674119]]. Action = [[ 0.08128077 -0.14907299  0.24462715 -0.82059556]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 153 is [True, False, False, False, False, True]
Current timestep = 154. State = [[-0.1936694   0.23058131]]. Action = [[-0.14511761 -0.05308811  0.22465086 -0.40651727]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 154 is [True, False, False, False, False, True]
Current timestep = 155. State = [[-0.19154441  0.21645655]]. Action = [[-0.02875045 -0.21025804 -0.04044357  0.08065689]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 155 is [True, False, False, False, False, True]
Current timestep = 156. State = [[-0.1895076   0.20446073]]. Action = [[0.01767331 0.02046546 0.13088918 0.20152164]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 156 is [True, False, False, False, False, True]
Current timestep = 157. State = [[-0.18892702  0.20174941]]. Action = [[ 0.00906903 -0.02037457  0.06241134 -0.14622307]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 157 is [True, False, False, False, False, True]
Scene graph at timestep 157 is [True, False, False, False, False, True]
State prediction error at timestep 157 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.19114842  0.18677881]]. Action = [[-0.0888501  -0.23772079  0.22839469 -0.37681085]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 158 is [True, False, False, False, False, True]
Current timestep = 159. State = [[-0.1991066   0.15868053]]. Action = [[-0.02523866 -0.17725292  0.15837705  0.7286768 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 159 is [True, False, False, False, False, True]
Current timestep = 160. State = [[-0.21160986  0.15637188]]. Action = [[-0.21282466  0.18791723  0.04292044  0.6047137 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 160 is [True, False, False, False, False, True]
Current timestep = 161. State = [[-0.23653798  0.17407113]]. Action = [[-0.10255978  0.14043471 -0.10518001  0.39726102]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 161 is [True, False, False, False, False, True]
Current timestep = 162. State = [[-0.2517893   0.18101814]]. Action = [[-0.1611216  -0.11077297 -0.02896653  0.15314364]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 162 is [True, False, False, False, False, True]
Scene graph at timestep 162 is [True, False, False, False, False, True]
State prediction error at timestep 162 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.26912886  0.17293529]]. Action = [[-0.14328292 -0.23383184 -0.11892115 -0.7244157 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 163 is [True, False, False, False, False, True]
Scene graph at timestep 163 is [True, False, False, False, False, True]
State prediction error at timestep 163 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.2691692   0.17277113]]. Action = [[-0.11556026  0.23541337 -0.22976872 -0.91844815]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 164 is [True, False, False, False, False, True]
Current timestep = 165. State = [[-0.2691692   0.17277113]]. Action = [[-0.20393194 -0.03282744 -0.10768935  0.47580183]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 165 is [True, False, False, False, False, True]
Scene graph at timestep 165 is [True, False, False, False, False, True]
State prediction error at timestep 165 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.26594058  0.15736951]]. Action = [[ 0.05953881 -0.24722035  0.09223482  0.66951895]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 166 is [True, False, False, False, False, True]
Current timestep = 167. State = [[-0.26144794  0.14701982]]. Action = [[ 0.16133314  0.12280405 -0.17084195 -0.3859141 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 167 is [True, False, False, False, False, True]
Current timestep = 168. State = [[-0.24720591  0.14228071]]. Action = [[ 0.21948761 -0.11996511 -0.1245558   0.3759396 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 168 is [True, False, False, False, False, True]
Scene graph at timestep 168 is [True, False, False, False, False, True]
State prediction error at timestep 168 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.22964989  0.13504969]]. Action = [[ 0.05081707  0.07830894 -0.17134318  0.95816755]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 169 is [True, False, False, False, False, True]
Current timestep = 170. State = [[-0.21913497  0.1472981 ]]. Action = [[ 0.18156299  0.17456982 -0.02392973  0.81582594]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 170 is [True, False, False, False, False, True]
Current timestep = 171. State = [[-0.19385365  0.14641622]]. Action = [[ 0.13608041 -0.23042816 -0.17040692  0.937212  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 171 is [True, False, False, False, False, True]
Current timestep = 172. State = [[-0.18178655  0.12979811]]. Action = [[-0.01286958 -0.11127622 -0.175141   -0.7598703 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 172 is [True, False, False, False, False, True]
Current timestep = 173. State = [[-0.17300938  0.10570087]]. Action = [[ 0.08533075 -0.23769924 -0.09988555  0.21009469]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 173 is [True, False, False, False, False, True]
Current timestep = 174. State = [[-0.17355014  0.09441928]]. Action = [[-0.22888765  0.10022044 -0.02236007  0.14894211]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 174 is [True, False, False, False, True, False]
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.17832151  0.09104592]]. Action = [[ 0.08278888 -0.09906599 -0.20745553  0.90993524]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 175 is [True, False, False, False, True, False]
Current timestep = 176. State = [[-0.17738755  0.08510396]]. Action = [[-0.00040872 -0.02293034 -0.12619598 -0.14042187]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 176 is [True, False, False, False, True, False]
Current timestep = 177. State = [[-0.16957831  0.09443629]]. Action = [[0.22631094 0.2312178  0.2078071  0.69355655]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.15110958  0.10803109]]. Action = [[ 0.1771816   0.03961495  0.24676234 -0.1328997 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.14325958  0.11621609]]. Action = [[-0.24882057  0.00952065 -0.17318018 -0.57348245]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.1493761   0.12536874]]. Action = [[ 0.04135367  0.12484008 -0.12445354 -0.05608195]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.15127833  0.1236815 ]]. Action = [[-0.06324242 -0.18225442  0.00486624  0.8066771 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 181 is [True, False, False, False, False, True]
Current timestep = 182. State = [[-0.14937995  0.10675514]]. Action = [[ 0.0253619  -0.1569542   0.17130238  0.10627317]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.14272349  0.09262668]]. Action = [[ 0.2128182  -0.02809393 -0.15378101 -0.6019765 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.12718822  0.07534048]]. Action = [[ 0.20497763 -0.16263495  0.01104572 -0.22997129]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 184 is [True, False, False, False, True, False]
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-0.10308058  0.04653381]]. Action = [[ 0.12516001 -0.23804386  0.01180953 -0.67561066]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.09111719  0.01502375]]. Action = [[-0.12272587 -0.21820961 -0.11477144  0.64440656]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.09174338 -0.00251654]]. Action = [[-0.04072301 -0.024206   -0.0807011   0.06316197]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 187 is [True, False, False, False, True, False]
Current timestep = 188. State = [[-0.09253569 -0.01743466]]. Action = [[ 0.00711125 -0.15796927 -0.15691167 -0.00899899]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 188 is [True, False, False, False, True, False]
Current timestep = 189. State = [[-0.09348745 -0.04256368]]. Action = [[-0.03066975 -0.20139024 -0.05860633 -0.7769038 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.10347465 -0.05033671]]. Action = [[-0.214098    0.16848642 -0.00900185  0.09607995]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 190 is [True, False, False, False, True, False]
Current timestep = 191. State = [[-0.11542206 -0.05877674]]. Action = [[ 0.05077091 -0.23771758 -0.215429   -0.16253781]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 191 is [True, False, False, False, True, False]
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.11545093 -0.07199953]]. Action = [[ 0.07583308  0.04549181 -0.22271469  0.7536526 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 192 is [True, False, False, False, True, False]
Current timestep = 193. State = [[-0.11641606 -0.0591762 ]]. Action = [[-0.07771391  0.21044677 -0.19467367 -0.824162  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.12191521 -0.04627387]]. Action = [[-0.14028074 -0.02008158  0.09007147  0.89829683]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.1242213  -0.04093886]]. Action = [[0.22066295 0.0763526  0.01222727 0.1818986 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.11251947 -0.03067469]]. Action = [[ 0.15035555  0.07119283 -0.22786422 -0.62889254]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.11004732 -0.01473758]]. Action = [[-0.21148042  0.16620076  0.07676217  0.37755477]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 197 is [True, False, False, False, True, False]
Current timestep = 198. State = [[-0.10896866  0.00531487]]. Action = [[ 0.22321182  0.11442196  0.09979218 -0.00684881]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 198 is [True, False, False, False, True, False]
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.10464928  0.01805896]]. Action = [[-0.20836717  0.00451177  0.1904968   0.39957345]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 199 is [True, False, False, False, True, False]
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.1091461  0.0314278]]. Action = [[ 0.10574067  0.2278561  -0.17870227 -0.87763256]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 200 is [True, False, False, False, True, False]
Current timestep = 201. State = [[-0.11842644  0.04920223]]. Action = [[-0.24555877  0.00320366 -0.19075167 -0.6113494 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.12728028  0.0524501 ]]. Action = [[-1.6394868e-02 -3.2631278e-02  1.6972423e-04  2.6931500e-01]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.12496322  0.04720245]]. Action = [[ 0.2361193  -0.06255534  0.11276031  0.9332614 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 203 is [True, False, False, False, True, False]
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(2.2995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.12562528  0.04923452]]. Action = [[-0.23626855  0.0891628  -0.08008733 -0.44062173]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.13915895  0.03803381]]. Action = [[-0.19066137 -0.24753614 -0.24256437  0.83216417]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 205 is [True, False, False, False, True, False]
Current timestep = 206. State = [[-0.15558627  0.03513644]]. Action = [[-0.00181685  0.2114225  -0.15208475 -0.2214207 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.15777539  0.03335819]]. Action = [[ 0.05116668 -0.2310199  -0.0690366  -0.6729319 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 207 is [True, False, False, False, True, False]
Current timestep = 208. State = [[-0.16071264  0.02574947]]. Action = [[-0.10864457  0.06748191  0.13280037  0.6076542 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 208 is [True, False, False, False, True, False]
Current timestep = 209. State = [[-0.16949114  0.02589939]]. Action = [[-0.122179   -0.0036867  -0.23093739 -0.24818265]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.17657414  0.0177576 ]]. Action = [[ 0.0971939  -0.13494487 -0.16788574 -0.2826202 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 210 is [True, False, False, False, True, False]
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.17847952  0.02047732]]. Action = [[-0.00925271  0.23348808 -0.20041594  0.89858174]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.17305036  0.02089818]]. Action = [[ 0.23049465 -0.22931753  0.13612336  0.07296705]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 212 is [True, False, False, False, True, False]
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(5.4113e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-0.15176006  0.01019987]]. Action = [[ 0.21204007  0.04881096 -0.04457197  0.79174876]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 213 is [True, False, False, False, True, False]
Current timestep = 214. State = [[-0.14085147  0.0081521 ]]. Action = [[-0.16212282 -0.10189259  0.05729163  0.56955934]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 214 is [True, False, False, False, True, False]
Current timestep = 215. State = [[-0.14911994 -0.00105095]]. Action = [[-0.18495864 -0.06653038 -0.18002315 -0.8650523 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.15184516 -0.00880493]]. Action = [[0.24340618 0.001701   0.10248652 0.47834623]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 216 is [True, False, False, False, True, False]
Current timestep = 217. State = [[-0.1463125  -0.01821751]]. Action = [[-0.08258814 -0.16265127 -0.1061312   0.5022117 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 217 is [True, False, False, False, True, False]
Current timestep = 218. State = [[-0.14409909 -0.02817736]]. Action = [[ 0.05985773  0.01951462  0.09809181 -0.21712929]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 218 is [True, False, False, False, True, False]
Current timestep = 219. State = [[-0.14389634 -0.02445774]]. Action = [[ 0.00369787  0.10342079  0.07534006 -0.21795225]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 219 is [True, False, False, False, True, False]
Current timestep = 220. State = [[-0.14794493 -0.00856174]]. Action = [[-0.16768874  0.22292858  0.23783225 -0.0109759 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.15799847 -0.0023918 ]]. Action = [[-0.09395048 -0.20785189 -0.16911867 -0.01037765]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.16709392 -0.00039537]]. Action = [[-0.04568502  0.1845825   0.09972057  0.38360453]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 222 is [True, False, False, False, True, False]
Current timestep = 223. State = [[-0.17672034  0.01975916]]. Action = [[-0.10102849  0.18387282 -0.03147765 -0.89978594]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.18673335  0.02756408]]. Action = [[-0.07744032 -0.11879787  0.12817413 -0.7827727 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 224 is [True, False, False, False, True, False]
Current timestep = 225. State = [[-0.18653801  0.02872971]]. Action = [[ 0.14941108  0.11253411 -0.1033226  -0.590937  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 225 is [True, False, False, False, True, False]
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.17708239  0.02663785]]. Action = [[ 0.21276832 -0.14760795  0.22302067 -0.20318246]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.16303585  0.00968613]]. Action = [[ 0.15206861 -0.16370833 -0.01419874  0.6758697 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.1434095   0.00669721]]. Action = [[0.11229336 0.18548486 0.16037542 0.9204191 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 228 is [True, False, False, False, True, False]
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.12007556  0.01059388]]. Action = [[ 0.20340875 -0.10434601 -0.04529664  0.35174608]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.10481805  0.00915616]]. Action = [[ 0.03616711  0.052717   -0.02097228  0.87212086]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 230 is [True, False, False, False, True, False]
Current timestep = 231. State = [[-0.08886983  0.01300754]]. Action = [[ 0.23237246  0.02067298  0.02124846 -0.9009833 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 231 is [True, False, False, False, True, False]
Current timestep = 232. State = [[-0.06942794  0.02378838]]. Action = [[-0.02743492  0.14885908  0.09615526  0.7994194 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.06553051  0.02912623]]. Action = [[-0.04860373 -0.06516725 -0.22562091 -0.888309  ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 233 is [True, False, False, False, True, False]
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 233 of 1
Current timestep = 234. State = [[-0.06410196  0.02341936]]. Action = [[ 0.04846945 -0.08047481  0.24366343  0.2599951 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 234 is [True, False, False, False, True, False]
Current timestep = 235. State = [[-0.05778067  0.02044994]]. Action = [[ 0.15089685  0.0273416   0.06027946 -0.04183745]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 235 is [True, False, False, False, True, False]
Current timestep = 236. State = [[-0.04836494  0.02725557]]. Action = [[-0.00362702  0.13089174  0.19208628  0.9103223 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 236 is [True, False, False, False, True, False]
Scene graph at timestep 236 is [False, True, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.03835956  0.04356515]]. Action = [[ 0.09577441  0.13534218 -0.22300936 -0.43224114]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 237 is [False, True, False, False, True, False]
Current timestep = 238. State = [[-0.15807392 -0.18808444]]. Action = [[ 0.02814716  0.07034406  0.23314977 -0.2361694 ]]. Reward = [100.]
Curr episode timestep = 87
Scene graph at timestep 238 is [False, True, False, False, True, False]
Current timestep = 239. State = [[-0.13373226 -0.19628747]]. Action = [[ 0.1707449   0.23883587 -0.10641     0.10830665]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 239 is [True, False, False, True, False, False]
Current timestep = 240. State = [[-0.11708773 -0.19376196]]. Action = [[ 0.11768067 -0.18598352  0.01854327  0.45311558]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 240 is [True, False, False, True, False, False]
Current timestep = 241. State = [[-0.1010109  -0.20951587]]. Action = [[ 0.04048392 -0.13263734  0.18842983 -0.40878308]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 241 is [True, False, False, True, False, False]
Current timestep = 242. State = [[-0.08961061 -0.23116305]]. Action = [[ 0.128402   -0.21234381 -0.14964935  0.3460548 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 242 is [True, False, False, True, False, False]
Current timestep = 243. State = [[-0.06693068 -0.24679394]]. Action = [[0.23544854 0.02721968 0.18767104 0.7110467 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 243 is [True, False, False, True, False, False]
Scene graph at timestep 243 is [True, False, False, True, False, False]
State prediction error at timestep 243 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.0388953  -0.25311568]]. Action = [[ 0.05845216 -0.03817645 -0.22165726 -0.13281918]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 244 is [True, False, False, True, False, False]
Scene graph at timestep 244 is [False, True, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.0250087  -0.24377365]]. Action = [[ 0.13535053  0.20549467  0.13231313 -0.03349775]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 245 is [False, True, False, True, False, False]
Scene graph at timestep 245 is [False, True, False, True, False, False]
State prediction error at timestep 245 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.00709731 -0.22517501]]. Action = [[ 0.24645278  0.05108166 -0.17161317  0.19763196]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 246 is [False, True, False, True, False, False]
Scene graph at timestep 246 is [False, True, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0092, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 0
Current timestep = 247. State = [[ 0.03235245 -0.2279512 ]]. Action = [[ 0.14631438 -0.08602634  0.18950987  0.6306927 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 247 is [False, True, False, True, False, False]
Scene graph at timestep 247 is [False, True, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[ 0.04660559 -0.2319367 ]]. Action = [[-0.19882776  0.1086044  -0.18463148  0.87918997]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 248 is [False, True, False, True, False, False]
Current timestep = 249. State = [[ 0.0437899  -0.22037408]]. Action = [[-0.02877568  0.10700232  0.06558233 -0.96952313]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 249 is [False, True, False, True, False, False]
Current timestep = 250. State = [[ 0.03473931 -0.21383098]]. Action = [[-0.2156847   0.00760666 -0.14909391 -0.38920003]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 250 is [False, True, False, True, False, False]
Scene graph at timestep 250 is [False, True, False, True, False, False]
State prediction error at timestep 250 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[ 0.01325162 -0.22152406]]. Action = [[-0.14263357 -0.15156342  0.12554318  0.4497993 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 251 is [False, True, False, True, False, False]
Current timestep = 252. State = [[ 0.00585667 -0.22391075]]. Action = [[0.05260685 0.11066991 0.22426659 0.78485525]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 252 is [False, True, False, True, False, False]
Current timestep = 253. State = [[ 0.00738999 -0.2077383 ]]. Action = [[ 0.00126714  0.19261742 -0.1966761  -0.44110763]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 253 is [False, True, False, True, False, False]
Current timestep = 254. State = [[ 0.00903674 -0.19270834]]. Action = [[ 0.18289924 -0.05674697  0.14781338  0.53089833]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 254 is [False, True, False, True, False, False]
Current timestep = 255. State = [[ 0.01133713 -0.1853261 ]]. Action = [[0.01369247 0.10428089 0.1427806  0.6969912 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 255 is [False, True, False, True, False, False]
Current timestep = 256. State = [[ 0.01511044 -0.16544159]]. Action = [[ 0.0548884   0.2423718  -0.19873364 -0.7373026 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 256 is [False, True, False, True, False, False]
Current timestep = 257. State = [[ 0.01432057 -0.15456097]]. Action = [[-0.16880432 -0.13096136  0.19765973 -0.8376665 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 257 is [False, True, False, True, False, False]
Current timestep = 258. State = [[ 0.01507909 -0.15808068]]. Action = [[ 0.20025542  0.00416148 -0.08535525  0.61094844]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 258 is [False, True, False, True, False, False]
Scene graph at timestep 258 is [False, True, False, True, False, False]
State prediction error at timestep 258 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[ 0.01736586 -0.16342023]]. Action = [[-0.01022114 -0.1128739  -0.11033431 -0.8165944 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 259 is [False, True, False, True, False, False]
Current timestep = 260. State = [[ 0.01968449 -0.15925206]]. Action = [[ 0.08113226  0.17045534 -0.03317222  0.37430978]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 260 is [False, True, False, True, False, False]
Scene graph at timestep 260 is [False, True, False, True, False, False]
State prediction error at timestep 260 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 1
Current timestep = 261. State = [[ 0.02887672 -0.1604514 ]]. Action = [[ 0.16460091 -0.18734005  0.193429   -0.00930613]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 261 is [False, True, False, True, False, False]
Current timestep = 262. State = [[ 0.05276559 -0.1581441 ]]. Action = [[ 0.20174414  0.20142967  0.05578077 -0.13056058]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 262 is [False, True, False, True, False, False]
Current timestep = 263. State = [[ 0.07445001 -0.13927174]]. Action = [[ 0.01517606  0.17375916  0.16633213 -0.96615416]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 263 is [False, False, True, True, False, False]
Current timestep = 264. State = [[ 0.07690795 -0.1192851 ]]. Action = [[-0.2416755   0.10655552  0.21652538 -0.5050543 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 264 is [False, False, True, True, False, False]
Scene graph at timestep 264 is [False, False, True, False, True, False]
State prediction error at timestep 264 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[ 0.07295036 -0.10798015]]. Action = [[ 0.11224604  0.19958106 -0.03058261  0.7366613 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 265 is [False, False, True, False, True, False]
Scene graph at timestep 265 is [False, False, True, False, True, False]
State prediction error at timestep 265 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[ 0.07295036 -0.10798015]]. Action = [[ 0.10490578  0.12062415  0.10617527 -0.16006613]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 266 is [False, False, True, False, True, False]
Current timestep = 267. State = [[ 0.07295036 -0.10798015]]. Action = [[0.02435604 0.20646566 0.23963052 0.8181207 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 267 is [False, False, True, False, True, False]
Current timestep = 268. State = [[ 0.07295036 -0.10798015]]. Action = [[ 0.15806419 -0.11741078 -0.02516326 -0.7156098 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 268 is [False, False, True, False, True, False]
Current timestep = 269. State = [[ 0.07295036 -0.10798015]]. Action = [[ 0.09638038  0.23844582 -0.12199624 -0.93539304]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 269 is [False, False, True, False, True, False]
Current timestep = 270. State = [[ 0.07295036 -0.10798015]]. Action = [[ 0.21749291  0.16979563 -0.20663291 -0.8993664 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 270 is [False, False, True, False, True, False]
Current timestep = 271. State = [[ 0.07295036 -0.10798015]]. Action = [[ 0.18638068  0.1895735  -0.06674135  0.67517936]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 271 is [False, False, True, False, True, False]
Current timestep = 272. State = [[ 0.06795967 -0.10100038]]. Action = [[-0.16740127  0.10674399  0.20656449  0.77846956]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 272 is [False, False, True, False, True, False]
Current timestep = 273. State = [[ 0.05923801 -0.09276646]]. Action = [[ 0.00662118 -0.13397723 -0.18615375  0.8762059 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 273 is [False, False, True, False, True, False]
Current timestep = 274. State = [[ 0.05376332 -0.09519515]]. Action = [[-0.11414433 -0.04435121  0.06591916  0.83306324]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 274 is [False, False, True, False, True, False]
Current timestep = 275. State = [[ 0.04507198 -0.10929713]]. Action = [[ 0.00532383 -0.1950887   0.18187636  0.2812289 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 275 is [False, False, True, False, True, False]
Current timestep = 276. State = [[ 0.04007542 -0.12139928]]. Action = [[ 0.00827605  0.00177142  0.04519048 -0.47652447]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 276 is [False, True, False, False, True, False]
Current timestep = 277. State = [[ 0.03169828 -0.11917856]]. Action = [[-0.19948512  0.09506333  0.19436756 -0.76670605]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 277 is [False, True, False, False, True, False]
Current timestep = 278. State = [[ 0.01771669 -0.10772789]]. Action = [[ 0.00965947  0.08609462 -0.11179659 -0.9903517 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 278 is [False, True, False, False, True, False]
Current timestep = 279. State = [[ 0.00973561 -0.11476925]]. Action = [[-0.10794097 -0.21714523 -0.04524305 -0.5955298 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 279 is [False, True, False, False, True, False]
Scene graph at timestep 279 is [False, True, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.00551344 -0.1424529 ]]. Action = [[-0.0651024  -0.18850471 -0.05645062  0.09682429]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 280 is [False, True, False, False, True, False]
Current timestep = 281. State = [[-0.00647246 -0.14591114]]. Action = [[0.1883311  0.12076923 0.18809903 0.08703351]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 281 is [False, True, False, True, False, False]
Current timestep = 282. State = [[-0.00582953 -0.14973542]]. Action = [[-0.15345173 -0.1303755  -0.01551373  0.16912222]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 282 is [False, True, False, True, False, False]
Scene graph at timestep 282 is [False, True, False, True, False, False]
State prediction error at timestep 282 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.01832044 -0.16305088]]. Action = [[-0.18534938 -0.07444321  0.03667101  0.04899442]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 283 is [False, True, False, True, False, False]
Current timestep = 284. State = [[-0.0266535 -0.1834634]]. Action = [[ 0.16252822 -0.23630151  0.00035974  0.17691052]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 284 is [False, True, False, True, False, False]
Current timestep = 285. State = [[-0.02604376 -0.21174754]]. Action = [[ 0.02323836 -0.20145012  0.14037532  0.43925905]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 285 is [False, True, False, True, False, False]
Current timestep = 286. State = [[-0.03522944 -0.24468188]]. Action = [[-0.18934946 -0.21244936 -0.17069942  0.63445187]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 286 is [False, True, False, True, False, False]
Current timestep = 287. State = [[-0.0407007  -0.27449518]]. Action = [[ 0.15196031 -0.199791   -0.1745398  -0.7152167 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 287 is [False, True, False, True, False, False]
Current timestep = 288. State = [[-0.04617601 -0.30136728]]. Action = [[-0.12586686 -0.15908672 -0.16743776 -0.11355859]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 288 is [False, True, False, True, False, False]
Current timestep = 289. State = [[-0.04636431 -0.308222  ]]. Action = [[ 0.12141776  0.14693505  0.21763128 -0.9150699 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 289 is [False, True, False, True, False, False]
Current timestep = 290. State = [[-0.04476089 -0.30284664]]. Action = [[-0.13542815  0.04900017 -0.2035277   0.44973397]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 290 is [False, True, False, True, False, False]
Current timestep = 291. State = [[-0.04320638 -0.29651076]]. Action = [[ 0.09925741  0.05523184  0.15589762 -0.229612  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 291 is [False, True, False, True, False, False]
Current timestep = 292. State = [[-0.04192275 -0.29113066]]. Action = [[-0.22209825 -0.18130304 -0.16163151 -0.6301083 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 292 is [False, True, False, True, False, False]
Scene graph at timestep 292 is [False, True, False, True, False, False]
State prediction error at timestep 292 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.04164071 -0.29008788]]. Action = [[-0.13434716 -0.1205509  -0.1467374  -0.76180094]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 293 is [False, True, False, True, False, False]
Current timestep = 294. State = [[-0.04381318 -0.28198949]]. Action = [[-0.16039756  0.15172964  0.11389437 -0.3039627 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 294 is [False, True, False, True, False, False]
Current timestep = 295. State = [[-0.04124166 -0.2597478 ]]. Action = [[ 0.23246294  0.18579942 -0.08918197  0.79629326]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 295 is [False, True, False, True, False, False]
Current timestep = 296. State = [[-0.03659076 -0.23282285]]. Action = [[-0.09659383  0.13741228 -0.14871119 -0.01552701]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 296 is [False, True, False, True, False, False]
Current timestep = 297. State = [[-0.03227962 -0.23105046]]. Action = [[ 0.2150613  -0.21576409  0.04165953  0.23746192]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 297 is [False, True, False, True, False, False]
Current timestep = 298. State = [[-0.02208064 -0.248722  ]]. Action = [[ 0.18015432 -0.17296281 -0.11044198 -0.6965272 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 298 is [False, True, False, True, False, False]
Current timestep = 299. State = [[-0.00030451 -0.27421343]]. Action = [[ 0.14213392 -0.22348264 -0.0846177  -0.66026396]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 299 is [False, True, False, True, False, False]
Current timestep = 300. State = [[ 0.01464053 -0.2919935 ]]. Action = [[-0.02888702  0.02775016 -0.21464254  0.8568299 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 300 is [False, True, False, True, False, False]
Scene graph at timestep 300 is [False, True, False, True, False, False]
State prediction error at timestep 300 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[ 0.01747925 -0.29656273]]. Action = [[ 0.08406726 -0.07162225  0.17456883 -0.90974146]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 301 is [False, True, False, True, False, False]
Current timestep = 302. State = [[ 0.024816  -0.2929698]]. Action = [[-0.01310286  0.18250704 -0.14325294  0.16595447]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 302 is [False, True, False, True, False, False]
Scene graph at timestep 302 is [False, True, False, True, False, False]
State prediction error at timestep 302 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 0
Current timestep = 303. State = [[ 0.02624099 -0.2850304 ]]. Action = [[-0.06111239 -0.17305902  0.1237908   0.4370457 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 303 is [False, True, False, True, False, False]
Scene graph at timestep 303 is [False, True, False, True, False, False]
State prediction error at timestep 303 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of 0
Current timestep = 304. State = [[ 0.02624099 -0.2850304 ]]. Action = [[ 0.23237282 -0.03364223  0.15088296  0.6074853 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 304 is [False, True, False, True, False, False]
Current timestep = 305. State = [[ 0.02624099 -0.2850304 ]]. Action = [[ 0.05944264 -0.210395    0.24279484  0.99208844]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 305 is [False, True, False, True, False, False]
Scene graph at timestep 305 is [False, True, False, True, False, False]
State prediction error at timestep 305 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 0
Current timestep = 306. State = [[ 0.02491327 -0.2817076 ]]. Action = [[-0.19088782  0.09244162  0.1605221  -0.4374647 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 306 is [False, True, False, True, False, False]
Scene graph at timestep 306 is [False, True, False, True, False, False]
State prediction error at timestep 306 is tensor(4.6697e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[ 0.02401739 -0.27747568]]. Action = [[-0.23290342 -0.22348951 -0.17211577 -0.47050458]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 307 is [False, True, False, True, False, False]
Scene graph at timestep 307 is [False, True, False, True, False, False]
State prediction error at timestep 307 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[ 0.02400481 -0.27733472]]. Action = [[ 0.12428117 -0.20224778  0.1363905  -0.5514849 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 308 is [False, True, False, True, False, False]
Current timestep = 309. State = [[ 0.023747   -0.27967694]]. Action = [[ 0.11500797 -0.10355535 -0.13788722 -0.7416775 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 309 is [False, True, False, True, False, False]
Scene graph at timestep 309 is [False, True, False, True, False, False]
State prediction error at timestep 309 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of 0
Current timestep = 310. State = [[ 0.02863942 -0.27330214]]. Action = [[0.15123409 0.1299605  0.1026879  0.23562503]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 310 is [False, True, False, True, False, False]
Scene graph at timestep 310 is [False, True, False, True, False, False]
State prediction error at timestep 310 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[ 0.03702721 -0.26630303]]. Action = [[ 0.19536763  0.16392416 -0.18832384 -0.7558292 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 311 is [False, True, False, True, False, False]
Current timestep = 312. State = [[ 0.03701815 -0.26630223]]. Action = [[ 0.19634664  0.19147128 -0.23875707 -0.9529709 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 312 is [False, True, False, True, False, False]
Scene graph at timestep 312 is [False, True, False, True, False, False]
State prediction error at timestep 312 is tensor(1.0024e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[ 0.0366243  -0.25875616]]. Action = [[-0.18522725  0.15265417 -0.22393245 -0.7454372 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 313 is [False, True, False, True, False, False]
Current timestep = 314. State = [[ 0.03691917 -0.23779117]]. Action = [[ 0.02128828  0.21441355 -0.03117174  0.33735275]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 314 is [False, True, False, True, False, False]
Current timestep = 315. State = [[ 0.03495912 -0.21787997]]. Action = [[-0.10988081  0.04555455 -0.15122622  0.3879454 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 315 is [False, True, False, True, False, False]
Current timestep = 316. State = [[ 0.03290471 -0.20042312]]. Action = [[ 0.1249671   0.15683639 -0.11018682 -0.7503285 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 316 is [False, True, False, True, False, False]
Current timestep = 317. State = [[ 0.03479559 -0.18357721]]. Action = [[ 0.04085079  0.05908898  0.2318618  -0.83038753]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 317 is [False, True, False, True, False, False]
Current timestep = 318. State = [[ 0.03393991 -0.16839592]]. Action = [[-0.13832071  0.1588921  -0.19406264  0.5805371 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 318 is [False, True, False, True, False, False]
Current timestep = 319. State = [[ 0.03357003 -0.1427837 ]]. Action = [[0.17967069 0.21470907 0.2200802  0.23210812]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 319 is [False, True, False, True, False, False]
Scene graph at timestep 319 is [False, True, False, True, False, False]
State prediction error at timestep 319 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[ 0.0311721  -0.10982019]]. Action = [[-0.22552724  0.17876884 -0.03680962 -0.02621782]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 320 is [False, True, False, True, False, False]
Current timestep = 321. State = [[ 0.01572473 -0.10537822]]. Action = [[-0.19685708 -0.1318762  -0.06698853  0.03812873]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 321 is [False, True, False, False, True, False]
Current timestep = 322. State = [[ 0.0040289  -0.10729393]]. Action = [[0.20588696 0.04651403 0.08390528 0.8304937 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 322 is [False, True, False, False, True, False]
Current timestep = 323. State = [[ 0.0088485  -0.11540525]]. Action = [[ 0.1283457  -0.20232521 -0.23100518 -0.14244956]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 323 is [False, True, False, False, True, False]
Scene graph at timestep 323 is [False, True, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[ 0.01337728 -0.12909442]]. Action = [[-0.10576053 -0.02498817  0.16852373 -0.6162757 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 324 is [False, True, False, False, True, False]
Current timestep = 325. State = [[ 0.01542749 -0.12476986]]. Action = [[ 0.12765628  0.12299937 -0.04080309 -0.5486544 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 325 is [False, True, False, True, False, False]
Current timestep = 326. State = [[ 0.01510171 -0.11284761]]. Action = [[-0.17201783  0.1250419  -0.12272324 -0.76735073]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 326 is [False, True, False, False, True, False]
Scene graph at timestep 326 is [False, True, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.24502154 -0.20484616]]. Action = [[0.20642284 0.11917004 0.24261707 0.88226247]]. Reward = [100.]
Curr episode timestep = 88
Scene graph at timestep 327 is [False, True, False, False, True, False]
Current timestep = 328. State = [[-0.2426947  -0.22740303]]. Action = [[-0.20539369  0.08958218 -0.1684739  -0.96037364]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 328 is [True, False, False, True, False, False]
Current timestep = 329. State = [[-0.23199973 -0.24159542]]. Action = [[ 0.19979626 -0.22797216 -0.21846506  0.2423675 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 329 is [True, False, False, True, False, False]
Current timestep = 330. State = [[-0.20938024 -0.26055783]]. Action = [[ 0.1562649  -0.03353834 -0.14584796 -0.5038501 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 330 is [True, False, False, True, False, False]
Scene graph at timestep 330 is [True, False, False, True, False, False]
State prediction error at timestep 330 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.20017779 -0.2594131 ]]. Action = [[-0.23450908  0.21416375 -0.12639824 -0.22832233]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 331 is [True, False, False, True, False, False]
Scene graph at timestep 331 is [True, False, False, True, False, False]
State prediction error at timestep 331 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.2103279 -0.2587419]]. Action = [[-0.03671448 -0.1584194  -0.11027008  0.6995381 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 332 is [True, False, False, True, False, False]
Scene graph at timestep 332 is [True, False, False, True, False, False]
State prediction error at timestep 332 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.2221245  -0.26142702]]. Action = [[-0.16409127  0.14801824 -0.08905898  0.92534184]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 333 is [True, False, False, True, False, False]
Current timestep = 334. State = [[-0.22635333 -0.24983779]]. Action = [[ 0.24097198  0.01607117 -0.0809444  -0.75400674]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 334 is [True, False, False, True, False, False]
Current timestep = 335. State = [[-0.22186665 -0.2504961 ]]. Action = [[-0.04110166 -0.11595647 -0.24421294 -0.553247  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 335 is [True, False, False, True, False, False]
Current timestep = 336. State = [[-0.22026189 -0.24406758]]. Action = [[-0.01019099  0.1940766   0.08414817  0.21161366]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 336 is [True, False, False, True, False, False]
Current timestep = 337. State = [[-0.22632903 -0.22436124]]. Action = [[-0.2081441   0.23390496  0.21177083  0.0535084 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 337 is [True, False, False, True, False, False]
Current timestep = 338. State = [[-0.24385601 -0.21534783]]. Action = [[-0.12587893 -0.13147798 -0.21971865 -0.6067361 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 338 is [True, False, False, True, False, False]
Current timestep = 339. State = [[-0.25010324 -0.20777301]]. Action = [[0.10011378 0.20180362 0.09810901 0.05060089]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 339 is [True, False, False, True, False, False]
Current timestep = 340. State = [[-0.24859257 -0.19470201]]. Action = [[-0.17909244  0.2382828   0.13804728 -0.6236754 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 340 is [True, False, False, True, False, False]
Scene graph at timestep 340 is [True, False, False, True, False, False]
State prediction error at timestep 340 is tensor(9.1678e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.24829778 -0.19244269]]. Action = [[-0.20817383  0.17859799  0.12220299 -0.95498705]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 341 is [True, False, False, True, False, False]
Current timestep = 342. State = [[-0.24160519 -0.18452726]]. Action = [[0.1782943  0.09296885 0.17322549 0.3448788 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 342 is [True, False, False, True, False, False]
Current timestep = 343. State = [[-0.22967824 -0.16557276]]. Action = [[0.10001212 0.1907011  0.01019201 0.4567716 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 343 is [True, False, False, True, False, False]
Current timestep = 344. State = [[-0.22543252 -0.16215022]]. Action = [[-0.0973106  -0.18771917  0.22366711  0.5520148 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 344 is [True, False, False, True, False, False]
Current timestep = 345. State = [[-0.23555379 -0.16704962]]. Action = [[-0.2156907   0.05509931  0.23474395 -0.4111973 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 345 is [True, False, False, True, False, False]
Scene graph at timestep 345 is [True, False, False, True, False, False]
State prediction error at timestep 345 is tensor(4.8931e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.2506075 -0.1645717]]. Action = [[-0.10542855  0.08303407 -0.14226429  0.31775808]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 346 is [True, False, False, True, False, False]
Current timestep = 347. State = [[-0.2591777  -0.15263009]]. Action = [[-0.10184713  0.10927784 -0.04292701  0.7412194 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 347 is [True, False, False, True, False, False]
Current timestep = 348. State = [[-0.26635522 -0.13108408]]. Action = [[0.19352067 0.17636439 0.18797904 0.77705884]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 348 is [True, False, False, True, False, False]
Current timestep = 349. State = [[-0.26419938 -0.11678922]]. Action = [[-0.22131966 -0.11055773  0.02893472  0.49480057]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 349 is [True, False, False, True, False, False]
Current timestep = 350. State = [[-0.2641859  -0.11526829]]. Action = [[-0.10171449  0.22154123  0.06536987 -0.20764196]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 350 is [True, False, False, False, True, False]
Current timestep = 351. State = [[-0.26381433 -0.12581573]]. Action = [[-0.0099092  -0.22372657 -0.12723067 -0.78136295]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 351 is [True, False, False, False, True, False]
Current timestep = 352. State = [[-0.26287827 -0.13589826]]. Action = [[-0.09194881 -0.00771278  0.05970454  0.56001186]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 352 is [True, False, False, True, False, False]
Current timestep = 353. State = [[-0.26325572 -0.13917185]]. Action = [[-0.17564058 -0.19543242 -0.19657178  0.5337297 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 353 is [True, False, False, True, False, False]
Scene graph at timestep 353 is [True, False, False, True, False, False]
State prediction error at timestep 353 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.26343518 -0.13984169]]. Action = [[-0.1672482   0.16777456  0.14120588 -0.15731084]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 354 is [True, False, False, True, False, False]
Scene graph at timestep 354 is [True, False, False, True, False, False]
State prediction error at timestep 354 is tensor(6.5938e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.254515   -0.12893198]]. Action = [[ 0.19922662  0.20866773 -0.14493962  0.5825021 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 355 is [True, False, False, True, False, False]
Current timestep = 356. State = [[-0.23278177 -0.1209479 ]]. Action = [[ 0.21299535 -0.08344457  0.1280252  -0.32199734]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 356 is [True, False, False, True, False, False]
Current timestep = 357. State = [[-0.22207464 -0.13749199]]. Action = [[-0.19906715 -0.221291    0.05835053 -0.7486477 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 357 is [True, False, False, False, True, False]
Current timestep = 358. State = [[-0.22003622 -0.1442409 ]]. Action = [[0.21963352 0.11040092 0.08825168 0.05540395]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 358 is [True, False, False, True, False, False]
Current timestep = 359. State = [[-0.2101887  -0.15019421]]. Action = [[ 0.01955509 -0.1585706   0.12143388 -0.4821571 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 359 is [True, False, False, True, False, False]
Current timestep = 360. State = [[-0.19647433 -0.14950952]]. Action = [[ 0.19226104  0.13185227 -0.16506818 -0.533515  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 360 is [True, False, False, True, False, False]
Scene graph at timestep 360 is [True, False, False, True, False, False]
State prediction error at timestep 360 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of 1
Current timestep = 361. State = [[-0.1779947  -0.14312443]]. Action = [[ 0.0496034  -0.03111091 -0.04966679  0.8426683 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 361 is [True, False, False, True, False, False]
Current timestep = 362. State = [[-0.16864324 -0.14577127]]. Action = [[ 0.11740467 -0.03699091 -0.08423634 -0.01845759]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 362 is [True, False, False, True, False, False]
Scene graph at timestep 362 is [True, False, False, True, False, False]
State prediction error at timestep 362 is tensor(2.2540e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.14454769 -0.1548316 ]]. Action = [[ 0.22627652 -0.11214811 -0.04997951  0.02516174]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 363 is [True, False, False, True, False, False]
Current timestep = 364. State = [[-0.13058367 -0.16116685]]. Action = [[-0.20947212  0.05895898  0.02565184 -0.330988  ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 364 is [True, False, False, True, False, False]
Current timestep = 365. State = [[-0.13178042 -0.17169617]]. Action = [[ 0.13778189 -0.19421513  0.07050347  0.20236897]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 365 is [True, False, False, True, False, False]
Scene graph at timestep 365 is [True, False, False, True, False, False]
State prediction error at timestep 365 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 0
Current timestep = 366. State = [[-0.12111876 -0.19161347]]. Action = [[ 0.1760549  -0.12763996 -0.2466107   0.85604024]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 366 is [True, False, False, True, False, False]
Current timestep = 367. State = [[-0.1075168  -0.19980639]]. Action = [[-0.01956579  0.00390124  0.18488544  0.1040448 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 367 is [True, False, False, True, False, False]
Current timestep = 368. State = [[-0.10158198 -0.20867427]]. Action = [[ 0.10084635 -0.11531776  0.12253803  0.16677427]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 368 is [True, False, False, True, False, False]
Current timestep = 369. State = [[-0.09877864 -0.22600605]]. Action = [[-0.10503682 -0.12893417  0.07796547 -0.13112819]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 369 is [True, False, False, True, False, False]
Current timestep = 370. State = [[-0.0945     -0.23971128]]. Action = [[ 0.17706907 -0.07176493  0.15656376  0.05422437]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 370 is [True, False, False, True, False, False]
Current timestep = 371. State = [[-0.08492514 -0.26050016]]. Action = [[-0.00441651 -0.22834094  0.12472117  0.27267325]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 371 is [True, False, False, True, False, False]
Current timestep = 372. State = [[-0.08702096 -0.2854465 ]]. Action = [[-0.21374945 -0.05761123  0.21054727  0.976385  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 372 is [True, False, False, True, False, False]
Current timestep = 373. State = [[-0.09782598 -0.29600483]]. Action = [[-0.1238279   0.02312273 -0.14233743 -0.94307816]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 373 is [True, False, False, True, False, False]
Current timestep = 374. State = [[-0.11181163 -0.29394457]]. Action = [[-0.13128585  0.0929383  -0.04078983  0.08053792]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 374 is [True, False, False, True, False, False]
Scene graph at timestep 374 is [True, False, False, True, False, False]
State prediction error at timestep 374 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.12517564 -0.28893158]]. Action = [[-0.11868998 -0.16406316  0.0925476   0.642555  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 375 is [True, False, False, True, False, False]
Scene graph at timestep 375 is [True, False, False, True, False, False]
State prediction error at timestep 375 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.12387574 -0.2766762 ]]. Action = [[ 0.03690594  0.21406525 -0.19069554 -0.07082379]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 376 is [True, False, False, True, False, False]
Current timestep = 377. State = [[-0.12108525 -0.26111647]]. Action = [[ 0.04576129 -0.01988868 -0.21203546  0.98681617]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 377 is [True, False, False, True, False, False]
Current timestep = 378. State = [[-0.11253    -0.24409118]]. Action = [[ 0.19478196  0.24439651 -0.10902458 -0.7926417 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 378 is [True, False, False, True, False, False]
Scene graph at timestep 378 is [True, False, False, True, False, False]
State prediction error at timestep 378 is tensor(4.1391e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.09494621 -0.23150948]]. Action = [[ 0.18470484 -0.2399329   0.10475904  0.7067417 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 379 is [True, False, False, True, False, False]
Current timestep = 380. State = [[-0.08146628 -0.25529632]]. Action = [[ 0.08474994 -0.2095135   0.20741978 -0.28890157]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 380 is [True, False, False, True, False, False]
Current timestep = 381. State = [[-0.06903934 -0.262925  ]]. Action = [[0.06554365 0.16589555 0.17975396 0.22626841]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 381 is [True, False, False, True, False, False]
Current timestep = 382. State = [[-0.057834   -0.26683083]]. Action = [[ 0.14126736 -0.17417581  0.21887422  0.8704412 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 382 is [True, False, False, True, False, False]
Scene graph at timestep 382 is [True, False, False, True, False, False]
State prediction error at timestep 382 is tensor(6.7648e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of 1
Current timestep = 383. State = [[-0.04639702 -0.28576744]]. Action = [[-0.20319507 -0.10040559 -0.09108686  0.5802798 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 383 is [True, False, False, True, False, False]
Current timestep = 384. State = [[-0.05700324 -0.30433002]]. Action = [[-0.11854404 -0.13172463  0.04351649 -0.6889799 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 384 is [False, True, False, True, False, False]
Current timestep = 385. State = [[-0.06317428 -0.31519115]]. Action = [[ 0.13761282 -0.12946177  0.22448075  0.67666674]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 385 is [True, False, False, True, False, False]
Current timestep = 386. State = [[-0.06749351 -0.31658953]]. Action = [[-0.08646756  0.06161237  0.21097761  0.925648  ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 386 is [True, False, False, True, False, False]
Current timestep = 387. State = [[-0.07321088 -0.31511742]]. Action = [[ 0.22978246 -0.15522492  0.12032068  0.86875033]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 387 is [True, False, False, True, False, False]
Current timestep = 388. State = [[-0.07392137 -0.31494305]]. Action = [[ 0.06641203  0.01267147 -0.06651048  0.03586841]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 388 is [True, False, False, True, False, False]
Current timestep = 389. State = [[-0.07585359 -0.30314144]]. Action = [[-0.06853241  0.23028508  0.07670161  0.71669173]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 389 is [True, False, False, True, False, False]
Current timestep = 390. State = [[-0.07775107 -0.27289867]]. Action = [[ 0.09683764  0.22872835 -0.01448019  0.89539695]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 390 is [True, False, False, True, False, False]
Current timestep = 391. State = [[-0.08051659 -0.24424903]]. Action = [[-0.19152288  0.14884585 -0.22716582 -0.6390753 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 391 is [True, False, False, True, False, False]
Current timestep = 392. State = [[-0.09014518 -0.2185699 ]]. Action = [[ 0.0966588   0.1688726  -0.21629944 -0.45147312]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 392 is [True, False, False, True, False, False]
Current timestep = 393. State = [[-0.09404049 -0.19516349]]. Action = [[-0.20810321  0.15855503 -0.16912727  0.1389463 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 393 is [True, False, False, True, False, False]
Current timestep = 394. State = [[-0.10842713 -0.18264423]]. Action = [[ 0.02321181 -0.03753684  0.01791862  0.8575423 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 394 is [True, False, False, True, False, False]
Current timestep = 395. State = [[-0.10469063 -0.18640849]]. Action = [[ 0.18857938 -0.1135771  -0.1121653   0.5612922 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 395 is [True, False, False, True, False, False]
Current timestep = 396. State = [[-0.10237957 -0.1851868 ]]. Action = [[-0.05394855  0.1023941   0.18208864 -0.02980411]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 396 is [True, False, False, True, False, False]
Current timestep = 397. State = [[-0.09597035 -0.19298731]]. Action = [[ 0.20722193 -0.20933515  0.15004915  0.7020259 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 397 is [True, False, False, True, False, False]
Scene graph at timestep 397 is [True, False, False, True, False, False]
State prediction error at timestep 397 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of 1
Current timestep = 398. State = [[-0.0831668  -0.19229668]]. Action = [[0.02512172 0.23279226 0.15176004 0.7249291 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 398 is [True, False, False, True, False, False]
Scene graph at timestep 398 is [True, False, False, True, False, False]
State prediction error at timestep 398 is tensor(5.1553e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of 1
Current timestep = 399. State = [[-0.08137607 -0.18709026]]. Action = [[-0.09998047 -0.16780244 -0.20277403 -0.52439755]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 399 is [True, False, False, True, False, False]
Current timestep = 400. State = [[-0.08281339 -0.18366598]]. Action = [[-0.06333175  0.23406288 -0.16318142  0.8048154 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 400 is [True, False, False, True, False, False]
Current timestep = 401. State = [[-0.08175641 -0.159137  ]]. Action = [[ 0.11788213  0.2134667  -0.17817797 -0.04241395]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 401 is [True, False, False, True, False, False]
Current timestep = 402. State = [[-0.07676714 -0.13970152]]. Action = [[ 0.12662566  0.01929787 -0.05671372 -0.36279804]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 402 is [True, False, False, True, False, False]
Current timestep = 403. State = [[-0.06252594 -0.14007579]]. Action = [[ 0.2422609  -0.11859539 -0.228669   -0.41859794]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 403 is [True, False, False, True, False, False]
Scene graph at timestep 403 is [True, False, False, True, False, False]
State prediction error at timestep 403 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of 1
Current timestep = 404. State = [[-0.04079654 -0.14559935]]. Action = [[-0.04793857 -0.04962739  0.18324566  0.22469401]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 404 is [True, False, False, True, False, False]
Scene graph at timestep 404 is [False, True, False, True, False, False]
State prediction error at timestep 404 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of 1
Current timestep = 405. State = [[-0.04252032 -0.13931075]]. Action = [[-0.10897732  0.20889485  0.01988465 -0.7794402 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 405 is [False, True, False, True, False, False]
Current timestep = 406. State = [[-0.0391024  -0.12153244]]. Action = [[ 0.19834048  0.11207888 -0.13152553 -0.27870297]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 406 is [False, True, False, True, False, False]
Scene graph at timestep 406 is [False, True, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[-0.2578618  -0.00939409]]. Action = [[ 0.01888007  0.12537688 -0.1409454  -0.67440194]]. Reward = [100.]
Curr episode timestep = 79
Scene graph at timestep 407 is [False, True, False, False, True, False]
Current timestep = 408. State = [[-0.2549502  -0.00220637]]. Action = [[ 0.05910081  0.17396373 -0.16701001 -0.03628212]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 408 is [True, False, False, False, True, False]
Current timestep = 409. State = [[-0.2531925   0.00559455]]. Action = [[-0.15320407  0.14099449 -0.06637004  0.7437105 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 409 is [True, False, False, False, True, False]
Current timestep = 410. State = [[-0.24729246  0.00654404]]. Action = [[ 0.10056078 -0.00398119  0.22280031  0.05533683]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 410 is [True, False, False, False, True, False]
Current timestep = 411. State = [[-0.2396823   0.00706053]]. Action = [[-0.20342757  0.14971805  0.0720011  -0.78175294]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 411 is [True, False, False, False, True, False]
Current timestep = 412. State = [[-0.24260531  0.00092066]]. Action = [[-0.17171073 -0.14812963  0.1288602   0.67052495]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 412 is [True, False, False, False, True, False]
Current timestep = 413. State = [[-0.24686636  0.00379135]]. Action = [[ 0.04383865  0.18489051 -0.00608324  0.7734293 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 413 is [True, False, False, False, True, False]
Current timestep = 414. State = [[-0.24986197  0.00296055]]. Action = [[-0.08652993 -0.18544559  0.23244765 -0.10524631]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 414 is [True, False, False, False, True, False]
Current timestep = 415. State = [[-0.24906842  0.00038902]]. Action = [[ 0.13597542  0.12416664 -0.03841022 -0.97178155]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 415 is [True, False, False, False, True, False]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of 0
Current timestep = 416. State = [[-0.23955812  0.00668975]]. Action = [[ 0.18424499  0.02467555  0.00056615 -0.43019497]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 416 is [True, False, False, False, True, False]
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(8.3922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of 1
Current timestep = 417. State = [[-0.22720322  0.00993655]]. Action = [[-0.21203338 -0.01693365 -0.09695768  0.1456927 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 417 is [True, False, False, False, True, False]
Current timestep = 418. State = [[-0.23934263 -0.00368396]]. Action = [[-0.12976384 -0.22108166  0.00254112  0.40747035]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.24305902 -0.02736747]]. Action = [[ 0.23016518 -0.14932793  0.0991354   0.58920014]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 419 is [True, False, False, False, True, False]
Current timestep = 420. State = [[-0.22588654 -0.05194292]]. Action = [[ 0.22892308 -0.21574268 -0.02735773  0.43928027]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 420 is [True, False, False, False, True, False]
Current timestep = 421. State = [[-0.2129084  -0.06296685]]. Action = [[-0.13696593  0.13330358 -0.12109873  0.6247642 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 421 is [True, False, False, False, True, False]
Current timestep = 422. State = [[-0.21217854 -0.05858428]]. Action = [[ 0.11155677  0.04117623 -0.17670196 -0.27097642]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(3.5732e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 0
Current timestep = 423. State = [[-0.20097391 -0.04569976]]. Action = [[ 0.1912795   0.10754567 -0.08134446  0.7541623 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 423 is [True, False, False, False, True, False]
Current timestep = 424. State = [[-0.18439586 -0.03834085]]. Action = [[ 0.01936203  0.02210724 -0.0173474   0.4094807 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 424 is [True, False, False, False, True, False]
Current timestep = 425. State = [[-0.18099084 -0.03140762]]. Action = [[0.05097485 0.07795572 0.0991084  0.2650355 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 425 is [True, False, False, False, True, False]
Current timestep = 426. State = [[-0.17863157 -0.01790243]]. Action = [[-0.12582998  0.1268332  -0.13914333 -0.065413  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 426 is [True, False, False, False, True, False]
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(5.9472e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.1846862   0.01014136]]. Action = [[-0.07773563  0.23924446 -0.15884486  0.6842941 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 427 is [True, False, False, False, True, False]
Current timestep = 428. State = [[-0.19104409  0.04057353]]. Action = [[ 0.04583722  0.23823744 -0.20034997  0.30917478]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.18746477  0.05669575]]. Action = [[ 0.11038312 -0.03861409 -0.15002769  0.87487626]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.17704669  0.06601522]]. Action = [[ 0.07616341  0.13358569 -0.15037757 -0.1555109 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 430 is [True, False, False, False, True, False]
Current timestep = 431. State = [[-0.16730803  0.0810829 ]]. Action = [[0.02596673 0.09211713 0.19602835 0.9192823 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 431 is [True, False, False, False, True, False]
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.15480185  0.07963529]]. Action = [[ 0.15672296 -0.18092513  0.16078565  0.8683249 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 432 is [True, False, False, False, True, False]
Current timestep = 433. State = [[-0.1477521   0.07917091]]. Action = [[-0.07539059  0.13790411 -0.21827574 -0.6090281 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.15494351  0.09618575]]. Action = [[-0.13351287  0.19931072  0.21043742  0.3577087 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 434 is [True, False, False, False, True, False]
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.15839319  0.11719453]]. Action = [[ 0.16142547  0.03252783  0.02301589 -0.63688254]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 435 is [True, False, False, False, True, False]
Current timestep = 436. State = [[-0.15785085  0.13099702]]. Action = [[-0.24077597  0.16361591  0.01835749 -0.5457032 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 436 is [True, False, False, False, True, False]
Current timestep = 437. State = [[-0.15685189  0.1312454 ]]. Action = [[ 0.23758388 -0.21026771 -0.19155    -0.13179779]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 437 is [True, False, False, False, False, True]
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(7.3649e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.14778645  0.10718487]]. Action = [[-0.0303687  -0.17775571 -0.17104527 -0.12585843]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 438 is [True, False, False, False, False, True]
Current timestep = 439. State = [[-0.14062835  0.10607363]]. Action = [[ 0.19290096  0.2036941  -0.15499347 -0.958908  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 439 is [True, False, False, False, True, False]
Current timestep = 440. State = [[-0.12189589  0.12873106]]. Action = [[0.12121177 0.23317033 0.13075542 0.6753721 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 440 is [True, False, False, False, True, False]
Current timestep = 441. State = [[-0.09993935  0.15496174]]. Action = [[ 0.23521924  0.16729087  0.20905364 -0.6319177 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 441 is [True, False, False, False, False, True]
Current timestep = 442. State = [[-0.07404915  0.16841143]]. Action = [[ 0.04199046 -0.06148788 -0.06014314 -0.02766526]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 442 is [True, False, False, False, False, True]
Current timestep = 443. State = [[-0.06814289  0.17459449]]. Action = [[-0.0221778   0.09675291  0.00201342  0.3629483 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 443 is [True, False, False, False, False, True]
Current timestep = 444. State = [[-0.05708308  0.17171808]]. Action = [[ 0.18729877 -0.13779555  0.1789701   0.7389277 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 444 is [True, False, False, False, False, True]
Current timestep = 445. State = [[-0.03499602  0.15132701]]. Action = [[ 0.16004306 -0.22597417 -0.14723794  0.7263653 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 445 is [True, False, False, False, False, True]
Current timestep = 446. State = [[-0.01041519  0.12490937]]. Action = [[ 0.19567502 -0.18308349  0.16754013 -0.48478758]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 446 is [False, True, False, False, False, True]
Current timestep = 447. State = [[-0.2493256  -0.01700575]]. Action = [[-0.06509507 -0.20323846 -0.22214317  0.83839345]]. Reward = [100.]
Curr episode timestep = 39
Scene graph at timestep 447 is [False, True, False, False, True, False]
Current timestep = 448. State = [[-0.24613264 -0.01890332]]. Action = [[-0.20237346  0.09755883  0.08442032  0.4876032 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 448 is [True, False, False, False, True, False]
Current timestep = 449. State = [[-0.2445595  -0.00966249]]. Action = [[ 0.04383555  0.19667286  0.11745721 -0.14461792]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 449 is [True, False, False, False, True, False]
Current timestep = 450. State = [[-0.23228627  0.01100414]]. Action = [[0.1819737  0.19441485 0.10017601 0.45597625]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 450 is [True, False, False, False, True, False]
Current timestep = 451. State = [[-0.22399446  0.025765  ]]. Action = [[-0.20044282 -0.01349089  0.16653612  0.44110274]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 451 is [True, False, False, False, True, False]
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of 1
Current timestep = 452. State = [[-0.22569115  0.03024395]]. Action = [[ 0.12428272  0.00211734 -0.2413675  -0.00628936]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 452 is [True, False, False, False, True, False]
Current timestep = 453. State = [[-0.22776328  0.04177394]]. Action = [[-0.0978369   0.19270036  0.15291199 -0.26159048]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 453 is [True, False, False, False, True, False]
Current timestep = 454. State = [[-0.23202975  0.05280526]]. Action = [[ 0.03163311 -0.01248279  0.07859346  0.12380755]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 454 is [True, False, False, False, True, False]
Current timestep = 455. State = [[-0.22046009  0.04783635]]. Action = [[ 0.23482507 -0.12489116 -0.17965586 -0.291407  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 455 is [True, False, False, False, True, False]
Current timestep = 456. State = [[-0.19760104  0.0451717 ]]. Action = [[ 0.14124113  0.02344573 -0.05084968  0.7829988 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 456 is [True, False, False, False, True, False]
Current timestep = 457. State = [[-0.17446545  0.03819367]]. Action = [[ 0.19259253 -0.10942575  0.21985179 -0.19450879]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 457 is [True, False, False, False, True, False]
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of 1
Current timestep = 458. State = [[-0.14447947  0.04037896]]. Action = [[ 0.18660283  0.20082489  0.20248789 -0.06688923]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 458 is [True, False, False, False, True, False]
Current timestep = 459. State = [[-0.1316107  0.0476443]]. Action = [[-0.11487678 -0.06686723 -0.06414543  0.74823   ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 459 is [True, False, False, False, True, False]
Current timestep = 460. State = [[-0.13518293  0.03327382]]. Action = [[-0.15541753 -0.22450343 -0.22758932 -0.3096639 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 460 is [True, False, False, False, True, False]
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 1
Current timestep = 461. State = [[-0.14265709  0.01604062]]. Action = [[ 0.08138844  0.03875753 -0.00862344 -0.79518306]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 461 is [True, False, False, False, True, False]
Current timestep = 462. State = [[-0.134049    0.02755382]]. Action = [[ 0.24225181  0.18214643 -0.11092544 -0.45315874]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 462 is [True, False, False, False, True, False]
Current timestep = 463. State = [[-0.11020782  0.03382462]]. Action = [[ 0.15660807 -0.11035794  0.06067318 -0.27831566]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 463 is [True, False, False, False, True, False]
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of 1
Current timestep = 464. State = [[-0.0963092   0.02956243]]. Action = [[-0.00876194 -0.05554961 -0.11761202 -0.7779669 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 464 is [True, False, False, False, True, False]
Current timestep = 465. State = [[-0.09935732  0.0378615 ]]. Action = [[-0.12803727  0.22489482  0.02697629 -0.51700395]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 465 is [True, False, False, False, True, False]
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(4.0684e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 0
Current timestep = 466. State = [[-0.10809354  0.04267867]]. Action = [[-0.16312845 -0.19253479 -0.04720046  0.74769497]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 466 is [True, False, False, False, True, False]
Current timestep = 467. State = [[-0.11039433  0.02646388]]. Action = [[ 0.10502875 -0.08007036  0.09199324  0.86050594]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 467 is [True, False, False, False, True, False]
Current timestep = 468. State = [[-0.11382563  0.01614961]]. Action = [[-0.18352291 -0.05471084 -0.10045575  0.5096245 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 468 is [True, False, False, False, True, False]
Current timestep = 469. State = [[-0.11637092 -0.00111858]]. Action = [[ 0.1571261  -0.18346477  0.16634566  0.03454864]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 469 is [True, False, False, False, True, False]
Current timestep = 470. State = [[-0.10904923 -0.01877308]]. Action = [[ 0.14694643 -0.08715266  0.19950396 -0.72832596]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 470 is [True, False, False, False, True, False]
Current timestep = 471. State = [[-0.10432225 -0.02590073]]. Action = [[-0.04788493  0.04670772  0.1108658  -0.03630495]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 471 is [True, False, False, False, True, False]
Current timestep = 472. State = [[-0.10368512 -0.0363808 ]]. Action = [[-0.04538983 -0.18361044 -0.07824692  0.51136994]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 472 is [True, False, False, False, True, False]
Current timestep = 473. State = [[-0.0951122  -0.05515263]]. Action = [[ 0.23821121 -0.13815805 -0.17475988  0.63138795]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 473 is [True, False, False, False, True, False]
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[-0.07404927 -0.0824362 ]]. Action = [[ 0.21078262 -0.20456691 -0.24252132  0.74021924]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 474 is [True, False, False, False, True, False]
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of 1
Current timestep = 475. State = [[-0.04499798 -0.09039415]]. Action = [[ 0.1965411   0.16997245  0.14085165 -0.200234  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 475 is [True, False, False, False, True, False]
Current timestep = 476. State = [[-0.1609396   0.00057218]]. Action = [[ 0.21152478  0.03061298 -0.14184715  0.3047974 ]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 476 is [False, True, False, False, True, False]
Current timestep = 477. State = [[-0.14597878  0.00127887]]. Action = [[ 0.00358874  0.03214511  0.06352791 -0.90232867]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 477 is [True, False, False, False, True, False]
Current timestep = 478. State = [[-0.13813126  0.01311274]]. Action = [[0.18338469 0.1708951  0.0428699  0.99362135]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 478 is [True, False, False, False, True, False]
Current timestep = 479. State = [[-0.12239604  0.01092027]]. Action = [[-0.03355923 -0.23230378 -0.06377573  0.6598954 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 479 is [True, False, False, False, True, False]
Current timestep = 480. State = [[-0.12499171  0.01071925]]. Action = [[-0.08990118  0.1716795  -0.22587243 -0.4664166 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 480 is [True, False, False, False, True, False]
Current timestep = 481. State = [[-0.11892682  0.01464893]]. Action = [[ 0.22665447 -0.05214959 -0.14420038 -0.7833115 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 481 is [True, False, False, False, True, False]
Current timestep = 482. State = [[-0.1096052   0.01693844]]. Action = [[-0.12431151  0.04807904  0.20810562 -0.5815082 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 482 is [True, False, False, False, True, False]
Current timestep = 483. State = [[-0.11968828  0.03125053]]. Action = [[-0.21859954  0.2097652   0.09063831 -0.64634764]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 483 is [True, False, False, False, True, False]
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 1
Current timestep = 484. State = [[-0.12601015  0.04204944]]. Action = [[ 0.19442695 -0.15840507  0.01903698 -0.17549771]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 484 is [True, False, False, False, True, False]
Current timestep = 485. State = [[-0.12549928  0.03886234]]. Action = [[-0.12861001  0.09888551 -0.14277133 -0.20351744]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 485 is [True, False, False, False, True, False]
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(1.2147e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of 1
Current timestep = 486. State = [[-0.12248524  0.04681614]]. Action = [[0.21955156 0.1080744  0.0219835  0.85651183]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 486 is [True, False, False, False, True, False]
Current timestep = 487. State = [[-0.10825218  0.06623995]]. Action = [[ 0.11740738  0.21978873 -0.20224383 -0.35975552]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 487 is [True, False, False, False, True, False]
Current timestep = 488. State = [[-0.08639229  0.07933252]]. Action = [[ 0.21026883 -0.06144345 -0.11200419  0.36764205]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 488 is [True, False, False, False, True, False]
Current timestep = 489. State = [[-0.06513306  0.06837507]]. Action = [[ 0.05042914 -0.20212777  0.12940761 -0.9721304 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 489 is [True, False, False, False, True, False]
Current timestep = 490. State = [[-0.05684111  0.04866615]]. Action = [[ 0.00100991 -0.12678444  0.14059699  0.47032166]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 490 is [True, False, False, False, True, False]
Current timestep = 491. State = [[-0.05793549  0.04530685]]. Action = [[-0.11361532  0.11844987 -0.12711541  0.01985502]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 491 is [True, False, False, False, True, False]
Current timestep = 492. State = [[-0.05618963  0.04832257]]. Action = [[ 0.17301977 -0.00688858  0.04975474 -0.5539883 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 492 is [True, False, False, False, True, False]
Current timestep = 493. State = [[-0.04244944  0.04397895]]. Action = [[ 0.17785147 -0.07097419  0.22884238 -0.19330966]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 493 is [True, False, False, False, True, False]
Current timestep = 494. State = [[-0.2709993   0.06773621]]. Action = [[-0.12049039 -0.22126369  0.05313325  0.25958848]]. Reward = [100.]
Curr episode timestep = 17
Scene graph at timestep 494 is [False, True, False, False, True, False]
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0274, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[-0.25901824  0.06702965]]. Action = [[ 0.18314266 -0.18135154  0.1815806   0.15863037]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 495 is [True, False, False, False, True, False]
Current timestep = 496. State = [[-0.24793239  0.05886008]]. Action = [[-0.16534808  0.09356782 -0.23951297  0.53697705]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.2462237   0.05572455]]. Action = [[-0.20553474  0.223759   -0.06264701  0.18928695]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(6.7422e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[-0.24275693  0.04961507]]. Action = [[ 0.05887714 -0.10132766  0.17879355  0.55726767]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 498 is [True, False, False, False, True, False]
Current timestep = 499. State = [[-0.22737353  0.03424519]]. Action = [[ 0.22901407 -0.1738036   0.08500385 -0.28445244]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 499 is [True, False, False, False, True, False]
Current timestep = 500. State = [[-0.21549693  0.03311817]]. Action = [[-0.20992735  0.21972758 -0.18835025 -0.59888184]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 500 is [True, False, False, False, True, False]
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of 0
Current timestep = 501. State = [[-0.22867167  0.03723646]]. Action = [[-0.18445657 -0.15358682  0.17210281  0.63852346]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.24789067  0.02273096]]. Action = [[-0.20809102 -0.05573834  0.02624175 -0.46080285]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 502 is [True, False, False, False, True, False]
Current timestep = 503. State = [[-0.26627707  0.02961357]]. Action = [[-0.00194116  0.2002317  -0.00384106  0.9105902 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 503 is [True, False, False, False, True, False]
Current timestep = 504. State = [[-0.27509564  0.05681769]]. Action = [[ 0.01644528  0.23482805 -0.11691642  0.34212732]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 504 is [True, False, False, False, True, False]
Current timestep = 505. State = [[-0.28082985  0.07329614]]. Action = [[-0.14493582 -0.21842313  0.18213964 -0.89506996]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 505 is [True, False, False, False, True, False]
Current timestep = 506. State = [[-0.27697307  0.09014105]]. Action = [[ 0.1233274   0.2365115  -0.1633524  -0.17931652]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 506 is [True, False, False, False, True, False]
Current timestep = 507. State = [[-0.26913825  0.10626884]]. Action = [[-0.06315342 -0.18894878  0.03070191 -0.13410598]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 507 is [True, False, False, False, True, False]
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.26303107  0.11375507]]. Action = [[ 0.10728899  0.0884769  -0.14507683  0.7184739 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 508 is [True, False, False, False, True, False]
Current timestep = 509. State = [[-0.24294822  0.12730065]]. Action = [[ 0.23282367  0.11598694  0.05109608 -0.3392414 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 509 is [True, False, False, False, True, False]
Scene graph at timestep 509 is [True, False, False, False, False, True]
State prediction error at timestep 509 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.21209368  0.13058913]]. Action = [[ 0.199929   -0.17657378  0.24549839 -0.4842708 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 510 is [True, False, False, False, False, True]
Current timestep = 511. State = [[-0.18822305  0.11686814]]. Action = [[ 0.15131077 -0.07409935 -0.01603808  0.6983477 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 511 is [True, False, False, False, False, True]
Current timestep = 512. State = [[-0.16629957  0.10782759]]. Action = [[ 0.11774963 -0.05717766 -0.00491646  0.8982842 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 512 is [True, False, False, False, True, False]
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of 1
Current timestep = 513. State = [[-0.15121561  0.08791475]]. Action = [[-0.12111592 -0.23747092  0.18025601  0.3620435 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 513 is [True, False, False, False, True, False]
Current timestep = 514. State = [[-0.15503338  0.05931905]]. Action = [[-0.09566775 -0.2192453   0.16640943  0.7760818 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 514 is [True, False, False, False, True, False]
Current timestep = 515. State = [[-0.16469525  0.0529828 ]]. Action = [[-0.07550834  0.17693132  0.1547494   0.46212137]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 515 is [True, False, False, False, True, False]
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[-0.16563143  0.05389059]]. Action = [[ 0.23879468 -0.11424553 -0.05830273 -0.06336272]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 516 is [True, False, False, False, True, False]
Current timestep = 517. State = [[-0.15670176  0.05524091]]. Action = [[0.07279268 0.16853842 0.03350794 0.65618277]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 517 is [True, False, False, False, True, False]
Current timestep = 518. State = [[-0.15181777  0.06349045]]. Action = [[ 0.01853803  0.03165728  0.17563733 -0.8528996 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 518 is [True, False, False, False, True, False]
Current timestep = 519. State = [[-0.14102899  0.06106224]]. Action = [[ 0.11654913 -0.12150326  0.15436465  0.4131577 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 519 is [True, False, False, False, True, False]
Current timestep = 520. State = [[-0.12354682  0.0687116 ]]. Action = [[ 0.20437664  0.2153337  -0.18192394 -0.360963  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.1040323  0.0685525]]. Action = [[-0.04346344 -0.21566324 -0.17665564 -0.4096747 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 521 is [True, False, False, False, True, False]
Current timestep = 522. State = [[-0.09360455  0.0643837 ]]. Action = [[ 0.21250254  0.09537128  0.1810607  -0.44054353]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.08122479  0.07009706]]. Action = [[-0.06558704  0.04638326 -0.21851996 -0.1349436 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 523 is [True, False, False, False, True, False]
Current timestep = 524. State = [[-0.07663334  0.06065698]]. Action = [[ 0.1307987  -0.20255631  0.23822883 -0.6641227 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.07227226  0.03527186]]. Action = [[-0.16888274 -0.23039171  0.08435923 -0.37666065]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 525 is [True, False, False, False, True, False]
Current timestep = 526. State = [[-0.07033359  0.010602  ]]. Action = [[ 0.13438416 -0.14278546 -0.20788929  0.6928351 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.05926745 -0.01640277]]. Action = [[ 0.19002074 -0.20447914 -0.15052173 -0.29138845]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 527 is [True, False, False, False, True, False]
Current timestep = 528. State = [[-0.04036699 -0.04069132]]. Action = [[ 0.10105819 -0.09257495  0.15412146  0.4493711 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 528 is [True, False, False, False, True, False]
Current timestep = 529. State = [[-0.23340449 -0.07130818]]. Action = [[ 0.14625955 -0.09090661 -0.18426557 -0.08649933]]. Reward = [100.]
Curr episode timestep = 34
Scene graph at timestep 529 is [False, True, False, False, True, False]
Current timestep = 530. State = [[-0.22175579 -0.08050384]]. Action = [[ 0.15393108 -0.03103077 -0.17448786 -0.50171363]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 530 is [True, False, False, False, True, False]
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of 1
Current timestep = 531. State = [[-0.20453103 -0.07442988]]. Action = [[0.04839015 0.18049592 0.0220629  0.9707403 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 531 is [True, False, False, False, True, False]
Current timestep = 532. State = [[-0.20720176 -0.06806941]]. Action = [[-0.20675808 -0.01722713  0.04081392 -0.5445045 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 532 is [True, False, False, False, True, False]
Current timestep = 533. State = [[-0.21062203 -0.06983159]]. Action = [[ 1.1691451e-04 -5.2271277e-02  1.9133341e-01 -4.8124480e-01]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 533 is [True, False, False, False, True, False]
Current timestep = 534. State = [[-0.21048546 -0.07159574]]. Action = [[ 0.12149492  0.02768502  0.21594638 -0.63922745]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.21262743 -0.06393383]]. Action = [[-0.14471762  0.11276215  0.24163765  0.15220928]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.21687606 -0.04512167]]. Action = [[ 0.04196408  0.16664454 -0.14969896 -0.73275214]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 536 is [True, False, False, False, True, False]
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.21974944 -0.03528293]]. Action = [[ 0.01393789 -0.0971878  -0.23394679 -0.44225836]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 537 is [True, False, False, False, True, False]
Current timestep = 538. State = [[-0.21779008 -0.04746171]]. Action = [[ 0.04632172 -0.14079852 -0.13313383  0.429255  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 538 is [True, False, False, False, True, False]
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of 1
Current timestep = 539. State = [[-0.21658847 -0.0666135 ]]. Action = [[-0.09380077 -0.12118423 -0.01532683 -0.11969191]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 539 is [True, False, False, False, True, False]
Current timestep = 540. State = [[-0.21938306 -0.08747532]]. Action = [[ 0.01615718 -0.19267403  0.07223809 -0.52340466]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.22076192 -0.09783211]]. Action = [[-0.05785286  0.07710624 -0.06497949  0.7408757 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 541 is [True, False, False, False, True, False]
Current timestep = 542. State = [[-0.22723359 -0.0861666 ]]. Action = [[-0.08634785  0.19789624 -0.11107419  0.77038145]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 542 is [True, False, False, False, True, False]
Current timestep = 543. State = [[-0.24027754 -0.0749629 ]]. Action = [[-0.14771114 -0.01883842 -0.16668314  0.9028616 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 543 is [True, False, False, False, True, False]
Current timestep = 544. State = [[-0.24787298 -0.07199775]]. Action = [[ 0.0145773   0.03295836  0.16624123 -0.68682057]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 544 is [True, False, False, False, True, False]
Current timestep = 545. State = [[-0.25423157 -0.08270869]]. Action = [[-0.12324297 -0.20922859 -0.02901554  0.33992052]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 545 is [True, False, False, False, True, False]
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of -1
Current timestep = 546. State = [[-0.25807768 -0.09887671]]. Action = [[ 0.24271044 -0.03517239 -0.08532381 -0.7342526 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.25052053 -0.09188184]]. Action = [[ 0.00900584  0.1461938   0.1092211  -0.8354603 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 547 is [True, False, False, False, True, False]
Current timestep = 548. State = [[-0.2430614  -0.07414289]]. Action = [[ 0.09531218  0.15886709  0.03614238 -0.9103139 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 548 is [True, False, False, False, True, False]
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[-0.22929737 -0.06537572]]. Action = [[ 0.15153229 -0.10625888  0.13188154 -0.7961012 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 549 is [True, False, False, False, True, False]
Current timestep = 550. State = [[-0.20656392 -0.05974272]]. Action = [[0.20754927 0.17033195 0.15730476 0.23957384]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 550 is [True, False, False, False, True, False]
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.17717108 -0.05834841]]. Action = [[ 0.18755412 -0.158857   -0.09436718 -0.18754506]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 551 is [True, False, False, False, True, False]
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.16177642 -0.08044253]]. Action = [[-0.11324215 -0.20384829 -0.13198742  0.2599591 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 552 is [True, False, False, False, True, False]
Current timestep = 553. State = [[-0.15791029 -0.10726699]]. Action = [[ 0.19698071 -0.21850106  0.08373326 -0.97537154]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 553 is [True, False, False, False, True, False]
Current timestep = 554. State = [[-0.13809887 -0.12617175]]. Action = [[ 0.18020803 -0.01302227 -0.12282103  0.74447393]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.12919481 -0.13589896]]. Action = [[-0.20981658 -0.06462541  0.04693711 -0.21030891]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 555 is [True, False, False, True, False, False]
Current timestep = 556. State = [[-0.12485982 -0.15054014]]. Action = [[ 0.23859832 -0.16431078  0.22431692  0.6219053 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 556 is [True, False, False, True, False, False]
Current timestep = 557. State = [[-0.11950111 -0.17477022]]. Action = [[-0.04825979 -0.21111484  0.10487849  0.98111415]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 557 is [True, False, False, True, False, False]
Scene graph at timestep 557 is [True, False, False, True, False, False]
State prediction error at timestep 557 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 557 of -1
Current timestep = 558. State = [[-0.12363333 -0.19852592]]. Action = [[-0.18135716 -0.01882456  0.21429849 -0.89731   ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 558 is [True, False, False, True, False, False]
Current timestep = 559. State = [[-0.14006811 -0.2056371 ]]. Action = [[-0.2324822   0.00121704  0.24885496 -0.37061036]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 559 is [True, False, False, True, False, False]
Current timestep = 560. State = [[-0.1460982  -0.19315758]]. Action = [[0.24357355 0.21937317 0.03508475 0.56230783]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 560 is [True, False, False, True, False, False]
Current timestep = 561. State = [[-0.14604767 -0.17807446]]. Action = [[-0.18648662  0.03249067  0.01592588 -0.82837874]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 561 is [True, False, False, True, False, False]
Current timestep = 562. State = [[-0.15414304 -0.16150017]]. Action = [[-0.10472925  0.2029792  -0.0772042   0.31158376]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 562 is [True, False, False, True, False, False]
Current timestep = 563. State = [[-0.17431302 -0.15204626]]. Action = [[-0.22277525 -0.09592134  0.13842687 -0.3216653 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 563 is [True, False, False, True, False, False]
Current timestep = 564. State = [[-0.1903301  -0.15741622]]. Action = [[ 0.06266305 -0.06230456 -0.22138278 -0.9577007 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 564 is [True, False, False, True, False, False]
Current timestep = 565. State = [[-0.1946571  -0.17043598]]. Action = [[-0.07976106 -0.14672299  0.12833494  0.60570455]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 565 is [True, False, False, True, False, False]
Current timestep = 566. State = [[-0.19754554 -0.18745568]]. Action = [[ 0.1144301  -0.12780303 -0.09041388 -0.43498993]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 566 is [True, False, False, True, False, False]
Current timestep = 567. State = [[-0.19475392 -0.18803594]]. Action = [[0.0333955  0.16977912 0.23989612 0.5927148 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 567 is [True, False, False, True, False, False]
Current timestep = 568. State = [[-0.19121097 -0.16975503]]. Action = [[-0.0456176   0.22583964 -0.1299527  -0.76083577]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 568 is [True, False, False, True, False, False]
Current timestep = 569. State = [[-0.18621987 -0.14080143]]. Action = [[ 0.1605807   0.17778677 -0.13449985 -0.06705356]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 569 is [True, False, False, True, False, False]
Current timestep = 570. State = [[-0.17778642 -0.13138583]]. Action = [[ 0.08607334 -0.14111759  0.16765672  0.36309803]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 570 is [True, False, False, True, False, False]
Scene graph at timestep 570 is [True, False, False, True, False, False]
State prediction error at timestep 570 is tensor(9.9688e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of 0
Current timestep = 571. State = [[-0.17546764 -0.13166876]]. Action = [[-0.18148674  0.07501909  0.19576427 -0.02964026]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 571 is [True, False, False, True, False, False]
Scene graph at timestep 571 is [True, False, False, True, False, False]
State prediction error at timestep 571 is tensor(2.1048e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.18755078 -0.13497554]]. Action = [[-0.19809353 -0.07637364  0.01079997 -0.6471679 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 572 is [True, False, False, True, False, False]
Scene graph at timestep 572 is [True, False, False, True, False, False]
State prediction error at timestep 572 is tensor(5.3692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.20930779 -0.14666463]]. Action = [[-0.04936281 -0.07818344  0.13950187  0.01962221]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 573 is [True, False, False, True, False, False]
Scene graph at timestep 573 is [True, False, False, True, False, False]
State prediction error at timestep 573 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.22082663 -0.15467682]]. Action = [[-0.17871654 -0.01253803 -0.00695597 -0.7106815 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 574 is [True, False, False, True, False, False]
Scene graph at timestep 574 is [True, False, False, True, False, False]
State prediction error at timestep 574 is tensor(5.8854e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.24226817 -0.14473772]]. Action = [[-0.10297218  0.17690963  0.15923673 -0.9308624 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 575 is [True, False, False, True, False, False]
Current timestep = 576. State = [[-0.2563738 -0.1226578]]. Action = [[-0.1789257   0.16047966  0.08155444  0.4664719 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 576 is [True, False, False, True, False, False]
Current timestep = 577. State = [[-0.27125475 -0.10582796]]. Action = [[0.10929298 0.058873   0.14619857 0.8326572 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.2662876  -0.09192267]]. Action = [[ 0.11273244  0.11200058 -0.11044693 -0.9283732 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.26388392 -0.08437917]]. Action = [[-0.15502751  0.11934501  0.02594689  0.1192857 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.25321114 -0.08613375]]. Action = [[ 0.19138938 -0.09285969  0.1991179   0.37625444]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 580 is [True, False, False, False, True, False]
Current timestep = 581. State = [[-0.24357487 -0.08665369]]. Action = [[-0.1810472  -0.11836672  0.10478345  0.39316034]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.2447393  -0.09890245]]. Action = [[-0.10859795 -0.18349342 -0.2239082  -0.3544333 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.23871653 -0.09860713]]. Action = [[ 0.20726946  0.22554219 -0.13890752 -0.01881218]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 583 is [True, False, False, False, True, False]
Current timestep = 584. State = [[-0.23126872 -0.08728513]]. Action = [[0.00507855 0.01046392 0.1386176  0.664953  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 584 is [True, False, False, False, True, False]
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(3.4031e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of 1
Current timestep = 585. State = [[-0.23047657 -0.09390566]]. Action = [[-0.12936838 -0.18262602 -0.21707305 -0.13822031]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 585 is [True, False, False, False, True, False]
Current timestep = 586. State = [[-0.23326229 -0.10156775]]. Action = [[-0.03804128  0.06086937 -0.04080474 -0.18360627]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 586 is [True, False, False, False, True, False]
Current timestep = 587. State = [[-0.24022642 -0.1138543 ]]. Action = [[-0.051687   -0.21247926 -0.09124613 -0.86265486]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.25028673 -0.12852758]]. Action = [[-0.10414518 -0.04043011  0.05616209 -0.8877451 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.25298798 -0.13766593]]. Action = [[ 0.17483613 -0.06260595  0.00142238 -0.1067946 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 589 is [True, False, False, True, False, False]
Current timestep = 590. State = [[-0.24360849 -0.14487006]]. Action = [[ 0.14348462 -0.03489903 -0.2100857   0.854269  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 590 is [True, False, False, True, False, False]
Current timestep = 591. State = [[-0.22757944 -0.13665004]]. Action = [[ 0.05723524  0.24013406  0.15761134 -0.9382608 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 591 is [True, False, False, True, False, False]
Scene graph at timestep 591 is [True, False, False, True, False, False]
State prediction error at timestep 591 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.22207806 -0.12896854]]. Action = [[-0.10790704 -0.19550458 -0.16664532 -0.11328745]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 592 is [True, False, False, True, False, False]
Scene graph at timestep 592 is [True, False, False, True, False, False]
State prediction error at timestep 592 is tensor(5.9590e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.22338735 -0.15196764]]. Action = [[ 0.10192692 -0.15566045  0.20672354  0.66426635]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 593 is [True, False, False, True, False, False]
Scene graph at timestep 593 is [True, False, False, True, False, False]
State prediction error at timestep 593 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.21717288 -0.16697928]]. Action = [[ 0.1079821  -0.06132957 -0.23054545  0.71719766]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 594 is [True, False, False, True, False, False]
Current timestep = 595. State = [[-0.20646024 -0.18326902]]. Action = [[-0.004354   -0.18256864 -0.16717415  0.6753421 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 595 is [True, False, False, True, False, False]
Scene graph at timestep 595 is [True, False, False, True, False, False]
State prediction error at timestep 595 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of 1
Current timestep = 596. State = [[-0.19836095 -0.19655271]]. Action = [[ 0.14690977  0.03020194  0.22165775 -0.08420503]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 596 is [True, False, False, True, False, False]
Current timestep = 597. State = [[-0.19351833 -0.1953626 ]]. Action = [[-0.14361666  0.03585443  0.06828988 -0.21290892]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 597 is [True, False, False, True, False, False]
Scene graph at timestep 597 is [True, False, False, True, False, False]
State prediction error at timestep 597 is tensor(9.7129e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of 1
Current timestep = 598. State = [[-0.19382565 -0.18384302]]. Action = [[ 0.04022223  0.18590915 -0.22230044  0.54535234]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 598 is [True, False, False, True, False, False]
Scene graph at timestep 598 is [True, False, False, True, False, False]
State prediction error at timestep 598 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of 1
Current timestep = 599. State = [[-0.19252217 -0.16796514]]. Action = [[-0.04898353  0.06582871  0.09794158  0.19680524]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 599 is [True, False, False, True, False, False]
Scene graph at timestep 599 is [True, False, False, True, False, False]
State prediction error at timestep 599 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of 1
Current timestep = 600. State = [[-0.20186378 -0.1634586 ]]. Action = [[-0.19995505 -0.00550199  0.13589507  0.6480658 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 600 is [True, False, False, True, False, False]
Current timestep = 601. State = [[-0.21298285 -0.16864718]]. Action = [[ 0.01665828 -0.11242396  0.14826217 -0.2867921 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 601 is [True, False, False, True, False, False]
Scene graph at timestep 601 is [True, False, False, True, False, False]
State prediction error at timestep 601 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.21965042 -0.17847107]]. Action = [[-0.07543491 -0.03954919 -0.23704563 -0.17515653]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 602 is [True, False, False, True, False, False]
Scene graph at timestep 602 is [True, False, False, True, False, False]
State prediction error at timestep 602 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.22239406 -0.18273202]]. Action = [[ 0.18530792  0.00383514  0.20790708 -0.4825347 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 603 is [True, False, False, True, False, False]
Scene graph at timestep 603 is [True, False, False, True, False, False]
State prediction error at timestep 603 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.21714208 -0.18118553]]. Action = [[-0.06730182  0.00431609  0.02932304  0.7140715 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 604 is [True, False, False, True, False, False]
Scene graph at timestep 604 is [True, False, False, True, False, False]
State prediction error at timestep 604 is tensor(1.7216e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of 1
Current timestep = 605. State = [[-0.21318467 -0.18176498]]. Action = [[ 0.14162469 -0.02832559 -0.13251409 -0.8141489 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 605 is [True, False, False, True, False, False]
Current timestep = 606. State = [[-0.20318294 -0.18426447]]. Action = [[ 0.096883   -0.04360148 -0.23906921  0.50112534]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 606 is [True, False, False, True, False, False]
Scene graph at timestep 606 is [True, False, False, True, False, False]
State prediction error at timestep 606 is tensor(4.2539e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of 1
Current timestep = 607. State = [[-0.19953798 -0.19532679]]. Action = [[-0.15512863 -0.13282223 -0.17683595  0.9352969 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 607 is [True, False, False, True, False, False]
Scene graph at timestep 607 is [True, False, False, True, False, False]
State prediction error at timestep 607 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.20849125 -0.2044993 ]]. Action = [[-0.09749091  0.07365888  0.11578673  0.551381  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 608 is [True, False, False, True, False, False]
Current timestep = 609. State = [[-0.20555188 -0.20320427]]. Action = [[ 0.20925596 -0.02935085  0.0077084  -0.31495732]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 609 is [True, False, False, True, False, False]
Current timestep = 610. State = [[-0.20153596 -0.19770233]]. Action = [[-0.04317994  0.09764135  0.14969695 -0.5671539 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 610 is [True, False, False, True, False, False]
Current timestep = 611. State = [[-0.1918477  -0.18237707]]. Action = [[ 0.18919009  0.15761063 -0.08291081  0.7150414 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 611 is [True, False, False, True, False, False]
Scene graph at timestep 611 is [True, False, False, True, False, False]
State prediction error at timestep 611 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of 1
Current timestep = 612. State = [[-0.1786438 -0.1681593]]. Action = [[ 0.0200761  -0.04205661 -0.19315723  0.5063864 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 612 is [True, False, False, True, False, False]
Current timestep = 613. State = [[-0.17172489 -0.17402074]]. Action = [[ 0.12598374 -0.0940195   0.13535595  0.9138138 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 613 is [True, False, False, True, False, False]
Current timestep = 614. State = [[-0.1534981  -0.18390758]]. Action = [[ 0.1480543  -0.07647313  0.07964462 -0.24997938]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 614 is [True, False, False, True, False, False]
Scene graph at timestep 614 is [True, False, False, True, False, False]
State prediction error at timestep 614 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of 1
Current timestep = 615. State = [[-0.14062318 -0.19348688]]. Action = [[-0.1607016   0.00614768 -0.02345176  0.7964785 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 615 is [True, False, False, True, False, False]
Current timestep = 616. State = [[-0.14070979 -0.1966156 ]]. Action = [[ 0.14063388 -0.05769689 -0.18658264 -0.35401034]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 616 is [True, False, False, True, False, False]
Scene graph at timestep 616 is [True, False, False, True, False, False]
State prediction error at timestep 616 is tensor(3.0287e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of 1
Current timestep = 617. State = [[-0.13737683 -0.19696502]]. Action = [[ 0.06438321  0.02960718  0.23782569 -0.25286043]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 617 is [True, False, False, True, False, False]
Scene graph at timestep 617 is [True, False, False, True, False, False]
State prediction error at timestep 617 is tensor(3.7698e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.11956568 -0.20145886]]. Action = [[ 0.2242747  -0.0993043  -0.2120175  -0.28124243]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 618 is [True, False, False, True, False, False]
Current timestep = 619. State = [[-0.09810916 -0.21040691]]. Action = [[ 0.02935675 -0.0499059   0.15856442  0.12792099]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 619 is [True, False, False, True, False, False]
Scene graph at timestep 619 is [True, False, False, True, False, False]
State prediction error at timestep 619 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.09432989 -0.20781617]]. Action = [[-0.21950097  0.19858408 -0.14257689 -0.637826  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 620 is [True, False, False, True, False, False]
Current timestep = 621. State = [[-0.09322955 -0.20345236]]. Action = [[ 0.21416706 -0.09558177 -0.03249884 -0.49397165]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 621 is [True, False, False, True, False, False]
Current timestep = 622. State = [[-0.07940742 -0.19292006]]. Action = [[ 0.22819054  0.19473231  0.03156972 -0.2669593 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 622 is [True, False, False, True, False, False]
Scene graph at timestep 622 is [True, False, False, True, False, False]
State prediction error at timestep 622 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.0478809  -0.17091794]]. Action = [[ 0.17900985  0.1450181  -0.12808126 -0.25495696]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 623 is [True, False, False, True, False, False]
Scene graph at timestep 623 is [False, True, False, True, False, False]
State prediction error at timestep 623 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of 1
Current timestep = 624. State = [[-0.02318198 -0.16139442]]. Action = [[ 0.24843776 -0.07118176 -0.07640454 -0.26977932]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 624 is [False, True, False, True, False, False]
Current timestep = 625. State = [[ 0.0085379  -0.16393292]]. Action = [[ 0.2179713  -0.03325631  0.20702654  0.84887874]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 625 is [False, True, False, True, False, False]
Current timestep = 626. State = [[ 0.0296033  -0.17742532]]. Action = [[-0.04123041 -0.14608447 -0.06243169  0.68994594]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 626 is [False, True, False, True, False, False]
Current timestep = 627. State = [[ 0.03175386 -0.18797095]]. Action = [[-0.01692139 -0.01162487 -0.20925772 -0.757375  ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 627 is [False, True, False, True, False, False]
Current timestep = 628. State = [[ 0.03904452 -0.18566   ]]. Action = [[ 0.18914804  0.09400034 -0.10945252 -0.06842917]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 628 is [False, True, False, True, False, False]
Current timestep = 629. State = [[ 0.05361997 -0.19390091]]. Action = [[-0.02632235 -0.21169703  0.1480062  -0.4186638 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 629 is [False, True, False, True, False, False]
Current timestep = 630. State = [[ 0.05551979 -0.19619563]]. Action = [[-0.16038133  0.20987597  0.10940057 -0.2728243 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 630 is [False, False, True, True, False, False]
Scene graph at timestep 630 is [False, False, True, True, False, False]
State prediction error at timestep 630 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[ 0.05400379 -0.18828641]]. Action = [[0.07994413 0.04660293 0.02744249 0.34744322]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 631 is [False, False, True, True, False, False]
Current timestep = 632. State = [[ 0.04770661 -0.19675384]]. Action = [[-0.15055104 -0.12703942  0.03574815  0.441898  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 632 is [False, False, True, True, False, False]
Current timestep = 633. State = [[ 0.03519625 -0.21899371]]. Action = [[-0.05462548 -0.23850778  0.09825087 -0.49615943]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 633 is [False, True, False, True, False, False]
Current timestep = 634. State = [[ 0.03454209 -0.2195506 ]]. Action = [[ 0.14732414  0.24014184 -0.12478006 -0.49171412]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 634 is [False, True, False, True, False, False]
Current timestep = 635. State = [[ 0.03552973 -0.21768007]]. Action = [[-0.05342039 -0.17803499 -0.03710741  0.33134842]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 635 is [False, True, False, True, False, False]
Scene graph at timestep 635 is [False, True, False, True, False, False]
State prediction error at timestep 635 is tensor(4.3107e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[ 0.03746555 -0.21425553]]. Action = [[ 0.12521803  0.2093775  -0.03811118 -0.0302037 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 636 is [False, True, False, True, False, False]
Current timestep = 637. State = [[ 0.03997191 -0.18955113]]. Action = [[-0.03173102  0.21227163 -0.10397851  0.44123507]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 637 is [False, True, False, True, False, False]
Current timestep = 638. State = [[ 0.0415176  -0.18098752]]. Action = [[ 0.08041909 -0.17193064 -0.10586451 -0.64339983]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 638 is [False, True, False, True, False, False]
Current timestep = 639. State = [[ 0.03821355 -0.19528213]]. Action = [[-0.19794631 -0.12430415 -0.18010192  0.09782767]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 639 is [False, True, False, True, False, False]
Scene graph at timestep 639 is [False, True, False, True, False, False]
State prediction error at timestep 639 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[ 0.0363811  -0.20638351]]. Action = [[ 0.22365555 -0.23446748 -0.15675196  0.28587496]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 640 is [False, True, False, True, False, False]
Scene graph at timestep 640 is [False, True, False, True, False, False]
State prediction error at timestep 640 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[ 0.0363811  -0.20638351]]. Action = [[ 0.23864624 -0.15914649  0.24835992 -0.32707638]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 641 is [False, True, False, True, False, False]
Scene graph at timestep 641 is [False, True, False, True, False, False]
State prediction error at timestep 641 is tensor(1.7079e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of -1
Current timestep = 642. State = [[ 0.03738641 -0.195603  ]]. Action = [[0.02867404 0.22261083 0.0906738  0.2491746 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 642 is [False, True, False, True, False, False]
Scene graph at timestep 642 is [False, True, False, True, False, False]
State prediction error at timestep 642 is tensor(2.0213e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of 1
Current timestep = 643. State = [[ 0.03842939 -0.17288774]]. Action = [[-0.01641215  0.12990469  0.07114521  0.5285425 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 643 is [False, True, False, True, False, False]
Scene graph at timestep 643 is [False, True, False, True, False, False]
State prediction error at timestep 643 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[ 0.02945386 -0.172971  ]]. Action = [[-0.2116891  -0.17310299 -0.17324224  0.0091418 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 644 is [False, True, False, True, False, False]
Scene graph at timestep 644 is [False, True, False, True, False, False]
State prediction error at timestep 644 is tensor(1.2617e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[ 0.01070443 -0.19417384]]. Action = [[-0.07107231 -0.12351997 -0.18419231  0.64794123]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 645 is [False, True, False, True, False, False]
Current timestep = 646. State = [[ 0.00707957 -0.1954249 ]]. Action = [[ 0.12167683  0.07897592  0.18200418 -0.21612513]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 646 is [False, True, False, True, False, False]
Current timestep = 647. State = [[ 0.01622971 -0.18612164]]. Action = [[ 0.21635276  0.0826214  -0.13151321 -0.07761669]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 647 is [False, True, False, True, False, False]
Scene graph at timestep 647 is [False, True, False, True, False, False]
State prediction error at timestep 647 is tensor(8.7121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[ 0.02854697 -0.17849186]]. Action = [[0.08335349 0.00152937 0.13130632 0.62744594]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 648 is [False, True, False, True, False, False]
Current timestep = 649. State = [[ 0.03302319 -0.1782566 ]]. Action = [[ 0.00620338 -0.04051711  0.10331929 -0.0076707 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 649 is [False, True, False, True, False, False]
Current timestep = 650. State = [[ 0.03587394 -0.18794718]]. Action = [[ 0.0173741  -0.1537042   0.14961976  0.19211304]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 650 is [False, True, False, True, False, False]
Current timestep = 651. State = [[ 0.03707787 -0.1997025 ]]. Action = [[-0.14102592 -0.01080689 -0.09825525  0.32320666]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 651 is [False, True, False, True, False, False]
Current timestep = 652. State = [[ 0.03675768 -0.19267838]]. Action = [[-0.03475633  0.1917446  -0.01666245  0.7316413 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 652 is [False, True, False, True, False, False]
Current timestep = 653. State = [[ 0.03832815 -0.1800663 ]]. Action = [[0.153211   0.05261964 0.05293438 0.6399301 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 653 is [False, True, False, True, False, False]
Scene graph at timestep 653 is [False, True, False, True, False, False]
State prediction error at timestep 653 is tensor(7.4405e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 1
Current timestep = 654. State = [[ 0.04219329 -0.16499643]]. Action = [[ 0.02224895  0.1462341  -0.21603976  0.6035236 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 654 is [False, True, False, True, False, False]
Current timestep = 655. State = [[ 0.04208527 -0.15474452]]. Action = [[-0.1486789  -0.00043489 -0.15826552  0.2544123 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 655 is [False, True, False, True, False, False]
Current timestep = 656. State = [[-0.24463002 -0.10122558]]. Action = [[0.09428024 0.18863654 0.00518391 0.35773242]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 656 is [False, True, False, True, False, False]
Current timestep = 657. State = [[-0.24739751 -0.12024167]]. Action = [[-0.1881275  -0.0974746   0.09183082  0.49896765]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 657 is [True, False, False, False, True, False]
Scene graph at timestep 657 is [True, False, False, False, True, False]
State prediction error at timestep 657 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 657 of -1
Current timestep = 658. State = [[-0.25725228 -0.12367124]]. Action = [[-0.0576997   0.16355911 -0.21666229 -0.7372698 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 658 is [True, False, False, False, True, False]
Current timestep = 659. State = [[-0.25546598 -0.11526327]]. Action = [[ 0.20489132 -0.0128824  -0.06289981 -0.6176898 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 659 is [True, False, False, False, True, False]
Current timestep = 660. State = [[-0.24814585 -0.11381713]]. Action = [[ 0.05994403 -0.02341209 -0.04085714 -0.362975  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 660 is [True, False, False, False, True, False]
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(2.1765e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 0
Current timestep = 661. State = [[-0.2381875  -0.11701284]]. Action = [[ 0.01704451 -0.05984887 -0.22926143  0.7867967 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 661 is [True, False, False, False, True, False]
Current timestep = 662. State = [[-0.22886418 -0.12544309]]. Action = [[ 0.14702806 -0.11541881  0.22546777 -0.4277349 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 662 is [True, False, False, False, True, False]
Current timestep = 663. State = [[-0.22618398 -0.1342964 ]]. Action = [[-0.2336417   0.00712058 -0.18729906  0.63371587]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 663 is [True, False, False, True, False, False]
Current timestep = 664. State = [[-0.23698126 -0.13443051]]. Action = [[-0.08304963  0.0632531   0.10723972  0.9264729 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 664 is [True, False, False, True, False, False]
Current timestep = 665. State = [[-0.23375423 -0.1363804 ]]. Action = [[ 0.24062827 -0.09072311  0.12354028  0.27612758]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 665 is [True, False, False, True, False, False]
Scene graph at timestep 665 is [True, False, False, True, False, False]
State prediction error at timestep 665 is tensor(1.4394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of -1
Current timestep = 666. State = [[-0.23145911 -0.13045336]]. Action = [[-0.1797813   0.1718576  -0.19542165 -0.5168475 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 666 is [True, False, False, True, False, False]
Current timestep = 667. State = [[-0.23308165 -0.11332728]]. Action = [[ 0.14426023  0.14049953  0.03274044 -0.38275784]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 667 is [True, False, False, True, False, False]
Current timestep = 668. State = [[-0.23548773 -0.115735  ]]. Action = [[-0.16007964 -0.22291681 -0.1812716   0.7893039 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.24009317 -0.11522736]]. Action = [[-0.00277705  0.1995076  -0.14729851 -0.9019273 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.23584351 -0.09124037]]. Action = [[ 0.17948157  0.23682651 -0.02378599 -0.9187791 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.22094569 -0.06060812]]. Action = [[ 0.20726639  0.20353591  0.19919407 -0.92930543]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.19787458 -0.04221415]]. Action = [[ 0.13651025  0.02038595 -0.17819725 -0.34119856]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 672 is [True, False, False, False, True, False]
Current timestep = 673. State = [[-0.17723852 -0.02662994]]. Action = [[ 0.12972724  0.15620309  0.18676972 -0.67697114]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 673 is [True, False, False, False, True, False]
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.16928943  0.00077409]]. Action = [[-0.19481121  0.24589732 -0.20642865  0.23674035]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.17164665  0.03336399]]. Action = [[0.18601829 0.19624186 0.01449978 0.49773252]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.16299693  0.04878288]]. Action = [[ 0.01903421 -0.00276656  0.19419515  0.7243072 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.1563252   0.06157921]]. Action = [[ 0.12818569  0.18068331 -0.18932298  0.71469235]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 677 is [True, False, False, False, True, False]
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(7.7353e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 677 of 1
Current timestep = 678. State = [[-0.13321765  0.06761263]]. Action = [[ 0.13747308 -0.19907811  0.01793981 -0.1735956 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 678 is [True, False, False, False, True, False]
Current timestep = 679. State = [[-0.11319938  0.04474616]]. Action = [[ 0.19575858 -0.19326374 -0.24066931 -0.06663692]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 679 is [True, False, False, False, True, False]
Current timestep = 680. State = [[-0.09002233  0.04179382]]. Action = [[ 0.18048811  0.2230277  -0.02109599  0.4633168 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.06941597  0.04955827]]. Action = [[ 0.00588903 -0.05549189  0.2433821   0.19825423]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 681 is [True, False, False, False, True, False]
Current timestep = 682. State = [[-0.07059401  0.05932927]]. Action = [[-0.16102898  0.15607983 -0.08099961  0.31228495]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 682 is [True, False, False, False, True, False]
Current timestep = 683. State = [[-0.07520115  0.07170467]]. Action = [[ 0.05427483  0.04525852 -0.0602434   0.97866905]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 683 is [True, False, False, False, True, False]
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of 1
Current timestep = 684. State = [[-0.08221198  0.07764271]]. Action = [[-0.23036347 -0.00674208 -0.15652579 -0.62508583]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 684 is [True, False, False, False, True, False]
Current timestep = 685. State = [[-0.08425769  0.06480991]]. Action = [[ 0.11912134 -0.24034904  0.18321794 -0.40269542]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 685 is [True, False, False, False, True, False]
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(5.0541e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.07935942  0.03631423]]. Action = [[ 0.13601059 -0.17055558 -0.03370625 -0.08098942]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 686 is [True, False, False, False, True, False]
Current timestep = 687. State = [[-0.06487821  0.01585958]]. Action = [[ 0.23327234 -0.13028161  0.08013731 -0.5137622 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.05037563  0.00619365]]. Action = [[-0.1324478   0.01870602 -0.00666223 -0.34419954]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 688 is [True, False, False, False, True, False]
Current timestep = 689. State = [[-0.04933707  0.00822939]]. Action = [[ 0.0822376   0.06822577  0.03997552 -0.8942825 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 689 is [True, False, False, False, True, False]
Scene graph at timestep 689 is [False, True, False, False, True, False]
State prediction error at timestep 689 is tensor(6.6538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 689 of 1
Current timestep = 690. State = [[-0.04809016  0.01028062]]. Action = [[ 0.0077666  -0.00422424 -0.13080937 -0.05734795]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 690 is [False, True, False, False, True, False]
Scene graph at timestep 690 is [False, True, False, False, True, False]
State prediction error at timestep 690 is tensor(7.2158e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of 1
Current timestep = 691. State = [[-0.05239421  0.02331109]]. Action = [[-0.16951211  0.21718091 -0.11977398  0.1741395 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 691 is [False, True, False, False, True, False]
Current timestep = 692. State = [[-0.05760685  0.03146762]]. Action = [[-0.07972921 -0.1314034  -0.13911721  0.94900954]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.05533551  0.01349092]]. Action = [[ 0.17978966 -0.2236779   0.18017507 -0.620437  ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.0512557   0.00412481]]. Action = [[ 0.05617952  0.11214244  0.00993341 -0.91072524]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 694 is [True, False, False, False, True, False]
Current timestep = 695. State = [[-0.04609071  0.01792242]]. Action = [[ 0.11728987  0.19741482 -0.12615703  0.7992475 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 695 is [True, False, False, False, True, False]
Current timestep = 696. State = [[-0.21475223 -0.11324293]]. Action = [[-0.08391677 -0.23287165 -0.19063479  0.33862734]]. Reward = [100.]
Curr episode timestep = 39
Scene graph at timestep 696 is [False, True, False, False, True, False]
Current timestep = 697. State = [[-0.20389703 -0.13219717]]. Action = [[ 0.02839467 -0.10826522  0.06878924  0.86017203]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.20349039 -0.13236928]]. Action = [[-0.07966012  0.17429781 -0.18323636  0.8140211 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 698 is [True, False, False, True, False, False]
Current timestep = 699. State = [[-0.19768137 -0.13661392]]. Action = [[ 0.19354907 -0.20797034  0.16967976 -0.58200455]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 699 is [True, False, False, True, False, False]
Current timestep = 700. State = [[-0.17600052 -0.14530618]]. Action = [[ 0.22889322  0.02874434 -0.10811728 -0.05722797]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 700 is [True, False, False, True, False, False]
Scene graph at timestep 700 is [True, False, False, True, False, False]
State prediction error at timestep 700 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of 1
Current timestep = 701. State = [[-0.15545265 -0.15745713]]. Action = [[-0.04020086 -0.18653402  0.20927674 -0.49041843]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 701 is [True, False, False, True, False, False]
Current timestep = 702. State = [[-0.16068438 -0.17344047]]. Action = [[-0.17073841 -0.05479884 -0.18868148 -0.33704138]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 702 is [True, False, False, True, False, False]
Current timestep = 703. State = [[-0.16849497 -0.17101505]]. Action = [[-0.05014983  0.19866025 -0.22502656  0.43947887]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 703 is [True, False, False, True, False, False]
Scene graph at timestep 703 is [True, False, False, True, False, False]
State prediction error at timestep 703 is tensor(9.2039e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-0.17391267 -0.15103735]]. Action = [[-0.0929673   0.15901226 -0.24206531  0.9641012 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 704 is [True, False, False, True, False, False]
Current timestep = 705. State = [[-0.1843642  -0.14184488]]. Action = [[-0.07334994 -0.05848254  0.1422877  -0.06521994]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 705 is [True, False, False, True, False, False]
Scene graph at timestep 705 is [True, False, False, True, False, False]
State prediction error at timestep 705 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.1951261 -0.1512947]]. Action = [[-0.01393913 -0.15857525  0.2244151   0.27096403]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 706 is [True, False, False, True, False, False]
Scene graph at timestep 706 is [True, False, False, True, False, False]
State prediction error at timestep 706 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[-0.20549484 -0.17597896]]. Action = [[-0.11336011 -0.19030888  0.0441719  -0.10507846]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 707 is [True, False, False, True, False, False]
Current timestep = 708. State = [[-0.21590412 -0.18424752]]. Action = [[-0.10479374  0.09857631 -0.19512531 -0.09440315]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 708 is [True, False, False, True, False, False]
Scene graph at timestep 708 is [True, False, False, True, False, False]
State prediction error at timestep 708 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[-0.23760542 -0.16921929]]. Action = [[-0.19345602  0.18297184  0.02036282  0.5175259 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 709 is [True, False, False, True, False, False]
Scene graph at timestep 709 is [True, False, False, True, False, False]
State prediction error at timestep 709 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[-0.24802123 -0.14244878]]. Action = [[ 0.19875997  0.21665925 -0.20026293 -0.06646156]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 710 is [True, False, False, True, False, False]
Scene graph at timestep 710 is [True, False, False, True, False, False]
State prediction error at timestep 710 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of 1
Current timestep = 711. State = [[-0.23535118 -0.11168542]]. Action = [[ 0.14445964  0.18418139 -0.24436578  0.5645571 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 711 is [True, False, False, True, False, False]
Current timestep = 712. State = [[-0.21974595 -0.09287395]]. Action = [[ 0.12059399  0.08254451 -0.15210259 -0.38953507]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 712 is [True, False, False, False, True, False]
Current timestep = 713. State = [[-0.21381034 -0.09593748]]. Action = [[-0.15534635 -0.17887673 -0.1943349   0.55253744]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 713 is [True, False, False, False, True, False]
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(6.2573e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.21722317 -0.10606641]]. Action = [[ 0.01047114 -0.02109963  0.11399204 -0.24175632]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 714 is [True, False, False, False, True, False]
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(8.3648e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.21499301 -0.09575091]]. Action = [[ 0.10838088  0.20788172  0.03210089 -0.44632435]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.20298147 -0.09511402]]. Action = [[ 0.18387574 -0.21033795  0.19627202 -0.70486045]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(9.5776e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.17518719 -0.09529296]]. Action = [[0.23506287 0.16780227 0.09531757 0.5567932 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 717 is [True, False, False, False, True, False]
Current timestep = 718. State = [[-0.16299038 -0.08625747]]. Action = [[-0.18224044  0.04111215 -0.24635255 -0.8114865 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 718 is [True, False, False, False, True, False]
Current timestep = 719. State = [[-0.16368073 -0.08346874]]. Action = [[ 0.09631854 -0.03409685 -0.19913538  0.2954644 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 719 is [True, False, False, False, True, False]
Current timestep = 720. State = [[-0.15422499 -0.07567162]]. Action = [[ 0.2067183  0.1302833  0.2316505 -0.9276344]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 720 is [True, False, False, False, True, False]
Current timestep = 721. State = [[-0.1430496  -0.05690473]]. Action = [[-0.09822196  0.18793249 -0.02480429  0.88319993]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 721 is [True, False, False, False, True, False]
Current timestep = 722. State = [[-0.14022104 -0.02784001]]. Action = [[ 0.13665909  0.23287177 -0.24473065  0.10811496]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 722 is [True, False, False, False, True, False]
Current timestep = 723. State = [[-0.12417605 -0.00899054]]. Action = [[ 0.16576672  0.01322344 -0.19314456 -0.7579806 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 723 is [True, False, False, False, True, False]
Scene graph at timestep 723 is [True, False, False, False, True, False]
State prediction error at timestep 723 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.1013516  -0.00535719]]. Action = [[ 0.12246072 -0.03771363  0.00113472 -0.56606215]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 724 is [True, False, False, False, True, False]
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of 1
Current timestep = 725. State = [[-0.08760366 -0.01305668]]. Action = [[-0.11714043 -0.10972656 -0.09863199 -0.38601238]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 725 is [True, False, False, False, True, False]
Scene graph at timestep 725 is [True, False, False, False, True, False]
State prediction error at timestep 725 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of -1
Current timestep = 726. State = [[-0.08745176 -0.02493347]]. Action = [[ 0.15572262 -0.06286317  0.12950909  0.68527687]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.07454468 -0.01748099]]. Action = [[ 0.23292744  0.18159044  0.12080294 -0.5495686 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 727 is [True, False, False, False, True, False]
Scene graph at timestep 727 is [True, False, False, False, True, False]
State prediction error at timestep 727 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[-0.04914708 -0.01246042]]. Action = [[-0.06563383 -0.10107349  0.09364009 -0.64709413]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 728 is [True, False, False, False, True, False]
Scene graph at timestep 728 is [False, True, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 0
Current timestep = 729. State = [[-0.04794668 -0.00465582]]. Action = [[ 0.11506876  0.225835    0.23315251 -0.77373105]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 729 is [False, True, False, False, True, False]
Current timestep = 730. State = [[-0.04687313  0.02240615]]. Action = [[-0.12267971  0.23920727 -0.04782359 -0.7091526 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 730 is [False, True, False, False, True, False]
Scene graph at timestep 730 is [False, True, False, False, True, False]
State prediction error at timestep 730 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of -1
Current timestep = 731. State = [[-0.05504106  0.05114589]]. Action = [[-0.02006301  0.06134573  0.00754255  0.06709945]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 731 is [False, True, False, False, True, False]
Current timestep = 732. State = [[-0.05664307  0.05354457]]. Action = [[-0.13898008 -0.04687169  0.08724523  0.64810514]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 732 is [True, False, False, False, True, False]
Scene graph at timestep 732 is [True, False, False, False, True, False]
State prediction error at timestep 732 is tensor(1.6330e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of -1
Current timestep = 733. State = [[-0.05734091  0.05422284]]. Action = [[0.17616624 0.06566995 0.02383837 0.78701353]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 733 is [True, False, False, False, True, False]
Current timestep = 734. State = [[-0.0434216  0.0650729]]. Action = [[0.24801177 0.12633675 0.10043168 0.8080524 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 734 is [True, False, False, False, True, False]
Scene graph at timestep 734 is [False, True, False, False, True, False]
State prediction error at timestep 734 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[-0.16052786  0.08476076]]. Action = [[-0.11571407 -0.2194756   0.02421075  0.6310036 ]]. Reward = [100.]
Curr episode timestep = 38
Scene graph at timestep 735 is [False, True, False, False, True, False]
Current timestep = 736. State = [[-0.1347945   0.08245034]]. Action = [[ 0.19737554 -0.23512927 -0.20719738  0.9027519 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 736 is [True, False, False, False, True, False]
Current timestep = 737. State = [[-0.11625741  0.07988547]]. Action = [[ 0.06682777  0.18511057 -0.10128653  0.44803166]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 737 is [True, False, False, False, True, False]
Current timestep = 738. State = [[-0.1055572  0.0750823]]. Action = [[ 0.00498819 -0.22538921  0.2054803  -0.07853687]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 738 is [True, False, False, False, True, False]
Current timestep = 739. State = [[-0.10956927  0.04912145]]. Action = [[-0.22252588 -0.2441892   0.14572105 -0.58726233]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 739 is [True, False, False, False, True, False]
Current timestep = 740. State = [[-0.10830633  0.0439897 ]]. Action = [[ 0.23635042  0.24209708 -0.22114867  0.51271546]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 740 is [True, False, False, False, True, False]
Current timestep = 741. State = [[-0.10511776  0.05164424]]. Action = [[-0.05391781 -0.05624743  0.20165494 -0.13600713]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 741 is [True, False, False, False, True, False]
Current timestep = 742. State = [[-0.10495901  0.05397865]]. Action = [[ 0.05312344  0.07301793 -0.05967981 -0.9448275 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 742 is [True, False, False, False, True, False]
Current timestep = 743. State = [[-0.09894721  0.07043601]]. Action = [[ 0.10958079  0.19847527 -0.03276809 -0.27870905]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 743 is [True, False, False, False, True, False]
Scene graph at timestep 743 is [True, False, False, False, True, False]
State prediction error at timestep 743 is tensor(8.1038e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 743 of 1
Current timestep = 744. State = [[-0.08404361  0.07635297]]. Action = [[ 0.01289171 -0.19007395 -0.2495044   0.2988305 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 744 is [True, False, False, False, True, False]
Current timestep = 745. State = [[-0.07777851  0.06904803]]. Action = [[ 0.14273667  0.01129666  0.1352731  -0.8461024 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 745 is [True, False, False, False, True, False]
Current timestep = 746. State = [[-0.06752179  0.07924004]]. Action = [[-0.00154093  0.21373492  0.04993924  0.61002564]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 746 is [True, False, False, False, True, False]
Current timestep = 747. State = [[-0.0611203   0.07763085]]. Action = [[ 0.07093212 -0.22292358 -0.19860348 -0.74948364]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 747 is [True, False, False, False, True, False]
Scene graph at timestep 747 is [True, False, False, False, True, False]
State prediction error at timestep 747 is tensor(1.4866e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of 1
Current timestep = 748. State = [[-0.04692083  0.07278176]]. Action = [[ 0.20245415  0.13948172 -0.04690282 -0.09677672]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 748 is [True, False, False, False, True, False]
Current timestep = 749. State = [[-0.23137029 -0.16732965]]. Action = [[-0.13099444 -0.01468995  0.08597034 -0.20806825]]. Reward = [100.]
Curr episode timestep = 13
Scene graph at timestep 749 is [False, True, False, False, True, False]
Current timestep = 750. State = [[-0.21651496 -0.18361184]]. Action = [[0.23102319 0.015802   0.21225893 0.0177362 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 750 is [True, False, False, True, False, False]
Current timestep = 751. State = [[-0.20626481 -0.19052781]]. Action = [[-0.19614427 -0.07013866  0.11563289  0.7106993 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 751 is [True, False, False, True, False, False]
Scene graph at timestep 751 is [True, False, False, True, False, False]
State prediction error at timestep 751 is tensor(8.5767e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of 1
Current timestep = 752. State = [[-0.21774541 -0.1957127 ]]. Action = [[-0.14419037  0.02867663  0.12849227  0.81438494]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 752 is [True, False, False, True, False, False]
Current timestep = 753. State = [[-0.22156757 -0.19481677]]. Action = [[0.07485169 0.0449228  0.11825988 0.5658138 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 753 is [True, False, False, True, False, False]
Current timestep = 754. State = [[-0.21958144 -0.1941775 ]]. Action = [[ 0.0815388  -0.02084383  0.01014972  0.57969165]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 754 is [True, False, False, True, False, False]
Scene graph at timestep 754 is [True, False, False, True, False, False]
State prediction error at timestep 754 is tensor(1.4421e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of -1
Current timestep = 755. State = [[-0.21143527 -0.17943692]]. Action = [[ 0.10160166  0.19605404  0.01094449 -0.7507646 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 755 is [True, False, False, True, False, False]
Current timestep = 756. State = [[-0.2109368  -0.17409909]]. Action = [[-0.17525102 -0.1058633   0.13558614  0.5867605 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 756 is [True, False, False, True, False, False]
Current timestep = 757. State = [[-0.20732056 -0.1689638 ]]. Action = [[ 0.21419877  0.13751802 -0.13507779  0.6719458 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 757 is [True, False, False, True, False, False]
Current timestep = 758. State = [[-0.2032253  -0.15804899]]. Action = [[-0.04196109  0.06227979  0.00764963  0.2159462 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 758 is [True, False, False, True, False, False]
Current timestep = 759. State = [[-0.21132125 -0.14105813]]. Action = [[-0.22022967  0.20904273  0.01393443  0.08693147]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 759 is [True, False, False, True, False, False]
Current timestep = 760. State = [[-0.22830111 -0.11832851]]. Action = [[-0.19425409  0.11330414  0.20473039  0.21438062]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 760 is [True, False, False, True, False, False]
Scene graph at timestep 760 is [True, False, False, False, True, False]
State prediction error at timestep 760 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 760 of -1
Current timestep = 761. State = [[-0.25414294 -0.11671567]]. Action = [[-0.17847058 -0.16903585  0.24454561  0.60415304]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 761 is [True, False, False, False, True, False]
Current timestep = 762. State = [[-0.26350626 -0.1121264 ]]. Action = [[0.1767667  0.23267052 0.22954515 0.9257276 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 762 is [True, False, False, False, True, False]
Current timestep = 763. State = [[-0.25484738 -0.10057734]]. Action = [[ 0.14004263 -0.01995459  0.15940976  0.56228554]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 763 is [True, False, False, False, True, False]
Current timestep = 764. State = [[-0.23641877 -0.10318492]]. Action = [[ 0.20345324 -0.09802212  0.01744476 -0.5033904 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 764 is [True, False, False, False, True, False]
Current timestep = 765. State = [[-0.21836534 -0.10007386]]. Action = [[ 0.05455473  0.12427056 -0.22651924 -0.6704942 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 765 is [True, False, False, False, True, False]
Scene graph at timestep 765 is [True, False, False, False, True, False]
State prediction error at timestep 765 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 0
Current timestep = 766. State = [[-0.21124858 -0.09795598]]. Action = [[-0.10885584 -0.11950953  0.02843148 -0.14066076]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 766 is [True, False, False, False, True, False]
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(3.7513e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 766 of -1
Current timestep = 767. State = [[-0.2121164  -0.10966782]]. Action = [[ 0.09566906 -0.08019516  0.169554    0.2674532 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 767 is [True, False, False, False, True, False]
Scene graph at timestep 767 is [True, False, False, False, True, False]
State prediction error at timestep 767 is tensor(3.8747e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of -1
Current timestep = 768. State = [[-0.21135959 -0.1270381 ]]. Action = [[-0.02543764 -0.18517178  0.1006684  -0.29718435]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 768 is [True, False, False, False, True, False]
Current timestep = 769. State = [[-0.20427413 -0.13882495]]. Action = [[ 0.17117584  0.01167053 -0.19585241  0.02789629]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 769 is [True, False, False, True, False, False]
Current timestep = 770. State = [[-0.20155293 -0.15182804]]. Action = [[-0.2206351  -0.14737074  0.03554618 -0.88792706]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 770 is [True, False, False, True, False, False]
Current timestep = 771. State = [[-0.20807676 -0.16311434]]. Action = [[ 0.00241628 -0.02996261  0.19048205 -0.73582894]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 771 is [True, False, False, True, False, False]
Current timestep = 772. State = [[-0.20789796 -0.17036177]]. Action = [[ 0.09799251 -0.07496266  0.01455304  0.75975823]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 772 is [True, False, False, True, False, False]
Current timestep = 773. State = [[-0.19936371 -0.1827395 ]]. Action = [[ 0.17090935 -0.09886298  0.01898652  0.19044125]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 773 is [True, False, False, True, False, False]
Current timestep = 774. State = [[-0.19381133 -0.20290834]]. Action = [[-0.19281901 -0.19177584  0.15078047  0.97700596]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 774 is [True, False, False, True, False, False]
Current timestep = 775. State = [[-0.20613135 -0.23053843]]. Action = [[-0.12854001 -0.17178078  0.12902045 -0.5180195 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 775 is [True, False, False, True, False, False]
Current timestep = 776. State = [[-0.20473303 -0.23842306]]. Action = [[ 0.2453289   0.12171352  0.06910926 -0.6767725 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 776 is [True, False, False, True, False, False]
Current timestep = 777. State = [[-0.18514052 -0.22694342]]. Action = [[0.2398082  0.07978797 0.23496291 0.424726  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 777 is [True, False, False, True, False, False]
Current timestep = 778. State = [[-0.16067836 -0.21963966]]. Action = [[ 0.06874517  0.0411709  -0.24133946  0.9299915 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 778 is [True, False, False, True, False, False]
Current timestep = 779. State = [[-0.15176007 -0.20723945]]. Action = [[-0.02579199  0.16905174 -0.12181264 -0.70409197]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 779 is [True, False, False, True, False, False]
Current timestep = 780. State = [[-0.1417197  -0.20714107]]. Action = [[ 0.1997565  -0.22714335 -0.20347796 -0.16806209]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 780 is [True, False, False, True, False, False]
Current timestep = 781. State = [[-0.11560954 -0.21647102]]. Action = [[ 0.2338208   0.00414592 -0.17355639 -0.89168584]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 781 is [True, False, False, True, False, False]
Current timestep = 782. State = [[-0.09858361 -0.22048473]]. Action = [[-0.09719348 -0.01036796 -0.00506981  0.70607436]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 782 is [True, False, False, True, False, False]
Current timestep = 783. State = [[-0.09417143 -0.22041872]]. Action = [[ 0.11960605  0.0287565   0.23120368 -0.0889852 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 783 is [True, False, False, True, False, False]
Current timestep = 784. State = [[-0.07676845 -0.23331691]]. Action = [[ 0.24022987 -0.24172916 -0.14595628 -0.6791994 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 784 is [True, False, False, True, False, False]
Current timestep = 785. State = [[-0.05477529 -0.26218584]]. Action = [[ 0.02641869 -0.23489815  0.13317841 -0.1907562 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 785 is [True, False, False, True, False, False]
Current timestep = 786. State = [[-0.04711789 -0.27445725]]. Action = [[ 0.00456148  0.1303767   0.16913071 -0.6542088 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 786 is [True, False, False, True, False, False]
Scene graph at timestep 786 is [False, True, False, True, False, False]
State prediction error at timestep 786 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[-0.04677645 -0.26462033]]. Action = [[-0.10537119  0.16347629 -0.01534918  0.39855826]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 787 is [False, True, False, True, False, False]
Scene graph at timestep 787 is [False, True, False, True, False, False]
State prediction error at timestep 787 is tensor(4.1726e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[-0.04166459 -0.25271922]]. Action = [[ 0.19503963 -0.01742752  0.03622758 -0.2147249 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 788 is [False, True, False, True, False, False]
Current timestep = 789. State = [[-0.03710163 -0.25386667]]. Action = [[-0.1574244  -0.02119355  0.04233032 -0.25629807]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 789 is [False, True, False, True, False, False]
Current timestep = 790. State = [[-0.03768038 -0.25430474]]. Action = [[0.03781959 0.02748311 0.14369944 0.72791135]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 790 is [False, True, False, True, False, False]
Current timestep = 791. State = [[-0.03634198 -0.24114004]]. Action = [[-0.00916338  0.2221381  -0.24116203  0.33326936]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 791 is [False, True, False, True, False, False]
Current timestep = 792. State = [[-0.03048582 -0.2283838 ]]. Action = [[ 0.19790494 -0.07382001 -0.18278995 -0.33741045]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 792 is [False, True, False, True, False, False]
Scene graph at timestep 792 is [False, True, False, True, False, False]
State prediction error at timestep 792 is tensor(9.2873e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.02665505 -0.2366271 ]]. Action = [[-0.1805896  -0.12639493  0.24028319 -0.9037819 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 793 is [False, True, False, True, False, False]
Current timestep = 794. State = [[-0.02866487 -0.25350803]]. Action = [[ 0.1563324  -0.17678277  0.09157902 -0.600687  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 794 is [False, True, False, True, False, False]
Current timestep = 795. State = [[-0.01955982 -0.25400284]]. Action = [[ 0.17348057  0.19286501 -0.12637909 -0.8991065 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 795 is [False, True, False, True, False, False]
Current timestep = 796. State = [[-0.01051234 -0.25910184]]. Action = [[-0.17757092 -0.1770236   0.07706338  0.49719226]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 796 is [False, True, False, True, False, False]
Current timestep = 797. State = [[-0.0166182  -0.27411267]]. Action = [[-0.09308097 -0.09399827 -0.0457533   0.25506783]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 797 is [False, True, False, True, False, False]
Current timestep = 798. State = [[-0.02086843 -0.28073823]]. Action = [[-0.02711427  0.04063988 -0.21606602 -0.35597044]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 798 is [False, True, False, True, False, False]
Current timestep = 799. State = [[-0.02239856 -0.28484127]]. Action = [[ 0.06039089 -0.08937398 -0.22697148 -0.40644288]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 799 is [False, True, False, True, False, False]
Current timestep = 800. State = [[-0.02300292 -0.28742588]]. Action = [[-0.04649043 -0.19753098 -0.22535396 -0.8427474 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 800 is [False, True, False, True, False, False]
Scene graph at timestep 800 is [False, True, False, True, False, False]
State prediction error at timestep 800 is tensor(1.1302e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[-0.01521724 -0.2763326 ]]. Action = [[ 0.23006496  0.20012283  0.10601473 -0.6918576 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 801 is [False, True, False, True, False, False]
Current timestep = 802. State = [[-0.00730902 -0.2735171 ]]. Action = [[-0.06423616 -0.14217646 -0.23703499 -0.1303668 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 802 is [False, True, False, True, False, False]
Current timestep = 803. State = [[-0.00800429 -0.2861697 ]]. Action = [[ 0.02926868 -0.15392812  0.23433417  0.9233856 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 803 is [False, True, False, True, False, False]
Scene graph at timestep 803 is [False, True, False, True, False, False]
State prediction error at timestep 803 is tensor(8.6473e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of -1
Current timestep = 804. State = [[ 0.00077132 -0.29605317]]. Action = [[ 0.20101365  0.03346592  0.2248247  -0.49530315]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 804 is [False, True, False, True, False, False]
Scene graph at timestep 804 is [False, True, False, True, False, False]
State prediction error at timestep 804 is tensor(9.1919e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of -1
Current timestep = 805. State = [[ 0.0274258 -0.2975049]]. Action = [[-0.07417819 -0.19966398 -0.1198571  -0.7494712 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 805 is [False, True, False, True, False, False]
Current timestep = 806. State = [[ 0.0270587 -0.2877354]]. Action = [[-0.1868626   0.24462774  0.112694   -0.3884977 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 806 is [False, True, False, True, False, False]
Current timestep = 807. State = [[ 0.0254521  -0.27812916]]. Action = [[-0.05518159 -0.02070349 -0.20054077 -0.1167503 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 807 is [False, True, False, True, False, False]
Current timestep = 808. State = [[ 0.02821503 -0.2652866 ]]. Action = [[ 0.20712468  0.20617217 -0.20315182 -0.0601027 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 808 is [False, True, False, True, False, False]
Current timestep = 809. State = [[ 0.03521197 -0.24973409]]. Action = [[ 0.1423184  -0.08685955 -0.18330318  0.9547123 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 809 is [False, True, False, True, False, False]
Current timestep = 810. State = [[ 0.03961307 -0.24994777]]. Action = [[ 0.22475582 -0.05194101  0.1662772  -0.0340246 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 810 is [False, True, False, True, False, False]
Current timestep = 811. State = [[ 0.04155307 -0.25000274]]. Action = [[-0.07763852  0.00528693  0.04178086 -0.22195113]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 811 is [False, True, False, True, False, False]
Scene graph at timestep 811 is [False, True, False, True, False, False]
State prediction error at timestep 811 is tensor(4.5319e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of 1
Current timestep = 812. State = [[ 0.04131715 -0.2502058 ]]. Action = [[0.22895867 0.10453814 0.07652009 0.06839585]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 812 is [False, True, False, True, False, False]
Current timestep = 813. State = [[ 0.04131715 -0.2502058 ]]. Action = [[0.19527704 0.1405671  0.00311464 0.60231423]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 813 is [False, True, False, True, False, False]
Current timestep = 814. State = [[ 0.03931913 -0.25403795]]. Action = [[-0.13627218 -0.03000654  0.23678136  0.13685763]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 814 is [False, True, False, True, False, False]
Current timestep = 815. State = [[ 0.0359313  -0.26163188]]. Action = [[-0.00944072 -0.06962213 -0.2391158  -0.46902496]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 815 is [False, True, False, True, False, False]
Current timestep = 816. State = [[ 0.03386412 -0.26563394]]. Action = [[0.2266495  0.06863093 0.22912768 0.90349305]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 816 is [False, True, False, True, False, False]
Current timestep = 817. State = [[ 0.03405389 -0.26547134]]. Action = [[ 0.11769933 -0.02182928 -0.17921716  0.2537129 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 817 is [False, True, False, True, False, False]
Current timestep = 818. State = [[ 0.03845644 -0.2566037 ]]. Action = [[ 0.12890112  0.15441486  0.02386117 -0.68789476]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 818 is [False, True, False, True, False, False]
Current timestep = 819. State = [[ 0.03821398 -0.2618028 ]]. Action = [[-0.13415527 -0.22731702 -0.01445436 -0.7892775 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 819 is [False, True, False, True, False, False]
Current timestep = 820. State = [[ 0.0350593  -0.28570583]]. Action = [[ 0.06508923 -0.23118523  0.11165506  0.4865632 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 820 is [False, True, False, True, False, False]
Current timestep = 821. State = [[ 0.03847957 -0.3051983 ]]. Action = [[ 0.13467926 -0.04901336 -0.17867364 -0.8190121 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 821 is [False, True, False, True, False, False]
Current timestep = 822. State = [[ 0.05734175 -0.30557925]]. Action = [[ 0.09717116  0.1305666   0.21052903 -0.7446558 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 822 is [False, True, False, True, False, False]
Current timestep = 823. State = [[ 0.07285207 -0.28986508]]. Action = [[ 0.03164828  0.23819286 -0.22310984 -0.81727415]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 823 is [False, False, True, True, False, False]
Scene graph at timestep 823 is [False, False, True, True, False, False]
State prediction error at timestep 823 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[ 0.07712691 -0.26829702]]. Action = [[-0.01494166  0.24130195  0.19393313 -0.40835673]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 824 is [False, False, True, True, False, False]
Current timestep = 825. State = [[ 0.07712691 -0.26829702]]. Action = [[ 0.20449269 -0.1263155   0.1250937   0.8214636 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 825 is [False, False, True, True, False, False]
Scene graph at timestep 825 is [False, False, True, True, False, False]
State prediction error at timestep 825 is tensor(7.9825e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[ 0.07712691 -0.26829702]]. Action = [[ 0.14693618 -0.11765119 -0.05344586 -0.612921  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 826 is [False, False, True, True, False, False]
Scene graph at timestep 826 is [False, False, True, True, False, False]
State prediction error at timestep 826 is tensor(2.1459e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of -1
Current timestep = 827. State = [[ 0.07712691 -0.26829702]]. Action = [[0.23675579 0.07722238 0.23587662 0.4290625 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 827 is [False, False, True, True, False, False]
Current timestep = 828. State = [[ 0.07712691 -0.26829702]]. Action = [[ 0.07863277 -0.19065297 -0.19644235  0.5139742 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 828 is [False, False, True, True, False, False]
Current timestep = 829. State = [[ 0.07712691 -0.26829702]]. Action = [[ 0.16181943 -0.00100037  0.06150198  0.29580665]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 829 is [False, False, True, True, False, False]
Scene graph at timestep 829 is [False, False, True, True, False, False]
State prediction error at timestep 829 is tensor(5.6738e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of -1
Current timestep = 830. State = [[ 0.07712691 -0.26829702]]. Action = [[0.04543757 0.0597246  0.17725205 0.5404587 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 830 is [False, False, True, True, False, False]
Scene graph at timestep 830 is [False, False, True, True, False, False]
State prediction error at timestep 830 is tensor(4.6408e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of -1
Current timestep = 831. State = [[ 0.07573955 -0.26729673]]. Action = [[-0.23653904  0.07634515 -0.16824366 -0.10848665]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 831 is [False, False, True, True, False, False]
Scene graph at timestep 831 is [False, False, True, True, False, False]
State prediction error at timestep 831 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[ 0.06996973 -0.26330575]]. Action = [[ 0.03207296  0.19837528 -0.22304817  0.24909711]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 832 is [False, False, True, True, False, False]
Current timestep = 833. State = [[ 0.06667102 -0.26563707]]. Action = [[-0.09929176 -0.0330594  -0.13658394 -0.367692  ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 833 is [False, False, True, True, False, False]
Current timestep = 834. State = [[ 0.05533361 -0.26591122]]. Action = [[-0.1679914   0.05009887 -0.10334286  0.7507968 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 834 is [False, False, True, True, False, False]
Current timestep = 835. State = [[ 0.03987546 -0.26296487]]. Action = [[ 0.08628643 -0.0192447  -0.17506671  0.9601364 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 835 is [False, False, True, True, False, False]
Current timestep = 836. State = [[ 0.03152526 -0.27390447]]. Action = [[-0.13925457 -0.1793131   0.15410823  0.58943594]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 836 is [False, True, False, True, False, False]
Current timestep = 837. State = [[ 0.01198368 -0.2736028 ]]. Action = [[-0.22891009  0.20445645 -0.1759354   0.3854612 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 837 is [False, True, False, True, False, False]
Scene graph at timestep 837 is [False, True, False, True, False, False]
State prediction error at timestep 837 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[-0.01377255 -0.2617063 ]]. Action = [[ 0.19506967 -0.11400503 -0.08985308  0.7990267 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 838 is [False, True, False, True, False, False]
Scene graph at timestep 838 is [False, True, False, True, False, False]
State prediction error at timestep 838 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 1
Current timestep = 839. State = [[-0.00148654 -0.25304455]]. Action = [[ 0.21015823  0.16972938  0.21244013 -0.35546982]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 839 is [False, True, False, True, False, False]
Scene graph at timestep 839 is [False, True, False, True, False, False]
State prediction error at timestep 839 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[ 0.0138483  -0.23074006]]. Action = [[ 0.09481072  0.18599242  0.04513642 -0.355641  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 840 is [False, True, False, True, False, False]
Current timestep = 841. State = [[ 0.02975452 -0.20297457]]. Action = [[ 0.23421869  0.22001791 -0.04614937 -0.54071695]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 841 is [False, True, False, True, False, False]
Current timestep = 842. State = [[ 0.04979166 -0.1864154 ]]. Action = [[ 0.2283085  -0.00574765  0.21357173  0.5316123 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 842 is [False, True, False, True, False, False]
Scene graph at timestep 842 is [False, True, False, True, False, False]
State prediction error at timestep 842 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 1
Current timestep = 843. State = [[ 0.05251709 -0.18285075]]. Action = [[ 0.17156535 -0.23490225 -0.00886993  0.19035757]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 843 is [False, True, False, True, False, False]
Current timestep = 844. State = [[ 0.05109531 -0.18780698]]. Action = [[-0.17173487 -0.08612162  0.05668062  0.28629768]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 844 is [False, False, True, True, False, False]
Scene graph at timestep 844 is [False, False, True, True, False, False]
State prediction error at timestep 844 is tensor(8.0999e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of -1
Current timestep = 845. State = [[ 0.04931404 -0.19400224]]. Action = [[-6.4526498e-04 -4.1299984e-02 -9.8349899e-02 -7.9383922e-01]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 845 is [False, False, True, True, False, False]
Current timestep = 846. State = [[ 0.0371494  -0.21049336]]. Action = [[-0.23000106 -0.19526601  0.19369423  0.71580374]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 846 is [False, True, False, True, False, False]
Scene graph at timestep 846 is [False, True, False, True, False, False]
State prediction error at timestep 846 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of -1
Current timestep = 847. State = [[ 0.02599201 -0.21769714]]. Action = [[ 0.1967848   0.18150708  0.16913664 -0.13506222]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 847 is [False, True, False, True, False, False]
Current timestep = 848. State = [[ 0.03679594 -0.20239113]]. Action = [[ 0.21599162  0.03018939  0.03694227 -0.67571515]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 848 is [False, True, False, True, False, False]
Current timestep = 849. State = [[ 0.05859523 -0.18648593]]. Action = [[ 0.10624856  0.20174092  0.11303848 -0.402233  ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 849 is [False, True, False, True, False, False]
Current timestep = 850. State = [[ 0.06946795 -0.17163172]]. Action = [[ 0.10959905 -0.03429058 -0.22098546  0.89608264]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 850 is [False, False, True, True, False, False]
Scene graph at timestep 850 is [False, False, True, True, False, False]
State prediction error at timestep 850 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of -1
Current timestep = 851. State = [[ 0.07453188 -0.1676591 ]]. Action = [[-0.0043814  -0.20794262 -0.04298109  0.94178164]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 851 is [False, False, True, True, False, False]
Current timestep = 852. State = [[ 0.07453188 -0.1676591 ]]. Action = [[ 0.18427578  0.01161277 -0.11288273  0.52110696]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 852 is [False, False, True, True, False, False]
Current timestep = 853. State = [[ 0.07453188 -0.1676591 ]]. Action = [[ 0.0061174  -0.22546579 -0.130252    0.11361229]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 853 is [False, False, True, True, False, False]
Current timestep = 854. State = [[ 0.07453188 -0.1676591 ]]. Action = [[ 0.20932704  0.03895542  0.18177897 -0.9091161 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 854 is [False, False, True, True, False, False]
Current timestep = 855. State = [[ 0.07453188 -0.1676591 ]]. Action = [[ 0.23252052 -0.20916559 -0.17529361 -0.78233254]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 855 is [False, False, True, True, False, False]
Current timestep = 856. State = [[ 0.07453188 -0.1676591 ]]. Action = [[0.12054142 0.19222379 0.19928408 0.27085936]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 856 is [False, False, True, True, False, False]
Scene graph at timestep 856 is [False, False, True, True, False, False]
State prediction error at timestep 856 is tensor(2.5064e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of -1
Current timestep = 857. State = [[ 0.07453188 -0.1676591 ]]. Action = [[ 0.24497324 -0.10779867  0.2433222   0.09734321]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 857 is [False, False, True, True, False, False]
Scene graph at timestep 857 is [False, False, True, True, False, False]
State prediction error at timestep 857 is tensor(1.2632e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of -1
Current timestep = 858. State = [[ 0.07235293 -0.17403388]]. Action = [[-0.21222103 -0.08995783 -0.23740329  0.20484614]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 858 is [False, False, True, True, False, False]
Current timestep = 859. State = [[ 0.06905869 -0.17950605]]. Action = [[ 0.15237606 -0.06926388  0.01539531  0.18316352]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 859 is [False, False, True, True, False, False]
Current timestep = 860. State = [[ 0.06881894 -0.17991795]]. Action = [[ 0.1626845 -0.012545   0.1662153 -0.3680818]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 860 is [False, False, True, True, False, False]
Current timestep = 861. State = [[ 0.06773034 -0.17888801]]. Action = [[-0.06163946  0.04506969  0.20038584 -0.63254815]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 861 is [False, False, True, True, False, False]
Current timestep = 862. State = [[ 0.06691736 -0.17834495]]. Action = [[ 0.1787656   0.18041095 -0.20342739 -0.21207559]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 862 is [False, False, True, True, False, False]
Current timestep = 863. State = [[ 0.06665343 -0.17843954]]. Action = [[ 0.22465801  0.23951948  0.16865101 -0.3629849 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 863 is [False, False, True, True, False, False]
Current timestep = 864. State = [[ 0.06527521 -0.17637704]]. Action = [[-0.04591861  0.03471187 -0.17491263  0.3046564 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 864 is [False, False, True, True, False, False]
Scene graph at timestep 864 is [False, False, True, True, False, False]
State prediction error at timestep 864 is tensor(1.1146e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of -1
Current timestep = 865. State = [[ 0.06033506 -0.1739535 ]]. Action = [[ 0.21445554  0.20484212 -0.13971078 -0.2366724 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 865 is [False, False, True, True, False, False]
Scene graph at timestep 865 is [False, False, True, True, False, False]
State prediction error at timestep 865 is tensor(8.7172e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 865 of -1
Current timestep = 866. State = [[ 0.06028799 -0.17396487]]. Action = [[ 0.17744815 -0.1958788  -0.17688158 -0.6772513 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 866 is [False, False, True, True, False, False]
Current timestep = 867. State = [[ 0.05509626 -0.1850479 ]]. Action = [[-0.07355042 -0.18341699 -0.20996992  0.2334199 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 867 is [False, False, True, True, False, False]
Current timestep = 868. State = [[ 0.04682376 -0.1943824 ]]. Action = [[0.09819725 0.21557271 0.11748967 0.64672923]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 868 is [False, False, True, True, False, False]
Current timestep = 869. State = [[ 0.04354379 -0.19836627]]. Action = [[-0.07405955 -0.02039725 -0.03280383 -0.18898445]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 869 is [False, True, False, True, False, False]
Current timestep = 870. State = [[ 0.04140479 -0.20103428]]. Action = [[ 0.11624154 -0.02930969  0.11229488 -0.69394124]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 870 is [False, True, False, True, False, False]
Current timestep = 871. State = [[ 0.03801393 -0.19060384]]. Action = [[-0.19593874  0.2210384   0.07614097 -0.36509287]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 871 is [False, True, False, True, False, False]
Current timestep = 872. State = [[ 0.02954714 -0.1631632 ]]. Action = [[0.00393096 0.22259557 0.23855361 0.49287617]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 872 is [False, True, False, True, False, False]
Current timestep = 873. State = [[ 0.02742244 -0.13926834]]. Action = [[ 0.04620838  0.06723964  0.04666588 -0.01718336]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 873 is [False, True, False, True, False, False]
Current timestep = 874. State = [[ 0.02559812 -0.12668245]]. Action = [[-0.07445955  0.07161975 -0.08763343  0.6225326 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 874 is [False, True, False, True, False, False]
Current timestep = 875. State = [[ 0.02327694 -0.1339032 ]]. Action = [[ 0.10414287 -0.2480416   0.16232261 -0.4827528 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 875 is [False, True, False, True, False, False]
Current timestep = 876. State = [[-0.17920174  0.06027061]]. Action = [[-0.1653904   0.10755879  0.23792747 -0.63841516]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 876 is [False, True, False, True, False, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.16310021  0.05812578]]. Action = [[-0.05325562 -0.21782267 -0.23651496  0.8425789 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 877 is [True, False, False, False, True, False]
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(2.0035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of 1
Current timestep = 878. State = [[-0.16930966  0.04390831]]. Action = [[-1.8127210e-01  3.2308698e-04  4.1715771e-02 -8.7930661e-01]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 878 is [True, False, False, False, True, False]
Current timestep = 879. State = [[-0.1738967   0.02790135]]. Action = [[ 0.04473007 -0.24226491 -0.12766519 -0.6648974 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 879 is [True, False, False, False, True, False]
Current timestep = 880. State = [[-0.1715749   0.02526908]]. Action = [[0.1703771  0.23476803 0.1353142  0.73589563]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.16807027  0.03292172]]. Action = [[-0.04362518 -0.02278663 -0.22292838  0.10009873]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 881 is [True, False, False, False, True, False]
Current timestep = 882. State = [[-0.17554079  0.02073335]]. Action = [[-0.24307679 -0.23827419  0.03646722 -0.3978778 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 882 is [True, False, False, False, True, False]
Current timestep = 883. State = [[-0.1851399   0.01095944]]. Action = [[ 0.01744458  0.06880179 -0.12212799  0.6690862 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 883 is [True, False, False, False, True, False]
Current timestep = 884. State = [[-0.18725163  0.01962129]]. Action = [[ 0.02754256  0.13307595 -0.21985738 -0.6194863 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.19024104  0.01760656]]. Action = [[-0.07092834 -0.19038236 -0.2367199   0.25732422]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.19463564 -0.00222521]]. Action = [[-0.04336195 -0.17451115  0.16559637  0.20659018]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 886 is [True, False, False, False, True, False]
Current timestep = 887. State = [[-0.19816735 -0.02414902]]. Action = [[ 0.01131406 -0.13494685 -0.06889549 -0.02920115]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 887 is [True, False, False, False, True, False]
Scene graph at timestep 887 is [True, False, False, False, True, False]
State prediction error at timestep 887 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of -1
Current timestep = 888. State = [[-0.1963287 -0.0283367]]. Action = [[ 0.14777431  0.17405441 -0.12950058 -0.16387641]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 888 is [True, False, False, False, True, False]
Current timestep = 889. State = [[-0.18875466 -0.03340858]]. Action = [[ 0.09093377 -0.23078251 -0.101174    0.20240378]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.18466917 -0.04006683]]. Action = [[-0.03609625  0.09997845  0.21965021  0.09824181]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 890 is [True, False, False, False, True, False]
Current timestep = 891. State = [[-0.19064087 -0.02456509]]. Action = [[-0.19416124  0.22951221  0.1481334  -0.51824194]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 891 is [True, False, False, False, True, False]
Current timestep = 892. State = [[-0.2015117  -0.01541837]]. Action = [[-0.12760843 -0.14275947 -0.14459985 -0.44743776]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 892 is [True, False, False, False, True, False]
Current timestep = 893. State = [[-0.20378864 -0.01990754]]. Action = [[ 0.18824428  0.01030731 -0.23004907  0.8288579 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 893 is [True, False, False, False, True, False]
Current timestep = 894. State = [[-0.20034131 -0.03024955]]. Action = [[-0.04996845 -0.18090184 -0.12215824 -0.37715757]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 894 is [True, False, False, False, True, False]
Current timestep = 895. State = [[-0.19371204 -0.04645801]]. Action = [[ 0.18489328 -0.07586379 -0.07143304 -0.9383102 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 895 is [True, False, False, False, True, False]
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of -1
Current timestep = 896. State = [[-0.17479807 -0.04676847]]. Action = [[ 0.22788638  0.16173083 -0.19290942  0.25717354]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 896 is [True, False, False, False, True, False]
Current timestep = 897. State = [[-0.15129411 -0.04658748]]. Action = [[ 0.13020712 -0.13603166  0.19398046  0.41156864]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 897 is [True, False, False, False, True, False]
Current timestep = 898. State = [[-0.14313298 -0.04382445]]. Action = [[-0.11738235  0.14095056  0.04725289 -0.14135367]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.13753654 -0.02642321]]. Action = [[ 0.17668378  0.1977247  -0.01338159 -0.9050005 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 899 is [True, False, False, False, True, False]
Current timestep = 900. State = [[-0.12908028 -0.0257381 ]]. Action = [[ 0.01254067 -0.24495026 -0.10808131 -0.5741406 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.12423841 -0.04132308]]. Action = [[ 0.05114457 -0.07700205  0.21232337 -0.09104687]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 901 is [True, False, False, False, True, False]
Current timestep = 902. State = [[-0.11357129 -0.05861799]]. Action = [[ 0.1428453  -0.16047293  0.21979761  0.6575146 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 902 is [True, False, False, False, True, False]
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 902 of 1
Current timestep = 903. State = [[-0.08940081 -0.07840443]]. Action = [[ 0.17086136 -0.09446412 -0.14573728  0.9305974 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.07305527 -0.07452513]]. Action = [[ 0.04836169  0.20555937 -0.03245239  0.24842346]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 904 is [True, False, False, False, True, False]
Current timestep = 905. State = [[-0.06844816 -0.05315785]]. Action = [[-0.05479234  0.19141167 -0.17140944  0.25029933]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.0711507  -0.03639061]]. Action = [[-0.02067266  0.02710775 -0.21565276 -0.00463837]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.06230491 -0.03938023]]. Action = [[ 0.2417754  -0.13100566 -0.23440596 -0.20694721]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.04942517 -0.04875815]]. Action = [[-0.13793221 -0.07956292 -0.12973978  0.6025963 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 908 is [True, False, False, False, True, False]
Current timestep = 909. State = [[-0.05066032 -0.05185331]]. Action = [[ 0.0197697   0.0616008  -0.22713716 -0.79490745]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 909 is [False, True, False, False, True, False]
Scene graph at timestep 909 is [True, False, False, False, True, False]
State prediction error at timestep 909 is tensor(1.8808e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 909 of 1
Current timestep = 910. State = [[-0.05697774 -0.05760748]]. Action = [[-0.21484527 -0.09883425  0.1938352  -0.01907712]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 910 is [True, False, False, False, True, False]
Current timestep = 911. State = [[-0.06957284 -0.05292666]]. Action = [[-0.17183092  0.16878247 -0.16144608 -0.7191848 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 911 is [True, False, False, False, True, False]
Scene graph at timestep 911 is [True, False, False, False, True, False]
State prediction error at timestep 911 is tensor(6.3575e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 911 of -1
Current timestep = 912. State = [[-0.08132169 -0.03593265]]. Action = [[0.24095607 0.09058303 0.04472589 0.76665187]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 912 is [True, False, False, False, True, False]
Current timestep = 913. State = [[-0.07685588 -0.02989945]]. Action = [[0.02761534 0.02342266 0.18588969 0.8421259 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.07640534 -0.04088204]]. Action = [[-0.15401189 -0.24070293 -0.23834287  0.7513883 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 914 is [True, False, False, False, True, False]
Current timestep = 915. State = [[-0.07708315 -0.03998943]]. Action = [[0.12743708 0.2278597  0.15583044 0.27959847]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 915 is [True, False, False, False, True, False]
Current timestep = 916. State = [[-0.06806704 -0.04063655]]. Action = [[ 0.1840406  -0.18645117  0.02129027 -0.76100534]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.05390646 -0.04767056]]. Action = [[ 0.03834671  0.00343627 -0.00114802 -0.729843  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.05068388 -0.03705502]]. Action = [[-0.02841094  0.21673197 -0.23670606 -0.6458046 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.04962754 -0.03111739]]. Action = [[ 0.0209364  -0.10474007 -0.04537761  0.14898479]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.0474463  -0.04477947]]. Action = [[ 0.02190143 -0.1924908  -0.22097535  0.0498836 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 920 is [False, True, False, False, True, False]
Current timestep = 921. State = [[-0.04895424 -0.05258351]]. Action = [[-0.21546514  0.09791034 -0.11411364 -0.27317023]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 921 is [False, True, False, False, True, False]
Scene graph at timestep 921 is [False, True, False, False, True, False]
State prediction error at timestep 921 is tensor(3.9814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 921 of 1
Current timestep = 922. State = [[-0.04997249 -0.0570588 ]]. Action = [[ 0.17901194 -0.11196581 -0.1676972   0.30630517]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 922 is [False, True, False, False, True, False]
Current timestep = 923. State = [[-0.04320177 -0.05388362]]. Action = [[ 0.13177824  0.1442306   0.01682267 -0.81016773]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 923 is [False, True, False, False, True, False]
Current timestep = 924. State = [[-0.1735336  -0.20737536]]. Action = [[-0.06635362  0.02944812 -0.19583958  0.04272008]]. Reward = [100.]
Curr episode timestep = 47
Scene graph at timestep 924 is [False, True, False, False, True, False]
Scene graph at timestep 924 is [True, False, False, True, False, False]
State prediction error at timestep 924 is tensor(0.0219, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 924 of 1
Current timestep = 925. State = [[-0.16040456 -0.219659  ]]. Action = [[0.01055709 0.19258368 0.19350424 0.82535934]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 925 is [True, False, False, True, False, False]
Scene graph at timestep 925 is [True, False, False, True, False, False]
State prediction error at timestep 925 is tensor(9.1167e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 925 of 1
Current timestep = 926. State = [[-0.15643597 -0.21043313]]. Action = [[-0.04431665 -0.00288697  0.18623912  0.3574983 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 926 is [True, False, False, True, False, False]
Scene graph at timestep 926 is [True, False, False, True, False, False]
State prediction error at timestep 926 is tensor(9.1614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.147821   -0.20510718]]. Action = [[ 0.23911646  0.06081724 -0.2154832   0.4864248 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 927 is [True, False, False, True, False, False]
Scene graph at timestep 927 is [True, False, False, True, False, False]
State prediction error at timestep 927 is tensor(2.3318e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of 1
Current timestep = 928. State = [[-0.12097875 -0.19430862]]. Action = [[ 0.23066258  0.08224416  0.15369147 -0.59501266]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 928 is [True, False, False, True, False, False]
Current timestep = 929. State = [[-0.09925139 -0.18565615]]. Action = [[ 0.0472292   0.07869837 -0.22988145 -0.2049191 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 929 is [True, False, False, True, False, False]
Current timestep = 930. State = [[-0.09190305 -0.18272917]]. Action = [[-0.0077755  -0.05329792  0.23597139  0.69801915]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 930 is [True, False, False, True, False, False]
Current timestep = 931. State = [[-0.08830116 -0.17324096]]. Action = [[ 0.05330786  0.17159307 -0.1827722   0.20283508]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 931 is [True, False, False, True, False, False]
Current timestep = 932. State = [[-0.07828669 -0.1730319 ]]. Action = [[ 0.17974395 -0.18761216 -0.04343396 -0.08480221]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 932 is [True, False, False, True, False, False]
Current timestep = 933. State = [[-0.05595032 -0.17278384]]. Action = [[ 0.11071163  0.1577012  -0.03938532  0.32415342]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 933 is [True, False, False, True, False, False]
Scene graph at timestep 933 is [True, False, False, True, False, False]
State prediction error at timestep 933 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 1
Current timestep = 934. State = [[-0.03350005 -0.17741533]]. Action = [[ 0.19288331 -0.21989961  0.23603463 -0.6616991 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 934 is [True, False, False, True, False, False]
Current timestep = 935. State = [[-0.00688048 -0.20214565]]. Action = [[ 0.22033447 -0.22131805 -0.02536026 -0.48787916]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 935 is [False, True, False, True, False, False]
Current timestep = 936. State = [[ 0.02128523 -0.23341887]]. Action = [[ 0.16638982 -0.23205797 -0.09697488 -0.8917582 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 936 is [False, True, False, True, False, False]
Scene graph at timestep 936 is [False, True, False, True, False, False]
State prediction error at timestep 936 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of -1
Current timestep = 937. State = [[ 0.04292424 -0.2720257 ]]. Action = [[-0.16596147 -0.15103798 -0.03846544  0.39326453]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 937 is [False, True, False, True, False, False]
Current timestep = 938. State = [[ 0.03825708 -0.28200325]]. Action = [[-0.17744178 -0.23113327 -0.01501305  0.26445842]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 938 is [False, True, False, True, False, False]
Current timestep = 939. State = [[ 0.03773453 -0.28354242]]. Action = [[ 0.24029219 -0.09981945 -0.05177172  0.8545673 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 939 is [False, True, False, True, False, False]
Current timestep = 940. State = [[ 0.03765592 -0.2836789 ]]. Action = [[ 0.02547538 -0.1927118  -0.12536936 -0.13547546]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 940 is [False, True, False, True, False, False]
Current timestep = 941. State = [[ 0.03768004 -0.28366405]]. Action = [[0.24474391 0.10371706 0.23594397 0.96864414]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 941 is [False, True, False, True, False, False]
Scene graph at timestep 941 is [False, True, False, True, False, False]
State prediction error at timestep 941 is tensor(3.2168e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of -1
Current timestep = 942. State = [[ 0.03768004 -0.28366405]]. Action = [[ 0.17149073 -0.0541755  -0.13838948  0.5955572 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 942 is [False, True, False, True, False, False]
Scene graph at timestep 942 is [False, True, False, True, False, False]
State prediction error at timestep 942 is tensor(5.3396e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 942 of -1
Current timestep = 943. State = [[ 0.03766234 -0.28349862]]. Action = [[ 0.04023582  0.02652872 -0.24084887  0.63622546]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 943 is [False, True, False, True, False, False]
Current timestep = 944. State = [[ 0.03742588 -0.28393584]]. Action = [[-0.03855675 -0.02916646 -0.16776767 -0.80004334]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 944 is [False, True, False, True, False, False]
Scene graph at timestep 944 is [False, True, False, True, False, False]
State prediction error at timestep 944 is tensor(5.9305e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of -1
Current timestep = 945. State = [[ 0.03437412 -0.2769622 ]]. Action = [[-0.18071854  0.18514854  0.05273855 -0.18536514]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 945 is [False, True, False, True, False, False]
Current timestep = 946. State = [[ 0.03158679 -0.27061304]]. Action = [[ 0.18964547  0.00767481 -0.19315003  0.86431885]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 946 is [False, True, False, True, False, False]
Scene graph at timestep 946 is [False, True, False, True, False, False]
State prediction error at timestep 946 is tensor(1.2943e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 946 of 1
Current timestep = 947. State = [[ 0.02799986 -0.27723795]]. Action = [[-0.03991395 -0.13731882  0.10420161  0.60130215]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 947 is [False, True, False, True, False, False]
Current timestep = 948. State = [[ 0.01764725 -0.2867961 ]]. Action = [[-0.12138081 -0.03217243  0.20212495 -0.07299042]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 948 is [False, True, False, True, False, False]
Current timestep = 949. State = [[ 0.0083175  -0.27777618]]. Action = [[-0.07049902  0.21888119  0.01358509  0.6139046 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 949 is [False, True, False, True, False, False]
Current timestep = 950. State = [[-0.0078509  -0.26006007]]. Action = [[-0.15529239  0.06880176 -0.19652323 -0.8510556 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 950 is [False, True, False, True, False, False]
Scene graph at timestep 950 is [False, True, False, True, False, False]
State prediction error at timestep 950 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of -1
Current timestep = 951. State = [[-0.02428948 -0.25733247]]. Action = [[ 0.07720456 -0.1275177  -0.24758819  0.20895481]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 951 is [False, True, False, True, False, False]
Scene graph at timestep 951 is [False, True, False, True, False, False]
State prediction error at timestep 951 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[-0.02345829 -0.26218638]]. Action = [[ 0.11036605 -0.04425703  0.0266037  -0.23530245]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 952 is [False, True, False, True, False, False]
Current timestep = 953. State = [[-0.02295148 -0.26604164]]. Action = [[-0.10191861 -0.01405063  0.20370871  0.30953836]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 953 is [False, True, False, True, False, False]
Scene graph at timestep 953 is [False, True, False, True, False, False]
State prediction error at timestep 953 is tensor(2.2578e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 953 of -1
Current timestep = 954. State = [[-0.02427169 -0.27542108]]. Action = [[ 0.09104779 -0.13470232 -0.00395262  0.7294862 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 954 is [False, True, False, True, False, False]
Scene graph at timestep 954 is [False, True, False, True, False, False]
State prediction error at timestep 954 is tensor(8.9653e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of -1
Current timestep = 955. State = [[-0.01576387 -0.27483636]]. Action = [[ 0.2115502   0.19504553  0.24158508 -0.7338919 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 955 is [False, True, False, True, False, False]
Current timestep = 956. State = [[-0.00592447 -0.26638505]]. Action = [[-0.14239158 -0.00165114 -0.0846023   0.7511549 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 956 is [False, True, False, True, False, False]
Current timestep = 957. State = [[-0.00734468 -0.27357802]]. Action = [[ 0.03542852 -0.15081605  0.04003626 -0.02759689]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 957 is [False, True, False, True, False, False]
Scene graph at timestep 957 is [False, True, False, True, False, False]
State prediction error at timestep 957 is tensor(1.4107e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of -1
Current timestep = 958. State = [[-0.00135377 -0.27941963]]. Action = [[ 0.21473509  0.00447556 -0.08425522  0.7110095 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 958 is [False, True, False, True, False, False]
Current timestep = 959. State = [[ 0.02237425 -0.2868428 ]]. Action = [[ 0.24877492 -0.14378929 -0.11463448  0.14841974]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 959 is [False, True, False, True, False, False]
Current timestep = 960. State = [[ 0.04493716 -0.29503095]]. Action = [[ 0.03478456 -0.23259313  0.22844154  0.23227346]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 960 is [False, True, False, True, False, False]
Current timestep = 961. State = [[ 0.05188377 -0.29683083]]. Action = [[ 0.19634199  0.00282636  0.19696105 -0.89407146]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 961 is [False, True, False, True, False, False]
Scene graph at timestep 961 is [False, False, True, True, False, False]
State prediction error at timestep 961 is tensor(4.1120e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 961 of -1
Current timestep = 962. State = [[ 0.05174833 -0.29751006]]. Action = [[ 0.24594998  0.23093313 -0.11159325 -0.8815808 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 962 is [False, False, True, True, False, False]
Current timestep = 963. State = [[ 0.04996976 -0.30108896]]. Action = [[-0.15202142 -0.00498575  0.20597574 -0.38853967]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 963 is [False, False, True, True, False, False]
Current timestep = 964. State = [[ 0.04792979 -0.30455363]]. Action = [[-0.2071008  -0.230228    0.20137697 -0.28099298]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 964 is [False, True, False, True, False, False]
Current timestep = 965. State = [[ 0.04564962 -0.2972777 ]]. Action = [[-0.13382177  0.18845347 -0.05898911 -0.6956643 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 965 is [False, True, False, True, False, False]
Scene graph at timestep 965 is [False, True, False, True, False, False]
State prediction error at timestep 965 is tensor(1.8008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of -1
Current timestep = 966. State = [[ 0.04295264 -0.2871224 ]]. Action = [[0.2367962  0.04283497 0.09935263 0.2848742 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 966 is [False, True, False, True, False, False]
Scene graph at timestep 966 is [False, True, False, True, False, False]
State prediction error at timestep 966 is tensor(1.4355e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of -1
Current timestep = 967. State = [[ 0.04296808 -0.28710526]]. Action = [[ 0.10502285 -0.03810593 -0.03131875 -0.8761811 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 967 is [False, True, False, True, False, False]
Scene graph at timestep 967 is [False, True, False, True, False, False]
State prediction error at timestep 967 is tensor(8.9856e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 967 of -1
Current timestep = 968. State = [[ 0.04400839 -0.28137168]]. Action = [[-0.03398907  0.11143807  0.03544503 -0.8826668 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 968 is [False, True, False, True, False, False]
Current timestep = 969. State = [[ 0.04533845 -0.27745637]]. Action = [[ 0.12683201 -0.09690267  0.21015513 -0.4764011 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 969 is [False, True, False, True, False, False]
Current timestep = 970. State = [[ 0.04121123 -0.2874273 ]]. Action = [[-0.2342542  -0.09798461  0.1581437   0.28889763]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 970 is [False, True, False, True, False, False]
Scene graph at timestep 970 is [False, True, False, True, False, False]
State prediction error at timestep 970 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of -1
Current timestep = 971. State = [[ 0.02699165 -0.3025895 ]]. Action = [[-0.18224776 -0.04198427  0.06861556  0.8116236 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 971 is [False, True, False, True, False, False]
Scene graph at timestep 971 is [False, True, False, True, False, False]
State prediction error at timestep 971 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of -1
Current timestep = 972. State = [[ 0.0123113 -0.3067231]]. Action = [[-0.10653789 -0.21454601 -0.05577023 -0.24667662]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 972 is [False, True, False, True, False, False]
Current timestep = 973. State = [[ 0.01320067 -0.30238515]]. Action = [[0.06003833 0.0826284  0.16547918 0.752579  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 973 is [False, True, False, True, False, False]
Current timestep = 974. State = [[ 0.01401176 -0.29904026]]. Action = [[ 0.19921595 -0.1430632   0.2109924  -0.14496917]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 974 is [False, True, False, True, False, False]
Current timestep = 975. State = [[ 0.01657943 -0.29084527]]. Action = [[ 0.10403275  0.09714198 -0.16277769  0.9218495 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 975 is [False, True, False, True, False, False]
Scene graph at timestep 975 is [False, True, False, True, False, False]
State prediction error at timestep 975 is tensor(2.0602e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[ 0.0187834 -0.2824119]]. Action = [[-0.03897931 -0.04145201 -0.02155408 -0.3575163 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 976 is [False, True, False, True, False, False]
Current timestep = 977. State = [[ 0.01629274 -0.28397968]]. Action = [[-0.12299931  0.01333156  0.1830047   0.00363719]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 977 is [False, True, False, True, False, False]
Current timestep = 978. State = [[ 0.01106692 -0.28404894]]. Action = [[-0.05017149 -0.23878326 -0.00278635 -0.1824277 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 978 is [False, True, False, True, False, False]
Current timestep = 979. State = [[ 0.01050236 -0.2839051 ]]. Action = [[ 0.15195209 -0.23982129  0.2450526  -0.9848995 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 979 is [False, True, False, True, False, False]
Current timestep = 980. State = [[ 0.01049645 -0.28388903]]. Action = [[ 0.20744318 -0.17106363  0.17251039 -0.53764033]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 980 is [False, True, False, True, False, False]
Current timestep = 981. State = [[ 0.01497963 -0.27319485]]. Action = [[0.20344073 0.16447958 0.24088061 0.11336875]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 981 is [False, True, False, True, False, False]
Scene graph at timestep 981 is [False, True, False, True, False, False]
State prediction error at timestep 981 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 981 of 1
Current timestep = 982. State = [[ 0.0190045  -0.26007968]]. Action = [[-0.0299802  -0.04693158  0.02869576  0.39439356]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 982 is [False, True, False, True, False, False]
Current timestep = 983. State = [[ 0.02295965 -0.2595465 ]]. Action = [[0.1632539  0.02494159 0.21060544 0.43662083]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 983 is [False, True, False, True, False, False]
Current timestep = 984. State = [[ 0.02648589 -0.25895748]]. Action = [[-0.24407026  0.04853737  0.15115044  0.7845075 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 984 is [False, True, False, True, False, False]
Current timestep = 985. State = [[ 0.02797458 -0.25289539]]. Action = [[ 0.20146155  0.06490183  0.20524555 -0.6371198 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 985 is [False, True, False, True, False, False]
Current timestep = 986. State = [[ 0.0288338  -0.23567528]]. Action = [[-0.1849907   0.19423929  0.11240119  0.64906096]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 986 is [False, True, False, True, False, False]
Current timestep = 987. State = [[ 0.01644457 -0.23300046]]. Action = [[-0.22793911 -0.16736695 -0.1099291  -0.07625449]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 987 is [False, True, False, True, False, False]
Current timestep = 988. State = [[ 0.00206614 -0.23716013]]. Action = [[ 0.09506586  0.04336286 -0.18050002  0.43136036]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 988 is [False, True, False, True, False, False]
Current timestep = 989. State = [[-0.00077343 -0.2241876 ]]. Action = [[-0.11776449  0.21227515  0.15100035  0.3988899 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 989 is [False, True, False, True, False, False]
Current timestep = 990. State = [[-0.00165238 -0.19514745]]. Action = [[ 0.20439106  0.21059608 -0.21684003 -0.10685313]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 990 is [False, True, False, True, False, False]
Current timestep = 991. State = [[ 0.00615625 -0.1782204 ]]. Action = [[ 0.15555236 -0.06829619 -0.0616463  -0.4667259 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 991 is [False, True, False, True, False, False]
Scene graph at timestep 991 is [False, True, False, True, False, False]
State prediction error at timestep 991 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[ 0.01353043 -0.17579299]]. Action = [[ 0.03912053  0.03948307 -0.07642162 -0.33609605]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 992 is [False, True, False, True, False, False]
Current timestep = 993. State = [[ 0.01457435 -0.16593173]]. Action = [[-0.1294814   0.14841747 -0.20727974  0.45220637]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 993 is [False, True, False, True, False, False]
Current timestep = 994. State = [[ 0.01289681 -0.16753837]]. Action = [[-0.02822326 -0.21142823 -0.02170508 -0.52754873]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 994 is [False, True, False, True, False, False]
Current timestep = 995. State = [[ 0.01217792 -0.1788644 ]]. Action = [[ 0.03703687 -0.04601994  0.2048938   0.9045143 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 995 is [False, True, False, True, False, False]
Scene graph at timestep 995 is [False, True, False, True, False, False]
State prediction error at timestep 995 is tensor(4.3510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of -1
Current timestep = 996. State = [[ 0.00414598 -0.19245757]]. Action = [[-0.23532933 -0.09774195 -0.19818358 -0.12849885]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 996 is [False, True, False, True, False, False]
Current timestep = 997. State = [[-0.00188903 -0.21298331]]. Action = [[ 0.1784063  -0.22445244 -0.09352265  0.796381  ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 997 is [False, True, False, True, False, False]
Current timestep = 998. State = [[-0.00509731 -0.24198832]]. Action = [[-0.11508694 -0.17311591  0.0525198  -0.5195612 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 998 is [False, True, False, True, False, False]
Scene graph at timestep 998 is [False, True, False, True, False, False]
State prediction error at timestep 998 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 998 of -1
Current timestep = 999. State = [[-0.0029085  -0.25567093]]. Action = [[ 0.20988935  0.05938998 -0.03259736 -0.7393772 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 999 is [False, True, False, True, False, False]
Current timestep = 1000. State = [[ 0.00378141 -0.24425675]]. Action = [[-0.09712139  0.1775909  -0.2257526  -0.15880823]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1000 is [False, True, False, True, False, False]
Scene graph at timestep 1000 is [False, True, False, True, False, False]
State prediction error at timestep 1000 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[ 0.00771358 -0.23424534]]. Action = [[ 0.16035977 -0.09275797 -0.19220729 -0.05161721]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1001 is [False, True, False, True, False, False]
Scene graph at timestep 1001 is [False, True, False, True, False, False]
State prediction error at timestep 1001 is tensor(7.4897e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of -1
Current timestep = 1002. State = [[ 0.00874987 -0.23554908]]. Action = [[ 0.00888455 -0.01078542 -0.20560141  0.09440351]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1002 is [False, True, False, True, False, False]
Current timestep = 1003. State = [[ 0.00872221 -0.24227765]]. Action = [[-0.01620214 -0.09559123  0.07762426 -0.12283927]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1003 is [False, True, False, True, False, False]
Current timestep = 1004. State = [[ 0.00748587 -0.25898594]]. Action = [[ 0.00968105 -0.15250511 -0.2350444   0.3654182 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1004 is [False, True, False, True, False, False]
Current timestep = 1005. State = [[ 0.00928508 -0.25883633]]. Action = [[-0.08375457  0.24103913  0.08256161 -0.2567153 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1005 is [False, True, False, True, False, False]
Scene graph at timestep 1005 is [False, True, False, True, False, False]
State prediction error at timestep 1005 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of -1
Current timestep = 1006. State = [[ 0.00676829 -0.25900385]]. Action = [[-0.10539104 -0.21257053 -0.23262854  0.71273065]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1006 is [False, True, False, True, False, False]
Scene graph at timestep 1006 is [False, True, False, True, False, False]
State prediction error at timestep 1006 is tensor(2.6164e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of -1
Current timestep = 1007. State = [[ 0.00434837 -0.2709387 ]]. Action = [[ 0.09312555  0.03679907 -0.16197352 -0.14900762]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1007 is [False, True, False, True, False, False]
Current timestep = 1008. State = [[ 0.0050784  -0.26807415]]. Action = [[ 0.01899773  0.03810883 -0.09150225 -0.80686784]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1008 is [False, True, False, True, False, False]
Current timestep = 1009. State = [[ 0.00764079 -0.25509325]]. Action = [[-0.0433234   0.2025612   0.1967442   0.43231583]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1009 is [False, True, False, True, False, False]
Current timestep = 1010. State = [[ 0.0104287 -0.2447294]]. Action = [[ 0.11780119 -0.10405955  0.04050231  0.555233  ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1010 is [False, True, False, True, False, False]
Current timestep = 1011. State = [[ 0.01588422 -0.24256934]]. Action = [[0.10359237 0.05314252 0.06568861 0.03966856]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1011 is [False, True, False, True, False, False]
Current timestep = 1012. State = [[ 0.03546309 -0.2368277 ]]. Action = [[ 0.19926092  0.06411543  0.06221762 -0.87539256]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1012 is [False, True, False, True, False, False]
Current timestep = 1013. State = [[ 0.04700479 -0.24008577]]. Action = [[-0.19935474 -0.10709491  0.14386392  0.9686637 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1013 is [False, True, False, True, False, False]
Scene graph at timestep 1013 is [False, True, False, True, False, False]
State prediction error at timestep 1013 is tensor(6.1150e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of -1
Current timestep = 1014. State = [[ 0.03805719 -0.25144616]]. Action = [[-0.21459259 -0.05968806 -0.19971678 -0.26454806]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1014 is [False, True, False, True, False, False]
Current timestep = 1015. State = [[ 0.02383727 -0.26395157]]. Action = [[-0.08428898 -0.09272331  0.22575933 -0.57934296]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1015 is [False, True, False, True, False, False]
Current timestep = 1016. State = [[ 0.00736224 -0.27868137]]. Action = [[-0.16282246 -0.10153005  0.23476282  0.9438729 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1016 is [False, True, False, True, False, False]
Scene graph at timestep 1016 is [False, True, False, True, False, False]
State prediction error at timestep 1016 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of -1
Current timestep = 1017. State = [[-0.00960668 -0.28863692]]. Action = [[ 0.18938649 -0.18278332 -0.23053662 -0.84246856]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1017 is [False, True, False, True, False, False]
Current timestep = 1018. State = [[-0.0045506  -0.28354996]]. Action = [[ 0.23496088  0.05865324 -0.00955467  0.77017987]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1018 is [False, True, False, True, False, False]
Current timestep = 1019. State = [[ 0.0088281 -0.2688865]]. Action = [[ 0.19793385  0.16691816 -0.16146208 -0.288638  ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1019 is [False, True, False, True, False, False]
Current timestep = 1020. State = [[ 0.03238987 -0.24378616]]. Action = [[ 0.15722892  0.19792765 -0.21966136  0.8514986 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1020 is [False, True, False, True, False, False]
Current timestep = 1021. State = [[ 0.04388894 -0.22850756]]. Action = [[ 0.24478936 -0.02500123 -0.18229663  0.09258807]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1021 is [False, True, False, True, False, False]
Current timestep = 1022. State = [[ 0.04516512 -0.22725266]]. Action = [[-0.0522393  -0.04939575 -0.18135448  0.6637205 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1022 is [False, True, False, True, False, False]
Current timestep = 1023. State = [[ 0.04776575 -0.21686023]]. Action = [[ 0.05914694  0.20177016 -0.02681504 -0.894913  ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1023 is [False, True, False, True, False, False]
Current timestep = 1024. State = [[ 0.04939023 -0.2164661 ]]. Action = [[ 0.05841342 -0.23543988  0.17687717  0.12779033]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1024 is [False, True, False, True, False, False]
Current timestep = 1025. State = [[ 0.05425029 -0.2161118 ]]. Action = [[ 0.01385415  0.19441596 -0.16722244  0.1291039 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1025 is [False, True, False, True, False, False]
Current timestep = 1026. State = [[ 0.05785202 -0.20977937]]. Action = [[ 0.10756856  0.1859101  -0.06746171 -0.46461123]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1026 is [False, False, True, True, False, False]
Current timestep = 1027. State = [[ 0.05831296 -0.20871484]]. Action = [[ 0.13299456 -0.22858478  0.22150397  0.49225652]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1027 is [False, False, True, True, False, False]
Current timestep = 1028. State = [[ 0.05831296 -0.20871484]]. Action = [[ 0.21190244 -0.13338614 -0.18406852  0.42838478]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1028 is [False, False, True, True, False, False]
Current timestep = 1029. State = [[ 0.05681123 -0.21685132]]. Action = [[-0.13422887 -0.14115712 -0.1786041   0.7697383 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1029 is [False, False, True, True, False, False]
Current timestep = 1030. State = [[ 0.0559583  -0.22210546]]. Action = [[ 0.12921682 -0.07096308  0.21297514  0.08949852]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1030 is [False, False, True, True, False, False]
Current timestep = 1031. State = [[ 0.05519802 -0.22586721]]. Action = [[ 0.01006159 -0.04398185  0.06134635  0.8276261 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1031 is [False, False, True, True, False, False]
Scene graph at timestep 1031 is [False, False, True, True, False, False]
State prediction error at timestep 1031 is tensor(1.5922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[ 0.050302   -0.22127865]]. Action = [[-0.21104842  0.18799269 -0.19647597 -0.20152998]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1032 is [False, False, True, True, False, False]
Current timestep = 1033. State = [[ 0.04115295 -0.21302424]]. Action = [[ 0.10731974  0.18918335 -0.16054128 -0.8410895 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1033 is [False, False, True, True, False, False]
Current timestep = 1034. State = [[ 0.04054863 -0.21005175]]. Action = [[0.0633969  0.02766597 0.16296595 0.8992679 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1034 is [False, True, False, True, False, False]
Current timestep = 1035. State = [[ 0.04179137 -0.20046271]]. Action = [[ 0.07523745  0.09578961 -0.14769176  0.541345  ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1035 is [False, True, False, True, False, False]
Current timestep = 1036. State = [[ 0.04258633 -0.19325745]]. Action = [[ 0.14618084  0.13303035  0.1094051  -0.27836645]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1036 is [False, True, False, True, False, False]
Current timestep = 1037. State = [[ 0.04623764 -0.192475  ]]. Action = [[ 0.13436437 -0.06025432  0.20775774 -0.35334885]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1037 is [False, True, False, True, False, False]
Current timestep = 1038. State = [[ 0.05236132 -0.19039547]]. Action = [[ 0.09357888  0.0513913  -0.04497898  0.12613487]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1038 is [False, True, False, True, False, False]
Current timestep = 1039. State = [[ 0.06076563 -0.18750395]]. Action = [[ 0.2358911  -0.1303654  -0.08468983  0.7879236 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1039 is [False, False, True, True, False, False]
Current timestep = 1040. State = [[ 0.06123781 -0.18749006]]. Action = [[ 0.09017569 -0.17460799  0.03999478  0.49916077]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1040 is [False, False, True, True, False, False]
Current timestep = 1041. State = [[ 0.06121108 -0.19576378]]. Action = [[-0.01039308 -0.14670733 -0.08280948  0.905257  ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1041 is [False, False, True, True, False, False]
Current timestep = 1042. State = [[ 0.06033354 -0.20986046]]. Action = [[-0.05962524 -0.09619424 -0.16416357  0.39661896]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1042 is [False, False, True, True, False, False]
Current timestep = 1043. State = [[ 0.05964147 -0.21812603]]. Action = [[ 0.16999704  0.07046625 -0.17391609 -0.43937862]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1043 is [False, False, True, True, False, False]
Current timestep = 1044. State = [[ 0.05969045 -0.21313283]]. Action = [[-0.06796023  0.14100778  0.01233152  0.86245394]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1044 is [False, False, True, True, False, False]
Current timestep = 1045. State = [[ 0.06001778 -0.20929511]]. Action = [[ 0.14072841 -0.22880298 -0.06726515 -0.96174836]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1045 is [False, False, True, True, False, False]
Current timestep = 1046. State = [[ 0.06004936 -0.20856458]]. Action = [[ 0.19445747 -0.20577346  0.0709652   0.03182709]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1046 is [False, False, True, True, False, False]
Scene graph at timestep 1046 is [False, False, True, True, False, False]
State prediction error at timestep 1046 is tensor(1.1307e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of -1
Current timestep = 1047. State = [[ 0.06021956 -0.20597121]]. Action = [[ 0.01036674  0.03801188  0.1821276  -0.2195738 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1047 is [False, False, True, True, False, False]
Current timestep = 1048. State = [[ 0.0596564  -0.19133314]]. Action = [[-0.10167998  0.22606239  0.20049185 -0.36659276]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1048 is [False, False, True, True, False, False]
Current timestep = 1049. State = [[ 0.05673999 -0.17232771]]. Action = [[0.14217389 0.0080502  0.13370651 0.96814966]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1049 is [False, False, True, True, False, False]
Current timestep = 1050. State = [[ 0.05522209 -0.16318755]]. Action = [[-0.06236973  0.1003755  -0.0549548  -0.79896426]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1050 is [False, False, True, True, False, False]
Scene graph at timestep 1050 is [False, False, True, True, False, False]
State prediction error at timestep 1050 is tensor(4.6497e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1050 of 1
Current timestep = 1051. State = [[-0.1709888   0.15676396]]. Action = [[-0.05312854 -0.1624091   0.02147591 -0.08172596]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1051 is [False, False, True, True, False, False]
Current timestep = 1052. State = [[-0.15403105  0.16982521]]. Action = [[-0.10273701 -0.13500139 -0.08180787 -0.34426713]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1052 is [True, False, False, False, False, True]
Current timestep = 1053. State = [[-0.14816165  0.1563144 ]]. Action = [[ 0.20353436 -0.09341165  0.02620783  0.09661674]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1053 is [True, False, False, False, False, True]
Current timestep = 1054. State = [[-0.14291914  0.1500774 ]]. Action = [[-0.04332262  0.02728555  0.06304795  0.87848854]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1054 is [True, False, False, False, False, True]
Current timestep = 1055. State = [[-0.13433798  0.14474006]]. Action = [[ 0.21035355 -0.07272734  0.21531612  0.74193525]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1055 is [True, False, False, False, False, True]
Scene graph at timestep 1055 is [True, False, False, False, False, True]
State prediction error at timestep 1055 is tensor(2.8215e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[-0.11906175  0.12775733]]. Action = [[-0.19493498 -0.2107562   0.12717885 -0.3537042 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1056 is [True, False, False, False, False, True]
Current timestep = 1057. State = [[-0.11989     0.11771686]]. Action = [[ 0.10423711  0.02916384 -0.05067389  0.26610208]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1057 is [True, False, False, False, False, True]
Current timestep = 1058. State = [[-0.11660964  0.10651477]]. Action = [[ 0.05620351 -0.15500227 -0.1633605   0.21560323]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Current timestep = 1059. State = [[-0.11557659  0.08575898]]. Action = [[-0.14152229 -0.18473192  0.15454847 -0.2990715 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Current timestep = 1060. State = [[-0.11141884  0.07987373]]. Action = [[ 0.22950041  0.15068698 -0.17325084  0.12508297]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Current timestep = 1061. State = [[-0.0970096   0.08352434]]. Action = [[ 0.23097765 -0.00622733 -0.19303691 -0.9025323 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Current timestep = 1062. State = [[-0.0702423   0.07326911]]. Action = [[ 0.14846784 -0.22135963  0.12809354 -0.9386769 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1062 is [True, False, False, False, True, False]
Current timestep = 1063. State = [[-0.04784837  0.04476464]]. Action = [[ 0.1654135  -0.23273692  0.00915551  0.53049755]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1063 is [True, False, False, False, True, False]
Current timestep = 1064. State = [[-0.23375972  0.1818256 ]]. Action = [[ 0.1778127  -0.02768914  0.03902343  0.6935437 ]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 1064 is [False, True, False, False, True, False]
Current timestep = 1065. State = [[-0.223714    0.19604948]]. Action = [[ 0.04201865 -0.14142801 -0.20962831  0.05645967]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1065 is [True, False, False, False, False, True]
Current timestep = 1066. State = [[-0.2189482   0.18146321]]. Action = [[ 0.01486838 -0.1262333  -0.09568012  0.40946698]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1066 is [True, False, False, False, False, True]
Current timestep = 1067. State = [[-0.21591648  0.17129628]]. Action = [[ 0.04801756 -0.00428376  0.14591366 -0.32121944]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1067 is [True, False, False, False, False, True]
Current timestep = 1068. State = [[-0.20957421  0.18009378]]. Action = [[ 0.10518587  0.21702701  0.17875275 -0.07569551]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1068 is [True, False, False, False, False, True]
Current timestep = 1069. State = [[-0.18831287  0.18153843]]. Action = [[ 0.13045746 -0.20649701 -0.06748888 -0.01893306]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1069 is [True, False, False, False, False, True]
Scene graph at timestep 1069 is [True, False, False, False, False, True]
State prediction error at timestep 1069 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1069 of 1
Current timestep = 1070. State = [[-0.16959332  0.17059146]]. Action = [[ 0.14551306 -0.00795211  0.23053357  0.4758873 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1070 is [True, False, False, False, False, True]
Scene graph at timestep 1070 is [True, False, False, False, False, True]
State prediction error at timestep 1070 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1070 of 1
Current timestep = 1071. State = [[-0.155324   0.1591629]]. Action = [[-0.1169111  -0.20153366 -0.11568467  0.3413689 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1071 is [True, False, False, False, False, True]
Current timestep = 1072. State = [[-0.16053234  0.14619033]]. Action = [[-0.1519569  -0.03940016 -0.16066493  0.14338994]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1072 is [True, False, False, False, False, True]
Current timestep = 1073. State = [[-0.16548169  0.13392848]]. Action = [[ 0.03203318 -0.11977473  0.17284912  0.710464  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1073 is [True, False, False, False, False, True]
Scene graph at timestep 1073 is [True, False, False, False, False, True]
State prediction error at timestep 1073 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 1
Current timestep = 1074. State = [[-0.16443892  0.12046985]]. Action = [[ 0.06984201 -0.03299336  0.1227656   0.9708772 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1074 is [True, False, False, False, False, True]
Scene graph at timestep 1074 is [True, False, False, False, True, False]
State prediction error at timestep 1074 is tensor(3.6806e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1074 of 1
Current timestep = 1075. State = [[-0.16406913  0.11791625]]. Action = [[-0.0679356  -0.00496158  0.04651546 -0.8346104 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1075 is [True, False, False, False, True, False]
Current timestep = 1076. State = [[-0.17553909  0.10506207]]. Action = [[-0.23535924 -0.21876799  0.09604529 -0.07701802]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1076 is [True, False, False, False, True, False]
Current timestep = 1077. State = [[-0.20109864  0.07715321]]. Action = [[-0.14690182 -0.21262455  0.17922577 -0.29844546]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1077 is [True, False, False, False, True, False]
Scene graph at timestep 1077 is [True, False, False, False, True, False]
State prediction error at timestep 1077 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1077 of -1
Current timestep = 1078. State = [[-0.22053525  0.04789556]]. Action = [[-0.1144706  -0.12025836  0.05900222 -0.13747704]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1078 is [True, False, False, False, True, False]
Current timestep = 1079. State = [[-0.23198839  0.02808351]]. Action = [[ 0.03146967 -0.18384013  0.12086946 -0.64782596]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1079 is [True, False, False, False, True, False]
Current timestep = 1080. State = [[-0.23142034  0.02293322]]. Action = [[0.0698905  0.19821888 0.23406526 0.38658535]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1080 is [True, False, False, False, True, False]
Current timestep = 1081. State = [[-0.23580323  0.03371406]]. Action = [[-0.1978203   0.03528869  0.15342706 -0.13126898]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1081 is [True, False, False, False, True, False]
Current timestep = 1082. State = [[-0.24817768  0.0276622 ]]. Action = [[-0.05820873 -0.19427958  0.08559725  0.65396607]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1082 is [True, False, False, False, True, False]
Current timestep = 1083. State = [[-0.24997474  0.00499322]]. Action = [[ 0.16430095 -0.18374826 -0.10959244 -0.357938  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1083 is [True, False, False, False, True, False]
Current timestep = 1084. State = [[-0.24158193 -0.01340643]]. Action = [[ 0.01928514 -0.04710349  0.20567375 -0.7376524 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1084 is [True, False, False, False, True, False]
Current timestep = 1085. State = [[-0.2407558  -0.02588966]]. Action = [[-0.06105168 -0.09759149 -0.24169849 -0.6297305 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1085 is [True, False, False, False, True, False]
Current timestep = 1086. State = [[-0.24505733 -0.03361183]]. Action = [[-0.0853202   0.02094704  0.16073114 -0.87933993]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1086 is [True, False, False, False, True, False]
Current timestep = 1087. State = [[-0.24923725 -0.02884652]]. Action = [[0.08286047 0.11537367 0.11903933 0.520211  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Scene graph at timestep 1087 is [True, False, False, False, True, False]
State prediction error at timestep 1087 is tensor(4.1235e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of -1
Current timestep = 1088. State = [[-0.23928703 -0.01062871]]. Action = [[0.23487696 0.21627182 0.20407432 0.3144374 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Current timestep = 1089. State = [[-0.2366251   0.00893212]]. Action = [[-0.21621186  0.0525685   0.13947034 -0.20364541]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Current timestep = 1090. State = [[-0.23617679  0.01761324]]. Action = [[ 0.19070345  0.02531487  0.1860213  -0.77888536]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Current timestep = 1091. State = [[-0.22970459  0.02071818]]. Action = [[ 0.09573773  0.00693199 -0.16899098 -0.4480527 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Current timestep = 1092. State = [[-0.2112738   0.02461224]]. Action = [[ 0.17706001  0.03572366  0.18549043 -0.27137184]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1092 is [True, False, False, False, True, False]
Current timestep = 1093. State = [[-0.18860646  0.01746581]]. Action = [[ 0.16389635 -0.17252475 -0.23394538 -0.4843427 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1093 is [True, False, False, False, True, False]
Scene graph at timestep 1093 is [True, False, False, False, True, False]
State prediction error at timestep 1093 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 1
Current timestep = 1094. State = [[-0.1802648   0.01271044]]. Action = [[-0.23860821  0.09882158  0.0135833  -0.3298887 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1094 is [True, False, False, False, True, False]
Current timestep = 1095. State = [[-0.18594885  0.0154328 ]]. Action = [[-0.0687366  -0.03632846 -0.12282698 -0.71279275]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Current timestep = 1096. State = [[-0.18983516  0.01610643]]. Action = [[ 0.08364332  0.05017844 -0.15521473  0.80779576]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Scene graph at timestep 1096 is [True, False, False, False, True, False]
State prediction error at timestep 1096 is tensor(4.4960e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1096 of -1
Current timestep = 1097. State = [[-0.18760768  0.02202188]]. Action = [[ 0.09540334  0.07085946 -0.07386279  0.5437572 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Scene graph at timestep 1097 is [True, False, False, False, True, False]
State prediction error at timestep 1097 is tensor(1.3349e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of 1
Current timestep = 1098. State = [[-0.17786849  0.03817863]]. Action = [[ 0.16174877  0.1939809  -0.14925161 -0.37425315]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Current timestep = 1099. State = [[-0.16761945  0.04442639]]. Action = [[ 0.03281355 -0.14609219  0.17086577  0.5875033 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Scene graph at timestep 1099 is [True, False, False, False, True, False]
State prediction error at timestep 1099 is tensor(9.9986e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1099 of 1
Current timestep = 1100. State = [[-0.15864575  0.02584998]]. Action = [[ 0.11327592 -0.22645594  0.11778933  0.8130095 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Current timestep = 1101. State = [[-0.13869992  0.01929897]]. Action = [[ 0.19997317  0.14928174  0.2066192  -0.6999385 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Scene graph at timestep 1101 is [True, False, False, False, True, False]
State prediction error at timestep 1101 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of 1
Current timestep = 1102. State = [[-0.11759236  0.02875169]]. Action = [[-0.1488668   0.03531513  0.09015381  0.15102327]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Current timestep = 1103. State = [[-0.12274311  0.03419137]]. Action = [[-0.10847673  0.04502073 -0.23235093  0.38743222]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Current timestep = 1104. State = [[-0.13205758  0.04930051]]. Action = [[-0.05728105  0.18318456  0.1861507   0.30003035]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Current timestep = 1105. State = [[-0.14671594  0.0741003 ]]. Action = [[-0.13218921  0.16828972  0.22542489  0.4199779 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Current timestep = 1106. State = [[-0.15646572  0.09705101]]. Action = [[ 0.11257255  0.1580118  -0.209327   -0.31871283]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Current timestep = 1107. State = [[-0.16384894  0.10863546]]. Action = [[-0.21325928 -0.02456397  0.13496754  0.16106212]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Current timestep = 1108. State = [[-0.18148327  0.12213254]]. Action = [[-0.19684723  0.15407127 -0.207901    0.29471433]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Current timestep = 1109. State = [[-0.19087267  0.11996302]]. Action = [[ 0.14321303 -0.21881379 -0.22864002  0.6777712 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Scene graph at timestep 1109 is [True, False, False, False, True, False]
State prediction error at timestep 1109 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of -1
Current timestep = 1110. State = [[-0.19204013  0.11593667]]. Action = [[ 0.00635806  0.20663065 -0.07154551  0.46071982]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Current timestep = 1111. State = [[-0.19927122  0.13096473]]. Action = [[-0.12931584  0.07241029 -0.07780698  0.6376097 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Current timestep = 1112. State = [[-0.20932326  0.14611213]]. Action = [[-0.09883225  0.08888718  0.20281917 -0.14925146]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1112 is [True, False, False, False, False, True]
Scene graph at timestep 1112 is [True, False, False, False, False, True]
State prediction error at timestep 1112 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1112 of -1
Current timestep = 1113. State = [[-0.22123663  0.16113655]]. Action = [[-0.06996709  0.07758859 -0.10771891 -0.58074003]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1113 is [True, False, False, False, False, True]
Current timestep = 1114. State = [[-0.22062376  0.15878963]]. Action = [[ 0.12172991 -0.1523086   0.21463671 -0.32744753]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1114 is [True, False, False, False, False, True]
Current timestep = 1115. State = [[-0.22187687  0.1617717 ]]. Action = [[-0.06049268  0.20531118  0.23379046 -0.29600644]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1115 is [True, False, False, False, False, True]
Current timestep = 1116. State = [[-0.22207353  0.16896722]]. Action = [[ 0.13815725 -0.00692691  0.23722494  0.82683444]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1116 is [True, False, False, False, False, True]
Scene graph at timestep 1116 is [True, False, False, False, False, True]
State prediction error at timestep 1116 is tensor(6.4373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of -1
Current timestep = 1117. State = [[-0.20808591  0.16503493]]. Action = [[ 0.16450691 -0.11056519 -0.03906332  0.7442093 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1117 is [True, False, False, False, False, True]
Scene graph at timestep 1117 is [True, False, False, False, False, True]
State prediction error at timestep 1117 is tensor(2.2128e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of 1
Current timestep = 1118. State = [[-0.19230177  0.16364808]]. Action = [[0.07033435 0.13638163 0.01696646 0.0434444 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1118 is [True, False, False, False, False, True]
Current timestep = 1119. State = [[-0.18769255  0.17619751]]. Action = [[-0.09745255  0.07472974  0.02078083 -0.29635632]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1119 is [True, False, False, False, False, True]
Current timestep = 1120. State = [[-0.18753101  0.188629  ]]. Action = [[ 0.10297024  0.11041483 -0.05377154  0.38127565]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1120 is [True, False, False, False, False, True]
Current timestep = 1121. State = [[-0.18980879  0.20186563]]. Action = [[-0.21145982  0.03868327 -0.21858929 -0.02802205]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1121 is [True, False, False, False, False, True]
Current timestep = 1122. State = [[-0.1923838   0.20512001]]. Action = [[ 0.13754535 -0.06932998 -0.10033557  0.33337927]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1122 is [True, False, False, False, False, True]
Current timestep = 1123. State = [[-0.19388959  0.21189922]]. Action = [[-0.03651555  0.1946314   0.15930337  0.2358228 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1123 is [True, False, False, False, False, True]
Current timestep = 1124. State = [[-0.20002824  0.20952998]]. Action = [[-0.17921591 -0.23911855 -0.00140044 -0.6404161 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1124 is [True, False, False, False, False, True]
Current timestep = 1125. State = [[-0.2094041   0.20704919]]. Action = [[-0.0750486   0.09301364 -0.08183384 -0.00419033]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1125 is [True, False, False, False, False, True]
Current timestep = 1126. State = [[-0.22176015  0.21993667]]. Action = [[-0.14640535  0.11831158  0.16213042 -0.69229466]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1126 is [True, False, False, False, False, True]
Scene graph at timestep 1126 is [True, False, False, False, False, True]
State prediction error at timestep 1126 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1126 of -1
Current timestep = 1127. State = [[-0.23554446  0.23949751]]. Action = [[ 0.09155229  0.21857482  0.21933919 -0.56023794]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1127 is [True, False, False, False, False, True]
Current timestep = 1128. State = [[-0.23158455  0.24040379]]. Action = [[ 0.1342302  -0.19230753  0.2143333   0.48079264]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1128 is [True, False, False, False, False, True]
Scene graph at timestep 1128 is [True, False, False, False, False, True]
State prediction error at timestep 1128 is tensor(5.4360e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1128 of -1
Current timestep = 1129. State = [[-0.21561275  0.23916009]]. Action = [[ 0.181741    0.19028991 -0.06458208  0.885051  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1129 is [True, False, False, False, False, True]
Current timestep = 1130. State = [[-0.20022401  0.2612949 ]]. Action = [[ 0.0631088   0.18910563 -0.11457354 -0.49011856]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1130 is [True, False, False, False, False, True]
Scene graph at timestep 1130 is [True, False, False, False, False, True]
State prediction error at timestep 1130 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of -1
Current timestep = 1131. State = [[-0.1831976   0.27098116]]. Action = [[-0.01466021 -0.216795   -0.039395    0.35685408]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1131 is [True, False, False, False, False, True]
Current timestep = 1132. State = [[-0.18346746  0.25633484]]. Action = [[-0.13379246 -0.11139403 -0.09011166 -0.9657077 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1132 is [True, False, False, False, False, True]
Current timestep = 1133. State = [[-0.19965765  0.24012482]]. Action = [[-0.23872748 -0.12643321  0.22968063  0.605669  ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1133 is [True, False, False, False, False, True]
Current timestep = 1134. State = [[-0.21343152  0.21322146]]. Action = [[ 0.12386495 -0.23267327 -0.22796595 -0.5035554 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1134 is [True, False, False, False, False, True]
Scene graph at timestep 1134 is [True, False, False, False, False, True]
State prediction error at timestep 1134 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1134 of 1
Current timestep = 1135. State = [[-0.21996038  0.1851693 ]]. Action = [[-0.17898013 -0.13345061  0.19951129  0.8359654 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1135 is [True, False, False, False, False, True]
Scene graph at timestep 1135 is [True, False, False, False, False, True]
State prediction error at timestep 1135 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of -1
Current timestep = 1136. State = [[-0.23564336  0.17952578]]. Action = [[-0.03681739  0.14311594 -0.01948616 -0.34988546]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1136 is [True, False, False, False, False, True]
Current timestep = 1137. State = [[-0.24901834  0.1971343 ]]. Action = [[-0.2225145   0.15907225  0.01052547  0.58665204]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1137 is [True, False, False, False, False, True]
Current timestep = 1138. State = [[-0.27036077  0.22435825]]. Action = [[-0.04242936  0.21641243 -0.18253581  0.35497296]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1138 is [True, False, False, False, False, True]
Current timestep = 1139. State = [[-0.2788028   0.24856344]]. Action = [[ 0.08499503  0.18371105 -0.03726307  0.38808048]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1139 is [True, False, False, False, False, True]
Current timestep = 1140. State = [[-0.2693956   0.25662467]]. Action = [[ 0.22324994 -0.05469577 -0.23998207 -0.38752615]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1140 is [True, False, False, False, False, True]
Current timestep = 1141. State = [[-0.24766229  0.25263572]]. Action = [[ 0.17407086 -0.10153505 -0.16257454  0.22059655]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1141 is [True, False, False, False, False, True]
Current timestep = 1142. State = [[-0.2325732  0.2365338]]. Action = [[-0.0544811  -0.17165461 -0.16559823 -0.20486009]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1142 is [True, False, False, False, False, True]
Current timestep = 1143. State = [[-0.22253546  0.22817342]]. Action = [[ 0.1862477   0.0710268   0.17428905 -0.9148702 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1143 is [True, False, False, False, False, True]
Scene graph at timestep 1143 is [True, False, False, False, False, True]
State prediction error at timestep 1143 is tensor(1.4808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1143 of -1
Current timestep = 1144. State = [[-0.21274877  0.23925278]]. Action = [[-0.05876783  0.15345284  0.23202068  0.81177187]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1144 is [True, False, False, False, False, True]
Current timestep = 1145. State = [[-0.20869309  0.23980318]]. Action = [[ 0.12022391 -0.10838863 -0.13609403 -0.5933408 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1145 is [True, False, False, False, False, True]
Current timestep = 1146. State = [[-0.19566706  0.22415483]]. Action = [[ 0.1556617  -0.19979347 -0.15169829  0.28649497]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1146 is [True, False, False, False, False, True]
Scene graph at timestep 1146 is [True, False, False, False, False, True]
State prediction error at timestep 1146 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1146 of 1
Current timestep = 1147. State = [[-0.17460713  0.20390375]]. Action = [[ 0.1446619  -0.03708333  0.18434545 -0.59507227]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1147 is [True, False, False, False, False, True]
Current timestep = 1148. State = [[-0.16559328  0.21286356]]. Action = [[-0.13646854  0.19450808 -0.18032524  0.8621197 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1148 is [True, False, False, False, False, True]
Current timestep = 1149. State = [[-0.16314442  0.22547482]]. Action = [[ 0.20947012  0.03800875 -0.17180325 -0.65418816]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1149 is [True, False, False, False, False, True]
Current timestep = 1150. State = [[-0.14385924  0.2194339 ]]. Action = [[ 0.16889706 -0.22551118 -0.22616686  0.8820684 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1150 is [True, False, False, False, False, True]
Current timestep = 1151. State = [[-0.12213112  0.19312367]]. Action = [[ 0.03945917 -0.21696988  0.19180378  0.17436779]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1151 is [True, False, False, False, False, True]
Current timestep = 1152. State = [[-0.11136517  0.17284673]]. Action = [[-0.02646661 -0.09104148  0.13865799  0.12385297]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1152 is [True, False, False, False, False, True]
Current timestep = 1153. State = [[-0.10705839  0.15723544]]. Action = [[-0.0078948  -0.13271563  0.20966393  0.40204036]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1153 is [True, False, False, False, False, True]
Current timestep = 1154. State = [[-0.09905898  0.13457088]]. Action = [[ 0.24111217 -0.17929125 -0.06481218 -0.5486234 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1154 is [True, False, False, False, False, True]
Scene graph at timestep 1154 is [True, False, False, False, False, True]
State prediction error at timestep 1154 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1154 of 1
Current timestep = 1155. State = [[-0.09139048  0.11050454]]. Action = [[-0.21739234 -0.07107897  0.18736303  0.7384733 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1155 is [True, False, False, False, False, True]
Scene graph at timestep 1155 is [True, False, False, False, True, False]
State prediction error at timestep 1155 is tensor(9.9233e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of -1
Current timestep = 1156. State = [[-0.09627994  0.09177954]]. Action = [[-0.01479852 -0.23862636 -0.24773048  0.756289  ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1156 is [True, False, False, False, True, False]
Current timestep = 1157. State = [[-0.09914623  0.08525594]]. Action = [[ 0.08318093  0.19069898 -0.2418544   0.7195759 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1157 is [True, False, False, False, True, False]
Current timestep = 1158. State = [[-0.09897598  0.0874268 ]]. Action = [[-0.05609694 -0.0700404   0.21655405 -0.46274626]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1158 is [True, False, False, False, True, False]
Scene graph at timestep 1158 is [True, False, False, False, True, False]
State prediction error at timestep 1158 is tensor(9.7183e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of -1
Current timestep = 1159. State = [[-0.09318206  0.09219415]]. Action = [[0.21041986 0.11646974 0.23741639 0.43433547]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1159 is [True, False, False, False, True, False]
Current timestep = 1160. State = [[-0.08823185  0.09895356]]. Action = [[-0.10857004  0.01505926  0.0636183   0.6759969 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1160 is [True, False, False, False, True, False]
Scene graph at timestep 1160 is [True, False, False, False, True, False]
State prediction error at timestep 1160 is tensor(3.5804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.08563127  0.0916833 ]]. Action = [[ 0.09229165 -0.15620391  0.2447781   0.36760223]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1161 is [True, False, False, False, True, False]
Current timestep = 1162. State = [[-0.08248199  0.07965559]]. Action = [[-0.0653694  -0.07998659 -0.14574589  0.86062634]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Current timestep = 1163. State = [[-0.08667935  0.08242119]]. Action = [[-0.1627943   0.13940698 -0.22446762 -0.8452301 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1163 is [True, False, False, False, True, False]
Scene graph at timestep 1163 is [True, False, False, False, True, False]
State prediction error at timestep 1163 is tensor(4.1450e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1163 of -1
Current timestep = 1164. State = [[-0.09649921  0.088346  ]]. Action = [[-0.08548248 -0.04402636 -0.18662728  0.88664746]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1164 is [True, False, False, False, True, False]
Current timestep = 1165. State = [[-0.10760305  0.09733595]]. Action = [[-0.03750864  0.21044153 -0.19821802  0.582845  ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Current timestep = 1166. State = [[-0.12416697  0.12222683]]. Action = [[-0.23640083  0.1551868   0.21699035 -0.16234547]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of -1
Current timestep = 1167. State = [[-0.14485152  0.1317545 ]]. Action = [[ 0.1265746  -0.10196476 -0.21115759 -0.87162966]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Scene graph at timestep 1167 is [True, False, False, False, False, True]
State prediction error at timestep 1167 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of 1
Current timestep = 1168. State = [[-0.13858443  0.12862931]]. Action = [[0.14558625 0.07038215 0.23288411 0.707438  ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1168 is [True, False, False, False, False, True]
Scene graph at timestep 1168 is [True, False, False, False, False, True]
State prediction error at timestep 1168 is tensor(7.0942e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[-0.12744877  0.12968107]]. Action = [[ 0.0854699  -0.02664524 -0.00118968 -0.9424352 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1169 is [True, False, False, False, False, True]
Current timestep = 1170. State = [[-0.11570832  0.11641569]]. Action = [[ 0.15223837 -0.19684455 -0.21425593  0.12785661]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1170 is [True, False, False, False, False, True]
Current timestep = 1171. State = [[-0.09313788  0.10353121]]. Action = [[ 0.17821768  0.05848703  0.13377053 -0.2413401 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Current timestep = 1172. State = [[-0.0778188   0.09438077]]. Action = [[-0.11770014 -0.21074261  0.1310176  -0.50439006]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Current timestep = 1173. State = [[-0.0675712   0.08526511]]. Action = [[ 0.24342948  0.06206191 -0.0984984  -0.2583282 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(2.9745e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.04555705  0.0730099 ]]. Action = [[ 0.14851356 -0.2431432   0.08932531  0.57568276]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Current timestep = 1175. State = [[-0.25339994 -0.01652173]]. Action = [[ 0.20971316 -0.18117283 -0.1988246   0.2064215 ]]. Reward = [100.]
Curr episode timestep = 110
Scene graph at timestep 1175 is [False, True, False, False, True, False]
Current timestep = 1176. State = [[-0.2433218 -0.011314 ]]. Action = [[ 0.16813892  0.17014158 -0.02321938 -0.8885657 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Scene graph at timestep 1176 is [True, False, False, False, True, False]
State prediction error at timestep 1176 is tensor(9.6195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of 1
Current timestep = 1177. State = [[-0.22933817 -0.0135153 ]]. Action = [[-0.00421265 -0.22051215  0.18400842 -0.07043678]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Current timestep = 1178. State = [[-0.23358354 -0.02643127]]. Action = [[-0.1761467  -0.03482738 -0.11350416 -0.681581  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Current timestep = 1179. State = [[-0.2323292  -0.02338653]]. Action = [[ 0.21950775  0.16390947 -0.24301903  0.8032334 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Current timestep = 1180. State = [[-0.22714716 -0.02025138]]. Action = [[-0.01189597 -0.07860222 -0.07198903  0.80662847]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Current timestep = 1181. State = [[-0.22170316 -0.01449959]]. Action = [[ 0.10017478  0.14129868 -0.08735707  0.5862491 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(1.7511e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of 0
Current timestep = 1182. State = [[-0.21596444 -0.00937249]]. Action = [[-0.17470486 -0.09591679  0.04990712  0.38805914]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Scene graph at timestep 1182 is [True, False, False, False, True, False]
State prediction error at timestep 1182 is tensor(5.1061e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of -1
Current timestep = 1183. State = [[-0.22817312 -0.02062369]]. Action = [[-0.18854171 -0.06763002  0.22270563  0.88415587]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Current timestep = 1184. State = [[-0.23613168 -0.02645933]]. Action = [[ 0.20522928 -0.03730212 -0.05909646 -0.9298465 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.23582207 -0.03449212]]. Action = [[-0.14157118 -0.09179515  0.08447918 -0.5309897 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Scene graph at timestep 1185 is [True, False, False, False, True, False]
State prediction error at timestep 1185 is tensor(5.4945e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of -1
Current timestep = 1186. State = [[-0.24618563 -0.03480921]]. Action = [[-0.12598613  0.17072803  0.00084528  0.32271147]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.25127277 -0.02409412]]. Action = [[ 0.09122661  0.01689199  0.12506425 -0.91549116]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Scene graph at timestep 1187 is [True, False, False, False, True, False]
State prediction error at timestep 1187 is tensor(7.3966e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1187 of -1
Current timestep = 1188. State = [[-0.24277768 -0.00745894]]. Action = [[ 0.19392031  0.20893961  0.08130303 -0.5046654 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Current timestep = 1189. State = [[-0.23000818  0.0028085 ]]. Action = [[ 0.04169023 -0.02126183  0.16131383 -0.2507468 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Current timestep = 1190. State = [[-0.22602372  0.0033761 ]]. Action = [[-0.03499795 -0.0120818   0.08562988  0.36579037]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Scene graph at timestep 1190 is [True, False, False, False, True, False]
State prediction error at timestep 1190 is tensor(2.3006e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1190 of 1
Current timestep = 1191. State = [[-0.22251529  0.00171495]]. Action = [[ 0.10245883 -0.06184432  0.19965863  0.8548118 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Current timestep = 1192. State = [[-0.21872614  0.01036367]]. Action = [[ 0.0239889   0.17940468  0.11855131 -0.2063188 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Current timestep = 1193. State = [[-0.21684553  0.01317341]]. Action = [[-0.12072295 -0.11830333  0.22594497 -0.22109008]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1193 is [True, False, False, False, True, False]
Current timestep = 1194. State = [[-0.21890074  0.01639774]]. Action = [[ 0.01662734  0.12382722  0.06368762 -0.82729393]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Current timestep = 1195. State = [[-0.22991772  0.01096462]]. Action = [[-0.24506481 -0.1930199  -0.21829389  0.5861976 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Current timestep = 1196. State = [[-0.23262827  0.00887469]]. Action = [[ 0.24225134  0.15167946  0.06032145 -0.66381454]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Current timestep = 1197. State = [[-0.22990976  0.02799949]]. Action = [[-0.0247408   0.22564954 -0.1812866   0.7852485 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Scene graph at timestep 1197 is [True, False, False, False, True, False]
State prediction error at timestep 1197 is tensor(9.9183e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1197 of -1
Current timestep = 1198. State = [[-0.22419651  0.04903909]]. Action = [[ 0.14447421  0.02760655  0.18704602 -0.7624401 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Current timestep = 1199. State = [[-0.21472178  0.04797504]]. Action = [[-0.12562153 -0.1023083  -0.11502537 -0.14965218]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Current timestep = 1200. State = [[-0.21599369  0.0467919 ]]. Action = [[ 0.03230157  0.07015017 -0.10240339  0.8208401 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1200 is [True, False, False, False, True, False]
Current timestep = 1201. State = [[-0.20839544  0.05905307]]. Action = [[ 0.2060371   0.17262459 -0.08743647 -0.67075485]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1201 is [True, False, False, False, True, False]
Current timestep = 1202. State = [[-0.20517358  0.08397215]]. Action = [[-0.20032945  0.19866642 -0.1236589   0.29915464]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1202 is [True, False, False, False, True, False]
Current timestep = 1203. State = [[-0.2093354   0.08808099]]. Action = [[ 0.08974147 -0.21584378  0.00944984  0.04495609]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1203 is [True, False, False, False, True, False]
Current timestep = 1204. State = [[-0.19554685  0.07237937]]. Action = [[ 0.2355128  -0.0946161   0.19161916  0.88994265]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1204 is [True, False, False, False, True, False]
Current timestep = 1205. State = [[-0.17464651  0.07360094]]. Action = [[0.10636079 0.16058043 0.11237076 0.10506034]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1205 is [True, False, False, False, True, False]
Scene graph at timestep 1205 is [True, False, False, False, True, False]
State prediction error at timestep 1205 is tensor(8.1727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of 1
Current timestep = 1206. State = [[-0.15469131  0.08115281]]. Action = [[ 0.08663359 -0.02420923 -0.1822951  -0.4488951 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1206 is [True, False, False, False, True, False]
Current timestep = 1207. State = [[-0.14332467  0.08246148]]. Action = [[0.12952656 0.03131074 0.06770352 0.62394047]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1207 is [True, False, False, False, True, False]
Current timestep = 1208. State = [[-0.12507094  0.08137591]]. Action = [[ 0.09479377 -0.06181082  0.21969187 -0.7049139 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1208 is [True, False, False, False, True, False]
Current timestep = 1209. State = [[-0.10836591  0.07735111]]. Action = [[ 0.15975201 -0.02367042 -0.06979397 -0.8998539 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Current timestep = 1210. State = [[-0.09558117  0.0757281 ]]. Action = [[-0.05388337 -0.0203644  -0.07070519  0.14667499]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Current timestep = 1211. State = [[-0.09234899  0.07239627]]. Action = [[ 0.09315792 -0.01936844  0.12314835  0.29318798]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Scene graph at timestep 1211 is [True, False, False, False, True, False]
State prediction error at timestep 1211 is tensor(1.7502e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of 1
Current timestep = 1212. State = [[-0.0776688  0.0812333]]. Action = [[ 0.20074692  0.21248996 -0.20319983 -0.4046539 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Current timestep = 1213. State = [[-0.06059678  0.09231949]]. Action = [[-0.06013823 -0.01621807 -0.08746365  0.61028385]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Current timestep = 1214. State = [[-0.05786208  0.1001908 ]]. Action = [[ 0.10756525  0.11824375 -0.1519765   0.9574909 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Current timestep = 1215. State = [[-0.05140338  0.10400559]]. Action = [[-0.16222772 -0.10639679  0.10288087  0.09668112]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.05121488  0.10281513]]. Action = [[ 0.1270206   0.05611777 -0.20653802  0.03255248]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Scene graph at timestep 1216 is [True, False, False, False, True, False]
State prediction error at timestep 1216 is tensor(5.0611e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1216 of 1
Current timestep = 1217. State = [[-0.05407446  0.10901318]]. Action = [[-0.19384076  0.04096928 -0.2356087  -0.60226434]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1217 is [True, False, False, False, True, False]
Current timestep = 1218. State = [[-0.056626    0.11052984]]. Action = [[ 0.096618   -0.05448167  0.23602661 -0.3960502 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1218 is [True, False, False, False, True, False]
Current timestep = 1219. State = [[-0.05442758  0.10122447]]. Action = [[ 0.0539237  -0.09068677 -0.01229675  0.8340423 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1219 is [True, False, False, False, True, False]
Current timestep = 1220. State = [[-0.04438406  0.09409559]]. Action = [[ 0.24519384 -0.00669689  0.17369497 -0.27483094]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1220 is [True, False, False, False, True, False]
Current timestep = 1221. State = [[-0.27912727  0.11797078]]. Action = [[-0.02853748 -0.01619948 -0.1901083  -0.379735  ]]. Reward = [100.]
Curr episode timestep = 45
Scene graph at timestep 1221 is [False, True, False, False, True, False]
Current timestep = 1222. State = [[-0.26989818  0.1344684 ]]. Action = [[ 0.14875403  0.04284042 -0.00352471 -0.24560714]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1222 is [True, False, False, False, True, False]
Current timestep = 1223. State = [[-0.24557675  0.12737246]]. Action = [[ 0.2424916  -0.19226144  0.11427373  0.37063646]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1223 is [True, False, False, False, False, True]
Current timestep = 1224. State = [[-0.22659312  0.11919126]]. Action = [[-0.21908472  0.17443568  0.19649976 -0.29167616]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1224 is [True, False, False, False, False, True]
Current timestep = 1225. State = [[-0.21257444  0.10662928]]. Action = [[ 0.21043554 -0.15692751  0.1709373   0.13112462]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1225 is [True, False, False, False, True, False]
Current timestep = 1226. State = [[-0.2004014   0.10528517]]. Action = [[-0.10451661  0.15145421 -0.08032998  0.6759844 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Scene graph at timestep 1226 is [True, False, False, False, True, False]
State prediction error at timestep 1226 is tensor(7.1100e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of 1
Current timestep = 1227. State = [[-0.20170125  0.10544287]]. Action = [[-0.04107557 -0.13411973  0.10170993  0.8259611 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Scene graph at timestep 1227 is [True, False, False, False, True, False]
State prediction error at timestep 1227 is tensor(1.7645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1227 of 1
Current timestep = 1228. State = [[-0.20705716  0.10880181]]. Action = [[-0.13160653  0.18398708  0.0035018   0.8307704 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Current timestep = 1229. State = [[-0.21145265  0.11709058]]. Action = [[ 0.11174017 -0.06327397  0.20482475  0.15257418]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1229 is [True, False, False, False, True, False]
Current timestep = 1230. State = [[-0.20092556  0.10408633]]. Action = [[ 0.19704717 -0.16623767 -0.06470904  0.50228286]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Current timestep = 1231. State = [[-0.19482578  0.10757469]]. Action = [[-0.16979793  0.23233289  0.080809    0.26062512]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Current timestep = 1232. State = [[-0.19768     0.10854941]]. Action = [[ 0.05057859 -0.18128613  0.18322274  0.687248  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.19211026  0.10457195]]. Action = [[ 0.12971789  0.08837301 -0.12270528  0.8527112 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.17650653  0.10563126]]. Action = [[ 0.16765893 -0.03271598  0.10157549 -0.54530996]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.17138717  0.11862014]]. Action = [[-0.18262231  0.21265763 -0.09157261  0.6274195 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1235 is [True, False, False, False, True, False]
Current timestep = 1236. State = [[-0.17139804  0.12354068]]. Action = [[ 0.14131159 -0.12944186  0.19504523  0.7207935 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Current timestep = 1237. State = [[-0.1668148   0.11790968]]. Action = [[-0.0307342  -0.03243163 -0.15872632  0.837981  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Current timestep = 1238. State = [[-0.17323601  0.12163049]]. Action = [[-0.21541406  0.08168527  0.16158986 -0.5938111 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1238 is [True, False, False, False, True, False]
Scene graph at timestep 1238 is [True, False, False, False, True, False]
State prediction error at timestep 1238 is tensor(6.1136e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 0
Current timestep = 1239. State = [[-0.17607792  0.13015808]]. Action = [[ 0.2101798   0.08483222 -0.24115288 -0.95527893]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1239 is [True, False, False, False, True, False]
Current timestep = 1240. State = [[-0.16685309  0.12434984]]. Action = [[ 0.0484134 -0.1671238 -0.143804   0.6253004]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1240 is [True, False, False, False, False, True]
Current timestep = 1241. State = [[-0.160676    0.10771316]]. Action = [[ 0.01494116 -0.13587683  0.15217587  0.47978938]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1241 is [True, False, False, False, True, False]
Current timestep = 1242. State = [[-0.15398303  0.08421094]]. Action = [[ 0.07415944 -0.21877433 -0.04849291  0.57636094]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1242 is [True, False, False, False, True, False]
Current timestep = 1243. State = [[-0.13843682  0.06735135]]. Action = [[ 0.2250635   0.0285854  -0.22431049 -0.32209325]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1243 is [True, False, False, False, True, False]
Current timestep = 1244. State = [[-0.11801314  0.06191888]]. Action = [[ 0.04292598 -0.07674921  0.10970449 -0.16058648]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1244 is [True, False, False, False, True, False]
Scene graph at timestep 1244 is [True, False, False, False, True, False]
State prediction error at timestep 1244 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of 1
Current timestep = 1245. State = [[-0.11529001  0.0560593 ]]. Action = [[-0.0967374   0.0288431  -0.19471113  0.80352473]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1245 is [True, False, False, False, True, False]
Current timestep = 1246. State = [[-0.10997113  0.05975352]]. Action = [[ 0.20467743  0.05378208 -0.07555133  0.64314604]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1246 is [True, False, False, False, True, False]
Current timestep = 1247. State = [[-0.10226286  0.0656129 ]]. Action = [[-0.16451761  0.02720055 -0.2390854  -0.04160142]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1247 is [True, False, False, False, True, False]
Current timestep = 1248. State = [[-0.10743356  0.05563132]]. Action = [[-0.14338471 -0.22299123 -0.22478007  0.41221094]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1248 is [True, False, False, False, True, False]
Scene graph at timestep 1248 is [True, False, False, False, True, False]
State prediction error at timestep 1248 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1248 of 1
Current timestep = 1249. State = [[-0.11884447  0.05232117]]. Action = [[-0.04267634  0.18238124  0.17412037  0.05121744]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1249 is [True, False, False, False, True, False]
