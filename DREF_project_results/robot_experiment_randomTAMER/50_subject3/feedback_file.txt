Current timestep = 0. State = [[-0.15785967  0.03568971]]. Action = [[-0.20755233  0.00062847 -0.00493112  0.2947433 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0170, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.16088516  0.04341546]]. Action = [[ 0.17340595  0.10423541 -0.21961847  0.63080215]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of 1
Current timestep = 2. State = [[-0.16460375  0.06440821]]. Action = [[-0.13148478  0.2462157  -0.04359776  0.06580794]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.18015696  0.09727414]]. Action = [[-0.24131507  0.19965559 -0.05156179  0.2839507 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.2049079   0.12730244]]. Action = [[-0.20488766  0.05902293  0.13234043 -0.3866191 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Scene graph at timestep 4 is [True, False, False, False, False, True]
State prediction error at timestep 4 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.21723323  0.14300673]]. Action = [[ 0.22950017  0.220373   -0.01846215 -0.58453137]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Scene graph at timestep 5 is [True, False, False, False, False, True]
State prediction error at timestep 5 is tensor(0.0285, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of 1
Current timestep = 6. State = [[-0.20422801  0.14442196]]. Action = [[ 0.15456891 -0.1854836   0.04171351  0.9782177 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Scene graph at timestep 6 is [True, False, False, False, False, True]
State prediction error at timestep 6 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.18755637  0.1329058 ]]. Action = [[ 0.12295777 -0.01136984  0.1784986  -0.8276312 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.17998238  0.14166544]]. Action = [[-0.21792488  0.14850199 -0.05721369 -0.74640626]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Scene graph at timestep 8 is [True, False, False, False, False, True]
State prediction error at timestep 8 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.18451494  0.14614257]]. Action = [[ 0.17918837 -0.10883698  0.09445459 -0.64071935]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Current timestep = 10. State = [[-0.18493864  0.14870003]]. Action = [[-0.08739252  0.16038835 -0.09978408  0.5075221 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0170, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.19330366  0.1631293 ]]. Action = [[-0.15630853  0.07113895 -0.23560987 -0.25853616]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Scene graph at timestep 11 is [True, False, False, False, False, True]
State prediction error at timestep 11 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.21055935  0.18390904]]. Action = [[-0.22367862  0.18366385 -0.16080026  0.7181524 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.22830437  0.18998708]]. Action = [[-0.06943965 -0.23946539 -0.02973165  0.21657383]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.24167669  0.1761016 ]]. Action = [[-0.19686162  0.05676991  0.17194939 -0.9277234 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Scene graph at timestep 14 is [True, False, False, False, False, True]
State prediction error at timestep 14 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.25927237  0.17905656]]. Action = [[-0.13944441 -0.04235047 -0.09900993  0.7989938 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.25502804  0.17677799]]. Action = [[ 0.17004865 -0.03052996 -0.12463143  0.042521  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Scene graph at timestep 16 is [True, False, False, False, False, True]
State prediction error at timestep 16 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.25298205  0.178355  ]]. Action = [[-0.0437406   0.1323958   0.09035733 -0.07491291]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.25227708  0.17314121]]. Action = [[ 0.00342312 -0.23264603  0.10245556  0.9162693 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Scene graph at timestep 18 is [True, False, False, False, False, True]
State prediction error at timestep 18 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.24929368  0.16188382]]. Action = [[ 0.02974877  0.05468541 -0.16465843 -0.9103277 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Current timestep = 20. State = [[-0.25510427  0.16794124]]. Action = [[-0.15179051  0.05281511 -0.21346205 -0.2269029 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.25955033  0.17300786]]. Action = [[-0.18017924 -0.22060965 -0.2424676   0.03474164]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.2601705   0.17353538]]. Action = [[-0.21143776 -0.2233905   0.11656612 -0.21777177]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.25579444  0.182522  ]]. Action = [[ 0.20975229  0.19397753 -0.18995437  0.7861897 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Scene graph at timestep 23 is [True, False, False, False, False, True]
State prediction error at timestep 23 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.25421205  0.20989789]]. Action = [[-0.07375106  0.24857044 -0.23313785 -0.8340485 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Scene graph at timestep 24 is [True, False, False, False, False, True]
State prediction error at timestep 24 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.26187432  0.22899763]]. Action = [[-0.14619145  0.20253327  0.08532548  0.79004383]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Current timestep = 26. State = [[-0.26007718  0.22552794]]. Action = [[ 0.00279436 -0.10398558  0.0324038   0.6719117 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, False, True]
State prediction error at timestep 26 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.26396585  0.23101135]]. Action = [[-0.06735072  0.18875962 -0.1239633  -0.08928955]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, False, True]
State prediction error at timestep 27 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.26643628  0.2369204 ]]. Action = [[ 0.00761187 -0.12931643 -0.19885653  0.8651601 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, False, True]
Current timestep = 29. State = [[-0.26447567  0.23211251]]. Action = [[-0.09424165  0.19711071  0.07568103  0.43985653]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, False, True]
Current timestep = 30. State = [[-0.25423062  0.23304324]]. Action = [[ 0.24462903  0.06967515 -0.0594098   0.819041  ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, False, True]
Scene graph at timestep 30 is [True, False, False, False, False, True]
State prediction error at timestep 30 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.2273221  0.2412896]]. Action = [[ 0.22471327  0.07689223  0.12278071 -0.4981073 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, False, True]
Current timestep = 32. State = [[-0.19736129  0.23642388]]. Action = [[ 0.22005743 -0.17117365  0.05448699 -0.00819933]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, False, True]
Current timestep = 33. State = [[-0.1668248   0.22655971]]. Action = [[ 0.2179341   0.00129417 -0.04737861 -0.03033346]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, False, True]
Scene graph at timestep 33 is [True, False, False, False, False, True]
State prediction error at timestep 33 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.1352334   0.23530091]]. Action = [[0.22760746 0.20800215 0.11121508 0.49960923]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, False, True]
Scene graph at timestep 34 is [True, False, False, False, False, True]
State prediction error at timestep 34 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.09500438  0.2414149 ]]. Action = [[ 0.2143271  -0.20114963  0.22722104 -0.8749453 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, False, True]
Scene graph at timestep 35 is [True, False, False, False, False, True]
State prediction error at timestep 35 is tensor(0.0138, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.06849085  0.22288156]]. Action = [[ 0.11258996 -0.0888055   0.14673829 -0.87683475]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, False, True]
Scene graph at timestep 36 is [True, False, False, False, False, True]
State prediction error at timestep 36 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.0565412   0.21661545]]. Action = [[ 0.14489543  0.06525642 -0.05664483 -0.20638835]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, False, True]
Scene graph at timestep 37 is [True, False, False, False, False, True]
State prediction error at timestep 37 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.03209918  0.23365285]]. Action = [[ 0.10727069  0.18076217 -0.080314    0.7953465 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, False, True]
Scene graph at timestep 38 is [False, True, False, False, False, True]
State prediction error at timestep 38 is tensor(0.0217, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.01542965  0.24285986]]. Action = [[-0.23598632 -0.1891779   0.20629203  0.9560143 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [False, True, False, False, False, True]
Current timestep = 40. State = [[-0.02614222  0.25044876]]. Action = [[-0.23053701  0.20492464 -0.09860951 -0.03649163]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [False, True, False, False, False, True]
Current timestep = 41. State = [[-0.05022171  0.25863254]]. Action = [[-0.18171814 -0.13134088 -0.20991395  0.2229104 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [False, True, False, False, False, True]
Scene graph at timestep 41 is [True, False, False, False, False, True]
State prediction error at timestep 41 is tensor(0.0211, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.07418377  0.25525272]]. Action = [[ 0.14557844  0.17499349 -0.18931746  0.725129  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, False, True]
Scene graph at timestep 42 is [True, False, False, False, False, True]
State prediction error at timestep 42 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.08163002  0.2740212 ]]. Action = [[-0.1669273   0.20401567  0.171262   -0.32623768]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, False, True]
Scene graph at timestep 43 is [True, False, False, False, False, True]
State prediction error at timestep 43 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.09614694  0.28986084]]. Action = [[-0.13673887 -0.14899147 -0.13854575  0.8551481 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, False, True]
Current timestep = 45. State = [[-0.1028343   0.27157986]]. Action = [[-0.14216089 -0.23100649  0.24570495 -0.61281866]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, False, True]
Current timestep = 46. State = [[-0.10724917  0.24574956]]. Action = [[ 0.18148148 -0.13426585 -0.21166363  0.01496577]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, False, True]
Current timestep = 47. State = [[-0.106132    0.22228947]]. Action = [[-0.18548077 -0.21897933 -0.00262228 -0.7315362 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, False, True]
Scene graph at timestep 47 is [True, False, False, False, False, True]
State prediction error at timestep 47 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.12422994  0.20508465]]. Action = [[-0.07766227  0.15106857 -0.22496514  0.64278483]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, False, True]
Scene graph at timestep 48 is [True, False, False, False, False, True]
State prediction error at timestep 48 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.14071375  0.21090716]]. Action = [[-0.23374675 -0.13489546 -0.23920174  0.11805964]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, False, True]
Current timestep = 50. State = [[-0.16812077  0.20781662]]. Action = [[-0.01655495  0.14398539 -0.05526692  0.07969677]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, False, True]
Scene graph at timestep 50 is [True, False, False, False, False, True]
State prediction error at timestep 50 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.17569888  0.20460917]]. Action = [[-0.08336164 -0.21986495 -0.22530754  0.24954629]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, False, True]
Current timestep = 52. State = [[-0.17729926  0.19481696]]. Action = [[0.23050427 0.15090209 0.22741443 0.08259451]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, False, True]
Scene graph at timestep 52 is [True, False, False, False, False, True]
State prediction error at timestep 52 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of -1
Current timestep = 53. State = [[-0.16758502  0.20831442]]. Action = [[0.17298669 0.20814082 0.05520141 0.3042295 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, False, True]
Scene graph at timestep 53 is [True, False, False, False, False, True]
State prediction error at timestep 53 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.15269399  0.21694718]]. Action = [[ 0.12932056 -0.05886185 -0.10742295 -0.785626  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, False, True]
Scene graph at timestep 54 is [True, False, False, False, False, True]
State prediction error at timestep 54 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.1461174  0.2168829]]. Action = [[ 0.04877511  0.05926198  0.09845349 -0.98791057]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, False, True]
Current timestep = 56. State = [[-0.12902768  0.22920956]]. Action = [[ 0.21693972  0.10824853  0.23106983 -0.15619642]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, False, True]
Scene graph at timestep 56 is [True, False, False, False, False, True]
State prediction error at timestep 56 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.09841602  0.24070121]]. Action = [[0.23568839 0.05530542 0.06114593 0.64712465]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, False, True]
Current timestep = 58. State = [[-0.06536164  0.23855574]]. Action = [[ 0.24616933 -0.13447757 -0.07937202  0.8868203 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, False, True]
Current timestep = 59. State = [[-0.03137976  0.22728257]]. Action = [[ 0.13343146 -0.06283891 -0.09643292 -0.54473895]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, False, True]
Scene graph at timestep 59 is [False, True, False, False, False, True]
State prediction error at timestep 59 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.01015038  0.22530594]]. Action = [[ 0.10949373  0.0568369  -0.01445967 -0.43232846]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [False, True, False, False, False, True]
Scene graph at timestep 60 is [False, True, False, False, False, True]
State prediction error at timestep 60 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[0.00132628 0.2332492 ]]. Action = [[-0.2345077  -0.0180777  -0.12725449 -0.69807464]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [False, True, False, False, False, True]
Current timestep = 62. State = [[-0.00373588  0.23348093]]. Action = [[-0.08947706 -0.024229    0.00846842 -0.27772313]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [False, True, False, False, False, True]
Current timestep = 63. State = [[-0.01408487  0.22070263]]. Action = [[-0.21253447 -0.22175959  0.04552647  0.47925448]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [False, True, False, False, False, True]
Current timestep = 64. State = [[-0.04046093  0.219891  ]]. Action = [[-0.19123809  0.2233538  -0.12402526 -0.40846193]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [False, True, False, False, False, True]
Current timestep = 65. State = [[-0.06469668  0.23587161]]. Action = [[-0.02611868  0.04056406  0.22734058  0.5283382 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [False, True, False, False, False, True]
Scene graph at timestep 65 is [True, False, False, False, False, True]
State prediction error at timestep 65 is tensor(0.0084, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.07999457  0.25258175]]. Action = [[-0.1955522   0.1926744   0.05837038 -0.8017081 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, False, True]
Scene graph at timestep 66 is [True, False, False, False, False, True]
State prediction error at timestep 66 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.09607018  0.26871553]]. Action = [[ 0.21764612  0.00305867  0.1157099  -0.26781958]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, False, True]
Current timestep = 68. State = [[-0.08458515  0.26615348]]. Action = [[ 0.21678749  0.04614827 -0.09452483  0.41840172]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, False, True]
Scene graph at timestep 68 is [True, False, False, False, False, True]
State prediction error at timestep 68 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.06562662  0.25631496]]. Action = [[ 0.13207695 -0.17928784 -0.03940541 -0.66775376]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, False, True]
Current timestep = 70. State = [[-0.04771979  0.22833729]]. Action = [[ 0.14899606 -0.22988823 -0.12192786 -0.8541243 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, False, True]
Scene graph at timestep 70 is [False, True, False, False, False, True]
State prediction error at timestep 70 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.03287409  0.2089513 ]]. Action = [[-0.22027968 -0.00639236 -0.10977539 -0.42169374]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [False, True, False, False, False, True]
Scene graph at timestep 71 is [False, True, False, False, False, True]
State prediction error at timestep 71 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.03600854  0.21831034]]. Action = [[ 0.11362612  0.17299408 -0.02277718  0.8561723 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [False, True, False, False, False, True]
Current timestep = 73. State = [[-0.03692641  0.21845675]]. Action = [[-0.10870847 -0.17961287  0.18865702  0.8893633 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [False, True, False, False, False, True]
Scene graph at timestep 73 is [False, True, False, False, False, True]
State prediction error at timestep 73 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.03747468  0.19646853]]. Action = [[ 0.05662304 -0.22318074 -0.13269915 -0.43838274]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [False, True, False, False, False, True]
Scene graph at timestep 74 is [False, True, False, False, False, True]
State prediction error at timestep 74 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.03888734  0.18374407]]. Action = [[-0.17690845  0.06173196  0.01772845 -0.8574437 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [False, True, False, False, False, True]
Current timestep = 76. State = [[-0.04603047  0.19285475]]. Action = [[-0.03036039  0.07532051  0.03148839  0.54174685]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [False, True, False, False, False, True]
Scene graph at timestep 76 is [False, True, False, False, False, True]
State prediction error at timestep 76 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.04693663  0.20243402]]. Action = [[ 0.21145272  0.10553572  0.08157218 -0.61334133]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [False, True, False, False, False, True]
Current timestep = 78. State = [[-0.04385576  0.19646527]]. Action = [[-0.04570825 -0.17229845  0.22045809  0.25596285]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [False, True, False, False, False, True]
Current timestep = 79. State = [[-0.04484465  0.19800998]]. Action = [[-0.08464795  0.12644947  0.10442477  0.72867274]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [False, True, False, False, False, True]
Current timestep = 80. State = [[-0.04515939  0.21074553]]. Action = [[ 0.1680296   0.1925075  -0.08110669 -0.8121993 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [False, True, False, False, False, True]
Scene graph at timestep 80 is [False, True, False, False, False, True]
State prediction error at timestep 80 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.04189858  0.21609378]]. Action = [[-0.19623396 -0.21597722 -0.07216853 -0.8957227 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [False, True, False, False, False, True]
Current timestep = 82. State = [[-0.0476816   0.20313701]]. Action = [[-0.01095816 -0.04724033 -0.22335187 -0.90023303]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [False, True, False, False, False, True]
Scene graph at timestep 82 is [False, True, False, False, False, True]
State prediction error at timestep 82 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.04144837  0.1848865 ]]. Action = [[ 0.21811324 -0.23044187 -0.04873583  0.6563842 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [False, True, False, False, False, True]
Scene graph at timestep 83 is [False, True, False, False, False, True]
State prediction error at timestep 83 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.02557404  0.1537708 ]]. Action = [[ 0.07990757 -0.13871445  0.2128349  -0.9517929 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [False, True, False, False, False, True]
Scene graph at timestep 84 is [False, True, False, False, False, True]
State prediction error at timestep 84 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.01741849  0.13543554]]. Action = [[ 0.0478811  -0.06766266 -0.20062006  0.8094276 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [False, True, False, False, False, True]
Scene graph at timestep 85 is [False, True, False, False, False, True]
State prediction error at timestep 85 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.17783055 -0.13597809]]. Action = [[-0.20119056 -0.20465283 -0.01822683  0.8793454 ]]. Reward = [100.]
Curr episode timestep = 86
Scene graph at timestep 86 is [False, True, False, False, False, True]
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0535, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 86 of 1
Current timestep = 87. State = [[-0.15747249 -0.14095801]]. Action = [[ 0.23279124  0.17273277 -0.13160576 -0.754341  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 87 is [True, False, False, True, False, False]
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0365, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.1268769  -0.12268165]]. Action = [[ 0.12998533  0.12045538  0.2110529  -0.6935926 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 88 is [True, False, False, True, False, False]
Current timestep = 89. State = [[-0.123867  -0.1082535]]. Action = [[-0.19386643  0.1423344   0.17658421 -0.5040469 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0254, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.13565218 -0.0933825 ]]. Action = [[-0.2145749   0.04095051  0.15120071 -0.6686577 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 90 is [True, False, False, False, True, False]
Current timestep = 91. State = [[-0.13917275 -0.09498233]]. Action = [[ 0.2314116  -0.10109442  0.221336   -0.43110698]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.13169873 -0.09686389]]. Action = [[ 0.07470429  0.01316401 -0.17081742  0.3111391 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 92 is [True, False, False, False, True, False]
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.12210535 -0.10341195]]. Action = [[ 0.13017762 -0.10899654 -0.1581043   0.2790668 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 93 is [True, False, False, False, True, False]
Current timestep = 94. State = [[-0.11412244 -0.09754403]]. Action = [[-0.11022028  0.23556885 -0.19182792 -0.1420759 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.12095764 -0.07195453]]. Action = [[-0.18748905  0.2031911   0.23120448 -0.37370324]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.12305006 -0.04241217]]. Action = [[0.21646997 0.21364957 0.20991284 0.7412257 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.11070128 -0.01401101]]. Action = [[ 0.21382636  0.19931465  0.23343375 -0.25406188]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 97 is [True, False, False, False, True, False]
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.08965445  0.00586821]]. Action = [[ 0.00284204 -0.03048915 -0.00956018 -0.32146317]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 98 is [True, False, False, False, True, False]
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.08403774 -0.0079388 ]]. Action = [[ 0.11560947 -0.22405183 -0.12077852 -0.8591124 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.06477091 -0.02921969]]. Action = [[ 0.24042219 -0.12111738 -0.12852307 -0.9380627 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 100 is [True, False, False, False, True, False]
Current timestep = 101. State = [[-0.04724026 -0.03751439]]. Action = [[-0.11936909  0.05180529 -0.09762742 -0.926052  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.05353411 -0.02587537]]. Action = [[-0.21349822  0.18491393 -0.13008581 -0.7713213 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 102 is [False, True, False, False, True, False]
Current timestep = 103. State = [[-0.05558665 -0.01564669]]. Action = [[ 0.22504246 -0.02711467  0.19898045 -0.7900522 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 103 is [True, False, False, False, True, False]
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.05077324 -0.02790962]]. Action = [[-0.08641364 -0.20873234  0.21256328  0.96680975]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 104 is [True, False, False, False, True, False]
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.05381228 -0.0483622 ]]. Action = [[-0.10934322 -0.07879263 -0.1122351   0.46214104]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-0.06306899 -0.05806142]]. Action = [[-0.17884919 -0.05561507 -0.18682148  0.3256955 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.07357076 -0.06419221]]. Action = [[ 0.1968551  -0.0053378   0.18289554  0.36198294]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.0715744  -0.06743044]]. Action = [[-0.04209058 -0.06979245  0.07012522  0.5692055 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 108 is [True, False, False, False, True, False]
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0159, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.07277332 -0.08617807]]. Action = [[-0.04770705 -0.21621558 -0.05011706 -0.718573  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 109 is [True, False, False, False, True, False]
Current timestep = 110. State = [[-0.0733826  -0.10607671]]. Action = [[ 0.05405438 -0.08123894 -0.04461922  0.3764087 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.0760023  -0.10742122]]. Action = [[-0.1702968   0.13653964 -0.00358038  0.68816614]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.07335276 -0.10196183]]. Action = [[ 0.23042503  0.01136458  0.18312252 -0.8504051 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 112 is [True, False, False, False, True, False]
Current timestep = 113. State = [[-0.07165261 -0.09545279]]. Action = [[-0.0281353   0.07840136 -0.0698884  -0.01306289]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 113 is [True, False, False, False, True, False]
Current timestep = 114. State = [[-0.07278612 -0.0891194 ]]. Action = [[-0.12981215  0.02692026  0.00889805  0.64925313]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 114 is [True, False, False, False, True, False]
Current timestep = 115. State = [[-0.0750742  -0.08055975]]. Action = [[-0.03591099  0.07376671 -0.04921471  0.08635759]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 115 is [True, False, False, False, True, False]
Current timestep = 116. State = [[-0.07772505 -0.07558672]]. Action = [[-0.05199416 -0.01133272  0.07535288  0.95639014]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 116 is [True, False, False, False, True, False]
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.07754359 -0.07675131]]. Action = [[ 0.16834942 -0.04127938  0.22777241  0.3065772 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 117 is [True, False, False, False, True, False]
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.07891185 -0.07253274]]. Action = [[-0.17757025  0.09175292 -0.14077905  0.5114473 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 118 is [True, False, False, False, True, False]
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.08045637 -0.07845013]]. Action = [[ 0.11299038 -0.20091245 -0.19807245 -0.33831608]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 119 is [True, False, False, False, True, False]
Current timestep = 120. State = [[-0.08572403 -0.0780758 ]]. Action = [[-0.24040525  0.21127307 -0.1242162   0.11720622]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.09630919 -0.0595999 ]]. Action = [[0.01299697 0.11954051 0.19819406 0.05800259]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 121 is [True, False, False, False, True, False]
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.10837707 -0.05944151]]. Action = [[-0.22047822 -0.18341564  0.2213571  -0.6405827 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 122 is [True, False, False, False, True, False]
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0092, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.12963475 -0.07373431]]. Action = [[-0.0311925  -0.01296699  0.08533573 -0.8584296 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 123 is [True, False, False, False, True, False]
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.13806695 -0.08126519]]. Action = [[-0.19955415 -0.11049458  0.0993312  -0.7819691 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 124 is [True, False, False, False, True, False]
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0106, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of -1
Current timestep = 125. State = [[-0.1632394  -0.08046876]]. Action = [[ 0.09208906  0.19864061 -0.05854869  0.6048292 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 125 is [True, False, False, False, True, False]
Current timestep = 126. State = [[-0.1537285 -0.076993 ]]. Action = [[ 0.20687163 -0.17584659  0.03897536  0.2522689 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 126 is [True, False, False, False, True, False]
Current timestep = 127. State = [[-0.14792399 -0.09652641]]. Action = [[-0.09803    -0.19600146  0.1709756   0.28443146]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 127 is [True, False, False, False, True, False]
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.14085855 -0.11769008]]. Action = [[ 0.21145952 -0.05788377 -0.20106612  0.74073124]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.13195379 -0.11145287]]. Action = [[ 0.01896223  0.17469889  0.13693053 -0.27347505]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.12805167 -0.10619851]]. Action = [[ 0.04562721 -0.06229085 -0.10176934 -0.87383604]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 130 is [True, False, False, False, True, False]
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.12839271 -0.09855196]]. Action = [[-0.22687468  0.15885234 -0.04382446 -0.30680776]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 131 is [True, False, False, False, True, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.14390193 -0.09434182]]. Action = [[-0.20536953 -0.0916096  -0.18540211  0.5681033 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 132 is [True, False, False, False, True, False]
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.15648556 -0.10771253]]. Action = [[ 0.07473844 -0.12900609 -0.03391905 -0.11750138]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 133 is [True, False, False, False, True, False]
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.1486014  -0.10479744]]. Action = [[ 0.23896778  0.19928548  0.05175644 -0.94289404]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 134 is [True, False, False, False, True, False]
Current timestep = 135. State = [[-0.13915408 -0.09543734]]. Action = [[-0.01375154 -0.02462628 -0.15087815 -0.3553233 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 135 is [True, False, False, False, True, False]
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.13291724 -0.08426074]]. Action = [[ 0.13017485  0.16883737 -0.22620997 -0.4804986 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.12202111 -0.06066943]]. Action = [[ 0.04355133  0.12794235 -0.03468025  0.6177106 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.11976921 -0.05320685]]. Action = [[-0.00901844  0.01803109 -0.06528923 -0.2720356 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 138 is [True, False, False, False, True, False]
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.1226985  -0.05574367]]. Action = [[-0.20634028 -0.12893035  0.16502702 -0.71086967]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.12819822 -0.06028927]]. Action = [[0.08284488 0.08877826 0.10582879 0.86654425]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 140 is [True, False, False, False, True, False]
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.13658026 -0.04503546]]. Action = [[-0.24255863  0.16340569 -0.1936886   0.13418114]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 141 is [True, False, False, False, True, False]
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.1479562  -0.01568557]]. Action = [[-0.01784906  0.23737758 -0.15450671 -0.91169673]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 142 is [True, False, False, False, True, False]
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.14898156  0.00745109]]. Action = [[0.1891802  0.05710697 0.0889242  0.5517454 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.13588075  0.01237719]]. Action = [[ 0.19705218  0.0318833  -0.15757011 -0.325634  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.11211021  0.01891805]]. Action = [[ 0.24291638  0.07076329  0.07733276 -0.5564842 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.08330446  0.01285162]]. Action = [[-0.03150305 -0.23244928 -0.09639961 -0.57825655]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.07997378  0.00259498]]. Action = [[ 0.10475937  0.00975829  0.13122845 -0.9292219 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.08142383 -0.00671677]]. Action = [[-0.20524493 -0.11471748 -0.02651501  0.3530457 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.08672697 -0.00987248]]. Action = [[-0.0942656   0.11286879 -0.00705165  0.7743665 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.10065056  0.00156673]]. Action = [[-0.22384037  0.07716909  0.01254922 -0.5584197 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 150 is [True, False, False, False, True, False]
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.11988667 -0.00306611]]. Action = [[ 0.01278901 -0.19582711 -0.03037514  0.58855414]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 151 is [True, False, False, False, True, False]
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of 0
Current timestep = 152. State = [[-0.12431128 -0.00439758]]. Action = [[ 0.0977858   0.21941662 -0.11510488  0.08611393]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 152 is [True, False, False, False, True, False]
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.12728271  0.02274996]]. Action = [[-0.05696617  0.21471235  0.00491011  0.69584095]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 153 is [True, False, False, False, True, False]
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.13664994  0.04387756]]. Action = [[-0.20463245  0.02013963  0.21909094  0.26527214]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.14181492  0.04886582]]. Action = [[ 0.17549011  0.00430039 -0.1736944   0.73952866]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.13634823  0.05891839]]. Action = [[ 0.08632904  0.17954087 -0.19776393 -0.8258549 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.13292973  0.0852446 ]]. Action = [[-0.05244696  0.23201501  0.16546687  0.09440458]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.1434445   0.10231587]]. Action = [[-0.22847784 -0.0121545   0.18189934  0.43356574]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.15891075  0.11457784]]. Action = [[-0.18612388  0.05711231  0.09327906  0.63458896]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 159 is [True, False, False, False, True, False]
Current timestep = 160. State = [[-0.16951998  0.11011318]]. Action = [[ 0.05965966 -0.11076361 -0.22466156 -0.01219583]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 160 is [True, False, False, False, True, False]
Current timestep = 161. State = [[-0.17008588  0.09879046]]. Action = [[-0.02484712 -0.08626276 -0.19687699  0.56306505]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 161 is [True, False, False, False, True, False]
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.17838357  0.09532204]]. Action = [[-0.10623962  0.08434376  0.14036596 -0.379699  ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 162 is [True, False, False, False, True, False]
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.19367735  0.11560627]]. Action = [[-0.22890624  0.2332814   0.02300119  0.8719387 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.2063888   0.13125253]]. Action = [[ 0.13279346 -0.06034073 -0.05256517  0.09503055]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 164 is [True, False, False, False, True, False]
Scene graph at timestep 164 is [True, False, False, False, False, True]
State prediction error at timestep 164 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.20042947  0.12069928]]. Action = [[ 0.08892995 -0.14498004  0.19592509 -0.61684006]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 165 is [True, False, False, False, False, True]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(9.0475e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.20029797  0.10536523]]. Action = [[-0.0693379  -0.06093183  0.23534536 -0.70894796]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 166 is [True, False, False, False, True, False]
Current timestep = 167. State = [[-0.2091259   0.10354697]]. Action = [[-0.1502832   0.03015903 -0.18325612  0.44746745]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.21516468  0.10664228]]. Action = [[ 0.02276927  0.05735701 -0.0670445   0.42249   ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.22560152  0.09765917]]. Action = [[-0.24400522 -0.22360592 -0.01319221  0.59671366]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 169 is [True, False, False, False, True, False]
Current timestep = 170. State = [[-0.24588011  0.06976663]]. Action = [[-0.00693642 -0.21588375  0.07243684  0.1624763 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 170 is [True, False, False, False, True, False]
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.251314    0.04787032]]. Action = [[-0.20806542 -0.2315212   0.14226788 -0.80504185]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 171 is [True, False, False, False, True, False]
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.2543773   0.05775818]]. Action = [[-0.02282827  0.18325257  0.10397822  0.9525002 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 172 is [True, False, False, False, True, False]
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.2584776   0.08306446]]. Action = [[ 0.067339    0.22270277 -0.06551543  0.6647763 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 173 is [True, False, False, False, True, False]
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 173 of -1
Current timestep = 174. State = [[-0.25329953  0.08910421]]. Action = [[ 0.14238197 -0.19028382  0.21593827 -0.82395065]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 174 is [True, False, False, False, True, False]
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.24535017  0.07974932]]. Action = [[-0.19290257  0.20982912  0.04146001  0.67634094]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 175 is [True, False, False, False, True, False]
Current timestep = 176. State = [[-0.2352228   0.06594305]]. Action = [[ 0.21424806 -0.20977029 -0.16142687  0.19405818]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 176 is [True, False, False, False, True, False]
Current timestep = 177. State = [[-0.22570564  0.05744743]]. Action = [[-0.11236897  0.16471726  0.11522982  0.7338872 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.22129782  0.056418  ]]. Action = [[ 0.18498266 -0.11433858 -0.01920211  0.3964157 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.21464506  0.04475522]]. Action = [[-0.0777694  -0.13899782 -0.17345023  0.5710628 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.21138819  0.03795639]]. Action = [[0.10074502 0.05137402 0.15473011 0.69547355]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.2128      0.04871845]]. Action = [[-0.08855976  0.19608182 -0.20929521 -0.6441925 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 181 is [True, False, False, False, True, False]
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.20724739  0.06529225]]. Action = [[ 0.17151448  0.05085802 -0.23062682 -0.579941  ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.20013392  0.05952508]]. Action = [[-0.15723908 -0.20412128 -0.18138304  0.00745273]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.20359288  0.04948985]]. Action = [[-0.07699376  0.02777198 -0.10878423  0.7483766 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.21410641  0.03548435]]. Action = [[-0.14139888 -0.23223116 -0.01356561 -0.6670574 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 185 is [True, False, False, False, True, False]
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.23202135  0.01947824]]. Action = [[-0.14926945  0.04597744  0.02372369  0.59676075]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.23854564  0.02155321]]. Action = [[ 0.17566073  0.02493715 -0.16349502  0.70504236]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 187 is [True, False, False, False, True, False]
Current timestep = 188. State = [[-0.23043638  0.03379505]]. Action = [[ 0.12448841  0.18973547 -0.05852263 -0.08865267]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(4.3001e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.2244893   0.04808833]]. Action = [[-0.09850357  0.00342095  0.00145611 -0.883995  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 189 is [True, False, False, False, True, False]
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.22408673  0.03788382]]. Action = [[ 0.05094102 -0.22106898 -0.11737441 -0.9936681 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 190 is [True, False, False, False, True, False]
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.21814047  0.01946597]]. Action = [[ 0.07139984 -0.10161304 -0.00385474 -0.75131243]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 191 is [True, False, False, False, True, False]
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(3.4124e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.21996257 -0.0056034 ]]. Action = [[-0.13183866 -0.24288793  0.19002458 -0.7537414 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 192 is [True, False, False, False, True, False]
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.22313897 -0.027744  ]]. Action = [[ 0.19013327 -0.00727288  0.2148084   0.8969815 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.22333631 -0.03958821]]. Action = [[-0.19821931 -0.16674905  0.10666952  0.5691774 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.22703408 -0.05023157]]. Action = [[-0.01205632  0.02076417 -0.13553998  0.8240049 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 195 is [True, False, False, False, True, False]
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.23154275 -0.06320504]]. Action = [[-0.06440625 -0.18721624  0.2066685  -0.31125998]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 196 is [True, False, False, False, True, False]
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.23061189 -0.08416843]]. Action = [[ 0.24159887 -0.07219735 -0.07003158  0.13697457]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.21100757 -0.10502676]]. Action = [[ 0.21996805 -0.23250099 -0.21087998  0.7649461 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 198 is [True, False, False, False, True, False]
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.18389428 -0.13734396]]. Action = [[ 0.15167892 -0.21153958 -0.01889433 -0.36400414]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 199 is [True, False, False, False, True, False]
Scene graph at timestep 199 is [True, False, False, True, False, False]
State prediction error at timestep 199 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.16023467 -0.1672204 ]]. Action = [[ 0.22255301 -0.2330986  -0.14450638  0.01918399]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 200 is [True, False, False, True, False, False]
Scene graph at timestep 200 is [True, False, False, True, False, False]
State prediction error at timestep 200 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.129255   -0.17925172]]. Action = [[ 0.123725    0.17627478 -0.11621547  0.03099668]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 201 is [True, False, False, True, False, False]
Current timestep = 202. State = [[-0.11601484 -0.17856394]]. Action = [[ 0.07975104 -0.16197093  0.12142551  0.10999143]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 202 is [True, False, False, True, False, False]
Scene graph at timestep 202 is [True, False, False, True, False, False]
State prediction error at timestep 202 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.10550991 -0.19127658]]. Action = [[ 0.0674907  -0.0950336  -0.02704139 -0.68890554]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 203 is [True, False, False, True, False, False]
Current timestep = 204. State = [[-0.08626226 -0.19011319]]. Action = [[ 0.19568682  0.13165736 -0.19394422  0.31484342]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 204 is [True, False, False, True, False, False]
Current timestep = 205. State = [[-0.07045327 -0.18702133]]. Action = [[-0.03130426 -0.05114317 -0.04011178  0.22318852]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 205 is [True, False, False, True, False, False]
Scene graph at timestep 205 is [True, False, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.05887149 -0.17851652]]. Action = [[ 0.20364398  0.16833383 -0.12637076  0.24711025]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 206 is [True, False, False, True, False, False]
Scene graph at timestep 206 is [True, False, False, True, False, False]
State prediction error at timestep 206 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.03293636 -0.15829669]]. Action = [[ 0.15708822  0.181158    0.07047039 -0.6328424 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 207 is [True, False, False, True, False, False]
Current timestep = 208. State = [[-0.013444  -0.1426635]]. Action = [[ 0.11137497  0.06841132 -0.1997779  -0.584333  ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 208 is [False, True, False, True, False, False]
Scene graph at timestep 208 is [False, True, False, True, False, False]
State prediction error at timestep 208 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[ 0.00716967 -0.1433906 ]]. Action = [[ 0.17342383 -0.20521384  0.18579698 -0.63280255]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 209 is [False, True, False, True, False, False]
Current timestep = 210. State = [[ 0.01979178 -0.16108865]]. Action = [[-0.03119127 -0.07733256  0.07189658  0.76562464]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 210 is [False, True, False, True, False, False]
Scene graph at timestep 210 is [False, True, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[ 0.02147329 -0.16761218]]. Action = [[-0.10061042  0.0510388  -0.24485146  0.8634529 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 211 is [False, True, False, True, False, False]
Current timestep = 212. State = [[ 0.02227056 -0.1716215 ]]. Action = [[ 0.13556182 -0.10485888  0.24330115  0.27900076]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 212 is [False, True, False, True, False, False]
Current timestep = 213. State = [[-0.16264287 -0.08541553]]. Action = [[ 0.05849952 -0.00893298  0.12991732 -0.34090304]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 213 is [False, True, False, True, False, False]
Current timestep = 214. State = [[-0.14336082 -0.10623184]]. Action = [[ 0.12991148 -0.19689465  0.12522787  0.21161592]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 214 is [True, False, False, False, True, False]
Current timestep = 215. State = [[-0.12247995 -0.10866389]]. Action = [[ 0.2432433   0.2311312   0.04726446 -0.7813897 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.09104749 -0.09379257]]. Action = [[0.18301699 0.07314235 0.14790729 0.29707372]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 216 is [True, False, False, False, True, False]
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.06833457 -0.07774045]]. Action = [[-0.02951187  0.12482241 -0.23392288  0.9908645 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 217 is [True, False, False, False, True, False]
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.06399099 -0.05522856]]. Action = [[0.16519517 0.20132211 0.23993582 0.48825717]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.05123012 -0.02794003]]. Action = [[-0.10123757  0.20751303 -0.1498683   0.36491835]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 219 is [True, False, False, False, True, False]
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.05690415 -0.02244242]]. Action = [[-0.07233424 -0.2330518  -0.22867598 -0.21910852]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.06627469 -0.03062502]]. Action = [[-0.2234695   0.0920991   0.14529616 -0.35871863]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.07528134 -0.01657381]]. Action = [[-0.05766249  0.17047477 -0.0402959   0.18315029]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 222 is [True, False, False, False, True, False]
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.09471007  0.00682911]]. Action = [[-0.19478525  0.11702845  0.11752519 -0.32599413]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.10814948  0.02557174]]. Action = [[0.08778948 0.16555557 0.11446339 0.9553286 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 224 is [True, False, False, False, True, False]
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.11212973  0.05033552]]. Action = [[ 0.00473285  0.19077426 -0.05072802 -0.6841563 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 225 is [True, False, False, False, True, False]
Current timestep = 226. State = [[-0.10880645  0.04899108]]. Action = [[ 0.16811475 -0.24180767  0.07438454 -0.29471684]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.09650908  0.0419042 ]]. Action = [[ 0.14219445  0.07123709  0.03570926 -0.862399  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.08580651  0.04432444]]. Action = [[-0.00231382  0.03278109 -0.13369195 -0.74607813]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 228 is [True, False, False, False, True, False]
Current timestep = 229. State = [[-0.08962511  0.05654608]]. Action = [[-0.18076654  0.15076149  0.01801184  0.92277694]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 229 is [True, False, False, False, True, False]
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of 1
Current timestep = 230. State = [[-0.09138992  0.05561693]]. Action = [[ 0.10659614 -0.23010468 -0.23587094  0.22645855]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 230 is [True, False, False, False, True, False]
Current timestep = 231. State = [[-0.08702147  0.02737087]]. Action = [[ 0.01135817 -0.24579684 -0.23336045  0.6886853 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.0749689 -0.0060074]]. Action = [[ 0.23213997 -0.16382273 -0.03264305  0.6163516 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.05929333 -0.00578152]]. Action = [[ 0.02330625  0.22495127 -0.0179622   0.9814011 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 233 is [True, False, False, False, True, False]
Current timestep = 234. State = [[-0.05549223  0.00710181]]. Action = [[0.03645429 0.04084069 0.10979819 0.45802927]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 234 is [True, False, False, False, True, False]
Current timestep = 235. State = [[-0.05517187  0.01049236]]. Action = [[-0.11967306 -0.04960343 -0.17696856  0.6518152 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 235 is [True, False, False, False, True, False]
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.05230907  0.0012355 ]]. Action = [[ 0.18177292 -0.138303   -0.0190783   0.5903648 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 236 is [True, False, False, False, True, False]
Current timestep = 237. State = [[-0.04164562 -0.008479  ]]. Action = [[ 0.18457198  0.00419292  0.19220039 -0.30764103]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 237 is [True, False, False, False, True, False]
Current timestep = 238. State = [[-0.20434232  0.08883243]]. Action = [[-0.06115991 -0.21431576  0.20709753  0.5444243 ]]. Reward = [100.]
Curr episode timestep = 24
Scene graph at timestep 238 is [False, True, False, False, True, False]
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.18758741  0.09902081]]. Action = [[ 0.13236088 -0.04805866  0.02529487 -0.5532582 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.18382847  0.10573419]]. Action = [[-0.20672017  0.08696929 -0.15639852  0.06401038]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 240 is [True, False, False, False, True, False]
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.19761397  0.11938021]]. Action = [[-0.19207156  0.10387245  0.155348    0.49187863]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 241 is [True, False, False, False, True, False]
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.22053362  0.11910839]]. Action = [[-0.23832747 -0.20394437 -0.05271564  0.40009475]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 242 is [True, False, False, False, True, False]
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.24949065  0.11350451]]. Action = [[-0.13765986  0.18673432  0.17736185  0.5698583 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 243 is [True, False, False, False, True, False]
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.26194456  0.12391807]]. Action = [[ 0.03424945 -0.1020211  -0.18261634  0.96254086]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 244 is [True, False, False, False, True, False]
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.25714543  0.12020887]]. Action = [[ 0.1502611   0.12191838  0.14392847 -0.86741275]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 245 is [True, False, False, False, True, False]
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.25378302  0.12585333]]. Action = [[-0.15885141 -0.15944426  0.1747343  -0.8546549 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 246 is [True, False, False, False, True, False]
Current timestep = 247. State = [[-0.25379375  0.12591545]]. Action = [[-0.19331706 -0.08579719  0.07157892  0.10061681]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 247 is [True, False, False, False, False, True]
Scene graph at timestep 247 is [True, False, False, False, False, True]
State prediction error at timestep 247 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.25198892  0.11991597]]. Action = [[-0.04221334 -0.14438538  0.12591058  0.4911158 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 248 is [True, False, False, False, False, True]
Current timestep = 249. State = [[-0.2497171  0.1111314]]. Action = [[ 0.06966946 -0.02542435  0.00374377  0.61364794]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 249 is [True, False, False, False, True, False]
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 1
Current timestep = 250. State = [[-0.24543335  0.09697839]]. Action = [[-0.00225693 -0.18531565 -0.00531662  0.7798953 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 250 is [True, False, False, False, True, False]
Current timestep = 251. State = [[-0.24664511  0.07874329]]. Action = [[-0.09649384 -0.10904649 -0.05747905 -0.55016464]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 251 is [True, False, False, False, True, False]
Current timestep = 252. State = [[-0.25084049  0.06897403]]. Action = [[-0.17243987  0.02479783  0.22068039  0.7294855 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 252 is [True, False, False, False, True, False]
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 1
Current timestep = 253. State = [[-0.24642816  0.07135134]]. Action = [[ 0.19050622  0.13778472  0.11490899 -0.7175369 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 253 is [True, False, False, False, True, False]
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of 0
Current timestep = 254. State = [[-0.23973697  0.07550512]]. Action = [[-0.07870081 -0.12488107 -0.24092032 -0.5837911 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 254 is [True, False, False, False, True, False]
Current timestep = 255. State = [[-0.23517242  0.06069759]]. Action = [[ 0.14622122 -0.12657979  0.17042947 -0.6313329 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 255 is [True, False, False, False, True, False]
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 255 of 1
Current timestep = 256. State = [[-0.22537953  0.05374713]]. Action = [[ 0.09571168  0.13043025  0.23682538 -0.5646524 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 256 is [True, False, False, False, True, False]
Current timestep = 257. State = [[-0.22439964  0.05787312]]. Action = [[-0.21926433 -0.06504235  0.19414812  0.5490596 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 257 is [True, False, False, False, True, False]
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.24140282  0.06457707]]. Action = [[-0.24298854  0.11634684 -0.1310491   0.6188104 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 258 is [True, False, False, False, True, False]
Current timestep = 259. State = [[-0.25896916  0.08168574]]. Action = [[ 0.02305216  0.1512022   0.10885555 -0.8509156 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 259 is [True, False, False, False, True, False]
Current timestep = 260. State = [[-0.25556183  0.09377315]]. Action = [[0.22046459 0.07059595 0.03119227 0.3162248 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 260 is [True, False, False, False, True, False]
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.23702113  0.10675757]]. Action = [[ 0.21928892  0.09078711  0.08800769 -0.5592965 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 261 is [True, False, False, False, True, False]
Current timestep = 262. State = [[-0.21912168  0.10013591]]. Action = [[ 0.00747064 -0.2315189   0.0102579   0.6800356 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 262 is [True, False, False, False, True, False]
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(7.1835e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of 1
Current timestep = 263. State = [[-0.20890999  0.07177566]]. Action = [[ 0.08109984 -0.21915257  0.12215021  0.27712655]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 263 is [True, False, False, False, True, False]
Current timestep = 264. State = [[-0.19850895  0.04416792]]. Action = [[ 0.09262359 -0.19327322 -0.13096544 -0.4503988 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 264 is [True, False, False, False, True, False]
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(5.4863e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of 1
Current timestep = 265. State = [[-0.18021381  0.02730117]]. Action = [[ 0.23910591  0.05562022 -0.15883526 -0.44215584]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 265 is [True, False, False, False, True, False]
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.16318724  0.03406737]]. Action = [[-0.17572276  0.06113103 -0.16259982 -0.29838616]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 266 is [True, False, False, False, True, False]
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.16083556  0.04946093]]. Action = [[ 0.22478247  0.20763782 -0.12307337  0.9021511 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.14639756  0.06472986]]. Action = [[ 0.18730834  0.01929414  0.13561952 -0.51666343]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 268 is [True, False, False, False, True, False]
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.13001989  0.08408133]]. Action = [[-0.12981948  0.22364742 -0.15834059 -0.9131061 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 269 is [True, False, False, False, True, False]
Current timestep = 270. State = [[-0.13104583  0.10477455]]. Action = [[0.13283545 0.11634156 0.19888967 0.6150737 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.11494683  0.10173204]]. Action = [[ 0.22069684 -0.21820983 -0.02834974  0.5576701 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 271 is [True, False, False, False, True, False]
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(3.1986e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of 1
Current timestep = 272. State = [[-0.09057958  0.08215208]]. Action = [[ 0.04950103 -0.15114921 -0.06638949 -0.02881205]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 272 is [True, False, False, False, True, False]
Current timestep = 273. State = [[-0.08006343  0.05683573]]. Action = [[ 0.1537494  -0.22890928  0.17387885  0.8389449 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 273 is [True, False, False, False, True, False]
Current timestep = 274. State = [[-0.06542613  0.03767613]]. Action = [[ 0.0212591  -0.03147341  0.01144361  0.31296122]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 274 is [True, False, False, False, True, False]
Current timestep = 275. State = [[-0.06628303  0.02150058]]. Action = [[-0.18018086 -0.1879853   0.21581554 -0.6064016 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 275 is [True, False, False, False, True, False]
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(2.6206e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.0708737   0.00541693]]. Action = [[-0.11986274 -0.04142588  0.17724437  0.0521481 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 276 is [True, False, False, False, True, False]
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.08248384  0.00821637]]. Action = [[-0.18703258  0.16863438 -0.01528384 -0.8120523 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.09366934  0.02319517]]. Action = [[ 0.05033258  0.05741784 -0.0062425  -0.599798  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 278 is [True, False, False, False, True, False]
Current timestep = 279. State = [[-0.10138365  0.04154271]]. Action = [[-0.17115521  0.20597321 -0.18565306  0.9669614 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 279 is [True, False, False, False, True, False]
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(3.4905e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.11362542  0.06225535]]. Action = [[ 0.16729355  0.00652817  0.15837196 -0.46694452]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.11377575  0.07288585]]. Action = [[-0.05630468  0.1748969  -0.12730545 -0.4450543 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.12175696  0.07847708]]. Action = [[-0.20637594 -0.11417121 -0.20942846  0.7452574 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 282 is [True, False, False, False, True, False]
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.12780848  0.07559824]]. Action = [[ 0.01622847  0.06622365  0.01006147 -0.7143654 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 283 is [True, False, False, False, True, False]
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(4.3073e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.14035437  0.09382981]]. Action = [[-0.24311216  0.23752183  0.24488944 -0.3887028 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 284 is [True, False, False, False, True, False]
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.16969219  0.10586286]]. Action = [[-0.14865306 -0.20401163 -0.15279773  0.5307975 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 285 is [True, False, False, False, True, False]
Current timestep = 286. State = [[-0.18901852  0.10277764]]. Action = [[-0.1464623   0.17943424 -0.05907735 -0.8394449 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.20021263  0.11699481]]. Action = [[ 0.0335052   0.05260712 -0.11700165  0.54521275]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 287 is [True, False, False, False, True, False]
Current timestep = 288. State = [[-0.2087063   0.11287808]]. Action = [[-0.19541025 -0.15701938 -0.11812668  0.32848263]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.23689848  0.1120934 ]]. Action = [[-0.23689304  0.14399338 -0.21013306  0.24963391]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 289 is [True, False, False, False, True, False]
Current timestep = 290. State = [[-0.25923818  0.12618864]]. Action = [[-0.0213009   0.07005984  0.20728505  0.3185823 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 290 is [True, False, False, False, True, False]
Scene graph at timestep 290 is [True, False, False, False, False, True]
State prediction error at timestep 290 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.26890364  0.13114788]]. Action = [[-0.2014246  -0.15788008  0.15757424  0.73588455]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 291 is [True, False, False, False, False, True]
Scene graph at timestep 291 is [True, False, False, False, False, True]
State prediction error at timestep 291 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.26936376  0.1311122 ]]. Action = [[-0.0879603   0.13584352  0.1850822   0.7508192 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 292 is [True, False, False, False, False, True]
Scene graph at timestep 292 is [True, False, False, False, False, True]
State prediction error at timestep 292 is tensor(2.9152e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.26980004  0.130978  ]]. Action = [[-0.14782448  0.13030034 -0.24857755  0.74840236]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 293 is [True, False, False, False, False, True]
Current timestep = 294. State = [[-0.27071917  0.12857096]]. Action = [[-0.04089859 -0.0405962  -0.16837363  0.5321921 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 294 is [True, False, False, False, False, True]
Current timestep = 295. State = [[-0.27280432  0.12556511]]. Action = [[-0.1829378   0.16825485 -0.22324286  0.9102144 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 295 is [True, False, False, False, False, True]
Scene graph at timestep 295 is [True, False, False, False, False, True]
State prediction error at timestep 295 is tensor(7.8938e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 295 of -1
Current timestep = 296. State = [[-0.27402762  0.1249272 ]]. Action = [[-0.23860076 -0.23115407 -0.10544309  0.7919631 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 296 is [True, False, False, False, False, True]
Current timestep = 297. State = [[-0.26858813  0.1217078 ]]. Action = [[ 0.2039567  -0.06356588 -0.14997795  0.3475901 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 297 is [True, False, False, False, True, False]
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[-0.2574838   0.11379023]]. Action = [[ 0.05198216 -0.04337496  0.11526889  0.8624873 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 298 is [True, False, False, False, True, False]
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(1.5149e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.2559288   0.11082432]]. Action = [[-0.18819593 -0.06367573  0.11655873  0.9774575 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 299 is [True, False, False, False, True, False]
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(1.6570e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.25797462  0.11014096]]. Action = [[-0.11758491 -0.01437044  0.17553878  0.02399492]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 300 is [True, False, False, False, True, False]
Current timestep = 301. State = [[-0.25903317  0.11909311]]. Action = [[ 0.12046424  0.21555749 -0.24367724 -0.15839285]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 301 is [True, False, False, False, True, False]
Current timestep = 302. State = [[-0.26001278  0.12760721]]. Action = [[-0.2362334   0.09494281 -0.02491391 -0.87944746]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 302 is [True, False, False, False, True, False]
Current timestep = 303. State = [[-0.24841616  0.1409435 ]]. Action = [[ 0.24185553  0.18113369  0.24326283 -0.686435  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 303 is [True, False, False, False, False, True]
Current timestep = 304. State = [[-0.22292961  0.14288498]]. Action = [[ 0.16115445 -0.18641146  0.22534078 -0.64626503]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 304 is [True, False, False, False, False, True]
Current timestep = 305. State = [[-0.19810371  0.12831311]]. Action = [[ 0.22758907 -0.12801297  0.22505382  0.7255149 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 305 is [True, False, False, False, False, True]
Current timestep = 306. State = [[-0.16927731  0.1304775 ]]. Action = [[ 0.24599093  0.23807079  0.21651846 -0.45061314]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 306 is [True, False, False, False, False, True]
Current timestep = 307. State = [[-0.13766654  0.14362268]]. Action = [[ 0.15812671 -0.0275581  -0.15699367 -0.444691  ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 307 is [True, False, False, False, False, True]
Current timestep = 308. State = [[-0.11849506  0.14631908]]. Action = [[ 0.04753429 -0.002454   -0.05605319 -0.6182916 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 308 is [True, False, False, False, False, True]
Current timestep = 309. State = [[-0.11565524  0.13748252]]. Action = [[-0.1547318  -0.17017923 -0.11041766  0.42305636]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 309 is [True, False, False, False, False, True]
Current timestep = 310. State = [[-0.11242227  0.13588148]]. Action = [[ 0.21108636  0.17110747  0.12683904 -0.7838429 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 310 is [True, False, False, False, False, True]
Current timestep = 311. State = [[-0.10527796  0.14183734]]. Action = [[-0.06132364 -0.02819821 -0.11061487  0.61990523]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 311 is [True, False, False, False, False, True]
Current timestep = 312. State = [[-0.11061209  0.13685071]]. Action = [[-0.2457201  -0.10920474  0.12029332  0.7988198 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 312 is [True, False, False, False, False, True]
Current timestep = 313. State = [[-0.12410097  0.1311757 ]]. Action = [[-0.15141912 -0.01514247 -0.23613189 -0.08778155]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 313 is [True, False, False, False, False, True]
Current timestep = 314. State = [[-0.1316095   0.11536922]]. Action = [[ 0.08732337 -0.22282647  0.09251353 -0.5683609 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 314 is [True, False, False, False, False, True]
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(9.7995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.1410981   0.08428253]]. Action = [[-0.20550305 -0.21803184 -0.24456216 -0.7371338 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 315 is [True, False, False, False, True, False]
Current timestep = 316. State = [[-0.16005686  0.08002866]]. Action = [[-0.10813949  0.20724982 -0.07263902 -0.66461414]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 316 is [True, False, False, False, True, False]
Current timestep = 317. State = [[-0.1617555   0.08001215]]. Action = [[ 0.17069569 -0.18663335 -0.03226559 -0.9857681 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 317 is [True, False, False, False, True, False]
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(4.9907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.16550222  0.05349859]]. Action = [[-0.23506096 -0.22895412 -0.02253944 -0.07749331]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 318 is [True, False, False, False, True, False]
Current timestep = 319. State = [[-0.17223023  0.02318002]]. Action = [[ 0.13027608 -0.21909538  0.00728932  0.60467315]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 319 is [True, False, False, False, True, False]
Current timestep = 320. State = [[-0.16289568  0.00188601]]. Action = [[ 0.22310442 -0.00649235 -0.00575152  0.73330843]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 320 is [True, False, False, False, True, False]
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.1493622  -0.01402756]]. Action = [[ 0.14873397 -0.1659213  -0.23220924  0.63978505]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 321 is [True, False, False, False, True, False]
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.13434526 -0.03439659]]. Action = [[ 3.2597780e-04 -8.8771671e-02  2.4226731e-01  4.4777727e-01]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.12850875 -0.04268335]]. Action = [[ 0.12200713 -0.02158991 -0.12150449  0.7941427 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 323 is [True, False, False, False, True, False]
Current timestep = 324. State = [[-0.12631257 -0.03553584]]. Action = [[-0.20679733  0.19055778 -0.00412823  0.5600219 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 324 is [True, False, False, False, True, False]
Current timestep = 325. State = [[-0.134782   -0.03626276]]. Action = [[-0.09002665 -0.20512092 -0.23909774 -0.2567079 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 325 is [True, False, False, False, True, False]
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(3.0098e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.1361297  -0.05763258]]. Action = [[ 0.11521032 -0.19863203  0.09083042  0.42952454]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 326 is [True, False, False, False, True, False]
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.13363406 -0.07567938]]. Action = [[-3.4535304e-02  3.6676794e-02  1.9311905e-05  9.7776198e-01]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 327 is [True, False, False, False, True, False]
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.12945534 -0.08688763]]. Action = [[ 0.17721471 -0.22173855 -0.13619752  0.6127453 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 328 is [True, False, False, False, True, False]
Current timestep = 329. State = [[-0.12380605 -0.09014259]]. Action = [[-0.03886338  0.2212714   0.22885525 -0.01909602]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 329 is [True, False, False, False, True, False]
Current timestep = 330. State = [[-0.11189464 -0.07076449]]. Action = [[ 0.2438421   0.14733243  0.22739395 -0.34816027]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 330 is [True, False, False, False, True, False]
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.08381154 -0.04182688]]. Action = [[ 0.1419071   0.22701275 -0.06930715  0.4911605 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 331 is [True, False, False, False, True, False]
Current timestep = 332. State = [[-0.07571413 -0.01849025]]. Action = [[-0.0706372   0.14445719  0.09588283 -0.9909506 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 332 is [True, False, False, False, True, False]
Current timestep = 333. State = [[-0.07550617 -0.01345821]]. Action = [[ 0.09198397 -0.1550779  -0.17802937 -0.2781961 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 333 is [True, False, False, False, True, False]
Current timestep = 334. State = [[-0.06756343 -0.00852533]]. Action = [[ 0.10539529  0.17810553  0.11650833 -0.6474234 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 334 is [True, False, False, False, True, False]
Current timestep = 335. State = [[-0.05211198 -0.00285146]]. Action = [[ 0.11337134 -0.06596562 -0.00291528 -0.32335675]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 335 is [True, False, False, False, True, False]
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.04874906 -0.01445495]]. Action = [[-0.21128944 -0.1958564   0.02056208 -0.44706774]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 336 is [True, False, False, False, True, False]
Scene graph at timestep 336 is [False, True, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.05067228 -0.03382406]]. Action = [[ 0.06129867 -0.07518746  0.0927622  -0.8023972 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 337 is [False, True, False, False, True, False]
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.05790832 -0.05493992]]. Action = [[-0.18986967 -0.22915745  0.13799638 -0.76024365]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 338 is [True, False, False, False, True, False]
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(5.6718e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of 1
Current timestep = 339. State = [[-0.06761707 -0.0699612 ]]. Action = [[ 0.07892391  0.08551565 -0.04924333  0.13169813]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 339 is [True, False, False, False, True, False]
Current timestep = 340. State = [[-0.05930336 -0.0612362 ]]. Action = [[ 0.19665682  0.10009256 -0.06411776 -0.7841544 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 340 is [True, False, False, False, True, False]
Current timestep = 341. State = [[-0.04501304 -0.04101436]]. Action = [[ 0.02572402  0.2184791  -0.15053238  0.9784316 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 341 is [True, False, False, False, True, False]
Current timestep = 342. State = [[-0.04385667 -0.03013881]]. Action = [[-0.07322748 -0.07958338  0.01374111 -0.06887901]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 342 is [False, True, False, False, True, False]
Current timestep = 343. State = [[-0.04090213 -0.0355882 ]]. Action = [[ 0.15858775 -0.08101191 -0.24490176  0.4343511 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 343 is [False, True, False, False, True, False]
Current timestep = 344. State = [[-0.24901375  0.0300389 ]]. Action = [[ 0.24829674  0.04489523 -0.04627825  0.38843155]]. Reward = [100.]
Curr episode timestep = 105
Scene graph at timestep 344 is [False, True, False, False, True, False]
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0237, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.24492495  0.0274702 ]]. Action = [[-0.01858284 -0.146663   -0.21910834  0.9066119 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 345 is [True, False, False, False, True, False]
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(6.4239e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of 1
Current timestep = 346. State = [[-0.24331895  0.00716205]]. Action = [[ 0.02588317 -0.20939136  0.15345216  0.45489323]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 346 is [True, False, False, False, True, False]
Current timestep = 347. State = [[-0.23860088 -0.01638593]]. Action = [[ 0.09767073 -0.16064331  0.19367504  0.9867034 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 347 is [True, False, False, False, True, False]
Current timestep = 348. State = [[-0.23569043 -0.03354274]]. Action = [[-0.16279398 -0.06014749  0.11465612  0.38694263]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 348 is [True, False, False, False, True, False]
Current timestep = 349. State = [[-0.2448809  -0.05057361]]. Action = [[-0.13509275 -0.13756691  0.22764915  0.7631147 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 349 is [True, False, False, False, True, False]
Current timestep = 350. State = [[-0.25317597 -0.07167632]]. Action = [[ 0.03050187 -0.1506164  -0.03362469 -0.0352515 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 350 is [True, False, False, False, True, False]
Current timestep = 351. State = [[-0.2516096  -0.09456866]]. Action = [[ 0.14081359 -0.19177504 -0.10205638  0.8011689 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 351 is [True, False, False, False, True, False]
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of 1
Current timestep = 352. State = [[-0.24448706 -0.10237809]]. Action = [[ 0.06596935  0.20235816 -0.02068521 -0.0874356 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 352 is [True, False, False, False, True, False]
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of 1
Current timestep = 353. State = [[-0.23896639 -0.10120273]]. Action = [[-0.02819929 -0.20097181 -0.17130487  0.9470079 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(8.0171e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.22760049 -0.12041292]]. Action = [[ 0.2255688  -0.11167774  0.18280193  0.6905248 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 354 is [True, False, False, False, True, False]
Current timestep = 355. State = [[-0.22020201 -0.13528213]]. Action = [[-0.20969287 -0.09563574  0.10325545 -0.59543204]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 355 is [True, False, False, False, True, False]
Scene graph at timestep 355 is [True, False, False, True, False, False]
State prediction error at timestep 355 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.22406085 -0.13707663]]. Action = [[ 0.13808346  0.17404458  0.05515048 -0.93207824]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 356 is [True, False, False, True, False, False]
Scene graph at timestep 356 is [True, False, False, True, False, False]
State prediction error at timestep 356 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of 1
Current timestep = 357. State = [[-0.21868567 -0.13105516]]. Action = [[-0.00211212 -0.13737251  0.05275154 -0.20607412]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 357 is [True, False, False, True, False, False]
Scene graph at timestep 357 is [True, False, False, True, False, False]
State prediction error at timestep 357 is tensor(3.8392e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.21863143 -0.1335572 ]]. Action = [[-0.07551023  0.12279591  0.13863394 -0.9326363 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 358 is [True, False, False, True, False, False]
Scene graph at timestep 358 is [True, False, False, True, False, False]
State prediction error at timestep 358 is tensor(8.0894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.21957618 -0.1360985 ]]. Action = [[-0.00799835 -0.14544144  0.24634838 -0.07154262]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 359 is [True, False, False, True, False, False]
Scene graph at timestep 359 is [True, False, False, True, False, False]
State prediction error at timestep 359 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.22301131 -0.15585229]]. Action = [[ 0.04287207 -0.19113164  0.22149825  0.46176672]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 360 is [True, False, False, True, False, False]
Scene graph at timestep 360 is [True, False, False, True, False, False]
State prediction error at timestep 360 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.22736372 -0.1821018 ]]. Action = [[-0.11861308 -0.16898923  0.05666983  0.5060822 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 361 is [True, False, False, True, False, False]
Scene graph at timestep 361 is [True, False, False, True, False, False]
State prediction error at timestep 361 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.22483891 -0.19381222]]. Action = [[0.23624337 0.04079711 0.18236396 0.49108255]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 362 is [True, False, False, True, False, False]
Scene graph at timestep 362 is [True, False, False, True, False, False]
State prediction error at timestep 362 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.21249416 -0.19759002]]. Action = [[-0.0118719  -0.10011843 -0.09131451 -0.21303415]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 363 is [True, False, False, True, False, False]
Scene graph at timestep 363 is [True, False, False, True, False, False]
State prediction error at timestep 363 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of 1
Current timestep = 364. State = [[-0.2115458  -0.19710408]]. Action = [[-0.02698569  0.15349641 -0.02055769  0.8605287 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 364 is [True, False, False, True, False, False]
Scene graph at timestep 364 is [True, False, False, True, False, False]
State prediction error at timestep 364 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 0
Current timestep = 365. State = [[-0.21221144 -0.18914591]]. Action = [[-0.12408294 -0.01066542  0.05210963 -0.36109853]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 365 is [True, False, False, True, False, False]
Current timestep = 366. State = [[-0.2241453  -0.18283169]]. Action = [[-0.17775694  0.13025838  0.03814504 -0.70693624]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 366 is [True, False, False, True, False, False]
Current timestep = 367. State = [[-0.24198796 -0.16990271]]. Action = [[-0.12078336  0.09916759  0.20880163  0.39303255]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 367 is [True, False, False, True, False, False]
Current timestep = 368. State = [[-0.2562835  -0.16944091]]. Action = [[-0.06440729 -0.11833471 -0.21042575 -0.23548865]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 368 is [True, False, False, True, False, False]
Current timestep = 369. State = [[-0.25473246 -0.1611239 ]]. Action = [[ 0.243545    0.2115184  -0.16117081  0.43924212]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 369 is [True, False, False, True, False, False]
Scene graph at timestep 369 is [True, False, False, True, False, False]
State prediction error at timestep 369 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 369 of 1
Current timestep = 370. State = [[-0.23760088 -0.14628096]]. Action = [[ 0.18767554 -0.02544005 -0.19983783 -0.91775835]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 370 is [True, False, False, True, False, False]
Scene graph at timestep 370 is [True, False, False, True, False, False]
State prediction error at timestep 370 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of 1
Current timestep = 371. State = [[-0.22059688 -0.1554593 ]]. Action = [[ 0.07743531 -0.21578428  0.0044181   0.35250664]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 371 is [True, False, False, True, False, False]
Current timestep = 372. State = [[-0.20478863 -0.16707036]]. Action = [[ 0.21421817  0.02142236 -0.180538    0.3233235 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 372 is [True, False, False, True, False, False]
Scene graph at timestep 372 is [True, False, False, True, False, False]
State prediction error at timestep 372 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.17398898 -0.1700704 ]]. Action = [[0.19363657 0.0150829  0.09201881 0.45284116]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 373 is [True, False, False, True, False, False]
Current timestep = 374. State = [[-0.16471432 -0.16015725]]. Action = [[-0.23084834  0.20403486  0.07512385  0.51630735]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 374 is [True, False, False, True, False, False]
Current timestep = 375. State = [[-0.17271948 -0.15703043]]. Action = [[-0.06983817 -0.15202244 -0.11127409 -0.861906  ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 375 is [True, False, False, True, False, False]
Current timestep = 376. State = [[-0.17614967 -0.16119075]]. Action = [[ 0.03146061  0.05077857 -0.12052041 -0.95153666]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 376 is [True, False, False, True, False, False]
Scene graph at timestep 376 is [True, False, False, True, False, False]
State prediction error at timestep 376 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.18293518 -0.17064132]]. Action = [[-0.1391352  -0.16955844  0.00885418  0.41354728]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 377 is [True, False, False, True, False, False]
Scene graph at timestep 377 is [True, False, False, True, False, False]
State prediction error at timestep 377 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.19441642 -0.18973005]]. Action = [[ 0.08394715 -0.14814784  0.22348201  0.55994296]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 378 is [True, False, False, True, False, False]
Scene graph at timestep 378 is [True, False, False, True, False, False]
State prediction error at timestep 378 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.18820967 -0.19105564]]. Action = [[ 0.13513777  0.13585627 -0.1699825   0.99263453]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 379 is [True, False, False, True, False, False]
Current timestep = 380. State = [[-0.18714075 -0.17428073]]. Action = [[-0.20986395  0.21451455 -0.17024039  0.37766385]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 380 is [True, False, False, True, False, False]
Current timestep = 381. State = [[-0.18582621 -0.14704883]]. Action = [[0.17420986 0.17008358 0.23463553 0.3376453 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 381 is [True, False, False, True, False, False]
Current timestep = 382. State = [[-0.17548302 -0.12553541]]. Action = [[ 0.18088523  0.11267066 -0.02849348 -0.22907645]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 382 is [True, False, False, True, False, False]
Current timestep = 383. State = [[-0.16508146 -0.10223018]]. Action = [[-0.06535429  0.20182997 -0.02613764  0.20192838]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 383 is [True, False, False, True, False, False]
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of 1
Current timestep = 384. State = [[-0.16560832 -0.08762378]]. Action = [[-0.00433615 -0.09911509  0.00220701  0.5416486 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 384 is [True, False, False, False, True, False]
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(8.8911e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of 1
Current timestep = 385. State = [[-0.165756   -0.10475701]]. Action = [[ 0.01104406 -0.22924443 -0.06776628 -0.5079091 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 385 is [True, False, False, False, True, False]
Current timestep = 386. State = [[-0.16161533 -0.10973704]]. Action = [[0.0926699  0.19051564 0.11250913 0.7585312 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 386 is [True, False, False, False, True, False]
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(9.2747e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of 1
Current timestep = 387. State = [[-0.15470451 -0.10069002]]. Action = [[ 0.0475103  -0.02726954  0.15256947 -0.9026639 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 387 is [True, False, False, False, True, False]
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of 1
Current timestep = 388. State = [[-0.14920467 -0.09166615]]. Action = [[0.08043262 0.17251456 0.00419778 0.8572557 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 388 is [True, False, False, False, True, False]
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of 1
Current timestep = 389. State = [[-0.13322425 -0.0923686 ]]. Action = [[ 0.14071217 -0.19822565  0.04973    -0.20036405]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 389 is [True, False, False, False, True, False]
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(2.5600e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of 1
Current timestep = 390. State = [[-0.11448189 -0.11479402]]. Action = [[ 0.09845638 -0.191621   -0.24323545 -0.3586914 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 390 is [True, False, False, False, True, False]
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of 1
Current timestep = 391. State = [[-0.10590766 -0.13446237]]. Action = [[-0.01671381 -0.07347876 -0.2283418   0.9305103 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 391 is [True, False, False, False, True, False]
Scene graph at timestep 391 is [True, False, False, True, False, False]
State prediction error at timestep 391 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.10580076 -0.1299251 ]]. Action = [[-0.00711863  0.21959132 -0.05470316 -0.30363095]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 392 is [True, False, False, True, False, False]
Current timestep = 393. State = [[-0.10827556 -0.12825412]]. Action = [[-0.15630776 -0.15658537 -0.08541319  0.8384299 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 393 is [True, False, False, True, False, False]
Current timestep = 394. State = [[-0.11917569 -0.12295887]]. Action = [[-0.16403627  0.22162485  0.16169012  0.164289  ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 394 is [True, False, False, True, False, False]
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(5.7301e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.12900367 -0.11893502]]. Action = [[-0.0413174  -0.15862957  0.04694358  0.23260891]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 395 is [True, False, False, False, True, False]
Current timestep = 396. State = [[-0.131502   -0.11434625]]. Action = [[ 0.03605357  0.22803181 -0.05360049  0.6354215 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 396 is [True, False, False, False, True, False]
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(9.6556e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.13265625 -0.09762847]]. Action = [[ 0.04468513  0.03080991 -0.24796648 -0.59929925]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 397 is [True, False, False, False, True, False]
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of 1
Current timestep = 398. State = [[-0.12349847 -0.0843427 ]]. Action = [[ 0.23604542  0.14939308 -0.11456472 -0.48364252]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 398 is [True, False, False, False, True, False]
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of 1
Current timestep = 399. State = [[-0.11187603 -0.07170779]]. Action = [[-0.00097592 -0.05217345 -0.18456541  0.21250892]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 399 is [True, False, False, False, True, False]
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.11123966 -0.07688556]]. Action = [[ 0.00272298 -0.064457    0.00163016 -0.72272587]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 400 is [True, False, False, False, True, False]
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of 1
Current timestep = 401. State = [[-0.1148195  -0.09593457]]. Action = [[-0.1685378  -0.20360075 -0.1458688  -0.89332926]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 401 is [True, False, False, False, True, False]
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.12850468 -0.10105982]]. Action = [[-0.20259146  0.23896313 -0.19981669  0.6038997 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 402 is [True, False, False, False, True, False]
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of 1
Current timestep = 403. State = [[-0.14046985 -0.08400421]]. Action = [[-0.06029761 -0.00504118 -0.1076031   0.89059985]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.15246706 -0.0747666 ]]. Action = [[-0.14548695  0.14164886 -0.06373587  0.02072859]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.1636598 -0.0630609]]. Action = [[ 0.10878909  0.02894583  0.05212608 -0.7976517 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.16858965 -0.04603252]]. Action = [[-0.18963905  0.2308029   0.21955788  0.5033027 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 406 is [True, False, False, False, True, False]
Current timestep = 407. State = [[-0.18834673 -0.03361718]]. Action = [[-0.22917223 -0.09438685 -0.12949027  0.7404808 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 407 is [True, False, False, False, True, False]
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.20887373 -0.04878309]]. Action = [[ 0.2002764  -0.22328067 -0.19381467 -0.64190996]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 408 is [True, False, False, False, True, False]
Current timestep = 409. State = [[-0.19440167 -0.05589861]]. Action = [[ 0.22599667  0.10544145 -0.2047286  -0.1921171 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 409 is [True, False, False, False, True, False]
Current timestep = 410. State = [[-0.18500343 -0.06162743]]. Action = [[-0.18631595 -0.13386992 -0.19176355  0.97926795]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 410 is [True, False, False, False, True, False]
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 410 of -1
Current timestep = 411. State = [[-0.18443029 -0.07612568]]. Action = [[ 0.16994196 -0.09581538  0.05045992  0.165416  ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 411 is [True, False, False, False, True, False]
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(6.4801e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of 1
Current timestep = 412. State = [[-0.18225487 -0.06936222]]. Action = [[-0.05397543  0.24843824 -0.1246894   0.12200224]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 412 is [True, False, False, False, True, False]
Current timestep = 413. State = [[-0.17350754 -0.04978854]]. Action = [[ 0.21884161  0.09274441 -0.05040921  0.36231446]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 413 is [True, False, False, False, True, False]
Current timestep = 414. State = [[-0.14841923 -0.02559156]]. Action = [[ 0.2454327   0.2478391   0.18821341 -0.5828453 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 414 is [True, False, False, False, True, False]
Current timestep = 415. State = [[-0.12942435  0.00148099]]. Action = [[-0.04724362  0.14946198  0.12780839 -0.8190511 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 415 is [True, False, False, False, True, False]
Current timestep = 416. State = [[-0.12457301  0.02770379]]. Action = [[0.13413322 0.19635707 0.12404329 0.52168775]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 416 is [True, False, False, False, True, False]
Current timestep = 417. State = [[-0.1144654   0.03257268]]. Action = [[-0.04382595 -0.20508613  0.20872733  0.45242727]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 417 is [True, False, False, False, True, False]
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(4.1240e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.12058724  0.03615895]]. Action = [[-0.20729874  0.2280477  -0.01369222  0.21072423]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.1228473   0.03745662]]. Action = [[ 0.20040458 -0.23640768 -0.09785622  0.9401028 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 419 is [True, False, False, False, True, False]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.12236216  0.03488214]]. Action = [[-0.14502971  0.20852157 -0.06890306 -0.8891661 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 420 is [True, False, False, False, True, False]
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(7.9126e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.12252743  0.03581133]]. Action = [[ 0.17806706 -0.2156155   0.17779744  0.12428284]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 421 is [True, False, False, False, True, False]
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 421 of 1
Current timestep = 422. State = [[-0.1033999   0.02428571]]. Action = [[ 0.24008468  0.03493243 -0.2365625   0.8823345 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.07213922  0.0365408 ]]. Action = [[ 0.1967268   0.1764785   0.21766886 -0.9045607 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 423 is [True, False, False, False, True, False]
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.04585285  0.0452274 ]]. Action = [[ 0.09095365 -0.05640578 -0.00221857  0.97312045]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 424 is [True, False, False, False, True, False]
Current timestep = 425. State = [[-0.24813177 -0.00291014]]. Action = [[0.21994185 0.1385777  0.24299127 0.5561502 ]]. Reward = [100.]
Curr episode timestep = 80
Scene graph at timestep 425 is [False, True, False, False, True, False]
Current timestep = 426. State = [[-0.2471848   0.00524483]]. Action = [[-0.02785161  0.1748963   0.18768284 -0.30909324]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 426 is [True, False, False, False, True, False]
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(4.5960e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.25207365  0.01870101]]. Action = [[-0.0852311   0.0710685   0.03238586 -0.5945706 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 427 is [True, False, False, False, True, False]
Current timestep = 428. State = [[-0.25037196  0.03214933]]. Action = [[0.13678676 0.12629497 0.13230893 0.6704087 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 428 is [True, False, False, False, True, False]
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(7.1593e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.23722515  0.04334171]]. Action = [[ 0.1548652   0.03396189 -0.06504799  0.29036283]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.21654697  0.04332203]]. Action = [[ 0.15028092 -0.07384092 -0.13395226 -0.25860536]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.19629605  0.04391727]]. Action = [[ 0.0807994   0.05437452  0.11458567 -0.9137476 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 431 is [True, False, False, False, True, False]
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.19264385  0.06026291]]. Action = [[-0.09952652  0.22752368  0.04319263  0.03235042]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(9.6688e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.18894352  0.08626787]]. Action = [[ 0.24804759  0.14948994 -0.01615427 -0.91907775]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.16125844  0.11038036]]. Action = [[0.20616314 0.22457239 0.17978233 0.60906816]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 434 is [True, False, False, False, True, False]
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.12933056  0.13140078]]. Action = [[ 0.18080586 -0.02244477  0.09303188  0.3981911 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 435 is [True, False, False, False, True, False]
Current timestep = 436. State = [[-0.1112086   0.13956219]]. Action = [[ 0.06470832  0.12894171  0.18701494 -0.42751706]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 436 is [True, False, False, False, False, True]
Current timestep = 437. State = [[-0.10509712  0.139843  ]]. Action = [[-0.16531387 -0.15693949  0.24846971  0.24086535]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 437 is [True, False, False, False, False, True]
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.10861029  0.11892897]]. Action = [[-0.1282608  -0.22887364  0.23929328  0.6293522 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 438 is [True, False, False, False, False, True]
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of 1
Current timestep = 439. State = [[-0.11423283  0.09105068]]. Action = [[-0.05145848 -0.16232486  0.07561153 -0.17126644]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 439 is [True, False, False, False, True, False]
Current timestep = 440. State = [[-0.11338775  0.06709906]]. Action = [[ 0.11172882 -0.21052094  0.09472194 -0.5254654 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 440 is [True, False, False, False, True, False]
Current timestep = 441. State = [[-0.10554051  0.05790021]]. Action = [[0.23429066 0.10563436 0.05024523 0.04500151]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 441 is [True, False, False, False, True, False]
Current timestep = 442. State = [[-0.09963584  0.04895093]]. Action = [[-0.0668709  -0.18378137 -0.19315077 -0.6397513 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 442 is [True, False, False, False, True, False]
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(7.0197e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[-0.09649601  0.04997743]]. Action = [[0.10301742 0.2488158  0.17343652 0.82509327]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 443 is [True, False, False, False, True, False]
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.08957317  0.07063203]]. Action = [[0.14643598 0.11161795 0.09036276 0.9658793 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 444 is [True, False, False, False, True, False]
Current timestep = 445. State = [[-0.08285316  0.08989599]]. Action = [[-0.2294543   0.15804487  0.2187537   0.2095449 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 445 is [True, False, False, False, True, False]
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(3.3614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.08667991  0.11921454]]. Action = [[ 0.22610432  0.24105468 -0.10693219  0.9255452 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 446 is [True, False, False, False, True, False]
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.0754372   0.13204646]]. Action = [[-0.03531918 -0.09955001 -0.01605879 -0.5954503 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 447 is [True, False, False, False, True, False]
Current timestep = 448. State = [[-0.07533436  0.12866874]]. Action = [[-0.0721662  -0.01073198 -0.08896528  0.9937775 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.07026199  0.11855271]]. Action = [[ 0.22846729 -0.1310111  -0.21937947  0.7178581 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 449 is [True, False, False, False, False, True]
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of 1
Current timestep = 450. State = [[-0.05617046  0.1141293 ]]. Action = [[ 0.07068664  0.12491387 -0.09411469  0.7607033 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 450 is [True, False, False, False, True, False]
Current timestep = 451. State = [[-0.05268027  0.11760916]]. Action = [[-0.21022886 -0.06636566 -0.14783487 -0.9030198 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 451 is [True, False, False, False, True, False]
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(8.2466e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.05429874  0.10725388]]. Action = [[-0.00441995 -0.1697793   0.0150831   0.45960736]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 452 is [True, False, False, False, True, False]
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of 1
Current timestep = 453. State = [[-0.0602194   0.09312429]]. Action = [[-0.17910303 -0.04228689 -0.0347302  -0.81292105]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 453 is [True, False, False, False, True, False]
Current timestep = 454. State = [[-0.06280916  0.08801798]]. Action = [[ 0.20669699 -0.02525672 -0.04503077 -0.400185  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 454 is [True, False, False, False, True, False]
Current timestep = 455. State = [[-0.05281599  0.07986338]]. Action = [[ 0.2117084  -0.0868592   0.06138411  0.49262   ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 455 is [True, False, False, False, True, False]
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(7.8305e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.03686539  0.07727126]]. Action = [[0.17383167 0.13413489 0.19440618 0.0305028 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 456 is [True, False, False, False, True, False]
Current timestep = 457. State = [[-0.26143992 -0.0039955 ]]. Action = [[ 0.17389667 -0.22831456  0.03376219 -0.71302235]]. Reward = [100.]
Curr episode timestep = 31
Scene graph at timestep 457 is [False, True, False, False, True, False]
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of 1
Current timestep = 458. State = [[-0.26209614 -0.00934305]]. Action = [[-0.04035784 -0.07133819  0.10765925  0.8028581 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 458 is [True, False, False, False, True, False]
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of 1
Current timestep = 459. State = [[-0.26041475 -0.01169676]]. Action = [[ 0.13951808  0.08387023 -0.20996265  0.3759756 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 459 is [True, False, False, False, True, False]
Current timestep = 460. State = [[-0.25954667 -0.00926529]]. Action = [[-0.09399123 -0.02756725  0.08133128 -0.12481946]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 460 is [True, False, False, False, True, False]
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(3.9699e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 1
Current timestep = 461. State = [[-0.25961447 -0.00926324]]. Action = [[-0.24177518 -0.17844887  0.19967797 -0.6937523 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 461 is [True, False, False, False, True, False]
Current timestep = 462. State = [[-0.25961447 -0.00926324]]. Action = [[-0.14641838  0.21920222  0.15532625 -0.91376173]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 462 is [True, False, False, False, True, False]
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of 1
Current timestep = 463. State = [[-0.26001796 -0.00919599]]. Action = [[-0.17988281 -0.17964725  0.15142295  0.17498398]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 463 is [True, False, False, False, True, False]
Current timestep = 464. State = [[-0.25476128 -0.00804167]]. Action = [[ 0.18983042  0.03824458 -0.13167895 -0.66168666]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 464 is [True, False, False, False, True, False]
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(1.7340e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of 1
Current timestep = 465. State = [[-0.24923143  0.00769929]]. Action = [[-0.00891986  0.2252948  -0.13843897  0.022928  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 465 is [True, False, False, False, True, False]
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.2493036   0.02605676]]. Action = [[-0.03292172  0.00545526  0.22295561 -0.8159666 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 466 is [True, False, False, False, True, False]
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of 1
Current timestep = 467. State = [[-0.24944852  0.02643398]]. Action = [[-0.18446076 -0.13224685  0.03720504  0.75823975]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 467 is [True, False, False, False, True, False]
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(6.8161e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 467 of 1
Current timestep = 468. State = [[-0.24616486  0.03402551]]. Action = [[0.10631415 0.117982   0.05966973 0.52789974]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 468 is [True, False, False, False, True, False]
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(5.0194e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 468 of 1
Current timestep = 469. State = [[-0.2314419   0.05153146]]. Action = [[0.21189848 0.15644968 0.05662608 0.8112738 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 469 is [True, False, False, False, True, False]
Current timestep = 470. State = [[-0.21792641  0.0518018 ]]. Action = [[-0.17104271 -0.23259486  0.03273016  0.68916607]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 470 is [True, False, False, False, True, False]
Current timestep = 471. State = [[-0.23113863  0.03728923]]. Action = [[-0.23787344 -0.0681269  -0.13962981  0.14364386]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 471 is [True, False, False, False, True, False]
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[-0.2501489   0.03817319]]. Action = [[ 0.07088989  0.18397063 -0.21987575 -0.7774183 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 472 is [True, False, False, False, True, False]
Current timestep = 473. State = [[-0.25579944  0.05933999]]. Action = [[-0.0563532   0.16256952  0.00349784  0.35334945]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 473 is [True, False, False, False, True, False]
Current timestep = 474. State = [[-0.26428044  0.08108145]]. Action = [[-0.06773344  0.13253242  0.23452544  0.87578285]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 474 is [True, False, False, False, True, False]
Current timestep = 475. State = [[-0.26982948  0.09251641]]. Action = [[-0.21769363 -0.04969169  0.11438519  0.6136098 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 475 is [True, False, False, False, True, False]
Current timestep = 476. State = [[-0.27042925  0.09359685]]. Action = [[-0.08197042 -0.02287246 -0.12592506  0.8653363 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 476 is [True, False, False, False, True, False]
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.2611344   0.09336948]]. Action = [[ 0.18726999 -0.03556657  0.12546569  0.01876044]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 477 is [True, False, False, False, True, False]
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(5.7644e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of 1
Current timestep = 478. State = [[-0.24826299  0.10394314]]. Action = [[0.02780929 0.2113229  0.08753765 0.44114804]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 478 is [True, False, False, False, True, False]
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(4.9610e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.24489748  0.12576652]]. Action = [[ 0.03348008  0.13992596  0.01298258 -0.46568763]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 479 is [True, False, False, False, True, False]
Scene graph at timestep 479 is [True, False, False, False, False, True]
State prediction error at timestep 479 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.23510161  0.12774594]]. Action = [[ 0.09305739 -0.20252728  0.09262431  0.80320525]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 480 is [True, False, False, False, False, True]
Current timestep = 481. State = [[-0.23731712  0.12896872]]. Action = [[-0.17533833  0.18413085 -0.08190346 -0.66576385]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 481 is [True, False, False, False, False, True]
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(5.3857e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.24853949  0.14880854]]. Action = [[-0.05701017  0.145168    0.15159282 -0.30212593]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 482 is [True, False, False, False, False, True]
Current timestep = 483. State = [[-0.25629732  0.15102352]]. Action = [[-0.10633039 -0.15173157  0.10383579  0.14773786]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 483 is [True, False, False, False, False, True]
Current timestep = 484. State = [[-0.2586289   0.15102908]]. Action = [[0.09439129 0.140585   0.06815937 0.7375002 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 484 is [True, False, False, False, False, True]
Current timestep = 485. State = [[-0.25369328  0.1469853 ]]. Action = [[ 0.12249374 -0.1628364   0.11216024  0.7538644 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 485 is [True, False, False, False, False, True]
Current timestep = 486. State = [[-0.24949522  0.15178367]]. Action = [[0.0152947  0.22248352 0.13020945 0.89985824]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 486 is [True, False, False, False, False, True]
Current timestep = 487. State = [[-0.23087361  0.14919752]]. Action = [[ 0.22913569 -0.23920994 -0.1508333   0.98352516]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 487 is [True, False, False, False, False, True]
Current timestep = 488. State = [[-0.21908414  0.13997006]]. Action = [[-0.15263382 -0.00424685  0.150154    0.39151573]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 488 is [True, False, False, False, False, True]
Scene graph at timestep 488 is [True, False, False, False, False, True]
State prediction error at timestep 488 is tensor(5.8886e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 1
Current timestep = 489. State = [[-0.21089402  0.12409925]]. Action = [[ 0.24220258 -0.1884755  -0.01822041  0.4358678 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 489 is [True, False, False, False, False, True]
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of 1
Current timestep = 490. State = [[-0.18569063  0.09538854]]. Action = [[ 0.21385822 -0.23900382  0.17211545 -0.946028  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 490 is [True, False, False, False, True, False]
Current timestep = 491. State = [[-0.16426381  0.07145872]]. Action = [[ 0.08679616 -0.10525006 -0.04702678 -0.10524756]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 491 is [True, False, False, False, True, False]
Current timestep = 492. State = [[-0.14409782  0.05752619]]. Action = [[ 0.2057808  -0.04970635 -0.06530219 -0.8205991 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 492 is [True, False, False, False, True, False]
Current timestep = 493. State = [[-0.13319597  0.04878233]]. Action = [[-0.24015683 -0.08678648  0.15213922  0.93165696]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 493 is [True, False, False, False, True, False]
Current timestep = 494. State = [[-0.13569863  0.03763895]]. Action = [[ 0.05122539 -0.06768796  0.22253883 -0.79297215]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 494 is [True, False, False, False, True, False]
Current timestep = 495. State = [[-0.13359497  0.02092016]]. Action = [[ 0.05037522 -0.18556422 -0.00456262  0.75816584]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 495 is [True, False, False, False, True, False]
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.13167365 -0.00514817]]. Action = [[-0.07149372 -0.14878953  0.1348334  -0.17945945]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.12832464 -0.0078887 ]]. Action = [[ 0.21232194  0.18219465 -0.12206998  0.11921811]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(1.0778e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[-0.1253206  -0.00880856]]. Action = [[-0.14777978 -0.2184617   0.03497741 -0.8103241 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 498 is [True, False, False, False, True, False]
Current timestep = 499. State = [[-0.12411807 -0.03422403]]. Action = [[ 0.08554107 -0.22186589 -0.17083062  0.53174424]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.11813194 -0.06754982]]. Action = [[ 0.06308764 -0.17040981  0.246288   -0.676715  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 500 is [True, False, False, False, True, False]
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.11919813 -0.07429102]]. Action = [[-0.14643683  0.16559163 -0.08126161  0.77874136]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.12299405 -0.06820199]]. Action = [[-0.04084027 -0.0621215   0.20683023  0.5023378 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 502 is [True, False, False, False, True, False]
Current timestep = 503. State = [[-0.12012721 -0.08255671]]. Action = [[ 0.19682223 -0.22083902  0.22933483 -0.88781977]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 503 is [True, False, False, False, True, False]
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(7.1540e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.11328343 -0.11464428]]. Action = [[ 0.08891827 -0.23594995  0.09465683  0.52330184]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 504 is [True, False, False, False, True, False]
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.09902034 -0.12391613]]. Action = [[ 0.1826306   0.1980167  -0.19588447  0.52548647]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 505 is [True, False, False, False, True, False]
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 505 of 1
Current timestep = 506. State = [[-0.07756354 -0.09858327]]. Action = [[0.08853906 0.20590383 0.08178499 0.20532835]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 506 is [True, False, False, False, True, False]
Current timestep = 507. State = [[-0.06593159 -0.08237259]]. Action = [[0.10181355 0.04617628 0.13933417 0.09750998]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 507 is [True, False, False, False, True, False]
Current timestep = 508. State = [[-0.06181645 -0.07217018]]. Action = [[-0.23839708  0.08139044 -0.07344827 -0.5764559 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 508 is [True, False, False, False, True, False]
Current timestep = 509. State = [[-0.06158653 -0.07819141]]. Action = [[ 0.23744273 -0.24254301  0.0785929  -0.6090336 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 509 is [True, False, False, False, True, False]
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.05690109 -0.08528193]]. Action = [[-0.23594178  0.15186816 -0.18020178 -0.42770982]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 510 is [True, False, False, False, True, False]
Current timestep = 511. State = [[-0.06204515 -0.06657922]]. Action = [[ 0.01507315  0.20992845  0.1371494  -0.8602773 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 511 is [True, False, False, False, True, False]
Current timestep = 512. State = [[-0.06722414 -0.03657107]]. Action = [[-0.07936913  0.19962168 -0.08877194 -0.7499034 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 512 is [True, False, False, False, True, False]
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of 1
Current timestep = 513. State = [[-0.07475075 -0.00195157]]. Action = [[-0.00980042  0.23983216  0.06182998 -0.56172496]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 513 is [True, False, False, False, True, False]
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of 1
Current timestep = 514. State = [[-0.0862857   0.03130869]]. Action = [[-0.19314745  0.20321718  0.1168763   0.02434647]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 514 is [True, False, False, False, True, False]
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.10924012  0.06613474]]. Action = [[-0.1873754   0.24140519 -0.01836933 -0.522082  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 515 is [True, False, False, False, True, False]
Current timestep = 516. State = [[-0.12728408  0.07578994]]. Action = [[-0.13752356 -0.15793741 -0.1840579   0.76101446]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 516 is [True, False, False, False, True, False]
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[-0.14605266  0.05965656]]. Action = [[-0.06213042 -0.11979058  0.02888176  0.9570074 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 517 is [True, False, False, False, True, False]
Current timestep = 518. State = [[-0.14434601  0.05269144]]. Action = [[ 0.22599012  0.01911232 -0.17913549  0.7842777 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 518 is [True, False, False, False, True, False]
Current timestep = 519. State = [[-0.14378136  0.05324575]]. Action = [[-0.19064316  0.01317692  0.07243252 -0.45353943]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 519 is [True, False, False, False, True, False]
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(3.1296e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of -1
Current timestep = 520. State = [[-0.147133    0.04533628]]. Action = [[ 0.05890116 -0.13101102  0.11653501 -0.9548885 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.1480988   0.03495193]]. Action = [[-0.04262644 -0.0334104  -0.02332433 -0.7651506 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 521 is [True, False, False, False, True, False]
Current timestep = 522. State = [[-0.15199448  0.04361278]]. Action = [[-0.04177484  0.21656936  0.14014372 -0.020702  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.15580158  0.05806369]]. Action = [[ 0.09091505  0.04720828  0.23843527 -0.9071118 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 523 is [True, False, False, False, True, False]
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.14613728  0.06865931]]. Action = [[ 0.23516917  0.10858992 -0.06408001  0.8066088 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.12384926  0.08908135]]. Action = [[0.19447383 0.22766313 0.21019524 0.9210725 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 525 is [True, False, False, False, True, False]
Current timestep = 526. State = [[-0.09665045  0.09582729]]. Action = [[ 0.2233012  -0.20225745  0.19066688  0.82992566]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.07576896  0.0910005 ]]. Action = [[0.10751086 0.06194615 0.049474   0.41546202]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 527 is [True, False, False, False, True, False]
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(6.7259e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 1
Current timestep = 528. State = [[-0.06050638  0.08746358]]. Action = [[-0.19489875 -0.1618636  -0.03186311  0.52202964]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 528 is [True, False, False, False, True, False]
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(5.2706e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of 1
Current timestep = 529. State = [[-0.06540899  0.06286663]]. Action = [[ 0.01691812 -0.20595832  0.06022277  0.60300183]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 529 is [True, False, False, False, True, False]
Current timestep = 530. State = [[-0.06612836  0.04187792]]. Action = [[-0.06352524 -0.12628202 -0.12298808 -0.07884783]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 530 is [True, False, False, False, True, False]
Current timestep = 531. State = [[-0.06447935  0.02378772]]. Action = [[ 0.15964621 -0.10205862 -0.09429544  0.5939269 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 531 is [True, False, False, False, True, False]
Current timestep = 532. State = [[-0.06204425  0.02606151]]. Action = [[-0.00869837  0.24697778 -0.00646417 -0.8459927 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 532 is [True, False, False, False, True, False]
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of 1
Current timestep = 533. State = [[-0.05592564  0.04315184]]. Action = [[ 0.23826152  0.04409119 -0.18803255 -0.4426484 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 533 is [True, False, False, False, True, False]
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of 1
Current timestep = 534. State = [[-0.17798768 -0.06940865]]. Action = [[-0.08706696 -0.18723294 -0.08838925 -0.63143957]]. Reward = [100.]
Curr episode timestep = 76
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.17289804 -0.08883169]]. Action = [[-0.22401874 -0.16489203  0.0665648  -0.40859795]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.18351491 -0.09997454]]. Action = [[-0.08904649  0.05483779  0.00721925  0.12897468]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 536 is [True, False, False, False, True, False]
Current timestep = 537. State = [[-0.19332868 -0.09026736]]. Action = [[-0.07728457  0.16681594  0.13395542  0.01649368]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 537 is [True, False, False, False, True, False]
Current timestep = 538. State = [[-0.19167444 -0.08969373]]. Action = [[ 0.2171408  -0.19890158 -0.00698981  0.7944963 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 538 is [True, False, False, False, True, False]
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of 1
Current timestep = 539. State = [[-0.1890592  -0.11266329]]. Action = [[-0.03709933 -0.21979026 -0.1279662   0.72043717]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of -1
Current timestep = 540. State = [[-0.19256736 -0.14330626]]. Action = [[-0.08602196 -0.16432944 -0.154165   -0.8838571 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 540 is [True, False, False, False, True, False]
Scene graph at timestep 540 is [True, False, False, True, False, False]
State prediction error at timestep 540 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 540 of -1
Current timestep = 541. State = [[-0.20502084 -0.15685102]]. Action = [[-0.23822853  0.04477018  0.21067595 -0.2809155 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 541 is [True, False, False, True, False, False]
Scene graph at timestep 541 is [True, False, False, True, False, False]
State prediction error at timestep 541 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[-0.2224135  -0.15622254]]. Action = [[ 0.03789756 -0.00846042  0.01667139  0.97711205]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 542 is [True, False, False, True, False, False]
Current timestep = 543. State = [[-0.22998793 -0.14360698]]. Action = [[-0.1880165   0.24183157 -0.00207652  0.27675784]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 543 is [True, False, False, True, False, False]
Scene graph at timestep 543 is [True, False, False, True, False, False]
State prediction error at timestep 543 is tensor(3.5082e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.25040668 -0.11280835]]. Action = [[-0.12700793  0.230634   -0.2476044   0.68474615]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 544 is [True, False, False, True, False, False]
Current timestep = 545. State = [[-0.2578261  -0.08625845]]. Action = [[-0.01706204  0.13297823 -0.1755028  -0.1999504 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 545 is [True, False, False, False, True, False]
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of -1
Current timestep = 546. State = [[-0.26074314 -0.07083615]]. Action = [[-0.22308826  0.04660681  0.05706     0.7430639 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.25679216 -0.06881065]]. Action = [[ 0.16206679  0.03424275  0.01110038 -0.66177124]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 547 is [True, False, False, False, True, False]
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(3.4641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.2538661  -0.06622053]]. Action = [[-0.23696592 -0.09261455  0.10955685 -0.0731371 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.25436306 -0.0663557 ]]. Action = [[-0.09363574 -0.01721656 -0.13388735 -0.3602559 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 549 is [True, False, False, False, True, False]
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(4.6311e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.25498572 -0.06690551]]. Action = [[-0.1392261  -0.24269584  0.16668016 -0.8961958 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 550 is [True, False, False, False, True, False]
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(1.5660e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of -1
Current timestep = 551. State = [[-0.25521043 -0.06689013]]. Action = [[-0.20100895  0.14517608 -0.07482034 -0.7670397 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 551 is [True, False, False, False, True, False]
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(5.1257e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of -1
Current timestep = 552. State = [[-0.25307316 -0.07760529]]. Action = [[ 0.10086879 -0.1879887   0.12975392  0.7724016 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 552 is [True, False, False, False, True, False]
Current timestep = 553. State = [[-0.25504258 -0.07668526]]. Action = [[-0.12143338  0.19419175 -0.22627829  0.2220658 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 553 is [True, False, False, False, True, False]
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(5.1960e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of -1
Current timestep = 554. State = [[-0.25762966 -0.06316974]]. Action = [[0.06196308 0.07517877 0.08907574 0.66154146]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.2582065  -0.05857199]]. Action = [[-0.21208979 -0.19903941 -0.08880301 -0.20637512]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 555 is [True, False, False, False, True, False]
Current timestep = 556. State = [[-0.24976504 -0.0557448 ]]. Action = [[0.19354156 0.02350855 0.18445975 0.10173905]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.2327149  -0.04838309]]. Action = [[ 0.1982665   0.06121188 -0.08008835  0.51820135]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.22041112 -0.05649725]]. Action = [[-0.15694351 -0.21491373 -0.04951183  0.9037986 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 558 is [True, False, False, False, True, False]
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of 1
Current timestep = 559. State = [[-0.22202057 -0.07890864]]. Action = [[ 0.07759786 -0.16086376 -0.12486437 -0.91243285]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 559 is [True, False, False, False, True, False]
Current timestep = 560. State = [[-0.22348565 -0.08886697]]. Action = [[-0.13795504  0.01274195  0.19374862  0.3310777 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 560 is [True, False, False, False, True, False]
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of -1
Current timestep = 561. State = [[-0.22075908 -0.0868859 ]]. Action = [[ 0.22637355  0.11269459  0.02133515 -0.77447873]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.2071717  -0.06511737]]. Action = [[ 0.13715485  0.23416379 -0.15542729  0.9018431 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 562 is [True, False, False, False, True, False]
Current timestep = 563. State = [[-0.18527338 -0.03538597]]. Action = [[ 0.19592631  0.21990836 -0.03164184 -0.80910844]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 563 is [True, False, False, False, True, False]
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of 1
Current timestep = 564. State = [[-0.16951334 -0.00178316]]. Action = [[-0.1383715   0.19682062 -0.14465857  0.91888785]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 564 is [True, False, False, False, True, False]
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.1662094   0.00872872]]. Action = [[ 0.24083239 -0.13747118  0.04399246  0.3398881 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 565 is [True, False, False, False, True, False]
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 565 of 1
Current timestep = 566. State = [[-0.14051607 -0.00100495]]. Action = [[ 0.20710513 -0.03728938  0.13581297 -0.00605303]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 566 is [True, False, False, False, True, False]
Current timestep = 567. State = [[-0.11702406 -0.00026413]]. Action = [[0.12215835 0.06898379 0.22701311 0.01284325]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.0944406  -0.00135306]]. Action = [[ 0.21229467 -0.06796086  0.03519768 -0.51514447]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 568 is [True, False, False, False, True, False]
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 568 of 1
Current timestep = 569. State = [[-0.07346233  0.005325  ]]. Action = [[-0.12124009  0.14828739  0.07706124 -0.3304261 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.07654218  0.0159134 ]]. Action = [[ 0.04573441  0.05950084 -0.1742138   0.5886977 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 570 is [True, False, False, False, True, False]
Current timestep = 571. State = [[-0.07245624  0.02799783]]. Action = [[0.15444654 0.12839288 0.07066414 0.8309231 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 571 is [True, False, False, False, True, False]
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.05292605  0.05357281]]. Action = [[ 0.18535596  0.24843186  0.0303103  -0.97003466]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 572 is [True, False, False, False, True, False]
Current timestep = 573. State = [[-0.0421596   0.07284789]]. Action = [[-0.17220731 -0.00401635  0.22281784  0.98247576]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 573 is [True, False, False, False, True, False]
Scene graph at timestep 573 is [False, True, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of 1
Current timestep = 574. State = [[-0.03672218  0.0637229 ]]. Action = [[ 0.2319785  -0.22105972 -0.1577856  -0.7244895 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 574 is [False, True, False, False, True, False]
Current timestep = 575. State = [[-0.1888363   0.01567892]]. Action = [[ 0.0252803   0.04224125 -0.22021145 -0.9519237 ]]. Reward = [100.]
Curr episode timestep = 40
Scene graph at timestep 575 is [False, True, False, False, True, False]
Current timestep = 576. State = [[-0.16694172  0.01820338]]. Action = [[ 0.23142695 -0.0036654   0.02625754  0.36636138]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 576 is [True, False, False, False, True, False]
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(7.9433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.14376526  0.01003227]]. Action = [[ 0.02523679 -0.17907093  0.04538772 -0.79268926]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.14199537 -0.01309047]]. Action = [[-0.01377492 -0.24112372  0.04316226  0.26490724]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 578 is [True, False, False, False, True, False]
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 578 of 1
Current timestep = 579. State = [[-0.14056662 -0.02528405]]. Action = [[ 0.06409925  0.16798002  0.18815076 -0.36846477]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.12741819 -0.03132157]]. Action = [[ 0.20697105 -0.23274828 -0.11127698 -0.1959145 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 580 is [True, False, False, False, True, False]
Current timestep = 581. State = [[-0.10045923 -0.04982935]]. Action = [[ 0.1037291  -0.12215975 -0.03596228 -0.3558557 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 581 is [True, False, False, False, True, False]
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 581 of 1
Current timestep = 582. State = [[-0.08794274 -0.04917677]]. Action = [[-0.03785914  0.23423705  0.17211744  0.4702747 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.08066078 -0.04938399]]. Action = [[ 0.1931014  -0.21307684 -0.00453427  0.29511058]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 583 is [True, False, False, False, True, False]
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(3.2572e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of 1
Current timestep = 584. State = [[-0.05337985 -0.07387945]]. Action = [[ 0.20382452 -0.23615032 -0.19306478 -0.64006954]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 584 is [True, False, False, False, True, False]
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of 1
Current timestep = 585. State = [[-0.02983591 -0.10620104]]. Action = [[ 0.12260735 -0.20340376 -0.21795636 -0.22963476]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 585 is [True, False, False, False, True, False]
Current timestep = 586. State = [[-0.01914679 -0.13204643]]. Action = [[-0.01857033 -0.16187061  0.14963418  0.59696853]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 586 is [False, True, False, False, True, False]
Scene graph at timestep 586 is [False, True, False, True, False, False]
State prediction error at timestep 586 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.00939622 -0.13965996]]. Action = [[ 0.21457618  0.13414204  0.22079036 -0.6496815 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 587 is [False, True, False, True, False, False]
Current timestep = 588. State = [[ 0.01536648 -0.14300402]]. Action = [[ 0.15390241 -0.14518495 -0.13852808  0.8126478 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 588 is [False, True, False, True, False, False]
Scene graph at timestep 588 is [False, True, False, True, False, False]
State prediction error at timestep 588 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[ 0.04059587 -0.15130147]]. Action = [[ 0.2350226   0.21127939 -0.11788853 -0.07440782]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 589 is [False, True, False, True, False, False]
Current timestep = 590. State = [[ 0.04223014 -0.14354968]]. Action = [[ 0.06469029  0.14020032  0.00516188 -0.8722863 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 590 is [False, True, False, True, False, False]
Scene graph at timestep 590 is [False, True, False, True, False, False]
State prediction error at timestep 590 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of -1
Current timestep = 591. State = [[ 0.04128246 -0.14523166]]. Action = [[-0.18265085 -0.14125416  0.23791948 -0.8690488 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 591 is [False, True, False, True, False, False]
Scene graph at timestep 591 is [False, True, False, True, False, False]
State prediction error at timestep 591 is tensor(4.1471e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[ 0.03869812 -0.15295206]]. Action = [[ 0.17679304 -0.01210894  0.10030007  0.4919281 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 592 is [False, True, False, True, False, False]
Current timestep = 593. State = [[ 0.03869436 -0.15315847]]. Action = [[ 0.21327749  0.18268555 -0.07224151  0.7105386 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 593 is [False, True, False, True, False, False]
Current timestep = 594. State = [[ 0.03022587 -0.16891423]]. Action = [[-0.21530941 -0.22271894 -0.01661745  0.47590458]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 594 is [False, True, False, True, False, False]
Scene graph at timestep 594 is [False, True, False, True, False, False]
State prediction error at timestep 594 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 594 of -1
Current timestep = 595. State = [[ 0.01505479 -0.1776667 ]]. Action = [[-0.16181761  0.21006304  0.10306123 -0.9333096 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 595 is [False, True, False, True, False, False]
Current timestep = 596. State = [[-0.00492075 -0.15587406]]. Action = [[-0.2009208   0.12942284 -0.04942253  0.7532122 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 596 is [False, True, False, True, False, False]
Current timestep = 597. State = [[-0.01825095 -0.13797425]]. Action = [[ 0.15158552  0.09119004 -0.14579074  0.84322286]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 597 is [False, True, False, True, False, False]
Current timestep = 598. State = [[-0.01635625 -0.11894072]]. Action = [[ 0.04886419  0.17815608 -0.21330616 -0.2491355 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 598 is [False, True, False, True, False, False]
Scene graph at timestep 598 is [False, True, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of 1
Current timestep = 599. State = [[-0.22679631 -0.17496467]]. Action = [[-0.13693997  0.13098842 -0.1610958  -0.57466424]]. Reward = [100.]
Curr episode timestep = 23
Scene graph at timestep 599 is [False, True, False, False, True, False]
Current timestep = 600. State = [[-0.22133823 -0.18986805]]. Action = [[-0.05552675  0.12423253 -0.2145617   0.5783572 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 600 is [True, False, False, True, False, False]
Scene graph at timestep 600 is [True, False, False, True, False, False]
State prediction error at timestep 600 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.21296892 -0.17190063]]. Action = [[ 0.20318955  0.21325952 -0.01603128 -0.81505036]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 601 is [True, False, False, True, False, False]
Scene graph at timestep 601 is [True, False, False, True, False, False]
State prediction error at timestep 601 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 601 of 1
Current timestep = 602. State = [[-0.20552716 -0.14954983]]. Action = [[-0.14416523  0.08162785 -0.21077761 -0.56986535]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 602 is [True, False, False, True, False, False]
Current timestep = 603. State = [[-0.20382059 -0.15411413]]. Action = [[ 0.16184121 -0.20154391  0.02769613 -0.61711365]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 603 is [True, False, False, True, False, False]
Scene graph at timestep 603 is [True, False, False, True, False, False]
State prediction error at timestep 603 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.19790228 -0.15681961]]. Action = [[-0.08887082  0.16392466  0.00993857 -0.45073283]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 604 is [True, False, False, True, False, False]
Current timestep = 605. State = [[-0.20006692 -0.1373805 ]]. Action = [[-0.04465637  0.21308279  0.2445522  -0.60266054]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 605 is [True, False, False, True, False, False]
Current timestep = 606. State = [[-0.19880258 -0.11510346]]. Action = [[0.14897007 0.087475   0.14365256 0.6039159 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 606 is [True, False, False, True, False, False]
Current timestep = 607. State = [[-0.19947729 -0.10251979]]. Action = [[-0.12381831  0.05683988 -0.21039453  0.97437775]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 607 is [True, False, False, False, True, False]
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(2.4979e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of 1
Current timestep = 608. State = [[-0.19778746 -0.10253853]]. Action = [[ 0.13706753 -0.13428296 -0.05048528 -0.35717088]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 608 is [True, False, False, False, True, False]
Current timestep = 609. State = [[-0.18880485 -0.09755167]]. Action = [[0.13925833 0.17421636 0.0970256  0.5948477 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 609 is [True, False, False, False, True, False]
Current timestep = 610. State = [[-0.17947811 -0.08039889]]. Action = [[-0.06006932  0.16846126  0.18427345  0.9560654 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 610 is [True, False, False, False, True, False]
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of 1
Current timestep = 611. State = [[-0.17801169 -0.05020969]]. Action = [[ 0.07665554  0.23542094 -0.13425174  0.41470122]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 611 is [True, False, False, False, True, False]
Current timestep = 612. State = [[-0.17109849 -0.04483679]]. Action = [[ 0.07536182 -0.20466931  0.10913908  0.99048996]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.16395846 -0.04311562]]. Action = [[ 2.0617247e-04  1.6195002e-01 -7.2828174e-02  9.6689939e-01]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.16080856 -0.04013626]]. Action = [[ 0.02141643 -0.07305333 -0.14685705 -0.71276873]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.14982855 -0.05312876]]. Action = [[ 0.15898687 -0.18884431 -0.17954944 -0.07427043]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 615 is [True, False, False, False, True, False]
Current timestep = 616. State = [[-0.13154028 -0.07497426]]. Action = [[ 0.11398122 -0.16930996 -0.13092199  0.03683805]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.12440927 -0.08782476]]. Action = [[-0.15642072  0.02706429 -0.16899553  0.19119167]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 617 is [True, False, False, False, True, False]
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.12775573 -0.0837858 ]]. Action = [[ 0.05244026  0.14892781 -0.09390685  0.5100877 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.13365914 -0.07933789]]. Action = [[-0.21395735 -0.06561875 -0.12857352  0.10551775]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 619 is [True, False, False, False, True, False]
Current timestep = 620. State = [[-0.13768703 -0.07586139]]. Action = [[ 0.14183137  0.09341896  0.2350257  -0.28053892]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 620 is [True, False, False, False, True, False]
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(3.7523e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of 1
Current timestep = 621. State = [[-0.12654103 -0.07188454]]. Action = [[ 0.24011344 -0.06317429 -0.19071348 -0.5490733 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 621 is [True, False, False, False, True, False]
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(1.6757e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of 1
Current timestep = 622. State = [[-0.11420923 -0.06270153]]. Action = [[-0.22599737  0.21049547  0.03670523 -0.94459337]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 622 is [True, False, False, False, True, False]
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.1180009  -0.05999505]]. Action = [[ 0.1555348  -0.23291707 -0.20308064 -0.660536  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 623 is [True, False, False, False, True, False]
Current timestep = 624. State = [[-0.10755102 -0.07006361]]. Action = [[ 0.20582134  0.03671181  0.05961189 -0.94498134]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 624 is [True, False, False, False, True, False]
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.0843252  -0.06618784]]. Action = [[ 0.09989136  0.0906457  -0.09145141  0.7985568 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 625 is [True, False, False, False, True, False]
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(2.7468e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.07712629 -0.07175823]]. Action = [[-0.1901455  -0.15470727 -0.20243108 -0.11335742]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 626 is [True, False, False, False, True, False]
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(5.7859e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.08940627 -0.09171534]]. Action = [[-0.1765266  -0.13230704  0.03604361 -0.9443827 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 627 is [True, False, False, False, True, False]
Current timestep = 628. State = [[-0.09750485 -0.0984776 ]]. Action = [[ 0.11592937  0.03980735 -0.24035716 -0.5162784 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 628 is [True, False, False, False, True, False]
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[-0.09778538 -0.10971408]]. Action = [[-0.0420436  -0.20535195  0.17572248 -0.03395128]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 629 is [True, False, False, False, True, False]
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.09763966 -0.13362172]]. Action = [[ 0.10366496 -0.19703498 -0.15160742 -0.9572744 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 630 is [True, False, False, False, True, False]
Scene graph at timestep 630 is [True, False, False, True, False, False]
State prediction error at timestep 630 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[-0.08630001 -0.14993817]]. Action = [[ 0.24391288 -0.01060073  0.11622736 -0.7676513 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 631 is [True, False, False, True, False, False]
Current timestep = 632. State = [[-0.06765783 -0.14878134]]. Action = [[ 0.00815153  0.07305053 -0.215947   -0.53552955]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 632 is [True, False, False, True, False, False]
Current timestep = 633. State = [[-0.05490284 -0.15753582]]. Action = [[ 0.22972447 -0.2176411   0.1021581  -0.732444  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 633 is [True, False, False, True, False, False]
Scene graph at timestep 633 is [True, False, False, True, False, False]
State prediction error at timestep 633 is tensor(8.3743e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.02327955 -0.1610701 ]]. Action = [[ 0.14889142  0.20781094 -0.03862441  0.7008196 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 634 is [True, False, False, True, False, False]
Scene graph at timestep 634 is [False, True, False, True, False, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.00186643 -0.13846228]]. Action = [[ 0.17579997  0.15966076  0.03068361 -0.7150688 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 635 is [False, True, False, True, False, False]
Current timestep = 636. State = [[ 0.02030118 -0.12040094]]. Action = [[ 0.10936165  0.13023627 -0.03867111 -0.43752253]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 636 is [False, True, False, True, False, False]
Scene graph at timestep 636 is [False, True, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[ 0.03597191 -0.11948077]]. Action = [[-0.14656164 -0.19279379  0.00895387  0.4516182 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 637 is [False, True, False, False, True, False]
Scene graph at timestep 637 is [False, True, False, False, True, False]
State prediction error at timestep 637 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[ 0.02773496 -0.1309185 ]]. Action = [[-0.1880239   0.05777493  0.03843743 -0.6831229 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 638 is [False, True, False, False, True, False]
Scene graph at timestep 638 is [False, True, False, True, False, False]
State prediction error at timestep 638 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 638 of 1
Current timestep = 639. State = [[ 0.01100053 -0.14143823]]. Action = [[-0.18967502 -0.18321311  0.18466878  0.44998598]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 639 is [False, True, False, True, False, False]
Scene graph at timestep 639 is [False, True, False, True, False, False]
State prediction error at timestep 639 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.01186931 -0.1450423 ]]. Action = [[-0.17929399  0.17639339  0.05032155 -0.94449216]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 640 is [False, True, False, True, False, False]
Current timestep = 641. State = [[-0.03922741 -0.11889155]]. Action = [[-0.24584797  0.24296752 -0.00241689  0.7059262 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 641 is [False, True, False, True, False, False]
Current timestep = 642. State = [[-0.07248576 -0.099075  ]]. Action = [[-0.20686431 -0.01907581 -0.21399716 -0.8662961 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 642 is [False, True, False, False, True, False]
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[-0.10352214 -0.09273513]]. Action = [[-0.06654239  0.05729866  0.02199492 -0.18370265]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 643 is [True, False, False, False, True, False]
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of -1
Current timestep = 644. State = [[-0.11150271 -0.09805491]]. Action = [[ 0.06353235 -0.18267661 -0.20210704 -0.14141935]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-0.11292125 -0.10221219]]. Action = [[-0.0934442   0.09499794 -0.13710546  0.11251724]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 645 is [True, False, False, False, True, False]
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(8.2748e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[-0.11765385 -0.10787486]]. Action = [[-0.06486438 -0.11870676 -0.18433057  0.74285865]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 646 is [True, False, False, False, True, False]
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[-0.13894458 -0.11702575]]. Action = [[-0.23637487  0.01232728 -0.13051248 -0.1915794 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 647 is [True, False, False, False, True, False]
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.16859795 -0.10727303]]. Action = [[-0.18254296  0.17646718  0.15582353  0.6407769 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 648 is [True, False, False, False, True, False]
Current timestep = 649. State = [[-0.19033647 -0.09133498]]. Action = [[-0.05298442  0.03142127 -0.18373175 -0.6456956 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 649 is [True, False, False, False, True, False]
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.20051368 -0.07862701]]. Action = [[0.00700933 0.12951875 0.10408449 0.9576683 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 650 is [True, False, False, False, True, False]
Current timestep = 651. State = [[-0.19922304 -0.05732516]]. Action = [[ 0.12347156  0.17441067 -0.15498048  0.71384597]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 651 is [True, False, False, False, True, False]
Current timestep = 652. State = [[-0.19987552 -0.04011555]]. Action = [[-0.09110756  0.04997769 -0.21813531 -0.03249907]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 652 is [True, False, False, False, True, False]
Current timestep = 653. State = [[-0.20482014 -0.02168668]]. Action = [[-0.08822     0.21346438 -0.18167159  0.40506816]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 653 is [True, False, False, False, True, False]
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(9.7479e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.21478844  0.0103001 ]]. Action = [[-0.08372386  0.16232863  0.10865098  0.70132494]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 654 is [True, False, False, False, True, False]
Current timestep = 655. State = [[-0.2244878   0.01829872]]. Action = [[-0.10705826 -0.07248929  0.12925282  0.9569378 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 655 is [True, False, False, False, True, False]
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of -1
Current timestep = 656. State = [[-0.23239563  0.00938159]]. Action = [[ 0.15317374 -0.1064018   0.01667279 -0.7102964 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 656 is [True, False, False, False, True, False]
Current timestep = 657. State = [[-0.22959688  0.01065301]]. Action = [[ 0.00474834  0.14789009 -0.24489985  0.5487031 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.23634967  0.02600712]]. Action = [[-0.18902983  0.13950303 -0.02967256 -0.38009953]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of -1
Current timestep = 659. State = [[-0.238359    0.04378337]]. Action = [[ 0.24318239  0.08086613 -0.20081079  0.06544113]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 659 is [True, False, False, False, True, False]
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.2214173   0.05415689]]. Action = [[ 0.1350525   0.10640779 -0.08476782  0.31483948]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 660 is [True, False, False, False, True, False]
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of -1
Current timestep = 661. State = [[-0.21882088  0.07262254]]. Action = [[-0.15404539  0.11784333 -0.13145562 -0.88102233]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 661 is [True, False, False, False, True, False]
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(9.7149e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of -1
Current timestep = 662. State = [[-0.2281799   0.09438767]]. Action = [[-0.03602763  0.19667763  0.14401457 -0.7587712 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 662 is [True, False, False, False, True, False]
Current timestep = 663. State = [[-0.23098572  0.09671179]]. Action = [[ 0.02321067 -0.22068211  0.10835803 -0.641013  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 663 is [True, False, False, False, True, False]
Current timestep = 664. State = [[-0.23101151  0.07714663]]. Action = [[-0.04704542 -0.18366218 -0.11414137 -0.4560495 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 664 is [True, False, False, False, True, False]
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of 1
Current timestep = 665. State = [[-0.2317288   0.06256361]]. Action = [[0.01582226 0.00903574 0.19808197 0.912622  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 665 is [True, False, False, False, True, False]
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.22328374  0.07350113]]. Action = [[ 0.22915524  0.23699251  0.24127758 -0.66579545]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 666 is [True, False, False, False, True, False]
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of 1
Current timestep = 667. State = [[-0.20917755  0.08840364]]. Action = [[-0.10949871 -0.04632781 -0.03806846  0.792423  ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 667 is [True, False, False, False, True, False]
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(7.1309e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.21867485  0.08762205]]. Action = [[-0.21089095 -0.01249777 -0.02043241 -0.94249296]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.22407532  0.07197194]]. Action = [[ 0.2006371  -0.2344341   0.04954016 -0.11762995]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.21995755  0.06733436]]. Action = [[ 0.0374395   0.20219302 -0.10592109 -0.9058953 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 670 is [True, False, False, False, True, False]
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(5.4675e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.21276014  0.06234796]]. Action = [[ 0.12650925 -0.24260469 -0.00104234  0.54925966]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.19253941  0.04847583]]. Action = [[ 0.2398262   0.01049343  0.0545463  -0.35844028]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 672 is [True, False, False, False, True, False]
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 672 of 1
Current timestep = 673. State = [[-0.16702338  0.04111999]]. Action = [[ 0.04485568 -0.11592132 -0.11086722 -0.26791072]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 673 is [True, False, False, False, True, False]
Current timestep = 674. State = [[-0.1690248   0.03994195]]. Action = [[-0.22288948  0.11626977  0.03596923  0.50694275]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.17218988  0.05524966]]. Action = [[0.11811027 0.18677717 0.09527537 0.3544488 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.17130066  0.07739121]]. Action = [[ 0.0820525   0.18620035 -0.128003    0.6893616 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.16163072  0.0983    ]]. Action = [[ 0.05769539  0.09506717 -0.16714483 -0.14196962]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 677 is [True, False, False, False, True, False]
Current timestep = 678. State = [[-0.15833963  0.11908252]]. Action = [[-0.0190659   0.1955992   0.1277957  -0.44179463]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 678 is [True, False, False, False, True, False]
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(9.4849e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[-0.16273716  0.14760897]]. Action = [[-0.11462756  0.16456717  0.24129191  0.8738512 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 679 is [True, False, False, False, True, False]
Current timestep = 680. State = [[-0.16587375  0.17339396]]. Action = [[0.13912284 0.2366423  0.20148566 0.5742719 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 680 is [True, False, False, False, False, True]
Scene graph at timestep 680 is [True, False, False, False, False, True]
State prediction error at timestep 680 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 680 of 1
Current timestep = 681. State = [[-0.14664128  0.18478249]]. Action = [[ 0.15242663 -0.22346537  0.1354855   0.77310514]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 681 is [True, False, False, False, False, True]
Scene graph at timestep 681 is [True, False, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.14100362  0.1773288 ]]. Action = [[-0.17820749  0.08874255 -0.1884129   0.36988854]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 682 is [True, False, False, False, False, True]
Current timestep = 683. State = [[-0.14802335  0.18098038]]. Action = [[-0.12229969 -0.04900426 -0.01553909  0.2830764 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 683 is [True, False, False, False, False, True]
Current timestep = 684. State = [[-0.15079117  0.1825858 ]]. Action = [[ 0.14378273  0.09430987 -0.16295339 -0.75912523]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 684 is [True, False, False, False, False, True]
Current timestep = 685. State = [[-0.14210245  0.18929426]]. Action = [[ 0.22881857  0.08262217 -0.1645786  -0.44041258]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 685 is [True, False, False, False, False, True]
Current timestep = 686. State = [[-0.1191202   0.19167623]]. Action = [[ 0.19301125 -0.1089934  -0.0220333  -0.75373703]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 686 is [True, False, False, False, False, True]
Scene graph at timestep 686 is [True, False, False, False, False, True]
State prediction error at timestep 686 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.09857274  0.17822228]]. Action = [[-0.17351118 -0.169375    0.16745824 -0.75708884]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 687 is [True, False, False, False, False, True]
Scene graph at timestep 687 is [True, False, False, False, False, True]
State prediction error at timestep 687 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 687 of 1
Current timestep = 688. State = [[-0.0927966   0.15429032]]. Action = [[ 0.20473158 -0.20318687  0.17755795  0.60613465]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 688 is [True, False, False, False, False, True]
Current timestep = 689. State = [[-0.07800374  0.12706333]]. Action = [[ 0.18150112 -0.1821163   0.2216813  -0.21157694]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 689 is [True, False, False, False, False, True]
Current timestep = 690. State = [[-0.06768674  0.11062992]]. Action = [[-0.05720395  0.00449714  0.11370599  0.05016506]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 690 is [True, False, False, False, False, True]
Scene graph at timestep 690 is [True, False, False, False, True, False]
State prediction error at timestep 690 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of 1
Current timestep = 691. State = [[-0.05971764  0.10694489]]. Action = [[ 0.22638226 -0.0011602  -0.21879204  0.18346977]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 691 is [True, False, False, False, True, False]
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.16474801 -0.04094042]]. Action = [[ 0.15370977  0.17013246  0.01919985 -0.8701046 ]]. Reward = [100.]
Curr episode timestep = 92
Scene graph at timestep 692 is [True, False, False, False, True, False]
Scene graph at timestep 692 is [True, False, False, False, True, False]
State prediction error at timestep 692 is tensor(0.0181, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 692 of 1
Current timestep = 693. State = [[-0.14977048 -0.03957969]]. Action = [[-0.01406626  0.13561288 -0.12370317 -0.5960083 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.15292391 -0.04406495]]. Action = [[-0.17416918 -0.17125347 -0.01768826  0.11660576]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 694 is [True, False, False, False, True, False]
Scene graph at timestep 694 is [True, False, False, False, True, False]
State prediction error at timestep 694 is tensor(9.9507e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 694 of -1
Current timestep = 695. State = [[-0.15909843 -0.04757231]]. Action = [[ 0.03483051  0.1214534  -0.19455357 -0.8162326 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 695 is [True, False, False, False, True, False]
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(9.1491e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of -1
Current timestep = 696. State = [[-0.15412065 -0.0424125 ]]. Action = [[ 0.18511838 -0.05315264 -0.07183868 -0.34625518]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.15166232 -0.03837333]]. Action = [[-0.14784063  0.10924995  0.0119085   0.52552223]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 697 is [True, False, False, False, True, False]
Scene graph at timestep 697 is [True, False, False, False, True, False]
State prediction error at timestep 697 is tensor(4.4738e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 697 of 1
Current timestep = 698. State = [[-0.14767432 -0.02331259]]. Action = [[ 0.22609341  0.15605801 -0.16232125 -0.16059983]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 698 is [True, False, False, False, True, False]
Scene graph at timestep 698 is [True, False, False, False, True, False]
State prediction error at timestep 698 is tensor(6.2314e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[-0.1344883  -0.02369023]]. Action = [[-0.12726086 -0.23554076 -0.10672301 -0.8674678 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 699 is [True, False, False, False, True, False]
Current timestep = 700. State = [[-0.14096758 -0.0515709 ]]. Action = [[-0.10314898 -0.23432712 -0.23143373  0.7699324 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.1572404  -0.05923987]]. Action = [[-0.21999921  0.18127742 -0.13797303 -0.6298312 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.16919352 -0.05188506]]. Action = [[ 0.10993183  0.02742699 -0.16150242 -0.20262939]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.16201037 -0.03523976]]. Action = [[0.20308399 0.23952699 0.11113679 0.5252483 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-1.5617537e-01  7.9399433e-05]]. Action = [[-0.18688336  0.1884386   0.17690086 -0.6917984 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 704 is [True, False, False, False, True, False]
Current timestep = 705. State = [[-0.16523047  0.02692929]]. Action = [[-0.08379941  0.22043082 -0.17956664 -0.25473773]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 705 is [True, False, False, False, True, False]
Current timestep = 706. State = [[-0.17511818  0.0329019 ]]. Action = [[-0.09056279 -0.2037794  -0.02218235  0.4636941 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 706 is [True, False, False, False, True, False]
Current timestep = 707. State = [[-0.1871812   0.03472253]]. Action = [[-0.14647688  0.18933457 -0.2048717  -0.19739074]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.20000646  0.04859129]]. Action = [[ 0.08495507  0.05377457 -0.17633267  0.21795583]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 708 is [True, False, False, False, True, False]
Current timestep = 709. State = [[-0.2052104   0.06023801]]. Action = [[-0.13094844  0.09479186 -0.15673822  0.16890514]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 709 is [True, False, False, False, True, False]
Current timestep = 710. State = [[-0.21147336  0.05799086]]. Action = [[-0.02489975 -0.18651693 -0.0352325  -0.97275335]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 710 is [True, False, False, False, True, False]
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(3.2580e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of -1
Current timestep = 711. State = [[-0.22356856  0.03205032]]. Action = [[-0.20153105 -0.19958887 -0.14083262  0.15621221]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 711 is [True, False, False, False, True, False]
Scene graph at timestep 711 is [True, False, False, False, True, False]
State prediction error at timestep 711 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 711 of -1
Current timestep = 712. State = [[-0.24134931  0.01664215]]. Action = [[0.14089334 0.02098614 0.14594424 0.5407038 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 712 is [True, False, False, False, True, False]
Current timestep = 713. State = [[-0.23014547  0.00956538]]. Action = [[ 0.16268265 -0.15840994 -0.07555841 -0.8765607 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 713 is [True, False, False, False, True, False]
Current timestep = 714. State = [[-0.21281584 -0.01181609]]. Action = [[ 0.15423301 -0.17374167  0.08474261 -0.85261756]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 714 is [True, False, False, False, True, False]
Current timestep = 715. State = [[-0.19341189 -0.02757001]]. Action = [[ 0.1557383  -0.00792938  0.17190611 -0.06872988]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 715 is [True, False, False, False, True, False]
Scene graph at timestep 715 is [True, False, False, False, True, False]
State prediction error at timestep 715 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of 1
Current timestep = 716. State = [[-0.16724588 -0.02880903]]. Action = [[ 0.24015683  0.05506802 -0.10331738 -0.6673135 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.14217742 -0.03784204]]. Action = [[-0.05268615 -0.2048081  -0.16283411  0.21453631]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 717 is [True, False, False, False, True, False]
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.13973637 -0.03844837]]. Action = [[ 0.06643575  0.24043864  0.20678142 -0.19701326]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 718 is [True, False, False, False, True, False]
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(9.6347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.13058636 -0.01234714]]. Action = [[ 1.8207425e-01  1.8190324e-01 -9.8407269e-05 -7.7013171e-01]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 719 is [True, False, False, False, True, False]
Current timestep = 720. State = [[-0.11464168 -0.00395953]]. Action = [[ 0.04411292 -0.08204579  0.16095471 -0.976273  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 720 is [True, False, False, False, True, False]
Current timestep = 721. State = [[-1.0751359e-01  7.8201074e-06]]. Action = [[0.05985448 0.10203522 0.23129624 0.04381645]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 721 is [True, False, False, False, True, False]
Scene graph at timestep 721 is [True, False, False, False, True, False]
State prediction error at timestep 721 is tensor(2.9229e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 721 of 1
Current timestep = 722. State = [[-0.0986472   0.00831316]]. Action = [[-0.08172378  0.03570187 -0.16456804 -0.9424701 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 722 is [True, False, False, False, True, False]
Current timestep = 723. State = [[-0.09859876  0.01222158]]. Action = [[ 0.08736575  0.03760374 -0.23926164 -0.5675571 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 723 is [True, False, False, False, True, False]
Scene graph at timestep 723 is [True, False, False, False, True, False]
State prediction error at timestep 723 is tensor(3.4935e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.09314652  0.01526859]]. Action = [[ 0.11878505 -0.03569457  0.04623961 -0.573863  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 724 is [True, False, False, False, True, False]
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(1.2797e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of 1
Current timestep = 725. State = [[-0.07738955  0.02566575]]. Action = [[ 0.08077794  0.1814403  -0.03699687 -0.41493   ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 725 is [True, False, False, False, True, False]
Scene graph at timestep 725 is [True, False, False, False, True, False]
State prediction error at timestep 725 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[-0.05850648  0.0415097 ]]. Action = [[ 0.2135556   0.08171442 -0.06528607 -0.28054392]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.15381277 -0.01932106]]. Action = [[ 0.15498918 -0.0757542  -0.18527173  0.2522552 ]]. Reward = [100.]
Curr episode timestep = 34
Scene graph at timestep 727 is [True, False, False, False, True, False]
Current timestep = 728. State = [[-0.13019451 -0.03035846]]. Action = [[ 0.17397955 -0.12188441 -0.09619628  0.5249405 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 728 is [True, False, False, False, True, False]
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.10570528 -0.03804819]]. Action = [[0.11534232 0.04068863 0.13113874 0.84152126]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 729 is [True, False, False, False, True, False]
Scene graph at timestep 729 is [True, False, False, False, True, False]
State prediction error at timestep 729 is tensor(5.9446e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.0969602  -0.04705581]]. Action = [[-0.16586278 -0.16049908 -0.12769747 -0.0027436 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 730 is [True, False, False, False, True, False]
Current timestep = 731. State = [[-0.09511974 -0.04674619]]. Action = [[ 0.22479403  0.19577429  0.15043402 -0.07541621]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 731 is [True, False, False, False, True, False]
Current timestep = 732. State = [[-0.09229634 -0.02652018]]. Action = [[-0.15990493  0.17065221 -0.11504732 -0.63654125]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 732 is [True, False, False, False, True, False]
Current timestep = 733. State = [[-0.08828068 -0.01613127]]. Action = [[ 0.24178088 -0.06199834 -0.09231909 -0.9424968 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 733 is [True, False, False, False, True, False]
Current timestep = 734. State = [[-0.07716082 -0.0160614 ]]. Action = [[-0.02657862  0.02065399 -0.06752583  0.9964776 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 734 is [True, False, False, False, True, False]
Scene graph at timestep 734 is [True, False, False, False, True, False]
State prediction error at timestep 734 is tensor(4.0769e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[-0.06734436 -0.01674987]]. Action = [[ 0.19394815 -0.03532146 -0.09613469  0.04973078]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 735 is [True, False, False, False, True, False]
Scene graph at timestep 735 is [True, False, False, False, True, False]
State prediction error at timestep 735 is tensor(4.0893e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 735 of 1
Current timestep = 736. State = [[-0.05005157 -0.01304466]]. Action = [[-0.16153657  0.07847285 -0.23037677  0.04756534]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 736 is [True, False, False, False, True, False]
Current timestep = 737. State = [[-0.05619467 -0.01632249]]. Action = [[-0.1310321  -0.13187064 -0.0780329  -0.02512723]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 737 is [True, False, False, False, True, False]
Current timestep = 738. State = [[-0.05677551 -0.01126844]]. Action = [[0.24715859 0.23286933 0.23349166 0.63262856]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 738 is [True, False, False, False, True, False]
Scene graph at timestep 738 is [True, False, False, False, True, False]
State prediction error at timestep 738 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[-0.05649277  0.01075343]]. Action = [[-0.04855466  0.08826572  0.16606647  0.5287552 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 739 is [True, False, False, False, True, False]
Scene graph at timestep 739 is [True, False, False, False, True, False]
State prediction error at timestep 739 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 739 of 1
Current timestep = 740. State = [[-0.05289019  0.03069931]]. Action = [[ 0.16440797  0.24425852 -0.03820249  0.9659971 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 740 is [True, False, False, False, True, False]
Scene graph at timestep 740 is [True, False, False, False, True, False]
State prediction error at timestep 740 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.24126954  0.03342176]]. Action = [[ 0.14773607  0.04828277  0.10108685 -0.86715364]]. Reward = [100.]
Curr episode timestep = 13
Scene graph at timestep 741 is [True, False, False, False, True, False]
Scene graph at timestep 741 is [True, False, False, False, True, False]
State prediction error at timestep 741 is tensor(0.0209, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.22949119  0.02811898]]. Action = [[ 0.18336949 -0.18861318 -0.0368423   0.83894205]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 742 is [True, False, False, False, True, False]
Scene graph at timestep 742 is [True, False, False, False, True, False]
State prediction error at timestep 742 is tensor(1.4565e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of 1
Current timestep = 743. State = [[-0.20073983  0.01761319]]. Action = [[ 0.22122681 -0.01164038 -0.06376243 -0.637642  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 743 is [True, False, False, False, True, False]
Scene graph at timestep 743 is [True, False, False, False, True, False]
State prediction error at timestep 743 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 743 of 1
Current timestep = 744. State = [[-0.17050532  0.01886033]]. Action = [[0.19394743 0.07692215 0.21981475 0.19706011]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 744 is [True, False, False, False, True, False]
Current timestep = 745. State = [[-0.14643139  0.02564157]]. Action = [[ 0.16017443  0.07632235  0.00223398 -0.0593487 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 745 is [True, False, False, False, True, False]
Current timestep = 746. State = [[-0.12020234  0.04392944]]. Action = [[0.22320014 0.2099567  0.19983572 0.7903156 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 746 is [True, False, False, False, True, False]
Current timestep = 747. State = [[-0.08718966  0.04873289]]. Action = [[ 0.24570233 -0.1870076   0.11266679  0.5907519 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 747 is [True, False, False, False, True, False]
Scene graph at timestep 747 is [True, False, False, False, True, False]
State prediction error at timestep 747 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of 1
Current timestep = 748. State = [[-0.05745699  0.02917525]]. Action = [[-0.01398568 -0.19598229  0.11235273 -0.9387451 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 748 is [True, False, False, False, True, False]
Scene graph at timestep 748 is [True, False, False, False, True, False]
State prediction error at timestep 748 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of 1
Current timestep = 749. State = [[-0.05740809  0.01248935]]. Action = [[ 0.00570565 -0.0338041   0.2173391   0.9555633 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 749 is [True, False, False, False, True, False]
Scene graph at timestep 749 is [True, False, False, False, True, False]
State prediction error at timestep 749 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[-0.04866028  0.01268141]]. Action = [[ 0.21282798  0.05612147 -0.01418129 -0.16731554]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 750 is [True, False, False, False, True, False]
Current timestep = 751. State = [[-0.15451139 -0.07810318]]. Action = [[-0.11199841 -0.00798565  0.16359884  0.40687525]]. Reward = [100.]
Curr episode timestep = 9
Scene graph at timestep 751 is [False, True, False, False, True, False]
Current timestep = 752. State = [[-0.13725804 -0.08574743]]. Action = [[ 0.09407091  0.01631853 -0.18075337  0.70076203]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 752 is [True, False, False, False, True, False]
Scene graph at timestep 752 is [True, False, False, False, True, False]
State prediction error at timestep 752 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.11421599 -0.09109533]]. Action = [[ 0.19837332 -0.07778223 -0.16516395 -0.01199168]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 753 is [True, False, False, False, True, False]
Scene graph at timestep 753 is [True, False, False, False, True, False]
State prediction error at timestep 753 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 753 of 1
Current timestep = 754. State = [[-0.08521427 -0.08624911]]. Action = [[ 0.23717636  0.18839747 -0.10698748 -0.59158576]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 754 is [True, False, False, False, True, False]
Scene graph at timestep 754 is [True, False, False, False, True, False]
State prediction error at timestep 754 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of 1
Current timestep = 755. State = [[-0.05282321 -0.08088318]]. Action = [[ 0.1824112  -0.11173108 -0.18466446 -0.8952469 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 755 is [True, False, False, False, True, False]
Scene graph at timestep 755 is [True, False, False, False, True, False]
State prediction error at timestep 755 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of 1
Current timestep = 756. State = [[-0.19300519  0.18063393]]. Action = [[ 0.13875118 -0.0806388   0.182168    0.42692423]]. Reward = [100.]
Curr episode timestep = 4
Scene graph at timestep 756 is [True, False, False, False, True, False]
Scene graph at timestep 756 is [True, False, False, False, False, True]
State prediction error at timestep 756 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 756 of 1
Current timestep = 757. State = [[-0.17495723  0.190373  ]]. Action = [[ 0.09241185 -0.2165346   0.21925992 -0.6157018 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 757 is [True, False, False, False, False, True]
Current timestep = 758. State = [[-0.15866403  0.16578138]]. Action = [[ 0.1654746  -0.20453675  0.07480794 -0.7665753 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 758 is [True, False, False, False, False, True]
Current timestep = 759. State = [[-0.13917257  0.14465052]]. Action = [[ 0.09818572 -0.10187626  0.08387038  0.09214389]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 759 is [True, False, False, False, False, True]
Current timestep = 760. State = [[-0.11975917  0.12312383]]. Action = [[ 0.1990211  -0.19521873 -0.10325484  0.08984423]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 760 is [True, False, False, False, False, True]
Current timestep = 761. State = [[-0.10721609  0.11152517]]. Action = [[-0.1511295   0.03531229  0.20029348  0.92328894]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 761 is [True, False, False, False, True, False]
Scene graph at timestep 761 is [True, False, False, False, True, False]
State prediction error at timestep 761 is tensor(5.7088e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[-0.10835862  0.11163662]]. Action = [[-0.0574576  -0.00798269 -0.11745268 -0.42189324]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 762 is [True, False, False, False, True, False]
Scene graph at timestep 762 is [True, False, False, False, True, False]
State prediction error at timestep 762 is tensor(1.1228e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of 1
Current timestep = 763. State = [[-0.10617818  0.10173257]]. Action = [[ 0.12449551 -0.1462964   0.20556968 -0.7102082 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 763 is [True, False, False, False, True, False]
Current timestep = 764. State = [[-0.10009892  0.08171714]]. Action = [[ 0.13144374 -0.15963389 -0.21629857 -0.86680275]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 764 is [True, False, False, False, True, False]
Current timestep = 765. State = [[-0.08347595  0.06759144]]. Action = [[ 0.21351403  0.02061945 -0.07238403 -0.7620899 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 765 is [True, False, False, False, True, False]
Scene graph at timestep 765 is [True, False, False, False, True, False]
State prediction error at timestep 765 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 1
Current timestep = 766. State = [[-0.05018501  0.05940668]]. Action = [[ 0.2095598  -0.14141253 -0.13879493  0.30951655]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 766 is [True, False, False, False, True, False]
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 766 of 1
Current timestep = 767. State = [[-0.22163056  0.16984774]]. Action = [[ 0.07450008  0.19086695 -0.00532755  0.8205948 ]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 767 is [True, False, False, False, True, False]
Scene graph at timestep 767 is [True, False, False, False, False, True]
State prediction error at timestep 767 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of 1
Current timestep = 768. State = [[-0.2051685   0.19721888]]. Action = [[ 0.22220474  0.17901641  0.13641727 -0.5252619 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 768 is [True, False, False, False, False, True]
Current timestep = 769. State = [[-0.17378086  0.19940391]]. Action = [[ 0.14312965 -0.2338348  -0.21136859  0.2766161 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 769 is [True, False, False, False, False, True]
Current timestep = 770. State = [[-0.15177222  0.18521897]]. Action = [[ 0.18227756 -0.0631979   0.18917745  0.42390418]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 770 is [True, False, False, False, False, True]
Current timestep = 771. State = [[-0.12622336  0.18867649]]. Action = [[0.2215085  0.18467033 0.15850636 0.5044112 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 771 is [True, False, False, False, False, True]
Current timestep = 772. State = [[-0.10041967  0.2071813 ]]. Action = [[ 0.11193058  0.16380799  0.1304192  -0.86291057]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 772 is [True, False, False, False, False, True]
Current timestep = 773. State = [[-0.07876728  0.2294493 ]]. Action = [[ 0.18561262  0.15689465 -0.04202083 -0.3142063 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 773 is [True, False, False, False, False, True]
Scene graph at timestep 773 is [True, False, False, False, False, True]
State prediction error at timestep 773 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 773 of -1
Current timestep = 774. State = [[-0.04087057  0.25753438]]. Action = [[ 0.24599111  0.15901971 -0.21257144 -0.5160043 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 774 is [True, False, False, False, False, True]
Scene graph at timestep 774 is [False, True, False, False, False, True]
State prediction error at timestep 774 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 774 of -1
Current timestep = 775. State = [[-2.1591835e-04  2.6402336e-01]]. Action = [[ 0.23062593 -0.17270301  0.1148856   0.38103604]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 775 is [False, True, False, False, False, True]
Current timestep = 776. State = [[0.02606484 0.24252674]]. Action = [[ 0.21382713 -0.22645028  0.18343446  0.44855893]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 776 is [False, True, False, False, False, True]
Scene graph at timestep 776 is [False, True, False, False, False, True]
State prediction error at timestep 776 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 776 of 1
Current timestep = 777. State = [[0.05542535 0.22152688]]. Action = [[ 0.05031815 -0.02063307  0.02481726 -0.8824032 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 777 is [False, True, False, False, False, True]
Scene graph at timestep 777 is [False, False, True, False, False, True]
State prediction error at timestep 777 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 1
Current timestep = 778. State = [[0.0591566  0.21805665]]. Action = [[-0.08862692 -0.0624709   0.15013766 -0.06800228]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 778 is [False, False, True, False, False, True]
Current timestep = 779. State = [[0.05987671 0.2150367 ]]. Action = [[ 0.06185639 -0.22772017  0.22212642  0.55402386]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 779 is [False, False, True, False, False, True]
Current timestep = 780. State = [[0.06018502 0.21366581]]. Action = [[ 0.12435552  0.16828    -0.21523395 -0.5074769 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 780 is [False, False, True, False, False, True]
Scene graph at timestep 780 is [False, False, True, False, False, True]
State prediction error at timestep 780 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 780 of 1
Current timestep = 781. State = [[0.06416002 0.20152977]]. Action = [[-0.00062257 -0.2235606  -0.15057516  0.46803856]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 781 is [False, False, True, False, False, True]
Current timestep = 782. State = [[0.06567408 0.19369978]]. Action = [[-0.12736419  0.06421345  0.04315606  0.74897444]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 782 is [False, False, True, False, False, True]
Scene graph at timestep 782 is [False, False, True, False, False, True]
State prediction error at timestep 782 is tensor(6.7238e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[0.06440163 0.19513166]]. Action = [[ 0.09037215 -0.03615794  0.03392646 -0.824777  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 783 is [False, False, True, False, False, True]
Scene graph at timestep 783 is [False, False, True, False, False, True]
State prediction error at timestep 783 is tensor(5.8553e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 783 of 1
Current timestep = 784. State = [[0.06204392 0.19931406]]. Action = [[-0.0540483  0.0510546 -0.096829   0.941592 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 784 is [False, False, True, False, False, True]
Scene graph at timestep 784 is [False, False, True, False, False, True]
State prediction error at timestep 784 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of 1
Current timestep = 785. State = [[0.05519652 0.19103418]]. Action = [[-0.19915408 -0.24521957 -0.10610093  0.5388082 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 785 is [False, False, True, False, False, True]
Current timestep = 786. State = [[0.03934716 0.17495501]]. Action = [[0.14850241 0.19665375 0.06494692 0.21854615]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 786 is [False, False, True, False, False, True]
Scene graph at timestep 786 is [False, True, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[0.0351787 0.1753441]]. Action = [[0.03076154 0.0955652  0.14023215 0.7764354 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 787 is [False, True, False, False, False, True]
Scene graph at timestep 787 is [False, True, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[0.02832841 0.19264862]]. Action = [[-0.15841156  0.22760653  0.19855025 -0.5887814 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 788 is [False, True, False, False, False, True]
Current timestep = 789. State = [[0.00870248 0.2184001 ]]. Action = [[-0.21791616  0.00910589  0.24370861  0.99275243]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 789 is [False, True, False, False, False, True]
Current timestep = 790. State = [[-0.00765729  0.22390081]]. Action = [[ 0.06212091  0.03031701  0.21960527 -0.8713805 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 790 is [False, True, False, False, False, True]
Scene graph at timestep 790 is [False, True, False, False, False, True]
State prediction error at timestep 790 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of -1
Current timestep = 791. State = [[-0.01280374  0.23426762]]. Action = [[-0.02819645  0.1586357  -0.07523164  0.6479361 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 791 is [False, True, False, False, False, True]
Scene graph at timestep 791 is [False, True, False, False, False, True]
State prediction error at timestep 791 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 791 of -1
Current timestep = 792. State = [[-0.01372077  0.23796216]]. Action = [[ 0.14731619 -0.12702943  0.1385476  -0.08272642]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 792 is [False, True, False, False, False, True]
Scene graph at timestep 792 is [False, True, False, False, False, True]
State prediction error at timestep 792 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.01351336  0.23143442]]. Action = [[-0.22182469  0.00200757 -0.08861625  0.30207098]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 793 is [False, True, False, False, False, True]
Current timestep = 794. State = [[-0.01630481  0.23445903]]. Action = [[ 0.20428419  0.07922578 -0.09317872  0.7955178 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 794 is [False, True, False, False, False, True]
Current timestep = 795. State = [[-0.02101713  0.24727972]]. Action = [[-0.18211894  0.14956358  0.11199075 -0.38785756]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 795 is [False, True, False, False, False, True]
Current timestep = 796. State = [[-0.03040968  0.25837475]]. Action = [[-0.14216946 -0.06551769  0.15368736 -0.5645947 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 796 is [False, True, False, False, False, True]
Current timestep = 797. State = [[-0.04638583  0.26580536]]. Action = [[-0.18735243  0.08499885 -0.15195473  0.80667496]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 797 is [False, True, False, False, False, True]
Current timestep = 798. State = [[-0.05760378  0.25566792]]. Action = [[ 0.17761254 -0.21864054  0.23741257  0.4906268 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 798 is [False, True, False, False, False, True]
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.05400715  0.23999457]]. Action = [[ 0.09703657  0.10295242 -0.12523249 -0.9059829 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 799 is [True, False, False, False, False, True]
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of -1
Current timestep = 800. State = [[-0.05548209  0.24411008]]. Action = [[-0.18494304 -0.04534176 -0.14355913 -0.851065  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 800 is [True, False, False, False, False, True]
Scene graph at timestep 800 is [True, False, False, False, False, True]
State prediction error at timestep 800 is tensor(6.3977e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[-0.05338962  0.23113944]]. Action = [[ 0.21924788 -0.17212987 -0.22448745  0.49387717]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 801 is [True, False, False, False, False, True]
Scene graph at timestep 801 is [True, False, False, False, False, True]
State prediction error at timestep 801 is tensor(6.3411e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of -1
Current timestep = 802. State = [[-0.03759841  0.20413712]]. Action = [[ 0.22545275 -0.20179792 -0.16939408  0.6813022 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 802 is [True, False, False, False, False, True]
Scene graph at timestep 802 is [False, True, False, False, False, True]
State prediction error at timestep 802 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of 1
Current timestep = 803. State = [[-0.01759454  0.180634  ]]. Action = [[0.1668312  0.03017622 0.22722676 0.80916536]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 803 is [False, True, False, False, False, True]
Current timestep = 804. State = [[-0.00731191  0.17212278]]. Action = [[ 0.03832394 -0.14715819 -0.00511093 -0.36335254]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 804 is [False, True, False, False, False, True]
Scene graph at timestep 804 is [False, True, False, False, False, True]
State prediction error at timestep 804 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of -1
Current timestep = 805. State = [[0.00462046 0.16072427]]. Action = [[ 0.14917302  0.02199394 -0.23180005  0.766629  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 805 is [False, True, False, False, False, True]
Scene graph at timestep 805 is [False, True, False, False, False, True]
State prediction error at timestep 805 is tensor(4.9782e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[0.01377699 0.17391479]]. Action = [[-0.10527951  0.21569699 -0.0970595   0.54416156]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 806 is [False, True, False, False, False, True]
Scene graph at timestep 806 is [False, True, False, False, False, True]
State prediction error at timestep 806 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of 1
Current timestep = 807. State = [[0.0069826  0.19129272]]. Action = [[-0.09547168  0.03047007 -0.0745063   0.06482077]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 807 is [False, True, False, False, False, True]
Scene graph at timestep 807 is [False, True, False, False, False, True]
State prediction error at timestep 807 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of -1
Current timestep = 808. State = [[0.00648959 0.19444941]]. Action = [[ 0.14248824 -0.01784289 -0.08621886 -0.8400543 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 808 is [False, True, False, False, False, True]
Scene graph at timestep 808 is [False, True, False, False, False, True]
State prediction error at timestep 808 is tensor(3.6458e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of -1
Current timestep = 809. State = [[0.0066566  0.19481349]]. Action = [[-0.18113787 -0.01441489  0.2282992   0.97793424]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 809 is [False, True, False, False, False, True]
Scene graph at timestep 809 is [False, True, False, False, False, True]
State prediction error at timestep 809 is tensor(7.3702e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 809 of -1
Current timestep = 810. State = [[0.0070764  0.19326322]]. Action = [[ 0.08721033 -0.03661986  0.12250647 -0.38546687]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 810 is [False, True, False, False, False, True]
Scene graph at timestep 810 is [False, True, False, False, False, True]
State prediction error at timestep 810 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of -1
Current timestep = 811. State = [[0.0117039  0.17713106]]. Action = [[ 0.05513752 -0.23518045  0.16973877  0.7329843 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 811 is [False, True, False, False, False, True]
Scene graph at timestep 811 is [False, True, False, False, False, True]
State prediction error at timestep 811 is tensor(4.4663e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[0.01691659 0.16449489]]. Action = [[-0.01054192  0.13163224  0.0935508  -0.581804  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 812 is [False, True, False, False, False, True]
Scene graph at timestep 812 is [False, True, False, False, False, True]
State prediction error at timestep 812 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of 1
Current timestep = 813. State = [[0.0085554  0.18125656]]. Action = [[-0.23408839  0.17145061  0.2108264   0.622246  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 813 is [False, True, False, False, False, True]
Current timestep = 814. State = [[0.003026 0.184753]]. Action = [[ 0.10775822 -0.23381133  0.1102792  -0.4046427 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 814 is [False, True, False, False, False, True]
Scene graph at timestep 814 is [False, True, False, False, False, True]
State prediction error at timestep 814 is tensor(6.7840e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of 1
Current timestep = 815. State = [[0.0108573  0.17124252]]. Action = [[ 0.23873901  0.03998822  0.01226646 -0.4841228 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 815 is [False, True, False, False, False, True]
Current timestep = 816. State = [[0.01707911 0.16315067]]. Action = [[-0.13825785 -0.13051245 -0.09062698  0.3571688 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 816 is [False, True, False, False, False, True]
Current timestep = 817. State = [[0.02156571 0.15153387]]. Action = [[ 0.10995024 -0.08980927  0.18058473  0.82309604]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 817 is [False, True, False, False, False, True]
Current timestep = 818. State = [[0.03185968 0.13665305]]. Action = [[ 0.23932463 -0.02847527  0.21737513 -0.39027357]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 818 is [False, True, False, False, False, True]
Scene graph at timestep 818 is [False, True, False, False, False, True]
State prediction error at timestep 818 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 818 of 1
Current timestep = 819. State = [[0.04605989 0.14253446]]. Action = [[-0.07190806  0.20600668 -0.2425673  -0.28449798]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 819 is [False, True, False, False, False, True]
Current timestep = 820. State = [[0.04182607 0.15335238]]. Action = [[ 0.20950645 -0.22545052  0.17760503  0.49916744]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 820 is [False, True, False, False, False, True]
Scene graph at timestep 820 is [False, True, False, False, False, True]
State prediction error at timestep 820 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of -1
Current timestep = 821. State = [[0.03745297 0.16503727]]. Action = [[-0.1447444   0.12327605  0.09153396 -0.9815341 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 821 is [False, True, False, False, False, True]
Current timestep = 822. State = [[0.03263125 0.17597115]]. Action = [[ 0.21407825 -0.08511586  0.2054713  -0.83428663]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 822 is [False, True, False, False, False, True]
Scene graph at timestep 822 is [False, True, False, False, False, True]
State prediction error at timestep 822 is tensor(4.6177e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 822 of -1
Current timestep = 823. State = [[0.03420228 0.17095084]]. Action = [[ 0.00246552 -0.14658104  0.11161688 -0.33482385]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 823 is [False, True, False, False, False, True]
Current timestep = 824. State = [[0.03957225 0.15672243]]. Action = [[ 0.12460625 -0.11604142 -0.15869209 -0.7298501 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 824 is [False, True, False, False, False, True]
Scene graph at timestep 824 is [False, True, False, False, False, True]
State prediction error at timestep 824 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of -1
Current timestep = 825. State = [[0.0427331  0.14946452]]. Action = [[-0.00923     0.11293054 -0.20450899  0.20544875]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 825 is [False, True, False, False, False, True]
Current timestep = 826. State = [[0.04568954 0.14523433]]. Action = [[ 0.13108873 -0.12280306 -0.02059995  0.04230821]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 826 is [False, True, False, False, False, True]
Current timestep = 827. State = [[0.05136571 0.14010754]]. Action = [[ 0.12780857  0.06135416  0.13902658 -0.32193398]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 827 is [False, True, False, False, False, True]
Current timestep = 828. State = [[0.05211305 0.13953839]]. Action = [[ 2.2128731e-01 -1.9904052e-01  1.9180393e-01 -6.8008900e-05]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 828 is [False, False, True, False, False, True]
Current timestep = 829. State = [[0.0521549  0.13937588]]. Action = [[ 0.19905281  0.0243656   0.24389261 -0.9152759 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 829 is [False, False, True, False, False, True]
Current timestep = 830. State = [[0.0521462  0.13937254]]. Action = [[ 0.16807017 -0.15854996  0.07622379  0.93606687]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 830 is [False, False, True, False, False, True]
Scene graph at timestep 830 is [False, False, True, False, False, True]
State prediction error at timestep 830 is tensor(7.4462e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of 1
Current timestep = 831. State = [[0.05204409 0.13933389]]. Action = [[-0.09889477 -0.03916684 -0.01083909 -0.7497346 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 831 is [False, False, True, False, False, True]
Scene graph at timestep 831 is [False, False, True, False, False, True]
State prediction error at timestep 831 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[0.05207571 0.1392508 ]]. Action = [[ 0.16375285 -0.21361811  0.10215503  0.40221274]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 832 is [False, False, True, False, False, True]
Current timestep = 833. State = [[0.055164  0.1294032]]. Action = [[ 0.0676963  -0.14220713 -0.16587321  0.3278724 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 833 is [False, False, True, False, False, True]
Current timestep = 834. State = [[0.05710003 0.12489218]]. Action = [[0.00856113 0.14004278 0.15857017 0.36040163]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 834 is [False, False, True, False, False, True]
Current timestep = 835. State = [[0.05539807 0.11805159]]. Action = [[-0.20650181 -0.22241849 -0.16633044 -0.02282405]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 835 is [False, False, True, False, True, False]
Scene graph at timestep 835 is [False, False, True, False, True, False]
State prediction error at timestep 835 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[0.05161741 0.10666597]]. Action = [[ 0.14606416 -0.07632037  0.21254313  0.85721517]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 836 is [False, False, True, False, True, False]
Current timestep = 837. State = [[0.04869635 0.11638315]]. Action = [[0.03189763 0.19554624 0.12716106 0.06287587]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 837 is [False, False, True, False, True, False]
Current timestep = 838. State = [[0.04251065 0.14119838]]. Action = [[-0.01309732  0.24085239 -0.18147965  0.19030225]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 838 is [False, True, False, False, True, False]
Current timestep = 839. State = [[0.03805965 0.15230255]]. Action = [[-0.05481473 -0.1465407   0.20478734  0.29808807]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 839 is [False, True, False, False, False, True]
Scene graph at timestep 839 is [False, True, False, False, False, True]
State prediction error at timestep 839 is tensor(9.8792e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[0.03823351 0.14964901]]. Action = [[ 0.24566391 -0.15798451  0.17253625 -0.17847437]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 840 is [False, True, False, False, False, True]
Current timestep = 841. State = [[0.03823351 0.14964901]]. Action = [[ 0.16271165  0.1145027  -0.21984842 -0.45102274]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 841 is [False, True, False, False, False, True]
Current timestep = 842. State = [[0.03752861 0.13491847]]. Action = [[-6.2742725e-02 -2.4629152e-01 -3.4223497e-04  9.8355269e-01]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 842 is [False, True, False, False, False, True]
Scene graph at timestep 842 is [False, True, False, False, False, True]
State prediction error at timestep 842 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 1
Current timestep = 843. State = [[-0.19122875 -0.03637463]]. Action = [[-0.12714027 -0.08474474  0.1940592   0.78941584]]. Reward = [100.]
Curr episode timestep = 75
Scene graph at timestep 843 is [False, True, False, False, False, True]
Current timestep = 844. State = [[-0.17191157 -0.03024283]]. Action = [[ 0.21192053  0.2103287   0.03850374 -0.9593814 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 844 is [True, False, False, False, True, False]
Scene graph at timestep 844 is [True, False, False, False, True, False]
State prediction error at timestep 844 is tensor(5.9765e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of 1
Current timestep = 845. State = [[-0.13891044 -0.00653529]]. Action = [[ 0.24331927  0.17123201  0.07387459 -0.7212666 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 845 is [True, False, False, False, True, False]
Scene graph at timestep 845 is [True, False, False, False, True, False]
State prediction error at timestep 845 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of 1
Current timestep = 846. State = [[-0.10101872  0.00329664]]. Action = [[ 0.22840768 -0.08994074  0.02379015  0.9577923 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 846 is [True, False, False, False, True, False]
Current timestep = 847. State = [[-0.08879627  0.01180517]]. Action = [[-0.12913753  0.20249718 -0.00416493  0.04628575]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 847 is [True, False, False, False, True, False]
Scene graph at timestep 847 is [True, False, False, False, True, False]
State prediction error at timestep 847 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of 1
Current timestep = 848. State = [[-0.09875826  0.03777183]]. Action = [[-0.21707799  0.23256278 -0.03261238  0.49400067]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.11825255  0.07108869]]. Action = [[-0.2264255   0.19177377 -0.01622303  0.37209105]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 849 is [True, False, False, False, True, False]
Scene graph at timestep 849 is [True, False, False, False, True, False]
State prediction error at timestep 849 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 849 of -1
Current timestep = 850. State = [[-0.14640439  0.09509537]]. Action = [[-0.14687505  0.03150952 -0.20993733  0.04479253]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 850 is [True, False, False, False, True, False]
Scene graph at timestep 850 is [True, False, False, False, True, False]
State prediction error at timestep 850 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of -1
Current timestep = 851. State = [[-0.15919706  0.10955706]]. Action = [[ 0.05582663  0.20750678  0.09585163 -0.253982  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(8.8033e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of -1
Current timestep = 852. State = [[-0.17062272  0.13806026]]. Action = [[-0.16194913  0.21230009  0.18136537 -0.9063509 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, False, True]
State prediction error at timestep 852 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.18276128  0.15892407]]. Action = [[-0.0670751  -0.022627    0.16963693  0.5771823 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 853 is [True, False, False, False, False, True]
Current timestep = 854. State = [[-0.18433566  0.15403706]]. Action = [[-0.04970481 -0.09384809  0.10095897 -0.85403717]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 854 is [True, False, False, False, False, True]
Current timestep = 855. State = [[-0.18651932  0.14038074]]. Action = [[-0.00244911 -0.12743692  0.13308287  0.7837223 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 855 is [True, False, False, False, False, True]
Current timestep = 856. State = [[-0.18259123  0.11981648]]. Action = [[ 0.22533104 -0.13213088  0.16461658 -0.9213554 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 856 is [True, False, False, False, False, True]
Scene graph at timestep 856 is [True, False, False, False, True, False]
State prediction error at timestep 856 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of 1
Current timestep = 857. State = [[-0.1723232   0.11690463]]. Action = [[0.09910628 0.20492199 0.18170369 0.45203006]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 857 is [True, False, False, False, True, False]
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(1.5458e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of 1
Current timestep = 858. State = [[-0.16401999  0.11698574]]. Action = [[ 0.03552291 -0.20505822 -0.2229813   0.24676776]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 858 is [True, False, False, False, True, False]
Current timestep = 859. State = [[-0.1507519   0.10053641]]. Action = [[ 0.2064122  -0.10226212  0.03458592  0.48877394]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 859 is [True, False, False, False, True, False]
Scene graph at timestep 859 is [True, False, False, False, True, False]
State prediction error at timestep 859 is tensor(9.9361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of 1
Current timestep = 860. State = [[-0.13658005  0.07746948]]. Action = [[-0.21345268 -0.21732628 -0.1640909  -0.3454085 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 860 is [True, False, False, False, True, False]
Scene graph at timestep 860 is [True, False, False, False, True, False]
State prediction error at timestep 860 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 860 of 1
Current timestep = 861. State = [[-0.15359862  0.06217316]]. Action = [[-0.22068287  0.02979121 -0.21192046 -0.67897034]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.16344705  0.06955555]]. Action = [[ 0.18934923  0.14007264 -0.1803338   0.7669333 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.15488881  0.08153175]]. Action = [[ 0.15669203  0.09032291 -0.24246202  0.92884946]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 863 is [True, False, False, False, True, False]
Scene graph at timestep 863 is [True, False, False, False, True, False]
State prediction error at timestep 863 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of 1
Current timestep = 864. State = [[-0.13434799  0.07926515]]. Action = [[ 0.22321078 -0.20869575  0.20069283  0.44569373]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 864 is [True, False, False, False, True, False]
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(6.5686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.10636357  0.05191365]]. Action = [[ 0.18418783 -0.19975466 -0.13099666 -0.4298479 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 865 is [True, False, False, False, True, False]
Current timestep = 866. State = [[-0.08042795  0.04864751]]. Action = [[0.22507271 0.18175983 0.18951505 0.3453312 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 866 is [True, False, False, False, True, False]
Current timestep = 867. State = [[-0.05698602  0.0528173 ]]. Action = [[ 0.04001415 -0.07824072 -0.18274604  0.6282551 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 867 is [True, False, False, False, True, False]
Scene graph at timestep 867 is [True, False, False, False, True, False]
State prediction error at timestep 867 is tensor(5.0877e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of 1
Current timestep = 868. State = [[-0.05259209  0.04045166]]. Action = [[-0.11664501 -0.1654142   0.19375056  0.40240145]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 868 is [True, False, False, False, True, False]
Current timestep = 869. State = [[-0.04635613  0.02688905]]. Action = [[ 0.23915374 -0.04692574  0.17456847  0.89993906]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 869 is [True, False, False, False, True, False]
Scene graph at timestep 869 is [False, True, False, False, True, False]
State prediction error at timestep 869 is tensor(4.8678e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of 1
Current timestep = 870. State = [[-0.25275156  0.02888154]]. Action = [[ 0.23958391 -0.14388596 -0.08364663  0.61249757]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 870 is [False, True, False, False, True, False]
Current timestep = 871. State = [[-0.24959716  0.03382144]]. Action = [[-0.10395047 -0.02443336  0.17000496 -0.8063517 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 871 is [True, False, False, False, True, False]
Current timestep = 872. State = [[-0.24621965  0.0442321 ]]. Action = [[0.18843406 0.19170776 0.05690712 0.8051833 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 872 is [True, False, False, False, True, False]
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(7.4100e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.2404489   0.06995395]]. Action = [[-0.00447613  0.18942612  0.09923446 -0.859242  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 1
Current timestep = 874. State = [[-0.24294357  0.0883372 ]]. Action = [[-0.10519022  0.03792542 -0.11127734  0.7452729 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.24023597  0.10621975]]. Action = [[ 0.1872338   0.22611776 -0.13558656 -0.09221214]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.22118227  0.11888546]]. Action = [[ 0.14630544 -0.06576896  0.03906208 -0.6915455 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 876 is [True, False, False, False, True, False]
Current timestep = 877. State = [[-0.19990665  0.12282082]]. Action = [[ 0.17940265  0.06523022 -0.10742705  0.7784147 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 877 is [True, False, False, False, True, False]
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(9.2120e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of 1
Current timestep = 878. State = [[-0.17177373  0.12519753]]. Action = [[ 0.20024374 -0.03689525 -0.19552083  0.13863873]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 878 is [True, False, False, False, True, False]
Current timestep = 879. State = [[-0.14594516  0.1277987 ]]. Action = [[ 0.17531443  0.06234848 -0.17326376 -0.39111972]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 879 is [True, False, False, False, False, True]
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of 1
Current timestep = 880. State = [[-0.12052023  0.13803269]]. Action = [[ 0.1060136   0.12464452  0.03247717 -0.83859277]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 880 is [True, False, False, False, False, True]
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of 1
Current timestep = 881. State = [[-0.11594079  0.15848108]]. Action = [[-0.205075    0.15393135 -0.19471441  0.7487242 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 881 is [True, False, False, False, False, True]
Current timestep = 882. State = [[-0.12909086  0.18139586]]. Action = [[-0.00031723  0.15567917  0.06015673 -0.21657646]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 882 is [True, False, False, False, False, True]
Current timestep = 883. State = [[-0.12513217  0.19764231]]. Action = [[0.23629075 0.11933321 0.09436741 0.34241366]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 883 is [True, False, False, False, False, True]
Scene graph at timestep 883 is [True, False, False, False, False, True]
State prediction error at timestep 883 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 883 of -1
Current timestep = 884. State = [[-0.09171117  0.20855318]]. Action = [[ 2.3071155e-01 -4.7575444e-02  2.4735928e-04 -2.8425288e-01]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 884 is [True, False, False, False, False, True]
Current timestep = 885. State = [[-0.07284628  0.19807652]]. Action = [[-0.05856454 -0.18480884 -0.00574654  0.24128199]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 885 is [True, False, False, False, False, True]
Current timestep = 886. State = [[-0.06366868  0.18071906]]. Action = [[ 0.17670739 -0.07033989 -0.23223361 -0.88497865]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 886 is [True, False, False, False, False, True]
Scene graph at timestep 886 is [True, False, False, False, False, True]
State prediction error at timestep 886 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.04994863  0.15995347]]. Action = [[-0.06353763 -0.22266775 -0.23898552 -0.25555277]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 887 is [True, False, False, False, False, True]
Scene graph at timestep 887 is [False, True, False, False, False, True]
State prediction error at timestep 887 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of 1
Current timestep = 888. State = [[-0.04423856  0.14061053]]. Action = [[ 0.16060442 -0.04750104 -0.19451411  0.43510413]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 888 is [False, True, False, False, False, True]
Current timestep = 889. State = [[-0.03299525  0.1272676 ]]. Action = [[ 0.14837748 -0.11154488  0.23832083 -0.94694173]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 889 is [False, True, False, False, False, True]
Current timestep = 890. State = [[-0.19112192  0.14305858]]. Action = [[ 0.09393939 -0.1380716  -0.08813503  0.76872754]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 890 is [False, True, False, False, False, True]
Current timestep = 891. State = [[-0.17893441  0.16073748]]. Action = [[-0.08837044 -0.034503   -0.18246076  0.633338  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 891 is [True, False, False, False, False, True]
Current timestep = 892. State = [[-0.18717162  0.17193677]]. Action = [[-0.15015252  0.18857157  0.2087279  -0.3313337 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 892 is [True, False, False, False, False, True]
Current timestep = 893. State = [[-0.19571856  0.1860757 ]]. Action = [[0.11805236 0.01838046 0.11849284 0.8461087 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 893 is [True, False, False, False, False, True]
Current timestep = 894. State = [[-0.19342458  0.18221623]]. Action = [[-0.0412426  -0.12558484 -0.08807307  0.40785086]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 894 is [True, False, False, False, False, True]
Current timestep = 895. State = [[-0.19352591  0.17063288]]. Action = [[-0.11646566 -0.11772268  0.0483495   0.1670084 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 895 is [True, False, False, False, False, True]
Current timestep = 896. State = [[-0.20264903  0.16882215]]. Action = [[-0.07898787  0.10668132 -0.15634854 -0.22183472]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 896 is [True, False, False, False, False, True]
Current timestep = 897. State = [[-0.20450129  0.17624731]]. Action = [[ 0.17999455  0.08557814 -0.10220362 -0.32929063]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 897 is [True, False, False, False, False, True]
Scene graph at timestep 897 is [True, False, False, False, False, True]
State prediction error at timestep 897 is tensor(2.5064e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.2103528  0.1925144]]. Action = [[-0.18818888  0.19253194 -0.09788451 -0.84403473]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 898 is [True, False, False, False, False, True]
Scene graph at timestep 898 is [True, False, False, False, False, True]
State prediction error at timestep 898 is tensor(7.0860e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.22003488  0.21664968]]. Action = [[ 0.11668268  0.1315422   0.23235661 -0.51977974]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 899 is [True, False, False, False, False, True]
Current timestep = 900. State = [[-0.2186196   0.22619455]]. Action = [[-0.14778696 -0.02537979 -0.16043612 -0.19605386]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 900 is [True, False, False, False, False, True]
Scene graph at timestep 900 is [True, False, False, False, False, True]
State prediction error at timestep 900 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 900 of -1
Current timestep = 901. State = [[-0.21659283  0.21617572]]. Action = [[ 0.12554961 -0.22180021 -0.24735364  0.2340107 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 901 is [True, False, False, False, False, True]
Scene graph at timestep 901 is [True, False, False, False, False, True]
State prediction error at timestep 901 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of 1
Current timestep = 902. State = [[-0.21411254  0.20446287]]. Action = [[-0.055114    0.1042695   0.1898061   0.74723923]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 902 is [True, False, False, False, False, True]
Scene graph at timestep 902 is [True, False, False, False, False, True]
State prediction error at timestep 902 is tensor(8.4320e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 902 of -1
Current timestep = 903. State = [[-0.20768102  0.21315067]]. Action = [[ 0.23678112  0.0768933  -0.23063497  0.6214986 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 903 is [True, False, False, False, False, True]
Current timestep = 904. State = [[-0.19235694  0.2204178 ]]. Action = [[ 0.01758009  0.0131602  -0.17488153  0.11628854]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 904 is [True, False, False, False, False, True]
Scene graph at timestep 904 is [True, False, False, False, False, True]
State prediction error at timestep 904 is tensor(2.7797e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of -1
Current timestep = 905. State = [[-0.19539534  0.2288779 ]]. Action = [[-0.16395272  0.08990285  0.15195668  0.16706145]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 905 is [True, False, False, False, False, True]
Scene graph at timestep 905 is [True, False, False, False, False, True]
State prediction error at timestep 905 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 905 of -1
Current timestep = 906. State = [[-0.19955239  0.23872669]]. Action = [[0.15008134 0.0641852  0.22481373 0.48147297]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 906 is [True, False, False, False, False, True]
Scene graph at timestep 906 is [True, False, False, False, False, True]
State prediction error at timestep 906 is tensor(8.1573e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 906 of -1
Current timestep = 907. State = [[-0.18489243  0.23013428]]. Action = [[ 0.04107234 -0.24876839  0.08351466  0.72979164]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 907 is [True, False, False, False, False, True]
Current timestep = 908. State = [[-0.170966    0.20314798]]. Action = [[ 0.23035252 -0.20007834 -0.1444469  -0.05965835]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 908 is [True, False, False, False, False, True]
Scene graph at timestep 908 is [True, False, False, False, False, True]
State prediction error at timestep 908 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 908 of -1
Current timestep = 909. State = [[-0.15986794  0.19575399]]. Action = [[-0.17080255  0.21770796 -0.22832334 -0.2942493 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 909 is [True, False, False, False, False, True]
Current timestep = 910. State = [[-0.17193599  0.21314152]]. Action = [[-0.11980706  0.03412899 -0.03117514  0.06434214]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 910 is [True, False, False, False, False, True]
Current timestep = 911. State = [[-0.17112471  0.20859836]]. Action = [[ 0.11486569 -0.19407512 -0.1874681   0.33867264]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 911 is [True, False, False, False, False, True]
Scene graph at timestep 911 is [True, False, False, False, False, True]
State prediction error at timestep 911 is tensor(5.9463e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 911 of 1
Current timestep = 912. State = [[-0.16992857  0.19073269]]. Action = [[-0.126969   -0.10905552  0.11944431 -0.39221138]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 912 is [True, False, False, False, False, True]
Scene graph at timestep 912 is [True, False, False, False, False, True]
State prediction error at timestep 912 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of 1
Current timestep = 913. State = [[-0.16554156  0.17713405]]. Action = [[ 0.22340304 -0.0526956   0.06040847  0.01792347]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 913 is [True, False, False, False, False, True]
Current timestep = 914. State = [[-0.15904401  0.16197458]]. Action = [[-0.09920435 -0.18352403 -0.23117775  0.7872157 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 914 is [True, False, False, False, False, True]
Current timestep = 915. State = [[-0.16765171  0.13940665]]. Action = [[-0.24011736 -0.20124792 -0.20720217 -0.3189237 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 915 is [True, False, False, False, False, True]
Current timestep = 916. State = [[-0.19322866  0.10832969]]. Action = [[-0.1884688  -0.21927464  0.2003566   0.7424309 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 916 is [True, False, False, False, False, True]
Scene graph at timestep 916 is [True, False, False, False, True, False]
State prediction error at timestep 916 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 916 of 1
Current timestep = 917. State = [[-0.21047051  0.08606841]]. Action = [[0.03680274 0.00609767 0.13991255 0.56784904]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.20938677  0.08802058]]. Action = [[0.09175813 0.07441041 0.14920264 0.69865274]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.20520112  0.1023487 ]]. Action = [[0.08044299 0.20555395 0.1321877  0.9551879 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 919 is [True, False, False, False, True, False]
Scene graph at timestep 919 is [True, False, False, False, True, False]
State prediction error at timestep 919 is tensor(5.9856e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of 1
Current timestep = 920. State = [[-0.20019871  0.12885258]]. Action = [[ 0.09729916  0.17906019 -0.03943512 -0.9906947 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.19414303  0.13693446]]. Action = [[-0.08579591 -0.09527749 -0.2087555  -0.7995098 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 921 is [True, False, False, False, False, True]
Current timestep = 922. State = [[-0.19609508  0.13390206]]. Action = [[-0.08044223 -0.05398326 -0.13172388  0.436342  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 922 is [True, False, False, False, False, True]
Scene graph at timestep 922 is [True, False, False, False, False, True]
State prediction error at timestep 922 is tensor(7.2243e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of -1
Current timestep = 923. State = [[-0.19403006  0.13122928]]. Action = [[ 0.19091487  0.07571751 -0.23401293  0.5280316 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 923 is [True, False, False, False, False, True]
Current timestep = 924. State = [[-0.18470585  0.130726  ]]. Action = [[ 0.09393534 -0.04650888  0.03485802 -0.35082424]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 924 is [True, False, False, False, False, True]
Current timestep = 925. State = [[-0.18275368  0.11791568]]. Action = [[-0.17449711 -0.2262398   0.07942411 -0.04830503]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 925 is [True, False, False, False, False, True]
Scene graph at timestep 925 is [True, False, False, False, True, False]
State prediction error at timestep 925 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 925 of 1
Current timestep = 926. State = [[-0.17791481  0.09181866]]. Action = [[ 0.14513123 -0.1630813  -0.13442001 -0.7993584 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 926 is [True, False, False, False, True, False]
Current timestep = 927. State = [[-0.16309823  0.08635437]]. Action = [[ 0.24049026  0.16765815  0.05294788 -0.88726544]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 927 is [True, False, False, False, True, False]
Scene graph at timestep 927 is [True, False, False, False, True, False]
State prediction error at timestep 927 is tensor(5.7997e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of 1
Current timestep = 928. State = [[-0.14271362  0.08410572]]. Action = [[-0.02976871 -0.18966208 -0.03074355  0.07845187]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.13449673  0.0728109 ]]. Action = [[ 0.17714807  0.00595534 -0.20957094  0.4693811 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [True, False, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of 1
Current timestep = 930. State = [[-0.11902603  0.07519766]]. Action = [[ 0.20761046  0.12449729  0.08882949 -0.68715316]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 930 is [True, False, False, False, True, False]
Scene graph at timestep 930 is [True, False, False, False, True, False]
State prediction error at timestep 930 is tensor(4.3620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 930 of 1
Current timestep = 931. State = [[-0.09845883  0.0917698 ]]. Action = [[-0.13142525  0.09031534 -0.19799033  0.07263565]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 931 is [True, False, False, False, True, False]
Scene graph at timestep 931 is [True, False, False, False, True, False]
State prediction error at timestep 931 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 931 of -1
Current timestep = 932. State = [[-0.09951999  0.08859448]]. Action = [[-0.00494695 -0.18521112 -0.23036276 -0.7256225 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 932 is [True, False, False, False, True, False]
Current timestep = 933. State = [[-0.10557948  0.06547593]]. Action = [[-0.19570701 -0.22490592 -0.03890377 -0.37831473]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 933 is [True, False, False, False, True, False]
Scene graph at timestep 933 is [True, False, False, False, True, False]
State prediction error at timestep 933 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 1
Current timestep = 934. State = [[-0.12016392  0.05131944]]. Action = [[ 0.00917861  0.12628585  0.13744092 -0.398247  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 934 is [True, False, False, False, True, False]
Current timestep = 935. State = [[-0.11501335  0.04762813]]. Action = [[ 0.2115975  -0.15488893 -0.24338223  0.6412065 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.1037693   0.04771809]]. Action = [[ 0.14652562  0.17988276  0.2251611  -0.79459244]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.08717375  0.04689603]]. Action = [[ 0.04709432 -0.16461731 -0.23674725  0.78062046]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 937 is [True, False, False, False, True, False]
Scene graph at timestep 937 is [True, False, False, False, True, False]
State prediction error at timestep 937 is tensor(3.6957e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 937 of 1
Current timestep = 938. State = [[-0.08098901  0.03600719]]. Action = [[ 0.07742912 -0.03322916 -0.06590402 -0.80878234]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 938 is [True, False, False, False, True, False]
Scene graph at timestep 938 is [True, False, False, False, True, False]
State prediction error at timestep 938 is tensor(4.3766e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of 1
Current timestep = 939. State = [[-0.07399581  0.02198427]]. Action = [[-0.08797747 -0.1962739  -0.22186522  0.5639014 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 939 is [True, False, False, False, True, False]
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.0678654  -0.00625506]]. Action = [[ 0.07555708 -0.22195931 -0.06091167  0.85827804]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 940 is [True, False, False, False, True, False]
Scene graph at timestep 940 is [True, False, False, False, True, False]
State prediction error at timestep 940 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 940 of 1
Current timestep = 941. State = [[-0.0552647  -0.01248678]]. Action = [[ 0.24666035  0.2482948   0.07604137 -0.8027704 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 941 is [True, False, False, False, True, False]
Current timestep = 942. State = [[-0.14913788  0.05723558]]. Action = [[ 0.19188768  0.14832509  0.22973955 -0.9427307 ]]. Reward = [100.]
Curr episode timestep = 51
Scene graph at timestep 942 is [True, False, False, False, True, False]
Scene graph at timestep 942 is [True, False, False, False, True, False]
State prediction error at timestep 942 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 942 of 1
Current timestep = 943. State = [[-0.13432729  0.08075386]]. Action = [[-0.10361439  0.20177972 -0.18626054 -0.4791745 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.13682461  0.09067194]]. Action = [[ 0.08717084 -0.06114182  0.19299215 -0.6300949 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 944 is [True, False, False, False, True, False]
Current timestep = 945. State = [[-0.13580725  0.08931403]]. Action = [[-0.01204824 -0.02199394 -0.1614017   0.7031009 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 945 is [True, False, False, False, True, False]
Current timestep = 946. State = [[-0.13033801  0.07908402]]. Action = [[ 0.10291848 -0.14494857  0.17160636 -0.39875746]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.12313089  0.07124881]]. Action = [[-0.05873728  0.01986521 -0.12440225 -0.8628063 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of 1
Current timestep = 948. State = [[-0.11522552  0.05626256]]. Action = [[ 0.20306468 -0.20367952 -0.04689258  0.30848646]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 948 is [True, False, False, False, True, False]
Scene graph at timestep 948 is [True, False, False, False, True, False]
State prediction error at timestep 948 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of 1
Current timestep = 949. State = [[-0.09023357  0.04718259]]. Action = [[ 0.20909354  0.11633682  0.21194059 -0.10403591]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 949 is [True, False, False, False, True, False]
Scene graph at timestep 949 is [True, False, False, False, True, False]
State prediction error at timestep 949 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 949 of 1
Current timestep = 950. State = [[-0.06051754  0.0498357 ]]. Action = [[ 0.14055198 -0.07375824 -0.13592139 -0.5891507 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 950 is [True, False, False, False, True, False]
Current timestep = 951. State = [[-0.05361191  0.04648949]]. Action = [[-0.181777   -0.02604344  0.2232216  -0.3141502 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 951 is [True, False, False, False, True, False]
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(7.6554e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of 1
Current timestep = 952. State = [[-0.04935733  0.028664  ]]. Action = [[ 0.23807839 -0.22602008 -0.09591573 -0.59615284]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.22448993 -0.1109334 ]]. Action = [[ 0.1435619   0.09595019 -0.13060898  0.8010957 ]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 953 is [False, True, False, False, True, False]
Current timestep = 954. State = [[-0.21088289 -0.12372105]]. Action = [[ 0.18562573 -0.04311438 -0.07054274 -0.22242177]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 954 is [True, False, False, False, True, False]
Current timestep = 955. State = [[-0.19054243 -0.12671325]]. Action = [[ 0.09624201  0.05081439  0.03901556 -0.5784342 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.1698532  -0.13870125]]. Action = [[ 0.20939398 -0.22505628 -0.1915207   0.65275884]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 956 is [True, False, False, True, False, False]
Scene graph at timestep 956 is [True, False, False, True, False, False]
State prediction error at timestep 956 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of 1
Current timestep = 957. State = [[-0.1414348  -0.16675511]]. Action = [[ 0.1646297  -0.22357807  0.12599206 -0.9568588 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 957 is [True, False, False, True, False, False]
Scene graph at timestep 957 is [True, False, False, True, False, False]
State prediction error at timestep 957 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of -1
Current timestep = 958. State = [[-0.11689901 -0.18872522]]. Action = [[ 0.13047212 -0.03575501  0.15108839  0.5900296 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 958 is [True, False, False, True, False, False]
Current timestep = 959. State = [[-0.10819928 -0.19078499]]. Action = [[-0.08717871  0.05949444  0.04750943 -0.19671273]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 959 is [True, False, False, True, False, False]
Scene graph at timestep 959 is [True, False, False, True, False, False]
State prediction error at timestep 959 is tensor(9.1137e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[-0.10910857 -0.19002861]]. Action = [[0.0106799  0.01091561 0.22276244 0.49302173]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 960 is [True, False, False, True, False, False]
Scene graph at timestep 960 is [True, False, False, True, False, False]
State prediction error at timestep 960 is tensor(3.6478e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of -1
Current timestep = 961. State = [[-0.1030011  -0.18003277]]. Action = [[ 0.15609294  0.13335419 -0.15464096 -0.8313636 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 961 is [True, False, False, True, False, False]
Current timestep = 962. State = [[-0.08863515 -0.18149477]]. Action = [[ 0.13999254 -0.17934337 -0.02295496 -0.46596915]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 962 is [True, False, False, True, False, False]
Current timestep = 963. State = [[-0.06438955 -0.17909372]]. Action = [[ 0.21130848  0.17446294  0.194637   -0.68683106]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 963 is [True, False, False, True, False, False]
Current timestep = 964. State = [[-0.04080118 -0.18411987]]. Action = [[ 0.12900287 -0.21793485 -0.02769126  0.5437386 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 964 is [True, False, False, True, False, False]
Current timestep = 965. State = [[-0.0171316 -0.1824918]]. Action = [[ 0.18604219  0.21949154 -0.22875358  0.9188702 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 965 is [False, True, False, True, False, False]
Scene graph at timestep 965 is [False, True, False, True, False, False]
State prediction error at timestep 965 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of 1
Current timestep = 966. State = [[ 0.00781658 -0.16527456]]. Action = [[-0.11539592  0.12578326  0.11256629 -0.83660316]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 966 is [False, True, False, True, False, False]
Scene graph at timestep 966 is [False, True, False, True, False, False]
State prediction error at timestep 966 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[ 0.00966146 -0.14856313]]. Action = [[ 0.15962282  0.10265779 -0.03892307 -0.9128782 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 967 is [False, True, False, True, False, False]
Current timestep = 968. State = [[ 0.0184934  -0.14549603]]. Action = [[ 0.19944894 -0.11324248 -0.17043258  0.7967174 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 968 is [False, True, False, True, False, False]
Current timestep = 969. State = [[ 0.03217454 -0.14589716]]. Action = [[-0.190979    0.08706632  0.06361115 -0.5908218 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 969 is [False, True, False, True, False, False]
Current timestep = 970. State = [[ 0.03416952 -0.13141255]]. Action = [[ 0.1415124   0.19110501 -0.10477473  0.5550978 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 970 is [False, True, False, True, False, False]
Scene graph at timestep 970 is [False, True, False, True, False, False]
State prediction error at timestep 970 is tensor(1.5141e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[ 0.03967067 -0.11496256]]. Action = [[ 0.19835585  0.18113467  0.22733918 -0.135275  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 971 is [False, True, False, True, False, False]
Scene graph at timestep 971 is [False, True, False, False, True, False]
State prediction error at timestep 971 is tensor(7.7145e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[ 0.04030108 -0.11493322]]. Action = [[ 0.186741    0.0916732   0.13922161 -0.81782615]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 972 is [False, True, False, False, True, False]
Scene graph at timestep 972 is [False, True, False, False, True, False]
State prediction error at timestep 972 is tensor(3.2678e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 972 of 1
Current timestep = 973. State = [[ 0.04030108 -0.11493322]]. Action = [[0.18529731 0.22177625 0.00905731 0.0042125 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 973 is [False, True, False, False, True, False]
Scene graph at timestep 973 is [False, True, False, False, True, False]
State prediction error at timestep 973 is tensor(4.6599e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 973 of 1
Current timestep = 974. State = [[ 0.04459177 -0.10040145]]. Action = [[ 0.13278466  0.22641355 -0.19799398  0.77557063]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 974 is [False, True, False, False, True, False]
Current timestep = 975. State = [[ 0.05461792 -0.08437456]]. Action = [[ 0.12809417 -0.16844064 -0.02721559  0.29691815]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 975 is [False, True, False, False, True, False]
Current timestep = 976. State = [[ 0.05701587 -0.08214808]]. Action = [[ 0.09369811 -0.07527202  0.20942503  0.65296483]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 976 is [False, False, True, False, True, False]
Scene graph at timestep 976 is [False, False, True, False, True, False]
State prediction error at timestep 976 is tensor(7.7703e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 1
Current timestep = 977. State = [[ 0.05690279 -0.08151389]]. Action = [[0.18787163 0.07301086 0.07826248 0.85983706]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 977 is [False, False, True, False, True, False]
Scene graph at timestep 977 is [False, False, True, False, True, False]
State prediction error at timestep 977 is tensor(1.9160e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 977 of 1
Current timestep = 978. State = [[ 0.05616681 -0.07232925]]. Action = [[-0.01827015  0.15122062  0.15435544  0.6652738 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 978 is [False, False, True, False, True, False]
Current timestep = 979. State = [[ 0.05425182 -0.07277791]]. Action = [[-0.07308796 -0.18873534 -0.16932496  0.9264977 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 979 is [False, False, True, False, True, False]
Current timestep = 980. State = [[ 0.05280948 -0.08228596]]. Action = [[ 0.07513011 -0.06170315 -0.01649064  0.6919725 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 980 is [False, False, True, False, True, False]
Current timestep = 981. State = [[ 0.05264544 -0.09700479]]. Action = [[ 0.05014887 -0.2286107   0.08644417 -0.3956188 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 981 is [False, False, True, False, True, False]
Scene graph at timestep 981 is [False, False, True, False, True, False]
State prediction error at timestep 981 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 981 of -1
Current timestep = 982. State = [[ 0.05130896 -0.1195316 ]]. Action = [[-0.09151989 -0.07655695 -0.01234718  0.07986534]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 982 is [False, False, True, False, True, False]
Current timestep = 983. State = [[ 0.05064209 -0.12484962]]. Action = [[ 0.11404327 -0.19312008 -0.23523907  0.05600595]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 983 is [False, False, True, False, True, False]
Scene graph at timestep 983 is [False, False, True, False, True, False]
State prediction error at timestep 983 is tensor(4.7492e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 983 of -1
Current timestep = 984. State = [[ 0.04793059 -0.13984585]]. Action = [[-0.08690655 -0.18516614 -0.24057557 -0.3323847 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 984 is [False, False, True, False, True, False]
Scene graph at timestep 984 is [False, True, False, True, False, False]
State prediction error at timestep 984 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 984 of -1
Current timestep = 985. State = [[ 0.04497791 -0.152903  ]]. Action = [[ 0.22713208  0.23860526 -0.0881526  -0.47636092]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 985 is [False, True, False, True, False, False]
Current timestep = 986. State = [[ 0.04486483 -0.15296423]]. Action = [[0.2398678  0.20470893 0.23379129 0.06653583]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 986 is [False, True, False, True, False, False]
Current timestep = 987. State = [[ 0.04486483 -0.15296423]]. Action = [[ 0.22797674  0.05550635 -0.08885023 -0.6763874 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 987 is [False, True, False, True, False, False]
Current timestep = 988. State = [[ 0.04043582 -0.14352843]]. Action = [[-0.18332186  0.18995729  0.22826654  0.00328684]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 988 is [False, True, False, True, False, False]
Scene graph at timestep 988 is [False, True, False, True, False, False]
State prediction error at timestep 988 is tensor(1.2868e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 988 of 1
Current timestep = 989. State = [[ 0.03501927 -0.1218321 ]]. Action = [[ 0.15686488  0.16846621 -0.12188756 -0.09677821]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 989 is [False, True, False, True, False, False]
Scene graph at timestep 989 is [False, True, False, False, True, False]
State prediction error at timestep 989 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[ 0.03443097 -0.10972562]]. Action = [[-0.05329296 -0.05663703  0.02128461  0.70226955]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 990 is [False, True, False, False, True, False]
Current timestep = 991. State = [[ 0.03161569 -0.10675524]]. Action = [[-0.1661578   0.08752695 -0.08252722  0.7576369 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 991 is [False, True, False, False, True, False]
Scene graph at timestep 991 is [False, True, False, False, True, False]
State prediction error at timestep 991 is tensor(2.9742e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.19343267 -0.22684883]]. Action = [[ 0.10012025  0.24203598 -0.0123972  -0.02883887]]. Reward = [100.]
Curr episode timestep = 38
Scene graph at timestep 992 is [False, True, False, False, True, False]
Current timestep = 993. State = [[-0.18147199 -0.24987994]]. Action = [[0.07394406 0.05596387 0.2240268  0.44298398]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 993 is [True, False, False, True, False, False]
Scene graph at timestep 993 is [True, False, False, True, False, False]
State prediction error at timestep 993 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 1
Current timestep = 994. State = [[-0.1720833  -0.24655798]]. Action = [[ 0.00161994  0.05328864 -0.17133875 -0.09544283]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 994 is [True, False, False, True, False, False]
Scene graph at timestep 994 is [True, False, False, True, False, False]
State prediction error at timestep 994 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 994 of 1
Current timestep = 995. State = [[-0.17394215 -0.25700706]]. Action = [[-0.03977737 -0.24386647  0.04780194  0.44036293]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 995 is [True, False, False, True, False, False]
Current timestep = 996. State = [[-0.17762862 -0.2832938 ]]. Action = [[-0.00420655 -0.20047808  0.10524014 -0.66110325]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 996 is [True, False, False, True, False, False]
Scene graph at timestep 996 is [True, False, False, True, False, False]
State prediction error at timestep 996 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of -1
Current timestep = 997. State = [[-0.17062059 -0.2931748 ]]. Action = [[ 0.16151708  0.15257195 -0.21307608  0.8300637 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 997 is [True, False, False, True, False, False]
Current timestep = 998. State = [[-0.1623793  -0.28721625]]. Action = [[-0.11969256  0.04463759  0.10119084 -0.6691268 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 998 is [True, False, False, True, False, False]
Scene graph at timestep 998 is [True, False, False, True, False, False]
State prediction error at timestep 998 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 998 of 1
Current timestep = 999. State = [[-0.16268729 -0.28662482]]. Action = [[ 0.18050486 -0.22971284  0.12502947 -0.17970473]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 999 is [True, False, False, True, False, False]
Scene graph at timestep 999 is [True, False, False, True, False, False]
State prediction error at timestep 999 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.161075   -0.27487618]]. Action = [[ 0.05213386  0.1915403   0.13421786 -0.56839365]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1000 is [True, False, False, True, False, False]
Scene graph at timestep 1000 is [True, False, False, True, False, False]
State prediction error at timestep 1000 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.16442323 -0.26744774]]. Action = [[-0.14326778 -0.15783688  0.08625531 -0.8515174 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1001 is [True, False, False, True, False, False]
Scene graph at timestep 1001 is [True, False, False, True, False, False]
State prediction error at timestep 1001 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of -1
Current timestep = 1002. State = [[-0.17229919 -0.27406892]]. Action = [[-0.00703229  0.07959509 -0.21726447 -0.78997266]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1002 is [True, False, False, True, False, False]
Current timestep = 1003. State = [[-0.17276305 -0.27315032]]. Action = [[ 0.07291573 -0.07486382 -0.14896087  0.50691676]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1003 is [True, False, False, True, False, False]
Current timestep = 1004. State = [[-0.17382018 -0.2649279 ]]. Action = [[-0.11264554  0.19905138 -0.05678768  0.14696717]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1004 is [True, False, False, True, False, False]
Scene graph at timestep 1004 is [True, False, False, True, False, False]
State prediction error at timestep 1004 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of -1
Current timestep = 1005. State = [[-0.17318864 -0.2528115 ]]. Action = [[ 0.17216244 -0.03311679 -0.04055281 -0.25176495]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1005 is [True, False, False, True, False, False]
Current timestep = 1006. State = [[-0.15796047 -0.24672411]]. Action = [[ 0.21034679  0.06516492 -0.13152215  0.8221184 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1006 is [True, False, False, True, False, False]
Current timestep = 1007. State = [[-0.13086025 -0.24429186]]. Action = [[ 0.22907764 -0.03010395 -0.16295218 -0.4829799 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1007 is [True, False, False, True, False, False]
Scene graph at timestep 1007 is [True, False, False, True, False, False]
State prediction error at timestep 1007 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.09540201 -0.2360157 ]]. Action = [[ 0.19362658  0.17696577 -0.02640314  0.7423171 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1008 is [True, False, False, True, False, False]
Current timestep = 1009. State = [[-0.07007689 -0.21076623]]. Action = [[ 0.18365073  0.21167675 -0.14134806  0.2955233 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1009 is [True, False, False, True, False, False]
Current timestep = 1010. State = [[-0.04832549 -0.19648395]]. Action = [[ 0.11943996 -0.04482318  0.03490755 -0.04599118]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1010 is [True, False, False, True, False, False]
Scene graph at timestep 1010 is [False, True, False, True, False, False]
State prediction error at timestep 1010 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of 1
Current timestep = 1011. State = [[-0.0213443  -0.18292165]]. Action = [[0.24712604 0.2123251  0.2353167  0.94396865]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1011 is [False, True, False, True, False, False]
Scene graph at timestep 1011 is [False, True, False, True, False, False]
State prediction error at timestep 1011 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1011 of 1
Current timestep = 1012. State = [[ 0.00478679 -0.17595923]]. Action = [[-0.05769452 -0.16178067  0.09200403 -0.44976646]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1012 is [False, True, False, True, False, False]
Current timestep = 1013. State = [[ 0.00577602 -0.17219953]]. Action = [[ 0.09551761  0.18465772 -0.232617    0.38906336]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1013 is [False, True, False, True, False, False]
Current timestep = 1014. State = [[ 0.0077546  -0.15289263]]. Action = [[-0.06738341  0.1810208  -0.08226198 -0.8668225 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1014 is [False, True, False, True, False, False]
Scene graph at timestep 1014 is [False, True, False, True, False, False]
State prediction error at timestep 1014 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1014 of 1
Current timestep = 1015. State = [[ 0.01660777 -0.14234105]]. Action = [[ 0.2475667  -0.13755345  0.2052989   0.7345524 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1015 is [False, True, False, True, False, False]
Scene graph at timestep 1015 is [False, True, False, True, False, False]
State prediction error at timestep 1015 is tensor(1.4217e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of -1
Current timestep = 1016. State = [[ 0.04346011 -0.15356842]]. Action = [[ 0.06412867 -0.01057975  0.15084863  0.15839458]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1016 is [False, True, False, True, False, False]
Current timestep = 1017. State = [[ 0.04376687 -0.15353541]]. Action = [[ 0.17849019  0.17491347 -0.1480459   0.5980419 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1017 is [False, True, False, True, False, False]
Scene graph at timestep 1017 is [False, True, False, True, False, False]
State prediction error at timestep 1017 is tensor(1.3806e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of -1
Current timestep = 1018. State = [[ 0.04287604 -0.1475695 ]]. Action = [[-0.18396856  0.1255779  -0.21842451 -0.27812278]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1018 is [False, True, False, True, False, False]
Current timestep = 1019. State = [[ 0.04098254 -0.14359608]]. Action = [[-0.05787377 -0.03204745 -0.10996938  0.66161346]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1019 is [False, True, False, True, False, False]
Scene graph at timestep 1019 is [False, True, False, True, False, False]
State prediction error at timestep 1019 is tensor(4.7124e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1019 of 1
Current timestep = 1020. State = [[ 0.03868897 -0.14610694]]. Action = [[ 0.122621   -0.0352506  -0.06315033  0.02652562]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1020 is [False, True, False, True, False, False]
Current timestep = 1021. State = [[ 0.04096502 -0.14592886]]. Action = [[ 0.09489915 -0.01289357 -0.09304912 -0.5909034 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1021 is [False, True, False, True, False, False]
Scene graph at timestep 1021 is [False, True, False, True, False, False]
State prediction error at timestep 1021 is tensor(3.6663e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of -1
Current timestep = 1022. State = [[ 0.04659641 -0.14231628]]. Action = [[-0.13816452  0.09832686  0.08351612  0.89095354]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1022 is [False, True, False, True, False, False]
Scene graph at timestep 1022 is [False, True, False, True, False, False]
State prediction error at timestep 1022 is tensor(3.8827e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1022 of 1
Current timestep = 1023. State = [[ 0.04383166 -0.1453518 ]]. Action = [[-0.02513537 -0.12906985  0.12631288  0.7484107 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1023 is [False, True, False, True, False, False]
Scene graph at timestep 1023 is [False, True, False, True, False, False]
State prediction error at timestep 1023 is tensor(6.7411e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of -1
Current timestep = 1024. State = [[ 0.04138662 -0.15449966]]. Action = [[0.18658945 0.04361221 0.06102702 0.76498616]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1024 is [False, True, False, True, False, False]
Current timestep = 1025. State = [[ 0.03941609 -0.14851111]]. Action = [[-0.08843124  0.10580361 -0.14095825 -0.7996571 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1025 is [False, True, False, True, False, False]
Current timestep = 1026. State = [[ 0.0380262  -0.14330348]]. Action = [[ 0.24591511 -0.01685376  0.14409518 -0.05801296]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1026 is [False, True, False, True, False, False]
Scene graph at timestep 1026 is [False, True, False, True, False, False]
State prediction error at timestep 1026 is tensor(6.4939e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of 1
Current timestep = 1027. State = [[ 0.03814525 -0.12874739]]. Action = [[-0.00924955  0.23872897 -0.05362855  0.59768486]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1027 is [False, True, False, True, False, False]
Current timestep = 1028. State = [[ 0.03612088 -0.10860749]]. Action = [[0.19225621 0.08000696 0.06768382 0.86339355]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1028 is [False, True, False, True, False, False]
Scene graph at timestep 1028 is [False, True, False, False, True, False]
State prediction error at timestep 1028 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[-0.2179467   0.14937307]]. Action = [[ 0.08411744  0.20123702 -0.24088745  0.22464752]]. Reward = [100.]
Curr episode timestep = 36
Scene graph at timestep 1029 is [False, True, False, False, True, False]
Scene graph at timestep 1029 is [True, False, False, False, False, True]
State prediction error at timestep 1029 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of 1
Current timestep = 1030. State = [[-0.21025424  0.17363787]]. Action = [[-0.00319387  0.14832264  0.23534435  0.7843834 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1030 is [True, False, False, False, False, True]
Scene graph at timestep 1030 is [True, False, False, False, False, True]
State prediction error at timestep 1030 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[-0.21481715  0.18402536]]. Action = [[-0.02568664 -0.03950009  0.06230262 -0.83685416]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1031 is [True, False, False, False, False, True]
Scene graph at timestep 1031 is [True, False, False, False, False, True]
State prediction error at timestep 1031 is tensor(1.8784e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of 1
Current timestep = 1032. State = [[-0.2184535   0.18929312]]. Action = [[-0.058181    0.10291246 -0.07707728 -0.36068028]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1032 is [True, False, False, False, False, True]
Scene graph at timestep 1032 is [True, False, False, False, False, True]
State prediction error at timestep 1032 is tensor(2.8286e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1032 of 1
Current timestep = 1033. State = [[-0.21333219  0.19709712]]. Action = [[ 0.2129151   0.01649389  0.0687443  -0.06921917]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1033 is [True, False, False, False, False, True]
Scene graph at timestep 1033 is [True, False, False, False, False, True]
State prediction error at timestep 1033 is tensor(3.1681e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of 1
Current timestep = 1034. State = [[-0.19749562  0.21632695]]. Action = [[-0.15210685  0.2064237   0.03068885  0.7221098 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1034 is [True, False, False, False, False, True]
Current timestep = 1035. State = [[-0.2004389   0.21836159]]. Action = [[ 0.12422368 -0.19111189  0.1576078  -0.40395355]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1035 is [True, False, False, False, False, True]
Current timestep = 1036. State = [[-0.1913742   0.21276185]]. Action = [[ 0.10453323  0.07493493  0.16243261 -0.41960126]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1036 is [True, False, False, False, False, True]
Current timestep = 1037. State = [[-0.1698557   0.20865133]]. Action = [[ 0.21423423 -0.10652548  0.17631942 -0.27682483]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1037 is [True, False, False, False, False, True]
Scene graph at timestep 1037 is [True, False, False, False, False, True]
State prediction error at timestep 1037 is tensor(7.5195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of 1
Current timestep = 1038. State = [[-0.14784771  0.20258039]]. Action = [[ 0.06754196  0.02342281  0.0525699  -0.4236669 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1038 is [True, False, False, False, False, True]
Current timestep = 1039. State = [[-0.14423493  0.2140944 ]]. Action = [[-0.06177339  0.16054282  0.13065523 -0.40438414]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1039 is [True, False, False, False, False, True]
Scene graph at timestep 1039 is [True, False, False, False, False, True]
State prediction error at timestep 1039 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[-0.13677843  0.23050377]]. Action = [[ 0.19850123  0.12669256 -0.08286667  0.23019373]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1040 is [True, False, False, False, False, True]
Current timestep = 1041. State = [[-0.11358023  0.24497482]]. Action = [[ 0.14847782  0.09253177 -0.0005357   0.25693   ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1041 is [True, False, False, False, False, True]
Current timestep = 1042. State = [[-0.09047648  0.2571839 ]]. Action = [[ 0.16201836  0.04998779 -0.13519476  0.73234344]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1042 is [True, False, False, False, False, True]
Current timestep = 1043. State = [[-0.07522968  0.26284063]]. Action = [[-0.07911119 -0.04935093  0.09357134  0.40581918]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1043 is [True, False, False, False, False, True]
Scene graph at timestep 1043 is [True, False, False, False, False, True]
State prediction error at timestep 1043 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[-0.06666808  0.25490633]]. Action = [[ 0.21678078 -0.09584564  0.12596065 -0.6141987 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1044 is [True, False, False, False, False, True]
Scene graph at timestep 1044 is [True, False, False, False, False, True]
State prediction error at timestep 1044 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1044 of 1
Current timestep = 1045. State = [[-0.03662937  0.24198219]]. Action = [[ 0.24481612 -0.09445177 -0.14219368 -0.06166524]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1045 is [True, False, False, False, False, True]
Scene graph at timestep 1045 is [False, True, False, False, False, True]
State prediction error at timestep 1045 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1045 of 1
Current timestep = 1046. State = [[-0.00221615  0.2430551 ]]. Action = [[ 0.22542247  0.13940027 -0.16748138  0.04934418]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1046 is [False, True, False, False, False, True]
Current timestep = 1047. State = [[0.023996   0.23961727]]. Action = [[ 0.1607753  -0.21500552  0.0327166   0.22565258]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1047 is [False, True, False, False, False, True]
Current timestep = 1048. State = [[0.03843761 0.24102442]]. Action = [[-0.05334152  0.18672335 -0.21988189  0.86220455]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1048 is [False, True, False, False, False, True]
Current timestep = 1049. State = [[0.04109438 0.25929138]]. Action = [[ 0.06645724  0.18376428 -0.00894821  0.32094336]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1049 is [False, True, False, False, False, True]
Scene graph at timestep 1049 is [False, True, False, False, False, True]
State prediction error at timestep 1049 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[0.05007745 0.27547395]]. Action = [[ 0.17899442  0.04776359  0.07364279 -0.1103707 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1050 is [False, True, False, False, False, True]
Current timestep = 1051. State = [[0.05224845 0.2687036 ]]. Action = [[-0.09613843 -0.20829773  0.04924765  0.8386669 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1051 is [False, False, True, False, False, True]
Current timestep = 1052. State = [[0.05378793 0.26328775]]. Action = [[ 0.15187377 -0.05030456 -0.16999555 -0.24771565]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1052 is [False, False, True, False, False, True]
Scene graph at timestep 1052 is [False, False, True, False, False, True]
State prediction error at timestep 1052 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[0.05467054 0.25573248]]. Action = [[-0.09804145 -0.13793486  0.01422256  0.8281865 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1053 is [False, False, True, False, False, True]
Scene graph at timestep 1053 is [False, False, True, False, False, True]
State prediction error at timestep 1053 is tensor(7.4899e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1053 of 1
Current timestep = 1054. State = [[0.05463441 0.25137874]]. Action = [[ 0.23817217 -0.04290357  0.16814137  0.12133884]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1054 is [False, False, True, False, False, True]
Scene graph at timestep 1054 is [False, False, True, False, False, True]
State prediction error at timestep 1054 is tensor(7.8701e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1054 of -1
Current timestep = 1055. State = [[0.05463441 0.25137874]]. Action = [[ 0.13801104 -0.0107969   0.00727984  0.77626157]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1055 is [False, False, True, False, False, True]
Scene graph at timestep 1055 is [False, False, True, False, False, True]
State prediction error at timestep 1055 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[0.05463441 0.25137874]]. Action = [[ 0.18914002 -0.09003973 -0.20198596  0.9346132 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1056 is [False, False, True, False, False, True]
Current timestep = 1057. State = [[0.05463441 0.25137874]]. Action = [[ 0.17482948 -0.09881923 -0.03152016  0.35338664]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1057 is [False, False, True, False, False, True]
Scene graph at timestep 1057 is [False, False, True, False, False, True]
State prediction error at timestep 1057 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of 1
Current timestep = 1058. State = [[0.05463441 0.25137874]]. Action = [[ 0.18289167  0.08072558  0.15433353 -0.07221609]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1058 is [False, False, True, False, False, True]
Scene graph at timestep 1058 is [False, False, True, False, False, True]
State prediction error at timestep 1058 is tensor(2.8841e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1058 of 1
Current timestep = 1059. State = [[0.05335061 0.25380853]]. Action = [[-0.01104337  0.0721733   0.10916674 -0.3561682 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1059 is [False, False, True, False, False, True]
Current timestep = 1060. State = [[0.04902641 0.26038432]]. Action = [[-0.14158684  0.02218348 -0.10162592 -0.22382599]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1060 is [False, False, True, False, False, True]
Current timestep = 1061. State = [[0.03928633 0.2765125 ]]. Action = [[-0.1580978   0.14844626  0.08736449 -0.6303705 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1061 is [False, True, False, False, False, True]
Scene graph at timestep 1061 is [False, True, False, False, False, True]
State prediction error at timestep 1061 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[0.03393504 0.28240713]]. Action = [[ 0.16806144 -0.1848145   0.15893173 -0.8122553 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1062 is [False, True, False, False, False, True]
Current timestep = 1063. State = [[0.04134677 0.26896113]]. Action = [[ 0.24704683 -0.24042095  0.23810112  0.85288167]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1063 is [False, True, False, False, False, True]
Current timestep = 1064. State = [[0.04424465 0.26341638]]. Action = [[ 0.04326341 -0.03185201 -0.13276994  0.6595712 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1064 is [False, True, False, False, False, True]
Current timestep = 1065. State = [[0.04582733 0.26036724]]. Action = [[ 0.20115072 -0.14941105  0.03121328 -0.46440303]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1065 is [False, True, False, False, False, True]
Current timestep = 1066. State = [[0.04948924 0.25153053]]. Action = [[-0.09059918 -0.21737972  0.12235075 -0.9205987 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1066 is [False, True, False, False, False, True]
Current timestep = 1067. State = [[0.05423874 0.24068612]]. Action = [[ 0.16380757 -0.07635096  0.00663084  0.72440124]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1067 is [False, True, False, False, False, True]
Current timestep = 1068. State = [[0.05327455 0.23886453]]. Action = [[-0.10764933 -0.04209682  0.04153866  0.4991765 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1068 is [False, False, True, False, False, True]
Current timestep = 1069. State = [[0.05206904 0.23855554]]. Action = [[ 0.15295994 -0.189426   -0.16311713  0.09571016]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1069 is [False, False, True, False, False, True]
Current timestep = 1070. State = [[0.05191483 0.23847267]]. Action = [[ 0.14186233 -0.07417956  0.16346279  0.9170563 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1070 is [False, False, True, False, False, True]
Current timestep = 1071. State = [[0.05170224 0.23841725]]. Action = [[ 0.15369406 -0.05117923  0.04393369 -0.64316046]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1071 is [False, False, True, False, False, True]
Scene graph at timestep 1071 is [False, False, True, False, False, True]
State prediction error at timestep 1071 is tensor(5.7890e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1071 of 1
Current timestep = 1072. State = [[0.05528704 0.23060529]]. Action = [[ 0.04375091 -0.12762584 -0.11220033  0.5738567 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1072 is [False, False, True, False, False, True]
Scene graph at timestep 1072 is [False, False, True, False, False, True]
State prediction error at timestep 1072 is tensor(5.0698e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1072 of 1
Current timestep = 1073. State = [[0.06085737 0.21922009]]. Action = [[ 0.12340572 -0.19211408  0.03357419  0.7203133 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1073 is [False, False, True, False, False, True]
Scene graph at timestep 1073 is [False, False, True, False, False, True]
State prediction error at timestep 1073 is tensor(9.3795e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 1
Current timestep = 1074. State = [[0.06113089 0.21229821]]. Action = [[-0.21699125 -0.2228429  -0.00711632  0.14256394]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1074 is [False, False, True, False, False, True]
Current timestep = 1075. State = [[0.06165251 0.20371741]]. Action = [[ 0.17088789 -0.0013767   0.02933145  0.98358655]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1075 is [False, False, True, False, False, True]
Scene graph at timestep 1075 is [False, False, True, False, False, True]
State prediction error at timestep 1075 is tensor(3.8909e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of 1
Current timestep = 1076. State = [[0.06208808 0.20191497]]. Action = [[ 0.13558477 -0.05227894 -0.01965739  0.8575094 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1076 is [False, False, True, False, False, True]
Current timestep = 1077. State = [[0.06137299 0.20175886]]. Action = [[-0.06835872 -0.0273319  -0.2412195   0.13500392]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1077 is [False, False, True, False, False, True]
Scene graph at timestep 1077 is [False, False, True, False, False, True]
State prediction error at timestep 1077 is tensor(4.3272e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1077 of 1
Current timestep = 1078. State = [[0.06080677 0.20162588]]. Action = [[ 0.23408279  0.02577695 -0.08579646 -0.12199152]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1078 is [False, False, True, False, False, True]
Current timestep = 1079. State = [[0.05876093 0.20578544]]. Action = [[-0.00424854  0.10044652 -0.179441   -0.4052167 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1079 is [False, False, True, False, False, True]
Scene graph at timestep 1079 is [False, False, True, False, False, True]
State prediction error at timestep 1079 is tensor(3.9292e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of 1
Current timestep = 1080. State = [[0.05721682 0.20909464]]. Action = [[ 0.18636847 -0.17582604 -0.05927122 -0.9193592 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1080 is [False, False, True, False, False, True]
Current timestep = 1081. State = [[0.05721682 0.20909464]]. Action = [[ 0.12715465 -0.22943199  0.01716828 -0.05331606]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1081 is [False, False, True, False, False, True]
Scene graph at timestep 1081 is [False, False, True, False, False, True]
State prediction error at timestep 1081 is tensor(3.3567e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1081 of 1
Current timestep = 1082. State = [[0.05721682 0.20909464]]. Action = [[ 0.18479028 -0.17295825  0.21001694  0.4984082 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1082 is [False, False, True, False, False, True]
Current timestep = 1083. State = [[0.05483877 0.21062937]]. Action = [[-0.19710803 -0.0650408   0.2038877   0.59888244]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1083 is [False, False, True, False, False, True]
Current timestep = 1084. State = [[0.05359895 0.21215308]]. Action = [[ 0.1729064  -0.12725623  0.14217472 -0.9319853 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1084 is [False, False, True, False, False, True]
Scene graph at timestep 1084 is [False, False, True, False, False, True]
State prediction error at timestep 1084 is tensor(2.8900e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1084 of 1
Current timestep = 1085. State = [[0.05317364 0.21292835]]. Action = [[ 0.1073868  -0.19779038  0.0219883  -0.08178276]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1085 is [False, False, True, False, False, True]
Scene graph at timestep 1085 is [False, False, True, False, False, True]
State prediction error at timestep 1085 is tensor(5.4072e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[0.05219515 0.21279351]]. Action = [[-0.0666886  -0.04991788  0.03071442  0.8920922 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1086 is [False, False, True, False, False, True]
Current timestep = 1087. State = [[0.05242046 0.2106556 ]]. Action = [[ 0.06036216 -0.01854476 -0.1779615   0.19249666]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1087 is [False, False, True, False, False, True]
Scene graph at timestep 1087 is [False, False, True, False, False, True]
State prediction error at timestep 1087 is tensor(3.3413e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of 1
Current timestep = 1088. State = [[0.05286321 0.20962879]]. Action = [[ 0.13254464 -0.22807875  0.07963115 -0.9136841 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1088 is [False, False, True, False, False, True]
Scene graph at timestep 1088 is [False, False, True, False, False, True]
State prediction error at timestep 1088 is tensor(6.2729e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1088 of 1
Current timestep = 1089. State = [[0.05286321 0.20962879]]. Action = [[ 0.19597632 -0.12309311  0.16692978  0.31028867]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1089 is [False, False, True, False, False, True]
Scene graph at timestep 1089 is [False, False, True, False, False, True]
State prediction error at timestep 1089 is tensor(2.3465e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of 1
Current timestep = 1090. State = [[0.04778594 0.22049507]]. Action = [[-0.06332621  0.21622539  0.06908876  0.53487754]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1090 is [False, False, True, False, False, True]
Scene graph at timestep 1090 is [False, True, False, False, False, True]
State prediction error at timestep 1090 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1090 of -1
Current timestep = 1091. State = [[0.03627551 0.23746096]]. Action = [[-0.24379432 -0.17008188 -0.16271229 -0.38377202]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1091 is [False, True, False, False, False, True]
Scene graph at timestep 1091 is [False, True, False, False, False, True]
State prediction error at timestep 1091 is tensor(2.8693e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of -1
Current timestep = 1092. State = [[0.02875635 0.24319331]]. Action = [[-0.14786269  0.07525134 -0.0712387   0.81567395]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1092 is [False, True, False, False, False, True]
Current timestep = 1093. State = [[0.0294123  0.23891029]]. Action = [[ 0.1736843  -0.17117146 -0.0843344   0.13504934]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1093 is [False, True, False, False, False, True]
Scene graph at timestep 1093 is [False, True, False, False, False, True]
State prediction error at timestep 1093 is tensor(7.3123e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 1
Current timestep = 1094. State = [[-0.09205691  0.12228282]]. Action = [[ 0.22797847 -0.20750988 -0.16453207  0.72274935]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1094 is [False, True, False, False, False, True]
Current timestep = 1095. State = [[-0.07676546  0.0931274 ]]. Action = [[ 0.17144775 -0.19887671 -0.23564602 -0.43318468]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Scene graph at timestep 1095 is [True, False, False, False, True, False]
State prediction error at timestep 1095 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1095 of 1
Current timestep = 1096. State = [[-0.06745788  0.07122274]]. Action = [[0.00158221 0.05463678 0.07992133 0.8132303 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Scene graph at timestep 1096 is [True, False, False, False, True, False]
State prediction error at timestep 1096 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1096 of 1
Current timestep = 1097. State = [[-0.07195851  0.08457764]]. Action = [[-0.17107184  0.18632981  0.21436745  0.17089486]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Scene graph at timestep 1097 is [True, False, False, False, True, False]
State prediction error at timestep 1097 is tensor(3.8913e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of 1
Current timestep = 1098. State = [[-0.08303717  0.11437989]]. Action = [[-0.07111399  0.21527612  0.19314718  0.8241627 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Scene graph at timestep 1098 is [True, False, False, False, True, False]
State prediction error at timestep 1098 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1098 of -1
Current timestep = 1099. State = [[-0.09236229  0.13074145]]. Action = [[-0.08057633 -0.07854548 -0.22866671 -0.5172944 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Current timestep = 1100. State = [[-0.09029786  0.12253746]]. Action = [[ 0.13148478 -0.07996127 -0.10768908 -0.17443281]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1100 is [True, False, False, False, False, True]
Scene graph at timestep 1100 is [True, False, False, False, True, False]
State prediction error at timestep 1100 is tensor(6.2820e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of -1
Current timestep = 1101. State = [[-0.08683132  0.10720058]]. Action = [[-0.02771392 -0.16414277 -0.23466161  0.85446274]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Scene graph at timestep 1101 is [True, False, False, False, True, False]
State prediction error at timestep 1101 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of 1
Current timestep = 1102. State = [[-0.08769434  0.09731114]]. Action = [[-0.13538864  0.04057634 -0.19318426  0.64961314]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Current timestep = 1103. State = [[-0.09171673  0.09917556]]. Action = [[ 0.13464838  0.07802504  0.20572343 -0.49555886]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Scene graph at timestep 1103 is [True, False, False, False, True, False]
State prediction error at timestep 1103 is tensor(4.3995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of -1
Current timestep = 1104. State = [[-0.09209438  0.11509303]]. Action = [[ 0.02399552  0.22941518 -0.1391014  -0.7210263 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Current timestep = 1105. State = [[-0.09889216  0.1407729 ]]. Action = [[-0.06543446  0.16396087 -0.07556094 -0.24420005]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Current timestep = 1106. State = [[-0.09780917  0.15004295]]. Action = [[ 0.16188973 -0.09522812 -0.04887643 -0.6144368 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1106 is [True, False, False, False, False, True]
Current timestep = 1107. State = [[-0.08645061  0.13996288]]. Action = [[ 0.17635733 -0.11751711  0.12395531  0.7631755 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1107 is [True, False, False, False, False, True]
Scene graph at timestep 1107 is [True, False, False, False, False, True]
State prediction error at timestep 1107 is tensor(1.6283e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1107 of 1
Current timestep = 1108. State = [[-0.05771873  0.12191103]]. Action = [[ 0.22149575 -0.15734547 -0.02223279 -0.83036715]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1108 is [True, False, False, False, False, True]
Current timestep = 1109. State = [[-0.20863332 -0.20598182]]. Action = [[ 0.24564618  0.1774165  -0.09879255  0.20624721]]. Reward = [100.]
Curr episode timestep = 79
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Current timestep = 1110. State = [[-0.19978926 -0.22014004]]. Action = [[ 0.05999488  0.15906033 -0.17223957 -0.0306285 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1110 is [True, False, False, True, False, False]
Current timestep = 1111. State = [[-0.19742227 -0.2041017 ]]. Action = [[-0.12794797  0.19863516  0.17385244 -0.3524027 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1111 is [True, False, False, True, False, False]
Current timestep = 1112. State = [[-0.20780464 -0.1818894 ]]. Action = [[-0.20538163  0.15044326  0.06334037  0.9296453 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1112 is [True, False, False, True, False, False]
Current timestep = 1113. State = [[-0.2179798  -0.15615752]]. Action = [[ 0.0556975   0.21171483 -0.14606704 -0.9270344 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1113 is [True, False, False, True, False, False]
Scene graph at timestep 1113 is [True, False, False, True, False, False]
State prediction error at timestep 1113 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of 1
Current timestep = 1114. State = [[-0.22486578 -0.12392474]]. Action = [[-0.13949712  0.1960553  -0.1831809  -0.91378343]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1114 is [True, False, False, True, False, False]
Scene graph at timestep 1114 is [True, False, False, False, True, False]
State prediction error at timestep 1114 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of -1
Current timestep = 1115. State = [[-0.23594806 -0.10137678]]. Action = [[ 0.07716042  0.06942225  0.18309215 -0.73720014]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1115 is [True, False, False, False, True, False]
Current timestep = 1116. State = [[-0.23308384 -0.0953267 ]]. Action = [[ 0.07035512  0.02412188 -0.14188881 -0.14034033]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1116 is [True, False, False, False, True, False]
Scene graph at timestep 1116 is [True, False, False, False, True, False]
State prediction error at timestep 1116 is tensor(7.8552e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of -1
Current timestep = 1117. State = [[-0.23118515 -0.09701862]]. Action = [[-0.07573968 -0.11652929 -0.1641438   0.31777096]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1117 is [True, False, False, False, True, False]
Scene graph at timestep 1117 is [True, False, False, False, True, False]
State prediction error at timestep 1117 is tensor(3.9523e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of 1
Current timestep = 1118. State = [[-0.23298296 -0.10155962]]. Action = [[ 0.0684154   0.07250524 -0.09283054  0.7535546 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1118 is [True, False, False, False, True, False]
Current timestep = 1119. State = [[-0.23280558 -0.09217356]]. Action = [[-0.03588486  0.10484314  0.1252827  -0.7747416 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1119 is [True, False, False, False, True, False]
Scene graph at timestep 1119 is [True, False, False, False, True, False]
State prediction error at timestep 1119 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of 1
Current timestep = 1120. State = [[-0.23354048 -0.07745739]]. Action = [[ 0.00159866  0.08292931 -0.13360432  0.813123  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1120 is [True, False, False, False, True, False]
Scene graph at timestep 1120 is [True, False, False, False, True, False]
State prediction error at timestep 1120 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1120 of 1
Current timestep = 1121. State = [[-0.22892542 -0.07720397]]. Action = [[ 0.10773548 -0.13831927 -0.15811026 -0.9556701 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1121 is [True, False, False, False, True, False]
Current timestep = 1122. State = [[-0.21855208 -0.08874021]]. Action = [[ 0.10738409 -0.12428772  0.16427779  0.844795  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1122 is [True, False, False, False, True, False]
Current timestep = 1123. State = [[-0.20370851 -0.08946558]]. Action = [[0.08572632 0.18078035 0.00916746 0.47322094]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1123 is [True, False, False, False, True, False]
Scene graph at timestep 1123 is [True, False, False, False, True, False]
State prediction error at timestep 1123 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1123 of 1
Current timestep = 1124. State = [[-0.19782478 -0.06975849]]. Action = [[-0.06659043  0.19319904 -0.03277215 -0.45808792]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Scene graph at timestep 1124 is [True, False, False, False, True, False]
State prediction error at timestep 1124 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of 1
Current timestep = 1125. State = [[-0.19186582 -0.05089027]]. Action = [[0.19694677 0.02400774 0.07668379 0.87288713]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Current timestep = 1126. State = [[-0.18290716 -0.0426844 ]]. Action = [[-0.09766828  0.11754206 -0.08633432  0.7649416 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Scene graph at timestep 1126 is [True, False, False, False, True, False]
State prediction error at timestep 1126 is tensor(4.0367e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1126 of 1
Current timestep = 1127. State = [[-0.19210976 -0.02013521]]. Action = [[-0.17211352  0.20261931  0.20047289 -0.70824844]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1127 is [True, False, False, False, True, False]
Scene graph at timestep 1127 is [True, False, False, False, True, False]
State prediction error at timestep 1127 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1127 of -1
Current timestep = 1128. State = [[-0.20188227 -0.00534201]]. Action = [[-0.03477357 -0.09813324 -0.12018117  0.37281406]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1128 is [True, False, False, False, True, False]
Current timestep = 1129. State = [[-0.20135269 -0.01247921]]. Action = [[ 0.12400988 -0.03283393 -0.01707532  0.04139066]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1129 is [True, False, False, False, True, False]
Scene graph at timestep 1129 is [True, False, False, False, True, False]
State prediction error at timestep 1129 is tensor(3.4882e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of -1
Current timestep = 1130. State = [[-0.2003881  -0.01886199]]. Action = [[-0.08230296 -0.06937619 -0.19170888 -0.91845626]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1130 is [True, False, False, False, True, False]
Scene graph at timestep 1130 is [True, False, False, False, True, False]
State prediction error at timestep 1130 is tensor(1.7295e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of 1
Current timestep = 1131. State = [[-0.20230773 -0.02432021]]. Action = [[-0.00589013  0.05702978 -0.01675363 -0.23010945]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1131 is [True, False, False, False, True, False]
Current timestep = 1132. State = [[-0.206719   -0.03174832]]. Action = [[-0.11688685 -0.17278557  0.08153132 -0.27211756]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1132 is [True, False, False, False, True, False]
Scene graph at timestep 1132 is [True, False, False, False, True, False]
State prediction error at timestep 1132 is tensor(5.6340e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of -1
Current timestep = 1133. State = [[-0.21389145 -0.02960761]]. Action = [[ 0.11984974  0.23144692 -0.10073951  0.08706737]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1133 is [True, False, False, False, True, False]
Scene graph at timestep 1133 is [True, False, False, False, True, False]
State prediction error at timestep 1133 is tensor(3.7292e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 1
Current timestep = 1134. State = [[-0.217878   -0.00288687]]. Action = [[-0.20310928  0.16906947  0.1000489   0.7545583 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1134 is [True, False, False, False, True, False]
Current timestep = 1135. State = [[-0.22976024 -0.00073763]]. Action = [[-0.09254536 -0.18770738 -0.1303111   0.5275959 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1135 is [True, False, False, False, True, False]
Scene graph at timestep 1135 is [True, False, False, False, True, False]
State prediction error at timestep 1135 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of -1
Current timestep = 1136. State = [[-0.2306711  -0.02558075]]. Action = [[ 0.23253322 -0.2135641  -0.18675499  0.27420712]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1136 is [True, False, False, False, True, False]
Scene graph at timestep 1136 is [True, False, False, False, True, False]
State prediction error at timestep 1136 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of 1
Current timestep = 1137. State = [[-0.22922829 -0.05545079]]. Action = [[-0.22712632 -0.23862346 -0.14704552  0.03797972]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1137 is [True, False, False, False, True, False]
Current timestep = 1138. State = [[-0.2405172  -0.07722689]]. Action = [[-0.07170314 -0.05908573  0.0174489   0.76527643]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1138 is [True, False, False, False, True, False]
Current timestep = 1139. State = [[-0.24715185 -0.072881  ]]. Action = [[-0.0014783   0.22343403 -0.03324793 -0.43593633]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1139 is [True, False, False, False, True, False]
Current timestep = 1140. State = [[-0.24799994 -0.05082066]]. Action = [[ 0.10509792  0.16472623 -0.06387433  0.5163306 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1140 is [True, False, False, False, True, False]
Scene graph at timestep 1140 is [True, False, False, False, True, False]
State prediction error at timestep 1140 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1140 of -1
Current timestep = 1141. State = [[-0.23549923 -0.03692828]]. Action = [[ 0.20443553 -0.0611853   0.06067842 -0.6285808 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1141 is [True, False, False, False, True, False]
Scene graph at timestep 1141 is [True, False, False, False, True, False]
State prediction error at timestep 1141 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of 1
Current timestep = 1142. State = [[-0.22458546 -0.03145247]]. Action = [[-0.0277465   0.11363393 -0.04230921  0.6105343 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1142 is [True, False, False, False, True, False]
Current timestep = 1143. State = [[-0.2156847  -0.01939339]]. Action = [[ 0.17780599  0.12339228 -0.22703747 -0.9836782 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1143 is [True, False, False, False, True, False]
Current timestep = 1144. State = [[-0.21250679 -0.00130339]]. Action = [[-0.22707608  0.13103732  0.2050955   0.61795676]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1144 is [True, False, False, False, True, False]
Current timestep = 1145. State = [[-0.21145535 -0.00371459]]. Action = [[ 0.21278381 -0.23953287  0.04948276 -0.36425686]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1145 is [True, False, False, False, True, False]
Current timestep = 1146. State = [[-0.20276758 -0.0121215 ]]. Action = [[ 0.05794904  0.07706252  0.09419072 -0.9610932 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1146 is [True, False, False, False, True, False]
Current timestep = 1147. State = [[-0.19239102 -0.00936754]]. Action = [[ 0.09043807  0.02508646 -0.18778546  0.01604426]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1147 is [True, False, False, False, True, False]
Current timestep = 1148. State = [[-0.17359681 -0.02022195]]. Action = [[ 0.20200127 -0.23624673  0.07123739 -0.6165711 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1148 is [True, False, False, False, True, False]
Current timestep = 1149. State = [[-0.1455441 -0.0331999]]. Action = [[ 0.23455173  0.01664412 -0.1989496   0.24217117]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1149 is [True, False, False, False, True, False]
Scene graph at timestep 1149 is [True, False, False, False, True, False]
State prediction error at timestep 1149 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of -1
Current timestep = 1150. State = [[-0.11750699 -0.03429331]]. Action = [[ 0.04616237 -0.01820721 -0.01748165  0.23603976]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1150 is [True, False, False, False, True, False]
Current timestep = 1151. State = [[-0.11254355 -0.03006717]]. Action = [[ 0.00357419  0.13094425 -0.05153452 -0.87859154]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1151 is [True, False, False, False, True, False]
Scene graph at timestep 1151 is [True, False, False, False, True, False]
State prediction error at timestep 1151 is tensor(4.8613e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of 1
Current timestep = 1152. State = [[-0.10357992 -0.03132143]]. Action = [[ 0.18411592 -0.14083906  0.23544341 -0.42190945]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1152 is [True, False, False, False, True, False]
Scene graph at timestep 1152 is [True, False, False, False, True, False]
State prediction error at timestep 1152 is tensor(2.1792e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1152 of 1
Current timestep = 1153. State = [[-0.07888266 -0.04478569]]. Action = [[ 0.14571518 -0.11180168 -0.24776098  0.19539356]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1153 is [True, False, False, False, True, False]
Current timestep = 1154. State = [[-0.06059817 -0.05054494]]. Action = [[ 0.10476685  0.03681231 -0.0428386   0.60087585]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1154 is [True, False, False, False, True, False]
Scene graph at timestep 1154 is [True, False, False, False, True, False]
State prediction error at timestep 1154 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1154 of -1
Current timestep = 1155. State = [[-0.0407029  -0.04055939]]. Action = [[ 0.17661357  0.16009483 -0.1553363   0.8592477 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1155 is [True, False, False, False, True, False]
Scene graph at timestep 1155 is [False, True, False, False, True, False]
State prediction error at timestep 1155 is tensor(4.5314e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 1
Current timestep = 1156. State = [[-0.20263539 -0.16998304]]. Action = [[ 0.18594038 -0.11954194 -0.16413124  0.5703645 ]]. Reward = [100.]
Curr episode timestep = 46
Scene graph at timestep 1156 is [False, True, False, False, True, False]
Scene graph at timestep 1156 is [True, False, False, True, False, False]
State prediction error at timestep 1156 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of -1
Current timestep = 1157. State = [[-0.19368199 -0.18816933]]. Action = [[ 0.02178559  0.00572026  0.06781346 -0.09437466]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1157 is [True, False, False, True, False, False]
Scene graph at timestep 1157 is [True, False, False, True, False, False]
State prediction error at timestep 1157 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of 1
Current timestep = 1158. State = [[-0.1916385  -0.17734398]]. Action = [[-0.02608193  0.225959   -0.09410977 -0.49402702]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1158 is [True, False, False, True, False, False]
Current timestep = 1159. State = [[-0.18248317 -0.15074795]]. Action = [[ 0.2083255   0.2285707   0.04057348 -0.84236455]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1159 is [True, False, False, True, False, False]
Current timestep = 1160. State = [[-0.16363102 -0.12770614]]. Action = [[ 0.14883399  0.0738706  -0.06766528  0.726037  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1160 is [True, False, False, True, False, False]
Scene graph at timestep 1160 is [True, False, False, True, False, False]
State prediction error at timestep 1160 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.13587463 -0.10363739]]. Action = [[0.18518004 0.2258777  0.19711775 0.49967432]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1161 is [True, False, False, True, False, False]
Scene graph at timestep 1161 is [True, False, False, False, True, False]
State prediction error at timestep 1161 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1161 of 1
Current timestep = 1162. State = [[-0.11433626 -0.07030537]]. Action = [[ 0.12751001  0.23451555  0.14946657 -0.8381869 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Scene graph at timestep 1162 is [True, False, False, False, True, False]
State prediction error at timestep 1162 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1162 of 1
Current timestep = 1163. State = [[-0.09043752 -0.05721059]]. Action = [[ 0.22272748 -0.12354316  0.1271618   0.6185994 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1163 is [True, False, False, False, True, False]
Current timestep = 1164. State = [[-0.07642875 -0.05945234]]. Action = [[-0.1955103   0.06893891 -0.09250304  0.5896151 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1164 is [True, False, False, False, True, False]
Current timestep = 1165. State = [[-0.07919403 -0.072239  ]]. Action = [[ 0.01173425 -0.22851458  0.10986167  0.9663751 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Current timestep = 1166. State = [[-0.07705539 -0.07109885]]. Action = [[0.16305876 0.242244   0.03455538 0.63218665]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(8.3476e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of 1
Current timestep = 1167. State = [[-0.07422875 -0.04421138]]. Action = [[-0.15032114  0.2393434   0.21530133  0.04266226]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Scene graph at timestep 1167 is [True, False, False, False, True, False]
State prediction error at timestep 1167 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of 1
Current timestep = 1168. State = [[-0.08526646 -0.01009817]]. Action = [[-0.16965662  0.17517313  0.01381817  0.01950061]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1168 is [True, False, False, False, True, False]
Current timestep = 1169. State = [[-0.08609369 -0.00407288]]. Action = [[ 0.22817218 -0.12921388 -0.11161301  0.6434636 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Scene graph at timestep 1169 is [True, False, False, False, True, False]
State prediction error at timestep 1169 is tensor(4.3007e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 1
Current timestep = 1170. State = [[-0.08479208 -0.00097665]]. Action = [[-0.01179132  0.16364837  0.07661572  0.9211283 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Scene graph at timestep 1170 is [True, False, False, False, True, False]
State prediction error at timestep 1170 is tensor(9.9184e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of 1
Current timestep = 1171. State = [[-0.08038729  0.01456616]]. Action = [[ 0.11907205  0.09879917 -0.23004465  0.35422158]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Scene graph at timestep 1171 is [True, False, False, False, True, False]
State prediction error at timestep 1171 is tensor(6.4560e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of 1
Current timestep = 1172. State = [[-0.05931303  0.02732836]]. Action = [[0.18505672 0.0892913  0.11803809 0.3688054 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Scene graph at timestep 1172 is [True, False, False, False, True, False]
State prediction error at timestep 1172 is tensor(8.9099e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.23092142  0.01817516]]. Action = [[ 0.22142008  0.10562477  0.09242159 -0.15551937]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Current timestep = 1174. State = [[-0.22437847  0.02251262]]. Action = [[-0.03014781  0.0312025  -0.2120543   0.66246164]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Current timestep = 1175. State = [[-0.225811    0.02115789]]. Action = [[-0.09053068 -0.10771826 -0.02801794 -0.80651444]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Scene graph at timestep 1175 is [True, False, False, False, True, False]
State prediction error at timestep 1175 is tensor(4.4642e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.22892803  0.0156592 ]]. Action = [[-0.05485131 -0.03361736  0.23777753 -0.68591934]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Scene graph at timestep 1176 is [True, False, False, False, True, False]
State prediction error at timestep 1176 is tensor(3.8640e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of 1
Current timestep = 1177. State = [[-0.23234795  0.01100436]]. Action = [[ 0.12702104 -0.0217797   0.17124057 -0.911844  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Current timestep = 1178. State = [[-0.23151238  0.01243545]]. Action = [[-0.02678253  0.07483533 -0.0371813   0.08051765]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Current timestep = 1179. State = [[-0.23028143  0.02808644]]. Action = [[ 0.0594843   0.22668666  0.18844295 -0.84640384]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Current timestep = 1180. State = [[-0.231607    0.03548147]]. Action = [[-0.1689179  -0.15144554 -0.00992671 -0.84331405]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Scene graph at timestep 1180 is [True, False, False, False, True, False]
State prediction error at timestep 1180 is tensor(4.6777e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1180 of 1
Current timestep = 1181. State = [[-0.23052135  0.01807395]]. Action = [[ 0.1624608  -0.2127401   0.14174914  0.8479686 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(6.5880e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of 1
Current timestep = 1182. State = [[-0.22765452  0.00494075]]. Action = [[-0.01485971  0.09572923 -0.1385541  -0.89919174]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Scene graph at timestep 1182 is [True, False, False, False, True, False]
State prediction error at timestep 1182 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of 1
Current timestep = 1183. State = [[-0.21881603 -0.00411469]]. Action = [[ 0.16645193 -0.22789167 -0.12710434 -0.6082188 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Scene graph at timestep 1183 is [True, False, False, False, True, False]
State prediction error at timestep 1183 is tensor(6.5083e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1183 of 1
Current timestep = 1184. State = [[-0.19660312 -0.02008658]]. Action = [[ 0.18343753 -0.04612398 -0.15404515  0.81838834]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.16959843 -0.01282673]]. Action = [[ 0.2395863   0.19836456 -0.10503525  0.7064905 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Current timestep = 1186. State = [[-0.1526013  -0.00819236]]. Action = [[-0.17243342 -0.08576271 -0.0226534   0.88846934]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Scene graph at timestep 1186 is [True, False, False, False, True, False]
State prediction error at timestep 1186 is tensor(5.8156e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1186 of 1
Current timestep = 1187. State = [[-0.15204102 -0.02362655]]. Action = [[ 0.1104342  -0.21021733 -0.11353818 -0.00506526]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Current timestep = 1188. State = [[-0.14057131 -0.02641696]]. Action = [[0.23437512 0.17143703 0.10454556 0.3559779 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Scene graph at timestep 1188 is [True, False, False, False, True, False]
State prediction error at timestep 1188 is tensor(4.8631e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1188 of 1
Current timestep = 1189. State = [[-0.11884197 -0.0093293 ]]. Action = [[ 0.01026058  0.15969473  0.03846133 -0.82337576]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Scene graph at timestep 1189 is [True, False, False, False, True, False]
State prediction error at timestep 1189 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1189 of 1
Current timestep = 1190. State = [[-0.10868695  0.01172026]]. Action = [[ 0.22108915  0.13105237  0.23026043 -0.33410132]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Current timestep = 1191. State = [[-0.08024237  0.01627268]]. Action = [[ 0.14609963 -0.0720861  -0.20848875  0.72104096]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Scene graph at timestep 1191 is [True, False, False, False, True, False]
State prediction error at timestep 1191 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of 1
Current timestep = 1192. State = [[-0.05972017  0.00140199]]. Action = [[ 0.09052557 -0.22049642  0.20643267 -0.50596577]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Current timestep = 1193. State = [[-0.05383115 -0.02573606]]. Action = [[-0.11521271 -0.20789309 -0.14181432 -0.4105237 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1193 is [True, False, False, False, True, False]
Current timestep = 1194. State = [[-0.05506443 -0.05445665]]. Action = [[ 0.04434147 -0.21168692  0.10252401 -0.15575927]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Scene graph at timestep 1194 is [True, False, False, False, True, False]
State prediction error at timestep 1194 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of 1
Current timestep = 1195. State = [[-0.05699244 -0.06317096]]. Action = [[-0.18345222  0.23017639 -0.24283642 -0.4161821 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Scene graph at timestep 1195 is [True, False, False, False, True, False]
State prediction error at timestep 1195 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1195 of 1
Current timestep = 1196. State = [[-0.06001954 -0.03282777]]. Action = [[ 0.1624147   0.23605508 -0.1522113  -0.5494377 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Scene graph at timestep 1196 is [True, False, False, False, True, False]
State prediction error at timestep 1196 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1196 of 1
Current timestep = 1197. State = [[-0.04973923 -0.00890045]]. Action = [[0.21048951 0.09308034 0.00793508 0.22139943]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Current timestep = 1198. State = [[-0.16236064  0.16157398]]. Action = [[ 0.23495138  0.12205431  0.00138199 -0.7146097 ]]. Reward = [100.]
Curr episode timestep = 24
Scene graph at timestep 1198 is [False, True, False, False, True, False]
Current timestep = 1199. State = [[-0.13853756  0.18858841]]. Action = [[ 0.15931049  0.13975334 -0.08815554  0.96504855]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1199 is [True, False, False, False, False, True]
Scene graph at timestep 1199 is [True, False, False, False, False, True]
State prediction error at timestep 1199 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of 1
Current timestep = 1200. State = [[-0.11942682  0.19497643]]. Action = [[-0.07955843 -0.16113056  0.21611336  0.81410885]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1200 is [True, False, False, False, False, True]
Scene graph at timestep 1200 is [True, False, False, False, False, True]
State prediction error at timestep 1200 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of 1
Current timestep = 1201. State = [[-0.11559206  0.17269859]]. Action = [[-0.00624819 -0.22519772  0.10463768 -0.30414128]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1201 is [True, False, False, False, False, True]
Scene graph at timestep 1201 is [True, False, False, False, False, True]
State prediction error at timestep 1201 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1201 of 1
Current timestep = 1202. State = [[-0.10910285  0.15957324]]. Action = [[ 0.23539749  0.10343641 -0.06528984  0.1451106 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1202 is [True, False, False, False, False, True]
Scene graph at timestep 1202 is [True, False, False, False, False, True]
State prediction error at timestep 1202 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1202 of 1
Current timestep = 1203. State = [[-0.09059457  0.15039793]]. Action = [[ 0.1871573  -0.20437802 -0.20827617 -0.05884337]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1203 is [True, False, False, False, False, True]
Current timestep = 1204. State = [[-0.06644522  0.13145764]]. Action = [[ 0.19718537 -0.11942942  0.23551425  0.0812577 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1204 is [True, False, False, False, False, True]
Scene graph at timestep 1204 is [True, False, False, False, False, True]
State prediction error at timestep 1204 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 1
Current timestep = 1205. State = [[-0.21932222  0.21858338]]. Action = [[ 0.18559209 -0.04417127 -0.19321905  0.71350217]]. Reward = [100.]
Curr episode timestep = 6
Scene graph at timestep 1205 is [True, False, False, False, False, True]
Scene graph at timestep 1205 is [True, False, False, False, False, True]
State prediction error at timestep 1205 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of 1
Current timestep = 1206. State = [[-0.2113357   0.24831775]]. Action = [[-0.1504033  -0.02258623 -0.05096394 -0.20853233]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1206 is [True, False, False, False, False, True]
Scene graph at timestep 1206 is [True, False, False, False, False, True]
State prediction error at timestep 1206 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of -1
Current timestep = 1207. State = [[-0.22062048  0.25811014]]. Action = [[0.04964855 0.17935002 0.02793843 0.9671929 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1207 is [True, False, False, False, False, True]
Current timestep = 1208. State = [[-0.2133425   0.25181952]]. Action = [[ 0.19544286 -0.2321971  -0.17456627  0.77876663]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1208 is [True, False, False, False, False, True]
Current timestep = 1209. State = [[-0.20250875  0.24802977]]. Action = [[-0.09968504  0.09324932 -0.13833396 -0.7258203 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1209 is [True, False, False, False, False, True]
Scene graph at timestep 1209 is [True, False, False, False, False, True]
State prediction error at timestep 1209 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1209 of 1
Current timestep = 1210. State = [[-0.2132622  0.2651411]]. Action = [[-0.15650506  0.21785566  0.03445467  0.25762844]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1210 is [True, False, False, False, False, True]
Current timestep = 1211. State = [[-0.21729167  0.2668923 ]]. Action = [[ 0.13544798 -0.21909074  0.1142619   0.5903331 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1211 is [True, False, False, False, False, True]
Current timestep = 1212. State = [[-0.20038623  0.24149244]]. Action = [[ 0.21913934 -0.23734444  0.11670339  0.12411678]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1212 is [True, False, False, False, False, True]
Scene graph at timestep 1212 is [True, False, False, False, False, True]
State prediction error at timestep 1212 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1212 of 1
Current timestep = 1213. State = [[-0.17645554  0.22130434]]. Action = [[ 0.10410276  0.00561413  0.18599516 -0.76719016]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1213 is [True, False, False, False, False, True]
Current timestep = 1214. State = [[-0.16142355  0.21851954]]. Action = [[ 0.10526127 -0.04274438  0.20304456 -0.06567705]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1214 is [True, False, False, False, False, True]
Scene graph at timestep 1214 is [True, False, False, False, False, True]
State prediction error at timestep 1214 is tensor(2.3032e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1214 of 1
Current timestep = 1215. State = [[-0.15471521  0.2257721 ]]. Action = [[-0.02707075  0.20998791 -0.2415706   0.03474343]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1215 is [True, False, False, False, False, True]
Scene graph at timestep 1215 is [True, False, False, False, False, True]
State prediction error at timestep 1215 is tensor(3.6617e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1215 of -1
Current timestep = 1216. State = [[-0.15186544  0.23535863]]. Action = [[-0.11019899 -0.1526333   0.19872308  0.14681149]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1216 is [True, False, False, False, False, True]
Current timestep = 1217. State = [[-0.14657238  0.23284522]]. Action = [[ 0.22060049  0.08304012 -0.1297931   0.7387779 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1217 is [True, False, False, False, False, True]
Current timestep = 1218. State = [[-0.13934085  0.22386009]]. Action = [[-0.02888528 -0.19126923 -0.05148703  0.75406384]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1218 is [True, False, False, False, False, True]
Current timestep = 1219. State = [[-0.1299742  0.2123616]]. Action = [[ 0.19258016  0.01674214 -0.13756672 -0.7610433 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1219 is [True, False, False, False, False, True]
Current timestep = 1220. State = [[-0.10490741  0.19573009]]. Action = [[ 0.23713657 -0.2160865  -0.18427327  0.37579358]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1220 is [True, False, False, False, False, True]
Scene graph at timestep 1220 is [True, False, False, False, False, True]
State prediction error at timestep 1220 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1220 of 1
Current timestep = 1221. State = [[-0.07764052  0.17022869]]. Action = [[-0.15129589 -0.20797361  0.0408498  -0.50531715]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1221 is [True, False, False, False, False, True]
Scene graph at timestep 1221 is [True, False, False, False, False, True]
State prediction error at timestep 1221 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of 1
Current timestep = 1222. State = [[-0.08161992  0.16634533]]. Action = [[ 0.00209847  0.2062152   0.18620649 -0.6550149 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1222 is [True, False, False, False, False, True]
Scene graph at timestep 1222 is [True, False, False, False, False, True]
State prediction error at timestep 1222 is tensor(8.2960e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of -1
Current timestep = 1223. State = [[-0.08047359  0.1791696 ]]. Action = [[ 0.22621834  0.04233891 -0.04547971 -0.00400645]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1223 is [True, False, False, False, False, True]
Current timestep = 1224. State = [[-0.06569226  0.17791443]]. Action = [[ 0.05394244 -0.09501147  0.07976687 -0.9413356 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1224 is [True, False, False, False, False, True]
Scene graph at timestep 1224 is [True, False, False, False, False, True]
State prediction error at timestep 1224 is tensor(7.3167e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1224 of 1
Current timestep = 1225. State = [[-0.05775896  0.17513621]]. Action = [[-0.05217962  0.04627699  0.20216572 -0.8536139 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1225 is [True, False, False, False, False, True]
Scene graph at timestep 1225 is [True, False, False, False, False, True]
State prediction error at timestep 1225 is tensor(3.7528e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of 1
Current timestep = 1226. State = [[-0.04951918  0.16202281]]. Action = [[ 0.20654285 -0.24399416 -0.00900249  0.78945684]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1226 is [True, False, False, False, False, True]
Scene graph at timestep 1226 is [False, True, False, False, False, True]
State prediction error at timestep 1226 is tensor(4.7357e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of 1
Current timestep = 1227. State = [[-0.03325232  0.14572601]]. Action = [[ 0.14618552  0.08029234 -0.15556869  0.2719369 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1227 is [False, True, False, False, False, True]
Scene graph at timestep 1227 is [False, True, False, False, False, True]
State prediction error at timestep 1227 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1227 of -1
Current timestep = 1228. State = [[-0.00292896  0.14922097]]. Action = [[ 0.23682839 -0.07847345  0.16680944  0.24514318]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1228 is [False, True, False, False, False, True]
Current timestep = 1229. State = [[0.02819945 0.13466284]]. Action = [[ 0.2130794  -0.13492571  0.09057435  0.9602866 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1229 is [False, True, False, False, False, True]
Scene graph at timestep 1229 is [False, True, False, False, False, True]
State prediction error at timestep 1229 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1229 of 1
Current timestep = 1230. State = [[0.05660422 0.12512866]]. Action = [[ 0.21385553 -0.11209394  0.02207661  0.4575014 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1230 is [False, True, False, False, False, True]
Current timestep = 1231. State = [[0.05389558 0.13138172]]. Action = [[-0.12576835  0.09463638  0.24841419 -0.6236915 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1231 is [False, False, True, False, False, True]
Current timestep = 1232. State = [[0.05207234 0.13571082]]. Action = [[0.07826144 0.08536297 0.16270554 0.42852688]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1232 is [False, False, True, False, False, True]
Scene graph at timestep 1232 is [False, False, True, False, False, True]
State prediction error at timestep 1232 is tensor(6.3849e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1232 of -1
Current timestep = 1233. State = [[0.05304579 0.13263547]]. Action = [[ 0.06563184 -0.07228285 -0.11667821  0.1034317 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1233 is [False, False, True, False, False, True]
Scene graph at timestep 1233 is [False, False, True, False, False, True]
State prediction error at timestep 1233 is tensor(2.7177e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1233 of 1
Current timestep = 1234. State = [[0.05377487 0.1299371 ]]. Action = [[ 0.22462636 -0.20987754  0.21262124  0.27801752]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1234 is [False, False, True, False, False, True]
Current timestep = 1235. State = [[0.05377487 0.1299371 ]]. Action = [[ 0.08521125 -0.09578939  0.14196873 -0.5402327 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1235 is [False, False, True, False, False, True]
Current timestep = 1236. State = [[0.05377487 0.1299371 ]]. Action = [[ 0.2068358  -0.09281331 -0.02425177 -0.37723708]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1236 is [False, False, True, False, False, True]
Current timestep = 1237. State = [[0.05377487 0.1299371 ]]. Action = [[0.21154699 0.20154443 0.00530475 0.9129729 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1237 is [False, False, True, False, False, True]
Scene graph at timestep 1237 is [False, False, True, False, False, True]
State prediction error at timestep 1237 is tensor(7.0973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of 1
Current timestep = 1238. State = [[0.05377487 0.1299371 ]]. Action = [[ 0.12241265 -0.01111192  0.19077054  0.5122148 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1238 is [False, False, True, False, False, True]
Scene graph at timestep 1238 is [False, False, True, False, False, True]
State prediction error at timestep 1238 is tensor(2.9983e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 1
Current timestep = 1239. State = [[0.05377487 0.1299371 ]]. Action = [[ 0.13403094 -0.15628421 -0.15242876  0.9701061 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1239 is [False, False, True, False, False, True]
Scene graph at timestep 1239 is [False, False, True, False, False, True]
State prediction error at timestep 1239 is tensor(3.4339e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[0.05164457 0.13576616]]. Action = [[-0.00748532  0.12014124  0.07139838  0.75549483]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1240 is [False, False, True, False, False, True]
Current timestep = 1241. State = [[0.0503977  0.12992056]]. Action = [[-0.20955883 -0.21612342 -0.00551666  0.39713478]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1241 is [False, False, True, False, False, True]
Scene graph at timestep 1241 is [False, False, True, False, False, True]
State prediction error at timestep 1241 is tensor(5.3028e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of 1
Current timestep = 1242. State = [[0.04989464 0.11970847]]. Action = [[ 0.22836217  0.17549157 -0.20454378 -0.6024711 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1242 is [False, False, True, False, False, True]
Current timestep = 1243. State = [[0.0501066  0.11070404]]. Action = [[-0.04422686 -0.13901158 -0.22998318  0.86826444]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1243 is [False, True, False, False, True, False]
Scene graph at timestep 1243 is [False, False, True, False, True, False]
State prediction error at timestep 1243 is tensor(2.6492e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1243 of 1
Current timestep = 1244. State = [[0.0498482  0.10040713]]. Action = [[0.20648435 0.21997496 0.16429448 0.5077244 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1244 is [False, False, True, False, True, False]
Scene graph at timestep 1244 is [False, True, False, False, True, False]
State prediction error at timestep 1244 is tensor(3.2224e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of 1
Current timestep = 1245. State = [[0.04932245 0.09983041]]. Action = [[ 0.2023105   0.20981354  0.20271468 -0.2927673 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1245 is [False, True, False, False, True, False]
Current timestep = 1246. State = [[0.04932245 0.09983041]]. Action = [[ 0.20676678 -0.04576445 -0.06374222  0.9461684 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1246 is [False, True, False, False, True, False]
Current timestep = 1247. State = [[0.04688883 0.1088301 ]]. Action = [[ 0.06516856  0.18069166 -0.09140922  0.5628855 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1247 is [False, True, False, False, True, False]
Current timestep = 1248. State = [[0.04417913 0.1185452 ]]. Action = [[ 0.15735707 -0.05716684  0.04210955 -0.7664113 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1248 is [False, True, False, False, True, False]
Scene graph at timestep 1248 is [False, True, False, False, True, False]
State prediction error at timestep 1248 is tensor(6.9304e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1248 of 1
Current timestep = 1249. State = [[0.04384246 0.1202158 ]]. Action = [[ 0.1409933   0.20109579 -0.00896245 -0.9041615 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1249 is [False, True, False, False, True, False]
