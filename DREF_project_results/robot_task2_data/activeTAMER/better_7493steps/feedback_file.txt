Current timestep = 0. State = [[-0.24121451 -0.05035207  0.11520937  1.        ]]. Action = [[ 0.6785612  -0.29493272  0.8596885   0.00308609]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3811, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.22983415 -0.05979533  0.12375659  1.        ]]. Action = [[-0.5955177   0.053002   -0.17504567  0.49344862]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3625, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.23203787 -0.06000554  0.12319694  1.        ]]. Action = [[-0.9717503   0.68392277  0.86191845  0.1477642 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3282, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.25366643  0.00645164  0.12446067  1.        ]]. Action = [[-0.6750768   0.45130587 -0.6786737  -0.7468105 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4. State = [[-0.25323546  0.00687836  0.11038057  1.        ]]. Action = [[-0.5207927  -0.16738117 -0.29886574 -0.5815223 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2769, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.25307703  0.00694056  0.1104074   1.        ]]. Action = [[-0.7940633   0.23856759 -0.01362491  0.22817397]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2610, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.25300223  0.00694112  0.11042184  1.        ]]. Action = [[-0.57073045 -0.92940617  0.06181467 -0.71888465]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.25300223  0.00694112  0.11042184  1.        ]]. Action = [[-0.7858174  -0.976321   -0.768836   -0.34165758]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.1913, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25300223  0.00694112  0.11042184  1.        ]]. Action = [[-0.8101769  -0.44392216 -0.09367132 -0.4569189 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1803, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.26218003  0.04945471  0.1237906   1.        ]]. Action = [[ 0.16165054 -0.62245595 -0.6092672  -0.7886798 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 10. State = [[-0.24874623  0.05717209  0.10880461  1.        ]]. Action = [[ 0.9034262   0.13529694 -0.2343235   0.29878688]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1329, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 0
Current timestep = 11. State = [[-0.22084227  0.06097038  0.10141198  1.        ]]. Action = [[ 0.6637523  -0.01984102  0.02845013  0.6435672 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1205, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.25929943 -0.0478102   0.12106562  1.        ]]. Action = [[ 0.6502199   0.9780874   0.48549283 -0.8565208 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 13. State = [[-0.25997856 -0.05319227  0.10787857  1.        ]]. Action = [[-0.6522307  -0.4844967   0.12046385  0.82033277]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.2600064  -0.05319585  0.10787273  1.        ]]. Action = [[-0.20156062 -0.16433954  0.89957166  0.72732973]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0587, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.24987067 -0.10323107  0.12349285  1.        ]]. Action = [[ 0.6077533   0.9259062  -0.13569891 -0.12909299]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 16. State = [[-0.24728559 -0.1247341   0.11551078  1.        ]]. Action = [[ 0.28992414 -0.600948    0.8417319   0.98772883]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.23362778 -0.12817846  0.11547074  1.        ]]. Action = [[ 0.9142344   0.63006175 -0.9292828   0.09464288]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0386, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.25846153  0.0407432   0.12310696  1.        ]]. Action = [[-0.54810643  0.31824267 -0.30080122 -0.6667814 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 19. State = [[-0.2561531  0.0458345  0.1101694  1.       ]]. Action = [[-0.85547256 -0.8460139   0.1312331   0.03351533]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0219, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.25592896  0.04585474  0.11021666  1.        ]]. Action = [[-0.29701614 -0.8351585   0.3087834  -0.4033574 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 20 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 20 is tensor(0.0244, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.2514408   0.043904    0.11416058  1.        ]]. Action = [[ 0.23121798 -0.1803633   0.6652899   0.95976233]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0215, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.25085878  0.05399161  0.11973105  1.        ]]. Action = [[-0.21400547  0.8316637  -0.01732254  0.28574204]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0423, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.26260716  0.15748301  0.1227278   1.        ]]. Action = [[ 0.49321103 -0.45805514 -0.80575675 -0.3308391 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 24. State = [[-0.26012266  0.1746298   0.11059967  1.        ]]. Action = [[-0.854727    0.88672876 -0.13468653 -0.1524257 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0465, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.25543404  0.16951177  0.11837797  1.        ]]. Action = [[-0.00902641 -0.44816923  0.971138    0.5692016 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 25 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0331, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.24903743  0.16310443  0.12433746  1.        ]]. Action = [[ 0.54422784  0.13478303 -0.6274977   0.2719382 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 26 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0515, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.24181299  0.16574173  0.1193831   1.        ]]. Action = [[-0.5797054   0.46520424  0.6948011  -0.4122129 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 27 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 27 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.24349593  0.1635711   0.11869305  1.        ]]. Action = [[-0.46956635 -0.25757754 -0.01752788  0.942369  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 28 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0508, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.2484463   0.14998233  0.1126957   1.        ]]. Action = [[-0.37250316 -0.7265733  -0.5349315   0.8272202 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 29 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.25552082  0.1345262   0.09857082  1.        ]]. Action = [[ 0.7385006   0.11902833 -0.26168644  0.36952162]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 30 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0590, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.22968975  0.13833553  0.0989286   1.        ]]. Action = [[0.90644836 0.1566577  0.81882167 0.29212403]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 31 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.20722322  0.1492079   0.11113388  1.        ]]. Action = [[0.21894026 0.57889795 0.01647532 0.7805629 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 32 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0590, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 0
Current timestep = 33. State = [[-0.2562639  -0.06984206  0.11911402  1.        ]]. Action = [[-0.5294811   0.48972154  0.9151617  -0.6935608 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 34. State = [[-0.25717986 -0.07725773  0.10661407  1.        ]]. Action = [[-0.55226505 -0.6435268  -0.8753206  -0.80400896]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.25352708 -0.08770551  0.10762987  1.        ]]. Action = [[ 0.28874826 -0.70024025  0.18880427  0.8235786 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0506, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 0
Current timestep = 36. State = [[-0.25018558 -0.10538557  0.10786693  1.        ]]. Action = [[-0.62929744  0.43830895  0.794871    0.7378025 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0471, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.25100356 -0.12064078  0.10251202  1.        ]]. Action = [[-0.00570184 -0.9970796  -0.5749966   0.34029198]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0569, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.25369045  0.01267292  0.12574562  1.        ]]. Action = [[-0.19281882 -0.7556213  -0.19796574 -0.34328353]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 39. State = [[-0.26055837  0.15223792  0.12234608  1.        ]]. Action = [[ 0.34396446 -0.20783186  0.59246373 -0.7951728 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 40. State = [[-0.24651384  0.16314676  0.10617399  1.        ]]. Action = [[ 0.88115513 -0.41870695 -0.21649998  0.10110092]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0553, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 0
Current timestep = 41. State = [[-0.23671937  0.16523339  0.09292989  1.        ]]. Action = [[-0.6031213   0.57545495 -0.45968783  0.9815508 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0504, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.26279825  0.18023998  0.11870808  1.        ]]. Action = [[-0.25833684 -0.279873   -0.4574312  -0.47415966]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 43. State = [[-0.2529689   0.20184813  0.10194936  1.        ]]. Action = [[ 0.93452215  0.24776149 -0.80733985  0.42436576]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 43 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0531, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.26021582 -0.0067213   0.11569824  1.        ]]. Action = [[ 0.49889445 -0.6295762  -0.6381594  -0.3574766 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 45. State = [[-0.26153785  0.03911275  0.1232191   1.        ]]. Action = [[ 0.67664313  0.92479825  0.53798354 -0.7292951 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 46. State = [[-0.2602685   0.04447583  0.10958202  1.        ]]. Action = [[-0.22348368  0.15809333 -0.8285978   0.37976944]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.25548282  0.01885717  0.12669592  1.        ]]. Action = [[-0.17917728 -0.25471163  0.6362333  -0.26315534]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 48. State = [[-0.25853476  0.06654944  0.12253333  1.        ]]. Action = [[ 0.6813996   0.78607917  0.4596429  -0.709566  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 49. State = [[-0.2588035   0.06870639  0.11041445  1.        ]]. Action = [[-0.14741498 -0.3744588   0.3905387   0.63102674]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 0
Current timestep = 50. State = [[-0.2561595  -0.00817506  0.12237861  1.        ]]. Action = [[ 0.70448375 -0.52436686 -0.2997141  -0.52620596]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 51. State = [[-0.25757337 -0.00919982  0.10836564  1.        ]]. Action = [[-0.47968543 -0.72972447 -0.4095245  -0.41029   ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 51 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 51 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.2529396  -0.01914004  0.11068624  1.        ]]. Action = [[ 0.31026018 -0.54582715  0.52536666  0.13843155]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0743, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.23372921 -0.03382845  0.1227045   1.        ]]. Action = [[0.9325969  0.0200845  0.6562307  0.07315826]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.26796028  0.09530795  0.12262789  1.        ]]. Action = [[-0.5023276   0.8313757  -0.7315571  -0.23498791]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 55. State = [[-0.25736716  0.10669175  0.11668984  1.        ]]. Action = [[ 0.43714046 -0.01697081  0.93197083  0.78269625]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0638, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.2485942   0.10745885  0.12845334  1.        ]]. Action = [[-0.88468903  0.56618524  0.67751765  0.81607103]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.0646, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.24857351  0.10733085  0.12845895  1.        ]]. Action = [[-0.46887934 -0.2830156   0.8941765   0.77380717]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.2583447  -0.05187914  0.11778332  1.        ]]. Action = [[-0.05509436 -0.7954755  -0.4254943  -0.7343811 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 59. State = [[-0.25988224  0.03332634  0.1253366   1.        ]]. Action = [[ 0.8533733   0.5019232   0.8805797  -0.31338137]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 60. State = [[-0.25746977  0.03820339  0.11114451  1.        ]]. Action = [[-0.8331591  -0.4897102  -0.04062617  0.8798685 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.0811, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.2532476  -0.13075449  0.12092888  1.        ]]. Action = [[ 0.4229778  -0.8308498   0.35447562 -0.2686531 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 62. State = [[-0.24587291 -0.14895755  0.10336942  1.        ]]. Action = [[ 0.7677518  -0.22909427 -0.3862487   0.5383866 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 62 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.24842787 -0.08070829  0.12428591  1.        ]]. Action = [[-0.71216214  0.8040631  -0.18484986 -0.42835033]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 64. State = [[-0.23925544 -0.08413122  0.11212889  1.        ]]. Action = [[0.773129   0.4726808  0.39643967 0.8181455 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.0945, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 0
Current timestep = 65. State = [[-0.23053275 -0.07609597  0.10992334  1.        ]]. Action = [[-0.5498539   0.32431817 -0.5646456   0.660162  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1057, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.2640478   0.02880659  0.12164629  1.        ]]. Action = [[-0.14597964  0.940547   -0.21210146 -0.14155704]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 67. State = [[-0.25905174  0.04376841  0.10490998  1.        ]]. Action = [[ 0.47730112  0.7075479  -0.2943964   0.9524262 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.0966, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.2392527   0.06303522  0.09686571  1.        ]]. Action = [[0.7157769  0.21287155 0.03133798 0.24624348]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 0
Current timestep = 69. State = [[-0.21887392  0.06914398  0.09734909  1.        ]]. Action = [[ 0.5088527  -0.16004121  0.17105722  0.75550854]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.20986366  0.05760476  0.10142774  1.        ]]. Action = [[-0.9777106 -0.8631233  0.4467485  0.4137323]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-0.2164454   0.03464951  0.09850414  1.        ]]. Action = [[ 0.97895885  0.01215923 -0.845207    0.70969355]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.0986, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.20701283  0.02645776  0.09292977  1.        ]]. Action = [[-0.71726537 -0.57933897  0.8361999   0.8373637 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.0716, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.21379066  0.00991422  0.09566301  1.        ]]. Action = [[ 0.34179878 -0.31182408 -0.80966264  0.48707902]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.1115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.21712182  0.01372584  0.08914429  1.        ]]. Action = [[-0.6937366   0.5957072   0.19446886  0.17958307]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.1054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.22337121  0.02701264  0.08795709  1.        ]]. Action = [[ 0.44317293  0.3588084  -0.17393607  0.42811215]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1126, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.21001306  0.02413006  0.08925185  1.        ]]. Action = [[ 0.83408594 -0.81654024  0.00669682  0.56057143]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 0
Current timestep = 77. State = [[-0.19942635 -0.0045448   0.09115975  1.        ]]. Action = [[-0.53444546 -0.9731076   0.1161797   0.07565284]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.0867, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.2039352  -0.03975001  0.09276532  1.        ]]. Action = [[-0.31057477 -0.9557468   0.12596786  0.64677835]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.0867, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.2587605  -0.02926538  0.12132372  1.        ]]. Action = [[ 0.40756273 -0.2957065  -0.42825592 -0.3041041 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 80. State = [[-0.2608403   0.07671866  0.12363044  1.        ]]. Action = [[ 0.99146104 -0.49244344  0.7823094  -0.6380106 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 81. State = [[-0.25345168  0.0753712   0.11744417  1.        ]]. Action = [[ 0.3916086  -0.69295657  0.8293972   0.74944437]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.0682, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.23748715  0.05370993  0.12790535  1.        ]]. Action = [[ 0.5604873  -0.72045386 -0.05234808  0.83063984]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.0755, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.26519972  0.1253367   0.12294744  1.        ]]. Action = [[ 0.25987482  0.4527856   0.6089165  -0.12811059]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 84. State = [[-0.26212645  0.1394099   0.10903671  1.        ]]. Action = [[-0.86490166 -0.46523106  0.36789107  0.771875  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0615, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.26176608  0.03018351  0.12030929  1.        ]]. Action = [[ 0.27986956  0.7212862   0.55476356 -0.61310697]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 86. State = [[-0.2612565   0.10964998  0.12411229  1.        ]]. Action = [[-0.08369958 -0.7176277   0.44479728 -0.6691598 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 87. State = [[-0.25929117  0.12189209  0.11161278  1.        ]]. Action = [[-0.50480455 -0.57058525  0.76169705 -0.7325527 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0610, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.24655516  0.12521195  0.11864527  1.        ]]. Action = [[0.76268184 0.12412238 0.8300996  0.73620343]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 88 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0636, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25736514  0.00292568  0.12128704  1.        ]]. Action = [[ 0.00336397 -0.90006083 -0.14079261 -0.7866445 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 90. State = [[-0.24548145 -0.0068239   0.11396255  1.        ]]. Action = [[ 0.90015745 -0.7104386   0.88904107  0.39138436]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0632, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.21530713 -0.02809827  0.1368099   1.        ]]. Action = [[ 0.747421   -0.47550517  0.9865241   0.88401926]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.19584043 -0.03718941  0.160544    1.        ]]. Action = [[ 0.63504255 -0.03075206  0.13768673 -0.21913618]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.0983, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.19091587 -0.04327849  0.17516057  1.        ]]. Action = [[ 0.0798229  -0.3326246   0.96570003  0.8511398 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0786, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.18049923 -0.05727889  0.21356565  1.        ]]. Action = [[ 0.48983812 -0.43418527  0.9170594   0.88059807]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0837, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.1676634  -0.08085772  0.25001958  1.        ]]. Action = [[-0.44590437 -0.73587203  0.8588102   0.92147875]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0842, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.16980262 -0.11051052  0.28499398  1.        ]]. Action = [[ 0.52287924 -0.81331426  0.8541913   0.7159846 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0987, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.16185798 -0.13792433  0.3001463   1.        ]]. Action = [[ 0.10785592 -0.58423364 -0.917326    0.37813902]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 97 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.1164, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.2553411  -0.08143995  0.10812316  1.        ]]. Action = [[-0.86904657  0.50823605  0.95604014 -0.17548925]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 99. State = [[-0.24893747 -0.09218553  0.08854948  1.        ]]. Action = [[ 0.68007743 -0.11197954 -0.44628477  0.80600584]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.1034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.25360534 -0.11702837  0.12112896  1.        ]]. Action = [[ 0.39362955 -0.98276454  0.6308992  -0.67861867]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 101. State = [[-0.24584313 -0.13083503  0.10243652  1.        ]]. Action = [[ 0.78857565 -0.17702043 -0.5369376   0.8928939 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 101 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.1009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.25597027  0.17273232  0.12901057  1.        ]]. Action = [[-0.6735903   0.33502293 -0.7460431  -0.8177959 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 103. State = [[-0.25359926  0.19213356  0.11597974  1.        ]]. Action = [[-0.5812184   0.3116473   0.20504713  0.5884671 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 103 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 103 is tensor(0.1063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.25356773  0.19220386  0.11600374  1.        ]]. Action = [[-0.89383256 -0.933189    0.06270778  0.833591  ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0803, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.25356773  0.19220386  0.11600374  1.        ]]. Action = [[-0.48657703 -0.741834    0.53937936  0.6914954 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 105 is tensor(0.0933, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.26397854  0.13496189  0.11942949  1.        ]]. Action = [[ 0.34752488 -0.12673628 -0.2859521  -0.8584546 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 107. State = [[-0.2613224   0.15027575  0.10580562  1.        ]]. Action = [[-0.53995615 -0.9164229   0.9397447   0.93447924]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0775, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.2523084   0.16236371  0.10556467  1.        ]]. Action = [[ 0.75831664  0.81023264 -0.15425944  0.77658725]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 108 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 108 is tensor(0.0896, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.22834525  0.18501486  0.09328213  1.        ]]. Action = [[ 0.78202045  0.41405308 -0.61085016  0.5346658 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 109 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 109 is tensor(0.0920, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.21313627  0.19038141  0.06708041  1.        ]]. Action = [[-0.48525375 -0.5244563  -0.7186755   0.7692921 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 110 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 110 is tensor(0.0920, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.20496224  0.17808627  0.0571214   1.        ]]. Action = [[ 0.8834605  -0.21168482  0.89174175  0.9552171 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 111 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0849, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.17761023  0.16231248  0.08033353  1.        ]]. Action = [[ 0.31858015 -0.7211131   0.9821422   0.7647371 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 112 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0842, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.17021944  0.14747256  0.10211115  1.        ]]. Action = [[ 0.56429124 -0.14604086  0.3418913   0.9615755 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 113 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 113 is tensor(0.0897, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.17010364  0.14699632  0.10224698  1.        ]]. Action = [[ 0.45396018 -0.5335734   0.10563695  0.479581  ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 114 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 114 is tensor(0.0955, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.17008752  0.14692552  0.10225347  1.        ]]. Action = [[ 0.95940375  0.24656653 -0.70579386 -0.07726496]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 115 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0742, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-0.17546776  0.14790997  0.09767567  1.        ]]. Action = [[-0.60040945  0.03556204 -0.5720006   0.18768787]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 116 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 116 is tensor(0.0851, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of 0
Current timestep = 117. State = [[-0.18039142  0.15304074  0.09901819  1.        ]]. Action = [[-0.1816473   0.12423897  0.55077505  0.37282944]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 117 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 117 is tensor(0.0892, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.17728949  0.16575132  0.1055791   1.        ]]. Action = [[0.8779483  0.71576524 0.12611794 0.23518503]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 118 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 118 is tensor(0.0788, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.17529266  0.18580884  0.11287967  1.        ]]. Action = [[-0.48831815  0.5671456   0.18342733  0.11974645]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 119 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 119 is tensor(0.0854, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.18291463  0.19754915  0.12198182  1.        ]]. Action = [[-0.2758162  -0.3730244   0.4805963   0.84448576]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 120 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0769, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.25946075 -0.13852598  0.11223625  1.        ]]. Action = [[ 0.30555868  0.79083276  0.51759624 -0.37699783]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 122. State = [[-0.2537      0.03973535  0.12448402  1.        ]]. Action = [[ 0.9392296   0.8836868   0.41483092 -0.23991251]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 123. State = [[-0.25188825  0.04536706  0.11047774  1.        ]]. Action = [[-0.40843856 -0.8923505   0.9512129   0.9034598 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0553, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.24001625  0.03384204  0.11176281  1.        ]]. Action = [[ 0.8508475  -0.9360032   0.11463988  0.59669065]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 0
Current timestep = 125. State = [[-0.22064905  0.01856154  0.1200406   1.        ]]. Action = [[0.22551548 0.20886075 0.8086951  0.9355166 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.20072603  0.02954709  0.12545256  1.        ]]. Action = [[ 0.8620111   0.6355777  -0.6306136   0.87151766]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0693, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.18187016  0.04050339  0.11692163  1.        ]]. Action = [[ 0.27603185 -0.7029229   0.77148294 -0.78188884]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 127 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 127 is tensor(0.0642, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.18187016  0.04050339  0.11692163  1.        ]]. Action = [[ 0.31524515  0.13613117 -0.8514388   0.38324606]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0775, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.18454565  0.04647416  0.11323972  1.        ]]. Action = [[-0.13866007  0.33459854 -0.28444815  0.86123157]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0727, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.1870004   0.05367461  0.11035829  1.        ]]. Action = [[ 0.9595765  -0.31363225  0.86306846  0.70799303]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0633, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.18706329  0.05517263  0.11031316  1.        ]]. Action = [[ 0.808233   -0.8736225   0.38941574  0.45608306]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0702, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of -1
Current timestep = 132. State = [[-0.18742403  0.0642451   0.10322919  1.        ]]. Action = [[ 0.27262962  0.5573354  -0.96511674  0.71220994]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0642, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.17267717  0.07595361  0.07406296  1.        ]]. Action = [[0.15352881 0.6778225  0.95773196 0.6602775 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0611, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.17272641  0.07617674  0.07403659  1.        ]]. Action = [[ 0.48303723 -0.8736578   0.24372983  0.5050374 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0746, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.17272641  0.07617674  0.07403659  1.        ]]. Action = [[ 0.6648216  -0.9003446  -0.43027508  0.45869374]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.17272641  0.07617674  0.07403659  1.        ]]. Action = [[ 0.88026583 -0.99759966  0.46135044 -0.07865733]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0643, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of -1
Current timestep = 137. State = [[-0.17272641  0.07617674  0.07403659  1.        ]]. Action = [[ 0.7456049  -0.38036346  0.8866705   0.9816363 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0603, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.17272641  0.07617674  0.07403659  1.        ]]. Action = [[ 0.547721  -0.860336   0.9484111 -0.3696409]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Scene graph at timestep 138 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.0603, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of -1
Current timestep = 139. State = [[-0.17566928  0.08532978  0.07761699  1.        ]]. Action = [[-0.42773223  0.52686083  0.5970615   0.7824731 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0642, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.25900707 -0.15475844  0.119941    1.        ]]. Action = [[-0.14338613 -0.35755205 -0.3314091  -0.00950766]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 141. State = [[-0.24877335 -0.17854889  0.11238028  1.        ]]. Action = [[ 0.7946818  -0.49758053  0.85382557  0.82074976]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 141 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.22607364 -0.18037409  0.11955912  1.        ]]. Action = [[ 0.53436756  0.65605617 -0.28082842  0.5413233 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 142 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0837, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 0
Current timestep = 143. State = [[-0.21531984 -0.17174166  0.12653892  1.        ]]. Action = [[ 0.09423482 -0.20381165  0.82613826  0.28589118]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 143 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0862, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.20845309 -0.17492644  0.14060831  1.        ]]. Action = [[0.93689346 0.6173682  0.3546201  0.7827203 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Scene graph at timestep 144 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.19547306 -0.1813349   0.15369035  1.        ]]. Action = [[ 0.76524043 -0.401402    0.8335662   0.9542954 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 145 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0639, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.17680897 -0.20240407  0.18225732  1.        ]]. Action = [[-0.16709471 -0.7191809   0.51891685  0.1682558 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 146 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0807, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.26338854  0.13652776  0.12760594  1.        ]]. Action = [[-0.657414   -0.44733393 -0.09814733 -0.69722575]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 148. State = [[-0.2623308   0.13001792  0.1217199   1.        ]]. Action = [[ 0.6136813  -0.37426615  0.6993313  -0.3713677 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 149. State = [[-0.25972924  0.14452513  0.10745734  1.        ]]. Action = [[-0.5490674  -0.02340078  0.18569243  0.7271352 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 149 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0486, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.25918743  0.1495302   0.10105364  1.        ]]. Action = [[ 0.17156982  0.35275984 -0.614709    0.9270961 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 150 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0414, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.25274843  0.1580324   0.08540468  1.        ]]. Action = [[-0.5056128   0.6623354   0.26358747  0.42077434]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 151 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0492, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.24916632  0.165013    0.07881699  1.        ]]. Action = [[ 0.47150433  0.49510086 -0.8702817   0.20513272]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 152 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.26633745  0.13452463  0.11869552  1.        ]]. Action = [[ 0.9608407   0.97247994  0.31307292 -0.49321228]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 154. State = [[-0.26335362  0.0967876   0.12101006  1.        ]]. Action = [[ 0.65309286 -0.9211235   0.80720985 -0.45514327]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 155. State = [[-0.24639699  0.11896561  0.11736692  1.        ]]. Action = [[0.9019947 0.6444886 0.9547975 0.9202018]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0378, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.22462705  0.12750553  0.13380654  1.        ]]. Action = [[ 0.09103823 -0.44026035  0.37016082  0.9268998 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 156 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0504, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.228563    0.12598887  0.15070565  1.        ]]. Action = [[-0.9234639   0.21416628  0.79863214  0.29525852]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 157 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0476, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.24219568  0.12159102  0.16649732  1.        ]]. Action = [[ 0.48652387 -0.56469125 -0.735685    0.5766361 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0562, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 0
Current timestep = 159. State = [[-0.23726279  0.11169063  0.15970288  1.        ]]. Action = [[-0.90854377 -0.13467503  0.4534514   0.31200397]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0503, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.22770128  0.0986146   0.15558328  1.        ]]. Action = [[ 0.7338021  -0.7536085  -0.34499848  0.6650553 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0567, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.22143187  0.09149447  0.14329173  1.        ]]. Action = [[-0.94622415  0.5642443  -0.25177288  0.96306276]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0478, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.2347962   0.10336661  0.137463    1.        ]]. Action = [[ 0.18219018  0.2737558  -0.24652076  0.6125053 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0684, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.23652448  0.10603145  0.12303326  1.        ]]. Action = [[ 0.04919636 -0.1401481  -0.54224575  0.88643765]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0643, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.23564254  0.10247181  0.11047564  1.        ]]. Action = [[ 0.08830452 -0.18719816  0.4158368   0.40874362]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of 0
Current timestep = 165. State = [[-0.22227491  0.09438152  0.12245239  1.        ]]. Action = [[ 0.7028301  -0.35502106  0.87923384  0.831491  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0545, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.19683413  0.08034857  0.1464789   1.        ]]. Action = [[ 0.68949676 -0.47440565  0.2381792   0.8661914 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0604, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.1819224   0.06327133  0.16050641  1.        ]]. Action = [[-0.30476153 -0.4772011   0.45212936  0.93965614]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.1820656   0.05291832  0.16876808  1.        ]]. Action = [[0.5126271 0.7005923 0.8717158 0.9271548]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.18201052  0.05162934  0.1690101   1.        ]]. Action = [[ 0.5944185  -0.57395023  0.5033127   0.981735  ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.19055557  0.06275216  0.18112125  1.        ]]. Action = [[-0.8408417   0.691532    0.85708797  0.44196022]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0491, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.2090416   0.07039432  0.19926244  1.        ]]. Action = [[-0.14198315 -0.47933996 -0.39944398  0.6113374 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0575, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.21149698  0.06326889  0.19882514  1.        ]]. Action = [[ 0.90302634 -0.00816238  0.5257431   0.8568847 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0454, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.26441115  0.10710248  0.11943619  1.        ]]. Action = [[ 0.7500894  -0.32974517 -0.7813817  -0.26999366]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 174. State = [[-0.26290712  0.11965898  0.10437049  1.        ]]. Action = [[-0.34280956 -0.08818185  0.4170574   0.42536247]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0490, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.25020027  0.13278185  0.11242929  1.        ]]. Action = [[0.9109404  0.83368325 0.86484385 0.8371999 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 175 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.25777012 -0.06288167  0.12180921  1.        ]]. Action = [[ 0.23783445  0.5789025   0.57022905 -0.4634806 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 177. State = [[-0.25217587 -0.07800246  0.10436614  1.        ]]. Action = [[ 0.66199267 -0.5610468  -0.38646066  0.7478905 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0560, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.23355871 -0.0876978   0.0934252   1.        ]]. Action = [[ 0.34460425  0.45650256 -0.07012266  0.60109246]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.25174809 -0.14803913  0.12448683  1.        ]]. Action = [[-0.37847012 -0.64029753  0.98886466 -0.39342523]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 180. State = [[-0.25290832 -0.16274907  0.11179111  1.        ]]. Action = [[-0.5938088  -0.84662783 -0.6502016   0.8944833 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 180 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0537, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.24138321 -0.16378973  0.11630775  1.        ]]. Action = [[0.89600396 0.12023807 0.68371415 0.6737993 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 181 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0686, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.2537573   0.0688847   0.12916623  1.        ]]. Action = [[-0.83619934  0.60771894  0.96538544 -0.36761415]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 183. State = [[-0.25297278  0.07726252  0.11474083  1.        ]]. Action = [[-0.1374098  -0.08682686  0.3292004   0.98239625]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 183 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0485, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.25297278  0.07726252  0.11474083  1.        ]]. Action = [[-0.5672124  -0.8172861   0.08870387  0.9645169 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 184 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0366, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.24532698  0.07642806  0.12557432  1.        ]]. Action = [[ 0.502198   -0.0610823   0.93257236  0.9145887 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 185 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.2390051   0.08673856  0.15570879  1.        ]]. Action = [[-0.12068617  0.7941257   0.90295756  0.24103832]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 186 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0515, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.2377488   0.10358121  0.1792298   1.        ]]. Action = [[-0.0426563  -0.2612909  -0.25392425  0.7267995 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 187 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0548, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.25637075 -0.1223784   0.12575759  1.        ]]. Action = [[-0.39919996 -0.04633021  0.85390294 -0.12650216]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 189. State = [[-0.25810415 -0.13576545  0.11169057  1.        ]]. Action = [[-0.8651982  -0.5043135   0.6587198   0.92431855]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 189 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0679, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.2522323  -0.14262989  0.11017915  1.        ]]. Action = [[ 0.5178299  -0.399521   -0.19501013  0.5090258 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 190 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.24678156 -0.14971222  0.09989332  1.        ]]. Action = [[-0.32001138  0.12506557 -0.4050219   0.5542536 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 191 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0797, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.24267265 -0.1558029   0.08933786  1.        ]]. Action = [[ 0.6448636  -0.4806038  -0.03214926  0.9016547 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 192 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0891, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.22192691 -0.15341143  0.09033617  1.        ]]. Action = [[0.8737416  0.8145292  0.45734262 0.9650705 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 193 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0827, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of 0
Current timestep = 194. State = [[-0.1971111  -0.13953622  0.09927102  1.        ]]. Action = [[0.18231487 0.16729283 0.56683576 0.81550074]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 194 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0895, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.19087861 -0.14182103  0.12032239  1.        ]]. Action = [[ 0.01057851 -0.45845336  0.756757    0.83403647]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 195 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.17619507 -0.15912114  0.14717284  1.        ]]. Action = [[ 0.9564878  -0.81915355  0.68473756  0.5183797 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 196 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0795, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.14998055 -0.17872857  0.16804074  1.        ]]. Action = [[0.09572029 0.09867799 0.7493737  0.9385464 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 197 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0875, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.15112025 -0.17886771  0.18115222  1.        ]]. Action = [[-0.53878736  0.22050273  0.94684935  0.84202456]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 198 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0856, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.15371573 -0.18581484  0.2113181   1.        ]]. Action = [[ 0.1776942  -0.5792791   0.56565356  0.05356276]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 199 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0903, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.15722016 -0.1944278   0.22495802  1.        ]]. Action = [[-0.15539038  0.9080421  -0.012142    0.6399678 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 200 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0809, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.15052614 -0.1930237   0.21976289  1.        ]]. Action = [[ 0.735368    0.01635039 -0.73037404  0.78683496]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 201 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0814, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.13660344 -0.19491138  0.21155456  1.        ]]. Action = [[0.15378809 0.6896107  0.17671454 0.40975714]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 202 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0823, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[-0.13040987 -0.19480255  0.21668231  1.        ]]. Action = [[0.37661362 0.11031497 0.60910845 0.1236248 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 203 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0886, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.12287582 -0.20796853  0.2323711   1.        ]]. Action = [[-0.6618625 -0.9111927  0.5361638  0.548213 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 204 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 0
Current timestep = 205. State = [[-0.13955352 -0.24136196  0.24038671  1.        ]]. Action = [[-0.17658228 -0.87421304 -0.691256    0.85203505]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 205 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0654, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.15261266 -0.26983336  0.23119958  1.        ]]. Action = [[-0.66387665 -0.57927257 -0.3729897   0.9014627 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 206 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0644, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.16981906 -0.28640547  0.22778216  1.        ]]. Action = [[-0.54895794  0.09785569  0.64557755  0.35451317]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 207 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0876, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 0
Current timestep = 208. State = [[-0.18137754 -0.28971753  0.24328047  1.        ]]. Action = [[-0.3764832   0.10612929  0.62091076  0.91376424]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 208 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0904, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 0
Current timestep = 209. State = [[-0.19533825 -0.28698266  0.25579217  1.        ]]. Action = [[-0.829713   -0.51633364  0.93125653  0.80174756]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 209 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0728, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.19559164 -0.28729346  0.25587928  1.        ]]. Action = [[-0.33001614 -0.684223   -0.79904467  0.5406195 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 210 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0734, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.19549453 -0.28787902  0.25560838  1.        ]]. Action = [[ 0.28476214 -0.29809552 -0.25868785  0.5073173 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 211 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0967, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.19534698 -0.2880271   0.25567824  1.        ]]. Action = [[-0.51215607 -0.5074164  -0.8506745   0.4646983 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 212 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0719, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.20126027 -0.27916762  0.2616991   1.        ]]. Action = [[-0.8609983   0.9011924   0.613287    0.39894533]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 213 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0794, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.21952379 -0.25592932  0.27540803  1.        ]]. Action = [[-0.36733305  0.6072651   0.31540966  0.94632053]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 214 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.0896, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of 0
Current timestep = 215. State = [[-0.2225049 -0.2471033  0.2940634  1.       ]]. Action = [[ 0.8168882  -0.64466524  0.88242865  0.9065379 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 215 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of 0
Current timestep = 216. State = [[-0.21532187 -0.25335923  0.3050596   1.        ]]. Action = [[-0.34711742  0.44379067 -0.99574214  0.75752664]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 216 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.0662, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 0
Current timestep = 217. State = [[-0.22946127 -0.23515749  0.29915112  1.        ]]. Action = [[-0.83653563  0.8654357   0.5173738   0.09719563]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 217 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.0737, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.24474125 -0.21438076  0.3052196   1.        ]]. Action = [[-0.7685154  -0.5622261   0.00930023  0.65126085]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 218 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.0683, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.23827963 -0.21652232  0.30988014  1.        ]]. Action = [[ 0.8085563  -0.23408413  0.39811707  0.7830515 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 219 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.0848, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 0
Current timestep = 220. State = [[-0.21910508 -0.2213112   0.31757414  1.        ]]. Action = [[ 0.8674128  -0.35030866 -0.07602888  0.36427808]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 220 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.0841, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.19399165 -0.23025954  0.31193283  1.        ]]. Action = [[ 0.8269553  -0.36322355 -0.7766445   0.5140382 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 221 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.0741, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 221 of 0
Current timestep = 222. State = [[-0.15357502 -0.23586033  0.3024606   1.        ]]. Action = [[0.91468954 0.30620408 0.9384749  0.69340825]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 222 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.13167512 -0.22388373  0.32618555  1.        ]]. Action = [[-0.312819    0.62613773  0.28110147  0.92445767]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 223 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.0757, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.12571602 -0.21352845  0.32932723  1.        ]]. Action = [[ 0.75569737 -0.34321773 -0.06467402  0.39584255]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 224 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.0775, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.12132151 -0.23089768  0.34079626  1.        ]]. Action = [[-0.5258904  -0.82307744  0.6039827   0.5466343 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 225 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.0551, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.13854823 -0.2551158   0.34754828  1.        ]]. Action = [[-0.9214171 -0.2202726 -0.8560699  0.2603171]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 226 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.0573, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.158148   -0.26787794  0.3401874   1.        ]]. Action = [[-0.1863476  -0.35564768  0.15683079  0.82060313]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 227 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.0653, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 227 of 0
Current timestep = 228. State = [[-0.17349215 -0.26520187  0.33894524  1.        ]]. Action = [[-0.9738833   0.81985366 -0.12186843  0.9757377 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 228 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.0567, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.19038531 -0.2574377   0.33019194  1.        ]]. Action = [[ 0.45810127 -0.5188871  -0.3782947   0.6613357 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 229 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.0662, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of 0
Current timestep = 230. State = [[-0.17703207 -0.24538858  0.31709492  1.        ]]. Action = [[ 0.97593784  0.9853176  -0.44533652  0.6075268 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 230 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 230 is tensor(0.0485, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of 0
Current timestep = 231. State = [[-0.15908268 -0.23767869  0.30831218  1.        ]]. Action = [[-0.14970171 -0.7838245   0.7035527   0.45333362]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 231 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.2568349  -0.07027496  0.10694315  1.        ]]. Action = [[ 0.04002964  0.7510741   0.85667586 -0.17018378]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 233. State = [[-0.25475767 -0.07779661  0.09895594  1.        ]]. Action = [[-0.02640784  0.10820615  0.9165757   0.8642187 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 233 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 233 is tensor(0.0335, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 233 of 1
Current timestep = 234. State = [[-0.25137848 -0.07819003  0.10833833  1.        ]]. Action = [[-0.6495329  -0.20463902  0.64845645  0.8185203 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 234 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 234 is tensor(0.0377, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.2509536  -0.08751425  0.11171892  1.        ]]. Action = [[ 0.22660053 -0.60688937  0.2824874   0.6515305 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 235 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 235 is tensor(0.0411, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.24339898 -0.09815693  0.13005711  1.        ]]. Action = [[0.5958873  0.15266287 0.9849714  0.9769342 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 236 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 236 is tensor(0.0265, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.21986558 -0.08622246  0.16653034  1.        ]]. Action = [[0.6998606  0.8095825  0.5802307  0.87170553]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 237 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 237 is tensor(0.0268, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[-0.20015302 -0.07192987  0.18207583  1.        ]]. Action = [[0.7320945  0.42999434 0.39590955 0.1360333 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 238 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 238 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.20834693 -0.06983239  0.1778843   1.        ]]. Action = [[-0.8235806   0.11034417 -0.60751617  0.46353984]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 239 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 239 is tensor(0.0374, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.21484166 -0.06305594  0.17438988  1.        ]]. Action = [[0.721823   0.28207254 0.22571647 0.20917308]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 240 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 240 is tensor(0.0401, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of 0
Current timestep = 241. State = [[-0.19933152 -0.0614628   0.17883249  1.        ]]. Action = [[ 0.27749944 -0.5469238   0.19260073  0.09520555]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 241 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 241 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of 0
Current timestep = 242. State = [[-0.19590984 -0.06771696  0.1809674   1.        ]]. Action = [[ 0.8705286  -0.1033386   0.07641995  0.5305232 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Scene graph at timestep 242 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 242 is tensor(0.0456, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.19430771 -0.08322918  0.19079915  1.        ]]. Action = [[-0.02453494 -0.8816951   0.8714478   0.10468507]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 243 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 243 is tensor(0.0376, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.19562562 -0.09488307  0.22094342  1.        ]]. Action = [[-0.50563216  0.68172884  0.98487175  0.89094305]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 244 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 244 is tensor(0.0331, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.19381908 -0.09590267  0.25524747  1.        ]]. Action = [[ 0.69536114 -0.83963346  0.26547623  0.78959477]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 245 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 245 is tensor(0.0463, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of 0
Current timestep = 246. State = [[-0.18027501 -0.11632553  0.2713594   1.        ]]. Action = [[ 0.3134141  -0.39850938  0.3983029   0.11974895]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 246 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 246 is tensor(0.0516, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 0
Current timestep = 247. State = [[-0.16983142 -0.11719284  0.2912337   1.        ]]. Action = [[-0.00165558  0.6510966   0.80283785  0.43994653]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 247 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 247 is tensor(0.0390, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.2688207   0.06631634  0.1173337   1.        ]]. Action = [[ 0.8992988   0.715466   -0.41110224 -0.09941792]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 249. State = [[-0.2605042   0.09109809  0.10566132  1.        ]]. Action = [[0.49302983 0.977721   0.15319502 0.66452813]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 249 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 249 is tensor(0.0289, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 0
Current timestep = 250. State = [[-0.25137204  0.11281049  0.10888977  1.        ]]. Action = [[-0.7734766  -0.9893328  -0.26200128  0.8003776 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 250 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 250 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.2489902   0.10117437  0.10461935  1.        ]]. Action = [[ 0.18552256 -0.86056423 -0.25784403  0.61660993]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 251 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 251 is tensor(0.0334, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.23419686  0.08765166  0.10033397  1.        ]]. Action = [[ 0.8606174  -0.01766723  0.01683509  0.79386854]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 252 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 252 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 0
Current timestep = 253. State = [[-0.22397307  0.08044183  0.08818474  1.        ]]. Action = [[-0.84981096 -0.46604502 -0.62264735  0.70790935]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 253 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 253 is tensor(0.0289, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.23161918  0.07173079  0.07421102  1.        ]]. Action = [[-0.8260332   0.35832143 -0.497258    0.01708353]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 254 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 254 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.23154384  0.07454138  0.0763358   1.        ]]. Action = [[-0.03109038  0.27031207  0.46445024  0.89506197]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 255 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 255 is tensor(0.0323, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 255 of 1
Current timestep = 256. State = [[-0.22923894  0.08600473  0.07317784  1.        ]]. Action = [[ 0.49370933  0.64994884 -0.7900864   0.8732852 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 256 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 256 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.224629    0.09646474  0.064747    1.        ]]. Action = [[-0.5058563  -0.42170322 -0.805312    0.63212097]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 257 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 257 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.22176051  0.09213446  0.07180669  1.        ]]. Action = [[-0.21033692 -0.42063922  0.9178721   0.6089003 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 258 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 258 is tensor(0.0313, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[-0.21048485  0.07433652  0.08635739  1.        ]]. Action = [[ 0.9177692  -0.8420248   0.08622313  0.22234571]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 259 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 259 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of 1
Current timestep = 260. State = [[-0.19296926  0.05528702  0.08513705  1.        ]]. Action = [[ 0.5369965  -0.1519171  -0.7916307   0.45255744]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 260 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 260 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.17863964  0.05349197  0.06967979  1.        ]]. Action = [[ 0.9510753  -0.7179432  -0.38551122  0.890908  ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Scene graph at timestep 261 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 261 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.17863964  0.05349197  0.06967979  1.        ]]. Action = [[ 0.34116173 -0.6662447   0.3156854  -0.0872134 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 262 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.0347, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.17664337  0.04602062  0.06948581  1.        ]]. Action = [[ 0.09574103 -0.5007612  -0.09846848  0.8865652 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 263 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.16942029  0.01878023  0.0750716   1.        ]]. Action = [[ 0.010818   -0.95392007  0.8595669   0.23051798]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 264 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of 1
Current timestep = 265. State = [[-1.6550525e-01 -7.6517108e-04  8.6248271e-02  1.0000000e+00]]. Action = [[ 0.34510362 -0.13776231 -0.24241543  0.82000256]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Scene graph at timestep 265 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 265 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-1.6550525e-01 -7.6517108e-04  8.6248271e-02  1.0000000e+00]]. Action = [[0.9002776  0.77475095 0.13278139 0.93578255]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Scene graph at timestep 266 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 266 is tensor(0.0288, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-1.6564165e-01 -7.5780402e-04  8.6240888e-02  1.0000000e+00]]. Action = [[ 0.70116043 -0.4671142   0.8635833   0.37804067]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Scene graph at timestep 267 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 267 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.1638162  -0.01537812  0.0992576   1.        ]]. Action = [[-0.2178266  -0.79058886  0.98293257  0.63563085]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 268 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.16299453 -0.03152957  0.124095    1.        ]]. Action = [[0.0902822  0.64246917 0.77026784 0.5896411 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Scene graph at timestep 269 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.0320, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.16697262 -0.04175408  0.13635468  1.        ]]. Action = [[-0.60915846 -0.503156    0.9130068   0.95330954]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 270 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.0318, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.19037712 -0.06801321  0.16133088  1.        ]]. Action = [[-0.45526373 -0.8435587  -0.00215268  0.6069293 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 271 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of 0
Current timestep = 272. State = [[-0.203313   -0.07654274  0.17510226  1.        ]]. Action = [[-0.5380995   0.82650244  0.85216     0.7587762 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 272 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 272 is tensor(0.0245, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.22607349 -0.07908291  0.20491846  1.        ]]. Action = [[-0.618282  -0.9401041  0.7481737  0.9027622]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 273 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.0230, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of 0
Current timestep = 274. State = [[-0.2464837  -0.10291114  0.23071474  1.        ]]. Action = [[ 0.18466973 -0.41470242  0.45733023  0.31505752]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 274 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 274 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.24624932 -0.10617986  0.24255076  1.        ]]. Action = [[0.06863856 0.42350078 0.02233136 0.87978435]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 275 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 275 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.23685418 -0.10351982  0.2527022   1.        ]]. Action = [[ 0.5916115  -0.25452173  0.81386757  0.88317037]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 276 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.0255, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.21759605 -0.11585595  0.28341225  1.        ]]. Action = [[ 0.38205016 -0.6532322   0.85263944  0.3302331 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 277 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.0287, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.2569771  -0.07279225  0.10560884  1.        ]]. Action = [[-0.95737433 -0.63324577  0.25350082 -0.10771412]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 279. State = [[-0.25837767 -0.07784392  0.08508558  1.        ]]. Action = [[ 0.11299336  0.07137835 -0.96608174  0.3706807 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 279 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 279 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.24899428 -0.09438497  0.062457    1.        ]]. Action = [[ 0.42483473 -0.9615571   0.92099345  0.1563443 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 280 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 280 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of 1
Current timestep = 281. State = [[-0.2385025  -0.1249046   0.08483362  1.        ]]. Action = [[-0.03659493 -0.52381235  0.95123076  0.9780222 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 281 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 281 is tensor(0.0323, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.23707117 -0.15075913  0.11705328  1.        ]]. Action = [[-0.1723808  -0.93088996  0.6403966   0.9218383 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 282 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 282 is tensor(0.0293, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 282 of 1
Current timestep = 283. State = [[-0.239503   -0.17448409  0.13246727  1.        ]]. Action = [[-0.85644615 -0.33727086  0.6366017   0.6071732 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 283 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 283 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.2326188  -0.17544454  0.14361496  1.        ]]. Action = [[0.61708426 0.09978497 0.79971373 0.65335333]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 284 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 284 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.21599959 -0.1731801   0.1568596   1.        ]]. Action = [[ 0.5698651  -0.02964056 -0.9804405   0.7094977 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 285 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 285 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.19455424 -0.1770093   0.1434989   1.        ]]. Action = [[ 0.55355287 -0.3518213  -0.05641693  0.83614874]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 286 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 286 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of 0
Current timestep = 287. State = [[-0.18345797 -0.18130039  0.14577228  1.        ]]. Action = [[-0.6783598   0.46765888  0.6430789   0.8046961 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 287 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 287 is tensor(0.0239, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 0
Current timestep = 288. State = [[-0.1957539  -0.1766203   0.15232475  1.        ]]. Action = [[0.8304603  0.7403363  0.79102373 0.6225195 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Scene graph at timestep 288 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 288 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of -1
Current timestep = 289. State = [[-0.20063294 -0.16875185  0.1483829   1.        ]]. Action = [[-0.30725718  0.5328164  -0.56532973  0.51960266]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 289 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 289 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.20201431 -0.16757084  0.15105376  1.        ]]. Action = [[ 0.12474871 -0.6525401   0.7320384   0.13826835]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 290 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 290 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.2014846  -0.1776729   0.17070839  1.        ]]. Action = [[-0.17502302 -0.03735554  0.8768356   0.65813136]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 291 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 291 is tensor(0.0242, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.19796929 -0.17095602  0.20083609  1.        ]]. Action = [[0.5937613  0.6722324  0.7499559  0.49418044]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 292 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 292 is tensor(0.0207, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.19080265 -0.16024491  0.21862745  1.        ]]. Action = [[ 0.05747604 -0.3578567  -0.10009992  0.83222795]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 293 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 293 is tensor(0.0236, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.19181658 -0.18087216  0.23204681  1.        ]]. Action = [[-0.19174147 -0.984309    0.8533951   0.52428293]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 294 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 294 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.1979672  -0.20872848  0.26240313  1.        ]]. Action = [[-0.49363077 -0.15948534  0.58686125  0.7368965 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 295 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 295 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 295 of 0
Current timestep = 296. State = [[-0.21454898 -0.227229    0.27817038  1.        ]]. Action = [[-0.6500401  -0.7177202  -0.20384634  0.08216882]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 296 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 296 is tensor(0.0215, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 296 of -1
Current timestep = 297. State = [[-0.22692747 -0.2317301   0.2746146   1.        ]]. Action = [[ 0.05933297  0.8758116  -0.6416849   0.5839014 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 297 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 297 is tensor(0.0138, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 0
Current timestep = 298. State = [[-0.22545263 -0.22355354  0.26990047  1.        ]]. Action = [[ 0.42487752 -0.8058715   0.9900068   0.7988775 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 298 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 298 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 0
Current timestep = 299. State = [[-0.21016754 -0.24813324  0.29194647  1.        ]]. Action = [[ 0.620528   -0.7926292   0.32607627  0.95773077]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 299 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 299 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.18936858 -0.26571733  0.29980093  1.        ]]. Action = [[ 0.99110866 -0.2303471  -0.45140076  0.53057814]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 300 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 300 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.16535674 -0.2753407   0.28870556  1.        ]]. Action = [[-0.6258406 -0.7993246  0.3867092  0.7057855]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 301 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 301 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.16535674 -0.2753407   0.28870556  1.        ]]. Action = [[-0.61130536 -0.8722118   0.919178    0.6685934 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 302 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 302 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of -1
Current timestep = 303. State = [[-0.15102018 -0.26645368  0.29908225  1.        ]]. Action = [[0.8154886 0.6346301 0.8862995 0.4652605]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 303 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 303 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.11818062 -0.24939138  0.31420282  1.        ]]. Action = [[ 0.8925766   0.33960712 -0.4311782   0.29282904]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 304 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 304 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 1
Current timestep = 305. State = [[-0.26059282 -0.18010685  0.10856529  1.        ]]. Action = [[-0.20954293  0.02380753  0.91139627 -0.02057499]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 306. State = [[-0.26233494 -0.19628336  0.08733063  1.        ]]. Action = [[ 0.05020368  0.15378654 -0.7038554   0.00556374]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 306 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 306 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.25515053 -0.19702637  0.07143951  1.        ]]. Action = [[ 0.41893864 -0.25589705  0.7265978   0.85886836]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 307 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 307 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[-0.24710166 -0.20095533  0.07720946  1.        ]]. Action = [[-0.7182166  0.5281452  0.9786463 -0.4273867]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 308 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 308 is tensor(0.0239, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.24352424 -0.20476829  0.08662717  1.        ]]. Action = [[ 0.18875897 -0.18849194  0.6638634   0.7252524 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 309 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 309 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of 1
Current timestep = 310. State = [[-0.2376712  -0.20883478  0.10131034  1.        ]]. Action = [[-0.88116115 -0.5561282   0.48390174  0.03141177]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 310 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 310 is tensor(0.0281, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.2376712  -0.20883478  0.10131034  1.        ]]. Action = [[-0.8558133  -0.3943364   0.571388    0.48481083]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 311 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 311 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.22766282 -0.21945499  0.09821939  1.        ]]. Action = [[ 0.7821002  -0.67376834 -0.60607284  0.8163128 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 312 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 312 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.21540216 -0.24552175  0.09926188  1.        ]]. Action = [[-0.46593118 -0.6145329   0.91012955  0.71926904]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 313 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 313 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.2137705  -0.26057744  0.11430206  1.        ]]. Action = [[ 0.6815796   0.13036919 -0.3612013   0.8288884 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 314 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 314 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of 0
Current timestep = 315. State = [[-0.19611216 -0.2605256   0.1157378   1.        ]]. Action = [[ 0.4456985  -0.17923152  0.35875142  0.4601786 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 315 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 315 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of 1
Current timestep = 316. State = [[-0.17575301 -0.25883126  0.1275408   1.        ]]. Action = [[0.6489037  0.5073798  0.58857024 0.79248345]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 316 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 316 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.15027334 -0.25868067  0.1443463   1.        ]]. Action = [[ 0.51149535 -0.4379127   0.3912698   0.8014132 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 317 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 317 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.13290027 -0.25534147  0.14794184  1.        ]]. Action = [[ 0.36275053  0.58968294 -0.7546933   0.41396737]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 318 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 318 is tensor(0.0157, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.11842518 -0.23189281  0.1452499   1.        ]]. Action = [[0.30529976 0.8453902  0.7852378  0.7771901 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 319 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 319 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.11222259 -0.20691736  0.16721527  1.        ]]. Action = [[-0.6830144   0.62464523  0.7755196   0.9499979 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 320 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 320 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.11626814 -0.18757877  0.18722355  1.        ]]. Action = [[0.47278678 0.2913382  0.1043601  0.73319983]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 321 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 321 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.25851685 -0.06849145  0.11777572  1.        ]]. Action = [[ 0.51143515 -0.14953369 -0.9255131  -0.0640859 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 323. State = [[-0.25156942 -0.08458711  0.09744356  1.        ]]. Action = [[ 0.78523445 -0.6371142  -0.7943827   0.8628342 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 323 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 323 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.22980778 -0.11572421  0.08092587  1.        ]]. Action = [[ 0.3604499  -0.9498498   0.880939    0.91978204]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 324 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 324 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.21346125 -0.14549705  0.09508585  1.        ]]. Action = [[ 0.5981209  -0.4755612   0.21212304  0.5734612 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 325 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 325 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.20305404 -0.16634457  0.0931205   1.        ]]. Action = [[-0.10022181 -0.8173178  -0.8708735   0.62192106]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 326 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 326 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.198322   -0.18415104  0.09073161  1.        ]]. Action = [[0.17428434 0.48248327 0.86086345 0.8352833 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 327 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 327 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.19856746 -0.17890261  0.09707747  1.        ]]. Action = [[-0.5012469   0.06671822 -0.45251465  0.57120013]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 328 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 328 is tensor(0.0237, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of 0
Current timestep = 329. State = [[-0.20236449 -0.18898785  0.09872691  1.        ]]. Action = [[ 0.29095304 -0.91383344  0.4921167   0.33223367]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 329 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 329 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.2595204   0.06589867  0.1243089   1.        ]]. Action = [[ 0.6898377  -0.4338978  -0.21210438 -0.23389298]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 331. State = [[-0.25967425  0.07310096  0.11168922  1.        ]]. Action = [[-0.9016173   0.4780059   0.42335737  0.85598755]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 331 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 331 is tensor(0.0261, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.2518522   0.0682613   0.11226457  1.        ]]. Action = [[ 0.60641384 -0.45099604 -0.04619378  0.4545039 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 332 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 332 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.24202673  0.07289161  0.10937855  1.        ]]. Action = [[-0.44959366  0.6363416   0.3499453   0.70580244]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 333 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 333 is tensor(0.0266, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.23979636  0.07851597  0.11340357  1.        ]]. Action = [[ 0.6435064  -0.3792519   0.352355    0.83652794]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 334 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 334 is tensor(0.0276, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 334 of 1
Current timestep = 335. State = [[-0.2290436   0.06760603  0.11200038  1.        ]]. Action = [[ 0.23883867 -0.3019488  -0.82142735  0.78683424]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 335 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 335 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.22542523  0.07385196  0.10652452  1.        ]]. Action = [[-0.37121153  0.7286917   0.6452246   0.96027017]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 336 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 336 is tensor(0.0246, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.22105162  0.07803205  0.11473926  1.        ]]. Action = [[ 0.64847374 -0.5122402   0.30543756  0.5372169 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 337 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 337 is tensor(0.0328, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.26229173  0.11654253  0.12217535  1.        ]]. Action = [[ 0.5739088   0.831313    0.88670945 -0.10648978]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 339. State = [[-0.25920212  0.12946822  0.10853818  1.        ]]. Action = [[-0.27141827 -0.6356469  -0.07467085  0.566756  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 339 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 339 is tensor(0.0321, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.2592359   0.1294638   0.10812373  1.        ]]. Action = [[-0.6071337 -0.8025379  0.8186114  0.5871241]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 340 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 340 is tensor(0.0398, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.2508615   0.14163873  0.11556981  1.        ]]. Action = [[0.54328394 0.7322252  0.8332864  0.22483218]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 341 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 341 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[-0.2389796   0.1589783   0.12641262  1.        ]]. Action = [[-0.60903233 -0.59355104  0.8707788  -0.5406194 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 342 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 342 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.22699973  0.17238946  0.1356833   1.        ]]. Action = [[0.94385886 0.90435743 0.6084448  0.87550926]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 343 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 343 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.19847329  0.20674706  0.1633703   1.        ]]. Action = [[0.3073206 0.7483697 0.6308367 0.5809643]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 344 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 344 is tensor(0.0320, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.17689957  0.214499    0.1873427   1.        ]]. Action = [[ 0.6494125  -0.70272094  0.641371    0.57016087]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 345 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 345 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of 1
Current timestep = 346. State = [[-0.26718953  0.06916378  0.11184271  1.        ]]. Action = [[ 0.3166772  -0.14301538  0.265903   -0.06441224]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 347. State = [[-0.26068127  0.06561984  0.09458139  1.        ]]. Action = [[ 0.4246174  -0.87509066 -0.26282614  0.6231725 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 347 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 347 is tensor(0.0194, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of 0
Current timestep = 348. State = [[-0.25062847  0.05263562  0.08924066  1.        ]]. Action = [[-0.8788745   0.01798165  0.811985    0.03724909]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 348 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 348 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.23938088  0.05648169  0.09321324  1.        ]]. Action = [[0.6695392  0.4031409  0.59430647 0.82737935]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 349 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 349 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of 1
Current timestep = 350. State = [[-0.22498372  0.07104225  0.10870796  1.        ]]. Action = [[-0.00644314  0.7036426   0.9167118   0.11557221]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 350 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 350 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 350 of 1
Current timestep = 351. State = [[-0.22748736  0.09813222  0.1302816   1.        ]]. Action = [[-0.38093388  0.58473206  0.14805627  0.53671503]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 351 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 351 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of 0
Current timestep = 352. State = [[-0.24009596  0.1007605   0.12932527  1.        ]]. Action = [[-0.5615365  -0.9029817  -0.52846634  0.6526227 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 352 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 352 is tensor(0.0175, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of 0
Current timestep = 353. State = [[-0.25041246  0.0930336   0.13297652  1.        ]]. Action = [[0.15554881 0.80645263 0.77672195 0.7071123 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 353 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 353 is tensor(0.0295, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.24842791  0.10096374  0.14986299  1.        ]]. Action = [[ 0.02269971 -0.5295205   0.43325377  0.4655962 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 354 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 354 is tensor(0.0314, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 354 of 1
Current timestep = 355. State = [[-0.23495477  0.08097787  0.16804482  1.        ]]. Action = [[ 0.8727765 -0.6765936  0.5578376  0.9196174]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 355 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 355 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of 1
Current timestep = 356. State = [[-0.2117025   0.05546721  0.18656203  1.        ]]. Action = [[ 0.3985963  -0.73472035  0.03586924  0.8819704 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 356 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 356 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of 1
Current timestep = 357. State = [[-0.2634996   0.03037397  0.12371267  1.        ]]. Action = [[-0.52342683 -0.5538145   0.38819242 -0.11976242]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 358. State = [[-0.25116092  0.04898224  0.110833    1.        ]]. Action = [[0.8985833  0.91242576 0.1313175  0.15919256]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 358 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 358 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.2326208   0.06739324  0.11248003  1.        ]]. Action = [[ 0.10249436 -0.39178944  0.10626161  0.21104193]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 359 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 359 is tensor(0.0314, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 0
Current timestep = 360. State = [[-0.21957415  0.06280909  0.11085665  1.        ]]. Action = [[ 0.7663908   0.0760932  -0.31041193  0.7812766 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 360 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 360 is tensor(0.0263, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.19418755  0.05139325  0.1105829   1.        ]]. Action = [[ 0.39301872 -0.82342714  0.9683385   0.7998396 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 361 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 361 is tensor(0.0220, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of 1
Current timestep = 362. State = [[-0.1788051   0.03337791  0.1293105   1.        ]]. Action = [[ 0.8316364  -0.04367077  0.6297178   0.4964919 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 362 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 362 is tensor(0.0236, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.2677614   0.1733885   0.12373346  1.        ]]. Action = [[-0.41822338  0.74104524  0.916111   -0.04753906]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 364. State = [[-0.2532633  0.181386   0.1049032  1.       ]]. Action = [[ 0.77233446 -0.7661554  -0.34991872  0.7246101 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 364 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 364 is tensor(0.0227, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.23122835  0.17373596  0.10399601  1.        ]]. Action = [[0.18398488 0.256109   0.9264405  0.9414691 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 365 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 365 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.2168973   0.18024112  0.1231918   1.        ]]. Action = [[0.66569424 0.2471348  0.38280916 0.61311924]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 366 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 366 is tensor(0.0305, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of 1
Current timestep = 367. State = [[-0.1971496   0.18300839  0.13464181  1.        ]]. Action = [[ 0.37067485 -0.22253722  0.01726007  0.73904586]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 367 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 367 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of 1
Current timestep = 368. State = [[-0.18884616  0.17882924  0.13862695  1.        ]]. Action = [[ 0.85584664 -0.4529519   0.98449695  0.8409152 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 368 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 368 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.18805948  0.17842788  0.13920256  1.        ]]. Action = [[ 0.72295976 -0.5968406   0.1973089   0.78938925]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 369 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 369 is tensor(0.0282, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 369 of -1
Current timestep = 370. State = [[-0.18805948  0.17842788  0.13920256  1.        ]]. Action = [[ 0.75772965 -0.8652627  -0.12074387  0.74009514]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 370 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 370 is tensor(0.0246, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.19316801  0.18097602  0.14196627  1.        ]]. Action = [[-0.8330328  -0.03682822  0.38785362  0.75158346]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 371 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 371 is tensor(0.0325, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of 0
Current timestep = 372. State = [[-0.20457517  0.17590617  0.15774906  1.        ]]. Action = [[-0.4119432 -0.4922632  0.7546922  0.5521529]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 372 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 372 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.20892055  0.17064229  0.18592757  1.        ]]. Action = [[0.7897568  0.5826701  0.69401467 0.6317289 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 373 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 373 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of 1
Current timestep = 374. State = [[-0.20163594  0.18957102  0.21430184  1.        ]]. Action = [[-0.41802788  0.5496352   0.8501198   0.69432664]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 374 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 374 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of 1
Current timestep = 375. State = [[-0.20122325  0.19644374  0.2444639   1.        ]]. Action = [[ 0.7128713  -0.4494002   0.50705326  0.91228735]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 375 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 375 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.18024689  0.2000457   0.27514052  1.        ]]. Action = [[0.4367671  0.74420834 0.98920834 0.36935115]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 376 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 376 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.16545106  0.22071664  0.309159    1.        ]]. Action = [[0.07239389 0.35552025 0.5060682  0.2649231 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 377 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 377 is tensor(0.0334, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 0
Current timestep = 378. State = [[-0.16224617  0.22162819  0.33660176  1.        ]]. Action = [[-0.517791   -0.72872895  0.9754565   0.9001517 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 378 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 378 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of 0
Current timestep = 379. State = [[-0.17745955  0.2135344   0.35722193  1.        ]]. Action = [[-0.44594586  0.264969   -0.6975948   0.8453224 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 379 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 379 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of 0
Current timestep = 380. State = [[-0.18878774  0.22587822  0.35642695  1.        ]]. Action = [[0.192698   0.66675735 0.8441644  0.9311626 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 380 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 380 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of 0
Current timestep = 381. State = [[-0.19183318  0.23579273  0.36391243  1.        ]]. Action = [[-0.64103574 -0.46892345 -0.3100227   0.16757238]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 381 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 381 is tensor(0.0314, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.20353393  0.22817813  0.35900036  1.        ]]. Action = [[-0.02576578 -0.11301917 -0.52855814  0.39975035]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 382 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 382 is tensor(0.0354, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.20566563  0.230522    0.36092386  1.        ]]. Action = [[0.2587142  0.48616505 0.9187021  0.8546636 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 383 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 383 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.21095684  0.24912246  0.36245394  1.        ]]. Action = [[-0.45333922  0.84313154 -0.53931504  0.8013451 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 384 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 384 is tensor(0.0358, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.22531801  0.26627535  0.3604075   1.        ]]. Action = [[0.9475713 0.781543  0.7840016 0.8767586]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 385 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 385 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.23277934  0.27277303  0.3666302   1.        ]]. Action = [[-0.76976645  0.0735153   0.5594647   0.6626861 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 386 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 386 is tensor(0.0363, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.24730627  0.27788234  0.37311104  1.        ]]. Action = [[-0.5169215   0.40800858  0.56364584  0.590431  ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 387 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 387 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.24738222  0.2778488   0.37310767  1.        ]]. Action = [[-0.09268236 -0.24859297  0.8520994   0.7859552 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 388 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 388 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.24038394  0.27335787  0.37824944  1.        ]]. Action = [[ 0.7862508  -0.19035208  0.3070736   0.6385112 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 389 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 389 is tensor(0.0354, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of 1
Current timestep = 390. State = [[-0.23395345  0.26595563  0.38529447  1.        ]]. Action = [[-0.69754547 -0.51837105  0.9783691   0.7475238 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 390 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 390 is tensor(0.0341, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.2339285   0.26589352  0.38530645  1.        ]]. Action = [[-0.3383128   0.6398133   0.58388174 -0.08590698]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 391 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 391 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.2297365   0.26769817  0.37816146  1.        ]]. Action = [[ 0.913059    0.5186688  -0.85662186  0.49952865]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 392 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 392 is tensor(0.0392, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of 1
Current timestep = 393. State = [[-0.2007653   0.2853035   0.35789514  1.        ]]. Action = [[-0.82028896  0.77538466 -0.48441702  0.67022014]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 393 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 393 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.2007653   0.2853035   0.35789514  1.        ]]. Action = [[-0.56123793  0.8161465  -0.19157219  0.6994281 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Scene graph at timestep 394 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 394 is tensor(0.0369, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.19391939  0.27421886  0.3625477   1.        ]]. Action = [[ 0.3705951  -0.65869063  0.35515583  0.493227  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 395 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 395 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of 1
Current timestep = 396. State = [[-0.25585806 -0.01146421  0.10181199  1.        ]]. Action = [[ 0.566725   -0.5943635  -0.41080606 -0.06044322]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 397. State = [[-0.25524092 -0.01282551  0.08910575  1.        ]]. Action = [[-0.75265235  0.9040332   0.9674883   0.9179251 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 397 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 397 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.25524092 -0.01282551  0.08910575  1.        ]]. Action = [[-0.40397     0.44385576 -0.49143732  0.9047719 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 398 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 398 is tensor(0.0251, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.24882682 -0.02183774  0.09536371  1.        ]]. Action = [[ 0.38432002 -0.42214024  0.8738115   0.668887  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 399 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 399 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of 1
Current timestep = 400. State = [[-0.24303813 -0.04619122  0.10641696  1.        ]]. Action = [[-0.05253249 -0.9314161   0.23456025  0.7659924 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 400 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 400 is tensor(0.0223, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of 0
Current timestep = 401. State = [[-0.24464908 -0.05942846  0.11678824  1.        ]]. Action = [[-0.43811643  0.7081505   0.40361023  0.86902714]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 401 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 401 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of 1
Current timestep = 402. State = [[-0.2480865  -0.05027883  0.12706752  1.        ]]. Action = [[-0.43192357  0.250054    0.93058467  0.84558713]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 402 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 402 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.24177438 -0.03511778  0.13239536  1.        ]]. Action = [[0.7413006  0.9254323  0.36015332 0.68150365]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 403 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 403 is tensor(0.0208, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of 1
Current timestep = 404. State = [[-0.231724   -0.01559697  0.14107518  1.        ]]. Action = [[ 0.07303035 -0.11916572 -0.23389786  0.31518817]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 404 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 404 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.21971138 -0.01022271  0.14268145  1.        ]]. Action = [[ 0.85677505  0.41320407 -0.03346032  0.6340606 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 405 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 405 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[-0.19836228 -0.00364984  0.15167764  1.        ]]. Action = [[-0.1503017  -0.30417562  0.69773555  0.7094443 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 406 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 406 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[-0.19954285 -0.01520021  0.16086085  1.        ]]. Action = [[-0.35024822 -0.49839568  0.08545482  0.7978802 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 407 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 407 is tensor(0.0179, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 407 of 0
Current timestep = 408. State = [[-0.20170644 -0.04340435  0.1686274   1.        ]]. Action = [[-0.02571315 -0.9528322   0.42911625  0.5550258 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 408 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 408 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of 1
Current timestep = 409. State = [[-0.20436159 -0.06265544  0.18001603  1.        ]]. Action = [[ 0.8032315  -0.32638204 -0.6256263   0.15963244]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Scene graph at timestep 409 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 409 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.21408148 -0.05696314  0.17749725  1.        ]]. Action = [[-0.82691365  0.4661616  -0.21562141  0.9328306 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 410 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 410 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 410 of 0
Current timestep = 411. State = [[-0.23147452 -0.04017413  0.18540536  1.        ]]. Action = [[-0.21543723  0.70222986  0.7725098   0.8644731 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 411 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 411 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of 1
Current timestep = 412. State = [[-0.23659119 -0.02999313  0.20180558  1.        ]]. Action = [[ 0.54487205 -0.49970448 -0.18150705  0.70621645]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 412 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 412 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of 1
Current timestep = 413. State = [[-0.22481014 -0.04884572  0.21088776  1.        ]]. Action = [[ 0.36859608 -0.81605184  0.625945    0.41313505]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 413 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 413 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of 1
Current timestep = 414. State = [[-0.2089654  -0.07800173  0.23522453  1.        ]]. Action = [[ 0.3581072  -0.69882846  0.85716224  0.95171046]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 414 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 414 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.19890797 -0.09776467  0.25646207  1.        ]]. Action = [[ 0.7340199   0.09288311 -0.8835889   0.8900163 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Scene graph at timestep 415 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 415 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.20037906 -0.09494749  0.2603671   1.        ]]. Action = [[-0.3346324  0.2440654  0.3005519  0.6643281]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 416 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 416 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of 0
Current timestep = 417. State = [[-0.20211661 -0.08968458  0.26663712  1.        ]]. Action = [[0.18444908 0.20367968 0.04824507 0.29365468]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 417 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 417 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.26273635  0.04090799  0.10936233  1.        ]]. Action = [[-0.39510608 -0.7967406   0.22720575 -0.20847166]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 419. State = [[-0.26028606  0.04688431  0.09487706  1.        ]]. Action = [[-0.46860468 -0.05847007  0.3937745   0.1845516 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 419 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 419 is tensor(0.0203, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.2517424   0.05510641  0.09666726  1.        ]]. Action = [[0.56854343 0.46671426 0.29175603 0.52321553]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 420 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 420 is tensor(0.0190, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.2438113   0.0681366   0.09924668  1.        ]]. Action = [[-0.11411488  0.19314575  0.12929654  0.5694215 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 421 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 421 is tensor(0.0219, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 421 of -1
Current timestep = 422. State = [[-0.23497379  0.06692302  0.10570174  1.        ]]. Action = [[ 0.6587924 -0.4273231  0.5371603  0.8444778]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 422 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 422 is tensor(0.0195, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.21579093  0.05934937  0.12299272  1.        ]]. Action = [[ 0.15274513 -0.07478333  0.5737076   0.5737592 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 423 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 423 is tensor(0.0233, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.2137728   0.07205354  0.14941458  1.        ]]. Action = [[-0.2859118  0.9132025  0.9372164  0.5193484]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 424 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 424 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.2569441  -0.16978043  0.12516244  1.        ]]. Action = [[-0.5153569   0.51116693  0.28913367 -0.30854285]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 426. State = [[-0.24894243 -0.18025538  0.11816203  1.        ]]. Action = [[0.7683792 0.6928829 0.9196875 0.8884747]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 426 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 426 is tensor(0.0206, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.23350699 -0.15996297  0.13563906  1.        ]]. Action = [[-0.24256408  0.67617035  0.60616016  0.09792459]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 427 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 427 is tensor(0.0190, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of 1
Current timestep = 428. State = [[-0.23966436 -0.143111    0.14856909  1.        ]]. Action = [[-0.60707533  0.23757589 -0.14876735  0.9303148 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 428 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 428 is tensor(0.0201, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.23967129 -0.12961231  0.15646994  1.        ]]. Action = [[0.8840318  0.47165012 0.65292084 0.40902185]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 429 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 429 is tensor(0.0207, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.2331511  -0.12362889  0.17189297  1.        ]]. Action = [[-0.22669864 -0.5112142  -0.3159839   0.7718686 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 430 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 430 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 0
Current timestep = 431. State = [[-0.22297643 -0.1413737   0.17741737  1.        ]]. Action = [[ 0.8970852  -0.94324076  0.42579842  0.7613392 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 431 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 431 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.20248267 -0.17297234  0.18008897  1.        ]]. Action = [[ 0.39779532 -0.74224544 -0.60494715  0.68906736]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 432 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 432 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.18748401 -0.1952568   0.17509378  1.        ]]. Action = [[ 0.05438018 -0.22932124  0.803236    0.5578582 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 433 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 433 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.17777248 -0.21335971  0.19420251  1.        ]]. Action = [[ 0.35130644 -0.59715724  0.488438    0.82740045]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 434 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 434 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.1575224  -0.21972035  0.21735267  1.        ]]. Action = [[0.6379123  0.5645554  0.71628296 0.28403223]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 435 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 435 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of 1
Current timestep = 436. State = [[-0.13191627 -0.21728659  0.24530417  1.        ]]. Action = [[ 0.6154015  -0.34421706  0.71633124  0.74987245]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 436 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 436 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.11600593 -0.22353134  0.26414418  1.        ]]. Action = [[ 0.01155722 -0.10761839 -0.1796251   0.8738134 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 437 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 437 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of 0
Current timestep = 438. State = [[-0.12226767 -0.22997095  0.27431458  1.        ]]. Action = [[-0.85896534 -0.0977025   0.6859329   0.16472757]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 438 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 438 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of 1
Current timestep = 439. State = [[-0.13866134 -0.24088521  0.28903028  1.        ]]. Action = [[-0.5887126  -0.11108994  0.08108294  0.7566178 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 439 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 439 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.16042195 -0.23679356  0.2892291   1.        ]]. Action = [[-0.36759222  0.39107013 -0.34099686  0.6884823 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 440 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 440 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.16519894 -0.21923636  0.29053757  1.        ]]. Action = [[0.23053789 0.6744647  0.64411414 0.6493089 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 441 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 441 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 441 of 1
Current timestep = 442. State = [[-0.15968859 -0.18855968  0.3116644   1.        ]]. Action = [[0.43475056 0.7267816  0.7090304  0.919106  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 442 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 442 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[-0.1512978  -0.15833424  0.32194898  1.        ]]. Action = [[ 0.5496104   0.6550642  -0.48637033  0.9384711 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 443 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 443 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of 1
Current timestep = 444. State = [[-0.13161889 -0.12760173  0.32720187  1.        ]]. Action = [[0.48691607 0.93241084 0.8068421  0.7144853 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 444 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 444 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 444 of 1
Current timestep = 445. State = [[-0.1184276  -0.10395026  0.33874345  1.        ]]. Action = [[ 0.05519485  0.20511532 -0.4849121   0.3201269 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 445 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 445 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of 1
Current timestep = 446. State = [[-0.11040557 -0.08470804  0.33338472  1.        ]]. Action = [[ 0.75362396  0.9343432  -0.08363491  0.70010257]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 446 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 446 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of 1
Current timestep = 447. State = [[-0.09421269 -0.07498834  0.33180076  1.        ]]. Action = [[-0.426044   -0.80256116  0.15353703  0.7713947 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 447 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 447 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.0925003  -0.07973217  0.33779904  1.        ]]. Action = [[0.68718624 0.6247091  0.30294538 0.8630601 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 448 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 448 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of 1
Current timestep = 449. State = [[-0.07760316 -0.06499424  0.34908977  1.        ]]. Action = [[0.5502186  0.27633095 0.22972012 0.8669995 ]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Scene graph at timestep 449 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 449 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of 1
Current timestep = 450. State = [[-0.05042735 -0.05594123  0.36435848  1.        ]]. Action = [[0.9070451  0.06112921 0.63130534 0.5921235 ]]. Reward = [0.]
Curr episode timestep = 24
Above hoop
Scene graph at timestep 450 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 450 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of 1
Current timestep = 451. State = [[-0.02554043 -0.05389676  0.38183329  1.        ]]. Action = [[0.8900995  0.6728096  0.89441013 0.8130554 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 451 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 451 is tensor(0.0138, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.02554043 -0.05389676  0.38183329  1.        ]]. Action = [[0.08933461 0.6731329  0.74005353 0.33789754]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 452 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 452 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.01738065 -0.04864987  0.38460883  1.        ]]. Action = [[0.8122363  0.32198715 0.2391727  0.31493652]]. Reward = [0.]
Curr episode timestep = 27
Above hoop
Scene graph at timestep 453 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 453 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of 1
Current timestep = 454. State = [[ 0.01104645 -0.04522641  0.39644215  1.        ]]. Action = [[ 0.60019505 -0.30318725  0.39870548  0.8145335 ]]. Reward = [0.]
Curr episode timestep = 28
Above hoop
Scene graph at timestep 454 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 454 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[ 0.02523738 -0.04832217  0.4057256   1.        ]]. Action = [[ 0.65698314 -0.14302635  0.77935207  0.47809446]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 455 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 455 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[ 0.02523738 -0.04832217  0.4057256   1.        ]]. Action = [[-0.09379405  0.8774183   0.5847142   0.7016244 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 456 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 456 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[ 0.02685332 -0.04589646  0.40403286  1.        ]]. Action = [[ 0.83923054  0.1389445  -0.29158533  0.8492948 ]]. Reward = [0.]
Curr episode timestep = 31
Above hoop
Scene graph at timestep 457 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 457 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[ 0.06140364 -0.02982529  0.3886032   1.        ]]. Action = [[-0.07573819  0.9076071  -0.45120782  0.7799033 ]]. Reward = [0.]
Curr episode timestep = 32
Above hoop
Scene graph at timestep 458 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 458 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[ 0.06441893 -0.01265927  0.37978497  1.        ]]. Action = [[ 0.88669443 -0.08514744  0.5588492   0.86341333]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 459 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 459 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[ 0.06441893 -0.01265927  0.37978497  1.        ]]. Action = [[0.54999065 0.9050865  0.8025315  0.66406155]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 460 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 460 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[ 0.06441893 -0.01265927  0.37978497  1.        ]]. Action = [[ 0.833524   -0.42304122  0.8057449   0.7162106 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 461 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 461 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[ 0.06142944 -0.00544196  0.3784526   1.        ]]. Action = [[-0.87138635  0.31779587 -0.31282055  0.7843909 ]]. Reward = [0.]
Curr episode timestep = 36
Above hoop
Scene graph at timestep 462 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 462 is tensor(0.0154, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[0.05506253 0.00444564 0.37823746 1.        ]]. Action = [[-0.12650889  0.12356722  0.91450644  0.785028  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 463 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 463 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[0.05506253 0.00444564 0.37823746 1.        ]]. Action = [[0.3372996  0.75057936 0.9035685  0.924832  ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 464 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 464 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[0.05506253 0.00444564 0.37823746 1.        ]]. Action = [[-0.74912035  0.82532346  0.8801526   0.8055718 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 465 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 465 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[0.05506253 0.00444564 0.37823746 1.        ]]. Action = [[-0.00956208 -0.8181506   0.8199029   0.9574735 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 466 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 466 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[ 0.05551149 -0.0078483   0.38325676  1.        ]]. Action = [[ 0.32475734 -0.73588175  0.6224905   0.68436694]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Scene graph at timestep 467 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 467 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[ 0.05510347 -0.02267294  0.39136124  1.        ]]. Action = [[0.34366262 0.2527778  0.73400927 0.45833862]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 468 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 468 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 468 of -1
Current timestep = 469. State = [[ 0.05510347 -0.02267294  0.39136124  1.        ]]. Action = [[-0.07002431  0.17413664  0.9680526   0.4976511 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 469 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 469 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[ 0.05510347 -0.02267294  0.39136124  1.        ]]. Action = [[-0.23270047  0.7963433   0.7693751   0.6229478 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 470 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 470 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[ 0.05537544 -0.02121884  0.38538846  1.        ]]. Action = [[ 0.22202384  0.16745162 -0.69783306  0.17884791]]. Reward = [0.]
Curr episode timestep = 45
Above hoop
Scene graph at timestep 471 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 471 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[ 0.05634006 -0.02065248  0.37280768  1.        ]]. Action = [[-0.16623175 -0.10885251  0.24059796  0.8477423 ]]. Reward = [0.]
Curr episode timestep = 46
Above hoop
Scene graph at timestep 472 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 472 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[ 0.0572106  -0.02507262  0.3723459   1.        ]]. Action = [[ 0.27780175 -0.26401556 -0.21896267  0.795779  ]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Scene graph at timestep 473 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 473 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[ 0.06193042 -0.01494878  0.3761572   1.        ]]. Action = [[0.5050676  0.94583094 0.46980286 0.9150393 ]]. Reward = [0.]
Curr episode timestep = 48
Above hoop
Scene graph at timestep 474 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 474 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[ 7.1802005e-02 -7.4513099e-04  3.8041642e-01  1.0000000e+00]]. Action = [[ 0.86680377  0.7029784  -0.567537    0.55183315]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 475 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 475 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[ 7.1791098e-02 -6.6559820e-04  3.8034448e-01  1.0000000e+00]]. Action = [[ 0.17444313 -0.90542454  0.9253292   0.7390611 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 476 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 476 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[ 7.1796030e-02 -6.6522829e-04  3.8027254e-01  1.0000000e+00]]. Action = [[ 0.9781387  -0.15826082 -0.5666459   0.68719053]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 477 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 477 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[ 0.0728168  -0.01410057  0.38175422  1.        ]]. Action = [[ 0.02744269 -0.97142804  0.01298356  0.364676  ]]. Reward = [0.]
Curr episode timestep = 52
Above hoop
Scene graph at timestep 478 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 478 is tensor(0.0176, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[ 0.07456215 -0.02911344  0.38270193  1.        ]]. Action = [[ 0.51713943  0.20589578 -0.3252195   0.5229504 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 479 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 479 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[ 0.07456215 -0.02911344  0.38270193  1.        ]]. Action = [[-0.22533357 -0.95527065  0.7871314   0.6586181 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 480 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 480 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[ 0.07548781 -0.03788724  0.37841836  1.        ]]. Action = [[ 0.09319663 -0.5200051  -0.5089313   0.8305578 ]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Scene graph at timestep 481 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 481 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[ 0.07918391 -0.04891673  0.36406055  1.        ]]. Action = [[ 0.82312644 -0.4365579   0.12430871  0.8382201 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 482 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 482 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[ 0.07918391 -0.04891673  0.36406055  1.        ]]. Action = [[ 0.69235647 -0.1394921  -0.7388747   0.7337997 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 483 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 483 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[ 0.07918391 -0.04891673  0.36406055  1.        ]]. Action = [[ 0.9288511  -0.64734244 -0.27481067  0.6383568 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 484 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 484 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[ 0.07918391 -0.04891673  0.36406055  1.        ]]. Action = [[ 0.7769501  -0.25208336  0.67069936  0.33166647]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 485 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 485 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[ 0.07921056 -0.04402255  0.36494324  1.        ]]. Action = [[-0.03014928  0.4301448   0.13149607  0.88635516]]. Reward = [0.]
Curr episode timestep = 60
Above hoop
Scene graph at timestep 486 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 486 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[ 0.08251829 -0.04059095  0.37040654  1.        ]]. Action = [[ 0.44226313  0.90919495 -0.05267572  0.76210666]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 487 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 487 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[ 0.08349319 -0.03583656  0.37357032  1.        ]]. Action = [[-0.02474195  0.26430202  0.30109286  0.2450825 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 488 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 488 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[-0.8113123   0.20495772  0.8520167   0.35861838]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Scene graph at timestep 489 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 489 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[0.8776407  0.60421014 0.89204574 0.7856271 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 490 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 490 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[ 0.6207762   0.6592667  -0.44870067  0.7745302 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Scene graph at timestep 491 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 491 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of -1
Current timestep = 492. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[0.6815617  0.25256717 0.6900549  0.7278662 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Scene graph at timestep 492 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 492 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[ 0.78074527 -0.16602433  0.33684027  0.91792655]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 493 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 493 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[-0.03575206 -0.95525986  0.8585266   0.48074543]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 494 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 494 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[ 0.08325664 -0.0311961   0.37464488  1.        ]]. Action = [[ 0.7731092 -0.5674591 -0.841705   0.657815 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Scene graph at timestep 495 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 495 is tensor(0.0131, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of -1
Current timestep = 496. State = [[ 0.08213782 -0.03276974  0.38488853  1.        ]]. Action = [[-0.30251074 -0.15976155  0.6081443   0.77035856]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 496 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 496 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[ 0.08018235 -0.03329004  0.40046754  1.        ]]. Action = [[-0.42660505  0.8252914   0.9359009   0.50357866]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 497 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 497 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[ 0.0801329  -0.03367935  0.4005573   1.        ]]. Action = [[ 0.89733005 -0.23003012  0.8804717   0.5627456 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 498 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 498 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[ 0.4656328  -0.02297938  0.16747737  0.76061225]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Scene graph at timestep 499 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 499 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[0.3843434  0.24243128 0.52766633 0.4102857 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Scene graph at timestep 500 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 500 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[ 0.50752354 -0.76244324  0.93904805  0.60989237]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 501 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 501 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[ 0.21757495 -0.12416458  0.88168526  0.88838077]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Scene graph at timestep 502 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 502 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[-0.63944924 -0.08064824  0.8060715   0.5313182 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 503 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 503 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[-0.35501546  0.45262074  0.37359464  0.49351108]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Scene graph at timestep 504 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 504 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[ 0.58965755  0.44758105 -0.28292936 -0.09339976]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 505 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 505 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 505 of -1
Current timestep = 506. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[ 0.9387233  -0.8136561   0.22049892  0.92680717]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Scene graph at timestep 506 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 506 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[ 0.08012586 -0.03373464  0.4005701   1.        ]]. Action = [[-0.20463026  0.3892231   0.6515503   0.6533184 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Scene graph at timestep 507 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 507 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[ 0.08047672 -0.03371048  0.39721504  1.        ]]. Action = [[ 0.1159668   0.010167   -0.36284876  0.6622069 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 508 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 508 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[-0.49741596  0.24530637  0.5269238   0.72184753]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 509 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 509 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.72753525 0.6945057  0.39836824 0.5020969 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Scene graph at timestep 510 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 510 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.0023613  0.20817685 0.6359565  0.80626965]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Scene graph at timestep 511 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 511 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.28717494 0.62639594 0.81018066 0.67185473]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 512 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 512 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[-0.42207444 -0.42507994  0.70389783  0.87463844]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Scene graph at timestep 513 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 513 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.0333966  0.35252    0.75369287 0.790902  ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Scene graph at timestep 514 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 514 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.6114273 0.7154956 0.5282161 0.6391823]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Scene graph at timestep 515 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 515 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.61473405 -0.12407398  0.74187994  0.90458584]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Scene graph at timestep 516 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 516 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.9055306  -0.14075279 -0.9225044   0.38050318]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Scene graph at timestep 517 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 517 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.8892143  0.42234695 0.48353755 0.7359935 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Scene graph at timestep 518 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 518 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.48009193  0.5301728  -0.26906252  0.88924026]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Scene graph at timestep 519 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 519 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of 0
Current timestep = 520. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.84365225 -0.41464418  0.813488    0.77024627]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Scene graph at timestep 520 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 520 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of 0
Current timestep = 521. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[-0.48781335  0.55780935  0.4002658   0.5797689 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Scene graph at timestep 521 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 521 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of 0
Current timestep = 522. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.12126386 -0.8854081   0.95141006  0.6065321 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Scene graph at timestep 522 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 522 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 522 of 0
Current timestep = 523. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[-0.31861877  0.8477489   0.5826442   0.35228765]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Scene graph at timestep 523 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 523 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 0
Current timestep = 524. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.03671372  0.18873107  0.91217387 -0.07666469]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Scene graph at timestep 524 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 524 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of 0
Current timestep = 525. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[0.5355978 0.6985223 0.8769436 0.7305074]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Scene graph at timestep 525 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 525 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of 0
Current timestep = 526. State = [[ 0.08055498 -0.0337143   0.39599147  1.        ]]. Action = [[ 0.5216689  -0.6928181  -0.18138772  0.78769803]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Scene graph at timestep 526 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 526 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 526 of 0
Current timestep = 527. State = [[-0.26886946  0.10370523  0.11661206  1.        ]]. Action = [[-0.17492932  0.19240463  0.18552327  0.42280746]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 528. State = [[-0.26586047  0.11542364  0.10209648  1.        ]]. Action = [[-0.17426366 -0.08397365  0.44049037  0.4801997 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 529. State = [[-0.26588824  0.11546821  0.10209649  1.        ]]. Action = [[-0.75605494  0.73329294  0.6595961   0.6382718 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 530. State = [[-0.25106707  0.10396086  0.10843673  1.        ]]. Action = [[ 0.9415028  -0.8275571   0.81836486  0.47957182]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 531. State = [[-0.22403081  0.09784319  0.12967847  1.        ]]. Action = [[0.653054   0.36019456 0.93964565 0.8885157 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 532. State = [[-0.19362876  0.09114198  0.16276897  1.        ]]. Action = [[ 0.869586   -0.6465161   0.78171563  0.82324076]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 533. State = [[-0.17231509  0.08203135  0.18232694  1.        ]]. Action = [[0.82166135 0.65513754 0.79549    0.59203887]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 534. State = [[-0.16955334  0.08071259  0.18446364  1.        ]]. Action = [[0.5986546  0.5752289  0.10075212 0.94291234]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 535. State = [[-0.16936205  0.08068964  0.18472466  1.        ]]. Action = [[0.20709682 0.47393847 0.22222042 0.01219738]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 536. State = [[-0.16798398  0.06680854  0.18655515  1.        ]]. Action = [[-0.21536463 -0.808198    0.06643021  0.627321  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 537. State = [[-0.16774592  0.05313578  0.1884207   1.        ]]. Action = [[ 0.18472636  0.10125172 -0.8184405   0.9107969 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 538. State = [[-0.1678692   0.05196042  0.18853933  1.        ]]. Action = [[ 0.35131478 -0.92053956  0.1469357   0.6605048 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 539. State = [[-0.16899407  0.04003393  0.19994488  1.        ]]. Action = [[-0.32495457 -0.67568284  0.7468271   0.7936437 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 540. State = [[-0.1723319   0.02703747  0.21690224  1.        ]]. Action = [[ 0.69681525 -0.7162819   0.18368256  0.726954  ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 541. State = [[-0.17300153  0.02553445  0.21989262  1.        ]]. Action = [[0.7806027  0.49222052 0.17547822 0.8270459 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 542. State = [[-0.17487095  0.02160356  0.2240341   1.        ]]. Action = [[-0.23989975 -0.23957014  0.16660953  0.79244745]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 543. State = [[-0.18462966  0.02656925  0.23375373  1.        ]]. Action = [[-0.7392908   0.6647637   0.56883264  0.73189664]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 544. State = [[-0.20922194  0.0299045   0.25651917  1.        ]]. Action = [[-0.42587817 -0.50640273  0.6428381   0.5905801 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 545. State = [[-0.21856739  0.02759744  0.27171257  1.        ]]. Action = [[ 0.58154404  0.4711492  -0.02798963  0.7882221 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 546. State = [[-0.20947772  0.04614247  0.28626403  1.        ]]. Action = [[0.40609908 0.7680571  0.9141424  0.68553305]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 547. State = [[-0.18908009  0.0525875   0.31150848  1.        ]]. Action = [[ 0.92953455 -0.6257176   0.29315066  0.38311505]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 548. State = [[-0.17380519  0.05116171  0.3220576   1.        ]]. Action = [[-0.04333544  0.36175442 -0.17574799  0.90930057]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 549. State = [[-0.17479019  0.04275477  0.3276064   1.        ]]. Action = [[-0.7358958  -0.815402    0.4516201   0.84388995]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 550. State = [[-0.17215009  0.029217    0.3491249   1.        ]]. Action = [[ 0.70663035 -0.02796334  0.9872649   0.6703024 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 551. State = [[-0.25468993 -0.09414418  0.10591999  1.        ]]. Action = [[ 0.78627944  0.80512595  0.911957   -0.15306616]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 552. State = [[-0.24662624 -0.09760936  0.09756512  1.        ]]. Action = [[0.746966   0.57426524 0.7443347  0.274024  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 553. State = [[-0.23522484 -0.09178357  0.10379228  1.        ]]. Action = [[-0.76308167 -0.16429037  0.9480741   0.6883416 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 554. State = [[-0.23501165 -0.08476273  0.11444044  1.        ]]. Action = [[-0.4423387  0.3199221  0.8797326  0.5149522]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 555. State = [[-0.23520562 -0.07939477  0.13362871  1.        ]]. Action = [[-0.90923107 -0.8483398   0.5475044   0.9277978 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 556. State = [[-0.23606725 -0.08791686  0.14207363  1.        ]]. Action = [[-0.2041133  -0.65189475  0.4004959   0.88475895]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 557. State = [[-0.23997404 -0.1088635   0.162022    1.        ]]. Action = [[-0.17121077 -0.6823002   0.88046193  0.57639146]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 558. State = [[-0.24911022 -0.11828294  0.1826188   1.        ]]. Action = [[-0.2475977   0.31054926 -0.12046796  0.66954327]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 559. State = [[-0.2518107  -0.11437838  0.18167771  1.        ]]. Action = [[ 0.33202946  0.17858565 -0.28620952  0.5883963 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 560. State = [[-0.24127258 -0.12496034  0.19084778  1.        ]]. Action = [[ 0.6726233  -0.94140995  0.9554069   0.935557  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 561. State = [[-0.2271443  -0.15044023  0.21358268  1.        ]]. Action = [[ 0.08524632 -0.7691915   0.55582297  0.6202147 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 562. State = [[-0.23059966 -0.17882012  0.23246418  1.        ]]. Action = [[-0.5707143  -0.57939     0.2918985   0.45745242]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 563. State = [[-0.23137192 -0.18036132  0.25162727  1.        ]]. Action = [[0.5487895 0.8672044 0.6539471 0.5723845]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 564. State = [[-0.21553363 -0.16472305  0.27171573  1.        ]]. Action = [[0.6009307  0.30841196 0.24309993 0.86529624]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 565. State = [[-0.19683795 -0.14956957  0.28686062  1.        ]]. Action = [[0.4102819  0.4688363  0.36207223 0.7509028 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 566. State = [[-0.18752387 -0.14818487  0.2947867   1.        ]]. Action = [[ 0.05154741 -0.63905126 -0.3149891   0.30594504]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 567. State = [[-0.17479715 -0.15199961  0.29976195  1.        ]]. Action = [[0.8681364  0.17509091 0.40533674 0.769799  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 568. State = [[-0.162708   -0.1610881   0.31862742  1.        ]]. Action = [[-0.6810051  -0.51657474  0.9567425   0.4186294 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 569. State = [[-0.15979709 -0.16038007  0.3483678   1.        ]]. Action = [[0.5768328  0.60198855 0.9190309  0.7491796 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 570. State = [[-0.148613   -0.16026452  0.37355325  1.        ]]. Action = [[-0.06503099 -0.48459637  0.02326632  0.84046924]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 571. State = [[-0.14728403 -0.17676121  0.3908568   1.        ]]. Action = [[ 0.11904025 -0.6685804   0.8101535   0.15057981]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 572. State = [[-0.14550559 -0.18957004  0.4107225   1.        ]]. Action = [[ 0.7815466  -0.08966434  0.7977048   0.7338636 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 573. State = [[-0.14625879 -0.1905401   0.41150498  1.        ]]. Action = [[ 0.8702358  -0.29865527  0.71559167  0.8489826 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 574. State = [[-0.14631806 -0.19065493  0.41152883  1.        ]]. Action = [[ 0.9767405  -0.62081367  0.9274523   0.6444855 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 575. State = [[-0.14631806 -0.19065493  0.41152883  1.        ]]. Action = [[ 0.8329916  -0.6007185   0.2536173   0.44001794]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 576. State = [[-0.14631806 -0.19065493  0.41152883  1.        ]]. Action = [[ 0.546422   -0.41588116  0.8046577   0.868592  ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 577. State = [[-0.14297561 -0.17997608  0.4047911   1.        ]]. Action = [[ 0.20565867  0.8090105  -0.75222945  0.68918264]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 578. State = [[-0.14040475 -0.16985783  0.3968561   1.        ]]. Action = [[ 0.84649277 -0.25934112  0.53474975  0.2716148 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 579. State = [[-0.1405453  -0.16940957  0.3979555   1.        ]]. Action = [[-0.23057353 -0.10722673  0.3171581   0.7066128 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 580. State = [[-0.14176966 -0.16315237  0.39904645  1.        ]]. Action = [[-0.34029078  0.4607916  -0.15529704  0.9597597 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 581. State = [[-0.14426404 -0.15448168  0.4001145   1.        ]]. Action = [[0.6378739  0.3190441  0.751631   0.23600423]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 582. State = [[-0.14936697 -0.14627472  0.39663246  1.        ]]. Action = [[-0.40129793  0.39939427 -0.21392584  0.17149985]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 583. State = [[-0.14829366 -0.13230474  0.39596948  1.        ]]. Action = [[0.95297587 0.33834028 0.12958705 0.68669105]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 584. State = [[-0.14650154 -0.11359721  0.38884068  1.        ]]. Action = [[-0.5578009   0.71759295 -0.5443475   0.3677578 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 585. State = [[-0.14875528 -0.09890814  0.3862389   1.        ]]. Action = [[ 0.95417976 -0.22508276  0.7507539   0.845093  ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 586. State = [[-0.14943999 -0.10634379  0.38671508  1.        ]]. Action = [[ 0.08410764 -0.69066346  0.0610162   0.88379085]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 587. State = [[-0.15155327 -0.11406785  0.38760468  1.        ]]. Action = [[-0.05962646  0.12977839  0.6306026   0.608212  ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 588. State = [[-0.15185076 -0.11491759  0.38779664  1.        ]]. Action = [[ 0.3369223   0.43506038  0.7032447  -0.16857237]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 589. State = [[-0.15199943 -0.11534037  0.38789245  1.        ]]. Action = [[-0.0692941   0.13412702  0.6593399   0.7389709 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 590. State = [[-0.14715703 -0.11044347  0.38175055  1.        ]]. Action = [[ 0.73087144  0.3942412  -0.3045603   0.7393395 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 591. State = [[-0.13269109 -0.09690583  0.36923215  1.        ]]. Action = [[0.20629239 0.5958711  0.10781562 0.7554672 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 592. State = [[-0.12943783 -0.07228078  0.36907336  1.        ]]. Action = [[-0.50830257  0.84854245 -0.02902079  0.4293884 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 593. State = [[-0.1327104  -0.05309403  0.36920387  1.        ]]. Action = [[0.37048316 0.33345294 0.9671165  0.7965052 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 594. State = [[-0.13304499 -0.05565451  0.36835286  1.        ]]. Action = [[ 0.00599802 -0.39690483 -0.28768831  0.8142338 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 595. State = [[-0.12401081 -0.04852641  0.3744741   1.        ]]. Action = [[0.99755883 0.78981936 0.74804986 0.54228854]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 596. State = [[-0.10805576 -0.03791462  0.3799426   1.        ]]. Action = [[0.7367213  0.06053174 0.82761633 0.84180284]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 597. State = [[-0.10557741 -0.03648379  0.3807292   1.        ]]. Action = [[ 0.9791924  -0.09020501  0.9592829   0.7296426 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 598. State = [[-0.10557912 -0.03623183  0.3806992   1.        ]]. Action = [[-0.8622526  -0.22099155  0.83763134  0.8409238 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 599. State = [[-0.11352947 -0.04688685  0.37362957  1.        ]]. Action = [[-0.8830463  -0.80664104 -0.8554116   0.5653721 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 600. State = [[-0.12947841 -0.04962083  0.36043     1.        ]]. Action = [[-0.540877    0.57860684 -0.24929684  0.46921086]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 601. State = [[-0.13987021 -0.05217903  0.35501233  1.        ]]. Action = [[-0.03839117 -0.6087191   0.2769885   0.07891655]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 602. State = [[-0.1450699  -0.07363733  0.363218    1.        ]]. Action = [[-0.10471833 -0.70525134  0.4427786   0.67890906]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 603. State = [[-0.1547447  -0.09398138  0.38257203  1.        ]]. Action = [[-0.5718989 -0.2611128  0.9545145  0.779143 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 604. State = [[-0.1681129  -0.1019522   0.40444383  1.        ]]. Action = [[-0.27778232 -0.59894395  0.8472929   0.4136312 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 605. State = [[-0.16932873 -0.10266832  0.40541777  1.        ]]. Action = [[-0.0553242   0.02898073 -0.07066703  0.5613631 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 606. State = [[-0.1667801  -0.11196215  0.4012911   1.        ]]. Action = [[ 0.6726513  -0.74125165 -0.44086254  0.6418085 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 607. State = [[-0.16785182 -0.12352629  0.38979933  1.        ]]. Action = [[-0.5501856  -0.05211604 -0.53044844  0.77102685]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 608. State = [[-0.1724267  -0.11638682  0.3844018   1.        ]]. Action = [[0.07886994 0.71338594 0.40747035 0.80540395]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 609. State = [[-0.17283222 -0.11676231  0.38841876  1.        ]]. Action = [[ 0.11959255 -0.6314122   0.3266058   0.78956854]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 610. State = [[-0.1697533  -0.13142624  0.39732066  1.        ]]. Action = [[ 0.37638605 -0.48003602  0.3118422   0.7656355 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 611. State = [[-0.16236544 -0.15385221  0.39682657  1.        ]]. Action = [[ 0.52430654 -0.9405976  -0.77582973  0.7645582 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 612. State = [[-0.14700453 -0.18265139  0.37732625  1.        ]]. Action = [[ 0.18652344 -0.37877417 -0.3603722   0.6853626 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 613. State = [[-0.14018254 -0.19285768  0.36946422  1.        ]]. Action = [[0.02674878 0.35774457 0.9476316  0.6532197 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 614. State = [[-0.13317196 -0.19393058  0.37674466  1.        ]]. Action = [[0.294636   0.08300531 0.71166265 0.52235246]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 615. State = [[-0.12585354 -0.1941217   0.3844847   1.        ]]. Action = [[0.03680789 0.5659257  0.94778    0.87696326]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 616. State = [[-0.11853196 -0.18807164  0.3772103   1.        ]]. Action = [[ 0.78711665  0.30356884 -0.69113517  0.50491905]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 617. State = [[-0.09325183 -0.17339936  0.37585995  1.        ]]. Action = [[0.4204955 0.6708915 0.6047493 0.7695875]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 618. State = [[-0.07626829 -0.16438791  0.39395854  1.        ]]. Action = [[ 0.4843011  -0.22905505  0.6431551   0.3057574 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 619. State = [[-0.05699107 -0.16422679  0.40650135  1.        ]]. Action = [[ 0.85234654 -0.09338874 -0.0343551   0.5402081 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 620. State = [[-0.03517988 -0.16581476  0.40813956  1.        ]]. Action = [[0.84873486 0.8131149  0.28911412 0.6532991 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 620 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 620 is tensor(8.7078e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of 0
Current timestep = 621. State = [[-0.03244469 -0.16558598  0.40906554  1.        ]]. Action = [[0.6538615  0.00585079 0.94047296 0.89610815]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Scene graph at timestep 621 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 621 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.03244469 -0.16558598  0.40906554  1.        ]]. Action = [[-0.34611142 -0.37215698  0.6259279   0.7718694 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 622 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 622 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.03244469 -0.16558598  0.40906554  1.        ]]. Action = [[-0.2774582  -0.07530469  0.7022301   0.87215555]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 623 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 623 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.03244469 -0.16558598  0.40906554  1.        ]]. Action = [[ 0.15640032 -0.3706628   0.22214723  0.93010247]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 624 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 624 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of -1
Current timestep = 625. State = [[-0.02982491 -0.15468532  0.41055676  1.        ]]. Action = [[-0.03407532  0.813696    0.02677631  0.6700878 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 625 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 625 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of -1
Current timestep = 626. State = [[-0.02750194 -0.14297092  0.4132378   1.        ]]. Action = [[-0.7519458   0.1418277   0.767086    0.37731814]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Scene graph at timestep 626 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 626 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.02750194 -0.14297092  0.4132378   1.        ]]. Action = [[ 0.85229206 -0.54423034  0.3854022   0.80388594]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 627 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 627 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.02596685 -0.13407168  0.40695304  1.        ]]. Action = [[ 0.03776848  0.4847207  -0.5648739   0.67408013]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 628 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 628 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.02648781 -0.13382849  0.3875577   1.        ]]. Action = [[-0.17582244 -0.69247    -0.9047123   0.92921734]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 629 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 629 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of 0
Current timestep = 630. State = [[-0.01849912 -0.13341963  0.3731109   1.        ]]. Action = [[0.34533882 0.67462444 0.6411655  0.59810674]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 630 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 630 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.01395551 -0.1240069   0.3776348   1.        ]]. Action = [[ 0.35810506 -0.09002125 -0.03387934  0.6945573 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 631 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 631 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of -1
Current timestep = 632. State = [[-0.00947698 -0.1335486   0.38542035  1.        ]]. Action = [[-0.16156888 -0.6541422   0.45282507  0.45903897]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 632 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 632 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of 0
Current timestep = 633. State = [[-0.01006094 -0.1451917   0.39271402  1.        ]]. Action = [[-0.69312996 -0.90522575  0.9312751   0.8313408 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Scene graph at timestep 633 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 633 is tensor(6.9060e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.01417519 -0.1550599   0.39882132  1.        ]]. Action = [[-0.43907905 -0.53814733  0.30638528  0.5835953 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 634 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.02029247 -0.16708173  0.4067777   1.        ]]. Action = [[0.32560837 0.758553   0.71643925 0.80622375]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 635 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.01740565 -0.16230378  0.3980191   1.        ]]. Action = [[ 0.7068703   0.22351313 -0.82687914  0.56031585]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 636 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 636 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.00446055 -0.15680645  0.38157946  1.        ]]. Action = [[0.0375638  0.80387306 0.8425267  0.44772553]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Scene graph at timestep 637 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 637 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[ 0.00220787 -0.14752592  0.38009107  1.        ]]. Action = [[ 0.81197405  0.53476    -0.32410967  0.56702733]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 638 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 638 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 638 of 1
Current timestep = 639. State = [[ 0.03610079 -0.1312195   0.3725907   1.        ]]. Action = [[0.7252227  0.42771363 0.58324623 0.31716824]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 639 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 639 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of 0
Current timestep = 640. State = [[ 0.05330504 -0.11275825  0.3800529   1.        ]]. Action = [[-0.47398096  0.5603266  -0.49499273  0.6552682 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 640 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 640 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of 1
Current timestep = 641. State = [[ 0.04710355 -0.10743806  0.36605752  1.        ]]. Action = [[-0.41511452 -0.08180684 -0.9541598   0.37395537]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 641 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 641 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of 1
Current timestep = 642. State = [[ 0.03986453 -0.11563616  0.3432153   1.        ]]. Action = [[-0.43487692 -0.4444543  -0.01817799  0.6116135 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 642 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 642 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of 0
Current timestep = 643. State = [[ 0.03559821 -0.1337004   0.34829926  1.        ]]. Action = [[ 0.33076668 -0.6712465   0.8134265   0.6212373 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 643 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 643 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of -1
Current timestep = 644. State = [[ 0.03761999 -0.14842601  0.37065527  1.        ]]. Action = [[ 0.30777407 -0.22185111  0.806231    0.6957191 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 644 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 644 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[ 0.03598866 -0.15544556  0.38357586  1.        ]]. Action = [[-0.22737354 -0.00162661  0.91297364  0.7235998 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Scene graph at timestep 645 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 645 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[ 0.03186347 -0.16645157  0.38280952  1.        ]]. Action = [[-0.4413638  -0.8307282  -0.37770903  0.14860046]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 646 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 646 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[ 0.03220339 -0.17730078  0.38733268  1.        ]]. Action = [[0.17142141 0.5576669  0.5792053  0.69762397]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 647 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 647 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[ 0.03871542 -0.16002029  0.38250017  1.        ]]. Action = [[ 0.8112881   0.61623955 -0.7861387   0.5697689 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 648 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 648 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of 1
Current timestep = 649. State = [[ 0.0559006  -0.13739839  0.37986207  1.        ]]. Action = [[-0.13849962  0.59972584  0.6144185   0.51825047]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 649 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 649 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of 1
Current timestep = 650. State = [[ 0.05798475 -0.11339986  0.3826168   1.        ]]. Action = [[-0.35170877  0.8126099  -0.7283177   0.8974917 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 651. State = [[ 0.04656211 -0.10679433  0.36954328  1.        ]]. Action = [[-0.57914627 -0.8449533  -0.61793894  0.83428955]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 651 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 651 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[ 0.0303557  -0.10876046  0.34092185  1.        ]]. Action = [[-0.39662814  0.85738707 -0.96585274  0.4186306 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 652 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 652 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 652 of 1
Current timestep = 653. State = [[-0.26922515  0.17800203  0.10658257  1.        ]]. Action = [[ 0.37066293  0.62835574 -0.04412967  0.60689044]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 654. State = [[-0.26695877  0.19842982  0.0931644   1.        ]]. Action = [[-0.10788071  0.76897836  0.9170282   0.583071  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 655. State = [[-0.2646675   0.20919566  0.10106094  1.        ]]. Action = [[0.1470033 0.6445453 0.9408636 0.374552 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 656. State = [[-0.25392616  0.2332665   0.11945409  1.        ]]. Action = [[0.58167124 0.6764064  0.4377594  0.57974625]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 657. State = [[-0.2305488   0.2588605   0.13914579  1.        ]]. Action = [[0.836781   0.766186   0.45279467 0.11242521]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 658. State = [[-0.21412018  0.28525433  0.16352019  1.        ]]. Action = [[-0.27625465  0.42300105  0.7344074   0.8475697 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 659. State = [[-0.2621043  -0.0145883   0.10389284  1.        ]]. Action = [[-0.18328208  0.04500031  0.98794913 -0.14323157]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 660. State = [[-0.2528073  -0.01844172  0.09268899  1.        ]]. Action = [[0.7863256  0.11395836 0.35037458 0.5907011 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 661. State = [[-0.22713985 -0.01988725  0.09874808  1.        ]]. Action = [[ 0.8853197  -0.14895469  0.39767838  0.19775844]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 662. State = [[-0.19821589 -0.0144728   0.10791362  1.        ]]. Action = [[ 0.6202707   0.5629878  -0.01922798  0.7878295 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 663. State = [[-0.18779854  0.00329418  0.11992399  1.        ]]. Action = [[-0.5262561   0.7395624   0.7681955   0.48938274]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 664. State = [[-0.19242892  0.02000685  0.13039869  1.        ]]. Action = [[0.81310225 0.54164445 0.81892574 0.44655883]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 665. State = [[-0.18934117  0.01038876  0.14253794  1.        ]]. Action = [[ 0.11308146 -0.80198884  0.9235244   0.66831744]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 666. State = [[-1.8769340e-01 -4.4668235e-05  1.6445768e-01  1.0000000e+00]]. Action = [[ 0.6412386  -0.3976876   0.84167206  0.8588903 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 667. State = [[-0.18806437 -0.00453378  0.17720324  1.        ]]. Action = [[-0.2807529  -0.07639617  0.83624077  0.33131635]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 668. State = [[-0.18857847 -0.01796026  0.20430726  1.        ]]. Action = [[ 0.07243931 -0.7069071   0.49876833  0.36840713]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 669. State = [[-0.18716797 -0.0340968   0.23116572  1.        ]]. Action = [[ 0.17860258 -0.20500857  0.9570463   0.7000681 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 670. State = [[-0.17326254 -0.03918823  0.25713864  1.        ]]. Action = [[ 0.86666274 -0.05086303  0.08972967  0.6679983 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 671. State = [[-0.16039251 -0.03161667  0.26042652  1.        ]]. Action = [[ 0.03158927  0.6666796  -0.57793295  0.17902231]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 672. State = [[-0.15462288 -0.02338058  0.2523627   1.        ]]. Action = [[ 0.12315595 -0.87123215 -0.7978748   0.40154195]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 673. State = [[-0.14426258 -0.01577466  0.24807297  1.        ]]. Action = [[ 0.76684     0.35847664 -0.25523973  0.67238414]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 674. State = [[-0.1272951  -0.00936824  0.24147813  1.        ]]. Action = [[ 0.84834075 -0.6787181  -0.4220109   0.8799448 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 675. State = [[-0.13035618  0.00365544  0.24562214  1.        ]]. Action = [[-0.46298933  0.7001786   0.48120344  0.7776017 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 676. State = [[-0.13452044  0.01750871  0.24887598  1.        ]]. Action = [[-0.23305708  0.9599166  -0.5649647   0.6723373 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 677. State = [[-0.13500327  0.01896311  0.24912494  1.        ]]. Action = [[ 0.8065026  -0.12549359 -0.5049209   0.88611174]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 678. State = [[-0.13503668  0.01914668  0.24911842  1.        ]]. Action = [[-0.67866886  0.04380059 -0.38058335  0.5074843 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 679. State = [[-0.13590418  0.01469487  0.25268313  1.        ]]. Action = [[-0.28422272 -0.34818393  0.17320573  0.87979436]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 680. State = [[-0.13037466 -0.00252917  0.2690421   1.        ]]. Action = [[ 0.67897284 -0.78540003  0.73867583  0.41248477]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 681. State = [[-0.11564637 -0.01606251  0.28342474  1.        ]]. Action = [[ 0.50961566 -0.04092216 -0.27673066  0.71702266]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 682. State = [[-0.10974737 -0.02481042  0.29727623  1.        ]]. Action = [[-0.28618634 -0.3113098   0.975541    0.63153195]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 683. State = [[-0.10836583 -0.01814289  0.30934834  1.        ]]. Action = [[ 0.12890005  0.909515   -0.39468253  0.6985259 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 684. State = [[-0.10303761 -0.00858495  0.30408195  1.        ]]. Action = [[ 0.63306284 -0.1952762  -0.38142908  0.8148874 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 685. State = [[-0.09086304 -0.02191441  0.29439345  1.        ]]. Action = [[-0.05710423 -0.89611155 -0.15730625  0.5940474 ]]. Reward = [0.]
Curr episode timestep = 25
Above hoop
Current timestep = 686. State = [[-0.08549882 -0.03561796  0.28446084  1.        ]]. Action = [[ 0.6863464  0.049842  -0.7472066  0.7730814]]. Reward = [0.]
Curr episode timestep = 26
Above hoop
Current timestep = 687. State = [[-0.07233996 -0.03279075  0.26443166  1.        ]]. Action = [[-0.5242835   0.39916766 -0.196984    0.40073502]]. Reward = [0.]
Curr episode timestep = 27
Above hoop
Current timestep = 688. State = [[-0.06507048 -0.02864185  0.26950645  1.        ]]. Action = [[0.7731502  0.00165761 0.70375454 0.85148287]]. Reward = [0.]
Curr episode timestep = 28
Above hoop
Current timestep = 689. State = [[-0.04506385 -0.01507714  0.28783402  1.        ]]. Action = [[0.7882751 0.7551279 0.7701849 0.6963489]]. Reward = [0.]
Curr episode timestep = 29
Above hoop
Scene graph at timestep 689 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 689 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 689 of 1
Current timestep = 690. State = [[-0.02406025  0.01635724  0.305285    1.        ]]. Action = [[ 0.6561067   0.97950387 -0.542648    0.6355628 ]]. Reward = [0.]
Curr episode timestep = 30
Above hoop
Scene graph at timestep 690 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 690 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of -1
Current timestep = 691. State = [[0.01261481 0.02777183 0.3012971  1.        ]]. Action = [[ 0.5451634  -0.7383778   0.94808507  0.60047936]]. Reward = [0.]
Curr episode timestep = 31
Above hoop
Scene graph at timestep 691 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 691 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of -1
Current timestep = 692. State = [[0.02213634 0.01595473 0.3261421  1.        ]]. Action = [[-0.8474037  -0.01144499  0.07240403  0.81657267]]. Reward = [0.]
Curr episode timestep = 32
Above hoop
Scene graph at timestep 692 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 692 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 692 of -1
Current timestep = 693. State = [[0.0081638  0.02835675 0.32245597 1.        ]]. Action = [[-0.8130572   0.93248034 -0.46677434  0.34060407]]. Reward = [0.]
Curr episode timestep = 33
Above hoop
Scene graph at timestep 693 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 693 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 693 of -1
Current timestep = 694. State = [[-0.01093315  0.06615329  0.3079947   1.        ]]. Action = [[ 0.41464877  0.8722491  -0.8457287   0.07749546]]. Reward = [0.]
Curr episode timestep = 34
Above hoop
Scene graph at timestep 694 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 694 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 694 of -1
Current timestep = 695. State = [[-0.01717041  0.09779321  0.27670872  1.        ]]. Action = [[-0.60964024  0.85069585 -0.6767353   0.352265  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 695 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 695 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of -1
Current timestep = 696. State = [[-0.02446824  0.11959437  0.26135412  1.        ]]. Action = [[ 0.87203836  0.47124755 -0.5826819   0.8434608 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Scene graph at timestep 696 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 696 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 696 of -1
Current timestep = 697. State = [[-0.01711416  0.11551148  0.26556534  1.        ]]. Action = [[ 0.94062066 -0.2248851   0.37341058  0.2576108 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 697 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 697 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 697 of -1
Current timestep = 698. State = [[-0.00619122  0.1035791   0.2747935   1.        ]]. Action = [[ 0.39349008 -0.4564491   0.3804512   0.34146655]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 698 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 698 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[0.00250673 0.10278463 0.28983876 1.        ]]. Action = [[-0.13111341  0.50958157  0.6784241   0.66931486]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 699 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 699 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 699 of -1
Current timestep = 700. State = [[0.0124843  0.12133727 0.30274883 1.        ]]. Action = [[ 0.7976353   0.7647952  -0.07304448  0.58610535]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 700 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 700 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of -1
Current timestep = 701. State = [[0.03187317 0.15397952 0.29477403 1.        ]]. Action = [[ 0.11982322  0.9964489  -0.77716994  0.7655003 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 701 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 701 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 701 of -1
Current timestep = 702. State = [[0.05001372 0.17690952 0.28186095 1.        ]]. Action = [[ 0.9639423 -0.0354293  0.554605   0.7496772]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 702 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 702 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 702 of -1
Current timestep = 703. State = [[0.07552177 0.18156065 0.29332608 1.        ]]. Action = [[ 0.81201124 -0.86891836  0.9520911   0.6753439 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 703 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 703 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[0.07389994 0.1845359  0.2926077  1.        ]]. Action = [[-0.46245766  0.1208415  -0.3028283   0.2680577 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 704 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 704 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of -1
Current timestep = 705. State = [[0.0715476  0.19777586 0.2986032  1.        ]]. Action = [[0.13887262 0.67322874 0.9435588  0.89854264]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 705 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 705 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[0.07282089 0.21108353 0.3191567  1.        ]]. Action = [[ 0.8855288   0.47386658 -0.81356806  0.8154776 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 706 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 706 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[0.07270981 0.21106094 0.31899327 1.        ]]. Action = [[ 0.7602943   0.140028   -0.7426431   0.49382925]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 707 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 707 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 707 of -1
Current timestep = 708. State = [[0.07550906 0.1990409  0.32477295 1.        ]]. Action = [[-0.18540263 -0.9066621   0.30637944  0.67391825]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 708 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 708 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of 1
Current timestep = 709. State = [[0.07690046 0.1844807  0.3363384  1.        ]]. Action = [[ 0.6025543   0.8230058  -0.28139257  0.7043679 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 709 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 709 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[0.07581592 0.1778767  0.330057   1.        ]]. Action = [[-0.3514632  -0.5270367  -0.67375135  0.01135576]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 710 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 710 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of 1
Current timestep = 711. State = [[0.06878302 0.15609212 0.32297742 1.        ]]. Action = [[-0.93979996 -0.8732848  -0.12653577  0.70726013]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 711 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 711 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 711 of 1
Current timestep = 712. State = [[0.05021663 0.13387462 0.31630278 1.        ]]. Action = [[ 0.96419525 -0.4586736   0.1743685   0.42650902]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 712 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 712 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[0.04214605 0.14108583 0.30760586 1.        ]]. Action = [[-0.8642824   0.3189267  -0.75125027  0.32735598]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 713 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 713 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of 0
Current timestep = 714. State = [[0.01410584 0.15483949 0.27379403 1.        ]]. Action = [[ 0.8053968   0.789206   -0.2967404   0.94545996]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 714 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 714 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of -1
Current timestep = 715. State = [[0.018864   0.15567835 0.26103562 1.        ]]. Action = [[ 0.31833065 -0.6887383  -0.43564272  0.76101255]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 715 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 715 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of 0
Current timestep = 716. State = [[0.02858821 0.1455718  0.2566846  1.        ]]. Action = [[0.28240395 0.02264893 0.7758142  0.81273603]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 716 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 716 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[0.04287544 0.13210575 0.28298217 1.        ]]. Action = [[ 0.7219603 -0.6479075  0.8274777  0.6539531]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 717 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 717 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[0.05267376 0.11866031 0.29228896 1.        ]]. Action = [[ 0.22431219  0.0874542  -0.9159193   0.7241583 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 719. State = [[0.05774995 0.12455781 0.2765201  1.        ]]. Action = [[-0.37349933  0.2483939   0.19072628  0.5624051 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 720. State = [[0.05232485 0.1303297  0.2760431  1.        ]]. Action = [[-0.95199716  0.02351081 -0.1380952   0.32853603]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 720 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 720 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of 0
Current timestep = 721. State = [[0.04418908 0.13467404 0.26540077 1.        ]]. Action = [[ 0.94941044  0.17690837 -0.7230606   0.64211226]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 721 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 721 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 721 of -1
Current timestep = 722. State = [[0.04616996 0.1518902  0.24131116 1.        ]]. Action = [[-0.27336425  0.96391964 -0.6016812   0.0989536 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 722 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 722 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of -1
Current timestep = 723. State = [[0.04593209 0.17142983 0.22350714 1.        ]]. Action = [[ 0.70095944 -0.7929644  -0.33026886  0.7618482 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Scene graph at timestep 723 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 723 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of -1
Current timestep = 724. State = [[0.04591145 0.17142114 0.22335584 1.        ]]. Action = [[ 0.89257085 -0.6761837   0.03698397  0.88382256]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Scene graph at timestep 724 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 724 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of -1
Current timestep = 725. State = [[0.05571816 0.17111932 0.23311248 1.        ]]. Action = [[ 0.9492347  -0.06140178  0.62598825  0.73120296]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 725 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 725 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[0.06710786 0.17538467 0.25189376 1.        ]]. Action = [[-0.99280834  0.01593137  0.5782106   0.78020597]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 726 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 726 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 726 of 1
Current timestep = 727. State = [[0.0600562  0.17391361 0.2595931  1.        ]]. Action = [[-0.6054109  -0.33936632 -0.30493474  0.44069123]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 727 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 727 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[0.05117379 0.16136101 0.26973516 1.        ]]. Action = [[ 0.17653692 -0.36003995  0.7511933   0.5773407 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 728 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 728 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[0.05614148 0.13935311 0.29246488 1.        ]]. Action = [[ 0.65479803 -0.8979231   0.50018907  0.8195714 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 729 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 729 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[0.05616488 0.13116126 0.29821065 1.        ]]. Action = [[-0.509304    0.82636046 -0.53683734  0.7015033 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 730 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 730 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of -1
Current timestep = 731. State = [[0.04912682 0.14461492 0.2948562  1.        ]]. Action = [[ 0.97964287  0.78798366 -0.9597235   0.78807235]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 731 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 731 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of -1
Current timestep = 732. State = [[0.04608781 0.15280643 0.2968381  1.        ]]. Action = [[-0.23953414  0.4257691   0.3554641   0.7687509 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 732 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 732 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of -1
Current timestep = 733. State = [[0.03707736 0.15195517 0.30537942 1.        ]]. Action = [[-0.7886693  -0.7631144   0.29629707  0.67899966]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 733 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 733 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[0.01418911 0.12640351 0.3162341  1.        ]]. Action = [[-0.3730775  -0.6547854  -0.11167872  0.5738238 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 734 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 734 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[0.00643578 0.09859002 0.3237177  1.        ]]. Action = [[-0.08652294 -0.8373109   0.646968    0.7287705 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 736. State = [[-0.00314204  0.06542581  0.35293278  1.        ]]. Action = [[-0.22090805 -0.83560497  0.9338186   0.67766976]]. Reward = [0.]
Curr episode timestep = 76
Above hoop
Current timestep = 737. State = [[-0.01620344  0.04467155  0.36098766  1.        ]]. Action = [[ 0.07834136 -0.05667609 -0.7765023   0.64769936]]. Reward = [0.]
Curr episode timestep = 77
Above hoop
Current timestep = 738. State = [[-0.02551383  0.03222783  0.35137627  1.        ]]. Action = [[-0.5075274  -0.48736203 -0.10506976  0.63105   ]]. Reward = [0.]
Curr episode timestep = 78
Above hoop
Current timestep = 739. State = [[-0.03169975  0.0112778   0.33846295  1.        ]]. Action = [[ 0.44194627 -0.6042861  -0.5164048   0.6193886 ]]. Reward = [0.]
Curr episode timestep = 79
Above hoop
Current timestep = 740. State = [[-0.03917911 -0.01569645  0.33298704  1.        ]]. Action = [[-0.9063444  -0.87163305 -0.01133633  0.8399904 ]]. Reward = [0.]
Curr episode timestep = 80
Above hoop
Current timestep = 741. State = [[-0.04770427 -0.04094954  0.33621913  1.        ]]. Action = [[ 0.71560454 -0.36885607  0.82883763  0.6252607 ]]. Reward = [0.]
Curr episode timestep = 81
Above hoop
Current timestep = 742. State = [[-0.03799753 -0.05394895  0.33719742  1.        ]]. Action = [[ 0.93198395 -0.10529202 -0.8873613   0.8055115 ]]. Reward = [0.]
Curr episode timestep = 82
Above hoop
Current timestep = 743. State = [[-0.01430577 -0.0530537   0.32410866  1.        ]]. Action = [[0.40215313 0.4616325  0.18359661 0.57982326]]. Reward = [0.]
Curr episode timestep = 83
Above hoop
Current timestep = 744. State = [[ 0.0012018  -0.04009471  0.32236663  1.        ]]. Action = [[0.29971004 0.37385166 0.00131357 0.5110918 ]]. Reward = [0.]
Curr episode timestep = 84
Above hoop
Current timestep = 745. State = [[ 0.00709337 -0.04253248  0.31416506  1.        ]]. Action = [[ 0.12517965 -0.62956583 -0.8804168   0.654515  ]]. Reward = [0.]
Curr episode timestep = 85
Above hoop
Current timestep = 746. State = [[ 0.02102084 -0.05914927  0.30430475  1.        ]]. Action = [[ 0.37191272 -0.47931802  0.7891078   0.5385766 ]]. Reward = [0.]
Curr episode timestep = 86
Above hoop
Current timestep = 747. State = [[ 0.02536696 -0.0727516   0.32381466  1.        ]]. Action = [[-0.47185302 -0.18528295  0.89207506  0.3278339 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 748. State = [[ 0.02934115 -0.08351006  0.34415177  1.        ]]. Action = [[ 0.88741446 -0.38542145  0.24732792  0.6426412 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 749. State = [[ 0.04260773 -0.08166978  0.3606702   1.        ]]. Action = [[-0.27314317  0.671726    0.49892473  0.4408723 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 750. State = [[ 0.04396617 -0.06034539  0.3623615   1.        ]]. Action = [[-0.1791637  0.8089895 -0.8612726  0.662078 ]]. Reward = [0.]
Curr episode timestep = 90
Above hoop
Current timestep = 751. State = [[ 0.04433918 -0.05280485  0.34361148  1.        ]]. Action = [[ 0.6051669  -0.61023724 -0.99754494  0.57339287]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 752. State = [[ 0.05129531 -0.05636558  0.32253358  1.        ]]. Action = [[-0.26078713  0.24091494  0.56272006  0.3585738 ]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 753. State = [[ 0.05449466 -0.04573644  0.31966132  1.        ]]. Action = [[ 0.30153096  0.60036016 -0.5065959   0.5322716 ]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Current timestep = 754. State = [[ 0.05977044 -0.02539603  0.30166     1.        ]]. Action = [[-0.04153109  0.6076714  -0.8875196   0.84341073]]. Reward = [0.]
Curr episode timestep = 94
Above hoop
Current timestep = 755. State = [[ 0.05933437 -0.00525638  0.28180593  1.        ]]. Action = [[-0.4639454   0.356364   -0.06953645  0.39563823]]. Reward = [0.]
Curr episode timestep = 95
Above hoop
Current timestep = 756. State = [[0.0608662  0.0068624  0.28136477 1.        ]]. Action = [[0.74124086 0.04728115 0.21056652 0.74312973]]. Reward = [0.]
Curr episode timestep = 96
Above hoop
Current timestep = 757. State = [[0.06301922 0.00835121 0.28277302 1.        ]]. Action = [[-0.20410752 -0.01584941  0.1665101   0.72010255]]. Reward = [0.]
Curr episode timestep = 97
Above hoop
Current timestep = 758. State = [[0.06276491 0.02042263 0.28272992 1.        ]]. Action = [[-0.16225696  0.7792696  -0.27478647  0.7722496 ]]. Reward = [0.]
Curr episode timestep = 98
Above hoop
Current timestep = 759. State = [[0.05417958 0.02713598 0.27336407 1.        ]]. Action = [[-0.6825564  -0.39808023 -0.78425384  0.47433937]]. Reward = [0.]
Curr episode timestep = 99
Above hoop
Current timestep = 760. State = [[0.03854571 0.0114474  0.25133702 1.        ]]. Action = [[-0.8706087  -0.7448972  -0.39767617  0.74506164]]. Reward = [0.]
Curr episode timestep = 100
Above hoop
Current timestep = 761. State = [[-0.26042596 -0.08507813  0.08971402  1.        ]]. Action = [[-0.84436923  0.94349456  0.42046857  0.55873644]]. Reward = [0.]
Curr episode timestep = 101
Above hoop
Current timestep = 762. State = [[-0.25189045 -0.10595531  0.0813763   1.        ]]. Action = [[ 0.71314645 -0.7291968   0.7643275   0.924263  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 763. State = [[-0.23028329 -0.12395415  0.09699749  1.        ]]. Action = [[ 0.86597705 -0.19488609  0.64875996  0.83837366]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 764. State = [[-0.20246638 -0.1422193   0.11564412  1.        ]]. Action = [[ 0.42667603 -0.68061864  0.16871989  0.88112473]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 765. State = [[-0.19199604 -0.16359007  0.12486388  1.        ]]. Action = [[-0.28129017 -0.46163082  0.39790773  0.18364608]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 766. State = [[-0.19102842 -0.18826517  0.12715773  1.        ]]. Action = [[ 0.24653184 -0.82330424 -0.8005923   0.79823685]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 767. State = [[-0.18570593 -0.22264336  0.12479849  1.        ]]. Action = [[ 0.13330829 -0.9211701   0.33572257  0.8195926 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 768. State = [[-0.1793097  -0.23143575  0.12180586  1.        ]]. Action = [[ 0.45277357  0.6790129  -0.43726808  0.7477572 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 769. State = [[-0.16400175 -0.21507782  0.11183368  1.        ]]. Action = [[ 0.55160093  0.52965343 -0.38765347  0.61133575]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 770. State = [[-0.15667449 -0.22194786  0.10930753  1.        ]]. Action = [[-0.7485038  -0.8937875   0.84743357  0.8789952 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 771. State = [[-0.1547767  -0.2417487   0.13301285  1.        ]]. Action = [[ 0.60179305 -0.42584097  0.98173225  0.7897428 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 772. State = [[-0.14335498 -0.2605967   0.15461093  1.        ]]. Action = [[ 0.36098385 -0.7574594  -0.3128792   0.6539149 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 773. State = [[-0.14230004 -0.28507727  0.16450748  1.        ]]. Action = [[-0.5353601 -0.3541218  0.8313918  0.7917414]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 774. State = [[-0.14800991 -0.29542485  0.17893213  1.        ]]. Action = [[ 0.38052177 -0.8418272   0.5359726   0.6315608 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 775. State = [[-0.14789242 -0.29629228  0.18048173  1.        ]]. Action = [[ 0.19890702 -0.00720656 -0.05285841  0.51120174]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 776. State = [[-0.14666907 -0.29560682  0.19248067  1.        ]]. Action = [[-0.1776185   0.19209981  0.92125916  0.5196228 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 777. State = [[-0.15028916 -0.28602216  0.21761394  1.        ]]. Action = [[-0.55391806  0.6872083   0.439919    0.34442008]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 778. State = [[-0.15407297 -0.2784717   0.23221977  1.        ]]. Action = [[ 0.8089309  -0.9747358   0.84431386  0.607393  ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 779. State = [[-0.15493792 -0.27564278  0.23300256  1.        ]]. Action = [[-0.16353464  0.15756083  0.05157518  0.816648  ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 780. State = [[-0.16246004 -0.26835862  0.23638196  1.        ]]. Action = [[-0.14276046  0.22869515 -0.00434428  0.8293805 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 781. State = [[-0.15831983 -0.26546836  0.23987682  1.        ]]. Action = [[ 0.8968513  -0.28952587  0.31189215  0.80938005]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 782. State = [[-0.14377971 -0.27756616  0.26083907  1.        ]]. Action = [[ 0.31597018 -0.76238394  0.9862628   0.28217816]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 783. State = [[-0.1337169  -0.28913802  0.28163743  1.        ]]. Action = [[ 0.03774095 -0.91092443  0.07326031  0.6630497 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 784. State = [[-0.12231702 -0.28774935  0.2884297   1.        ]]. Action = [[0.89193463 0.23220825 0.11109877 0.8719163 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 785. State = [[-0.10637781 -0.2864326   0.29651338  1.        ]]. Action = [[-0.16488057 -0.46015263  0.7066853   0.81926394]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 786. State = [[-0.09990726 -0.28053486  0.3052446   1.        ]]. Action = [[0.11476862 0.3689325  0.7207749  0.7054585 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 787. State = [[-0.08963362 -0.26765877  0.32040307  1.        ]]. Action = [[0.08597791 0.51765203 0.12770486 0.32716763]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 788. State = [[-0.07427237 -0.24268302  0.3313931   1.        ]]. Action = [[0.9416008  0.7995379  0.41282284 0.7918546 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 789. State = [[-0.06067046 -0.23175542  0.356707    1.        ]]. Action = [[-0.1677792  -0.39784253  0.7590854   0.5858698 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 790. State = [[-0.04757975 -0.23596002  0.37147143  1.        ]]. Action = [[ 0.83278465 -0.06161666 -0.09166205  0.75732017]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 791. State = [[-0.03273521 -0.22206187  0.36441284  1.        ]]. Action = [[ 0.24475718  0.9562435  -0.84369963  0.51018906]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 792. State = [[-0.02889499 -0.21517237  0.3509226   1.        ]]. Action = [[-0.87306213 -0.43531948 -0.13292205  0.3482914 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 793. State = [[-0.03451623 -0.20965719  0.34155124  1.        ]]. Action = [[-0.08548349  0.6427542  -0.6209426   0.27578676]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 794. State = [[-0.03607529 -0.19731967  0.32514703  1.        ]]. Action = [[ 0.31963027  0.09664631 -0.25711954  0.63849664]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 795. State = [[-0.0323914  -0.18241024  0.3209997   1.        ]]. Action = [[-0.08113706  0.78685474  0.37425387  0.599509  ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 796. State = [[-0.02858181 -0.15434447  0.31538907  1.        ]]. Action = [[ 0.61603177  0.5309541  -0.5044242   0.34057856]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 797. State = [[-0.02602178 -0.14007437  0.30754572  1.        ]]. Action = [[-0.35104454  0.09881926 -0.02156508  0.6618092 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 798. State = [[-0.02153883 -0.12862559  0.30783725  1.        ]]. Action = [[0.68234336 0.4201498  0.14420068 0.1764102 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 799. State = [[-0.00744413 -0.12129505  0.3166591   1.        ]]. Action = [[ 0.5458255  -0.12134582  0.5699241   0.8832307 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 800. State = [[ 9.0611540e-04 -1.0531831e-01  3.1867218e-01  1.0000000e+00]]. Action = [[-0.1404028  0.9702972 -0.6185813  0.6639745]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 801. State = [[ 0.0038182  -0.07237993  0.3119303   1.        ]]. Action = [[-0.01861936  0.9470923   0.41596377  0.40848708]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 802. State = [[ 0.01285258 -0.04754345  0.32337224  1.        ]]. Action = [[0.9611927  0.14215016 0.41631973 0.77098906]]. Reward = [0.]
Curr episode timestep = 40
Above hoop
Current timestep = 803. State = [[ 0.03426534 -0.0556119   0.32747418  1.        ]]. Action = [[ 0.416857   -0.90728027 -0.42080843  0.70104265]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Current timestep = 804. State = [[ 0.04617027 -0.0788358   0.30781326  1.        ]]. Action = [[-0.35376036 -0.6453768  -0.6542793   0.6277776 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 805. State = [[ 0.04867353 -0.08536486  0.29223487  1.        ]]. Action = [[ 0.25971866  0.45234656 -0.3057955   0.40722466]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 806. State = [[ 0.05473648 -0.08044571  0.27775428  1.        ]]. Action = [[ 0.5844867   0.04801798 -0.71227705  0.79828405]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 807. State = [[ 0.07333563 -0.07918127  0.25730065  1.        ]]. Action = [[ 0.9839206   0.28360426 -0.9708189   0.1179955 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 808. State = [[ 0.0855945  -0.07681036  0.2623694   1.        ]]. Action = [[0.49185133 0.15753162 0.59568226 0.39012623]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 809. State = [[ 0.09302239 -0.07544459  0.2736249   1.        ]]. Action = [[ 0.2683103  -0.08960235 -0.64020026  0.21837234]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 810. State = [[ 0.0947147  -0.07815309  0.26921663  1.        ]]. Action = [[-0.27554667 -0.30379367 -0.44400585  0.5688515 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 811. State = [[ 0.09406876 -0.08048533  0.2635852   1.        ]]. Action = [[0.19079709 0.7859688  0.39678812 0.6607537 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 812. State = [[ 0.09374909 -0.08123091  0.26120186  1.        ]]. Action = [[ 0.26910985 -0.1433729   0.5763197   0.47552562]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 813. State = [[ 0.09162989 -0.09474141  0.2740532   1.        ]]. Action = [[-0.77794045 -0.63832223  0.8909912   0.6855104 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 814. State = [[ 0.081641  -0.098604   0.2808964  1.       ]]. Action = [[-0.5735437   0.5413902  -0.73795086  0.50375104]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 815. State = [[ 0.06963035 -0.10696182  0.28491455  1.        ]]. Action = [[-0.39540088 -0.8645286   0.65643954  0.57901263]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 816. State = [[ 0.04946129 -0.13591482  0.2871667   1.        ]]. Action = [[-0.61494637 -0.9434212  -0.5733105   0.7116997 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 817. State = [[ 0.03212264 -0.16881968  0.2781378   1.        ]]. Action = [[ 0.02987075 -0.89889264  0.02733159  0.6364939 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 818. State = [[ 0.02733279 -0.19936429  0.2817867   1.        ]]. Action = [[ 0.22724843 -0.77621174 -0.01979792  0.7452996 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 819. State = [[ 0.02730769 -0.21280968  0.28137577  1.        ]]. Action = [[ 0.09626603  0.50340533 -0.12197995  0.5729847 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 820. State = [[ 0.02100523 -0.20956372  0.26890683  1.        ]]. Action = [[-0.6075209   0.12103236 -0.8034963   0.84502864]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 821. State = [[ 0.00809766 -0.2002854   0.24675088  1.        ]]. Action = [[-0.36422646  0.4775424  -0.10716146  0.68881226]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 822. State = [[ 0.00670333 -0.19299874  0.24907736  1.        ]]. Action = [[ 0.6010957  -0.30922568  0.8740947   0.6427281 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 823. State = [[ 0.00861728 -0.2071492   0.26067427  1.        ]]. Action = [[ 0.18991756 -0.903426    0.03223407  0.47970176]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 824. State = [[ 0.01476133 -0.21696828  0.25962523  1.        ]]. Action = [[ 0.7103416   0.36413586 -0.28208935  0.8413397 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 825. State = [[ 0.02581886 -0.21453846  0.25269946  1.        ]]. Action = [[ 0.15677953 -0.01215345 -0.23762405  0.7689636 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 826. State = [[ 0.02453973 -0.22332653  0.23951915  1.        ]]. Action = [[-0.93054944 -0.5862068  -0.6934367   0.6119113 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 827. State = [[ 0.02499929 -0.22739474  0.22563781  1.        ]]. Action = [[ 0.88549733  0.30898106 -0.73312044  0.42318797]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 828. State = [[ 0.0300121  -0.22626486  0.20741504  1.        ]]. Action = [[-0.4581982   0.11097813  0.717433    0.8111594 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 829. State = [[ 0.03809846 -0.21915     0.22232312  1.        ]]. Action = [[0.58286405 0.38554752 0.8039379  0.8549719 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 830. State = [[ 0.05583345 -0.20759156  0.24319832  1.        ]]. Action = [[0.929      0.19525766 0.36940324 0.6257601 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 830 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 830 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of 0
Current timestep = 831. State = [[ 0.06978872 -0.19658703  0.25046846  1.        ]]. Action = [[-0.6942005   0.3114946  -0.8075055   0.55426407]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 831 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 831 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[ 0.06713407 -0.19411364  0.24025452  1.        ]]. Action = [[ 0.64022624 -0.38278806  0.7942889   0.7265307 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 832 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 832 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of -1
Current timestep = 833. State = [[ 0.06711032 -0.19412965  0.24023183  1.        ]]. Action = [[0.9242604  0.69608986 0.69443357 0.65506446]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 833 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 833 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 833 of -1
Current timestep = 834. State = [[ 0.06745592 -0.18978015  0.24013783  1.        ]]. Action = [[0.28596675 0.20331633 0.15384984 0.7973889 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 834 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 834 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 834 of -1
Current timestep = 835. State = [[ 0.06754944 -0.18417451  0.23429427  1.        ]]. Action = [[ 0.21359205 -0.0957697  -0.65576893  0.57560825]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 835 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 835 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of -1
Current timestep = 836. State = [[ 0.07038754 -0.1832749   0.21610212  1.        ]]. Action = [[ 0.14200544  0.52581    -0.09074056  0.5536642 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Scene graph at timestep 836 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 836 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of -1
Current timestep = 837. State = [[ 0.07041239 -0.18328702  0.21610233  1.        ]]. Action = [[ 0.63027644 -0.45503592  0.0283941   0.83451056]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 837 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 837 is tensor(8.2727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[ 0.07384366 -0.19356069  0.22210996  1.        ]]. Action = [[ 0.37809372 -0.7069098   0.49784112  0.55094385]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 838 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 838 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 0
Current timestep = 839. State = [[ 0.07988333 -0.20037521  0.23927864  1.        ]]. Action = [[-0.69694525  0.5332589   0.7176137   0.78202605]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 839 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 839 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 0
Current timestep = 840. State = [[ 0.07789921 -0.19574177  0.25309262  1.        ]]. Action = [[0.595726   0.75616074 0.1308825  0.80193734]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Scene graph at timestep 840 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 840 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of -1
Current timestep = 841. State = [[ 0.07789921 -0.19574177  0.25309262  1.        ]]. Action = [[ 0.8186126  -0.14072454 -0.9526542   0.18252397]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 841 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 841 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of -1
Current timestep = 842. State = [[ 0.0726502  -0.20214032  0.26122957  1.        ]]. Action = [[-0.6574036 -0.2862122  0.3716843  0.5475153]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 842 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 842 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 0
Current timestep = 843. State = [[ 0.05378602 -0.19656208  0.28346226  1.        ]]. Action = [[-0.6746797   0.84142995  0.666993    0.30120742]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 844. State = [[ 0.02535159 -0.19110668  0.31428677  1.        ]]. Action = [[-0.803022   -0.6531172   0.83742964  0.32781327]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 845. State = [[ 0.00827161 -0.18572693  0.33399755  1.        ]]. Action = [[ 0.42403722  0.640561   -0.01677799  0.67312753]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 846. State = [[ 0.00931019 -0.17199637  0.3310839   1.        ]]. Action = [[ 0.26536775  0.1893872  -0.38201666  0.5676787 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 847. State = [[ 0.01283109 -0.17460316  0.32410523  1.        ]]. Action = [[ 0.9793508  -0.77437794 -0.6376342   0.25298953]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 848. State = [[ 0.02868541 -0.1908891   0.30948278  1.        ]]. Action = [[-0.26916653 -0.25483686  0.64592147  0.56607914]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 849. State = [[ 0.03908269 -0.19117853  0.31387994  1.        ]]. Action = [[0.92702293 0.26697564 0.01123023 0.65974665]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 850. State = [[ 0.06045652 -0.1760625   0.3162359   1.        ]]. Action = [[0.39243507 0.78738666 0.08977008 0.37883067]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 851. State = [[ 0.07026932 -0.16601048  0.33235028  1.        ]]. Action = [[-0.43991518 -0.05567533  0.8629546   0.70226085]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 852. State = [[ 0.07312078 -0.15171191  0.34700045  1.        ]]. Action = [[ 0.08659446  0.8069887  -0.05620182  0.27236485]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 853. State = [[ 0.07177593 -0.14042243  0.3576235   1.        ]]. Action = [[-0.10647368 -0.25778556  0.600497    0.20605826]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 854. State = [[ 0.0709189  -0.14381824  0.3611411   1.        ]]. Action = [[ 0.34801888 -0.23751807 -0.9770291   0.05648947]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 855. State = [[ 0.0727106  -0.15773925  0.33558396  1.        ]]. Action = [[ 0.29246712 -0.8511012  -0.75955135  0.4316697 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 856. State = [[ 0.08312425 -0.17559128  0.31512478  1.        ]]. Action = [[ 0.5862806   0.7008867  -0.31874204  0.4985391 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 857. State = [[ 0.08517394 -0.18473738  0.31184164  1.        ]]. Action = [[ 0.18684185 -0.44416642 -0.03779358  0.5983584 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 858. State = [[ 0.09335133 -0.1835427   0.31441793  1.        ]]. Action = [[-0.23320192  0.80408084  0.3218249   0.4542551 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 859. State = [[ 0.09377777 -0.17452884  0.31530276  1.        ]]. Action = [[ 0.23201203  0.4736277  -0.5008132   0.8867154 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 860. State = [[ 0.08552349 -0.18928638  0.31566957  1.        ]]. Action = [[-0.94417477 -0.9790652  -0.2428096   0.72598505]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 861. State = [[ 0.07227543 -0.20508194  0.3056314   1.        ]]. Action = [[-0.16257787 -0.07357031 -0.79430205  0.8002651 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 862. State = [[ 0.06335632 -0.205718    0.29542446  1.        ]]. Action = [[-0.34184837  0.37285638  0.26071966  0.5158255 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 863. State = [[-0.25998482 -0.16375923  0.1065009   1.        ]]. Action = [[-0.67665666 -0.3834926   0.19673622  0.76165617]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 864. State = [[-0.25469133 -0.1721402   0.09879207  1.        ]]. Action = [[0.5134921  0.655323   0.7142036  0.61669874]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 865. State = [[-0.24182087 -0.15346974  0.11223479  1.        ]]. Action = [[0.49885595 0.7351593  0.64755714 0.6430483 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 866. State = [[-0.22103833 -0.14938626  0.12708484  1.        ]]. Action = [[ 0.81477535 -0.8359951  -0.17510676  0.64332914]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 867. State = [[-0.19599235 -0.15248984  0.13988632  1.        ]]. Action = [[0.42551863 0.4571638  0.8517122  0.5806415 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 868. State = [[-0.18209365 -0.1388783   0.16592288  1.        ]]. Action = [[-0.15828842  0.6868516   0.96869254  0.79882514]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 869. State = [[-0.18129906 -0.11268775  0.1828058   1.        ]]. Action = [[ 0.03649974  0.7952244  -0.7525228   0.31574368]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 870. State = [[-0.18138121 -0.09672144  0.17760609  1.        ]]. Action = [[ 0.39567614 -0.17301852 -0.07974082  0.54380536]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 871. State = [[-0.18432833 -0.09924872  0.17919058  1.        ]]. Action = [[-0.6258487  -0.41722488  0.4352212   0.74550056]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 872. State = [[-0.19093175 -0.10554466  0.1826845   1.        ]]. Action = [[0.5256779  0.23175144 0.91208816 0.5716045 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 873. State = [[-0.19358811 -0.09744862  0.19191007  1.        ]]. Action = [[-0.20831329  0.5680587   0.67071795  0.6646005 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 874. State = [[-0.19722752 -0.08818591  0.2068868   1.        ]]. Action = [[ 0.6477363  -0.22008592 -0.21639204  0.5074215 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 875. State = [[-0.19747381 -0.08748407  0.20921203  1.        ]]. Action = [[0.8301773  0.56453633 0.16870582 0.39721322]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 876. State = [[-0.19959708 -0.07783284  0.20584059  1.        ]]. Action = [[ 0.00521553  0.5340378  -0.5433203   0.646744  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 877. State = [[-0.20223604 -0.06858166  0.202765    1.        ]]. Action = [[ 0.6880021  -0.6282513   0.51758194  0.6089511 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 878. State = [[-0.1990686  -0.06796357  0.20570225  1.        ]]. Action = [[ 0.31445968 -0.04152709  0.62615275  0.4557631 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 879. State = [[-0.19008069 -0.05989631  0.21321945  1.        ]]. Action = [[0.6036483 0.4572258 0.0951153 0.5762501]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 880. State = [[-0.17405505 -0.05738102  0.22664706  1.        ]]. Action = [[ 0.31284404 -0.44410717  0.61905575  0.8288107 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 881. State = [[-0.16042098 -0.0623874   0.24619885  1.        ]]. Action = [[0.11215675 0.00777757 0.6921971  0.74046516]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 882. State = [[-0.15731479 -0.06173576  0.26047063  1.        ]]. Action = [[-0.34497917  0.13943481 -0.5154727   0.65936756]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 883. State = [[-0.1651937  -0.06094068  0.26867494  1.        ]]. Action = [[-0.71151185 -0.01704246  0.80966234  0.7635572 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 884. State = [[-0.1716361  -0.04718221  0.29026467  1.        ]]. Action = [[0.71539974 0.77943313 0.61892223 0.47900772]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 885. State = [[-0.17346779 -0.04647477  0.30998495  1.        ]]. Action = [[-0.79878026 -0.90249044  0.4872402   0.67696965]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 886. State = [[-0.18448342 -0.05708683  0.33560416  1.        ]]. Action = [[0.16800737 0.1713078  0.7704439  0.7476586 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 887. State = [[-0.1824135  -0.06190606  0.36024526  1.        ]]. Action = [[ 0.32999682 -0.3773402   0.5498829   0.3964703 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 888. State = [[-0.1670444  -0.06031417  0.38330585  1.        ]]. Action = [[0.82274544 0.53996277 0.42518246 0.74829984]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 889. State = [[-0.14939933 -0.05705989  0.40682703  1.        ]]. Action = [[ 0.27577126 -0.30779386  0.42170846  0.6660203 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 890. State = [[-0.14067231 -0.05862609  0.4181      1.        ]]. Action = [[-0.03484631  0.3991995   0.8210547   0.78146386]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 891. State = [[-0.13793744 -0.05910608  0.42046466  1.        ]]. Action = [[ 0.70647895 -0.33650565  0.5957134   0.61586356]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 892. State = [[-0.13333207 -0.07151442  0.41285747  1.        ]]. Action = [[ 0.9200969  -0.796481   -0.8860636   0.61352015]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 893. State = [[-0.10735152 -0.08579066  0.39341077  1.        ]]. Action = [[0.22908628 0.52780807 0.55134475 0.77829504]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 894. State = [[-0.10481823 -0.09212963  0.38321903  1.        ]]. Action = [[-0.34901005 -0.15707445 -0.65565914  0.76213574]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 895. State = [[-0.10148823 -0.09159476  0.36439037  1.        ]]. Action = [[ 0.61110234  0.2564416  -0.9440779   0.6914035 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 896. State = [[-0.08139469 -0.09185007  0.34169385  1.        ]]. Action = [[ 0.856868   -0.23065811  0.47935688  0.42731965]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 897. State = [[-0.0587317  -0.08067421  0.338186    1.        ]]. Action = [[ 0.5197636  0.9070078 -0.3342021  0.7717804]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 898. State = [[-0.03938362 -0.06669375  0.32695335  1.        ]]. Action = [[ 0.4219873   0.06810772 -0.34897476  0.7234585 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 898 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 898 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.02949101 -0.07099322  0.31322658  1.        ]]. Action = [[-0.82677317 -0.653174   -0.7093174   0.4918735 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 899 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 899 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of 0
Current timestep = 900. State = [[-0.04459346 -0.0817222   0.29539686  1.        ]]. Action = [[-0.25189447 -0.03771514 -0.46360815  0.6048207 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 901. State = [[-0.04689984 -0.09172229  0.2789166   1.        ]]. Action = [[ 0.7633219  -0.5347666  -0.86166364  0.7500043 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 902. State = [[-0.03899861 -0.0933237   0.24472094  1.        ]]. Action = [[ 0.00675225  0.5943332  -0.17292207  0.6486237 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 903. State = [[-0.03722589 -0.08861896  0.23753884  1.        ]]. Action = [[-0.812157   -0.8760852  -0.30503905 -0.04905027]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 904. State = [[-0.02570613 -0.09943684  0.2429239   1.        ]]. Action = [[ 0.85748446 -0.9040094   0.3171165   0.16522121]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 905. State = [[ 0.00147128 -0.10178414  0.2544621   1.        ]]. Action = [[0.9087775  0.7030437  0.6863985  0.58097076]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 906. State = [[ 0.02268848 -0.09597857  0.26604772  1.        ]]. Action = [[ 0.42037642  0.7112299  -0.81338567  0.6044105 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 907. State = [[ 0.03125164 -0.08147006  0.27159372  1.        ]]. Action = [[-0.05067956  0.85109425  0.3478017   0.59884703]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 908. State = [[ 0.02981104 -0.08010518  0.27919722  1.        ]]. Action = [[-0.6554799  -0.9348804  -0.08698207  0.89773846]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 909. State = [[ 0.01524871 -0.1029531   0.27163628  1.        ]]. Action = [[-0.9618085  -0.596577   -0.7557913   0.42915142]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 910. State = [[ 0.00423622 -0.12200548  0.26704872  1.        ]]. Action = [[ 0.7080538  -0.44921547  0.7172737   0.46165478]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 911. State = [[ 0.00347479 -0.13903773  0.2682315   1.        ]]. Action = [[-0.22626019 -0.4746399  -0.6284844   0.69569755]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 912. State = [[ 0.00371433 -0.16314837  0.268034    1.        ]]. Action = [[ 0.41089976 -0.8744092   0.21303868  0.5849273 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 913. State = [[ 0.00456037 -0.171808    0.26072177  1.        ]]. Action = [[-0.12991524  0.6331198  -0.43887508  0.7345989 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 914. State = [[ 0.00948326 -0.15603736  0.25356603  1.        ]]. Action = [[ 0.6106851   0.59599924 -0.14663255  0.8352828 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 915. State = [[ 0.02234493 -0.1513849   0.25368828  1.        ]]. Action = [[-0.01523119 -0.32178628  0.60828257  0.5909469 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 916. State = [[ 0.02468437 -0.15914741  0.26514813  1.        ]]. Action = [[-0.28007525 -0.31671572  0.5094273   0.79522395]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 917. State = [[ 0.02370235 -0.16389048  0.27311897  1.        ]]. Action = [[-0.09720606  0.24531913 -0.7637692   0.7125399 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 918. State = [[ 0.02420443 -0.15843041  0.27465543  1.        ]]. Action = [[ 0.3229996   0.4643767  -0.11996782  0.7277527 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 919. State = [[ 0.02816832 -0.16231993  0.2710896   1.        ]]. Action = [[ 0.87018    -0.82727474 -0.64590186  0.08262634]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 920. State = [[ 0.05332899 -0.16639532  0.24906723  1.        ]]. Action = [[ 0.47492552  0.50951517 -0.33241028  0.6387304 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 920 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 920 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 920 of -1
Current timestep = 921. State = [[ 0.08302096 -0.16726518  0.24999179  1.        ]]. Action = [[ 0.34542072 -0.4222859   0.8843646   0.49200547]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 922. State = [[ 0.09036396 -0.17049195  0.2676441   1.        ]]. Action = [[ 0.37311792 -0.3438568  -0.48457116  0.68279624]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 923. State = [[ 0.09323531 -0.17098495  0.27173838  1.        ]]. Action = [[ 0.5314778  -0.2815734   0.82641625  0.7821691 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 924. State = [[ 0.09219337 -0.1630213   0.26557884  1.        ]]. Action = [[-0.35071468  0.6542165  -0.6096506   0.4699955 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 925. State = [[ 0.09061608 -0.14656493  0.25507784  1.        ]]. Action = [[-0.44923306  0.6880591  -0.09915727  0.43756068]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 926. State = [[ 0.08957638 -0.13076067  0.2539478   1.        ]]. Action = [[-0.51824296  0.6972883  -0.6655664   0.80686045]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 927. State = [[ 0.0893848  -0.12890011  0.25372753  1.        ]]. Action = [[ 0.74595356 -0.7778263  -0.65391946  0.4450214 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 928. State = [[ 0.08936268 -0.1288394   0.2537037   1.        ]]. Action = [[0.5617031  0.87135625 0.56168604 0.78286624]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 929. State = [[ 0.08936268 -0.1288394   0.2537037   1.        ]]. Action = [[ 0.00651586 -0.09191895 -0.9132908   0.7307813 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 930. State = [[ 0.08936268 -0.1288394   0.2537037   1.        ]]. Action = [[ 0.75008535 -0.8136254  -0.7381765   0.55426085]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 931. State = [[ 0.08859839 -0.13880228  0.25545618  1.        ]]. Action = [[ 0.05805397 -0.65690297  0.235587    0.8222445 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 932. State = [[ 0.08053508 -0.15379474  0.2547955   1.        ]]. Action = [[-0.8489554  -0.3219782  -0.21478218  0.57391477]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 933. State = [[ 0.06750363 -0.16269273  0.2503511   1.        ]]. Action = [[ 0.46411085  0.41323876 -0.87458503  0.6055689 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 934. State = [[ 0.05835762 -0.1715106   0.2623641   1.        ]]. Action = [[-0.9644753  -0.24535489  0.67292213  0.5678978 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 935. State = [[ 0.03202229 -0.17603141  0.27635655  1.        ]]. Action = [[ 0.849661    0.11305642 -0.03382015  0.4865837 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 936. State = [[-0.26986578  0.14257413  0.11971924  1.        ]]. Action = [[-0.9049893  -0.98366964  0.6127186  -0.00192446]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 937. State = [[-0.27029368  0.14468968  0.11967169  1.        ]]. Action = [[-0.63443077 -0.4158988   0.9677055   0.78971577]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 938. State = [[-0.26717535  0.14294857  0.12396681  1.        ]]. Action = [[ 0.09674191 -0.22233588  0.64246845  0.87706697]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 939. State = [[-0.25134334  0.14323947  0.14187403  1.        ]]. Action = [[0.78929746 0.2067467  0.9853554  0.6739954 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 940. State = [[-0.24086048  0.15309273  0.17379127  1.        ]]. Action = [[-0.3468169   0.35296988  0.81485176  0.5076759 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 941. State = [[-0.23784871  0.16464992  0.1973486   1.        ]]. Action = [[0.4968419  0.33559084 0.1907934  0.63796663]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 942. State = [[-0.23283103  0.18365961  0.21344428  1.        ]]. Action = [[-0.36431855  0.74023557  0.5863097   0.87069046]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 943. State = [[-0.23238482  0.19577792  0.23128459  1.        ]]. Action = [[ 0.14603913 -0.381276    0.34723282  0.40481412]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 944. State = [[-0.2320528   0.18687649  0.24622557  1.        ]]. Action = [[-0.5633205  -0.4629407   0.35780883  0.83941615]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 945. State = [[-0.2322001   0.16595991  0.2488652   1.        ]]. Action = [[ 0.48006368 -0.83300614 -0.9383786   0.7589948 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 946. State = [[-0.2210235   0.13988729  0.23428181  1.        ]]. Action = [[ 0.8254657  -0.5027726  -0.3519174   0.76054835]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 947. State = [[-0.2048664   0.14053585  0.23134698  1.        ]]. Action = [[0.28726482 0.8706831  0.7113521  0.7645695 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 948. State = [[-0.2021637   0.15984082  0.24586119  1.        ]]. Action = [[-0.523953    0.5258111   0.5529411   0.50052726]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 949. State = [[-0.20433812  0.17829315  0.26779187  1.        ]]. Action = [[0.3280778 0.3622824 0.8406906 0.4846357]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 950. State = [[-0.19334617  0.182648    0.2988805   1.        ]]. Action = [[ 0.15072203 -0.42010224  0.7289157   0.5831915 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 951. State = [[-0.17880626  0.17602643  0.3215441   1.        ]]. Action = [[ 0.87066674 -0.03350991  0.05071425  0.50896835]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 952. State = [[-0.16382398  0.1731749   0.33961913  1.        ]]. Action = [[-0.13751334 -0.15117037  0.8206024   0.8433864 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 953. State = [[-0.16120633  0.16344008  0.34956074  1.        ]]. Action = [[-0.39673018 -0.5914761  -0.63285536  0.45823038]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 954. State = [[-0.1616468   0.16193156  0.3373166   1.        ]]. Action = [[ 0.9042256   0.7650037  -0.95421666  0.90234566]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 955. State = [[-0.15151398  0.16224015  0.3062692   1.        ]]. Action = [[-0.6218945  -0.7829574  -0.48577023  0.8765179 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 956. State = [[-0.15017068  0.15626137  0.28810963  1.        ]]. Action = [[ 0.8895142  0.3564738 -0.8780988  0.836988 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 957. State = [[-0.12962031  0.14876504  0.2569656   1.        ]]. Action = [[ 0.8995142  -0.62234676 -0.15154362  0.4386549 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 958. State = [[-0.10849153  0.12927718  0.2593958   1.        ]]. Action = [[-0.48133987 -0.61586833  0.9023566   0.746089  ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 959. State = [[-0.10352875  0.102003    0.27414426  1.        ]]. Action = [[ 0.08059883 -0.9425535   0.01536119  0.591897  ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 960. State = [[-0.10048116  0.08439207  0.28087687  1.        ]]. Action = [[0.21633446 0.03567028 0.13592851 0.6894548 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 961. State = [[-0.10146102  0.09227536  0.27642986  1.        ]]. Action = [[ 0.13702226  0.9000013  -0.5755738   0.3770814 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 962. State = [[-0.10652564  0.11863269  0.27304238  1.        ]]. Action = [[-0.6088494  0.9182267  0.6867442  0.8552842]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 963. State = [[-0.12163809  0.14259903  0.2746624   1.        ]]. Action = [[-0.59867525  0.018929   -0.6745855   0.59018946]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 964. State = [[-0.1380299   0.15939133  0.27510354  1.        ]]. Action = [[-0.75517154  0.6630013   0.55156493  0.2731142 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 965. State = [[-0.15126516  0.17840044  0.29561195  1.        ]]. Action = [[0.39051414 0.2810818  0.9767046  0.6371906 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 966. State = [[-0.15117642  0.17010356  0.30246127  1.        ]]. Action = [[ 0.31199026 -0.80227345 -0.8599252   0.7861464 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 967. State = [[-0.14059858  0.14477019  0.2918781   1.        ]]. Action = [[ 0.71741605 -0.83999753 -0.39535344  0.4883095 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 968. State = [[-0.13439338  0.13176483  0.27775025  1.        ]]. Action = [[-0.8361696   0.32568026 -0.44944656  0.6795908 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 969. State = [[-0.14939396  0.14300461  0.2729356   1.        ]]. Action = [[-0.9719593   0.57112813  0.37936652  0.84812546]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 970. State = [[-0.16855739  0.15694912  0.27326402  1.        ]]. Action = [[ 0.85469234 -0.84826833 -0.8415263   0.82309747]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 971. State = [[-0.16814123  0.14588395  0.28522855  1.        ]]. Action = [[ 0.04985464 -0.84995216  0.8108525   0.8446436 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 972. State = [[-0.16604327  0.11953599  0.29283673  1.        ]]. Action = [[ 0.69990754 -0.6129741  -0.8832423   0.6208656 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 973. State = [[-0.15592511  0.11277409  0.27727088  1.        ]]. Action = [[ 0.42000496  0.5682744  -0.15136147  0.763695  ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 974. State = [[-0.15180577  0.12350988  0.27420416  1.        ]]. Action = [[-0.1483317   0.4329002   0.15390456  0.45477867]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 975. State = [[-0.1445708   0.12391308  0.2701599   1.        ]]. Action = [[ 0.76091266 -0.3501504  -0.26616538  0.76623845]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 976. State = [[-0.13322593  0.11798013  0.27300903  1.        ]]. Action = [[-0.44811195 -0.36662626  0.68783     0.57979167]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 977. State = [[-0.1348818   0.11362927  0.2732901   1.        ]]. Action = [[-0.26666427  0.05781364 -0.6375616   0.6591904 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 978. State = [[-0.1452993   0.10592765  0.26306447  1.        ]]. Action = [[-0.64662695 -0.5226565  -0.3961237   0.5756831 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 979. State = [[-0.15431413  0.10270861  0.25490612  1.        ]]. Action = [[0.38437653 0.45129287 0.30773818 0.60406923]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 980. State = [[-0.15425006  0.1043937   0.25547782  1.        ]]. Action = [[-0.06899625  0.35140324 -0.7019413   0.65875506]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 981. State = [[-0.15438378  0.10467473  0.25547788  1.        ]]. Action = [[ 0.33155322 -0.7071332  -0.7752337   0.60440946]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 982. State = [[-0.15082327  0.09843327  0.2574396   1.        ]]. Action = [[ 0.24353528 -0.5258792   0.05593538  0.4990101 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 983. State = [[-0.1526091   0.07896788  0.26676837  1.        ]]. Action = [[-0.71064293 -0.8326864   0.43998122  0.17286777]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 984. State = [[-0.16618066  0.059098    0.26953834  1.        ]]. Action = [[-0.33222783 -0.17927343 -0.5856121   0.48962927]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 985. State = [[-0.16722631  0.0564059   0.2679532   1.        ]]. Action = [[0.77029276 0.20950651 0.5740917  0.48182786]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 986. State = [[-0.16566958  0.06778895  0.26411995  1.        ]]. Action = [[-0.15683424  0.75692594 -0.7468918   0.39883065]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 987. State = [[-0.1701811  0.066934   0.2620716  1.       ]]. Action = [[-0.6131276  -0.8800351   0.44086862  0.62036705]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 988. State = [[-0.16893649  0.05985573  0.27206033  1.        ]]. Action = [[0.38421357 0.26523066 0.58059585 0.5518744 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 989. State = [[-0.17198245  0.04993112  0.29278824  1.        ]]. Action = [[-0.61757797 -0.691777    0.74083495  0.42263222]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 990. State = [[-0.17717318  0.02235903  0.31835604  1.        ]]. Action = [[ 0.40962505 -0.8922911   0.38560915  0.64904356]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 991. State = [[-0.1830462 -0.0076903  0.3423272  1.       ]]. Action = [[-0.5805792 -0.6943134  0.7207488  0.5255513]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 992. State = [[-0.1921114  -0.01973198  0.35417166  1.        ]]. Action = [[-0.22759855  0.33372104 -0.4262308   0.67536163]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 993. State = [[-0.19165558 -0.02283549  0.34868765  1.        ]]. Action = [[ 0.7783308 -0.3405006 -0.2497428  0.6192002]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 994. State = [[-0.1803748  -0.03559078  0.33644083  1.        ]]. Action = [[ 0.79555225 -0.5373898  -0.855244    0.5822525 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 995. State = [[-0.16047285 -0.0379574   0.3151277   1.        ]]. Action = [[0.01579952 0.76943994 0.00397682 0.71476936]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 996. State = [[-0.14850818 -0.04103666  0.3107006   1.        ]]. Action = [[ 0.871585   -0.94656706  0.0920403   0.7256825 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 997. State = [[-0.11904927 -0.06240921  0.32243735  1.        ]]. Action = [[ 0.8821018  -0.4793139   0.9738213   0.41160738]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 998. State = [[-0.09248166 -0.08296686  0.34244406  1.        ]]. Action = [[ 0.6672995  -0.6062236   0.23984945  0.4028709 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 999. State = [[-0.07775926 -0.09480593  0.3444079   1.        ]]. Action = [[-0.56062585  0.1451695  -0.50382507  0.70563483]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1000. State = [[-0.07097193 -0.10527673  0.3471977   1.        ]]. Action = [[ 0.91653943 -0.6289826   0.6026375   0.3792994 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1001. State = [[-0.04984721 -0.11000669  0.34927213  1.        ]]. Action = [[ 0.7508937   0.2979058  -0.43042642  0.6477511 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1001 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1001 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of 1
Current timestep = 1002. State = [[-0.01892339 -0.10531255  0.3379341   1.        ]]. Action = [[ 0.97666645 -0.02784824 -0.4039997   0.77279747]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1002 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1002 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of 1
Current timestep = 1003. State = [[ 0.02026385 -0.10889211  0.318365    1.        ]]. Action = [[ 0.9425632  -0.17447007 -0.6801974   0.02403617]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1004. State = [[ 0.04033951 -0.11058459  0.3012841   1.        ]]. Action = [[-0.54232395  0.3149227   0.12409079  0.75372505]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1005. State = [[ 0.04433002 -0.12014682  0.31021327  1.        ]]. Action = [[ 0.5068995 -0.8040883  0.7364838  0.6453006]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1006. State = [[ 0.04507411 -0.13516189  0.33625436  1.        ]]. Action = [[-0.7707124  -0.082385    0.81498694  0.6256565 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1007. State = [[ 0.03951978 -0.13283813  0.34273595  1.        ]]. Action = [[ 0.36153412  0.4754119  -0.98078775  0.69168186]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1008. State = [[ 0.04253301 -0.13611063  0.32298234  1.        ]]. Action = [[ 0.16673744 -0.7503057  -0.4322555   0.6014824 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1009. State = [[ 0.04070317 -0.15236638  0.32091117  1.        ]]. Action = [[-0.90200603 -0.2436657   0.5226464   0.55558157]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1010. State = [[ 0.0423857  -0.1528509   0.33509475  1.        ]]. Action = [[0.81705487 0.43504846 0.6902821  0.44815958]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1011. State = [[ 0.04715631 -0.13840245  0.34869027  1.        ]]. Action = [[-0.46801537  0.51364803  0.18222594  0.58061767]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1012. State = [[ 0.0348036  -0.133976    0.37012902  1.        ]]. Action = [[-0.8776259 -0.2789303  0.6897116  0.6689863]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1013. State = [[ 0.01711206 -0.13684611  0.38345033  1.        ]]. Action = [[-0.06350034 -0.06175435 -0.337645    0.38628995]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1014. State = [[ 0.00798125 -0.12804724  0.389957    1.        ]]. Action = [[-0.77389663  0.71800756  0.47696662  0.59536076]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1015. State = [[-0.02385346 -0.12758115  0.4005968   1.        ]]. Action = [[-0.5382524  -0.93609715 -0.02794504  0.332165  ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1016. State = [[-0.03500372 -0.12999424  0.40426454  1.        ]]. Action = [[0.29139042 0.77789843 0.2823423  0.3648386 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1017. State = [[-0.03516518 -0.11889893  0.4065481   1.        ]]. Action = [[-0.27657437 -0.8986189   0.27045655  0.6119218 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 1018. State = [[-0.03785571 -0.1113865   0.39840934  1.        ]]. Action = [[-0.1953026   0.34083414 -0.9880909   0.5056734 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1019. State = [[-0.04319095 -0.10903036  0.38823947  1.        ]]. Action = [[-0.08280087 -0.29026914 -0.05923367  0.74627674]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1020. State = [[-0.05176548 -0.12546979  0.37651965  1.        ]]. Action = [[-0.37099606 -0.8971936  -0.5561789   0.8728993 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1021. State = [[-0.06340425 -0.14044106  0.35853815  1.        ]]. Action = [[ 0.00362742  0.10278738 -0.30107868  0.77465165]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1022. State = [[-0.06453037 -0.13285416  0.3559184   1.        ]]. Action = [[-0.22593123  0.6265563   0.13031697  0.5614594 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1023. State = [[-0.06375162 -0.12655394  0.35659194  1.        ]]. Action = [[ 0.4300444  -0.23835886  0.27333117  0.58606696]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1024. State = [[-0.05626442 -0.13462354  0.36254406  1.        ]]. Action = [[ 0.6209899  -0.4868108   0.3792932   0.80921125]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1025. State = [[-0.04999987 -0.13850717  0.3579801   1.        ]]. Action = [[ 0.23983121  0.22052288 -0.6938746   0.5522026 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1026. State = [[-0.04633645 -0.13023345  0.3357043   1.        ]]. Action = [[-0.56250644  0.50873864 -0.75099087  0.44200873]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1027. State = [[-0.03817179 -0.12897848  0.33268422  1.        ]]. Action = [[ 0.81023204 -0.41662526  0.8749893   0.6223178 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1028. State = [[-0.0358377  -0.1406263   0.34808993  1.        ]]. Action = [[-0.7838611  -0.42941058  0.5524508   0.59345365]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1029. State = [[-0.04457159 -0.13809992  0.3612397   1.        ]]. Action = [[-0.61572856  0.9137764   0.02258539  0.7308521 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1030. State = [[-0.05530695 -0.108248    0.36635238  1.        ]]. Action = [[0.27828765 0.9733348  0.06770742 0.7594795 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1031. State = [[-0.25779983 -0.06581138  0.09832931  1.        ]]. Action = [[ 0.6192086   0.73164725  0.8875439  -0.17435771]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1032. State = [[-0.24843241 -0.08585498  0.08877081  1.        ]]. Action = [[ 0.6845329  -0.78620845  0.61962247  0.7568581 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1033. State = [[-0.23051216 -0.11344918  0.09753677  1.        ]]. Action = [[ 0.45547986 -0.5878254   0.41176796  0.49086666]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1034. State = [[-0.21099724 -0.13899633  0.11391907  1.        ]]. Action = [[ 0.61799574 -0.81809574  0.61683655  0.8100573 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1035. State = [[-0.18473217 -0.15234016  0.12827195  1.        ]]. Action = [[ 0.7977674   0.3195057  -0.30965936  0.55163145]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1036. State = [[-0.16935514 -0.15244697  0.13044906  1.        ]]. Action = [[ 0.6621797  -0.08669102 -0.2397232   0.47425222]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 1037. State = [[-0.15480521 -0.16457194  0.1413719   1.        ]]. Action = [[ 0.76508975 -0.8519269   0.9394945   0.85685575]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1038. State = [[-0.13796891 -0.18003514  0.16774641  1.        ]]. Action = [[-0.5143849   0.00757456  0.85625064  0.46370697]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1039. State = [[-0.1401303  -0.18480116  0.18887414  1.        ]]. Action = [[0.66848373 0.0345149  0.06744313 0.48400223]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 1040. State = [[-0.14809167 -0.18414485  0.20323694  1.        ]]. Action = [[-0.8495058  0.2544086  0.8981042  0.9172057]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1041. State = [[-0.1724684  -0.19750023  0.22902222  1.        ]]. Action = [[-0.7632578  -0.9293189   0.2392062   0.53435063]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1042. State = [[-0.19455743 -0.20312688  0.24881242  1.        ]]. Action = [[-0.35875767  0.68496203  0.881979    0.56164515]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1043. State = [[-0.20557262 -0.18724498  0.27189776  1.        ]]. Action = [[-0.17225593  0.48754954  0.03873265  0.5427325 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1044. State = [[-0.20195152 -0.17464726  0.28167024  1.        ]]. Action = [[0.8646234  0.05175459 0.4492885  0.56548107]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1045. State = [[-0.19020027 -0.1624232   0.2807233   1.        ]]. Action = [[ 0.514148    0.4211923  -0.8889843   0.34989476]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1046. State = [[-0.16982025 -0.14256603  0.2678668   1.        ]]. Action = [[ 0.6819817   0.6243398  -0.16695023  0.5135751 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1047. State = [[-0.15083303 -0.11922158  0.25500512  1.        ]]. Action = [[ 0.30173087  0.7290461  -0.49632806  0.86960375]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1048. State = [[-0.13163893 -0.09812611  0.25262907  1.        ]]. Action = [[0.5865772  0.38085222 0.70645964 0.7110822 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1049. State = [[-0.11006171 -0.08968779  0.27335683  1.        ]]. Action = [[ 0.5327482  -0.16775209  0.92440796  0.6505439 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1050. State = [[-0.09287361 -0.0779504   0.29121915  1.        ]]. Action = [[ 0.10883892  0.7489071  -0.24189997  0.7495867 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1051. State = [[-0.09215084 -0.07705515  0.2827683   1.        ]]. Action = [[ 0.23907971 -0.83721507 -0.74311435  0.59022653]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1052. State = [[-0.07809927 -0.09728444  0.26361585  1.        ]]. Action = [[ 0.70078146 -0.6785038  -0.7853136   0.7139238 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1053. State = [[-0.05316709 -0.11491063  0.25112885  1.        ]]. Action = [[ 0.28892994 -0.16503543  0.53603315  0.65596354]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1054. State = [[-0.04045285 -0.1102078   0.26069456  1.        ]]. Action = [[0.5610814  0.5893947  0.21351326 0.66983926]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1055. State = [[-0.0340437  -0.09979291  0.27534905  1.        ]]. Action = [[-0.74093467  0.3517164   0.87964666  0.747452  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1056. State = [[-0.04195074 -0.10687     0.29597282  1.        ]]. Action = [[-0.2512505  -0.8709903   0.33317292  0.7415788 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1057. State = [[-0.04980339 -0.1147589   0.3096654   1.        ]]. Action = [[-0.54269594  0.36924243  0.22618365  0.7174549 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1058. State = [[-0.05318372 -0.10784744  0.31900492  1.        ]]. Action = [[0.70546913 0.20431018 0.04098272 0.35596633]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1059. State = [[-0.04463483 -0.09938646  0.32240686  1.        ]]. Action = [[0.7004864  0.27656984 0.18096209 0.62086856]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1060. State = [[-0.02603221 -0.08924618  0.33718035  1.        ]]. Action = [[0.32405925 0.28479648 0.5938873  0.6099107 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1061. State = [[-0.01979947 -0.08306275  0.3561824   1.        ]]. Action = [[-0.51422983 -0.01320821  0.3402562   0.36234772]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1062. State = [[-0.02174539 -0.07733724  0.35778922  1.        ]]. Action = [[-0.27485538  0.24877524 -0.5460532   0.5664221 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1063. State = [[-0.01859728 -0.05961778  0.3546531   1.        ]]. Action = [[0.88764024 0.76958346 0.13634527 0.8103504 ]]. Reward = [0.]
Curr episode timestep = 31
Above hoop
Current timestep = 1064. State = [[-0.00357581 -0.04786953  0.3651546   1.        ]]. Action = [[ 0.5199822  -0.22416031  0.8171115   0.73990655]]. Reward = [0.]
Curr episode timestep = 32
Above hoop
Current timestep = 1065. State = [[ 0.00241792 -0.04788348  0.37630823  1.        ]]. Action = [[-0.5105817  -0.07892787 -0.55631876  0.4225769 ]]. Reward = [0.]
Curr episode timestep = 33
Above hoop
Current timestep = 1066. State = [[ 0.00410763 -0.03717999  0.36358836  1.        ]]. Action = [[ 0.9545747   0.6956495  -0.7503502   0.78470206]]. Reward = [0.]
Curr episode timestep = 34
Above hoop
Current timestep = 1067. State = [[ 0.02441476 -0.03564233  0.332129    1.        ]]. Action = [[ 0.50016356 -0.57317555 -0.6984574   0.4933324 ]]. Reward = [0.]
Curr episode timestep = 35
Above hoop
Current timestep = 1068. State = [[ 0.04640896 -0.04118787  0.31757355  1.        ]]. Action = [[-0.19371217  0.01571751  0.34798086  0.6348798 ]]. Reward = [0.]
Curr episode timestep = 36
Above hoop
Current timestep = 1069. State = [[ 0.04926059 -0.05720992  0.31430167  1.        ]]. Action = [[ 0.8516556  -0.9256613  -0.61403406  0.80513453]]. Reward = [0.]
Curr episode timestep = 37
Above hoop
Current timestep = 1070. State = [[ 0.07739089 -0.07805626  0.30054966  1.        ]]. Action = [[ 0.6720191  -0.2197727   0.21529174  0.67821443]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1071. State = [[ 0.09169005 -0.0875268   0.2927293   1.        ]]. Action = [[-0.5185342  -0.1556822  -0.46096653  0.8478937 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1072. State = [[ 0.08759051 -0.07803517  0.27982262  1.        ]]. Action = [[-0.18945038  0.87172174 -0.82241553  0.68682766]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1073. State = [[ 0.08540227 -0.06612151  0.2660261   1.        ]]. Action = [[-0.27467102 -0.00668705  0.27194524  0.6847669 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1074. State = [[ 0.08428074 -0.05754718  0.27021992  1.        ]]. Action = [[-0.28427994  0.3861562   0.29258013  0.09010887]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1075. State = [[ 0.08072501 -0.04891518  0.27449334  1.        ]]. Action = [[-0.7600014   0.60806704 -0.8640244   0.46290886]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 1076. State = [[ 0.08018249 -0.04747395  0.27511892  1.        ]]. Action = [[0.93650126 0.6807494  0.70628285 0.72580695]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 1077. State = [[ 0.08018249 -0.04747395  0.27511892  1.        ]]. Action = [[ 0.7848644 -0.643608   0.6163467  0.5155238]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 1078. State = [[ 0.08018249 -0.04747395  0.27511892  1.        ]]. Action = [[ 0.7857338  -0.7932157  -0.84980524  0.40261304]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 1079. State = [[ 0.07570489 -0.04962051  0.27499256  1.        ]]. Action = [[-0.7939659  -0.11251098 -0.11445296  0.87917495]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Current timestep = 1080. State = [[ 0.06497676 -0.03616001  0.29011706  1.        ]]. Action = [[-0.2810589   0.89596295  0.92834115  0.60357606]]. Reward = [0.]
Curr episode timestep = 48
Above hoop
Current timestep = 1081. State = [[ 0.04092711 -0.02254883  0.30142632  1.        ]]. Action = [[-0.8985323  -0.33131504 -0.5551417   0.26381195]]. Reward = [0.]
Curr episode timestep = 49
Above hoop
Current timestep = 1082. State = [[ 0.01279539 -0.01031145  0.2855384   1.        ]]. Action = [[-0.46985972  0.8740871  -0.6305495   0.67868125]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Current timestep = 1083. State = [[-0.00587713  0.0159386   0.27685475  1.        ]]. Action = [[-0.50744826  0.5504893   0.43465328  0.37619138]]. Reward = [0.]
Curr episode timestep = 51
Above hoop
Current timestep = 1084. State = [[-0.02434671  0.02950811  0.2846473   1.        ]]. Action = [[-0.23312473  0.1952604  -0.92799455  0.45444083]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Above hoop
Current timestep = 1085. State = [[-0.02442311  0.03983367  0.2909482   1.        ]]. Action = [[0.49458253 0.51587725 0.44661474 0.75266707]]. Reward = [0.]
Curr episode timestep = 53
Above hoop
Current timestep = 1086. State = [[-0.02670219  0.0340092   0.29675427  1.        ]]. Action = [[-0.6563054  -0.9313675  -0.03935707  0.4463184 ]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Current timestep = 1087. State = [[-0.03750475  0.01347761  0.30743995  1.        ]]. Action = [[-0.63350564 -0.34060955  0.38074064  0.52748597]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Current timestep = 1088. State = [[-0.05183779  0.00721905  0.32058606  1.        ]]. Action = [[0.79869294 0.12372971 0.27671003 0.66255283]]. Reward = [0.]
Curr episode timestep = 56
Above hoop
Current timestep = 1089. State = [[-0.0507938   0.01785692  0.3182588   1.        ]]. Action = [[-0.00959897  0.7252197  -0.580006    0.6014283 ]]. Reward = [0.]
Curr episode timestep = 57
Above hoop
Current timestep = 1090. State = [[-0.05551198  0.0191468   0.30352753  1.        ]]. Action = [[-0.529691   -0.70292646 -0.6954828   0.73406255]]. Reward = [0.]
Curr episode timestep = 58
Above hoop
Current timestep = 1091. State = [[-0.05945039  0.00981141  0.29489884  1.        ]]. Action = [[ 0.4920752  -0.11027873  0.61831975  0.19052231]]. Reward = [0.]
Curr episode timestep = 59
Above hoop
Current timestep = 1092. State = [[-0.0490498  -0.00766727  0.31320068  1.        ]]. Action = [[ 0.6253514  -0.8247445   0.9484539   0.49264312]]. Reward = [0.]
Curr episode timestep = 60
Above hoop
Current timestep = 1093. State = [[-0.04669076 -0.02380549  0.3302893   1.        ]]. Action = [[-0.68050045  0.08232677  0.19183981  0.6728668 ]]. Reward = [0.]
Curr episode timestep = 61
Above hoop
Current timestep = 1094. State = [[-0.04912167 -0.04160649  0.3404122   1.        ]]. Action = [[ 0.24335778 -0.9629795   0.06220353  0.6468048 ]]. Reward = [0.]
Curr episode timestep = 62
Above hoop
Current timestep = 1095. State = [[-0.0563731  -0.06516933  0.35837105  1.        ]]. Action = [[-0.86409247 -0.31388795  0.91960764  0.5930618 ]]. Reward = [0.]
Curr episode timestep = 63
Above hoop
Current timestep = 1096. State = [[-0.06958622 -0.06739493  0.3731548   1.        ]]. Action = [[-0.29624593  0.55367804 -0.5702339   0.6307975 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1097. State = [[-0.08546203 -0.06108832  0.36024243  1.        ]]. Action = [[-0.7776628  -0.13309532 -0.58432364  0.8485185 ]]. Reward = [0.]
Curr episode timestep = 65
Above hoop
Current timestep = 1098. State = [[-0.09801124 -0.05173449  0.34965992  1.        ]]. Action = [[0.62754345 0.7214904  0.31849527 0.52783823]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1099. State = [[-0.09910575 -0.04122217  0.34666142  1.        ]]. Action = [[-0.19994271 -0.10702032 -0.54761684  0.7740748 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1100. State = [[-0.09584796 -0.04609076  0.33714554  1.        ]]. Action = [[ 0.6295396  -0.3956293  -0.1848619   0.43743217]]. Reward = [0.]
Curr episode timestep = 68
Above hoop
Current timestep = 1101. State = [[-0.09164181 -0.05564572  0.34244886  1.        ]]. Action = [[-0.40284574 -0.35763586  0.9368621   0.54426336]]. Reward = [0.]
Curr episode timestep = 69
Above hoop
Current timestep = 1102. State = [[-0.0922289  -0.06140058  0.35100585  1.        ]]. Action = [[ 0.34710968  0.20925152 -0.08243233  0.4897585 ]]. Reward = [0.]
Curr episode timestep = 70
Above hoop
Current timestep = 1103. State = [[-0.09195291 -0.06410445  0.3565013   1.        ]]. Action = [[-0.09460205 -0.29885328  0.45744038  0.68855596]]. Reward = [0.]
Curr episode timestep = 71
Above hoop
Current timestep = 1104. State = [[-0.0921686  -0.06049807  0.35829172  1.        ]]. Action = [[ 0.17920005  0.48862028 -0.42026782  0.37888145]]. Reward = [0.]
Curr episode timestep = 72
Above hoop
Current timestep = 1105. State = [[-0.08505652 -0.06367583  0.3641431   1.        ]]. Action = [[ 0.44119287 -0.5854387   0.5901153   0.48416674]]. Reward = [0.]
Curr episode timestep = 73
Above hoop
Current timestep = 1106. State = [[-0.07089975 -0.06266747  0.36899078  1.        ]]. Action = [[ 0.76353085  0.4721403  -0.27356756  0.3433429 ]]. Reward = [0.]
Curr episode timestep = 74
Above hoop
Current timestep = 1107. State = [[-0.04349842 -0.0617589   0.36115003  1.        ]]. Action = [[ 0.9543464  -0.3557713  -0.25929165  0.6494372 ]]. Reward = [0.]
Curr episode timestep = 75
Above hoop
Current timestep = 1108. State = [[-0.0135126  -0.05501765  0.36424237  1.        ]]. Action = [[0.38402092 0.74327767 0.6377027  0.43393803]]. Reward = [0.]
Curr episode timestep = 76
Above hoop
Current timestep = 1109. State = [[ 0.00323357 -0.04499044  0.38694948  1.        ]]. Action = [[ 0.29430032 -0.17817056  0.610903    0.5100769 ]]. Reward = [0.]
Curr episode timestep = 77
Above hoop
Current timestep = 1110. State = [[ 0.01341462 -0.05639143  0.39288068  1.        ]]. Action = [[ 0.8173835  -0.69859564 -0.88995963  0.737427  ]]. Reward = [0.]
Curr episode timestep = 78
Above hoop
Current timestep = 1111. State = [[ 0.0366476  -0.07603479  0.36122707  1.        ]]. Action = [[-0.0302527  -0.2828226  -0.890165    0.72164726]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1112. State = [[ 0.04923644 -0.07755211  0.32864457  1.        ]]. Action = [[ 0.17507577  0.33651865 -0.8293155   0.7319312 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1113. State = [[ 0.06183931 -0.07129328  0.2969841   1.        ]]. Action = [[ 0.6547122   0.04583871 -0.9837019   0.72157025]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1114. State = [[ 0.07176651 -0.08304727  0.25738066  1.        ]]. Action = [[-0.8824845  -0.91361517 -0.9637746   0.48485112]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1115. State = [[ 0.07230071 -0.09583847  0.24479572  1.        ]]. Action = [[0.13799047 0.31035566 0.67485905 0.6424961 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1116. State = [[ 0.06850863 -0.1114063   0.25580913  1.        ]]. Action = [[-0.48987073 -0.954105    0.38763237  0.69906926]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1117. State = [[ 0.06034709 -0.14416836  0.27781615  1.        ]]. Action = [[-0.19251943 -0.9675444   0.678833    0.43641508]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1117 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 1117 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of -1
Current timestep = 1118. State = [[ 0.04418753 -0.17049024  0.28334206  1.        ]]. Action = [[-0.02436101 -0.10114163 -0.84376657  0.67184556]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1119. State = [[ 0.04285135 -0.17175135  0.27546546  1.        ]]. Action = [[-0.02603865  0.08957326  0.42473388  0.76259494]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1120. State = [[-0.26675934  0.16689949  0.10393527  1.        ]]. Action = [[ 0.61011386 -0.67425436 -0.8529638  -0.09159905]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1121. State = [[-0.25594306  0.18568432  0.09613376  1.        ]]. Action = [[ 0.4809848  -0.07679594  0.7003279   0.8705028 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1122. State = [[-0.24502745  0.18889375  0.10791301  1.        ]]. Action = [[0.1662029 0.2950909 0.527622  0.7016747]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1123. State = [[-0.23910153  0.18138522  0.12646829  1.        ]]. Action = [[-0.4173618  -0.9341207   0.5863507   0.51679873]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1124. State = [[-0.23885849  0.18320365  0.15271035  1.        ]]. Action = [[0.34033322 0.95270014 0.7475636  0.5822853 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1125. State = [[-0.23592761  0.20986094  0.1808567   1.        ]]. Action = [[0.15746641 0.8372507  0.7241137  0.7521167 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1126. State = [[-0.22321944  0.22119328  0.20021251  1.        ]]. Action = [[ 0.5316262  -0.4431237  -0.05466479  0.66799474]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1127. State = [[-0.20890558  0.22715165  0.21017992  1.        ]]. Action = [[0.49603796 0.7684822  0.23252463 0.48276126]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1128. State = [[-0.19063239  0.23231399  0.21372135  1.        ]]. Action = [[ 0.5606091  -0.4371276  -0.40991163  0.68111706]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1129. State = [[-0.17180377  0.23673566  0.20975518  1.        ]]. Action = [[0.50503397 0.48622096 0.24748921 0.60817814]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1130. State = [[-0.15783599  0.25477478  0.22004463  1.        ]]. Action = [[-0.02265108  0.6480017   0.48257887  0.6381049 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1131. State = [[-0.15096611  0.25642064  0.2243812   1.        ]]. Action = [[ 0.2589525  -0.71771187 -0.4159125   0.6731318 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1132. State = [[-0.14719349  0.24713357  0.21717268  1.        ]]. Action = [[ 0.3724395   0.10470665 -0.6853435   0.58237934]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1133. State = [[-0.14050764  0.25206438  0.19444431  1.        ]]. Action = [[-0.5929933   0.04927111 -0.12348753  0.32870603]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1134. State = [[-0.13948148  0.25041682  0.18759817  1.        ]]. Action = [[ 0.36049592 -0.17838621 -0.15427226  0.80820477]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1135. State = [[-0.1288858   0.23609972  0.18141535  1.        ]]. Action = [[ 0.69920206 -0.6260147  -0.35490716  0.83411527]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1136. State = [[-0.10230078  0.21265075  0.18053678  1.        ]]. Action = [[ 0.6007036  -0.91655016  0.73284316  0.2888552 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1137. State = [[-0.07935948  0.18039234  0.19877928  1.        ]]. Action = [[ 0.45952797 -0.8067647   0.57775784  0.73878   ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1138. State = [[-0.0689938   0.15721162  0.20989954  1.        ]]. Action = [[-0.45368648 -0.39705586 -0.03789246  0.6023202 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1139. State = [[-0.07911693  0.16178979  0.20588231  1.        ]]. Action = [[-0.7740825   0.9174526  -0.46851033  0.70359373]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1140. State = [[-0.09665746  0.16444096  0.2096269   1.        ]]. Action = [[-0.709099   -0.83827865  0.6913439   0.51247644]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1141. State = [[-0.11081666  0.15376274  0.21487086  1.        ]]. Action = [[ 0.4682498   0.08954048 -0.54869497  0.58353364]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1142. State = [[-0.11117966  0.15263224  0.21136339  1.        ]]. Action = [[ 0.30184937 -0.6866301   0.39675653  0.22250438]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 1143. State = [[-0.11117765  0.15248251  0.21114221  1.        ]]. Action = [[-0.0801484  -0.97855663  0.30150115  0.91512966]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 1144. State = [[-0.11123777  0.15252852  0.21088268  1.        ]]. Action = [[ 0.6005182  -0.31956863  0.24837637  0.6994821 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 1145. State = [[-0.11123777  0.15252852  0.21088268  1.        ]]. Action = [[ 0.63161254 -0.7474255   0.20318556  0.5462204 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 1146. State = [[-0.11123777  0.15252852  0.21088268  1.        ]]. Action = [[ 0.7237576 -0.5009043  0.4519074  0.7369654]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 1147. State = [[-0.11123777  0.15252852  0.21088268  1.        ]]. Action = [[ 0.19336891 -0.12522566  0.2928238   0.53346324]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 1148. State = [[-0.10530156  0.13984105  0.21805273  1.        ]]. Action = [[ 0.27274406 -0.7116962   0.6114472   0.01472437]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1149. State = [[-0.09948756  0.12871507  0.22541928  1.        ]]. Action = [[-0.24445188 -0.46194243 -0.56324023  0.70727634]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 1150. State = [[-0.09895273  0.12609133  0.22634146  1.        ]]. Action = [[ 0.6261691   0.35831523 -0.01708156  0.43269348]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 1151. State = [[-0.09973799  0.12091061  0.23370498  1.        ]]. Action = [[-0.4509024  -0.27717257  0.4895239   0.58495617]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1152. State = [[-0.11279409  0.09959898  0.25585827  1.        ]]. Action = [[-0.7275303 -0.9487801  0.8141897  0.5598593]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1153. State = [[-0.13322997  0.07962234  0.2753786   1.        ]]. Action = [[ 0.19491637  0.25499678 -0.5962281   0.45034468]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 1154. State = [[-0.13690746  0.08730109  0.27347913  1.        ]]. Action = [[ 0.04734313  0.7453215  -0.4363942   0.9139408 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1155. State = [[-0.13145453  0.09223507  0.27908728  1.        ]]. Action = [[ 0.7862735  -0.3239259   0.73000836  0.61792135]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1156. State = [[-0.12718035  0.0791153   0.29074937  1.        ]]. Action = [[-0.36098737 -0.6109643   0.3356985   0.7478409 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1157. State = [[-0.1271105   0.08019079  0.29235017  1.        ]]. Action = [[ 0.39305532  0.9075241  -0.42233324  0.80090845]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1158. State = [[-0.13107882  0.10507046  0.2789331   1.        ]]. Action = [[-0.55316573  0.80567193 -0.7648444   0.48933887]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1159. State = [[-0.14083359  0.12799324  0.26709756  1.        ]]. Action = [[-0.42748547  0.24109864 -0.06892174  0.89150715]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1160. State = [[-0.1407427   0.13431807  0.2697575   1.        ]]. Action = [[ 0.612095   -0.19006109  0.5374534   0.53727865]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1161. State = [[-0.13042651  0.12271415  0.26759106  1.        ]]. Action = [[ 0.75418496 -0.6518327  -0.59157765  0.7813034 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1162. State = [[-0.26547214  0.04539966  0.11291886  1.        ]]. Action = [[-0.5205658  -0.79418975  0.79642785 -0.00474936]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1163. State = [[-0.25574976  0.04124789  0.10438737  1.        ]]. Action = [[ 0.47905016 -0.7707192   0.65180826  0.7783679 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1164. State = [[-0.23621128  0.0370855   0.11920675  1.        ]]. Action = [[0.8435066 0.5098349 0.8376031 0.6526582]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1165. State = [[-0.21717304  0.04141783  0.13630797  1.        ]]. Action = [[-0.7143489  -0.97021437  0.16711223  0.43352902]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1166. State = [[-0.21244855  0.03786493  0.14760205  1.        ]]. Action = [[-0.25673097 -0.3962853   0.60103357  0.61600447]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1167. State = [[-0.21322082  0.03537364  0.17183754  1.        ]]. Action = [[-0.15407562  0.16928506  0.82559896  0.5771208 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1168. State = [[-0.22328681  0.03341073  0.2046788   1.        ]]. Action = [[-0.8097101 -0.2848978  0.9683852  0.8530108]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1169. State = [[-0.235956    0.02206782  0.24130954  1.        ]]. Action = [[ 0.21860373 -0.4517535   0.86073637  0.6291146 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1170. State = [[-0.23484373  0.01403844  0.25981236  1.        ]]. Action = [[ 0.476758   -0.0166567  -0.4533527   0.49842048]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1171. State = [[-0.23221666  0.01432375  0.2644414   1.        ]]. Action = [[-0.47466588  0.10326803  0.62021565  0.5120176 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1172. State = [[-0.23041198  0.02054035  0.2834578   1.        ]]. Action = [[0.48943222 0.44443655 0.6789354  0.7714932 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1173. State = [[-0.22843713  0.01766441  0.29906946  1.        ]]. Action = [[-0.69568324 -0.68547535 -0.00600535  0.6856736 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1174. State = [[-0.24018942  0.00231842  0.31536642  1.        ]]. Action = [[-0.43590122 -0.31036252  0.6364939   0.421461  ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1175. State = [[-0.25021723 -0.01063334  0.34013933  1.        ]]. Action = [[ 0.27929306 -0.3601029   0.763185    0.6425097 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1176. State = [[-0.24860425 -0.0113675   0.3522203   1.        ]]. Action = [[ 0.45907998  0.51592803 -0.39789224  0.26411653]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1177. State = [[-0.23156159 -0.00800819  0.36083338  1.        ]]. Action = [[ 0.8069842  -0.25833023  0.6261455   0.710166  ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1178. State = [[-0.20950384 -0.00416152  0.365927    1.        ]]. Action = [[ 0.87985206  0.3743013  -0.62603694  0.7231741 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1179. State = [[-0.1793799  0.011039   0.3524027  1.       ]]. Action = [[ 0.23038363  0.66534114 -0.10778016  0.50420356]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1180. State = [[-0.15913571  0.03873464  0.3593963   1.        ]]. Action = [[0.8431158  0.99068356 0.7885859  0.58096766]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1181. State = [[-0.13123144  0.07256363  0.39003107  1.        ]]. Action = [[0.78684044 0.8241308  0.95782566 0.15690804]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1182. State = [[-0.109698    0.09131264  0.41231775  1.        ]]. Action = [[0.23661602 0.5083456  0.5979533  0.67501783]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1183. State = [[-0.10406146  0.09578945  0.41197878  1.        ]]. Action = [[ 0.11077476  0.09480023 -0.28882098  0.48202682]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1184. State = [[-0.10082571  0.0947961   0.40828848  1.        ]]. Action = [[-0.31648254 -0.39499444 -0.03987521  0.83218503]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1185. State = [[-0.10061825  0.09060497  0.4086616   1.        ]]. Action = [[ 0.9305079  -0.2743771   0.85212517  0.21360219]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1186. State = [[-0.10496345  0.08683908  0.40352765  1.        ]]. Action = [[-0.6075123  -0.22986054 -0.5149843   0.594205  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1187. State = [[-0.11362018  0.07083858  0.39503634  1.        ]]. Action = [[-0.37613326 -0.7608399  -0.20291185  0.16465938]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1188. State = [[-0.12314173  0.06199414  0.37992272  1.        ]]. Action = [[ 0.17303026  0.32896483 -0.769487    0.31498134]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1189. State = [[-0.12822519  0.0741053   0.3680397   1.        ]]. Action = [[-0.4442078   0.56415343  0.26776123  0.8072963 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1190. State = [[-0.14171761  0.09744601  0.35729364  1.        ]]. Action = [[-0.32835197  0.6464381  -0.92116255  0.73733926]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1191. State = [[-0.15442106  0.10779914  0.32889256  1.        ]]. Action = [[ 0.09366369 -0.1037665  -0.63277864  0.5774777 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1192. State = [[-0.1592887   0.10882209  0.30545425  1.        ]]. Action = [[-0.517515   -0.01545441 -0.46833646  0.3265643 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1193. State = [[-0.16859059  0.11443942  0.2978505   1.        ]]. Action = [[-0.01601428  0.33600247  0.54210234  0.8174828 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1194. State = [[-0.16288438  0.12147621  0.30396113  1.        ]]. Action = [[0.9288784  0.03845024 0.16197264 0.7069671 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1195. State = [[-0.15270698  0.11526435  0.30758482  1.        ]]. Action = [[ 0.44047117 -0.41516572 -0.04176337  0.76534486]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1196. State = [[-0.14637712  0.11879493  0.30761155  1.        ]]. Action = [[-0.31924516  0.7005122   0.12300313  0.7090852 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1197. State = [[-0.14511414  0.13098899  0.30397582  1.        ]]. Action = [[ 0.53366303  0.34857297 -0.44812232  0.84238434]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1198. State = [[-0.12751251  0.15124287  0.29003555  1.        ]]. Action = [[ 0.6721661   0.81286407 -0.43786585  0.09031391]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1199. State = [[-0.11146796  0.17435679  0.28362614  1.        ]]. Action = [[-0.2521417   0.30117285  0.38448334  0.62815   ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1200. State = [[-0.10869099  0.17882851  0.28986782  1.        ]]. Action = [[ 0.33918047 -0.33455408  0.30438304  0.5842562 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1201. State = [[-0.09923417  0.180392    0.29093522  1.        ]]. Action = [[ 0.7977395   0.38291895 -0.7202521   0.53997135]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1202. State = [[-0.0724532   0.20246214  0.28740576  1.        ]]. Action = [[0.6147709  0.9541303  0.8057673  0.57567966]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1203. State = [[-0.05226542  0.2218726   0.30701122  1.        ]]. Action = [[-0.01822251  0.06040609  0.37899446  0.5719149 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1204. State = [[-0.05231401  0.21689999  0.3101611   1.        ]]. Action = [[-0.869552  -0.806337  -0.6123365  0.6295433]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1205. State = [[-0.06215583  0.20695561  0.29556146  1.        ]]. Action = [[-0.382796   -0.06819344 -0.9104282   0.47753096]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1206. State = [[-0.07622807  0.20623787  0.28165326  1.        ]]. Action = [[-0.52536553  0.09725261  0.45630813  0.6466104 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1207. State = [[-0.08109798  0.2119652   0.28080982  1.        ]]. Action = [[ 0.8333118   0.28870237 -0.16205537  0.61250734]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1208. State = [[-0.07778156  0.20687662  0.28983086  1.        ]]. Action = [[-0.40178156 -0.46855438  0.8661518   0.5609579 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1209. State = [[-0.08311543  0.20435014  0.3002268   1.        ]]. Action = [[-0.56140953  0.06232512 -0.24743187  0.784251  ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1210. State = [[-0.08825451  0.20786189  0.3003407   1.        ]]. Action = [[0.6269442  0.26474535 0.1009593  0.74918437]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1211. State = [[-0.08604443  0.20872438  0.29431966  1.        ]]. Action = [[ 0.51482177  0.04440129 -0.86052215  0.42615604]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1212. State = [[-0.08395278  0.21467398  0.26585278  1.        ]]. Action = [[-0.8332183   0.09992361 -0.63982856  0.50373816]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1213. State = [[-0.08754163  0.21758758  0.26621813  1.        ]]. Action = [[-0.3449968  -0.12653935  0.90248585  0.7677549 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1214. State = [[-0.09855977  0.2097562   0.2694383   1.        ]]. Action = [[-0.41419387 -0.4680255  -0.5198239   0.46672738]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1215. State = [[-0.09619632  0.18880714  0.26710823  1.        ]]. Action = [[ 0.8599906  -0.7541124   0.10123396  0.5806575 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1216. State = [[-0.08550121  0.16158669  0.2827561   1.        ]]. Action = [[ 0.08180881 -0.8066479   0.8458774   0.62489223]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1217. State = [[-0.07544737  0.13554396  0.29962668  1.        ]]. Action = [[ 0.5353427  -0.38686544  0.27063048  0.6419549 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1218. State = [[-0.07524047  0.12725802  0.29677045  1.        ]]. Action = [[-0.2933967   0.24102437 -0.87232053  0.53250337]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1219. State = [[-0.07079625  0.11395588  0.2803425   1.        ]]. Action = [[ 0.5028391  -0.89211565 -0.39296895  0.6686001 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1220. State = [[-0.06009812  0.08858193  0.27496472  1.        ]]. Action = [[-0.06025577 -0.60595864  0.30723548  0.64833045]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1221. State = [[-0.06171115  0.07971813  0.27590537  1.        ]]. Action = [[-7.4601895e-01  2.3920131e-01 -2.3865700e-04  7.5194287e-01]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1222. State = [[-0.07585976  0.07136758  0.26802376  1.        ]]. Action = [[-0.1768443  -0.46607548 -0.4915554   0.49260676]]. Reward = [0.]
Curr episode timestep = 59
Above hoop
Current timestep = 1223. State = [[-0.09006949  0.05396822  0.2553919   1.        ]]. Action = [[-0.46423268 -0.5640706  -0.1876651   0.6645198 ]]. Reward = [0.]
Curr episode timestep = 60
Above hoop
Current timestep = 1224. State = [[-0.10653165  0.03658447  0.2531248   1.        ]]. Action = [[-0.5291158  -0.2409882   0.32444012  0.5090177 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1225. State = [[-0.13015386  0.04148567  0.25989246  1.        ]]. Action = [[-0.7036777   0.8676559  -0.03963584  0.681931  ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1226. State = [[-0.14660062  0.06509296  0.25438383  1.        ]]. Action = [[ 0.28133774  0.56751776 -0.4772228   0.67097783]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1227. State = [[-0.1463006   0.07189493  0.25522923  1.        ]]. Action = [[ 0.27410114 -0.26555943  0.8009758   0.23956394]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1228. State = [[-0.14480773  0.07090122  0.26352692  1.        ]]. Action = [[-0.4876626   0.44692254 -0.47401154  0.7737868 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 1229. State = [[-0.14127883  0.06270353  0.27142072  1.        ]]. Action = [[ 0.254125   -0.48290062  0.54495203  0.25664926]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1230. State = [[-0.13581303  0.04553118  0.28940213  1.        ]]. Action = [[ 0.20401824 -0.5122797   0.62053895  0.66834116]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1231. State = [[-0.12635161  0.019892    0.31479394  1.        ]]. Action = [[ 0.8192352  -0.93496174  0.65205336  0.61433554]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1232. State = [[-0.11562595  0.00592535  0.32700637  1.        ]]. Action = [[-0.2622106   0.45726943 -0.36545604  0.50079596]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1233. State = [[-0.1145796   0.02120056  0.31510004  1.        ]]. Action = [[ 0.46938777  0.8436568  -0.82017285  0.5832741 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1234. State = [[-0.09620809  0.04055448  0.30436468  1.        ]]. Action = [[0.32722545 0.25179434 0.45193195 0.6975453 ]]. Reward = [0.]
Curr episode timestep = 71
Above hoop
Current timestep = 1235. State = [[-0.08551864  0.06149309  0.31646106  1.        ]]. Action = [[0.0891192  0.8091533  0.46996427 0.14051557]]. Reward = [0.]
Curr episode timestep = 72
Above hoop
Current timestep = 1236. State = [[-0.07538154  0.08332466  0.33237335  1.        ]]. Action = [[0.6306591  0.4113078  0.4247923  0.58423936]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1237. State = [[-0.07023399  0.10619307  0.3309078   1.        ]]. Action = [[-0.58505213  0.67746377 -0.97979075  0.4078151 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1238. State = [[-0.07490037  0.13043012  0.30593657  1.        ]]. Action = [[ 0.44073462  0.75969553 -0.8215518   0.5222057 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1239. State = [[-0.06726632  0.14443786  0.27593744  1.        ]]. Action = [[-0.50048685 -0.37399662 -0.4311204   0.74131656]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1240. State = [[-0.07463375  0.14692122  0.26625952  1.        ]]. Action = [[-0.8064364   0.08110213 -0.06645817  0.6397141 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1241. State = [[-0.08381615  0.14168032  0.26292026  1.        ]]. Action = [[ 0.18525481 -0.5249824   0.24953794  0.3933593 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1242. State = [[-0.0940664  0.1284148  0.2754062  1.       ]]. Action = [[-0.9943012  -0.24115515  0.5639831   0.63014245]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1243. State = [[-0.12147135  0.11601751  0.29038692  1.        ]]. Action = [[-0.30462247 -0.3256697   0.08713758  0.73818564]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1244. State = [[-0.13157427  0.09688028  0.29836035  1.        ]]. Action = [[ 0.08916354 -0.7289197   0.16924477  0.6649604 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1245. State = [[-0.13658933  0.06834324  0.30021873  1.        ]]. Action = [[-0.17508167 -0.9833718  -0.43831933  0.67818046]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1246. State = [[-0.14312862  0.03510914  0.28589886  1.        ]]. Action = [[ 0.29519737 -0.8315019  -0.48503357  0.5551604 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1247. State = [[-0.13751702  0.02387479  0.26978263  1.        ]]. Action = [[ 0.36290395  0.8239739  -0.4368899   0.6342044 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1248. State = [[-0.13429841  0.03230612  0.25430506  1.        ]]. Action = [[-0.14577425  0.0854919  -0.19299817  0.7449751 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1249. State = [[-0.12540634  0.03929237  0.2548021   1.        ]]. Action = [[0.74741685 0.13873315 0.58935785 0.781037  ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1250. State = [[-0.11723832  0.04342394  0.26063374  1.        ]]. Action = [[ 0.61317897  0.6446688  -0.6441036   0.782243  ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 1251. State = [[-0.10899356  0.04251822  0.26824954  1.        ]]. Action = [[ 0.37655854 -0.22783637  0.57798684  0.5096023 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1252. State = [[-0.10382098  0.04084733  0.27348363  1.        ]]. Action = [[ 0.47939754 -0.29586327 -0.8824106   0.5947299 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 1253. State = [[-0.09837218  0.02650645  0.2860879   1.        ]]. Action = [[ 0.4294417  -0.82204247  0.63037026  0.6322217 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1254. State = [[-0.07873247  0.02109566  0.2957591   1.        ]]. Action = [[ 0.9585397  0.6909903 -0.5185858  0.8128407]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 1255. State = [[-0.05647342  0.03378072  0.28627044  1.        ]]. Action = [[-0.08188671  0.30515814 -0.01470596  0.69781804]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 1256. State = [[-0.04923894  0.04224902  0.29287758  1.        ]]. Action = [[0.07211876 0.07825541 0.44487274 0.74091196]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Scene graph at timestep 1256 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1256 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1256 of 1
Current timestep = 1257. State = [[-0.04302518  0.03534754  0.29272902  1.        ]]. Action = [[ 0.68786263 -0.74992096 -0.69877625  0.6921824 ]]. Reward = [0.]
Curr episode timestep = 94
Above hoop
Scene graph at timestep 1257 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1257 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1257 of -1
Current timestep = 1258. State = [[-0.01892084  0.00831558  0.28021598  1.        ]]. Action = [[ 0.21460891 -0.7476392   0.17411137  0.68452287]]. Reward = [0.]
Curr episode timestep = 95
Above hoop
Current timestep = 1259. State = [[-0.01227205 -0.01839413  0.2796233   1.        ]]. Action = [[ 0.14277983 -0.6569463  -0.23684841  0.6818111 ]]. Reward = [0.]
Curr episode timestep = 96
Above hoop
Current timestep = 1260. State = [[-0.0052767  -0.02825496  0.28214073  1.        ]]. Action = [[0.22008288 0.28426743 0.3900137  0.5464461 ]]. Reward = [0.]
Curr episode timestep = 97
Above hoop
Current timestep = 1261. State = [[-0.0020958  -0.01526689  0.28051582  1.        ]]. Action = [[-0.02858251  0.7135103  -0.6436578   0.9309027 ]]. Reward = [0.]
Curr episode timestep = 98
Above hoop
Current timestep = 1262. State = [[ 0.0052528 -0.0148131  0.2647043  1.       ]]. Action = [[ 0.07738376 -0.6577466  -0.09401584  0.8197565 ]]. Reward = [0.]
Curr episode timestep = 99
Above hoop
Current timestep = 1263. State = [[ 0.00745767 -0.02792098  0.27027017  1.        ]]. Action = [[-0.05271798 -0.38075125  0.65162814  0.39211106]]. Reward = [0.]
Curr episode timestep = 100
Above hoop
Current timestep = 1264. State = [[-0.26791605  0.08789627  0.10270122  1.        ]]. Action = [[-0.72768605  0.17848349 -0.37794948  0.33324206]]. Reward = [0.]
Curr episode timestep = 101
Above hoop
Current timestep = 1265. State = [[-0.2606268   0.10923516  0.09708722  1.        ]]. Action = [[0.3043263 0.6684351 0.9342632 0.7610073]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1266. State = [[-0.25656438  0.12376382  0.1056314   1.        ]]. Action = [[-0.44362283  0.6176703   0.51135254  0.7304921 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 1267. State = [[-0.24743766  0.12981325  0.11016146  1.        ]]. Action = [[0.6180192  0.3277831  0.10276949 0.72575235]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1268. State = [[-0.23092963  0.13423124  0.12619737  1.        ]]. Action = [[ 0.13459647 -0.20070386  0.8487892   0.779374  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1269. State = [[-0.21429211  0.12120139  0.14938338  1.        ]]. Action = [[ 0.73932934 -0.76580113  0.44648123  0.92531896]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1270. State = [[-0.20687819  0.12232822  0.1698398   1.        ]]. Action = [[-0.7893333   0.83848596  0.61265206  0.7260902 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1271. State = [[-0.20611563  0.12262782  0.1931974   1.        ]]. Action = [[ 0.7950934  -0.8313532   0.63701296  0.18120217]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1272. State = [[-0.18923542  0.1103299   0.21701097  1.        ]]. Action = [[ 0.15276921 -0.01104808  0.53047895  0.4793769 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1273. State = [[-0.18872222  0.11300968  0.23460187  1.        ]]. Action = [[-0.45977664  0.26890302  0.33686626  0.6848382 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1274. State = [[-0.19367535  0.11745026  0.2426323   1.        ]]. Action = [[ 0.03669202  0.07626963 -0.33131778  0.6184428 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1275. State = [[-0.19914111  0.12269525  0.2504833   1.        ]]. Action = [[-0.5300094   0.10284722  0.63174367  0.5312152 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1276. State = [[-0.19971633  0.12951843  0.2605827   1.        ]]. Action = [[ 0.86893976  0.37735748 -0.0714559   0.80311227]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1277. State = [[-0.18440348  0.13312477  0.2600912   1.        ]]. Action = [[ 0.55829334 -0.14231145 -0.24582332  0.4980569 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1278. State = [[-0.17140418  0.12980196  0.2624292   1.        ]]. Action = [[-0.02176505 -0.2807663   0.49499738  0.4988097 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1279. State = [[-0.16624454  0.11996799  0.282891    1.        ]]. Action = [[-0.22055411 -0.32729697  0.8769243   0.3726529 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1280. State = [[-0.1713342  0.122145   0.2902859  1.       ]]. Action = [[ 0.01418746  0.5916915  -0.9067279   0.1292845 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1281. State = [[-0.16602641  0.11497566  0.2717257   1.        ]]. Action = [[ 0.8546374  -0.8436471  -0.89148253  0.39859354]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1282. State = [[-0.14732496  0.09165738  0.2600751   1.        ]]. Action = [[-0.3138829  -0.9302699   0.96220076  0.74304295]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1283. State = [[-0.13684028  0.0818022   0.28021345  1.        ]]. Action = [[0.7972332  0.67677426 0.6949959  0.7049153 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1284. State = [[-0.1178529   0.09419151  0.29791087  1.        ]]. Action = [[ 0.4934497   0.36504173 -0.07803041  0.6718066 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1285. State = [[-0.1139388  0.1064949  0.3120665  1.       ]]. Action = [[-0.7357088   0.23706591  0.7866539   0.35884237]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1286. State = [[-0.12916058  0.117598    0.32359645  1.        ]]. Action = [[-0.9344965   0.04218221 -0.4055478   0.3846575 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1287. State = [[-0.14278245  0.11830186  0.31878945  1.        ]]. Action = [[ 0.10230291 -0.12193352 -0.31386793  0.7541909 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1288. State = [[-0.14589417  0.11736583  0.30892226  1.        ]]. Action = [[ 0.16575289  0.14213216 -0.44256586  0.64704514]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1289. State = [[-0.14315708  0.12865676  0.29046762  1.        ]]. Action = [[ 0.9262204   0.840085   -0.9286953   0.87590873]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1290. State = [[-0.13353734  0.14259647  0.25261438  1.        ]]. Action = [[-0.5021922  -0.08040154 -0.68149805  0.87880826]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1291. State = [[-0.13357241  0.14446573  0.23786987  1.        ]]. Action = [[-0.02065945  0.26132035 -0.9148955   0.414047  ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 1292. State = [[-0.13304666  0.1510791   0.23039092  1.        ]]. Action = [[ 0.33004844  0.3418182  -0.62965727  0.55726314]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1293. State = [[-0.12461577  0.1679458   0.22065094  1.        ]]. Action = [[0.51903033 0.8585167  0.69721985 0.71661484]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1294. State = [[-0.11561036  0.1829529   0.2294725   1.        ]]. Action = [[-0.68197054 -0.54083735  0.28472733  0.3859682 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 1295. State = [[-0.12064723  0.1987201   0.23637253  1.        ]]. Action = [[-0.6145677   0.6615176   0.66999865  0.7187686 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1296. State = [[-0.12955725  0.21965848  0.24075523  1.        ]]. Action = [[-0.04918242  0.3199377  -0.40845138  0.3837806 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1297. State = [[-0.12752694  0.21507096  0.24516745  1.        ]]. Action = [[ 0.30186796 -0.7529415   0.5489526   0.7055373 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1298. State = [[-0.11135452  0.20065618  0.26464555  1.        ]]. Action = [[ 0.83422184 -0.26856923  0.9064585   0.8279443 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1299. State = [[-0.09102286  0.20672105  0.29541045  1.        ]]. Action = [[0.533095   0.89414394 0.37891245 0.5365015 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1300. State = [[-0.07266116  0.2226263   0.30416325  1.        ]]. Action = [[ 0.44708276  0.15353894 -0.270288    0.73518157]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1301. State = [[-0.06220237  0.21922983  0.30767596  1.        ]]. Action = [[-0.7551356  -0.8919498   0.46204042  0.40398192]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1302. State = [[-0.06338263  0.2043318   0.31511253  1.        ]]. Action = [[-0.21923882 -0.1813137  -0.01197916  0.54069996]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1303. State = [[-0.06670617  0.19962855  0.3065606   1.        ]]. Action = [[ 0.03209007  0.06217372 -0.9612249   0.87982225]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1304. State = [[-0.07413182  0.19079645  0.29640558  1.        ]]. Action = [[-0.8523395  -0.6414431   0.13524866  0.23908448]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1305. State = [[-0.08825402  0.17966975  0.28400934  1.        ]]. Action = [[ 0.27695417 -0.0321905  -0.79092675  0.90703535]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1306. State = [[-0.09471986  0.18071742  0.26461253  1.        ]]. Action = [[-0.23973167  0.387231   -0.33567095  0.69981503]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1307. State = [[-0.09836905  0.1851601   0.26011854  1.        ]]. Action = [[0.35313332 0.12359083 0.3993547  0.749177  ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1308. State = [[-0.09068205  0.17612569  0.2571728   1.        ]]. Action = [[ 0.6515937  -0.7761515  -0.49042934  0.587549  ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1309. State = [[-0.07353193  0.15228365  0.2570944   1.        ]]. Action = [[ 0.56450725 -0.6592299   0.65926194  0.83025193]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1310. State = [[-0.05921885  0.12415346  0.2640808   1.        ]]. Action = [[ 0.12450671 -0.82258093 -0.02845347  0.47397232]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1311. State = [[-0.05342724  0.10243183  0.2744708   1.        ]]. Action = [[-0.47504246 -0.3393016   0.67530906  0.60373974]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1312. State = [[-0.05204461  0.08437065  0.27829275  1.        ]]. Action = [[ 0.42930257 -0.5198715  -0.77633244  0.29423738]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1313. State = [[-0.04851592  0.06784103  0.2614842   1.        ]]. Action = [[ 0.06647074 -0.24236178 -0.79910123  0.787603  ]]. Reward = [0.]
Curr episode timestep = 48
Above hoop
Current timestep = 1314. State = [[-0.03608253  0.06624226  0.24999449  1.        ]]. Action = [[0.90176094 0.31367147 0.55627584 0.43264127]]. Reward = [0.]
Curr episode timestep = 49
Above hoop
Current timestep = 1315. State = [[-0.0235782   0.06258475  0.25638285  1.        ]]. Action = [[-0.11726582 -0.44742697  0.4499067   0.7736231 ]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Current timestep = 1316. State = [[-0.01830239  0.0596691   0.27314308  1.        ]]. Action = [[0.08589673 0.268548   0.7814435  0.7655232 ]]. Reward = [0.]
Curr episode timestep = 51
Above hoop
Current timestep = 1317. State = [[-0.01843691  0.04741394  0.30537397  1.        ]]. Action = [[-0.9353394  -0.89979404  0.85312986  0.5835719 ]]. Reward = [0.]
Curr episode timestep = 52
Above hoop
Current timestep = 1318. State = [[-0.04042101  0.03853028  0.32219654  1.        ]]. Action = [[-0.7490376  0.3644197 -0.5609123  0.6544218]]. Reward = [0.]
Curr episode timestep = 53
Above hoop
Current timestep = 1319. State = [[-0.05374183  0.04589425  0.3125932   1.        ]]. Action = [[ 0.46104288  0.2776252  -0.43285942  0.17002523]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Current timestep = 1320. State = [[-0.04812784  0.04125619  0.30359045  1.        ]]. Action = [[ 0.9001887  -0.44773602 -0.34282517  0.8088888 ]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Current timestep = 1321. State = [[-0.02795479  0.02500802  0.30127993  1.        ]]. Action = [[ 0.7164959  -0.6888624   0.80802584  0.70143473]]. Reward = [0.]
Curr episode timestep = 56
Above hoop
Current timestep = 1322. State = [[-0.01237021 -0.00138492  0.31040412  1.        ]]. Action = [[-0.4470327 -0.7442029 -0.1657018  0.3082013]]. Reward = [0.]
Curr episode timestep = 57
Above hoop
Current timestep = 1323. State = [[-0.010878   -0.01907509  0.30576175  1.        ]]. Action = [[ 0.00388777 -0.13416684 -0.48010737  0.6476898 ]]. Reward = [0.]
Curr episode timestep = 58
Above hoop
Current timestep = 1324. State = [[-0.00811464 -0.02164442  0.3037188   1.        ]]. Action = [[0.2502787  0.15386379 0.6293298  0.8442975 ]]. Reward = [0.]
Curr episode timestep = 59
Above hoop
Current timestep = 1325. State = [[-0.00258332 -0.02710787  0.30014348  1.        ]]. Action = [[ 0.8242476 -0.3261397 -0.613307   0.6382768]]. Reward = [0.]
Curr episode timestep = 60
Above hoop
Current timestep = 1326. State = [[ 0.01978311 -0.02519176  0.29990643  1.        ]]. Action = [[0.4854071 0.4156549 0.731632  0.7428483]]. Reward = [0.]
Curr episode timestep = 61
Above hoop
Current timestep = 1327. State = [[ 0.03739087 -0.00723674  0.31591263  1.        ]]. Action = [[0.6367178  0.74531925 0.16538167 0.5914706 ]]. Reward = [0.]
Curr episode timestep = 62
Above hoop
Current timestep = 1328. State = [[ 5.1908053e-02 -5.8326329e-04  3.0839971e-01  1.0000000e+00]]. Action = [[-0.19397765 -0.5211179  -0.7126083   0.5326741 ]]. Reward = [0.]
Curr episode timestep = 63
Above hoop
Current timestep = 1329. State = [[ 0.05783099 -0.02053447  0.29493773  1.        ]]. Action = [[ 0.44235063 -0.9482353  -0.1413452   0.60460854]]. Reward = [0.]
Curr episode timestep = 64
Above hoop
Current timestep = 1330. State = [[ 0.06632368 -0.03901288  0.28744408  1.        ]]. Action = [[-0.93902254  0.12002015 -0.23261476  0.53226733]]. Reward = [0.]
Curr episode timestep = 65
Above hoop
Current timestep = 1331. State = [[ 0.06320891 -0.04082745  0.28437704  1.        ]]. Action = [[ 0.8295498  -0.81533206 -0.48907912  0.5553503 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Above hoop
Current timestep = 1332. State = [[ 0.06292103 -0.04093546  0.28405207  1.        ]]. Action = [[ 0.79045    -0.06696761 -0.99586093  0.597798  ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Above hoop
Current timestep = 1333. State = [[ 0.05687312 -0.05088156  0.274546    1.        ]]. Action = [[-0.45437002 -0.59153086 -0.7991498   0.19194031]]. Reward = [0.]
Curr episode timestep = 68
Above hoop
Current timestep = 1334. State = [[ 0.04350149 -0.07177678  0.26418844  1.        ]]. Action = [[-0.21464312 -0.5420923   0.39270544  0.31235754]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1335. State = [[ 0.04283429 -0.078448    0.26288694  1.        ]]. Action = [[ 0.8619553   0.28696704 -0.3562026   0.73329926]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1336. State = [[ 0.05742162 -0.08704928  0.2685625   1.        ]]. Action = [[ 0.89689195 -0.80154556  0.84541845  0.6984806 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1337. State = [[ 0.07493844 -0.10279069  0.28068125  1.        ]]. Action = [[ 0.769557   -0.42554402 -0.9034657   0.59669614]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 1338. State = [[ 0.07719306 -0.10435153  0.2828667   1.        ]]. Action = [[ 0.5525594 -0.6039649  0.4797958  0.8285817]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 1339. State = [[ 0.07137887 -0.11576791  0.27672613  1.        ]]. Action = [[-0.77463365 -0.7744066  -0.62987673  0.7475729 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1340. State = [[ 0.0708852  -0.13214935  0.27918014  1.        ]]. Action = [[0.5382209  0.03459322 0.96265626 0.7168801 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1340 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 1340 is tensor(2.8992e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1340 of -1
Current timestep = 1341. State = [[ 0.07671509 -0.13855352  0.2853062   1.        ]]. Action = [[ 0.38895702 -0.28988206 -0.4547366   0.48713326]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1342. State = [[ 0.07760062 -0.15329063  0.2746987   1.        ]]. Action = [[-0.6754617  -0.44308245 -0.44787353  0.7908354 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1343. State = [[ 0.06890412 -0.17492537  0.26538166  1.        ]]. Action = [[-0.25997204 -0.8654961  -0.39242035  0.7491865 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1344. State = [[ 0.06570577 -0.20423645  0.2621452   1.        ]]. Action = [[-0.05664563 -0.45373523  0.6029408   0.48804855]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1345. State = [[ 0.06467116 -0.2152524   0.2659916   1.        ]]. Action = [[ 0.35206306  0.6622832  -0.65555525  0.39512575]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 1346. State = [[ 0.06502974 -0.2301195   0.27486023  1.        ]]. Action = [[ 0.20785522 -0.8267863   0.39843762  0.8545604 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1347. State = [[ 0.06049317 -0.25633097  0.2687412   1.        ]]. Action = [[-0.0677731  -0.61025876 -0.9928953   0.53225756]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1348. State = [[ 0.05638742 -0.28284046  0.24748133  1.        ]]. Action = [[-0.13411808 -0.74754244 -0.48797137  0.09409535]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1349. State = [[ 0.04669524 -0.3086127   0.2259036   1.        ]]. Action = [[-0.8089576 -0.250611  -0.7378803  0.5150852]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1350. State = [[ 0.03928898 -0.3118345   0.20812319  1.        ]]. Action = [[ 0.3434118   0.33593166 -0.21311587  0.53940606]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1351. State = [[ 0.05161409 -0.30592597  0.20715739  1.        ]]. Action = [[0.79210734 0.10592282 0.61096954 0.71879387]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1352. State = [[ 0.06382072 -0.30021644  0.21514258  1.        ]]. Action = [[-0.09276909  0.15974903  0.228024    0.6911571 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1353. State = [[ 0.06549151 -0.2935037   0.20880792  1.        ]]. Action = [[ 0.16101253  0.24841964 -0.7244691   0.7557576 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1354. State = [[ 0.07193922 -0.2816476   0.21045439  1.        ]]. Action = [[-0.2266751   0.5383184   0.88156223  0.7294303 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1355. State = [[ 0.07442199 -0.27145645  0.21962884  1.        ]]. Action = [[ 0.80299747 -0.347601    0.809849    0.09559047]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 1356. State = [[ 0.07297166 -0.26017305  0.23455465  1.        ]]. Action = [[-0.61301     0.71749246  0.7340791   0.1484909 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1357. State = [[ 0.05061499 -0.238012    0.23897363  1.        ]]. Action = [[-0.5764055  0.3515072 -0.930411   0.7251588]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1358. State = [[ 0.03048976 -0.21669075  0.21996164  1.        ]]. Action = [[-0.6379181  0.7892562 -0.5949538  0.4927057]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1359. State = [[ 0.00723437 -0.19445612  0.20033053  1.        ]]. Action = [[-0.5626467   0.2931353  -0.3590213   0.64835143]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1360. State = [[-6.8918400e-04 -1.8557423e-01  1.9222461e-01  1.0000000e+00]]. Action = [[ 0.7625325  -0.14624411  0.05753255  0.6754451 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1361. State = [[ 0.00253277 -0.1972899   0.19857404  1.        ]]. Action = [[ 0.1300894  -0.8373306   0.44382834  0.6262555 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1362. State = [[ 0.0069745  -0.21509776  0.21387069  1.        ]]. Action = [[ 0.16843915 -0.20216203  0.865886    0.3748448 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1363. State = [[ 0.00921156 -0.2266555   0.22540355  1.        ]]. Action = [[ 0.29730487 -0.39557302 -0.17801088  0.5885699 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1364. State = [[ 0.02173417 -0.24468958  0.23417471  1.        ]]. Action = [[ 0.82013226 -0.85303396  0.44501722  0.573941  ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1365. State = [[ 0.04525657 -0.25773016  0.25658455  1.        ]]. Action = [[0.09776402 0.45217526 0.8338952  0.66684246]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1366. State = [[-0.26684603  0.17217502  0.09366677  1.        ]]. Action = [[ 0.64369    -0.43269372 -0.15808564  0.6957998 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1367. State = [[-0.2564056   0.19866142  0.08364151  1.        ]]. Action = [[0.5019889  0.43880928 0.6912383  0.43616998]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1368. State = [[-0.23776515  0.19685654  0.09414309  1.        ]]. Action = [[ 0.4975896  -0.69173545  0.29398477  0.6592363 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1369. State = [[-0.21559232  0.17348713  0.10850209  1.        ]]. Action = [[ 0.686805   -0.78377914  0.6360159   0.07987821]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1370. State = [[-0.18970323  0.14957559  0.13667771  1.        ]]. Action = [[ 0.5160892  -0.62020457  0.90740633  0.09009397]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1371. State = [[-0.17594606  0.134855    0.17263936  1.        ]]. Action = [[ 0.04791141 -0.16565663  0.9428034   0.50072086]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1372. State = [[-0.17278694  0.12996523  0.19512464  1.        ]]. Action = [[ 0.32462275 -0.09430069  0.7737466   0.7517741 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 1373. State = [[-0.17141871  0.129372    0.19758746  1.        ]]. Action = [[ 0.7808056  -0.33596778 -0.12899971  0.71729565]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 1374. State = [[-0.16638027  0.13892657  0.20363179  1.        ]]. Action = [[0.39911127 0.7058058  0.21283638 0.37306535]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1375. State = [[-0.16951789  0.1615645   0.21923496  1.        ]]. Action = [[-0.92610115  0.7092354   0.63546467  0.62871444]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1376. State = [[-0.17363603  0.17018466  0.2407791   1.        ]]. Action = [[ 0.58196425 -0.5240269   0.55332065  0.6150594 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1377. State = [[-0.16985065  0.15814652  0.26103094  1.        ]]. Action = [[-0.81393135 -0.57988364  0.51156473  0.7999501 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1378. State = [[-0.17774677  0.16134898  0.2744022   1.        ]]. Action = [[ 0.43722153  0.9761846  -0.2965923   0.12862396]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1379. State = [[-0.17110768  0.18662101  0.28337997  1.        ]]. Action = [[0.91873455 0.9576855  0.63221955 0.83142376]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1380. State = [[-0.14906977  0.21567944  0.30661428  1.        ]]. Action = [[0.520025   0.65890884 0.7021295  0.7660799 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1381. State = [[-0.131705    0.23703037  0.3244016   1.        ]]. Action = [[0.19777286 0.33892822 0.17698395 0.68110394]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1382. State = [[-0.13010749  0.24821699  0.33562195  1.        ]]. Action = [[-0.7653856  -0.13867706  0.24849153  0.83620775]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1383. State = [[-0.13539143  0.24364606  0.34783095  1.        ]]. Action = [[-0.19316685 -0.5872759   0.5308826   0.7479118 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1384. State = [[-0.14559978  0.24578537  0.37078625  1.        ]]. Action = [[-0.3511988   0.6790173   0.6744795   0.76842904]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1385. State = [[-0.154938    0.26609114  0.38496     1.        ]]. Action = [[0.5610707  0.90956557 0.09630597 0.7205976 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1386. State = [[-0.1518441   0.27730584  0.3840077   1.        ]]. Action = [[-0.47112703 -0.49090117 -0.8729474   0.7452166 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1387. State = [[-0.15127647  0.27595544  0.3653139   1.        ]]. Action = [[ 0.568985    0.18259478 -0.7170087   0.5573609 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1388. State = [[-0.13719955  0.27478644  0.35179982  1.        ]]. Action = [[ 0.5656123  -0.17102993  0.6356858   0.54864407]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1389. State = [[-0.13465069  0.26962727  0.3487689   1.        ]]. Action = [[-0.7523599  -0.32364357 -0.91315144  0.5665941 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1390. State = [[-0.13911918  0.26632723  0.33975723  1.        ]]. Action = [[-0.887526    0.6459124   0.28887558  0.69146526]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1391. State = [[-0.14080632  0.25509778  0.33028692  1.        ]]. Action = [[-0.32480884 -0.7854905  -0.736423    0.62070465]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1392. State = [[-0.13765836  0.22362618  0.31866655  1.        ]]. Action = [[ 0.68607855 -0.960695    0.5326642   0.66356134]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1393. State = [[-0.13472377  0.21357757  0.3302364   1.        ]]. Action = [[-0.4433967   0.63001645  0.64548826  0.5591705 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1394. State = [[-0.13361703  0.2243229   0.33680534  1.        ]]. Action = [[ 0.6838646   0.5729556  -0.04994595  0.7994697 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1395. State = [[-0.13551895  0.23225754  0.3285185   1.        ]]. Action = [[-0.5204574  -0.00282001 -0.860941    0.3777696 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1396. State = [[-0.14172997  0.24487342  0.31384885  1.        ]]. Action = [[-0.02980912  0.565542   -0.00374866  0.7984078 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1397. State = [[-0.14108881  0.24177971  0.31548074  1.        ]]. Action = [[-0.03145045 -0.96992767  0.32193244  0.5299058 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1398. State = [[-0.13339171  0.23295422  0.32691368  1.        ]]. Action = [[0.52554727 0.17102182 0.69250023 0.36496806]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1399. State = [[-0.13242792  0.22101201  0.33344793  1.        ]]. Action = [[-0.7912703  -0.80424625 -0.3705902   0.6818061 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1400. State = [[-0.14179912  0.19713147  0.325114    1.        ]]. Action = [[-0.31011522 -0.7853912  -0.8504575   0.72726154]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1401. State = [[-0.1450135   0.18338062  0.3141164   1.        ]]. Action = [[0.8175367  0.312639   0.37834454 0.74858046]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1402. State = [[-0.13854384  0.18305807  0.31995013  1.        ]]. Action = [[ 0.18072248 -0.01320076  0.3314805   0.58693266]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1403. State = [[-0.13361102  0.1842497   0.32689014  1.        ]]. Action = [[0.03856838 0.16377461 0.28957283 0.64302754]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1404. State = [[-0.120924    0.17267872  0.338749    1.        ]]. Action = [[ 0.9226525  -0.7334752   0.38340473  0.51586497]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1405. State = [[-0.10931913  0.16401167  0.34263727  1.        ]]. Action = [[-0.18204081  0.43813515 -0.2589031   0.6799302 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1406. State = [[-0.10411082  0.16089103  0.3347426   1.        ]]. Action = [[ 0.23395538 -0.43118578 -0.4659186   0.5905111 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1407. State = [[-0.09376965  0.14510214  0.3395377   1.        ]]. Action = [[ 0.47610414 -0.70035386  0.9656514   0.6860876 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1408. State = [[-0.08387887  0.13646087  0.35665146  1.        ]]. Action = [[-0.19504917  0.2237289   0.23340118  0.744532  ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1409. State = [[-0.08701935  0.13003467  0.35551268  1.        ]]. Action = [[-0.68899006 -0.67161494 -0.8311968   0.552964  ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1410. State = [[-0.10195966  0.11556111  0.33780545  1.        ]]. Action = [[-0.91594476 -0.2395848  -0.8807767   0.47672474]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1411. State = [[-0.12474848  0.10211873  0.30636275  1.        ]]. Action = [[-0.59119564 -0.3353176  -0.6933944   0.7132108 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1412. State = [[-0.13866068  0.09973729  0.29889894  1.        ]]. Action = [[0.12267435 0.34439516 0.87706184 0.4544338 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1413. State = [[-0.13780504  0.11228949  0.31369266  1.        ]]. Action = [[0.5405307  0.6028727  0.19683206 0.6803658 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1414. State = [[-0.14001621  0.12032685  0.31466788  1.        ]]. Action = [[-0.1473403   0.07042062 -0.14223403  0.6101613 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1415. State = [[-0.13348103  0.11584764  0.3216023   1.        ]]. Action = [[ 0.74302197 -0.5277527   0.5915605   0.8869941 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1416. State = [[-0.1290832   0.11587137  0.32244807  1.        ]]. Action = [[-0.49271166  0.4028796  -0.6359736   0.7212138 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1417. State = [[-0.12882479  0.10900958  0.3167012   1.        ]]. Action = [[ 0.1366328  -0.74862075 -0.07623315  0.7157358 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1418. State = [[-0.127787    0.11028166  0.3029626   1.        ]]. Action = [[ 0.36632347  0.8106754  -0.8937662   0.8590107 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1419. State = [[-0.12829168  0.1341861   0.2748684   1.        ]]. Action = [[-0.41884387  0.9073355  -0.3993101   0.39601707]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1420. State = [[-0.13547628  0.14416578  0.27806923  1.        ]]. Action = [[-0.7221422 -0.6161269  0.9416425  0.7049241]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1421. State = [[-0.14829387  0.12897477  0.30069548  1.        ]]. Action = [[-0.8687646  -0.86867017  0.5816679   0.7493577 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1422. State = [[-0.16683523  0.09937239  0.32761183  1.        ]]. Action = [[ 0.4376843  -0.64390975  0.76391506  0.521765  ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1423. State = [[-0.17167632  0.095961    0.34390053  1.        ]]. Action = [[-0.25642884  0.7939155   0.16457748  0.6304333 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1424. State = [[-0.1684286   0.09886294  0.36053467  1.        ]]. Action = [[ 0.8490546  -0.41791368  0.9374423   0.7253618 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1425. State = [[-0.15733524  0.08702262  0.3887465   1.        ]]. Action = [[ 0.30364382 -0.41223526  0.516819    0.6229229 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1426. State = [[-0.15677547  0.08201917  0.40157855  1.        ]]. Action = [[-0.7610121   0.11371779 -0.21612698  0.6598009 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1427. State = [[-0.15895647  0.08180826  0.39639926  1.        ]]. Action = [[ 0.24588203 -0.00429749 -0.57749     0.6533054 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1428. State = [[-0.15543258  0.0903947   0.38350415  1.        ]]. Action = [[ 0.9461349   0.690377   -0.60228884  0.85138965]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1429. State = [[-0.1364387   0.1098481   0.35230327  1.        ]]. Action = [[ 0.26661444  0.47613835 -0.8242277   0.57693815]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1430. State = [[-0.11661599  0.10857112  0.32532057  1.        ]]. Action = [[ 0.7888231 -0.7913776 -0.6947864  0.5618074]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1431. State = [[-0.09583366  0.09810391  0.30541137  1.        ]]. Action = [[0.15405762 0.08402097 0.01749194 0.73195815]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1432. State = [[-0.08878915  0.09376147  0.31241974  1.        ]]. Action = [[-0.07773066 -0.25979412  0.8326225   0.69802845]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1433. State = [[-0.08277255  0.08182946  0.31849745  1.        ]]. Action = [[ 0.04979944 -0.5698943  -0.5420132   0.4586345 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1434. State = [[-0.08408678  0.05765826  0.307939    1.        ]]. Action = [[-0.46847737 -0.7768436  -0.8175629   0.7060902 ]]. Reward = [0.]
Curr episode timestep = 67
Above hoop
Current timestep = 1435. State = [[-0.08611961  0.03760829  0.29955286  1.        ]]. Action = [[ 0.51811385 -0.18985337  0.63024557  0.36168957]]. Reward = [0.]
Curr episode timestep = 68
Above hoop
Current timestep = 1436. State = [[-0.0870939   0.03988576  0.31348288  1.        ]]. Action = [[-0.9002      0.5763868   0.80365026  0.80598307]]. Reward = [0.]
Curr episode timestep = 69
Above hoop
Current timestep = 1437. State = [[-0.09806761  0.05661077  0.3295977   1.        ]]. Action = [[-0.01214039  0.45129824 -0.07750398  0.7908189 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1438. State = [[-0.10054284  0.07210832  0.3190341   1.        ]]. Action = [[ 0.64073324  0.47680807 -0.9710707   0.75525606]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1439. State = [[-0.09365643  0.07862019  0.29063535  1.        ]]. Action = [[ 0.12938046 -0.12148225 -0.8608588   0.5774596 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1440. State = [[-0.09433394  0.07884617  0.27058643  1.        ]]. Action = [[-0.750811   -0.07680845 -0.03652549  0.8056519 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1441. State = [[-0.09933139  0.08634315  0.2690968   1.        ]]. Action = [[-0.22021031  0.45377898  0.04952931  0.5515641 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1442. State = [[-0.10651346  0.09920537  0.2682511   1.        ]]. Action = [[-0.18209457  0.24132645  0.04625344  0.70924187]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1443. State = [[-0.1130079   0.09349811  0.27605364  1.        ]]. Action = [[-0.23398799 -0.6907853   0.64541304  0.63139486]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1444. State = [[-0.12029343  0.08152745  0.29458663  1.        ]]. Action = [[ 0.04509199 -0.08129418  0.48030078  0.8922926 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1445. State = [[-0.12297801  0.07428978  0.31667358  1.        ]]. Action = [[-0.32895434 -0.1976363   0.77020836  0.83872676]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1446. State = [[-0.13683328  0.07046113  0.32226565  1.        ]]. Action = [[-0.3631857   0.11999452 -0.87686545  0.5264951 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1447. State = [[-0.14143084  0.05846306  0.30451038  1.        ]]. Action = [[ 0.73864126 -0.76921976 -0.71749365  0.81737316]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1448. State = [[-0.12445731  0.03703375  0.29536572  1.        ]]. Action = [[ 0.8158214  -0.552328    0.5493721   0.54839015]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1449. State = [[-0.10561207  0.02627546  0.3063572   1.        ]]. Action = [[0.43269098 0.12195528 0.83604383 0.79370654]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1450. State = [[-0.09990939  0.01927878  0.31143978  1.        ]]. Action = [[-0.7948336  -0.4914087  -0.6231408   0.71086454]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1451. State = [[-0.10326563  0.0222415   0.3005094   1.        ]]. Action = [[ 0.61780715  0.8350191  -0.42391694  0.7983954 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1452. State = [[-0.09742368  0.01688536  0.3049513   1.        ]]. Action = [[-0.37960792 -0.9542622   0.7871487   0.8312323 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1453. State = [[-0.10036098  0.0126172   0.31276956  1.        ]]. Action = [[-0.35162115  0.42655742 -0.13643402  0.7112305 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1454. State = [[-0.1001184   0.00647183  0.3124022   1.        ]]. Action = [[ 0.49497724 -0.6221212  -0.25274193  0.70696497]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1455. State = [[-1.0006508e-01  7.3482300e-04  3.1319144e-01  1.0000000e+00]]. Action = [[-0.43205774  0.22751999  0.27465296  0.57116604]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1456. State = [[-0.0953474   0.01553472  0.31522095  1.        ]]. Action = [[0.9314672  0.98377955 0.01974249 0.86693764]]. Reward = [0.]
Curr episode timestep = 89
Above hoop
Current timestep = 1457. State = [[-0.0787925   0.04306802  0.32800293  1.        ]]. Action = [[0.82224584 0.6445031  0.8240752  0.78668547]]. Reward = [0.]
Curr episode timestep = 90
Above hoop
Current timestep = 1458. State = [[-0.06575479  0.05693768  0.35427642  1.        ]]. Action = [[-0.1960674  -0.14584637  0.8924929   0.4973874 ]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 1459. State = [[-0.05630568  0.05130234  0.37160856  1.        ]]. Action = [[ 0.6091809  -0.43939078 -0.02913672  0.6795008 ]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 1460. State = [[-0.0442787   0.05646588  0.368799    1.        ]]. Action = [[ 0.68022096  0.83087254 -0.9246482   0.6476954 ]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Scene graph at timestep 1460 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1460 is tensor(4.6122e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1460 of 1
Current timestep = 1461. State = [[-0.26025552 -0.06580482  0.09690209  1.        ]]. Action = [[-0.76625645  0.35163736  0.83122337 -0.4921639 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1462. State = [[-0.25665846 -0.06576556  0.08902918  1.        ]]. Action = [[0.38819146 0.6685014  0.72891617 0.16231298]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1463. State = [[-0.24264865 -0.06074994  0.10445157  1.        ]]. Action = [[ 0.71560335 -0.08739918  0.758219    0.58819234]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1464. State = [[-0.21377106 -0.05453958  0.13074252  1.        ]]. Action = [[0.94719446 0.39026642 0.53329396 0.51759815]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1465. State = [[-0.19024369 -0.06072384  0.15563302  1.        ]]. Action = [[-0.17708802 -0.94238764  0.8847654   0.8347726 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1466. State = [[-0.18929784 -0.07258332  0.17699602  1.        ]]. Action = [[ 0.73001194 -0.48029017  0.810397    0.72647405]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 1467. State = [[-0.18516882 -0.06597133  0.1900115   1.        ]]. Action = [[0.2381326  0.6525552  0.76520455 0.83800936]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1468. State = [[-0.17897865 -0.06148993  0.21809123  1.        ]]. Action = [[-0.11756432 -0.31048453  0.6932328   0.58201945]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1469. State = [[-0.18117164 -0.07577943  0.23850666  1.        ]]. Action = [[-0.2400462  -0.7380517   0.21056592  0.4646381 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1470. State = [[-0.19148116 -0.07840992  0.24716826  1.        ]]. Action = [[-0.70721775  0.68883777  0.12596059  0.7654369 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1471. State = [[-0.19734156 -0.07073994  0.25309613  1.        ]]. Action = [[ 0.45322275  0.02621162 -0.3144064   0.70020854]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1472. State = [[-0.19477546 -0.05602375  0.25703442  1.        ]]. Action = [[0.10997021 0.8203635  0.5854547  0.43438935]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1473. State = [[-0.18881702 -0.02871397  0.27129757  1.        ]]. Action = [[0.32251394 0.6681311  0.7928641  0.7106118 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1474. State = [[-0.1763201   0.00252754  0.30228868  1.        ]]. Action = [[0.305202   0.92778015 0.7653352  0.5795069 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1475. State = [[-0.17177959  0.02304101  0.33425838  1.        ]]. Action = [[-0.41298044 -0.04607314  0.8065729   0.6999335 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1476. State = [[-0.18266442  0.03377683  0.35616735  1.        ]]. Action = [[-0.6153048   0.37342095 -0.06443423  0.48044336]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1477. State = [[-0.19370309  0.03167468  0.35431552  1.        ]]. Action = [[ 0.07855761 -0.58275586 -0.86984634  0.37407947]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1478. State = [[-0.20256643  0.02489427  0.33540973  1.        ]]. Action = [[-0.5628685   0.12157071 -0.73029804  0.5654849 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1479. State = [[-0.20783359  0.0290191   0.30797818  1.        ]]. Action = [[ 0.91547775  0.37116444 -0.8049619   0.58335114]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1480. State = [[-0.19366935  0.03268574  0.28496242  1.        ]]. Action = [[ 0.18472803 -0.15399545  0.34092414  0.53597116]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1481. State = [[-0.18302837  0.0429879   0.29412448  1.        ]]. Action = [[0.31961215 0.64384353 0.60658455 0.16478646]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1482. State = [[-0.17977765  0.05918458  0.3012215   1.        ]]. Action = [[-0.17913854  0.41674674 -0.06630188  0.7309048 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1483. State = [[-0.18838726  0.07711963  0.30886966  1.        ]]. Action = [[-0.6411145   0.45680177  0.6028725   0.37291253]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1484. State = [[-0.19458562  0.10144565  0.3189794   1.        ]]. Action = [[ 0.47170174  0.81092596 -0.12681818  0.62693954]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1485. State = [[-0.20101227  0.13137922  0.31130722  1.        ]]. Action = [[-0.8283053   0.7914345  -0.82850385  0.5874002 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1486. State = [[-0.21747583  0.16175632  0.30451685  1.        ]]. Action = [[-0.29029042  0.6761528   0.6923218   0.6860429 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1487. State = [[-0.22878411  0.18136409  0.30782145  1.        ]]. Action = [[-0.2143119   0.11260772 -0.4031366   0.7445973 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1488. State = [[-0.23667146  0.19647382  0.30991337  1.        ]]. Action = [[-0.11656404  0.66667354  0.48929024  0.4264047 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1489. State = [[-0.23756664  0.21299711  0.3182853   1.        ]]. Action = [[0.32039416 0.2518561  0.10602486 0.7285838 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1490. State = [[-0.23176205  0.21201144  0.33314726  1.        ]]. Action = [[-0.04495585 -0.52682805  0.7112632   0.7937679 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1491. State = [[-0.22163026  0.19595563  0.34428674  1.        ]]. Action = [[ 0.5857439  -0.55053097 -0.14472777  0.78674316]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1492. State = [[-0.2164558  0.189697   0.3478659  1.       ]]. Action = [[-0.09841985  0.31789315  0.04966772  0.5910373 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1493. State = [[-0.21162271  0.1880722   0.3606184   1.        ]]. Action = [[ 0.09528852 -0.29794377  0.79397726  0.23261869]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1494. State = [[-0.20460008  0.19049399  0.36602286  1.        ]]. Action = [[ 0.36639035  0.4557953  -0.6296582   0.79007614]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1495. State = [[-0.19843207  0.19167031  0.3680359   1.        ]]. Action = [[-0.35313594 -0.42778844  0.5411246   0.38813877]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1496. State = [[-0.20724876  0.19131283  0.36708507  1.        ]]. Action = [[-0.60785866  0.07604778 -0.67994505  0.7756903 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1497. State = [[-0.2103683   0.19521193  0.3533719   1.        ]]. Action = [[ 0.84807587  0.39412224 -0.71796256  0.6773498 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1498. State = [[-0.20091116  0.20877396  0.3443297   1.        ]]. Action = [[0.04589212 0.55299485 0.7968354  0.7132497 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1499. State = [[-0.20458905  0.22524966  0.3506833   1.        ]]. Action = [[-0.71701896  0.20380735 -0.29444826  0.55825615]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1500. State = [[-0.20770758  0.24483588  0.35128024  1.        ]]. Action = [[0.82142735 0.86738455 0.189677   0.826035  ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1501. State = [[-0.19118284  0.24466921  0.35145622  1.        ]]. Action = [[ 0.46868968 -0.893968    0.01365602  0.62724686]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1502. State = [[-0.18039384  0.22382344  0.34889668  1.        ]]. Action = [[ 0.23008251 -0.6548143  -0.37124676  0.40001118]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1503. State = [[-0.1635368   0.22134104  0.34237882  1.        ]]. Action = [[0.9389987  0.7515323  0.05736327 0.46959996]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1504. State = [[-0.13228676  0.22806107  0.35079208  1.        ]]. Action = [[ 0.90916276 -0.2446909   0.75606775  0.78631353]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1505. State = [[-0.1083714   0.21307912  0.36931172  1.        ]]. Action = [[-0.12241518 -0.84835386  0.14165485  0.6699183 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1506. State = [[-0.11294364  0.21213597  0.37333864  1.        ]]. Action = [[-0.9031803   0.6923404  -0.14549834  0.43972468]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1507. State = [[-0.12599085  0.21538371  0.3679968   1.        ]]. Action = [[-0.65657127 -0.6011344  -0.56383544  0.80819535]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1508. State = [[-0.13069652  0.19850586  0.3698107   1.        ]]. Action = [[ 0.5025188 -0.4794535  0.7073684  0.640206 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1509. State = [[-0.12071896  0.17183577  0.3794552   1.        ]]. Action = [[ 0.94875646 -0.917904   -0.1448      0.6805419 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1510. State = [[-0.1085939   0.15722498  0.37088022  1.        ]]. Action = [[ 0.19101644  0.4966097  -0.74321765  0.754568  ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1511. State = [[-0.09577415  0.15958531  0.3504955   1.        ]]. Action = [[ 0.61037326 -0.03203392 -0.04326683  0.8432795 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1512. State = [[-0.07959437  0.16918306  0.3399713   1.        ]]. Action = [[ 0.7224271   0.6353569  -0.5303923   0.42500055]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1513. State = [[-0.05740491  0.17097862  0.33139235  1.        ]]. Action = [[-0.21296561 -0.7373912   0.38586092  0.74842215]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1514. State = [[-0.0508486   0.14836942  0.34403864  1.        ]]. Action = [[-0.03883541 -0.93929803  0.47553802  0.13556278]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1515. State = [[-0.0548031   0.13312063  0.34378305  1.        ]]. Action = [[-0.5206975   0.21006083 -0.85428554  0.7457223 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1516. State = [[-0.0600973   0.12363563  0.34803727  1.        ]]. Action = [[-0.4397316 -0.5686648  0.8009938  0.5923221]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1517. State = [[-0.06612442  0.11532667  0.36140147  1.        ]]. Action = [[0.40541255 0.14857697 0.36011827 0.6694312 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1518. State = [[-0.0654221   0.12487761  0.37262058  1.        ]]. Action = [[0.10161734 0.62088144 0.631297   0.5947726 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1519. State = [[-0.06816962  0.1264432   0.3872747   1.        ]]. Action = [[-0.82052743 -0.6453689  -0.08811164  0.8491379 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1520. State = [[-0.09086582  0.11134046  0.3884205   1.        ]]. Action = [[-0.8940538  -0.42246985 -0.51872265  0.6460905 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1521. State = [[-0.11413102  0.08910997  0.36917508  1.        ]]. Action = [[ 0.01709783 -0.77665216 -0.95136416  0.5426558 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1522. State = [[-0.12029847  0.05851963  0.33785358  1.        ]]. Action = [[ 0.44335425 -0.9128668  -0.8404315   0.5490209 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1523. State = [[-0.11955603  0.02115464  0.30638677  1.        ]]. Action = [[-0.81830263 -0.9367705  -0.93056273  0.70278096]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1524. State = [[-0.13917556  0.0115375   0.27847812  1.        ]]. Action = [[-0.16416216  0.94047713 -0.21761268  0.3694756 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1525. State = [[-0.14124054  0.02963247  0.26803052  1.        ]]. Action = [[0.44921064 0.4929695  0.02349353 0.70919204]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1526. State = [[-0.14016353  0.03807994  0.26289085  1.        ]]. Action = [[-0.21247768 -0.1276505  -0.19607067  0.8238876 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1527. State = [[-0.14490253  0.02330183  0.27266285  1.        ]]. Action = [[-0.62444454 -0.9423048   0.8905804   0.26276624]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1528. State = [[-0.15857206  0.02388165  0.2952419   1.        ]]. Action = [[-0.26842898  0.9053011   0.93481994  0.92792046]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1529. State = [[-0.17365941  0.04336135  0.30664805  1.        ]]. Action = [[-0.26423335  0.49617636 -0.6267997   0.48381257]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1530. State = [[-0.18258475  0.05160634  0.31244022  1.        ]]. Action = [[-0.30262375 -0.20831764  0.7661953   0.7284324 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1531. State = [[-0.1845833   0.04676537  0.31642535  1.        ]]. Action = [[ 0.7290907  -0.18616498 -0.5918264   0.30332828]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1532. State = [[-0.17074455  0.05597419  0.31228375  1.        ]]. Action = [[0.8591449  0.8341117  0.275589   0.64996576]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1533. State = [[-0.15149774  0.08006182  0.30256006  1.        ]]. Action = [[ 0.56917465  0.8085408  -0.885316    0.74795866]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1534. State = [[-0.1343458   0.11351378  0.28404242  1.        ]]. Action = [[-0.16790193  0.950212    0.14611769  0.34992957]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1535. State = [[-0.1369099   0.12295269  0.29352337  1.        ]]. Action = [[-0.6380594  -0.7668189   0.70542336  0.7748499 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1536. State = [[-0.13509409  0.102819    0.30315518  1.        ]]. Action = [[ 0.7487602  -0.8599312  -0.36090004  0.71790063]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1537. State = [[-0.12246481  0.07814914  0.30448994  1.        ]]. Action = [[ 0.5118301  -0.4221593   0.38386083  0.77918696]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1538. State = [[-0.1182846   0.07637744  0.31268194  1.        ]]. Action = [[-0.4530754   0.503369    0.41299176  0.559551  ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1539. State = [[-0.11660681  0.07961256  0.32847175  1.        ]]. Action = [[ 0.27549374 -0.14709395  0.63364863  0.62019205]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1540. State = [[-0.11674819  0.06421617  0.3389021   1.        ]]. Action = [[-0.41005224 -0.90788764 -0.4834941   0.7862878 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1541. State = [[-0.1278688   0.05053854  0.33000877  1.        ]]. Action = [[-0.56546956  0.08135438 -0.66742814  0.8557154 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1542. State = [[-0.13344236  0.0444588   0.3261      1.        ]]. Action = [[ 0.5738851  -0.17447972  0.71881056  0.5426018 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1543. State = [[-0.13022467  0.03187263  0.33562455  1.        ]]. Action = [[-0.09799963 -0.5638447   0.22150779  0.48932552]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1544. State = [[-0.13002333  0.03180582  0.33533973  1.        ]]. Action = [[ 0.22618127  0.71589637 -0.49721372  0.52396894]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1545. State = [[-0.11996736  0.04285272  0.3396261   1.        ]]. Action = [[0.4971155  0.14586854 0.84827375 0.7491025 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1546. State = [[-0.11189888  0.04778517  0.35349065  1.        ]]. Action = [[-0.20123607 -0.09644228  0.22200859  0.74431324]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1547. State = [[-0.1066061   0.05113889  0.36913258  1.        ]]. Action = [[0.39684153 0.23897958 0.72949934 0.71405697]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1548. State = [[-0.09340347  0.06678843  0.37920156  1.        ]]. Action = [[ 0.78465915  0.72269297 -0.6397506   0.68518484]]. Reward = [0.]
Curr episode timestep = 86
Above hoop
Current timestep = 1549. State = [[-0.08240405  0.08580304  0.36488584  1.        ]]. Action = [[-0.4026631   0.40351105 -0.4430648   0.59367156]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1550. State = [[-0.07634094  0.08042382  0.363549    1.        ]]. Action = [[ 0.75384426 -0.85978806  0.4181217   0.85542655]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1551. State = [[-0.0693513   0.05532782  0.3649837   1.        ]]. Action = [[-0.82089126 -0.9072001  -0.43874884  0.6606109 ]]. Reward = [0.]
Curr episode timestep = 89
Above hoop
Current timestep = 1552. State = [[-0.07751422  0.03466104  0.35841805  1.        ]]. Action = [[-0.28665715 -0.33552933  0.02941227  0.8785149 ]]. Reward = [0.]
Curr episode timestep = 90
Above hoop
Current timestep = 1553. State = [[-0.08096655  0.03277165  0.3507699   1.        ]]. Action = [[ 0.49372864  0.6243913  -0.65527934  0.6485586 ]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 1554. State = [[-0.06996077  0.04571854  0.34696075  1.        ]]. Action = [[0.66448677 0.45481777 0.57498074 0.778337  ]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 1555. State = [[-0.05843387  0.06373866  0.34990537  1.        ]]. Action = [[ 0.6225257   0.58630025 -0.7228883   0.8750529 ]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Current timestep = 1556. State = [[-0.04676695  0.0839159   0.31616414  1.        ]]. Action = [[-0.53590167  0.410604   -0.8983338   0.56630325]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1556 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1556 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1556 of 1
Current timestep = 1557. State = [[-0.05123283  0.10110235  0.28881982  1.        ]]. Action = [[-0.80931413  0.3145888  -0.7126652   0.83685505]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1558. State = [[-0.06431754  0.10023855  0.27529526  1.        ]]. Action = [[ 0.08233786 -0.5697422  -0.04181904  0.6418066 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1559. State = [[-0.07221916  0.09418232  0.2642549   1.        ]]. Action = [[-0.4581113   0.15547681 -0.5132565   0.39905   ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1560. State = [[-0.07506624  0.08496173  0.26062077  1.        ]]. Action = [[ 0.31671834 -0.6618776   0.501601    0.23207319]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1561. State = [[-0.07066427  0.07735939  0.2630506   1.        ]]. Action = [[-0.07011586 -0.21306616 -0.62708724  0.76377964]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 1562. State = [[-0.0704184   0.07607429  0.2630914   1.        ]]. Action = [[-0.48136318 -0.93976474 -0.8551444   0.6364136 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 1563. State = [[-0.26650628  0.05563862  0.10117351  1.        ]]. Action = [[-0.29736924  0.3058356  -0.00907958  0.57298267]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1564. State = [[-0.25955394  0.05866003  0.09455571  1.        ]]. Action = [[ 0.3179543  -0.19964099  0.8324914   0.46955347]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1565. State = [[-0.25007382  0.06334349  0.10500111  1.        ]]. Action = [[0.38434267 0.46396494 0.1456871  0.81564534]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1566. State = [[-0.24697188  0.08491803  0.11709099  1.        ]]. Action = [[-0.36782056  0.8820056   0.641523    0.6162205 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1567. State = [[-0.24510781  0.11385385  0.13420919  1.        ]]. Action = [[0.47339284 0.51579165 0.13052833 0.6483053 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1568. State = [[-0.23108913  0.13565327  0.15182522  1.        ]]. Action = [[0.33210516 0.48721433 0.85368145 0.6790426 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1569. State = [[-0.21735087  0.15382594  0.18276729  1.        ]]. Action = [[0.15768981 0.31778502 0.9177451  0.8470919 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1570. State = [[-0.20969935  0.16700883  0.21370019  1.        ]]. Action = [[0.09039319 0.29339588 0.5435071  0.7518897 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1571. State = [[-0.19568926  0.17956522  0.23847443  1.        ]]. Action = [[0.7253181 0.3645184 0.7448255 0.7019341]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1572. State = [[-0.17779154  0.1823745   0.26874962  1.        ]]. Action = [[-0.14658362 -0.408386    0.668226    0.79013157]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1573. State = [[-0.17451638  0.16370583  0.29119936  1.        ]]. Action = [[ 0.18137765 -0.81758535  0.4187132   0.5556395 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1574. State = [[-0.17076041  0.16055639  0.30187467  1.        ]]. Action = [[ 0.12758088  0.83794403 -0.19461524  0.57941175]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1575. State = [[-0.17028521  0.15989546  0.3100154   1.        ]]. Action = [[-0.43562436 -0.85306257  0.35227525  0.70408416]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1576. State = [[-0.17798056  0.16063495  0.31739077  1.        ]]. Action = [[-0.17674994  0.676721    0.27049434  0.79331887]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1577. State = [[-0.18983896  0.18156141  0.31565276  1.        ]]. Action = [[-0.0136351   0.776083   -0.68987155  0.6789875 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1578. State = [[-0.1896964   0.18048732  0.31232646  1.        ]]. Action = [[ 0.19666266 -0.8773553   0.26481056  0.6079558 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1579. State = [[-0.18615228  0.16028814  0.3244719   1.        ]]. Action = [[-0.3977629  -0.68823206  0.63154507  0.73775506]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1580. State = [[-0.19207591  0.13239193  0.33647162  1.        ]]. Action = [[-0.46709573 -0.9256355  -0.04105246  0.6682534 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1581. State = [[-0.20048274  0.12525748  0.33656576  1.        ]]. Action = [[ 0.1549573   0.791904   -0.3383481   0.66464114]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1582. State = [[-0.20276545  0.12749013  0.3406194   1.        ]]. Action = [[-0.12024903 -0.47372985  0.773587    0.59421074]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1583. State = [[-0.20700279  0.11250321  0.35368913  1.        ]]. Action = [[-0.4290384  -0.6139272   0.08866847  0.633827  ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1584. State = [[-0.2529008  -0.09097535  0.10474017  1.        ]]. Action = [[ 0.93223333  0.9670247  -0.6255411  -0.04504985]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1585. State = [[-0.24487065 -0.11453681  0.09570848  1.        ]]. Action = [[ 0.6020237  -0.85244334  0.5838765   0.49811172]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1586. State = [[-0.23168947 -0.1257006   0.10873383  1.        ]]. Action = [[0.28177893 0.43888974 0.8046578  0.70739603]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1587. State = [[-0.22054161 -0.12021098  0.13594961  1.        ]]. Action = [[0.24679387 0.30334127 0.76243377 0.7163098 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1588. State = [[-0.20506321 -0.1228672   0.16174917  1.        ]]. Action = [[ 0.62718415 -0.6068785   0.5230937   0.77437854]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1589. State = [[-0.17793837 -0.14021014  0.1885261   1.        ]]. Action = [[ 0.62835336 -0.62606597  0.8180108   0.5937846 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1590. State = [[-0.15960644 -0.15795167  0.21203366  1.        ]]. Action = [[ 0.11668456 -0.3013891   0.3744676   0.6723938 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1591. State = [[-0.14462548 -0.17565206  0.23243737  1.        ]]. Action = [[ 0.51504636 -0.489089    0.6596348   0.70509696]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1592. State = [[-0.12227265 -0.17470919  0.25987688  1.        ]]. Action = [[0.8105693  0.78388715 0.5936136  0.6943767 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1593. State = [[-0.10407443 -0.16552866  0.2795206   1.        ]]. Action = [[ 0.8447672   0.65704    -0.6159791   0.65732574]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 1594. State = [[-0.10101237 -0.15818469  0.27468848  1.        ]]. Action = [[ 0.00662506  0.33010602 -0.8273473   0.6032355 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1595. State = [[-0.0943857  -0.15043712  0.26646748  1.        ]]. Action = [[0.64265275 0.02123451 0.18760109 0.6862531 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1596. State = [[-0.07961953 -0.15075944  0.26729473  1.        ]]. Action = [[ 0.10820723 -0.104684    0.11093211  0.7339406 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1597. State = [[-0.06848856 -0.13875037  0.26184124  1.        ]]. Action = [[ 0.7885233   0.66347253 -0.46904123  0.8216114 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1598. State = [[-0.04285388 -0.14002393  0.25462186  1.        ]]. Action = [[ 0.52534723 -0.7543003   0.15249443  0.90840816]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1599. State = [[-0.02247356 -0.16023436  0.26732978  1.        ]]. Action = [[ 0.47304654 -0.6975087   0.7316816   0.60327506]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1600. State = [[-9.9862029e-04 -1.6233201e-01  2.8660268e-01  1.0000000e+00]]. Action = [[0.80301094 0.75375736 0.1751517  0.59212446]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1601. State = [[ 0.01347435 -0.16412467  0.2802646   1.        ]]. Action = [[-0.24336255 -0.63288116 -0.7645561   0.4739324 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1602. State = [[ 0.01738502 -0.15884787  0.258839    1.        ]]. Action = [[ 0.2198404   0.81468296 -0.5599736   0.7660279 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1603. State = [[ 0.02275566 -0.14020471  0.24165566  1.        ]]. Action = [[-0.8732511   0.5738158  -0.32675576  0.78552437]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1604. State = [[ 0.02070719 -0.11954983  0.24463351  1.        ]]. Action = [[-0.15850282  0.5354793   0.6897466   0.7439482 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1604 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1604 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1604 of 1
Current timestep = 1605. State = [[ 0.01311242 -0.11820496  0.26518318  1.        ]]. Action = [[-0.3295036  -0.7869405   0.77873564  0.624568  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1605 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1605 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1605 of -1
Current timestep = 1606. State = [[-7.7471236e-04 -1.2754980e-01  2.8405863e-01  1.0000000e+00]]. Action = [[-7.2300452e-01  4.2202091e-01 -6.3467026e-04  9.0171552e-01]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1607. State = [[-0.02168203 -0.11312585  0.3017398   1.        ]]. Action = [[-0.40044427  0.42846215  0.8850496   0.8612797 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1608. State = [[-0.03818984 -0.09830472  0.32428512  1.        ]]. Action = [[0.07036519 0.23917961 0.2787043  0.53618026]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1609. State = [[-0.03895871 -0.08654835  0.32186878  1.        ]]. Action = [[ 0.39828253  0.34370458 -0.7938638   0.7975694 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1610. State = [[-0.03977388 -0.06854281  0.31843263  1.        ]]. Action = [[-0.2603451  0.727746   0.6580496  0.6852523]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1611. State = [[-0.04032665 -0.05442976  0.3183565   1.        ]]. Action = [[ 0.35989022 -0.21458858 -0.5469297   0.77349067]]. Reward = [0.]
Curr episode timestep = 26
Above hoop
Current timestep = 1612. State = [[-0.03360363 -0.04696034  0.32503593  1.        ]]. Action = [[0.50053024 0.5400044  0.9333308  0.72246957]]. Reward = [0.]
Curr episode timestep = 27
Above hoop
Current timestep = 1613. State = [[-0.02845175 -0.0468922   0.3497723   1.        ]]. Action = [[-0.37164432 -0.530433    0.94928     0.7525804 ]]. Reward = [0.]
Curr episode timestep = 28
Above hoop
Current timestep = 1614. State = [[-0.03070539 -0.05310582  0.35978487  1.        ]]. Action = [[ 0.12053037 -0.05926692 -0.9717458   0.8962085 ]]. Reward = [0.]
Curr episode timestep = 29
Above hoop
Current timestep = 1615. State = [[-0.03394767 -0.05567371  0.35663623  1.        ]]. Action = [[-0.896094    0.01635087  0.45276642  0.86061764]]. Reward = [0.]
Curr episode timestep = 30
Above hoop
Current timestep = 1616. State = [[-0.04118339 -0.04578903  0.37103447  1.        ]]. Action = [[0.44382906 0.6171113  0.7305378  0.7326689 ]]. Reward = [0.]
Curr episode timestep = 31
Above hoop
Current timestep = 1617. State = [[-0.03420229 -0.0357119   0.38999924  1.        ]]. Action = [[ 0.9748627  -0.12324882  0.5143957   0.7591057 ]]. Reward = [0.]
Curr episode timestep = 32
Above hoop
Current timestep = 1618. State = [[-0.02276961 -0.03174707  0.39392453  1.        ]]. Action = [[ 0.25242066  0.16939032 -0.87376434  0.44017887]]. Reward = [0.]
Curr episode timestep = 33
Above hoop
Current timestep = 1619. State = [[-7.0889504e-04 -3.0635117e-02  3.8132158e-01  1.0000000e+00]]. Action = [[ 0.88458824 -0.10365629  0.3038125   0.834785  ]]. Reward = [0.]
Curr episode timestep = 34
Above hoop
Current timestep = 1620. State = [[ 0.01547348 -0.03277118  0.38614923  1.        ]]. Action = [[-0.3748486  -0.09409535  0.16022682  0.26497054]]. Reward = [0.]
Curr episode timestep = 35
Above hoop
Current timestep = 1621. State = [[ 0.0151577  -0.03455684  0.386815    1.        ]]. Action = [[ 0.4461434  -0.64838934  0.9436202   0.87420416]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Above hoop
Current timestep = 1622. State = [[ 0.01503757 -0.03529884  0.38708663  1.        ]]. Action = [[0.8685725  0.10851574 0.574257   0.3452654 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Above hoop
Current timestep = 1623. State = [[ 0.01498215 -0.0356702   0.38720107  1.        ]]. Action = [[-0.54751086 -0.5305058   0.7489884   0.79905605]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Above hoop
Current timestep = 1624. State = [[ 0.00987977 -0.04677156  0.3818074   1.        ]]. Action = [[-0.57697314 -0.673348   -0.7366976   0.45081866]]. Reward = [0.]
Curr episode timestep = 39
Above hoop
Current timestep = 1625. State = [[ 0.00355562 -0.05282478  0.36734     1.        ]]. Action = [[ 0.05916023  0.2721696  -0.8986987   0.43118405]]. Reward = [0.]
Curr episode timestep = 40
Above hoop
Current timestep = 1626. State = [[ 0.01085923 -0.06396266  0.3380731   1.        ]]. Action = [[ 0.66265726 -0.8159171  -0.40484798  0.6623533 ]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Current timestep = 1627. State = [[ 0.02277146 -0.06807711  0.31018904  1.        ]]. Action = [[ 0.63777196  0.6613357  -0.9454345   0.8493049 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1628. State = [[ 0.0346642  -0.06597734  0.28760427  1.        ]]. Action = [[-0.8131554  -0.2684809   0.22593844  0.8274672 ]]. Reward = [0.]
Curr episode timestep = 43
Above hoop
Current timestep = 1629. State = [[ 0.03637296 -0.06995234  0.28349409  1.        ]]. Action = [[ 0.77473044 -0.13080919 -0.26108098  0.72176623]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1630. State = [[ 0.03993912 -0.07599565  0.28526008  1.        ]]. Action = [[-0.67623436 -0.215208    0.5321773   0.7250296 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1631. State = [[ 0.03881634 -0.08918919  0.285099    1.        ]]. Action = [[ 0.22998023 -0.46110916 -0.54936093  0.68324125]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1632. State = [[ 0.04396814 -0.08745841  0.28059465  1.        ]]. Action = [[0.65234184 0.6365781  0.11227763 0.6806135 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1633. State = [[ 0.04941165 -0.08925791  0.2830204   1.        ]]. Action = [[-0.80374354 -0.5654615   0.2032913   0.3885715 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1634. State = [[ 0.05107901 -0.10839397  0.28147435  1.        ]]. Action = [[ 0.8348789  -0.79818606 -0.3350876   0.6798974 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1635. State = [[ 0.06210872 -0.1313352   0.27261     1.        ]]. Action = [[ 0.76101196 -0.3983842  -0.16277826  0.5732913 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1635 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 1635 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1635 of -1
Current timestep = 1636. State = [[ 0.07994217 -0.14145802  0.26528123  1.        ]]. Action = [[-0.269392   -0.10967207 -0.94385594  0.7908809 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 1637. State = [[ 0.07607143 -0.15393594  0.259003    1.        ]]. Action = [[-0.21510017 -0.95291    -0.7841485   0.4552704 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1638. State = [[ 0.07640263 -0.17613828  0.2399111   1.        ]]. Action = [[ 0.1761359   0.13304281 -0.592164    0.48694968]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 1639. State = [[ 0.07644419 -0.17908996  0.23783977  1.        ]]. Action = [[-0.00593632  0.81442475 -0.8857101   0.38304162]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 1640. State = [[ 0.0764525  -0.17919573  0.23781358  1.        ]]. Action = [[ 0.49074006 -0.50687796  0.48258567  0.83816385]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 1641. State = [[ 0.07644273 -0.17919552  0.23774146  1.        ]]. Action = [[ 0.78347135 -0.741493    0.66178334  0.72443926]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 1642. State = [[ 0.07644273 -0.17919552  0.23774146  1.        ]]. Action = [[ 0.23307312  0.24107945 -0.463722    0.574085  ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 1643. State = [[ 0.07709806 -0.18002726  0.23093736  1.        ]]. Action = [[ 0.12859857 -0.06514788 -0.85310423  0.61118424]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1644. State = [[ 0.08104083 -0.18171437  0.2066193   1.        ]]. Action = [[-0.14063346  0.00993526 -0.07302535  0.89024115]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1645. State = [[ 0.08210991 -0.18413922  0.21481499  1.        ]]. Action = [[-0.34053814  0.06116462  0.8264301   0.7476423 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1646. State = [[ 0.08092258 -0.19866483  0.23623236  1.        ]]. Action = [[-0.37512928 -0.7466229   0.6805217   0.68158937]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1647. State = [[ 0.07006232 -0.21106303  0.2435067   1.        ]]. Action = [[-0.27909476  0.10158217 -0.7110181   0.53323555]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1648. State = [[ 0.05894664 -0.22456707  0.24030125  1.        ]]. Action = [[-0.34630632 -0.7411898   0.2390703   0.7821201 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1649. State = [[ 0.04478928 -0.24127741  0.24495324  1.        ]]. Action = [[-0.5898214  -0.12777323  0.22190809  0.6063092 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1650. State = [[ 0.03299347 -0.2603747   0.25324452  1.        ]]. Action = [[ 0.39648187 -0.83729017  0.13184631  0.13339412]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1651. State = [[ 0.02593806 -0.28226498  0.25765815  1.        ]]. Action = [[-0.6241816  -0.34753644  0.05635774  0.6303544 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1652. State = [[ 0.0059472  -0.30132058  0.2593972   1.        ]]. Action = [[-0.91231704 -0.39612824 -0.10753793  0.37778902]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1653. State = [[-0.00603688 -0.30437705  0.25837103  1.        ]]. Action = [[0.51411855 0.5020313  0.04480124 0.6529844 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1654. State = [[-0.00442005 -0.29749224  0.25845325  1.        ]]. Action = [[ 0.8335464  -0.2876376   0.57248855  0.79076004]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 1655. State = [[-0.00414786 -0.29640788  0.2584932   1.        ]]. Action = [[-0.29807514 -0.9799869   0.8093512   0.8036089 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 1656. State = [[-0.00417226 -0.2962741   0.2584525   1.        ]]. Action = [[ 0.67980313 -0.96228594  0.7226559   0.5199647 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 1657. State = [[ 2.2447921e-05 -2.8395247e-01  2.4756730e-01  1.0000000e+00]]. Action = [[ 0.59792745  0.6346551  -0.9044759   0.8084675 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1658. State = [[ 0.00853726 -0.25934032  0.23902552  1.        ]]. Action = [[-0.32916093  0.8624363   0.5761993   0.61556745]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1659. State = [[ 0.00487527 -0.23999213  0.25322467  1.        ]]. Action = [[-0.3929584   0.03899896  0.635501    0.14003706]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1660. State = [[ 0.00438959 -0.21983941  0.25982237  1.        ]]. Action = [[ 0.9000671  0.7939733 -0.6594503  0.3700019]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1661. State = [[ 0.00995658 -0.20297086  0.2500712   1.        ]]. Action = [[ 0.5928652   0.95626307 -0.9162143   0.86643887]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 1662. State = [[ 0.02142555 -0.19005412  0.25191987  1.        ]]. Action = [[0.83105016 0.5703013  0.3175876  0.6616578 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1663. State = [[ 0.04168018 -0.1762604   0.26311597  1.        ]]. Action = [[0.65344656 0.05103195 0.62536263 0.67725885]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1664. State = [[-0.26378208 -0.01330402  0.1006885   1.        ]]. Action = [[ 0.6889286   0.12446499  0.93128896 -0.08352506]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1665. State = [[-0.26523802 -0.01639022  0.08873778  1.        ]]. Action = [[-0.41168594  0.31487203  0.19860697  0.78857255]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1666. State = [[-0.26523802 -0.01639022  0.08873778  1.        ]]. Action = [[-0.40169764 -0.16974038  0.8025732   0.37960672]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 1667. State = [[-0.2652435  -0.01645089  0.08874307  1.        ]]. Action = [[-0.28889793  0.21649587  0.5205648   0.57510066]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1668. State = [[-0.2652435  -0.01645089  0.08874307  1.        ]]. Action = [[-0.25999773  0.87667704  0.37739992  0.6294353 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 1669. State = [[-0.26020345 -0.0139405   0.09585629  1.        ]]. Action = [[0.34379375 0.32742798 0.945982   0.4300109 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1670. State = [[-0.2448748  -0.02392484  0.11764532  1.        ]]. Action = [[ 0.6839206 -0.8151099  0.9237524  0.0886631]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1671. State = [[-0.22952802 -0.04149329  0.15223965  1.        ]]. Action = [[-0.13314718 -0.400787    0.95099115  0.6924249 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1672. State = [[-0.22966042 -0.04691296  0.1868636   1.        ]]. Action = [[-0.2963624   0.3105544   0.7731774   0.43690443]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1673. State = [[-0.22919533 -0.03239556  0.21909644  1.        ]]. Action = [[0.26071   0.8383206 0.9409354 0.8437345]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1674. State = [[-0.22953428 -0.01544495  0.24913745  1.        ]]. Action = [[-0.16616184  0.02120101  0.43926227  0.5196154 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1675. State = [[-0.22918405 -0.00573349  0.27287558  1.        ]]. Action = [[0.07539296 0.34822142 0.66400075 0.5488837 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1676. State = [[-0.22214285 -0.00346617  0.28902745  1.        ]]. Action = [[ 0.4560156  -0.35435212 -0.2016784   0.8586824 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1677. State = [[-0.20824878  0.00520269  0.2934018   1.        ]]. Action = [[0.7861481  0.78696156 0.03630137 0.76680255]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1678. State = [[-0.18060549  0.0160589   0.30386794  1.        ]]. Action = [[0.646379   0.11189651 0.6252253  0.5123255 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1679. State = [[-0.1590431   0.03599109  0.32658735  1.        ]]. Action = [[0.235497   0.9005666  0.5908729  0.65312564]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1680. State = [[-0.15172657  0.05410989  0.3381204   1.        ]]. Action = [[ 0.01100433  0.08437216 -0.33397818  0.2560475 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1681. State = [[-0.1438088   0.06542066  0.3422701   1.        ]]. Action = [[0.3115101  0.38139546 0.6317332  0.71866846]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1682. State = [[-0.13511732  0.06540275  0.34588215  1.        ]]. Action = [[ 0.29849005 -0.53222615 -0.60159117  0.8369777 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1683. State = [[-0.11650769  0.0485912   0.33830306  1.        ]]. Action = [[ 0.6796303  -0.7098314   0.07387662  0.6041211 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1684. State = [[-0.09338022  0.03283831  0.34684032  1.        ]]. Action = [[ 0.63181436 -0.17462313  0.6030456   0.6750493 ]]. Reward = [0.]
Curr episode timestep = 19
Above hoop
Current timestep = 1685. State = [[-0.07796995  0.02015524  0.35353398  1.        ]]. Action = [[-0.01730728 -0.43921    -0.341174    0.5675583 ]]. Reward = [0.]
Curr episode timestep = 20
Above hoop
Current timestep = 1686. State = [[-0.0802789   0.00351007  0.34480795  1.        ]]. Action = [[-0.8028867  -0.54267466 -0.6504513   0.4708879 ]]. Reward = [0.]
Curr episode timestep = 21
Above hoop
Current timestep = 1687. State = [[-0.08973152 -0.02149111  0.32721934  1.        ]]. Action = [[-0.12114054 -0.807066   -0.75077784  0.44623554]]. Reward = [0.]
Curr episode timestep = 22
Above hoop
Current timestep = 1688. State = [[-0.09035063 -0.0276373   0.3135856   1.        ]]. Action = [[0.33446872 0.6898854  0.27089024 0.6258609 ]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Current timestep = 1689. State = [[-0.08835239 -0.02143665  0.30726877  1.        ]]. Action = [[ 0.30652404  0.01053596 -0.40044367  0.21155035]]. Reward = [0.]
Curr episode timestep = 24
Above hoop
Current timestep = 1690. State = [[-0.07235834 -0.01103739  0.30759025  1.        ]]. Action = [[0.8240912 0.5431011 0.6941284 0.5619278]]. Reward = [0.]
Curr episode timestep = 25
Above hoop
Current timestep = 1691. State = [[-0.05686307 -0.00355651  0.30712247  1.        ]]. Action = [[ 0.51527476 -0.13157624 -0.78275985  0.54696536]]. Reward = [0.]
Curr episode timestep = 26
Above hoop
Current timestep = 1692. State = [[-0.04170327  0.00185292  0.30051017  1.        ]]. Action = [[-0.48697114  0.25808012  0.7426692   0.68120027]]. Reward = [0.]
Curr episode timestep = 27
Above hoop
Scene graph at timestep 1692 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1692 is tensor(9.2394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1692 of 1
Current timestep = 1693. State = [[-0.04144037 -0.00425563  0.29910704  1.        ]]. Action = [[ 0.26594448 -0.64979625 -0.72255623  0.67560923]]. Reward = [0.]
Curr episode timestep = 28
Above hoop
Scene graph at timestep 1693 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1693 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1693 of -1
Current timestep = 1694. State = [[-0.04534438 -0.00266075  0.28865308  1.        ]]. Action = [[-0.9002438   0.7498207  -0.32835162  0.15570354]]. Reward = [0.]
Curr episode timestep = 29
Above hoop
Scene graph at timestep 1694 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 1694 is tensor(4.1585e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1694 of -1
Current timestep = 1695. State = [[-0.06161067 -0.003338    0.29133248  1.        ]]. Action = [[-0.82452184 -0.9361218   0.59364486  0.6060622 ]]. Reward = [0.]
Curr episode timestep = 30
Above hoop
Current timestep = 1696. State = [[-0.07440966 -0.01604469  0.29127017  1.        ]]. Action = [[ 0.8795204   0.2653103  -0.84025407  0.64850974]]. Reward = [0.]
Curr episode timestep = 31
Above hoop
Current timestep = 1697. State = [[-0.06339215 -0.02602778  0.27122417  1.        ]]. Action = [[ 0.60778356 -0.7647302  -0.21977705  0.65515137]]. Reward = [0.]
Curr episode timestep = 32
Above hoop
Current timestep = 1698. State = [[-0.05345026 -0.05083001  0.263314    1.        ]]. Action = [[-0.06726253 -0.68213755 -0.02723384  0.51364136]]. Reward = [0.]
Curr episode timestep = 33
Above hoop
Current timestep = 1699. State = [[-0.05078347 -0.07010669  0.2691698   1.        ]]. Action = [[-0.25175774 -0.2180078   0.68532574  0.92061794]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1700. State = [[-0.04710321 -0.08797651  0.28523493  1.        ]]. Action = [[ 0.401415   -0.64554524  0.6609645   0.4709506 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1701. State = [[-0.03508646 -0.10513867  0.29749143  1.        ]]. Action = [[ 0.8536179  -0.31230175 -0.24100363  0.48342252]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1702. State = [[-0.01827699 -0.10811632  0.30941474  1.        ]]. Action = [[0.03036129 0.39747274 0.848227   0.8362839 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1703. State = [[-0.00645688 -0.09637344  0.33903587  1.        ]]. Action = [[0.18959355 0.48913503 0.95119715 0.810488  ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1704. State = [[ 0.00471931 -0.09423506  0.3654994   1.        ]]. Action = [[ 0.41827714 -0.4779247   0.24940276  0.66730726]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1705. State = [[ 0.00780679 -0.10572978  0.36748412  1.        ]]. Action = [[-0.45889926 -0.4441992  -0.9763145   0.07422781]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1706. State = [[ 0.00152002 -0.11788811  0.3534334   1.        ]]. Action = [[-0.7834399  -0.06816798 -0.11157501  0.77533793]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1707. State = [[-0.01010424 -0.12315225  0.33979368  1.        ]]. Action = [[-0.01978523  0.07794046 -0.6641091   0.8660548 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1708. State = [[-0.01851756 -0.12723292  0.32047728  1.        ]]. Action = [[-0.17133123 -0.27432734 -0.49301952  0.80158305]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1709. State = [[-0.02034098 -0.11893583  0.30445185  1.        ]]. Action = [[ 0.6657096   0.7029321  -0.1823039   0.82550097]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1710. State = [[-0.01671708 -0.10653564  0.30007014  1.        ]]. Action = [[-0.566982    0.17518473  0.49590373  0.69616556]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1711. State = [[-0.02099594 -0.10649234  0.29484946  1.        ]]. Action = [[-0.08150661 -0.4467922  -0.93380415  0.58393335]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1712. State = [[-0.02237697 -0.11282085  0.2800947   1.        ]]. Action = [[ 0.43209183 -0.13119477 -0.14149946  0.5417025 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1713. State = [[-0.02320438 -0.11988443  0.28067657  1.        ]]. Action = [[-0.7425781  -0.21059406  0.5664005   0.5883044 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1714. State = [[-0.02647417 -0.1315935   0.29612327  1.        ]]. Action = [[ 0.42155898 -0.3327582   0.7219565   0.82456875]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1715. State = [[-0.02293643 -0.13253757  0.3093801   1.        ]]. Action = [[0.45226717 0.33858526 0.13989389 0.80704737]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1716. State = [[-0.01241682 -0.12874727  0.32127544  1.        ]]. Action = [[ 0.6014085  -0.06724143  0.41141117  0.52677345]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1717. State = [[-4.2479270e-04 -1.1533775e-01  3.3828491e-01  1.0000000e+00]]. Action = [[0.05322075 0.76608944 0.48682117 0.5959567 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1718. State = [[ 0.00750456 -0.09733146  0.34338742  1.        ]]. Action = [[ 0.4006902   0.46289003 -0.44073105  0.5901041 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1719. State = [[ 0.01324697 -0.09755547  0.3309236   1.        ]]. Action = [[-0.6640472  -0.68361723 -0.6104005   0.77870023]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1720. State = [[ 0.01427986 -0.11091375  0.3221903   1.        ]]. Action = [[ 0.91639876 -0.43446982 -0.32697403  0.44190228]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1721. State = [[ 0.02684706 -0.10950024  0.29928103  1.        ]]. Action = [[ 0.07608879  0.78072286 -0.43433118  0.5250187 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1722. State = [[ 0.03482421 -0.101248    0.28650144  1.        ]]. Action = [[-0.2091381  -0.02503794 -0.16515762  0.80593896]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1723. State = [[ 0.03602136 -0.08951538  0.27891785  1.        ]]. Action = [[ 0.23871386  0.5216824  -0.8352584   0.8736863 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1724. State = [[ 0.0359221  -0.07358234  0.24208407  1.        ]]. Action = [[-0.6727465   0.40468442 -0.88917875  0.7020744 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1725. State = [[ 0.0255359  -0.07649843  0.22008146  1.        ]]. Action = [[-0.8937973  -0.7741607  -0.06580997  0.7148719 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1726. State = [[ 0.00108765 -0.08017062  0.21811968  1.        ]]. Action = [[-0.85520357  0.5340756   0.6802244   0.7690065 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1727. State = [[-0.02451185 -0.07189459  0.23357084  1.        ]]. Action = [[ 0.05806422  0.79598236 -0.24383456  0.654307  ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 1728. State = [[-0.02497833 -0.07346967  0.24651898  1.        ]]. Action = [[ 0.345163   -0.13320255  0.8219986   0.36822248]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1729. State = [[-0.02295007 -0.07350409  0.26226     1.        ]]. Action = [[ 0.32254338 -0.29851353 -0.3934685   0.5524341 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 1730. State = [[-0.0256655  -0.08536734  0.27809504  1.        ]]. Action = [[-0.43051326 -0.55938643  0.9257984   0.55076396]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1731. State = [[-0.04299997 -0.08112609  0.3020148   1.        ]]. Action = [[-0.7239783   0.88804317  0.09719789  0.6591524 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1732. State = [[-0.05859042 -0.07762563  0.30627963  1.        ]]. Action = [[ 0.64756227 -0.8423408  -0.26637232  0.4726014 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1733. State = [[-0.06366603 -0.08756673  0.29573062  1.        ]]. Action = [[-0.6005627   0.14566123 -0.7796093   0.6288779 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1734. State = [[-0.07414649 -0.09576181  0.27903396  1.        ]]. Action = [[-0.43761313 -0.4818735  -0.10099781  0.9199264 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1735. State = [[-0.08862133 -0.1015135   0.26994964  1.        ]]. Action = [[-0.6079189   0.21038008 -0.03120667  0.830693  ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1736. State = [[-0.1015724  -0.09007569  0.2687072   1.        ]]. Action = [[0.05196726 0.8016598  0.17815709 0.7441728 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1737. State = [[-0.09783085 -0.07890212  0.2733728   1.        ]]. Action = [[ 0.7311808  -0.38895136  0.19296753  0.8518579 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1738. State = [[-0.09589581 -0.07395282  0.27603883  1.        ]]. Action = [[-0.521849    0.45014358 -0.04147708  0.8041308 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1739. State = [[-0.09600811 -0.07255952  0.2722852   1.        ]]. Action = [[ 0.45785916 -0.3007257  -0.34632158  0.7051699 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1740. State = [[-0.08812036 -0.07559915  0.28013325  1.        ]]. Action = [[ 0.56989765 -0.10642272  0.9289042   0.6001868 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1741. State = [[-0.08484013 -0.06374381  0.29450977  1.        ]]. Action = [[-0.6519915   0.92308664  0.40865815  0.6989088 ]]. Reward = [0.]
Curr episode timestep = 76
Above hoop
Current timestep = 1742. State = [[-0.09011988 -0.04027656  0.29719183  1.        ]]. Action = [[ 0.31958663  0.5322213  -0.56909925  0.7615044 ]]. Reward = [0.]
Curr episode timestep = 77
Above hoop
Current timestep = 1743. State = [[-0.09169389 -0.03241822  0.29419518  1.        ]]. Action = [[-0.00422198 -0.17153823  0.07953799  0.8231976 ]]. Reward = [0.]
Curr episode timestep = 78
Above hoop
Current timestep = 1744. State = [[-0.09264728 -0.021783    0.28718382  1.        ]]. Action = [[ 0.05351746  0.68241405 -0.48789942  0.90527606]]. Reward = [0.]
Curr episode timestep = 79
Above hoop
Current timestep = 1745. State = [[-0.09472228  0.00143554  0.27960387  1.        ]]. Action = [[-0.19686896  0.73697317  0.0191797   0.5391979 ]]. Reward = [0.]
Curr episode timestep = 80
Above hoop
Current timestep = 1746. State = [[-0.09675588  0.00694348  0.28633273  1.        ]]. Action = [[-0.2124331  -0.71071786  0.5079043   0.66406417]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1747. State = [[-0.09394016 -0.00386759  0.2825543   1.        ]]. Action = [[ 0.6261802 -0.1753152 -0.7452516  0.7505326]]. Reward = [0.]
Curr episode timestep = 82
Above hoop
Current timestep = 1748. State = [[-0.09273817  0.00270076  0.27632073  1.        ]]. Action = [[-0.646523    0.58622956  0.37507665  0.77518225]]. Reward = [0.]
Curr episode timestep = 83
Above hoop
Current timestep = 1749. State = [[-0.08988661  0.01072266  0.28220138  1.        ]]. Action = [[ 0.62102175 -0.009354    0.27638125  0.5851939 ]]. Reward = [0.]
Curr episode timestep = 84
Above hoop
Current timestep = 1750. State = [[-0.07816258  0.02501553  0.29157716  1.        ]]. Action = [[0.5713923  0.83541584 0.2529881  0.6303352 ]]. Reward = [0.]
Curr episode timestep = 85
Above hoop
Current timestep = 1751. State = [[-0.06840397  0.03278153  0.29338545  1.        ]]. Action = [[-0.03537798 -0.42303628 -0.30117822  0.34575844]]. Reward = [0.]
Curr episode timestep = 86
Above hoop
Current timestep = 1752. State = [[-0.07129441  0.01814968  0.2933449   1.        ]]. Action = [[-0.5909495  -0.8188968   0.04691708  0.693944  ]]. Reward = [0.]
Curr episode timestep = 87
Above hoop
Current timestep = 1753. State = [[-0.07485972  0.01519347  0.3046694   1.        ]]. Action = [[0.04068255 0.6243112  0.8666433  0.5309597 ]]. Reward = [0.]
Curr episode timestep = 88
Above hoop
Current timestep = 1754. State = [[-0.07337521  0.01785427  0.31006905  1.        ]]. Action = [[ 0.66064286 -0.09254736 -0.56126803  0.8315568 ]]. Reward = [0.]
Curr episode timestep = 89
Above hoop
Current timestep = 1755. State = [[-0.06347667  0.02058382  0.29521307  1.        ]]. Action = [[ 0.74232984  0.10854518 -0.8428105   0.6883607 ]]. Reward = [0.]
Curr episode timestep = 90
Above hoop
Current timestep = 1756. State = [[-0.04711456  0.02164853  0.28104872  1.        ]]. Action = [[-0.7564938  -0.01856041  0.44441748  0.78691363]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 1757. State = [[-0.05421048  0.01098535  0.29337215  1.        ]]. Action = [[-0.8551526  -0.64679277  0.5297127   0.44052887]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 1758. State = [[-0.07134435  0.00134307  0.31849837  1.        ]]. Action = [[-0.03501904  0.11859441  0.91484964  0.7636124 ]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Current timestep = 1759. State = [[-0.07804721 -0.00799565  0.3304749   1.        ]]. Action = [[ 0.18415296 -0.67923325 -0.74657017  0.7793319 ]]. Reward = [0.]
Curr episode timestep = 94
Above hoop
Current timestep = 1760. State = [[-0.08010785 -0.00697592  0.3251127   1.        ]]. Action = [[-0.22563666  0.8239572  -0.04200393  0.39568245]]. Reward = [0.]
Curr episode timestep = 95
Above hoop
Current timestep = 1761. State = [[-0.08489606  0.01550016  0.32545823  1.        ]]. Action = [[-0.14058137  0.70611477  0.31638503  0.6790309 ]]. Reward = [0.]
Curr episode timestep = 96
Above hoop
Current timestep = 1762. State = [[-0.09430252  0.04536805  0.33328706  1.        ]]. Action = [[-0.55349785  0.87906694  0.48542368  0.7355342 ]]. Reward = [0.]
Curr episode timestep = 97
Above hoop
Current timestep = 1763. State = [[-0.10851474  0.07403351  0.33593285  1.        ]]. Action = [[-0.09742218  0.52229    -0.6385364   0.8278388 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1764. State = [[-0.1148616   0.07846676  0.32512486  1.        ]]. Action = [[-0.05367744 -0.56156236 -0.43541533  0.8001727 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1765. State = [[-0.11698235  0.08285558  0.321156    1.        ]]. Action = [[0.44522202 0.78838277 0.42355466 0.49956656]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1766. State = [[-0.2705104   0.09014383  0.12421324  1.        ]]. Action = [[0.8103626  0.27834356 0.2972443  0.70865965]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1767. State = [[-0.2697346   0.10133376  0.11062056  1.        ]]. Action = [[-0.34206665  0.32602096  0.60865545  0.61763215]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1768. State = [[-0.26520172  0.11460495  0.11370078  1.        ]]. Action = [[0.35290134 0.86033976 0.46587503 0.76818347]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1769. State = [[-0.24967107  0.12377407  0.12718266  1.        ]]. Action = [[ 0.61561716 -0.56479293  0.8005693   0.46146858]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1770. State = [[-0.22694588  0.12456307  0.15240516  1.        ]]. Action = [[0.5082749 0.3899815 0.6819804 0.5670738]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1771. State = [[-0.20478256  0.142142    0.17748307  1.        ]]. Action = [[0.70705676 0.80242157 0.63594854 0.724947  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1772. State = [[-0.19528103  0.16808036  0.19403584  1.        ]]. Action = [[-0.4199903   0.52702427 -0.28820193  0.80101943]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1773. State = [[-0.19543244  0.17110927  0.20232435  1.        ]]. Action = [[ 0.2404033 -0.5722537  0.6817007  0.7199794]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1774. State = [[-0.18725857  0.17481011  0.21011767  1.        ]]. Action = [[ 0.22459555  0.6630585  -0.20142174  0.6825049 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1775. State = [[-0.18527544  0.19759853  0.21790592  1.        ]]. Action = [[-0.10797745  0.8088218   0.38058817  0.58074737]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1776. State = [[-0.18991873  0.21180724  0.23515812  1.        ]]. Action = [[-0.5537984 -0.358222   0.7136059  0.8401053]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1777. State = [[-0.19158822  0.22073601  0.2442113   1.        ]]. Action = [[ 0.63370585  0.7839985  -0.35565722  0.8221296 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1778. State = [[-0.18125899  0.2411746   0.23984034  1.        ]]. Action = [[ 0.695456   0.7729747 -0.6384756  0.8630526]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1779. State = [[-0.15205625  0.24819493  0.23177636  1.        ]]. Action = [[ 0.60243034 -0.73214394  0.63520885  0.8100306 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1780. State = [[-0.13960525  0.24038371  0.23858717  1.        ]]. Action = [[ 0.0572536  -0.07530302 -0.8759527   0.86422324]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1781. State = [[-0.13308294  0.23070447  0.21739857  1.        ]]. Action = [[-0.29389727 -0.6300176   0.01372206  0.71835804]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1782. State = [[-0.13718908  0.22812177  0.20606618  1.        ]]. Action = [[-0.14067781  0.5369277  -0.7965987   0.707466  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1783. State = [[-0.13674325  0.24396195  0.19230664  1.        ]]. Action = [[0.39149487 0.7471516  0.35707474 0.78769326]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1784. State = [[-0.12405775  0.2438496   0.19520295  1.        ]]. Action = [[ 0.3821714  -0.9590284   0.02159643  0.8202777 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1785. State = [[-0.11332232  0.21948746  0.20681328  1.        ]]. Action = [[-0.19492006 -0.87014323  0.799227    0.86401486]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1786. State = [[-0.11107778  0.21838656  0.22965875  1.        ]]. Action = [[0.01334858 0.97543645 0.84022593 0.5966594 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1787. State = [[-0.1157427   0.24380721  0.2564249   1.        ]]. Action = [[-0.1470567   0.82055426  0.5604243   0.12640655]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1788. State = [[-0.11985534  0.25087553  0.26738262  1.        ]]. Action = [[-0.5182206  -0.85883254 -0.38733947  0.9128051 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1789. State = [[-0.12858628  0.24105184  0.27613002  1.        ]]. Action = [[-0.56356925 -0.18319309  0.63235915  0.78073096]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1790. State = [[-0.14023672  0.22417027  0.3039179   1.        ]]. Action = [[-0.03290433 -0.68929327  0.848207    0.48899126]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1791. State = [[-0.15709239  0.22075385  0.31451356  1.        ]]. Action = [[-0.6408946   0.76655173 -0.7327163   0.33556688]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1792. State = [[-0.17286825  0.21665452  0.3013563   1.        ]]. Action = [[ 0.03843391 -0.9869867  -0.49807376  0.8421687 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1793. State = [[-0.16971579  0.18989855  0.30125493  1.        ]]. Action = [[ 0.42629504 -0.7617647   0.76310134  0.5520551 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1794. State = [[-0.16823249  0.1776294   0.3109831   1.        ]]. Action = [[-0.49053288  0.26330423 -0.00750959  0.8909317 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1795. State = [[-0.1684851   0.18255615  0.31214875  1.        ]]. Action = [[0.79733217 0.579074   0.1272217  0.5358777 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1796. State = [[-0.1713776   0.18101977  0.31180868  1.        ]]. Action = [[-0.9047625  -0.6838167  -0.23274899  0.4048959 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1797. State = [[-0.17103003  0.17322482  0.31487992  1.        ]]. Action = [[ 0.54421556 -0.08279604  0.5295756   0.5934191 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1798. State = [[-0.1692636   0.16850768  0.31573603  1.        ]]. Action = [[-0.15414059 -0.00248075 -0.5313505   0.6464124 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1799. State = [[-0.17663835  0.16370392  0.30781522  1.        ]]. Action = [[-0.8444964  -0.43978012 -0.45187306  0.51675415]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1800. State = [[-0.19033252  0.15526177  0.30662403  1.        ]]. Action = [[-0.3079785  -0.08414906  0.7997289   0.7743466 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1801. State = [[-0.20941372  0.1531016   0.30909917  1.        ]]. Action = [[-0.22097242  0.21359885 -0.70702994  0.77048755]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1802. State = [[-0.21152122  0.16433641  0.31126347  1.        ]]. Action = [[0.60809684 0.75935864 0.9149234  0.5389631 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1803. State = [[-0.20725079  0.18470477  0.3113809   1.        ]]. Action = [[ 0.49390268  0.69300437 -0.6574556   0.9413701 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1804. State = [[-0.19153418  0.19231144  0.29384634  1.        ]]. Action = [[ 0.7173083  -0.41520965 -0.7369048   0.7177417 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1805. State = [[-0.1624146   0.19903392  0.29032362  1.        ]]. Action = [[0.93240297 0.47151303 0.92169595 0.690127  ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1806. State = [[-0.13718215  0.20293987  0.3025561   1.        ]]. Action = [[ 0.6597173  -0.07847089 -0.10967469  0.68533015]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1807. State = [[-0.12143742  0.19732817  0.30304185  1.        ]]. Action = [[-0.68956697 -0.63207656  0.06012964  0.6063621 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1808. State = [[-0.12495434  0.18053243  0.29757822  1.        ]]. Action = [[-0.25022173 -0.57584965 -0.7280614   0.45398867]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1809. State = [[-0.12284467  0.16805746  0.29595175  1.        ]]. Action = [[0.67779183 0.03284442 0.7258456  0.85465467]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1810. State = [[-0.12212469  0.17300506  0.2933648   1.        ]]. Action = [[-0.229729   0.6707231 -0.5681391  0.7508323]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1811. State = [[-0.1223959   0.17762019  0.2965896   1.        ]]. Action = [[-0.311449   -0.33027565  0.68133736  0.7988732 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1812. State = [[-0.11906371  0.16773155  0.30627882  1.        ]]. Action = [[ 0.19669688 -0.6180211  -0.06426871  0.8436141 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1813. State = [[-0.11253107  0.14500716  0.32265785  1.        ]]. Action = [[ 0.40984368 -0.6867702   0.9650285   0.62425697]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1814. State = [[-0.11419644  0.12485272  0.34875694  1.        ]]. Action = [[-0.70999116 -0.29745865  0.47184062  0.8065088 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1815. State = [[-0.12495691  0.10766599  0.35383362  1.        ]]. Action = [[-0.39901948 -0.6323331  -0.9440477   0.86193943]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1816. State = [[-0.1333857   0.09316326  0.3283832   1.        ]]. Action = [[ 0.57613516  0.03888154 -0.8386223   0.73958266]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1817. State = [[-0.11779002  0.08038762  0.31029913  1.        ]]. Action = [[ 0.90281534 -0.7810401  -0.05625516  0.21555603]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1818. State = [[-0.10909479  0.05879977  0.2916482   1.        ]]. Action = [[-0.7116236 -0.1370014 -0.7689813  0.8521038]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1819. State = [[-0.1048141   0.06027522  0.28889576  1.        ]]. Action = [[0.8414371  0.5176238  0.8976377  0.47241747]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1820. State = [[-0.09314976  0.07920389  0.303967    1.        ]]. Action = [[0.16997111 0.79476    0.54512095 0.88966036]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1821. State = [[-0.07878567  0.08429549  0.32954732  1.        ]]. Action = [[ 0.72078454 -0.65858424  0.9536189   0.83830786]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1822. State = [[-0.0690355   0.06436335  0.33831048  1.        ]]. Action = [[-0.3408488 -0.8725946 -0.9741566  0.5405314]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Current timestep = 1823. State = [[-0.05896914  0.03986966  0.3252036   1.        ]]. Action = [[ 0.73961914 -0.43197376 -0.0724076   0.6305194 ]]. Reward = [0.]
Curr episode timestep = 56
Above hoop
Current timestep = 1824. State = [[-0.05473471  0.0117751   0.33254376  1.        ]]. Action = [[-0.8901469  -0.97578233  0.8499429   0.6668706 ]]. Reward = [0.]
Curr episode timestep = 57
Above hoop
Current timestep = 1825. State = [[-0.05835601  0.0032077   0.35749525  1.        ]]. Action = [[0.4480945  0.60864246 0.8241559  0.62001824]]. Reward = [0.]
Curr episode timestep = 58
Above hoop
Current timestep = 1826. State = [[-0.2589797  -0.10973042  0.09968692  1.        ]]. Action = [[ 0.32641912 -0.0504694  -0.8742166  -0.05705422]]. Reward = [1000.]
Curr episode timestep = 59
Above hoop
Current timestep = 1827. State = [[-0.2579195  -0.13270816  0.08678667  1.        ]]. Action = [[ 0.18884933 -0.67157036  0.13416982  0.56387424]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1828. State = [[-0.24970296 -0.15834366  0.09364076  1.        ]]. Action = [[ 0.41050673 -0.733642    0.746037    0.31888747]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1829. State = [[-0.23558177 -0.16954225  0.10969197  1.        ]]. Action = [[0.69503236 0.363559   0.567163   0.75149214]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1830. State = [[-0.20939048 -0.15505747  0.1329483   1.        ]]. Action = [[0.7606313  0.97642064 0.68810725 0.71609473]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1831. State = [[-0.1847268  -0.12347158  0.15036732  1.        ]]. Action = [[ 0.36761665  0.80963993 -0.08010834  0.87245846]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1832. State = [[-0.1707036  -0.09548661  0.16446355  1.        ]]. Action = [[0.25762272 0.6213615  0.8740567  0.7822404 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1833. State = [[-0.15990375 -0.0796005   0.18007517  1.        ]]. Action = [[ 0.53294015 -0.44338012 -0.49472952  0.50128853]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 1834. State = [[-0.15948212 -0.0889394   0.19194041  1.        ]]. Action = [[-0.516546  -0.6853652  0.5815326  0.3757013]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1835. State = [[-0.164016   -0.10049888  0.20558503  1.        ]]. Action = [[ 0.67349327 -0.40964746 -0.43869185  0.63271046]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 1836. State = [[-0.1649091  -0.10326727  0.20895389  1.        ]]. Action = [[-0.04480058  0.42979062  0.65146804  0.58472776]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 1837. State = [[-0.26356885  0.16522266  0.11339075  1.        ]]. Action = [[ 0.35773754  0.79437065  0.8195689  -0.27048337]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1838. State = [[-0.25682047  0.19688019  0.10205376  1.        ]]. Action = [[0.25766027 0.75491476 0.525123   0.3217851 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1839. State = [[-0.24658638  0.1969581   0.10792962  1.        ]]. Action = [[ 0.3388753  -0.96670127  0.151613    0.6362686 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1840. State = [[-0.23106942  0.19186032  0.11916818  1.        ]]. Action = [[0.59689116 0.6915393  0.59140205 0.3312819 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1841. State = [[-0.20352796  0.18715385  0.12832509  1.        ]]. Action = [[ 0.90959764 -0.9633195  -0.44843596  0.46161866]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1842. State = [[-0.18841793  0.1739144   0.12739554  1.        ]]. Action = [[ 0.71647334 -0.6678207  -0.23241216  0.6648698 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 1843. State = [[-0.17325729  0.17200157  0.1339882   1.        ]]. Action = [[0.8071009  0.03020191 0.51528    0.5446458 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1844. State = [[-0.15494247  0.1733617   0.14428417  1.        ]]. Action = [[ 0.23382127 -0.5893981  -0.6243747   0.16726828]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 1845. State = [[-0.14839633  0.17279436  0.14396009  1.        ]]. Action = [[ 0.3492508  -0.01198804 -0.19112414  0.530534  ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1846. State = [[-0.1397378   0.17250796  0.13895081  1.        ]]. Action = [[ 0.6594181  -0.64269775  0.18661141  0.33816195]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 1847. State = [[-0.13711365  0.17586784  0.13598342  1.        ]]. Action = [[ 0.39611673  0.4191271  -0.58402836  0.29221487]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1848. State = [[-0.1233473   0.1826406   0.11829748  1.        ]]. Action = [[ 0.41110742 -0.85823876 -0.671733    0.49678266]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 1849. State = [[-0.12054842  0.18410516  0.11731316  1.        ]]. Action = [[ 0.70497274 -0.56787574  0.6699891  -0.11370063]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 1850. State = [[-0.12054842  0.18410516  0.11731316  1.        ]]. Action = [[ 0.86353004 -0.7645692   0.20862901  0.58978057]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 1851. State = [[-0.10942385  0.17722745  0.12212034  1.        ]]. Action = [[ 0.8544239  -0.46258998  0.30092907  0.21089911]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1852. State = [[-0.0918256   0.17342876  0.12681602  1.        ]]. Action = [[ 0.74435735 -0.6660715  -0.50738513  0.2830335 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 1853. State = [[-0.08953366  0.17171045  0.12854281  1.        ]]. Action = [[ 0.5229354  -0.4316364  -0.73316383  0.52053905]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 1854. State = [[-0.08932418  0.17101453  0.12867743  1.        ]]. Action = [[ 0.41601157 -0.8304281  -0.85830396  0.39165998]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 1855. State = [[-0.08387501  0.1697659   0.12672572  1.        ]]. Action = [[ 0.75925076  0.10267854 -0.6648315   0.14635551]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1856. State = [[-0.06459349  0.17348434  0.10449771  1.        ]]. Action = [[ 0.6138326 -0.3015995 -0.5469315  0.3130101]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 1857. State = [[-0.05669626  0.17571951  0.10080858  1.        ]]. Action = [[ 0.6811485  -0.31310368 -0.78921825  0.21001077]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 1858. State = [[-0.05625556  0.17562993  0.10043497  1.        ]]. Action = [[ 0.19301629 -0.49381542 -0.7698288   0.15194511]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 1859. State = [[-0.05628137  0.17533328  0.10016786  1.        ]]. Action = [[ 0.1140089  -0.500384   -0.79606384  0.10470998]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 1860. State = [[-0.05629876  0.17531201  0.10014962  1.        ]]. Action = [[ 0.02623522 -0.57520115 -0.92402166  0.06378663]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 1861. State = [[-0.05629876  0.17531201  0.10014962  1.        ]]. Action = [[-0.02936745 -0.7308089  -0.8487853  -0.24869674]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 1862. State = [[-0.05629876  0.17531201  0.10014962  1.        ]]. Action = [[ 0.1592344  -0.69001013 -0.8459032  -0.11378258]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 1863. State = [[-0.05629876  0.17531201  0.10014962  1.        ]]. Action = [[-0.6178057  -0.54667646 -0.6785682  -0.06140774]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 1864. State = [[-0.05629876  0.17531201  0.10014962  1.        ]]. Action = [[ 0.14822483 -0.5480696  -0.98202854  0.06085861]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 1865. State = [[-0.26771268  0.13096699  0.11539371  1.        ]]. Action = [[ 0.10125399 -0.28635985 -0.725585   -0.18406808]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1866. State = [[-0.2641505   0.14170355  0.09514032  1.        ]]. Action = [[ 0.25802326 -0.2979511  -0.8156792   0.00630796]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1867. State = [[-0.2555409   0.1394122   0.06412241  1.        ]]. Action = [[ 0.37787187 -0.0068571  -0.8237539   0.06630003]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1868. State = [[-0.24500228  0.13778579  0.04162394  1.        ]]. Action = [[ 0.6004143  -0.19397646 -0.768254   -0.181175  ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1869. State = [[-0.24404822  0.1368827   0.03881247  1.        ]]. Action = [[ 0.66431665 -0.3147446  -0.7872252  -0.01000345]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 1870. State = [[-0.24401386  0.13660012  0.03878033  1.        ]]. Action = [[ 0.3506658  -0.42174035 -0.7207055   0.02611434]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 1871. State = [[-0.24379624  0.13665923  0.03882571  1.        ]]. Action = [[ 0.34855354 -0.34527957 -0.24244153 -0.03950524]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 1872. State = [[-0.24379624  0.13665923  0.03882571  1.        ]]. Action = [[ 5.66899538e-01 -1.34109855e-02 -7.64661551e-01 -5.71668148e-04]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 1873. State = [[-0.24379624  0.13665923  0.03882571  1.        ]]. Action = [[ 0.40445137 -0.34243453 -0.8963959  -0.04740846]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 1874. State = [[-0.24379624  0.13665923  0.03882571  1.        ]]. Action = [[ 0.59342456 -0.17316341 -0.8300585  -0.07371968]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 1875. State = [[-0.24357912  0.1367182   0.03887108  1.        ]]. Action = [[ 0.61189556 -0.286749   -0.84655076  0.02834976]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 1876. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.5855682  -0.03488463 -0.6261067   0.11366594]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1877. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.7132238  -0.2357294  -0.6472209   0.03543234]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 1878. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.5083089  -0.23810732 -0.6831559   0.00608277]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 1879. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.54874015 -0.02182347 -0.88616574 -0.01603383]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 1880. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.49727428 -0.2861923  -0.6754888  -0.08971256]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 1881. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.6830652  -0.0280475  -0.6800079  -0.05575705]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1882. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.15167654 -0.17007637 -0.8741789  -0.08444619]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1883. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.55223536 -0.42140722 -0.94232196  0.04813313]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 1884. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.375386    0.12039089 -0.8581496   0.03228509]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1885. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.35031176 -0.3515538  -0.9308304  -0.09637046]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1886. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.673331   -0.00508231 -0.96593744  0.02432358]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 1887. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.12792623 -0.08831263 -0.9422937  -0.01748955]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 1888. State = [[-0.24350218  0.13673909  0.03888718  1.        ]]. Action = [[ 0.65200806 -0.25379717 -0.9125184  -0.00289083]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1889. State = [[-0.24331369  0.13679057  0.03888811  1.        ]]. Action = [[ 0.3190899  -0.13172543 -0.91160685 -0.0651226 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1890. State = [[-0.24331369  0.13679057  0.03888811  1.        ]]. Action = [[ 0.7858479  -0.02881873 -0.944659   -0.04924554]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 1891. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.30260265 -0.2339037  -0.89794075 -0.02908927]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1892. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.59393024 -0.13963878 -0.9502785   0.0427556 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1893. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.45901716 -0.25767386 -0.96736765 -0.01259279]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1894. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.5775666   0.11266482 -0.933699   -0.05219924]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 1895. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.44927442 -0.16368878 -0.49827105 -0.06110847]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 1896. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.4163506  -0.19889802 -0.9622421   0.01499188]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 1897. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.5878879   0.03728807 -0.9672994  -0.00544453]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 1898. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.50697243  0.10389304 -0.96636695 -0.03596514]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 1899. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.36486924 -0.04121041 -0.973601    0.00747836]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 1900. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.46266484  0.00187159 -0.982125    0.07249522]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 1901. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.3100283  -0.14232206 -0.94753283 -0.03227526]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 1902. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.46377075  0.12778735 -0.9285217   0.06766224]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 1903. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.19996679  0.16549969 -0.8686937  -0.0332911 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 1904. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.42323875  0.18443954 -0.84940994  0.03922844]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 1905. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.36338866  0.06826365 -0.8994092   0.06711257]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 1906. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.4689281  -0.08590287 -0.9463101   0.07404995]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 1907. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.64631844  0.09836566 -0.9116147  -0.00364709]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 1908. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.4984219  -0.06287652 -0.8397513   0.03579688]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 1909. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.3954208   0.07559645 -0.9493854  -0.00284171]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 1910. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.28590798 -0.14057368 -0.9003759  -0.001517  ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 1911. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.46673596  0.06572175 -0.8984924   0.06946313]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 1912. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.25021982 -0.03337967 -0.82195204  0.04996896]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 1913. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.37337244  0.08730674 -0.87915635  0.04948485]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 1914. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.30025768  0.09821773 -0.88402015  0.08680904]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 1915. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.37087083  0.1588614  -0.73040354  0.00548172]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 1916. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.52366364  0.16505706 -0.897643    0.10006082]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 1917. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.46586752 -0.03301859 -0.8032096   0.11616421]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 1918. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.25420332  0.05252624 -0.75917125  0.12881684]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 1919. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.4553374   0.06547213 -0.9122975   0.10243201]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 1920. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.48176932  0.06635213 -0.9091774   0.11088538]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 1921. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.15118754  0.08369184 -0.91614944  0.0489918 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 1922. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.2779572   0.05779099 -0.89495426  0.07826078]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 1923. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.4581988   0.05775118 -0.8660728   0.15284431]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 1924. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.27515233  0.07112992 -0.93526566  0.04305804]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 1925. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.24917936  0.02099204 -0.89241886  0.05559909]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 1926. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.1556493   0.2004683  -0.938125    0.11446786]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 1927. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.37797165  0.11192226 -0.9719456   0.21956182]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 1928. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.26161766  0.06472623 -0.88581896  0.10455585]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 1929. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.18385637  0.00914383 -0.8978716   0.06148863]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 1930. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.04003751  0.07405269 -0.8781983   0.11266029]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 1931. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.3297193   0.13901734 -0.9397508   0.06326962]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 1932. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.4345503 -0.0772863 -0.7663995  0.1273582]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 1933. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.38777995  0.10768402 -0.8773434   0.07762527]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 1934. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.3118856   0.02819049 -0.966187    0.10748029]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 1935. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.24658811  0.08691239 -0.90316427  0.117764  ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 1936. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.39469147 -0.04150945 -0.8535106   0.08908427]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 1937. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.34688187  0.05020821 -0.95010793  0.11028194]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 1938. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.17068803  0.02421176 -0.9339226   0.086447  ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 1939. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.06921899  0.10915351 -0.9260449  -0.00890964]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 1940. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.24192047  0.1947918  -0.86376446  0.04311132]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 1941. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.29949105  0.1337223  -0.9482888   0.06129014]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 1942. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.37874782  0.02820694 -0.9301538   0.0832597 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 1943. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.24674177  0.10879457 -0.8749328   0.14331317]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 1944. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.18147671  0.06460226 -0.93488604  0.07628059]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 1945. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.2698431  -0.03278297 -0.9642743   0.09991765]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 1946. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.26245522  0.13815069 -0.9566637   0.20195317]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 1947. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.1858648   0.06655681 -0.95365494  0.17795038]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 1948. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.41877186 -0.01701558 -0.9550361   0.1654377 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 1949. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.2505356  -0.01515532 -0.96661764  0.18423986]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 1950. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.31660855  0.06139064 -0.8477101   0.19786191]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 1951. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.1693449   0.03197241 -0.8405794   0.23202467]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 1952. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.2055651   0.02364695 -0.94095826  0.19995868]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 1953. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.2535963   0.02114427 -0.9523214   0.19995928]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 1954. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.23991823  0.13252735 -0.986701    0.20349073]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 1955. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.18155444  0.12954366 -0.8793913   0.20505297]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 1956. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.37053895  0.00597072 -0.9536243   0.2665758 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 1957. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.25971055 -0.08010507 -0.93240684  0.21472394]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 1958. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.29088688  0.1485256  -0.9550517   0.34000313]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 1959. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.30271304  0.05047357 -0.8788703   0.21489942]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 1960. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.09185219 -0.01081192 -0.91760945  0.18572402]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 1961. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.31269062  0.0068233  -0.75295115  0.21412945]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 1962. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.35563898  0.11646152 -0.8901159   0.2440126 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 1963. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.25301397  0.12213254 -0.9199763   0.25011778]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 1964. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.3272338   0.18394041 -0.8746696   0.19257379]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 1965. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.340127   -0.05796951 -0.898251    0.23818564]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 1966. State = [[-0.24324115  0.13681026  0.03890333  1.        ]]. Action = [[ 0.2924912   0.00507879 -0.894556    0.20338202]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 1967. State = [[-0.26140082  0.11867442  0.11791594  1.        ]]. Action = [[ 0.37627923  0.10424185 -0.9616931   0.14743865]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 1968. State = [[-0.25553682  0.13307205  0.09782594  1.        ]]. Action = [[ 0.41981554  0.05937481 -0.84557927  0.11634386]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1969. State = [[-0.24577095  0.13627575  0.06601878  1.        ]]. Action = [[ 0.2479049   0.05540824 -0.94174826  0.1312803 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1970. State = [[-0.23691589  0.13900809  0.03946702  1.        ]]. Action = [[ 0.40578544  0.07875836 -0.9618481   0.02840257]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1971. State = [[-0.23528731  0.13946334  0.03759509  1.        ]]. Action = [[ 0.2800367   0.10677993 -0.942336    0.04548013]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 1972. State = [[-0.2352148   0.13948242  0.03761095  1.        ]]. Action = [[ 0.4023943   0.1498096  -0.9832951   0.13408995]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 1973. State = [[-0.2352148   0.13948242  0.03761095  1.        ]]. Action = [[ 0.42751217  0.07062149 -0.9523777   0.1444813 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 1974. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.39389944  0.07842243 -0.98008215  0.0983429 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 1975. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.40611076  0.03698373 -0.9612873   0.05952418]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 1976. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.26131308  0.02803421 -0.9264478   0.14931762]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 1977. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.22329831  0.01821423 -0.9859255   0.17259872]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 1978. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.1867181  -0.03433537 -0.9839196   0.1384033 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1979. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2966392  -0.06973612 -0.961586    0.13291466]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 1980. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.36328566 -0.09234613 -0.8400589   0.118258  ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 1981. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.08601451  0.16025102 -0.8714376   0.12787223]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 1982. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.3640592   0.02126467 -0.9371536   0.14657307]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 1983. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.4187262   0.04410827 -0.92396504  0.1603533 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1984. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.37697256  0.09593034 -0.9509447   0.21640491]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1985. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.49922085  0.0223031  -0.95386356  0.18609726]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 1986. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.15240991 -0.07957727 -0.93781495  0.19889104]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1987. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.35433495  0.08984709 -0.96981394  0.1465025 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1988. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2602998   0.07909477 -0.92532     0.21292746]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 1989. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.34940374 -0.02969342 -0.9180569   0.21664238]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 1990. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.26210058 -0.07015538 -0.7511697   0.2657038 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1991. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.27649772  0.01913548 -0.97621095  0.25312293]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1992. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.20101845  0.05058527 -0.9626958   0.19439173]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 1993. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.29602814  0.02017331 -0.9859971   0.23433554]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1994. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.12527514  0.01098657 -0.8729346   0.23024607]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1995. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.08450782  0.04660368 -0.9816947   0.17869341]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1996. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.31708074  0.06028497 -0.94600666  0.13727331]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 1997. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.13470602  0.00814998 -0.9865861   0.13235092]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 1998. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.35963607 -0.03981274 -0.9463497   0.19205141]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 1999. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.24847698  0.02374732 -0.90998584  0.11323857]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2000. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2636304  -0.0536443  -0.9827985   0.19284868]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2001. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.17319512 -0.03113729 -0.92988044  0.20285797]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2002. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.29763317 -0.03047502 -0.9819865   0.11684787]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2003. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2691065   0.17660666 -0.9390173   0.20696688]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2004. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.3134631   0.10043633 -0.88262475  0.13662314]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2005. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.45806575 -0.0884859  -0.979825    0.23174834]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2006. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.27260125 -0.06073093 -0.9461088   0.24092484]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2007. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.32097113 -0.14100975 -0.90233666  0.17049396]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2008. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.18862414  0.0357691  -0.95900446  0.24070239]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2009. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.1623838  -0.11421639 -0.8716273   0.26019526]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2010. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.4323789   0.07529581 -0.9377705   0.19564414]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2011. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.41418052  0.05882525 -0.8311638   0.21362948]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2012. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.3894291  -0.04919159 -0.94888234  0.18277109]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2013. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.35231924  0.11130106 -0.9832537   0.27176905]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2014. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.27653122 -0.0444845  -0.96004057  0.24757504]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2015. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.33935034  0.09170318 -0.9148817   0.203614  ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2016. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.39450777  0.1543299  -0.78063273  0.23263383]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2017. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.4355415   0.00118685 -0.96282005  0.26907456]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2018. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.20743084  0.11979735 -0.98292553  0.24986517]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2019. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.24504888  0.02418923 -0.95026785  0.14282763]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2020. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.21078718 -0.03812337 -0.97276956  0.21098375]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2021. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.24412596 -0.00470167 -0.82161474  0.2644012 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2022. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.31936932  0.03591383 -0.93188906  0.17204034]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2023. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.27690685 -0.03477979 -0.96687317  0.2039342 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2024. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.23201978 -0.00803393 -0.95534956  0.14285994]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2025. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.07617307  0.11068273 -0.9649953   0.092538  ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2026. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.31824303  0.12103319 -0.92884165  0.21238315]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2027. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.09929812  0.13652718 -0.92147094  0.23745704]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2028. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.38702416  0.00111914 -0.9650227   0.1078831 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2029. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.38993263 -0.03604418 -0.89035916  0.20671344]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2030. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.30065072 -0.01472372 -0.9598645   0.14617193]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2031. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.08500874  0.08559358 -0.9881119   0.16834545]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2032. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.23994768 -0.04181844 -0.959076    0.12949395]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2033. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2610289  -0.11164343 -0.92845756  0.08159673]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2034. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[-0.00505751 -0.19725269 -0.9859515   0.3027351 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2035. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.1775285  -0.14987421 -0.9795185   0.2189132 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2036. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.08358562 -0.14271915 -0.9107154   0.22522736]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2037. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.12110615 -0.05230868 -0.9602226   0.14553177]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2038. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.11894524 -0.05790484 -0.97716707  0.16233552]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2039. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.107921    0.05354273 -0.9315091   0.14688325]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2040. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2633469  -0.05009645 -0.9326931   0.21562743]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2041. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.09669399 -0.0189479  -0.96846527  0.13042307]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2042. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[-0.02821207  0.02668107 -0.9641972   0.220662  ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2043. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.21354795 -0.03782344 -0.98282695  0.18796504]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2044. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.3903482  -0.02032697 -0.8541738   0.17284858]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2045. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.22310579  0.04596353 -0.934253    0.14681983]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2046. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.2439059   0.02265751 -0.9385798   0.18012285]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2047. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.24374759  0.04890871 -0.98634875  0.22659457]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2048. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.0104177  -0.06582546 -0.9377003   0.20319855]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2049. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.16592956 -0.04558814 -0.9297866   0.23720264]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2050. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.14545345  0.02568233 -0.961086    0.23657835]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2051. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.14898622 -0.07569098 -0.92294884  0.18873799]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2052. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.21295977 -0.06609869 -0.97683734  0.22663271]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2053. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.20019293  0.02036667 -0.91724026  0.143543  ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2054. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 2.9062843e-01 -3.3050776e-04 -9.6127450e-01  1.6614115e-01]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2055. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.11834145  0.05271363 -0.92624     0.25252104]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2056. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.32464206  0.08377934 -0.9604018   0.12936282]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2057. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.24233377 -0.03076571 -0.9686193   0.16032445]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2058. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.23718345 -0.04458153 -0.9509867   0.13544297]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2059. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.03618252  0.07876945 -0.9541116   0.06541729]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2060. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.21174741  0.08427989 -0.97099894  0.08869565]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2061. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.31265068  0.06944609 -0.9769111   0.07718658]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2062. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.32777214 -0.01112443 -0.9302013   0.07171035]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2063. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.23051345 -0.01931202 -0.96946114  0.2480582 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2064. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.12851131 -0.07362425 -0.9844748   0.14965701]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2065. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.33119774  0.04381657 -0.98777497  0.06841302]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2066. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.15940976 -0.05433047 -0.9413759   0.15463257]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2067. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.02722025  0.16543281 -0.97350514  0.15384185]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2068. State = [[-0.2352737   0.13946785  0.03751991  1.        ]]. Action = [[ 0.18734121  0.00602329 -0.9743603   0.14360559]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2069. State = [[-0.26136297  0.11549401  0.11829158  1.        ]]. Action = [[ 1.5616620e-01 -4.8708916e-04 -8.8985711e-01  9.2836738e-02]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2070. State = [[-0.2594908   0.12858433  0.09733669  1.        ]]. Action = [[ 0.02482927 -0.04293185 -0.93785787  0.09680831]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2071. State = [[-0.258222    0.12834434  0.06300841  1.        ]]. Action = [[ 0.26114404 -0.08020711 -0.9693342   0.11725342]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2072. State = [[-0.25199962  0.12862588  0.03505062  1.        ]]. Action = [[ 0.22104442 -0.06520069 -0.97365993  0.08603299]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2073. State = [[-0.2486505   0.12819873  0.03317777  1.        ]]. Action = [[ 0.24003065  0.01373565 -0.9099875   0.09901404]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2074. State = [[-0.24812296  0.12790449  0.03324044  1.        ]]. Action = [[ 0.37215078 -0.04544896 -0.97350127  0.09954298]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2075. State = [[-0.24810816  0.12783569  0.03324348  1.        ]]. Action = [[ 0.21449673 -0.17711389 -0.91990566  0.09517348]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2076. State = [[-0.24810816  0.12783569  0.03324348  1.        ]]. Action = [[ 0.20115805 -0.08222461 -0.8405568   0.07626164]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2077. State = [[-0.24806307  0.12744781  0.03326509  1.        ]]. Action = [[ 0.2105242  -0.06953919 -0.97773015  0.03300345]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2078. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.29940057 -0.05655104 -0.9645382   0.10352623]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2079. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.1616962  -0.03742105 -0.970646    0.22662556]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2080. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.10290527 -0.08947271 -0.9267205   0.05656362]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2081. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.06430936  0.03117299 -0.9655937   0.07407105]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2082. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.24416399 -0.01778525 -0.9748711   0.15644705]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2083. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.08206451  0.01676488 -0.794075    0.10224426]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2084. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.21755552  0.0134536  -0.942327    0.12221491]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2085. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.25289643  0.00360775 -0.98863256  0.09928453]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2086. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.18248677  0.09747148 -0.9554475   0.20553231]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2087. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.14633131 -0.03271222 -0.9770731   0.05818284]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2088. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.19848442  0.02417135 -0.97631115  0.12636209]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2089. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.11171007  0.04229164 -0.9222676   0.10604084]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2090. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.27531242 -0.08452177 -0.9599976   0.19679499]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2091. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.1409328   0.07126164 -0.97740877  0.11879694]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2092. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.03962648 -0.0826962  -0.71386313  0.0593642 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2093. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.2121222   0.00874794 -0.9655012   0.1152674 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2094. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.09642112 -0.01067877 -0.93214536  0.17529893]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2095. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.09843862 -0.05678582 -0.9373894   0.16671419]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2096. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.31002796 -0.02706999 -0.9901761   0.07685244]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2097. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.1810807  -0.02696657 -0.98394495  0.11044121]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2098. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.19755065  0.05080509 -0.9550854   0.14973879]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2099. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.08539331  0.17913175 -0.8349085   0.19144833]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2100. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.0443759  -0.12309945 -0.949276    0.15820885]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2101. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.24525642  0.10714042 -0.8359407   0.17183149]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2102. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.29072845 -0.02172458 -0.9699888   0.05045354]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2103. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.13213587  0.0355978  -0.9722717   0.07540607]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2104. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.08225536 -0.12670135 -0.9655126   0.08267033]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2105. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.05301237 -0.07206881 -0.97298133  0.03826058]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2106. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.25448346 -0.00611341 -0.9629223   0.19772208]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2107. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.18004322 -0.17176217 -0.9552131   0.14415777]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2108. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.14582253 -0.07250625 -0.92944205  0.09710455]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2109. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.13107193 -0.1457076  -0.9396239   0.11039078]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2110. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.19842482 -0.1362924  -0.9663343   0.02839947]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2111. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.08021688  0.01722443 -0.9280422   0.08569443]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2112. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.1301812  -0.06117076 -0.95585936  0.12409139]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2113. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.17499018 -0.07734746 -0.9745444   0.16912103]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2114. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.16280293 -0.06277269 -0.9502525   0.11475623]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2115. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.18962812  0.05633736 -0.99151105  0.12770653]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2116. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.21963716  0.04424226 -0.9851223   0.08973193]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2117. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.09506512 -0.06950647 -0.976752    0.12929726]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2118. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.28767562 -0.09796584 -0.9235052   0.10940266]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2119. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.3010224   0.0448103  -0.9327494   0.08528852]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2120. State = [[-0.24801879  0.1272414   0.03327446  1.        ]]. Action = [[ 0.29161477 -0.02093828 -0.98330706  0.03894091]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2121. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.21543705  0.0414058  -0.8876941   0.05058026]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2122. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.02176702 -0.06866735 -0.9440655   0.03736806]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2123. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.25166714  0.03725648 -0.94820875  0.16301084]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2124. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.21154225  0.0489794  -0.98407906  0.05204272]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2125. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.20608759 -0.04027587 -0.958572    0.07057345]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2126. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.07513952 -0.00718528 -0.9457836   0.07931721]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2127. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.27056026  0.08555794 -0.97834074  0.09396946]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2128. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.13674557  0.0161624  -0.97719723  0.06622636]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2129. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.0597198   0.02065969 -0.9764166   0.14482617]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2130. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.13391924 -0.02857536 -0.9790589   0.13581371]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2131. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.07188702 -0.00199741 -0.9676647   0.13677669]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2132. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.12902474 -0.24372739 -0.9720919   0.09531116]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2133. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.20448732 -0.00542998 -0.8594846   0.07499337]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2134. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.2271676   0.05292249 -0.9085878   0.09882879]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2135. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.167943    0.02878213 -0.9249943   0.08415258]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2136. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.14046383 -0.03025901 -0.96274555  0.11249089]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2137. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.11476636  0.06640112 -0.95121557  0.13857698]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2138. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.07270932 -0.07840651 -0.9701645   0.12590456]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2139. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.17049348  0.04483509 -0.944239    0.055812  ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2140. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.25260854 -0.08000267 -0.80306256  0.08021665]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2141. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.08206761 -0.00177699 -0.88214946  0.09962893]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2142. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.07559371  0.07894194 -0.9755768   0.10191762]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2143. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.04592109  0.03513408 -0.96653944  0.00828993]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2144. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.09527552  0.01637256 -0.7017318   0.07828164]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2145. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.16718149 -0.02468258 -0.97923577  0.07137895]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2146. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.16776383 -0.03144592 -0.9735301   0.07878613]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2147. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.26602876  0.07383156 -0.9089589   0.04425883]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2148. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.15737319 -0.06085849 -0.9622795   0.01470053]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2149. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.24165726  0.04744494 -0.98099476  0.18245506]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2150. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.24718606  0.10199749 -0.9352738   0.07067466]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2151. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.14662147 -0.01695365 -0.98391277  0.04741478]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2152. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.14099658  0.15830147 -0.8874135  -0.06662655]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2153. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.09918177  0.04179418 -0.9350927  -0.0160141 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2154. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.21017027  0.02889252 -0.9897403   0.08355415]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2155. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.15747929  0.16895556 -0.9627502   0.08197618]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2156. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.10592055  0.0544858  -0.9009902   0.04336894]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2157. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.23800433 -0.11582541 -0.891512    0.07604754]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2158. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.22524929  0.10131645 -0.9630745   0.06439555]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2159. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.22915828  0.08051407 -0.9785692   0.08306587]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2160. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.06924605  0.03943717 -0.96656454 -0.04910004]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2161. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.1417973  -0.09635609 -0.9154162   0.05619252]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2162. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[-4.1699409e-04 -1.4154613e-02 -9.6772075e-01  5.9163451e-02]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2163. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.07371664  0.01803458 -0.9756855   0.06176162]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2164. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.07075739  0.03196692 -0.9746251  -0.08426052]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2165. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[-0.00766087 -0.00728858 -0.8819845   0.10637856]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2166. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.11714971  0.07687974 -0.97926617 -0.03001165]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2167. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.12548351  0.1255182  -0.94863385  0.04755592]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2168. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.2611469  -0.05826312 -0.94899446  0.02100551]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2169. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.01554322 -0.08213151 -0.93233716  0.03075838]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2170. State = [[-0.24804884  0.1272341   0.03323038  1.        ]]. Action = [[ 0.19697297 -0.1569348  -0.9550194   0.08634031]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2171. State = [[-0.25914532  0.02998802  0.11947284  1.        ]]. Action = [[ 0.15331149  0.06886184 -0.9538607   0.07184815]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2172. State = [[-0.26292074  0.0865354   0.122959    1.        ]]. Action = [[ 0.16161633  0.00519359 -0.9828109  -0.050165  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2173. State = [[-0.2607242   0.09656594  0.10125431  1.        ]]. Action = [[ 0.14550221 -0.06633425 -0.722076    0.06328928]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2174. State = [[-0.25753078  0.09728707  0.07268275  1.        ]]. Action = [[ 0.12316644  0.0643959  -0.92077494  0.05351186]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2175. State = [[-0.25275943  0.09844717  0.04882075  1.        ]]. Action = [[ 0.16554666  0.0179677  -0.9836664   0.0513382 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2176. State = [[-0.25102395  0.09879037  0.04708572  1.        ]]. Action = [[ 0.10085773  0.02205849 -0.9753515   0.07821274]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2177. State = [[-0.25094953  0.09880499  0.04709818  1.        ]]. Action = [[ 0.19826198  0.10084736 -0.9902697   0.13531864]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2178. State = [[-0.2509751   0.09879391  0.04709212  1.        ]]. Action = [[ 0.19469237 -0.00595409 -0.96389836  0.12348247]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2179. State = [[-0.2509751   0.09879391  0.04709212  1.        ]]. Action = [[ 0.15624487 -0.0899117  -0.9542369   0.06713843]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2180. State = [[-0.2509751   0.09879391  0.04709212  1.        ]]. Action = [[ 0.19267309  0.05925453 -0.9862652   0.06280804]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2181. State = [[-0.2509751   0.09879391  0.04709212  1.        ]]. Action = [[ 0.1442492   0.03216434 -0.9632318   0.09803867]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2182. State = [[-0.2509751   0.09879391  0.04709212  1.        ]]. Action = [[ 0.15504491 -0.00562936 -0.9576714   0.01955199]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2183. State = [[-0.25100386  0.09878863  0.04705034  1.        ]]. Action = [[ 0.02051783  0.02307808 -0.9857792   0.1234051 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2184. State = [[-0.25100386  0.09878863  0.04705034  1.        ]]. Action = [[ 0.11280918 -0.06956458 -0.89916176  0.10860693]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2185. State = [[-0.25100386  0.09878863  0.04705034  1.        ]]. Action = [[ 0.07444203 -0.11102515 -0.9741742   0.01107049]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2186. State = [[-0.25100386  0.09878863  0.04705034  1.        ]]. Action = [[ 0.0862788  -0.04720676 -0.95396286 -0.03051221]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2187. State = [[-0.25085506  0.09881788  0.04707526  1.        ]]. Action = [[ 0.18499982 -0.0973466  -0.952334    0.02557755]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2188. State = [[-0.25078064  0.0988325   0.04708774  1.        ]]. Action = [[ 0.05721045 -0.00528288 -0.9583559   0.04338956]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2189. State = [[-0.25078064  0.0988325   0.04708774  1.        ]]. Action = [[ 0.15531754 -0.01489997 -0.9912626   0.10482562]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2190. State = [[-0.25078064  0.0988325   0.04708774  1.        ]]. Action = [[ 0.10479414 -0.06670469 -0.97123295  0.04861307]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2191. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.0860498  -0.01006639 -0.97297776  0.08505332]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2192. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.14785779  0.06460249 -0.9876354   0.13172078]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2193. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15689051  0.05660307 -0.96645635 -0.02362746]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2194. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15037191 -0.00211096 -0.9851352   0.05468833]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2195. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.03416824  0.0500946  -0.8796034   0.06693339]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2196. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.02005398  0.04079807 -0.98519844  0.0884124 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2197. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.07298601  0.04175472 -0.93974507  0.12552202]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2198. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.02394867 -0.02593863 -0.9116385   0.05608881]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2199. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.02017528  0.03353512 -0.9804967   0.14068222]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2200. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10493529 -0.03539628 -0.9882464   0.00303733]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2201. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.01842213  0.1112299  -0.980828   -0.05404735]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2202. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.04491711  0.08549082 -0.97654474  0.08464634]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2203. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.18773675  0.09219372 -0.9570175  -0.057702  ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2204. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06367636 -0.01114976 -0.9944974   0.08618593]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2205. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.1895299  -0.01195765 -0.9880182   0.14290392]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2206. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15047383  0.12659895 -0.97196966  0.06967223]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2207. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 7.4918747e-02  1.5034437e-02 -9.6851838e-01 -2.0235777e-04]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2208. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.08510971  0.07687104 -0.85904366  0.00604594]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2209. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.09021306  0.1198684  -0.9833554   0.02370119]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2210. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.07734978 -0.00867069 -0.9873076   0.06805754]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2211. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.20598876 -0.00276744 -0.9561046   0.06648517]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2212. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06850946 -0.03063685 -0.9296867   0.10392344]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2213. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10506558 -0.12713253 -0.9903367  -0.0148614 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2214. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.14015305 -0.1456151  -0.98420084  0.10496068]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2215. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.08616316  0.07145119 -0.9868128   0.09434402]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2216. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15288031  0.02612448 -0.95560193 -0.07585299]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2217. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.04457378  0.06813276 -0.9848611  -0.00265437]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2218. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.1764797   0.14547718 -0.97054183 -0.01194352]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2219. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15028548  0.00714767 -0.9568873  -0.02816296]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2220. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10749745  0.03298271 -0.9574135   0.02023077]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2221. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.09097767 -0.07551891 -0.9907399  -0.04763377]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2222. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.05458724  0.03876233 -0.98182243  0.05765331]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2223. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06157613  0.0577786  -0.98447293 -0.02264053]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2224. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.00521719  0.02510512 -0.96856195  0.00327015]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2225. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.13658631  0.00490487 -0.9768952  -0.01360756]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2226. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.00356668 -0.04729998 -0.96005166 -0.01011336]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2227. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10607696 -0.11317509 -0.9783769  -0.00318128]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2228. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.21219623 -0.08633304 -0.99512285 -0.03192019]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2229. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.08153152 -0.01480764 -0.987937   -0.10437965]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2230. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.00964594  0.02236104 -0.97072417 -0.05425376]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2231. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.14048624 -0.047867   -0.99330056  0.01351452]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2232. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06388485  0.05690992 -0.96231073  0.05634904]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2233. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15058053  0.0720427  -0.9848369  -0.02013797]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2234. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.0741812  -0.04324424 -0.90315205 -0.02981001]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2235. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06086028 -0.10170865 -0.9159467  -0.03270298]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2236. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10279715  0.01047373 -0.9666545  -0.08615601]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2237. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06362247 -0.01294607 -0.9686615   0.08398962]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2238. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10288298  0.10000467 -0.98584145  0.00975323]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2239. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-5.7263255e-02 -2.1911800e-02 -9.6728414e-01 -1.2576580e-05]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2240. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.08295965  0.05202556 -0.94758236 -0.09208941]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2241. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.13018918  0.0897727  -0.9508642  -0.04259807]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2242. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.00146031 -0.11923933 -0.9238669   0.04183292]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2243. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10807824 -0.06829584 -0.98283106  0.0092504 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2244. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.07973385  0.03515387 -0.94802696 -0.00915283]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2245. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.02703106 -0.0730685  -0.9827983  -0.0011704 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2246. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10786378 -0.05933684 -0.9585988  -0.02277482]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2247. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.03866345  0.02896726 -0.90347475 -0.02254844]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2248. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.03205812 -0.01812059 -0.97947747  0.04508042]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2249. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.03513062 -0.13585788 -0.943792    0.03539836]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2250. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06964707 -0.23061997 -0.97268754  0.01483703]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2251. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.05461574 -0.05084664 -0.99312246  0.01193964]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2252. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.05413043 -0.03028929 -0.9327632   0.0149436 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2253. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.12843072  0.04527879 -0.96179557 -0.02104026]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2254. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.0119586   0.05757082 -0.97902954 -0.04338837]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2255. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.0550226   0.04349232 -0.98204076  0.12241447]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2256. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.04192108 -0.15153521 -0.9906113   0.0379554 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2257. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[-0.01116598 -0.04655236 -0.9848325   0.02766669]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2258. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.04242074  0.06567883 -0.9749345  -0.02988869]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2259. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.05479574 -0.09272724 -0.99468476 -0.09346753]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2260. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06612468 -0.1244505  -0.98252076  0.09125459]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2261. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.04759085 -0.21056408 -0.9851301  -0.02862972]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2262. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.05593812 -0.03240675 -0.9209436  -0.03866047]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2263. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.16410065 -0.05766249 -0.9595219   0.01626801]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2264. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.10709858 -0.06630635 -0.96914107  0.06780934]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2265. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.02616346 -0.09306729 -0.98788345 -0.05798531]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2266. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.01829743 -0.0310595  -0.9621461  -0.03368586]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2267. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.11014068  0.00568497 -0.95141214  0.03094697]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2268. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.11863303 -0.10739726 -0.9658854   0.00838268]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2269. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.15659213 -0.06204981 -0.98578084 -0.09290189]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2270. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.07977605 -0.02330238 -0.99578714  0.01382804]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2271. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.17126942 -0.138435   -0.9835414   0.04640484]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2272. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.08210218  0.13436067 -0.9310392   0.01884472]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2273. State = [[-0.25070676  0.09884702  0.04710015  1.        ]]. Action = [[ 0.06087708  0.01315737 -0.98066413 -0.00486249]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2274. State = [[-0.26495785  0.16168247  0.12278664  1.        ]]. Action = [[ 0.14765954 -0.0302946  -0.90504634  0.05367315]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2275. State = [[-0.2641879   0.17963707  0.10172667  1.        ]]. Action = [[ 0.02764893  0.06104088 -0.9822207   0.01168501]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2276. State = [[-0.26129368  0.11952826  0.11892692  1.        ]]. Action = [[ 0.06127501  0.06546533 -0.96954477 -0.0098682 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2277. State = [[-0.25868478  0.13305654  0.09885215  1.        ]]. Action = [[ 0.14114177  0.06310606 -0.9615981   0.0052166 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2278. State = [[-0.25646493  0.13439131  0.06258386  1.        ]]. Action = [[ 0.10248625 -0.00733066 -0.99019635  0.0345304 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2279. State = [[-0.25171673  0.1361384   0.03392888  1.        ]]. Action = [[ 0.09454501 -0.01914674 -0.9653547  -0.01949227]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2280. State = [[-0.2507159   0.13642706  0.03239075  1.        ]]. Action = [[ 0.08843756 -0.143395   -0.99020916 -0.07299054]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2281. State = [[-0.25055727  0.1364704   0.03234347  1.        ]]. Action = [[ 0.00163078 -0.08048046 -0.98557156 -0.03237188]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2282. State = [[-0.25069386  0.13643718  0.03204937  1.        ]]. Action = [[ 0.0265255  -0.14042187 -0.99520904  0.08010972]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2283. State = [[-0.25087354  0.13639197  0.03178598  1.        ]]. Action = [[ 0.13862681 -0.14270163 -0.9837325  -0.11485153]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2284. State = [[-0.25087354  0.13639197  0.03178598  1.        ]]. Action = [[ 0.17977214 -0.11206764 -0.9967124  -0.07194686]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2285. State = [[-0.25083426  0.13640527  0.03156315  1.        ]]. Action = [[ 5.7217240e-02 -2.5345087e-03 -9.9063617e-01 -5.7023764e-04]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2286. State = [[-0.2508213   0.13640969  0.03148872  1.        ]]. Action = [[ 0.05247104 -0.27517778 -0.9954455  -0.13690752]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2287. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.11522162  0.00532675 -0.9587811  -0.20165646]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2288. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.1160078   0.0084424  -0.95733744 -0.0885762 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2289. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.10737526  0.02385783 -0.9283127  -0.05512375]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2290. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.01153588 -0.02323866 -0.97250205 -0.19875228]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2291. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.13212776 -0.14372218 -0.98122656 -0.14579535]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2292. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.05021322 -0.09912014 -0.97760904 -0.03491241]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2293. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.16255474 -0.14035237 -0.97668    -0.13381624]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2294. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.02459276 -0.22290051 -0.9626231  -0.19225049]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2295. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.26706052 -0.0586701  -0.97427374 -0.10031492]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2296. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.18700933 -0.06231582 -0.970019   -0.1662724 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2297. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.09553182 -0.04781008 -0.97567034 -0.06727129]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2298. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.19830585 -0.28930855 -0.99173194 -0.16831934]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2299. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.04673314 -0.06626254 -0.9530892  -0.16916203]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2300. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.05258787  0.01382506 -0.9332145  -0.17866766]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2301. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.08333898 -0.20502615 -0.99583906 -0.25305736]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2302. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.04822958 -0.1648407  -0.9725464  -0.0621438 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2303. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.10611081 -0.0334816  -0.98800135 -0.07508224]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2304. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.14379501 -0.12585205 -0.9613191  -0.03580767]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2305. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.06689072  0.00913918 -0.9743331  -0.1735369 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2306. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.14652324 -0.11712807 -0.99267894 -0.09123677]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2307. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.07784295 -0.16804707 -0.97637844 -0.09173924]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2308. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.08339798 -0.19032913 -0.8522877  -0.18282229]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2309. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.11182749 -0.08444953 -0.97070247 -0.13100648]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2310. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.07449794 -0.32100934 -0.9885574  -0.13121319]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2311. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.17627048 -0.12929296 -0.98314494 -0.1310237 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2312. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.1649028  -0.1406402  -0.9883712  -0.21660167]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2313. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.00180078 -0.10247004 -0.9890148  -0.20883894]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2314. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.01913655 -0.10493201 -0.9905304  -0.10859603]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2315. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.19195282 -0.28370428 -0.96929485 -0.08335871]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2316. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.04210532 -0.2988667  -0.9892573  -0.24997425]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2317. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.03654408 -0.09801    -0.98149335 -0.07542557]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2318. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.07641518 -0.09195983 -0.98383296 -0.03934914]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2319. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.0591923  -0.16063535 -0.9727762  -0.19354779]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2320. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.1055845  -0.10682708 -0.9874517  -0.12897134]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2321. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.04394817 -0.03588563 -0.986111   -0.20921195]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2322. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.16349983 -0.18982267 -0.97792995 -0.14504439]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2323. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.13894367 -0.14916235 -0.97617877 -0.12845105]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2324. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.13303757 -0.07589996 -0.9723196  -0.13743776]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2325. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.04844666 -0.14429992 -0.972093   -0.05442393]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2326. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.0334512  -0.10928357 -0.9934673  -0.16610515]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2327. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[-0.11361653 -0.0202949  -0.95524454 -0.15896219]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2328. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.01718688 -0.25365663 -0.99047273 -0.09313953]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2329. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.09460747 -0.07199603 -0.99743253 -0.1666153 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2330. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.03284061 -0.226125   -0.97315615 -0.19629782]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2331. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.15913653 -0.1135006  -0.99022317 -0.19727862]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2332. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.16488993 -0.16141915 -0.90391934 -0.2325989 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2333. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.19151545 -0.13837665 -0.99003375 -0.23715043]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2334. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.11645865 -0.08621782 -0.95937246 -0.18094444]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2335. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.13079822 -0.2779175  -0.98409927 -0.15394813]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2336. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.08727205  0.00157261 -0.9713019  -0.17122525]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2337. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[-0.01510859 -0.04006243 -0.9751452  -0.18745387]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2338. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.17114234 -0.07872397 -0.98011726 -0.10496354]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2339. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[-0.02697438 -0.3005054  -0.9869696  -0.07335317]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2340. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.08430839 -0.15944946 -0.9926047  -0.25463766]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2341. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.13247895 -0.17412233 -0.990498   -0.10365063]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2342. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.0336014  -0.17842734 -0.9963645  -0.23758167]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2343. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.07633543 -0.05452538 -0.9768958  -0.13978285]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2344. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.12453175 -0.02294785 -0.9652967  -0.14521164]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2345. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.19633329 -0.18563616 -0.95228183 -0.11255038]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2346. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.15515804  0.00921845 -0.97060436 -0.19016314]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2347. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.2510563  -0.1406846  -0.98702294 -0.13776171]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2348. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.07478929 -0.03069979 -0.9794201  -0.13818473]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2349. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.16631305 -0.08547646 -0.98997754 -0.15837944]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2350. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.01122153 -0.03323054 -0.99575114 -0.17017978]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2351. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.23405015 -0.09185147 -0.9785212  -0.15851122]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2352. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.17106068 -0.14455932 -0.99551666 -0.12427962]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2353. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.18070745 -0.13218206 -0.96866804 -0.24997556]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2354. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[-0.0503037  -0.11219972 -0.9939482  -0.09877753]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2355. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.16177619 -0.20067078 -0.99787533 -0.16607171]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2356. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.10866165 -0.14518452 -0.9829325  -0.06956631]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2357. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.09026301 -0.06377929 -0.9922072  -0.1960268 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2358. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.23296273 -0.11409163 -0.99192894 -0.12854236]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2359. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.11719036 -0.14571619 -0.99019825 -0.14042926]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2360. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.18706572 -0.02915275 -0.9591081  -0.2581132 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2361. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.1708684  -0.20389825 -0.98520017 -0.2614156 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2362. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.14743996 -0.12884426 -0.9882056  -0.18568808]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2363. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.23782647 -0.08775431 -0.987428   -0.25486076]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2364. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.20361888 -0.17653203 -0.97733    -0.1334365 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2365. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.0458957  -0.1503647  -0.95265913 -0.2339971 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2366. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.18431604 -0.27894986 -0.99425447 -0.2607513 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2367. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.1382711  -0.03970027 -0.9898385  -0.23392415]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2368. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.18408453 -0.13919246 -0.98831534 -0.17135352]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2369. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.23506427 -0.274374   -0.9559536  -0.11128998]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2370. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.25389004 -0.13562173 -0.9783837  -0.2040394 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2371. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.192227   -0.20550889 -0.991722   -0.13314384]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2372. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.23545635 -0.16485697 -0.95231724 -0.26151597]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2373. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.15842032 -0.16817707 -0.9872922  -0.10592604]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2374. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.27271867 -0.2380805  -0.98774236 -0.32277572]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2375. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.07322335 -0.21801394 -0.9933119  -0.2991321 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2376. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.13804686 -0.09748894 -0.9746784  -0.22945511]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2377. State = [[-0.25082135  0.13641201  0.03129681  1.        ]]. Action = [[ 0.26842403 -0.15049553 -0.99426055 -0.23862725]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2378. State = [[-0.26128167  0.11334142  0.11948696  1.        ]]. Action = [[ 0.10409367 -0.1475941  -0.9829514  -0.21736383]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2379. State = [[-0.25569776 -0.16996413  0.12108292  1.        ]]. Action = [[ 0.25251973 -0.05929375 -0.9951311  -0.09884238]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2380. State = [[-0.25770828 -0.18843623  0.0996244   1.        ]]. Action = [[ 0.02144337 -0.07022685 -0.9311718   0.23897636]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2381. State = [[-0.25873718 -0.18943296  0.06386824  1.        ]]. Action = [[-0.00214088 -0.04138201 -0.9910416   0.21917653]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2382. State = [[-0.25884554 -0.19215935  0.03603995  1.        ]]. Action = [[-0.12976032 -0.04151505 -0.9570769   0.26026058]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2383. State = [[-0.25842682 -0.1926189   0.0323025   1.        ]]. Action = [[-0.1049214  -0.14928466 -0.96160436  0.18097222]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2384. State = [[-0.25856394 -0.19258344  0.03211898  1.        ]]. Action = [[-0.04143095 -0.06320244 -0.96501935  0.05879581]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2385. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.10392219 -0.1125347  -0.9803149   0.15857089]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2386. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.01591754 -0.04852062 -0.8615336   0.28745282]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2387. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.07600635 -0.12917274 -0.99439245  0.20230532]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2388. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.10340595 -0.03313065 -0.9699216   0.21432424]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2389. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.03104508 -0.11161125 -0.9932325   0.10179901]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2390. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.03577518 -0.1272484  -0.83821976  0.19205666]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2391. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.10254627 -0.07252455 -0.9251734   0.07072902]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2392. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.00441206 -0.03360432 -0.9776438   0.19569278]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2393. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.06106639 -0.147995   -0.98359776  0.10016918]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2394. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.00673026 -0.1179933  -0.96505004  0.24931848]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2395. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.07803726 -0.10254574 -0.9894627   0.1841116 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2396. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.06916445 -0.04407686 -0.971949    0.09758103]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2397. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.11576724  0.0543617  -0.9446696   0.25083554]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2398. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.03194046 -0.00670558 -0.93246895  0.02414107]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2399. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.01561588 -0.09824538 -0.9939705   0.17965138]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2400. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.02881706 -0.14931679 -0.9662911   0.16338098]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2401. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.03847408 -0.02732313 -0.9142965   0.18028045]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2402. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.03520203 -0.15855634 -0.93269503  0.10763085]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2403. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.10760307 -0.02505368 -0.97411126  0.24081433]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2404. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.00378144 -0.04870796 -0.9860343   0.18055117]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2405. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.02929854  0.05385566 -0.968845    0.08308673]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2406. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.01787579 -0.10332721 -0.98489094  0.22697592]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2407. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.08175647 -0.10280406 -0.93849534  0.14775443]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2408. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.01579738 -0.12152994 -0.96756727  0.24524164]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2409. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.03887898 -0.12506974 -0.9565906   0.22272277]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2410. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.12743902 -0.07652795 -0.9707078   0.19956076]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2411. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.05397946 -0.1767925  -0.96538657  0.21137846]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2412. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.0036571  -0.05905122 -0.98026806  0.20464778]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2413. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.04082036 -0.07310265 -0.9921911   0.08632171]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2414. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.15763515  0.02586877 -0.9788557   0.1580069 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2415. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.13188529  0.08139777 -0.9225671   0.11373043]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2416. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.03019649 -0.01631379 -0.93244433  0.2220372 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2417. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.10114133 -0.04526764 -0.98386014  0.22491097]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2418. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.02403909  0.00523484 -0.9516241   0.21662176]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2419. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.02298051  0.00695837 -0.9586766   0.21093428]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2420. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.04994142 -0.04994506 -0.97427326  0.20271242]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2421. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.02713108 -0.07901764 -0.9954054   0.2558403 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2422. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.04380053 -0.10904646 -0.97322124  0.30143476]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2423. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.00967085 -0.27337658 -0.9662516   0.08999443]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2424. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.16186082 -0.19035238 -0.9383109   0.25985324]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2425. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.05366993 -0.20360303 -0.96341974  0.08110642]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2426. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.10205889 -0.10646403 -0.92154104  0.29823804]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2427. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.04545051 -0.05096823 -0.98472846  0.07731545]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2428. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.06900036 -0.01231652 -0.95637095  0.24997032]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2429. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.11213088 -0.09335995 -0.9624574   0.12359726]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2430. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.05200768  0.05874157 -0.9600589   0.23003125]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2431. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.03448296 -0.04071105 -0.9124928   0.14586556]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2432. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.04749322 -0.08363712 -0.9854895   0.20972586]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2433. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.01597953 -0.18177521 -0.99459803  0.21796727]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2434. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.17185605 -0.00406671 -0.9730027   0.18717766]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2435. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.04072589 -0.10259777 -0.97375435  0.23463857]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2436. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 4.6813488e-04 -1.0751325e-01 -9.8432779e-01  1.3227737e-01]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2437. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.04077566 -0.14364988 -0.9620044   0.31687403]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2438. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.0762217  -0.11466777 -0.990843    0.20008385]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2439. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.02932554 -0.09671783 -0.9752716   0.2585764 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2440. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.08413887  0.01888108 -0.9610916   0.20064807]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2441. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.01394582 -0.0478853  -0.8593542   0.18229973]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2442. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.00556874 -0.04477519 -0.9005759   0.21792245]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2443. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.06072688  0.04364312 -0.9574308   0.24233913]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2444. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.06986558 -0.08096826 -0.95880765  0.2577976 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2445. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.10085928 -0.08765042 -0.9042242   0.26174498]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2446. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.1512481  -0.02746069 -0.9535599   0.1469214 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2447. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.14870954 -0.07979459 -0.9445888   0.17826867]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2448. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.06828213 -0.1145466  -0.9793263   0.21757913]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2449. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.13724422 -0.04408896 -0.9873727   0.26241028]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2450. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.13264954 -0.12934208 -0.9854411   0.2908132 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2451. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.08798254 -0.20475316 -0.88492876  0.20703411]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2452. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.03063881  0.02803349 -0.9842657   0.21572208]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2453. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.02461481 -0.13998812 -0.97280157  0.217116  ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2454. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.01135015 -0.04774714 -0.8486865   0.1378758 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2455. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.00489992 -0.17720753 -0.93185294  0.11248839]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2456. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.00167966 -0.10981923 -0.880907    0.07162356]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2457. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.0713048   0.00402272 -0.9783955   0.19433069]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2458. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.06415308 -0.04164529 -0.96662897  0.16216612]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2459. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.05855799 -0.09739035 -0.83891255  0.14130175]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2460. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.00196594  0.04228413 -0.98656964  0.18671107]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2461. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.09658289 -0.06872898 -0.9970572   0.20165074]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2462. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.06355995 -0.0030936  -0.87798554  0.30582666]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2463. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.08497918  0.07277107 -0.82631695  0.203897  ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2464. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.06968302 -0.0418418  -0.9735237   0.3204826 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2465. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.02178705 -0.01027948 -0.969962    0.14005494]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2466. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.05370402 -0.11120993 -0.9801986   0.20403183]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2467. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.07528269 -0.06323177 -0.9528069   0.18058884]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2468. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.17171979 -0.10855603 -0.9965527   0.3139043 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2469. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.07204819 -0.10977983 -0.97862905  0.11448026]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2470. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.07107294 -0.03253555 -0.94267654  0.202618  ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2471. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.10376215 -0.02643085 -0.8472986   0.17096996]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2472. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.02042913 -0.06534082 -0.9334799   0.1862905 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2473. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.00623798 -0.04931259 -0.7517446   0.18047786]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2474. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[-0.07546854 -0.11569333 -0.9644638   0.24902487]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2475. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.02987802  0.06094337 -0.9903509   0.18855906]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2476. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.21500313 -0.20262122 -0.7666011   0.24195755]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2477. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.01800811 -0.09681064 -0.9183675   0.39029956]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2478. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.08949637 -0.05108237 -0.9834409   0.26388383]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2479. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.04969311 -0.07736671 -0.9294321   0.14878964]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2480. State = [[-0.25851965 -0.19260348  0.03212091  1.        ]]. Action = [[ 0.17220092 -0.07380915 -0.98798436  0.19949913]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2481. State = [[-0.25164312  0.05278258  0.12532131  1.        ]]. Action = [[ 0.05383515  0.03795457 -0.92963105  0.21229541]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2482. State = [[-0.25311637  0.05822453  0.10430045  1.        ]]. Action = [[ 0.12586927 -0.03886259 -0.9371989   0.06376147]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2483. State = [[-0.24973871  0.05924189  0.06886252  1.        ]]. Action = [[ 0.14385581  0.05233097 -0.9676512   0.01634765]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2484. State = [[-0.24380459  0.06031448  0.04227824  1.        ]]. Action = [[ 0.3698969  -0.0841397  -0.998138   -0.03228545]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2485. State = [[-0.24241266  0.06050747  0.0397612   1.        ]]. Action = [[ 0.38110328 -0.03140855 -0.98629016 -0.14727312]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2486. State = [[-0.24227706  0.06051983  0.03962404  1.        ]]. Action = [[ 0.27255845  0.09129369 -0.991106   -0.06457227]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2487. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.29694772 -0.02847588 -0.97731626 -0.19727522]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2488. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.26344562  0.07525074 -0.99000216 -0.0597685 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2489. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.5178971   0.08614349 -0.99457276  0.00148845]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2490. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.34601688  0.06347883 -0.9824182  -0.12430739]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2491. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.22955167  0.02962208 -0.8473126  -0.19657981]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2492. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.43291926  0.13401067 -0.98595023  0.01728785]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2493. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.23985064 -0.14344609 -0.9442525  -0.09672403]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2494. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.23455787 -0.02622658 -0.9740106  -0.04026264]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2495. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.16355836  0.05058813 -0.9698668   0.06709266]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2496. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.4372387   0.11081338 -0.970672   -0.02576894]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2497. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.58624053 -0.06730956 -0.99398917 -0.06281281]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2498. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.37784815 -0.0156225  -0.9850499  -0.15059704]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2499. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.5082164  -0.05931079 -0.9728085  -0.15004486]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2500. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.3771671  -0.03701216 -0.97048384 -0.1421811 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2501. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.29413795  0.00820708 -0.95216036  0.01391625]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2502. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.3857125  -0.03253376 -0.96633357 -0.07678384]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2503. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.44652712  0.00319076 -0.97556424 -0.07120752]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2504. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.18540823 -0.00790024 -0.9617864  -0.13596547]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2505. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.37807202  0.04191864 -0.97780275 -0.07213628]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2506. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.315992   -0.00163466 -0.9915358   0.04888594]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2507. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.4291607  -0.0482198  -0.98400456 -0.05569339]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2508. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.3830017  -0.02425331 -0.98613554  0.01562226]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2509. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.4943465  -0.1038692  -0.9835596  -0.07651418]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2510. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.32297528 -0.09424978 -0.8546339  -0.13333738]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2511. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.26471257 -0.00470543 -0.99188095 -0.22998947]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2512. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.29470587  0.01779425 -0.97248936 -0.00731266]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2513. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.39060163 -0.033898   -0.9195933  -0.08110982]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2514. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.18399072 -0.01916498 -0.955992   -0.1353867 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2515. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.40250897 -0.03017706 -0.9864307  -0.07763839]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2516. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.3981607  -0.17618978 -0.95727885 -0.14975834]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2517. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.20031703 -0.06682146 -0.8770915  -0.2145338 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2518. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.330114   -0.06283474 -0.9952337  -0.1859985 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2519. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.4188677  -0.28237033 -0.9581776  -0.12108403]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2520. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.31709552 -0.05981416 -0.98753077 -0.01339304]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2521. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.46498466 -0.09694201 -0.98063153  0.02202165]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2522. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.52292013 -0.09064221 -0.93514854 -0.16554546]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2523. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.52440476 -0.1224364  -0.9613571  -0.09386885]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2524. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.48644805 -0.05151689 -0.9642023  -0.09222174]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2525. State = [[-0.24199592  0.06055686  0.03966048  1.        ]]. Action = [[ 0.5061772  -0.0025261  -0.91855586 -0.1294837 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2526. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.36289454  0.02614403 -0.9833489  -0.10246485]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2527. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.41464663 -0.08136934 -0.98935395 -0.09923893]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2528. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.40602696 -0.06155801 -0.9559716  -0.09940124]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2529. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.35585117  0.02100539 -0.93572605 -0.08351928]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2530. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.3621415  -0.04464579 -0.9372693  -0.11569893]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2531. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.44899297 -0.03157634 -0.92261183 -0.06660193]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2532. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.3711878  -0.11626112 -0.99585044 -0.05098814]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2533. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.2850139  -0.09936738 -0.9929501   0.04264474]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2534. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.5909704   0.01562476 -0.9765023  -0.17575121]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2535. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.20333767 -0.00497556 -0.9943732  -0.20751238]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2536. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.15507817 -0.03256708 -0.9738955  -0.05759257]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2537. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.40889657 -0.027282   -0.8378028   0.00910819]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2538. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.28643084 -0.05906546 -0.94039935 -0.13717926]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2539. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.34913492 -0.13694048 -0.9735309  -0.0731982 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2540. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.2770654  -0.03565007 -0.99046284 -0.01552314]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2541. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.27257538 -0.13777137 -0.9947103  -0.04235518]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2542. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.24575889 -0.10018957 -0.9770754  -0.01995164]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2543. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.26410866 -0.07423353 -0.9867238   0.01575124]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2544. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.45038795 -0.07584155 -0.98953724 -0.00899285]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2545. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.31162858 -0.01322144 -0.96959734 -0.21923977]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2546. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.46941304  0.01429009 -0.91080725 -0.07731485]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2547. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.4294405  -0.08657777 -0.9736     -0.08355075]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2548. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.33692503 -0.01291245 -0.921667   -0.10178673]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2549. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.26581955 -0.00125521 -0.9144969  -0.03890359]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2550. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.45942044  0.06813312 -0.96002966 -0.09220892]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2551. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.45568252 -0.01334244 -0.98211855 -0.0266754 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2552. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.26637936  0.08176398 -0.9740875  -0.09766054]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2553. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.24010766  0.01322174 -0.98284155 -0.20488042]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2554. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.22910476  0.04928613 -0.9807735  -0.05219871]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2555. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.37412143 -0.01656443 -0.97409666 -0.22442555]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2556. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.33240807 -0.06567049 -0.92777944 -0.18248993]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2557. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.36940455 -0.0669449  -0.9874687  -0.07074851]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2558. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.32957113 -0.07059479 -0.9650909  -0.08397728]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2559. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.28112483 -0.02634704 -0.9930169   0.03600323]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2560. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.17888832 -0.07011902 -0.97271395 -0.05851209]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2561. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.23352087 -0.08440936 -0.9866889  -0.08390474]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2562. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.4059248  -0.0512346  -0.98830426 -0.05261987]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2563. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.33913827  0.00190592 -0.9762558  -0.07268822]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2564. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.38834465 -0.06305915 -0.9938512  -0.09788764]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2565. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.310143   -0.04602087 -0.9833585  -0.05584502]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2566. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.4718306   0.06723177 -0.9858252  -0.08436054]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2567. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.3097962  -0.13744915 -0.9599256  -0.04242086]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2568. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.430511   -0.04639703 -0.9787962  -0.18394464]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2569. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.50194764 -0.12855852 -0.9974983  -0.09111899]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2570. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.53942895 -0.12179935 -0.9925561  -0.07148117]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2571. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.53506505 -0.0621556  -0.99633425 -0.12414008]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2572. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.3499806  -0.09787726 -0.978485   -0.03160697]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2573. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.19210005 -0.10803008 -0.98506284 -0.28751063]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2574. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.4770521  -0.15616369 -0.9544672  -0.09783399]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2575. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.34984016 -0.15365052 -0.980548   -0.13016337]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2576. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.45042872 -0.1632421  -0.9757294  -0.08085781]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2577. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.26252067 -0.08021396 -0.9926158  -0.05840033]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2578. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.14950049 -0.05937582 -0.978309    0.01303899]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2579. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.26728356 -0.09684837 -0.9256322  -0.03721869]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2580. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.2833321  -0.14514172 -0.9970607  -0.05907136]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2581. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.42205513 -0.10121745 -0.9700703  -0.16972047]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2582. State = [[-0.24192016  0.06056539  0.03967093  1.        ]]. Action = [[ 0.26920986 -0.07443255 -0.99399793 -0.04545605]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2583. State = [[-0.26351634  0.14884353  0.12229826  1.        ]]. Action = [[ 0.4368627  -0.15499032 -0.98196787  0.01662254]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2584. State = [[-0.26094595  0.16489342  0.10235125  1.        ]]. Action = [[ 0.26632547  0.10593975 -0.7855695   0.06544065]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2585. State = [[-0.26134855  0.12099676  0.12094354  1.        ]]. Action = [[ 0.1520971  -0.08211726 -0.9828603  -0.10704476]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2586. State = [[-0.25836235  0.13442945  0.09910194  1.        ]]. Action = [[ 0.2461245  -0.10667986 -0.8837348   0.13140011]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2587. State = [[-0.25404146  0.13536826  0.06604286  1.        ]]. Action = [[ 0.12075126  0.06136835 -0.97833806  0.07623005]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2588. State = [[-0.24774835  0.1373565   0.03977183  1.        ]]. Action = [[ 0.3295294  -0.07697529 -0.977246   -0.18311214]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2589. State = [[-0.24672821  0.13739567  0.03598024  1.        ]]. Action = [[ 0.37779462 -0.14060217 -0.99682367 -0.04274708]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2590. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.4631424  -0.14644969 -0.9831525  -0.0721637 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2591. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.4934168  -0.04031533 -0.97127056 -0.11800081]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2592. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.39200974 -0.08150077 -0.9977026  -0.15325302]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2593. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.37858427 -0.03627568 -0.99545246 -0.09474164]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2594. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.41526842 -0.04150891 -0.9727357   0.01805675]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2595. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.26977956  0.08266819 -0.9895063  -0.11515009]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2596. State = [[-0.24684688  0.13736705  0.03580339  1.        ]]. Action = [[ 0.34259093 -0.00884295 -0.9838375  -0.10882241]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2597. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.4277146  -0.14448684 -0.97728527 -0.00261384]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2598. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.37616086 -0.01454329 -0.9577853  -0.03451574]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2599. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.28185833 -0.09454364 -0.9733353   0.07026041]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2600. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.47529304  0.05927217 -0.9505005  -0.03495681]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2601. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5387826  -0.03345519 -0.9938502  -0.0162729 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2602. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.48116875 -0.13857156 -0.9824862  -0.08390892]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2603. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.41394138 -0.15390229 -0.97585374 -0.02136272]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2604. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.4854927   0.00378144 -0.971489   -0.04445899]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2605. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.37904537 -0.12420636 -0.88572717 -0.13127828]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2606. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.27439117  0.05076778 -0.9266587  -0.07675779]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2607. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.22259438 -0.14717662 -0.98810834 -0.07935631]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2608. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.20767379 -0.1323716  -0.9690134  -0.26826328]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2609. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.57641983 -0.07028121 -0.97352254 -0.08506906]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2610. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.4481113   0.00529397 -0.9070546  -0.01760918]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2611. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.28211093 -0.03797102 -0.9877038  -0.12494111]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2612. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.4519019   0.00984418 -0.92250293 -0.13846958]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2613. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.38391554  0.00275505 -0.99413013  0.0160774 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2614. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.29992414 -0.12977237 -0.94038844 -0.02443707]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2615. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.39862287 -0.06497931 -0.9771433  -0.24271744]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2616. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.35199153  0.01334071 -0.9886958  -0.17810696]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2617. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3630916   0.05686438 -0.9600757  -0.0553183 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2618. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3674004  -0.14745826 -0.9924249  -0.04748094]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2619. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.46480238 -0.09338224 -0.98537874 -0.00559509]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2620. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.36073077  0.02341294 -0.9934954  -0.08028907]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2621. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.33467376  0.03239334 -0.98802227 -0.0300867 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2622. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.22467911 -0.1070298  -0.96501523 -0.16436887]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2623. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.2638991  -0.07569635 -0.9973052   0.06017303]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2624. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.37115407  0.0104723  -0.991702   -0.21932673]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2625. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.46547043 -0.07589447 -0.9693412  -0.07206833]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2626. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.425426   -0.15172136 -0.9570493  -0.02758449]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2627. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.49433744 -0.11977291 -0.95838636  0.0425905 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2628. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.41555905 -0.23820823 -0.9333622  -0.18740958]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2629. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.31486118 -0.01439196 -0.9836625  -0.17656165]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2630. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.38758028  0.01286483 -0.99545807 -0.07515478]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2631. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.30798984 -0.05492073 -0.97756714 -0.1317035 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2632. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.2805829   0.00894332 -0.95678896 -0.15923154]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2633. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.43067467 -0.15712148 -0.9939905  -0.13121986]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2634. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.337013   -0.23748678 -0.9672467  -0.13349235]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2635. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.31982005 -0.1848005  -0.98943424 -0.03849632]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2636. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3350644  -0.04417652 -0.94915915  0.08142281]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2637. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.325572   -0.04934788 -0.9736416  -0.04134285]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2638. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3792013  -0.10074919 -0.9843737   0.09387374]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2639. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3250382  -0.08749288 -0.9859927  -0.09409845]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2640. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.48033106  0.06783462 -0.9946606  -0.09418619]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2641. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.37394297 -0.17581534 -0.9919057  -0.17106837]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2642. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5146656  -0.04095256 -0.9209559  -0.12863815]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2643. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.28781366 -0.08692682 -0.9934543  -0.16821241]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2644. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.45784223 -0.03181219 -0.967944   -0.05056494]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2645. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.4860853   0.0290668  -0.9816611  -0.10598886]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2646. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.58847463  0.0704962  -0.9772502  -0.19762838]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2647. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.23149633 -0.06896269 -0.9947097  -0.05719286]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2648. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5109602  -0.11859775 -0.9503369  -0.17847186]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2649. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.35401106 -0.2314533  -0.9914911  -0.08756238]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2650. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.34915555 -0.06419569 -0.97919    -0.02698439]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2651. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.30743396  0.01446402 -0.98273724 -0.02516311]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2652. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.18279266 -0.08137745 -0.9925626  -0.04723489]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2653. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.43153143  0.09591103 -0.9736473  -0.06874824]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2654. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.31082058 -0.05944908 -0.9830962  -0.11114335]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2655. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.58636343  0.00670815 -0.95580226 -0.25145018]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2656. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.27602684 -0.14041758 -0.97883743 -0.11426115]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2657. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 3.4423113e-01 -1.4560896e-01 -9.7980547e-01  5.0437450e-04]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2658. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3071494  -0.06713444 -0.98962104 -0.17128092]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2659. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.47027802  0.06917763 -0.98554283 -0.18032795]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2660. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5617461  -0.00377899 -0.9959401  -0.10023504]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2661. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.3239149  -0.02736449 -0.99434686 -0.06521988]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2662. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5423775  -0.11445099 -0.98514485 -0.07191646]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2663. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.43151248 -0.1626916  -0.98858136 -0.3294562 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2664. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.20797515 -0.11225343 -0.9896087  -0.1952101 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2665. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.6192881  -0.02981544 -0.9696609   0.00368273]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2666. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5357342  -0.11051762 -0.9865163  -0.12308323]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2667. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5262387  -0.10161316 -0.9439139  -0.04168785]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2668. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.42976475  0.03102112 -0.9883933   0.00424266]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2669. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5231992  -0.0525738  -0.98388034  0.04048336]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2670. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.4602177  -0.02531481 -0.9966653  -0.2297926 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2671. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.48147798  0.01500881 -0.99139977 -0.13651305]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2672. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.51435804 -0.09441429 -0.9967811  -0.27302086]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2673. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.46559596  0.07698333 -0.988202   -0.02958751]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2674. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5423424  -0.02114117 -0.9975368  -0.10593897]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2675. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.39654946 -0.10027307 -0.9582987  -0.0043245 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2676. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.49796677 -0.0243119  -0.9649655  -0.15973842]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2677. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.5033572  -0.05509704 -0.9579086   0.09633768]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2678. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.38463807 -0.05423427 -0.9716496  -0.1494751 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2679. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.36289883  0.09048402 -0.9852884  -0.0882206 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2680. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.39243937  0.06724811 -0.9896363  -0.00694859]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2681. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.42397964  0.08614874 -0.99175096 -0.08012307]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2682. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 4.8254275e-01  2.7871132e-04 -9.6863294e-01 -1.0140282e-01]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2683. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.27149904 -0.02764368 -0.9075949  -0.0934636 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2684. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.42526972  0.05793035 -0.97033554  0.00730979]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2685. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.28690648 -0.02849615 -0.9966759   0.06215477]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2686. State = [[-0.2467744   0.137386    0.03581744  1.        ]]. Action = [[ 0.53948176 -0.08906066 -0.9738745  -0.25558794]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2687. State = [[-0.26083434  0.14796877  0.11898378  1.        ]]. Action = [[ 0.31972814  0.03773606 -0.98253757 -0.18191952]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2688. State = [[-0.26135245  0.16342795  0.09760255  1.        ]]. Action = [[ 0.03611124  0.07646787 -0.95693755  0.15953982]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2689. State = [[-0.2577988   0.16638622  0.06450622  1.        ]]. Action = [[ 0.23610044  0.1115073  -0.9656056   0.18577671]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2690. State = [[-0.25018132  0.1699939   0.03570974  1.        ]]. Action = [[ 0.1351068   0.05873275 -0.9901401   0.06679845]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2691. State = [[-0.2484789   0.17075352  0.03329315  1.        ]]. Action = [[ 0.5111735  -0.1186102  -0.9909056  -0.00979233]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2692. State = [[-0.24750088  0.17130966  0.03299172  1.        ]]. Action = [[ 0.3338492  -0.04558331 -0.96728975 -0.13057268]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2693. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.18237936 -0.01684797 -0.9931594  -0.15994644]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2694. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.32676315  0.0200206  -0.98181444 -0.21493465]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2695. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4237578  -0.17587566 -0.9770237  -0.05460137]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2696. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.48689795 -0.11177647 -0.9951426   0.07392776]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2697. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.36610258 -0.14612293 -0.99126726 -0.27239442]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2698. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.43969297  0.04908037 -0.9871613  -0.13440639]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2699. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.46813083  0.13482463 -0.8644088  -0.14318836]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2700. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.27560174 -0.02021712 -0.95915467 -0.05712044]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2701. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.47381067 -0.05094486 -0.9890257   0.00895154]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2702. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5339236   0.03693056 -0.9657744   0.1322012 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2703. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.28053284  0.00714111 -0.97824347 -0.044397  ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2704. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6502743  -0.02947509 -0.88231444 -0.04432136]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2705. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.45723128 -0.05241495 -0.9866302  -0.22768062]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2706. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6210871   0.12381721 -0.9970297  -0.08909285]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2707. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.24742496  0.08515429 -0.98004293 -0.11869401]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2708. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.49185312 -0.07917744 -0.99062645 -0.07435548]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2709. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5652597   0.03852463 -0.977991   -0.13093239]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2710. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.30717957  0.00131249 -0.95404094 -0.08114648]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2711. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.46452355 -0.02130896 -0.9594086  -0.10526258]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2712. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.51414514 -0.13454854 -0.95764613 -0.08008599]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2713. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.10570025 -0.11546886 -0.9904733  -0.13677037]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2714. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.53638816  0.04992974 -0.97439533 -0.07790005]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2715. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4188769  -0.00951546 -0.9724997   0.03787112]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2716. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.32722962  0.04943979 -0.9891851  -0.03358668]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2717. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.47840166  0.00600517 -0.97936153 -0.3314842 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2718. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.36892796  0.16781044 -0.993379    0.04524803]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2719. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5208583   0.10230303 -0.99612063 -0.03205383]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2720. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5637598  -0.01701725 -0.9771359   0.1280911 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2721. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.33151972  0.0108813  -0.9672255   0.00725579]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2722. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.55696726 -0.08582681 -0.9806319  -0.11925006]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2723. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.59048676 -0.02238458 -0.99530584 -0.02367514]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2724. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6894951   0.00699973 -0.9874876   0.04393303]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2725. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.32218003 -0.04686248 -0.9854564  -0.11314613]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2726. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.30497277  0.01886642 -0.89862764 -0.07431912]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2727. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5790342  -0.04960877 -0.9650534   0.07428014]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2728. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5777776  -0.03152692 -0.9892055  -0.17047292]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2729. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.55823183  0.00170743 -0.9764818  -0.1697548 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2730. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5161915   0.05633247 -0.9901599  -0.11234021]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2731. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.24740839 -0.07093662 -0.9874323   0.02726507]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2732. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6052271   0.01368058 -0.9924541   0.01728082]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2733. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.46383882  0.07392478 -0.97632635 -0.06619793]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2734. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4394611   0.0034461  -0.9972716   0.13620627]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2735. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.48441076  0.00329733 -0.99807125 -0.02138251]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2736. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5680474  -0.10139447 -0.9975581  -0.16597825]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2737. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4025494  -0.03224277 -0.95839834 -0.23021173]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2738. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.58329535 -0.09151876 -0.99148864 -0.02857095]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2739. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.45052612  0.05597448 -0.9900836   0.10274696]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2740. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.71884584  0.00710642 -0.98725694 -0.0043593 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2741. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.65455604  0.04242551 -0.9658804   0.08964598]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2742. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.55060196 -0.00407094 -0.9916197   0.00259435]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2743. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5006287  -0.03619063 -0.98940885 -0.17819166]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2744. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4381634   0.05562818 -0.96468323  0.06454098]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2745. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.58468425  0.1606456  -0.9928319  -0.10898232]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2746. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.42485213  0.07350743 -0.987873   -0.07418728]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2747. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.55964684  0.04532099 -0.97834015  0.02585268]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2748. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.2759514   0.11091244 -0.987124   -0.05058974]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2749. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.46844554 -0.04195511 -0.98042184 -0.05848753]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2750. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.346601   -0.02975643 -0.9630087   0.02659297]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2751. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.3030542   0.05977774 -0.98949724 -0.01538956]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2752. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.46922672 -0.09376514 -0.9880808  -0.10792381]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2753. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.52753687 -0.05644739 -0.981868   -0.03946078]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2754. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4934622   0.15826881 -0.990919   -0.27011824]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2755. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.41823077  0.01930571 -0.9404877  -0.0958178 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2756. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.2377255   0.0637542  -0.9772919   0.14858937]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2757. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.51517844  0.02878511 -0.9767723  -0.02318919]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2758. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6041117   0.02243066 -0.97217566  0.22039032]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2759. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5684364   0.03233993 -0.9421747   0.14993334]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2760. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5117338   0.0730114  -0.9807789   0.00529337]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2761. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.642743    0.02227688 -0.99261177 -0.03982025]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2762. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.25908804  0.01149762 -0.9755957   0.05408585]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2763. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.68572927  0.01574266 -0.98808974 -0.2582692 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2764. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.57643974 -0.18489307 -0.9895253   0.07893848]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2765. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.40513265 -0.15037984 -0.9921899  -0.05213565]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2766. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.40249372  0.03377664 -0.99699503 -0.0889473 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2767. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4195187  -0.01823264 -0.9891106  -0.03799844]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2768. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.4382813   0.08712864 -0.9915413  -0.15342271]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2769. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.27684712 -0.02088594 -0.9909479  -0.18817258]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2770. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6072142   0.14238977 -0.994566   -0.19336623]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2771. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 4.6190107e-01 -7.2741508e-04 -9.9162853e-01 -1.2325275e-01]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2772. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.65579855  0.04214346 -0.9950297  -0.25394106]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2773. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.2625283   0.02181208 -0.98966455 -0.23467481]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2774. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.513811    0.00367689 -0.98766065 -0.0070492 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2775. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.35150027 -0.13231838 -0.9613477  -0.13411254]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2776. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5376532  -0.03096205 -0.9796339   0.15258062]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2777. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.43073583  0.13827515 -0.87001437 -0.197784  ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2778. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5422516   0.04900253 -0.947739   -0.07118511]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2779. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5756986   0.00113404 -0.99219054  0.02129316]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2780. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.63649035 -0.03048885 -0.98756045  0.08083391]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2781. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5675112   0.06308556 -0.9148823  -0.01490462]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2782. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.605543   -0.07390261 -0.9879949  -0.08009571]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2783. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.55276775 -0.02363831 -0.9714034  -0.14920485]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2784. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6005697   0.04847801 -0.97569007 -0.12642509]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2785. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.5522634  -0.02392298 -0.9353695  -0.00408196]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2786. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.6212678  -0.09581077 -0.9604775  -0.15714157]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2787. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.45580196 -0.05891562 -0.9906789  -0.11539495]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2788. State = [[-0.24749671  0.1714256   0.0330077   1.        ]]. Action = [[ 0.56052136 -0.03284615 -0.9952234  -0.12659615]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2789. State = [[-0.25601494 -0.17799792  0.11903245  1.        ]]. Action = [[ 0.4177873  -0.06712788 -0.9942267  -0.04530293]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2790. State = [[-0.25611767 -0.19652902  0.09675915  1.        ]]. Action = [[ 0.13716364 -0.00645542 -0.99761695  0.27978158]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2791. State = [[-0.2523453  -0.19507651  0.05950743  1.        ]]. Action = [[ 0.14100301  0.07038164 -0.979899    0.2858039 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2792. State = [[-0.2501109  -0.1962867   0.03176759  1.        ]]. Action = [[ 0.21382785 -0.00654966 -0.9806004   0.31997883]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2793. State = [[-0.24870165 -0.19677013  0.02963611  1.        ]]. Action = [[ 0.11568224 -0.10290051 -0.942749    0.28313363]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2794. State = [[-0.2486762  -0.19678123  0.0296363   1.        ]]. Action = [[ 0.27406466  0.01918364 -0.9554059   0.23913729]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2795. State = [[-0.2486762  -0.19678123  0.0296363   1.        ]]. Action = [[ 0.27101874 -0.08879435 -0.9249357   0.28464973]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2796. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[ 0.12046587  0.03847194 -0.97788095  0.40749705]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2797. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[ 0.25280452  0.0668118  -0.97632515  0.3624444 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2798. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[ 0.17097616  0.11563802 -0.99319595  0.21807575]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2799. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[ 0.107916    0.04935014 -0.96222454  0.32365513]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2800. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[ 0.38201225  0.01926422 -0.9876268   0.40455294]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2801. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[-0.06491768 -0.03714216 -0.9550056   0.3490672 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2802. State = [[-0.24860553 -0.19681051  0.02964086  1.        ]]. Action = [[ 0.13154447 -0.02787799 -0.94849914  0.33625388]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2803. State = [[-0.24863887 -0.19680057  0.02956711  1.        ]]. Action = [[-0.07709062  0.1574328  -0.9878206   0.3156948 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2804. State = [[-0.24866529 -0.19679117  0.02956524  1.        ]]. Action = [[ 0.20036125  0.01311183 -0.9906594   0.3116771 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2805. State = [[-0.24866529 -0.19679117  0.02956524  1.        ]]. Action = [[ 0.22620618 -0.10670114 -0.9741106   0.32603157]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2806. State = [[-0.24866529 -0.19679117  0.02956524  1.        ]]. Action = [[ 0.11199307  0.05200589 -0.900949    0.3642907 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2807. State = [[-0.24866529 -0.19679117  0.02956524  1.        ]]. Action = [[ 0.23467958  0.01197577 -0.98021555  0.2596588 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2808. State = [[-0.24866529 -0.19679117  0.02956524  1.        ]]. Action = [[ 0.08552253  0.00492549 -0.9878149   0.23691797]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2809. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.29358506  0.09623313 -0.97682846  0.30238867]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2810. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.40736794 -0.10972512 -0.9696498   0.33630216]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2811. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.17372692  0.00156605 -0.96917367  0.28060126]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2812. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.12246323 -0.0288682  -0.9946133   0.25298798]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2813. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.15098453 -0.09738052 -0.8890097   0.3882606 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2814. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 4.2065382e-03 -8.6736679e-04 -9.2636722e-01  2.7828562e-01]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2815. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.07954407 -0.0102011  -0.96385604  0.38042212]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2816. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.2615409   0.01082969 -0.9728937   0.24557543]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2817. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.16581488  0.02078199 -0.97467464  0.27778137]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2818. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.11525655  0.04314375 -0.99106216  0.1752733 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2819. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[-0.05253506  0.10467207 -0.94233346  0.34072816]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2820. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.07664931 -0.00114495 -0.94826055  0.32256043]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2821. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.03844941  0.04915452 -0.9914852   0.29020822]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2822. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.20186651 -0.07702589 -0.98118937  0.24722743]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2823. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.12116468 -0.04286909 -0.96238095  0.35707045]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2824. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.11815286 -0.02988666 -0.9735784   0.25978827]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2825. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.03176451 -0.01376003 -0.93382555  0.33485508]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2826. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.03920114  0.02668273 -0.9723517   0.3992586 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2827. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.13745642 -0.12048149 -0.95270175  0.27787375]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2828. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.34590912  0.08814049 -0.17457587  0.26404095]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2829. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.24987864  0.04813659 -0.9334468   0.30076635]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2830. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.1438446   0.05675852 -0.8330657   0.24715877]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2831. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.1935184   0.0148809  -0.95275646  0.36788929]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2832. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.09233582 -0.11191517 -0.96230996  0.3713187 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2833. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.2799964   0.09005916 -0.5926793   0.2580948 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2834. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.01452208  0.08885276 -0.79731226  0.31910944]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2835. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.32083178  0.19752347 -0.99750924  0.24482   ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2836. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.34874392  0.00195551 -0.8639871   0.24844003]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2837. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.19396842  0.01680183 -0.8337987   0.32369685]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2838. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.17323363 -0.14412212 -0.9650859   0.3394599 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2839. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.12230718 -0.05017734 -0.9137395   0.23568845]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2840. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.23660004 -0.02248919 -0.30893862  0.33213556]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2841. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.22219181 -0.0567981  -0.81396633  0.37725043]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2842. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.04043114  0.03201401 -0.98591     0.30786014]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2843. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.24492192 -0.1358968  -0.9027622   0.36018264]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2844. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[-0.04701614 -0.16818368 -0.9944025   0.34726775]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2845. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.2224226  -0.05518311 -0.95897686  0.24757648]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2846. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.20753145  0.06766784 -0.9924009   0.36914933]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2847. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.13883305 -0.04364002 -0.9483743   0.31141484]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2848. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.09129107 -0.1394754  -0.9298001   0.24663198]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2849. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.20490456 -0.02046728 -0.9743363   0.22835183]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2850. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.00175512 -0.08860391 -0.9614116   0.24869418]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2851. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.21409142  0.15649915 -0.6964382   0.25966835]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2852. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.00289834  0.0046736  -0.9635339   0.31108963]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2853. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.1476332   0.06769991 -0.9712895   0.18856144]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2854. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.14864123  0.12002742 -0.9789225   0.29415786]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2855. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.1665467  -0.05121899 -0.9152374   0.30524313]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2856. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[-0.03467447  0.16762066 -0.9787836   0.3499422 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2857. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.31322193 -0.03317809 -0.9740619   0.28678942]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2858. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.05571699  0.02348471 -0.8082892   0.2671684 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2859. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.20116448 -0.07739651 -0.99615026  0.31217718]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2860. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.04929113  0.08109581 -0.8932763   0.23398113]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2861. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.09063148  0.1503545  -0.98439795  0.26231408]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2862. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.25163972  0.01286006 -0.9485128   0.24042511]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2863. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.11688149  0.12181759 -0.81712306  0.2956003 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2864. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.06793618  0.06665015 -0.956658    0.34095693]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2865. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.11619174  0.08513474 -0.3310607   0.37117136]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2866. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[-0.07547045  0.13543653 -0.9805625   0.258232  ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2867. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[-0.04973543  0.01715469 -0.90870786  0.35232592]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2868. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.27099395  0.04145491 -0.54966867  0.2843852 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2869. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.0467025   0.02752137 -0.97951734  0.3699751 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2870. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.21215308  0.17091465 -0.91951394  0.24332476]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2871. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.40289545  0.00547385 -0.9470569   0.2674762 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2872. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.24924171  0.01360035 -0.7980793   0.36990833]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2873. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.3186711  -0.08643895 -0.99531996  0.25936162]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2874. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.07469571 -0.16715175 -0.9522997   0.28926945]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2875. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.2257073   0.12930942 -0.7437371   0.28182483]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2876. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.21110618  0.0860759  -0.97633857  0.31837833]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2877. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.14887285 -0.03709489 -0.9367411   0.2918439 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2878. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.10560465  0.12776959 -0.91102844  0.2945956 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2879. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.05567181  0.02970433 -0.7964847   0.343215  ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2880. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.3511176  -0.09103167 -0.931015    0.29607177]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2881. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.04428136  0.00511253 -0.8337518   0.30416465]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2882. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.23097742  0.03670502 -0.7825897   0.32538605]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2883. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[-0.14541304  0.1111654  -0.95075125  0.33326018]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2884. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.09129202  0.07044053 -0.99246466  0.40312862]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2885. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.08266175  0.1285249  -0.9323675   0.2999606 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2886. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.15758467  0.00815594 -0.9401611   0.2764567 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2887. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.05707848  0.05464721 -0.92608464  0.24174082]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2888. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.1671269   0.0407331  -0.96019226  0.25132656]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2889. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.18651915  0.04171789 -0.9751632   0.32983804]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2890. State = [[-0.24869448 -0.19678073  0.02952845  1.        ]]. Action = [[ 0.1359824  -0.09234542 -0.9822387   0.34437132]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2891. State = [[-0.24963117 -0.16481401  0.12382899  1.        ]]. Action = [[ 0.17316318 -0.08924234 -0.92897654  0.36703575]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2892. State = [[-0.2516706  -0.18260936  0.10303201  1.        ]]. Action = [[ 0.02123237 -0.10777706 -0.973049    0.18549013]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2893. State = [[-0.25188318 -0.1840164   0.06635278  1.        ]]. Action = [[ 0.0593847   0.0391221  -0.92607975  0.20355916]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2894. State = [[-0.25013095 -0.18569194  0.04059374  1.        ]]. Action = [[ 0.07535493  0.08244562 -0.97644895  0.27645838]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2895. State = [[-0.24974242 -0.18550736  0.03909726  1.        ]]. Action = [[ 0.12675047  0.0924083  -0.9829719   0.33047712]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2896. State = [[-0.24947935 -0.18561229  0.0390765   1.        ]]. Action = [[ 0.06064534 -0.01179612 -0.9458974   0.3578781 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2897. State = [[-0.24947935 -0.18561229  0.0390765   1.        ]]. Action = [[ 0.06844985  0.05227292 -0.99009764  0.2473439 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2898. State = [[-0.24947935 -0.18561229  0.0390765   1.        ]]. Action = [[ 0.20055258 -0.05335093 -0.96406657  0.23405695]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2899. State = [[-0.24945301 -0.18555334  0.03906692  1.        ]]. Action = [[-0.04403973 -0.04795241 -0.9784295   0.33054066]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2900. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[-0.0792135   0.09050059 -0.8107357   0.29292333]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2901. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[-0.00436538 -0.0175426  -0.8889735   0.27458882]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2902. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.01875114  0.03392255 -0.7579788   0.27135503]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2903. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.15464687  0.02096391 -0.7634079   0.22526073]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2904. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.05956495  0.11468363 -0.9539803   0.22034109]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2905. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.23946452  0.13889444 -0.96609443  0.29835713]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 2906. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.14744937 -0.03905833 -0.9256364   0.3049426 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2907. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[-0.07304084  0.06564379 -0.9975064   0.22062278]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2908. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.1564213   0.05278683 -0.8129888   0.3349961 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2909. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.11976194 -0.04543179 -0.6351002   0.29413915]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2910. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.1646353   0.15418589 -0.90860736  0.27409327]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2911. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.11487758 -0.03242093 -0.96971583  0.20250642]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2912. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.17956078  0.03054261 -0.93689734  0.27687836]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2913. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 2.0797729e-01 -7.9399347e-04 -8.8071299e-01  2.5469673e-01]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2914. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.06016326  0.06452239 -0.93993187  0.35352564]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2915. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.012537    0.04533684 -0.9004557   0.31143057]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2916. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.15982652  0.12803543 -0.95014     0.2126404 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2917. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.08090198 -0.08903825 -0.98409957  0.31947696]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 2918. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.06720066  0.07648599 -0.98794913  0.345551  ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 2919. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.06503701  0.0891093  -0.8329192   0.346622  ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2920. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.05799031  0.01799572 -0.98164755  0.24522388]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2921. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.10066116 -0.1299671  -0.94781154  0.25712645]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2922. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.07804179  0.04147041 -0.9209268   0.32190883]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2923. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.03138518  0.03854227 -0.9873426   0.33823705]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2924. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.13803005 -0.02375907 -0.97002757  0.3177116 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2925. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.11195934  0.15048623 -0.8801187   0.28514254]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2926. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[-0.00384206  0.07273066 -0.9521775   0.3161583 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2927. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.23521781  0.03396189 -0.99384147  0.18915999]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 2928. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.20681071 -0.03836268 -0.77098936  0.2760942 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 2929. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.0102185   0.18436325 -0.97702014  0.2616396 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 2930. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.26463938  0.04332793 -0.899094    0.33142793]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 2931. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.09596705  0.02916467 -0.9814776   0.32867348]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 2932. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.20739865  0.03920138 -0.9596451   0.30774903]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2933. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.2247181   0.00185728 -0.9872277   0.331038  ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 2934. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.26567638 -0.08562684 -0.88682324  0.21673596]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 2935. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.04695559  0.09486032 -0.9232779   0.317047  ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2936. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.1820761  -0.01323694 -0.9348941   0.28061056]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 2937. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.09767127  0.11704016 -0.9855888   0.24853492]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2938. State = [[-0.24952461 -0.18553418  0.03888831  1.        ]]. Action = [[ 0.2383194  -0.02324128 -0.8806552   0.31001163]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2939. State = [[-0.2494981  -0.18547483  0.03887867  1.        ]]. Action = [[ 0.24669802  0.15979815 -0.91883075  0.32946754]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2940. State = [[-0.2494981  -0.18547483  0.03887867  1.        ]]. Action = [[ 0.11587    -0.00751519 -0.97397834  0.42008615]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2941. State = [[-0.2494981  -0.18547483  0.03887867  1.        ]]. Action = [[ 0.24522305 -0.00926018 -0.9574489   0.255893  ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2942. State = [[-0.2494981  -0.18547483  0.03887867  1.        ]]. Action = [[ 0.29782295 -0.07349288 -0.8737772   0.3622427 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2943. State = [[-0.2494981  -0.18547483  0.03887867  1.        ]]. Action = [[ 0.21271515  0.18393934 -0.8693762   0.37465608]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2944. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[-0.13049042  0.04662669 -0.7928455   0.3440361 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2945. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.21332443  0.06882358 -0.4206351   0.2759552 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2946. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.06175327 -0.11563581 -0.9012904   0.2850151 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 2947. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.00787306  0.10903597 -0.9771959   0.37191236]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 2948. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.23937559 -0.06406367 -0.92580885  0.4164008 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 2949. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.00278544 -0.00124776 -0.988645    0.33722222]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 2950. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.05949235 -0.04024178 -0.9911632   0.363868  ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2951. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.16020668 -0.17929679 -0.9140723   0.42914653]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2952. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[-5.0848186e-02  6.2477589e-04 -7.3600411e-01  3.6650658e-01]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 2953. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.2625383  -0.12894458 -0.6859581   0.27533078]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 2954. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.22135103  0.00919604 -0.85634124  0.2924763 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2955. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.19355989  0.00952017 -0.985194    0.3105979 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2956. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.08265519 -0.1028651  -0.80896324  0.40071702]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 2957. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.05073166 -0.17923653 -0.8002845   0.37863255]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2958. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.37260497 -0.08599794 -0.8401119   0.4069519 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2959. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.19150984 -0.05851811 -0.9733461   0.3639748 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 2960. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.1763258   0.04237902 -0.99551994  0.37125707]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2961. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.10199666 -0.05438238 -0.98109436  0.4468    ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2962. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.28015125  0.09387589 -0.90450495  0.33874583]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2963. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.16518939 -0.01925987 -0.71972936  0.40163815]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2964. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.183249   -0.13941193 -0.9832093   0.3403759 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2965. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.16302454 -0.00342178 -0.91739047  0.30293083]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 2966. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.16674447 -0.07453781 -0.97719955  0.38038588]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2967. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.18467987 -0.09040862 -0.71097654  0.3840437 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2968. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.21904743  0.0602504  -0.96017593  0.29968047]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2969. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.2609129   0.01492012 -0.72810423  0.3494724 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2970. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.27513742  0.09787166 -0.96506226  0.3363979 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2971. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.20171416 -0.08024216 -0.80028343  0.3387525 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2972. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.27019954  0.06949151 -0.99162644  0.3371203 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2973. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.26946092  0.01743066 -0.93999565  0.317904  ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2974. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.26718903  0.02433205 -0.9785771   0.3245232 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2975. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.15393388  0.03075993 -0.92270076  0.3344381 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2976. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.28250074 -0.02065426 -0.60568255  0.3487996 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2977. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.15084445  0.1215806  -0.75099     0.3663864 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2978. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.22480965  0.10495019 -0.9372625   0.32732356]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2979. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.21396255  0.09487092 -0.8607172   0.33580482]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2980. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.09790337  0.01402783 -0.83239704  0.31660724]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2981. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.42267346 -0.12009972 -0.76250106  0.3162186 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2982. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.11826873  0.12797558 -0.9137824   0.35552   ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2983. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.17580771  0.0434947  -0.96109146  0.43442822]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 2984. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.1796819  -0.02492738 -0.9870562   0.28784704]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2985. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.31551528  0.17259872 -0.9385517   0.33067358]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2986. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.2992835  -0.00342309 -0.5980109   0.3339665 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2987. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.15992713  0.12231803 -0.8818821   0.3607148 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2988. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.16715991  0.03323996 -0.92407924  0.2624812 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2989. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.13919532 -0.10321021 -0.99557596  0.30555558]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2990. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.18394852  0.02752066 -0.99523646  0.29585826]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2991. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.29329693  0.05592668 -0.9835346   0.3659525 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2992. State = [[-0.24944507 -0.1853561   0.03885942  1.        ]]. Action = [[ 0.14345717  0.12029028 -0.99573386  0.3197446 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 2993. State = [[-0.25557163  0.03033424  0.12524     1.        ]]. Action = [[-0.02338469  0.1573174  -0.75459373  0.28372908]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2994. State = [[-0.25101724  0.03575817  0.1036938   1.        ]]. Action = [[ 0.36157954  0.03652418 -0.98501974  0.2892728 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2995. State = [[-0.23752348  0.03659629  0.06710918  1.        ]]. Action = [[ 0.54859185 -0.11771017 -0.96594924  0.18320334]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2996. State = [[-0.22289932  0.03697968  0.04037222  1.        ]]. Action = [[ 0.50033116 -0.04999852 -0.99305266  0.16449451]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2997. State = [[-0.22112712  0.03704524  0.03869782  1.        ]]. Action = [[ 0.75185275 -0.10238463 -0.97927326  0.20795536]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2998. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.83342123 -0.17499715 -0.97472167  0.21490598]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2999. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6958666  -0.20650601 -0.96338755  0.17003632]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3000. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.79026556 -0.21069813 -0.9730325   0.21696627]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3001. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7943965  -0.07326162 -0.970693    0.20172441]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3002. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.63691545 -0.03722245 -0.95730305  0.180794  ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3003. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.76680386 -0.20446718 -0.95705837  0.19122767]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3004. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8203013  -0.20665127 -0.9931294   0.18657613]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3005. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.5602242  -0.2641096  -0.9706049   0.20423841]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3006. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7956916  -0.15690553 -0.9596421   0.14874077]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3007. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7730565  -0.21419108 -0.9884839   0.16468251]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3008. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.5784073  -0.26287985 -0.9031393   0.11499155]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3009. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.67332244 -0.04802871 -0.99002737  0.14948845]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3010. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7695427  -0.12909383 -0.93327945  0.20926607]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3011. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.5852618  -0.19560307 -0.9830643   0.15931463]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3012. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8293396  -0.21914095 -0.9684272   0.16678679]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3013. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.72567344 -0.00905383 -0.97392035  0.18897974]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3014. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.718773  -0.0463382 -0.7547802  0.2032206]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3015. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7805624  -0.1647836  -0.98520935  0.13481283]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3016. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.54195786 -0.17316926 -0.9736731   0.15793884]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3017. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8394036  -0.22102696 -0.90209967  0.16078389]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3018. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7619798  -0.2831279  -0.9328148   0.21603453]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3019. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7502866  -0.12377173 -0.98837966  0.1645236 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3020. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.662045   -0.28384644 -0.9534374   0.1641618 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3021. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7156029  -0.29160047 -0.94292986  0.17916417]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3022. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.76873875 -0.14624989 -0.98365813  0.13733578]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3023. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7563901  -0.19826812 -0.9299935   0.1247251 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3024. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7775254  -0.1255588  -0.89742565  0.15209186]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3025. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7103132  -0.16551024 -0.8557219   0.21541262]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3026. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.75186217 -0.06975043 -0.9364536   0.15045094]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3027. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.721678   -0.1763342  -0.97833306  0.1981324 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3028. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8636558  -0.17813551 -0.94554234  0.22792196]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3029. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.76573443 -0.15470356 -0.98415464  0.1761303 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3030. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.79879105 -0.05268013 -0.9618668   0.21359527]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 3031. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.84811544 -0.10581106 -0.5421127   0.20144677]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3032. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7483722  -0.15185785 -0.73069817  0.16510701]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 3033. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7264918  -0.15305614 -0.939094    0.16503668]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3034. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.82117534  0.05622268 -0.98398095  0.18993938]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3035. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8381219  -0.11456019 -0.9476682   0.15607655]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3036. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.67923665 -0.23966765 -0.9052205   0.1260612 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3037. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.74593496 -0.21708673 -0.8601127   0.15565455]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 3038. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.81425786 -0.2302202  -0.982912    0.1882987 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3039. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6684196  -0.18522286 -0.99835724  0.19813573]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3040. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8562629  -0.18778867 -0.98776096  0.19402051]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3041. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.59871626 -0.23334342 -0.890385    0.20315576]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3042. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.68842685 -0.26251727 -0.97660506  0.18224978]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3043. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.85704803 -0.2431426  -0.9889875   0.20995176]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3044. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8089292  -0.1059925  -0.95459     0.16858888]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3045. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7206508  -0.15600616 -0.9366505   0.18175721]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3046. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7807194  -0.02843505 -0.94098485  0.12698627]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3047. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.75217795 -0.15188843 -0.83273923  0.18577814]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3048. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.68028164 -0.17310548 -0.993631    0.1600684 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3049. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.79079866 -0.18433893 -0.99358153  0.17741251]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 3050. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.79802036 -0.2103188  -0.95016724  0.17832589]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 3051. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6682129  -0.13629538 -0.97666514  0.25186205]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3052. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6841614  -0.19767702 -0.9861945   0.14279795]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 3053. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6459327  -0.02604705 -0.982364    0.22373617]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3054. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7732091  -0.25766957 -0.9806328   0.21998644]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 3055. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7295399  -0.15666074 -0.94380057  0.23459005]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3056. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.75640523 -0.10143244 -0.78290427  0.2198708 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 3057. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.69709504 -0.13193375 -0.895463    0.21365082]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3058. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.86334085 -0.10754007 -0.96817684  0.17727864]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3059. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7520635  -0.06718105 -0.94662696  0.19373667]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3060. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.48068738 -0.13719487 -0.9154371   0.12812579]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3061. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.83131766 -0.056593   -0.91488665  0.19393313]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3062. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6344012  -0.13162911 -0.6778632   0.09910238]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 3063. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.4639939  -0.02970988 -0.99337345  0.16965103]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3064. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.81776464 -0.21294254 -0.79667085  0.15093684]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3065. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.834262   -0.12837094 -0.9665312   0.22329521]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3066. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.717361   -0.31681222 -0.9013644   0.22813952]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3067. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6771431  -0.22147393 -0.9965745   0.14388025]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3068. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7537255  -0.17408836 -0.98333925  0.19584823]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3069. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.63018787 -0.22831494 -0.8672494   0.24015832]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 3070. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7521516  -0.09749556 -0.958861    0.20871031]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3071. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7226442  -0.22873211 -0.98643607  0.19579315]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3072. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7346895  -0.27842832 -0.9812998   0.20252323]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3073. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7264557  -0.17277622 -0.96040386  0.19048738]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3074. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.65984297 -0.12503058 -0.98731315  0.15459919]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3075. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7435262  -0.07054579 -0.9612136   0.17630398]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3076. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.7241     -0.02621353 -0.95882434  0.2275567 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 3077. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6725123  -0.1864202  -0.91402835  0.18055737]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3078. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8011658  -0.19210082 -0.9204823   0.16592109]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 3079. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.64039683 -0.1779185  -0.9896646   0.20030558]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3080. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6506778  -0.02074397 -0.9608334   0.19163752]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3081. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.71748185  0.0317024  -0.9738771   0.2047528 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 3082. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.69108856 -0.16067213 -0.8508745   0.20963526]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 3083. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.58787084 -0.08465159 -0.9826047   0.122069  ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3084. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.68400156 -0.2536106  -0.96370214  0.16583145]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3085. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.635061   -0.05661833 -0.8434236   0.21231878]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 3086. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6887125  -0.11417526 -0.95119137  0.11379862]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3087. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.63669634 -0.12076712 -0.95243347  0.18659818]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 3088. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6417346  -0.09919113 -0.9814324   0.21612167]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 3089. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6181977  -0.0967049  -0.79468465  0.19541204]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3090. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6425307  -0.24445331 -0.8924985   0.19496548]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3091. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6579516  -0.1950475  -0.9329026   0.22518611]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3092. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.6842345  -0.08876699 -0.9813774   0.20812702]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3093. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.5465858   0.04768562 -0.83343965  0.19889498]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 3094. State = [[-0.22103018  0.03704552  0.03871544  1.        ]]. Action = [[ 0.8279841  -0.17103177 -0.7381346   0.1686027 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3095. State = [[-0.2569961  -0.10903769  0.11871752  1.        ]]. Action = [[ 0.53430474 -0.14721143 -0.8652576   0.13956654]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3096. State = [[-0.25796562 -0.11707091  0.09691404  1.        ]]. Action = [[ 0.19544017  0.10814345 -0.9934076   0.20470357]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3097. State = [[-0.2551073  -0.11858883  0.06540728  1.        ]]. Action = [[ 0.15941656 -0.09612334 -0.38820624  0.2691779 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3098. State = [[-0.2504254  -0.12033244  0.05400047  1.        ]]. Action = [[ 0.12888515 -0.07615674 -0.7670055   0.23547435]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3099. State = [[-0.2502395  -0.12044287  0.05405737  1.        ]]. Action = [[ 0.2590363   0.04957986 -0.9569149   0.28926635]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3100. State = [[-0.2501845  -0.12045183  0.05406058  1.        ]]. Action = [[ 0.074669    0.01642764 -0.97704107  0.3006171 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3101. State = [[-0.2501845  -0.12045183  0.05406058  1.        ]]. Action = [[ 0.14305532 -0.00099438 -0.8732823   0.20108747]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3102. State = [[-0.24785355 -0.12053933  0.04881764  1.        ]]. Action = [[ 0.29716682 -0.0230785  -0.46910977  0.26068258]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3103. State = [[-0.24169609 -0.12241279  0.03700736  1.        ]]. Action = [[ 0.14156854  0.05045009 -0.6502593   0.22012103]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3104. State = [[-0.24086794 -0.12260871  0.03690128  1.        ]]. Action = [[ 0.09024572 -0.05349201 -0.9447114   0.37873328]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3105. State = [[-0.240793   -0.12262572  0.03690765  1.        ]]. Action = [[ 0.02787018  0.20669091 -0.89754945  0.25249922]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3106. State = [[-0.240793   -0.12262572  0.03690765  1.        ]]. Action = [[ 0.29252458 -0.04692227 -0.9964615   0.31354332]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3107. State = [[-0.23612946 -0.12365313  0.0381414   1.        ]]. Action = [[ 0.2951671  -0.05356103  0.4161693   0.23034048]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 3108. State = [[-0.23089953 -0.12482097  0.0388087   1.        ]]. Action = [[ 0.20089233  0.04701614 -0.7237604   0.33891535]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3109. State = [[-0.23062806 -0.12488537  0.03883713  1.        ]]. Action = [[-0.02161902  0.06323469 -0.94341487  0.32424545]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3110. State = [[-0.23010485 -0.12500319  0.03889314  1.        ]]. Action = [[ 0.07944298  0.13243937 -0.45694095  0.22602415]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3111. State = [[-0.23003054 -0.12501992  0.03890114  1.        ]]. Action = [[ 0.20130348  0.04982865 -0.28293562  0.26774144]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3112. State = [[-0.23003054 -0.12501992  0.03890114  1.        ]]. Action = [[ 0.19783449 -0.05892849 -0.68011135  0.2991984 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3113. State = [[-0.23000093 -0.12502508  0.03893987  1.        ]]. Action = [[ 0.2756939  -0.13983983 -0.9953258   0.38866115]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3114. State = [[-0.23000093 -0.12502508  0.03893987  1.        ]]. Action = [[ 0.29040277  0.05829656 -0.81332284  0.3721981 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3115. State = [[-0.23000093 -0.12502508  0.03893987  1.        ]]. Action = [[ 2.4347985e-01  5.5301189e-04 -8.7609130e-01  2.5217402e-01]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3116. State = [[-0.22346103 -0.12625737  0.04173728  1.        ]]. Action = [[0.5082687  0.02635646 0.25305295 0.3577075 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 3117. State = [[-0.21527807 -0.12793732  0.04418329  1.        ]]. Action = [[ 0.21070409  0.00835264 -0.6101679   0.42248523]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3118. State = [[-0.2132897  -0.12838851  0.04460477  1.        ]]. Action = [[ 0.14771461  0.12755263 -0.7949213   0.36348045]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3119. State = [[-0.21268953 -0.12851986  0.04472359  1.        ]]. Action = [[-0.00767869  0.11655152 -0.6282746   0.31135035]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3120. State = [[-0.20882392 -0.12999153  0.04529335  1.        ]]. Action = [[ 0.26550627 -0.25218403 -0.10524696  0.31986332]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 3121. State = [[-0.19826613 -0.13333912  0.05586793  1.        ]]. Action = [[ 0.21335196 -0.06535888  0.8634857   0.42640674]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 3122. State = [[-0.19221304 -0.13451992  0.07167473  1.        ]]. Action = [[ 0.2427653   0.10130274 -0.8840388   0.41690314]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3123. State = [[-0.18911621 -0.13516499  0.07916     1.        ]]. Action = [[0.11211634 0.04011798 0.4593222  0.42368996]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 3124. State = [[-0.18363479 -0.13612112  0.08287346  1.        ]]. Action = [[ 0.35284364 -0.10615242 -0.7225978   0.2988218 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 3125. State = [[-0.17725185 -0.13309641  0.06825143  1.        ]]. Action = [[ 0.14938533  0.21607304 -0.93677163  0.40862048]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 3126. State = [[-0.16929212 -0.13014159  0.04712117  1.        ]]. Action = [[ 0.2583729  -0.08956248 -0.99722594  0.40788507]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3127. State = [[-0.16739239 -0.13010372  0.04361254  1.        ]]. Action = [[ 0.33348453 -0.01033503 -0.58672816  0.50783753]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3128. State = [[-0.16750205 -0.1300163   0.04357972  1.        ]]. Action = [[ 0.3201046   0.08008361 -0.6451092   0.30724835]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3129. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.23552    -0.09036797 -0.9571384   0.43894386]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3130. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.24269521 -0.00238311 -0.9829335   0.4569614 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3131. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.36210394 0.00584817 0.09484744 0.42349517]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 3132. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2036115  -0.18791896 -0.86026084  0.44335008]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3133. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[-0.0791679  -0.181871   -0.8997806   0.43349206]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3134. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2161429  -0.12491179 -0.9647395   0.45086503]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3135. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3139348   0.1065625  -0.9960218   0.37118876]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3136. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.20884597  0.1887145  -0.9008456   0.42121816]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3137. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.13047552 -0.1945433  -0.94886214  0.40734196]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3138. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2691599  -0.11969423 -0.8925107   0.45449734]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3139. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.36671758  0.25327396 -0.35633677  0.22564149]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3140. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2006222  -0.10154903  0.97846866  0.23603785]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 3141. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.35719335  0.12168133 -0.9825914   0.33252275]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3142. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.07765317  0.1125381  -0.96336955  0.4825766 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3143. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3677299   0.00570083 -0.99509853  0.36576366]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3144. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2713493  -0.29820764 -0.7502755   0.33451867]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3145. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3370613  -0.03210807 -0.2505499   0.48485732]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 3146. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.39599454 -0.04852045 -0.9281326   0.30800748]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3147. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2897184   0.0203234  -0.41233373  0.3722695 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3148. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.09297991  0.11733747 -0.9670744   0.48318815]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3149. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.31720555 -0.10003412 -0.7695734   0.30479026]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3150. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2557267  -0.21659744 -0.24440682  0.4442184 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 3151. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.28519142  0.05637705 -0.46508813  0.3299328 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3152. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.46257806 -0.0699659   0.3823483   0.47178626]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 3153. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.13740158 -0.02544045 -0.9645222   0.40521836]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3154. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2497145   0.11847246 -0.95910746  0.29981625]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3155. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.35883248 -0.09401327 -0.7904096   0.44002187]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3156. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.17255425 -0.24528491  0.9702389   0.32920992]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 3157. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.286677   -0.18534118  0.9574559   0.4120767 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 3158. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.43023133 -0.28791606 -0.2592075   0.38214874]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 3159. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.20627642 0.02179992 0.9867705  0.4223733 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 3160. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.18633115 -0.3166927  -0.38512468  0.42061043]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3161. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.24591517 0.0651834  0.5726255  0.39453435]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 3162. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.23577452 -0.03274703  0.93504214  0.4541986 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 3163. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.10760331 -0.17538768 -0.90561277  0.5532181 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3164. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3418752  -0.18721819 -0.8722433   0.32964492]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3165. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3204521  -0.27277124 -0.78616285  0.36606848]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3166. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.09115815  0.25058174 -0.19839597  0.359864  ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 3167. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.39646506 -0.0702638  -0.27264416  0.37346768]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 3168. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.10440278 -0.06078964  0.66843545  0.3946879 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 3169. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.4126804  -0.08726752  0.72279954  0.40665543]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 3170. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.17008388 -0.26761508 -0.8157108   0.35046482]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3171. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.05236399 -0.30862164 -0.6634418   0.3069104 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3172. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2614138  -0.19494873 -0.85057235  0.43407607]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3173. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.26208282  0.34888577 -0.8213536   0.4451797 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3174. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.28092957 -0.04125381  0.4735664   0.38728094]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 3175. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3810978 -0.0227586  0.9722481  0.3265103]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 3176. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.29474676  0.0676496  -0.7508808   0.4650693 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3177. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.3061136  -0.13127786 -0.9988463   0.40956724]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3178. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.22475052  0.34142637 -0.30557156  0.21709967]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 3179. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.15796542  0.0353384  -0.921662    0.5115125 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3180. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.41417003 -0.19525123  0.02065122  0.45025527]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 3181. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.18448639  0.36425614 -0.9367953   0.35479283]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3182. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.52849376 -0.11367607 -0.3099389   0.43686962]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 3183. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.21878314 0.17475438 0.60857344 0.3952334 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 3184. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.36401892 -0.18655747  0.39896476  0.4151399 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 3185. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.26619184 0.14291894 0.29200196 0.5518806 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 3186. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.47766972 -0.13435137 -0.9258537   0.36314428]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3187. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.29988182 0.11156619 0.17223799 0.47492325]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 3188. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.22472763  0.05780745 -0.94271517  0.49207819]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3189. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[0.12699449 0.3992343  0.17855489 0.55669785]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 3190. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.17090404 -0.08893341 -0.51060146  0.5398102 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3191. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.474146   -0.11354029  0.63341117  0.53659034]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 3192. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.21776974  0.17453253 -0.0465129   0.45237446]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 3193. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.2811662  -0.26173186 -0.97403115  0.5425029 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3194. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.10506773  0.02577066 -0.78326756  0.19426215]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3195. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.36103308 -0.19819164  0.9821546   0.5300021 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 3196. State = [[-0.16737467 -0.12953901  0.04351725  1.        ]]. Action = [[ 0.40121806 -0.1504761  -0.91736615  0.42180383]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3197. State = [[-0.26339886  0.08140206  0.11163161  1.        ]]. Action = [[ 0.45370495  0.10850203 -0.65342706  0.34685206]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3198. State = [[-0.25615785  0.08718924  0.09099807  1.        ]]. Action = [[ 0.5074389  -0.34747636 -0.96923184  0.410563  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3199. State = [[-0.23468538  0.08027855  0.05573455  1.        ]]. Action = [[ 0.8766483  -0.22672665 -0.99766225  0.4406575 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3200. State = [[-0.21450141  0.07793807  0.02617198  1.        ]]. Action = [[ 0.9311435  -0.62461066 -0.9295194   0.40200937]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3201. State = [[-0.21136238  0.07749686  0.02223429  1.        ]]. Action = [[ 0.906307   -0.48241436 -0.9757284   0.44578195]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3202. State = [[-0.21063387  0.07738968  0.0217904   1.        ]]. Action = [[ 0.95944273 -0.5496447  -0.9801231   0.37349498]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3203. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.77111185 -0.5509373  -0.98931026  0.38352168]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3204. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.9588691  -0.57760817 -0.8853108   0.38654768]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3205. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.8851036  -0.57150537 -0.9570586   0.4508524 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3206. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.8403816  -0.5604021  -0.90877384  0.3889618 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3207. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.85209894 -0.60738003 -0.98317266  0.49772406]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3208. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.90265024 -0.48283148 -0.98792255  0.48137975]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3209. State = [[-0.2106878   0.07737701  0.02178357  1.        ]]. Action = [[ 0.9144428  -0.5784664  -0.96826476  0.36249185]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3210. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.93655956 -0.5379184  -0.9805184   0.34787118]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3211. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9369118  -0.6085457  -0.85931003  0.38905668]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3212. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.90315866 -0.59570867 -0.97440505  0.4013288 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3213. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.88190866 -0.5649257  -0.9505059   0.44388938]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3214. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.6178913  -0.45540452 -0.96826965  0.30609822]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3215. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.93070734 -0.5033572  -0.9683019   0.3979373 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3216. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8800417  -0.6127039  -0.97385263  0.3592826 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3217. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9005835  -0.60869175 -0.96870404  0.31027818]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3218. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8002238 -0.5245017 -0.8072234  0.3154496]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3219. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.90767956 -0.5462925  -0.85371584  0.4236709 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3220. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7005949  -0.56085575 -0.9661401   0.3956213 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3221. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.84559083 -0.2526772  -0.92663753  0.3286277 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3222. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9251771 -0.3834635 -0.9354071  0.3746879]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3223. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.87854254 -0.46442038 -0.85435766  0.43709064]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3224. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8739778  -0.57477486 -0.92416555  0.4169402 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3225. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.75708187 -0.45708036 -0.96533537  0.35162187]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3226. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.95609736 -0.4696864  -0.9525484   0.31453717]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3227. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7039765 -0.5435385 -0.98763    0.3368572]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3228. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.90774703 -0.4908508  -0.975145    0.24025345]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3229. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.85653925 -0.64228326 -0.9584634   0.42927158]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3230. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8308269  -0.63729024 -0.8013579   0.4087845 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3231. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9184003  -0.6061566  -0.94825524  0.32144654]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3232. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.97292614 -0.5456546  -0.9585917   0.31963682]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3233. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.87273717 -0.5244582  -0.97282726  0.46153426]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3234. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8962685  -0.52651507 -0.7504089   0.33176804]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3235. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8605411  -0.52207464 -0.9560901   0.27019215]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3236. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.95853925 -0.61314327 -0.9869513   0.35815883]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3237. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.94716644 -0.5792037  -0.9520072   0.40493608]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3238. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.97997284 -0.66707546 -0.99576455  0.38623643]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3239. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9676331  -0.6665565  -0.97567827  0.4649527 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3240. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9400823  -0.6066128  -0.8877164   0.51941276]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3241. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.5490221  -0.6350934  -0.9866178   0.49253047]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 3242. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9527178  -0.65812105 -0.94017714  0.47560763]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3243. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9090457  -0.63460296 -0.9819779   0.42116284]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3244. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.6463151  -0.6559611  -0.89834356  0.42980802]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3245. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.77812576 -0.5894794  -0.92516017  0.426615  ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3246. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9227922  -0.6522703  -0.84743214  0.41381288]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3247. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8742671  -0.5676732  -0.98822486  0.3447578 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3248. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.6358453  -0.48359036 -0.9182827   0.442631  ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3249. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7805686  -0.59119314 -0.95647216  0.3717854 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3250. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.859419  -0.5226696 -0.9687362  0.4228289]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3251. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.948663   -0.5856017  -0.9633139   0.36286473]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3252. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.75593936 -0.49212027 -0.9663066   0.38434243]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3253. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.88369966 -0.6064087  -0.9534909   0.44191897]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3254. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.94445634 -0.46436155 -0.9325981   0.41973722]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3255. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.73651195 -0.5897583  -0.95139724  0.40345955]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3256. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.933319   -0.45734876 -0.8841387   0.34901118]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3257. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.63954675 -0.38803184 -0.97166586  0.40274823]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3258. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.913074   -0.5102796  -0.90484744  0.4591353 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3259. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8357254  -0.5885121  -0.95801985  0.35189784]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3260. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9212079  -0.30406344 -0.6388778   0.41213167]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3261. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.84277546 -0.53035235 -0.98076403  0.37168646]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3262. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.35261416 -0.53277767 -0.9564923   0.33547187]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3263. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8965719  -0.41765618 -0.97580034  0.43964458]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3264. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.86194396 -0.45453763 -0.94875854  0.36247015]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3265. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8268714 -0.6688925 -0.9698446  0.4567871]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3266. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.92913723 -0.57378757 -0.95064604  0.44933236]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3267. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9141431  -0.47235012 -0.97006506  0.41959524]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3268. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9716387  -0.621915   -0.9464881   0.48352766]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3269. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7422824  -0.5944434  -0.96836746  0.4581251 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3270. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9614091  -0.45130146 -0.9497698   0.39986157]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3271. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9443226  -0.5363577  -0.9082102   0.45069742]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3272. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.6829206 -0.6346416 -0.9559363  0.4115858]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3273. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9829216  -0.4888947  -0.87770814  0.38671088]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3274. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8778608  -0.6822217  -0.9372625   0.45179796]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3275. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.79080653 -0.6055411  -0.9349696   0.33173418]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3276. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.93798804 -0.5127641  -0.9172636   0.35422945]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3277. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9408164 -0.6176366 -0.8723857  0.4346881]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3278. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9318342 -0.7212894 -0.9887666  0.387084 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3279. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8638599  -0.4504934  -0.9092348   0.43418002]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3280. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9328121  -0.68499225 -0.96234983  0.3965491 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3281. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9407959  -0.6108099  -0.91979855  0.38676667]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3282. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9316517  -0.52063054 -0.9304693   0.45168257]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3283. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[-0.4286741  -0.5868125  -0.98796016  0.47290516]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3284. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.92819023 -0.64122885 -0.88337487  0.35770798]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3285. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9659548  -0.61234915 -0.9681245   0.50910413]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3286. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.99045706 -0.47553885 -0.87313575  0.40807462]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3287. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.93099844 -0.5666285  -0.9904108   0.4779501 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3288. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.87567854 -0.6742519  -0.9363926   0.4087348 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3289. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9494567  -0.55881757 -0.9821856   0.42829835]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3290. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7183137  -0.46207225 -0.91352475  0.4177754 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3291. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.97095513 -0.55824727 -0.9055511   0.39237475]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3292. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7876291 -0.6252854 -0.9561039  0.3721218]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 3293. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.9089949  -0.47586966 -0.9101369   0.3041185 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3294. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.7650645  -0.62904185 -0.9264225   0.27840436]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3295. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.6535578  -0.5631607  -0.58171535  0.41281867]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3296. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.89674485 -0.5326427  -0.9793775   0.43768167]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3297. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8853719  -0.51427215 -0.9404927   0.5012766 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3298. State = [[-0.2106131   0.07738803  0.02179763  1.        ]]. Action = [[ 0.8274746  -0.57464284 -0.9872367   0.49658358]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3299. State = [[-0.25879964 -0.08021186  0.11718996  1.        ]]. Action = [[ 0.37280035 -0.59640723 -0.98900306  0.44225168]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3300. State = [[-0.25189832 -0.09143417  0.10540334  1.        ]]. Action = [[ 0.57818043 -0.20449078  0.30219686  0.42751288]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3301. State = [[-0.23979144 -0.10126651  0.10542037  1.        ]]. Action = [[ 0.30602288 -0.36695242 -0.26990974  0.60525227]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3302. State = [[-0.22372158 -0.11642808  0.10519739  1.        ]]. Action = [[ 0.7293048  -0.33591962  0.02168977  0.49127758]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3303. State = [[-0.20354511 -0.12422124  0.10212222  1.        ]]. Action = [[ 0.45900345 -0.01894343 -0.06958622  0.53056836]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 3304. State = [[-0.18505833 -0.12812848  0.10343877  1.        ]]. Action = [[ 0.44473946 -0.06598604  0.35806322  0.660017  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 3305. State = [[-0.17227767 -0.13073167  0.10852554  1.        ]]. Action = [[ 0.36523783 -0.48150665  0.86473846  0.4446758 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 3306. State = [[-0.1701719  -0.13115868  0.1092938   1.        ]]. Action = [[0.33411288 0.17187631 0.26300097 0.5915108 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 3307. State = [[-0.16993625 -0.1312169   0.10937405  1.        ]]. Action = [[ 0.40312922 -0.40134406 -0.6355307   0.4354061 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 3308. State = [[-0.1698922  -0.1312352   0.10938703  1.        ]]. Action = [[ 0.44557405 -0.22190857  0.69893754  0.20648217]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 3309. State = [[-0.16991112 -0.13121837  0.10942989  1.        ]]. Action = [[ 0.34930205 -0.2582432  -0.05797625  0.3877858 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 3310. State = [[-0.16993801 -0.13121216  0.10942824  1.        ]]. Action = [[0.25989676 0.09147263 0.27210033 0.40145802]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 3311. State = [[-0.16993801 -0.13121216  0.10942824  1.        ]]. Action = [[ 0.2966957  -0.23104978  0.25542688  0.37141287]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 3312. State = [[-0.16993801 -0.13121216  0.10942824  1.        ]]. Action = [[0.58393145 0.01277435 0.05940211 0.50161004]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 3313. State = [[-0.1698046  -0.13123865  0.10949101  1.        ]]. Action = [[ 0.49510503 -0.2928148  -0.6563758   0.56020224]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 3314. State = [[-0.1698046  -0.13123865  0.10949101  1.        ]]. Action = [[ 0.35149074 -0.03248161 -0.1093654   0.38737583]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 3315. State = [[-0.1698046  -0.13123865  0.10949101  1.        ]]. Action = [[ 0.36366904 -0.12724245 -0.97193664  0.4058621 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 3316. State = [[-0.1698046  -0.13123865  0.10949101  1.        ]]. Action = [[ 0.3221861 -0.3619827 -0.6024003  0.4021392]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 3317. State = [[-0.1698046  -0.13123865  0.10949101  1.        ]]. Action = [[ 0.6020889  -0.2210648   0.08834958  0.41352606]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 3318. State = [[-0.1698046  -0.13123865  0.10949101  1.        ]]. Action = [[ 0.09752285 -0.34843063  0.04926503  0.27146137]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 3319. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.53919697 -0.24434066 -0.728478    0.29931474]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 3320. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.5169525  -0.2704329  -0.13508761  0.44272494]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 3321. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.4325025  -0.12382174  0.68890166  0.28074074]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 3322. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.5007173  -0.16038477 -0.01314223  0.4076805 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 3323. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.50696325 -0.2133559  -0.3047647   0.4366696 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 3324. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.46842766 -0.3031634   0.11757827  0.48255086]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 3325. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.3735087  -0.2077682  -0.34172213  0.37369764]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 3326. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.28950334 -0.2885425   0.08909571  0.45357692]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 3327. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.22002864 -0.22358537  0.40088558  0.45110464]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 3328. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.16842031 -0.18999434 -0.79351664  0.4162352 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 3329. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.48020458 -0.18222058  0.7395339   0.3323493 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 3330. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.22556853 -0.45773864  0.17800999  0.4634068 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 3331. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.43419635 -0.0472796   0.0124017   0.3940766 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 3332. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.53505707 -0.3271268  -0.49394405  0.27466083]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 3333. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.04872751 -0.1825602  -0.03181142  0.43274522]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 3334. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.466897   -0.195431   -0.09268999  0.44150257]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 3335. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.25869608 -0.03909302  0.274642    0.338562  ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 3336. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.5648701  -0.27384102 -0.6366769   0.33064103]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 3337. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.18958306 -0.3221386   0.7493787   0.18395185]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 3338. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.3566718  -0.20588255 -0.1854186   0.32262266]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 3339. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.63654697 -0.42526472  0.16671681  0.38473344]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 3340. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[0.82540274 0.05036879 0.3250203  0.48024976]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 3341. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.14689565  0.03076267 -0.5403195   0.33457196]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 3342. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.641448   -0.12061918  0.6472924   0.48148108]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 3343. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.42439187 -0.13854373  0.0773946   0.5473933 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 3344. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.52455616 -0.17511708  0.41346502  0.2678063 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 3345. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.38279474 -0.23260158 -0.64698136  0.40098655]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 3346. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.7017232  -0.37864423  0.5774307   0.33259594]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 3347. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.41469646 -0.09550542  0.06351352  0.36985397]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 3348. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.43730903 -0.25912136  0.6063535   0.31585598]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 3349. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.595973   -0.2674271  -0.47424823  0.30689788]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 3350. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[0.13363814 0.02734947 0.608073   0.44594622]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 3351. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.51377726 -0.20314997  0.03541374  0.30777586]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 3352. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.30392766 -0.27012193  0.5817598   0.30837798]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 3353. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.40973568 -0.26778626  0.32364714  0.19038486]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 3354. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.28512406 -0.3849662  -0.33883423  0.4532888 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 3355. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.13511693 -0.2344597   0.6285964   0.39087296]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 3356. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.59288526 -0.34635687 -0.06189704  0.43207383]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 3357. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.14409924 -0.18988585 -0.16217905  0.2975632 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 3358. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.5785258  -0.18795991 -0.12657845  0.2604655 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 3359. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.58935213 -0.04145831  0.92146516  0.4045819 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 3360. State = [[-0.16973315 -0.13125372  0.1095136   1.        ]]. Action = [[ 0.51904774 -0.27423674  0.7881228   0.45660615]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 3361. State = [[-0.1699857  -0.13258384  0.10962886  1.        ]]. Action = [[-0.18598402 -0.17066628  0.04886448  0.57063556]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 3362. State = [[-0.17073087 -0.13392803  0.10975038  1.        ]]. Action = [[ 0.60803354 -0.28120077  0.49987328  0.35260427]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 3363. State = [[-0.17081557 -0.13411613  0.10977626  1.        ]]. Action = [[ 0.39854383 -0.3061973   0.76591873  0.6791034 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 3364. State = [[-0.17085415 -0.13424607  0.10979463  1.        ]]. Action = [[ 0.74369025 -0.27473265  0.39285696  0.39089525]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 3365. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.58639395 -0.40699422  0.9692943   0.60487247]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 3366. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.39799643 -0.23451942  0.46133947  0.5559149 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 3367. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.6262739  -0.25865602  0.95430756  0.4468037 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 3368. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.3690008  -0.2969234   0.8619726   0.48565626]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 3369. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.6669891  -0.19537348  0.6107352   0.3723781 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 3370. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.32089806 -0.28712988  0.92502356  0.43199027]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 3371. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.49710536  0.02307475 -0.35682964  0.5228772 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 3372. State = [[-0.17087345 -0.13431102  0.10980383  1.        ]]. Action = [[ 0.3992349  -0.32962334  0.64870405  0.29966855]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 3373. State = [[-0.16999231 -0.1405532   0.11952958  1.        ]]. Action = [[-0.02753007 -0.32830048  0.8534845   0.358845  ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 3374. State = [[-0.17032857 -0.1469896   0.13438278  1.        ]]. Action = [[ 0.39961767 -0.04244703  0.75316095  0.5006285 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 3375. State = [[-0.17052647 -0.14882609  0.13534322  1.        ]]. Action = [[ 0.40863013 -0.38403064  0.03089881  0.32278967]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 3376. State = [[-0.17065442 -0.14886045  0.13534743  1.        ]]. Action = [[ 0.2765659  -0.3327915  -0.5702871   0.34878278]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 3377. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.21097708 -0.35779727  0.25772095  0.5187763 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 3378. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.32222724 -0.26311803 -0.3826993   0.2804513 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 3379. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.5446     -0.2552476  -0.07726145  0.45378006]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 3380. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[0.10605443 0.04105079 0.4829166  0.33265638]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 3381. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.38828254 -0.21199024 -0.8705522   0.3623563 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 3382. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.2519616  -0.12476897 -0.750453    0.36788535]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 3383. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.28624487 -0.26189303  0.5254519   0.39879715]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 3384. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.5675707  -0.09782279  0.00636959  0.30210555]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 3385. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.45647776 -0.15399814  0.44646955  0.36934793]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 3386. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.30252802 -0.23123449 -0.7857882   0.47624588]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 3387. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.18553698 -0.29950738  0.5792749   0.56055975]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 3388. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.38540077 -0.43680644  0.7893884   0.3590201 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 3389. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.5595269  -0.27434045 -0.38234007  0.44296432]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 3390. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.44149828 -0.19083726  0.6311965   0.45097136]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 3391. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.36088204 -0.21994877  0.85663915  0.42562008]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 3392. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.5468905  -0.08667088 -0.60895234  0.4537791 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 3393. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.21744478 -0.09961587  0.05333674  0.3762349 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 3394. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.36423695 -0.25349736 -0.5837888   0.41421688]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 3395. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.60992837 -0.11537451  0.29496777  0.3188566 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 3396. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.45984483 -0.24285549 -0.12995017  0.51103127]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 3397. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.3355099  -0.05985785  0.13117409  0.44696426]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 3398. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.52554774 -0.18601108  0.34729195  0.3101133 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 3399. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.4223324  -0.21523494 -0.30056804  0.49284434]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 3400. State = [[-0.170813   -0.14881718  0.13533911  1.        ]]. Action = [[ 0.56599927 -0.21023077 -0.38622195  0.47683907]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 3401. State = [[-0.26614496  0.09895532  0.12256251  1.        ]]. Action = [[ 0.47199357 -0.24403757  0.35956955  0.5008812 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 3402. State = [[-0.26249242  0.11133385  0.10931523  1.        ]]. Action = [[-0.23540592 -0.47251028 -0.99152577  0.28574193]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 3403. State = [[-0.25416446  0.10156266  0.10417013  1.        ]]. Action = [[ 0.8450806  -0.8132119  -0.94375914  0.24372637]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3404. State = [[-0.23644824  0.09056488  0.07658044  1.        ]]. Action = [[-0.6760658 -0.5829975 -0.9765618  0.1812967]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3405. State = [[-0.23336579  0.08759632  0.07195912  1.        ]]. Action = [[-0.7732895  -0.72966653 -0.7788945   0.19954121]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3406. State = [[-0.23322935  0.08651029  0.07187068  1.        ]]. Action = [[ 0.75782907 -0.66305727 -0.9791004   0.20054841]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3407. State = [[-0.23311633  0.08625433  0.07190575  1.        ]]. Action = [[-0.9998746  -0.6407319  -0.9824808   0.20490468]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3408. State = [[-0.23306406  0.08584917  0.07193217  1.        ]]. Action = [[ 0.12310958 -0.62174886 -0.9557928   0.1852212 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3409. State = [[-0.23305535  0.08578156  0.0719366   1.        ]]. Action = [[ 0.89066553 -0.7496371  -0.90788096  0.25494576]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3410. State = [[-0.23305535  0.08578156  0.0719366   1.        ]]. Action = [[ 0.7594323  -0.83251935 -0.9281813   0.19686306]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3411. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.77579486 -0.7525656  -0.97914827  0.18451393]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3412. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.8607325  -0.79169023 -0.97242725  0.17871606]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3413. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.9705891  -0.73744774 -0.9837836   0.20849013]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3414. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.91866875 -0.76658213 -0.99582684  0.25447845]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3415. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.09536576 -0.82632035 -0.9387825   0.19489479]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3416. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.99999833 -0.7641696  -0.94710296  0.12673128]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3417. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.9492127 -0.6565581 -0.99764    0.1813358]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3418. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.34418178 -0.8321112  -0.96888053  0.20658565]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3419. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.8482403  -0.797792   -0.8262554   0.23209012]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3420. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.9994719  -0.8338986  -0.9528776   0.25205445]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3421. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.20497525 -0.57956815 -0.9386283   0.12793362]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3422. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.38158286 -0.76607746 -0.979714    0.22778535]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3423. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.9804723  -0.6105197  -0.9790836   0.28241968]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3424. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.9996686  -0.6962502  -0.9549143   0.22817004]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3425. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.84838367 -0.8325361  -0.95595026  0.2510103 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3426. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.99974364 -0.55802834 -0.9408122   0.16748023]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3427. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.7216156 -0.6076051 -0.967512   0.2029345]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3428. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.50259256 -0.84814376 -0.9559694   0.22847342]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3429. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.98851794 -0.68559754 -0.9037596   0.15808654]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3430. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.6673825  -0.720591   -0.9645041   0.20881557]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3431. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.8320331  -0.71294403 -0.9623118   0.19749367]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3432. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.5958459  -0.73151284 -0.97380644  0.21679878]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3433. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.97023726 -0.62543494 -0.96815234  0.14949238]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3434. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.9820263 -0.6698971 -0.8766723  0.1627779]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3435. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.464144   -0.532337   -0.9688933   0.15994227]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3436. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.537321   -0.6837335  -0.9187548   0.17152715]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3437. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.81847656 -0.63024986 -0.9775502   0.19329536]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3438. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.996768   -0.8208646  -0.98264015  0.17474854]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 3439. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.8212246 -0.734119  -0.9451755  0.2008555]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3440. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.59631157 -0.8512605  -0.9640329   0.14301002]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 3441. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.9998878  -0.79877836 -0.9029143   0.15238702]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3442. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.6512437  -0.734788   -0.99589014  0.20342875]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3443. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[-0.54265165 -0.73656917 -0.9913765   0.19509733]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3444. State = [[-0.23298174  0.08579468  0.07195321  1.        ]]. Action = [[ 0.9322972  -0.6047591  -0.9244362   0.16110682]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3445. State = [[-0.2211016   0.07714474  0.06650073  1.        ]]. Action = [[ 0.9990126  -0.6298516  -0.85689574  0.16081572]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3446. State = [[-0.1990645   0.06693777  0.04218222  1.        ]]. Action = [[-0.9435196  -0.75499254 -0.99068403  0.15176547]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3447. State = [[-0.19467208  0.0662407   0.04067338  1.        ]]. Action = [[-0.9994158  -0.800703   -0.96996874  0.08555627]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3448. State = [[-0.19355586  0.06604861  0.04059099  1.        ]]. Action = [[-0.9962417  -0.7760283  -0.92653483  0.136747  ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3449. State = [[-0.19344507  0.06583959  0.04039039  1.        ]]. Action = [[-0.7317117  -0.6196756  -0.99242616  0.1299175 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3450. State = [[-0.19334699  0.06577933  0.04014041  1.        ]]. Action = [[-0.9995633  -0.7940745  -0.9658721   0.13240778]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3451. State = [[-0.19352542  0.06575368  0.03986095  1.        ]]. Action = [[ 0.81347656 -0.67253715 -0.7544884   0.16005313]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3452. State = [[-0.19352542  0.06575368  0.03986095  1.        ]]. Action = [[ 0.9169872  -0.6637807  -0.9648852   0.13501596]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3453. State = [[-0.19352542  0.06575368  0.03986095  1.        ]]. Action = [[ 0.25413656 -0.6126876  -0.94758165  0.26224065]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3454. State = [[-0.19352542  0.06575368  0.03986095  1.        ]]. Action = [[-0.83282584 -0.80850476 -0.81679624  0.21934795]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3455. State = [[-0.19352542  0.06575368  0.03986095  1.        ]]. Action = [[-0.88272804 -0.9122784  -0.989919    0.17329848]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3456. State = [[-0.19352542  0.06575368  0.03986095  1.        ]]. Action = [[ 0.15623808 -0.70968795 -0.985759    0.22477949]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3457. State = [[-0.1936147   0.06574085  0.03972127  1.        ]]. Action = [[-0.40383214 -0.82475793 -0.9001736   0.10766482]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 3458. State = [[-0.19364466  0.06573654  0.03967442  1.        ]]. Action = [[-0.65376735 -0.4985999  -0.79092777  0.12359452]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 3459. State = [[-0.19370402  0.065728    0.03958161  1.        ]]. Action = [[-0.9846918  -0.81075644 -0.936707    0.16829896]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3460. State = [[-0.19370402  0.065728    0.03958161  1.        ]]. Action = [[-0.98468554 -0.54544705 -0.95732135  0.13546014]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 3461. State = [[-0.19370402  0.065728    0.03958161  1.        ]]. Action = [[-0.38243663 -0.75168514 -0.99554384  0.08047402]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3462. State = [[-0.19370402  0.065728    0.03958161  1.        ]]. Action = [[-0.657184   -0.85896456 -0.8864529   0.14831913]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 3463. State = [[-0.19361643  0.06559108  0.03961069  1.        ]]. Action = [[-0.96616894 -0.70410234 -0.9862452   0.11792469]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3464. State = [[-0.19363979  0.06551543  0.03956955  1.        ]]. Action = [[ 0.69278073 -0.9032406  -0.9244931   0.1549753 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 3465. State = [[-0.19363979  0.06551543  0.03956955  1.        ]]. Action = [[ 0.17969406 -0.6228085  -0.9700631   0.12381136]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3466. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9556307  -0.7379578  -0.98812026  0.11808312]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3467. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9616299  -0.6588763  -0.8566608   0.11576283]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3468. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.20435792 -0.72498894 -0.92460734  0.13632202]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3469. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.19648874 -0.580422   -0.899078    0.12273216]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3470. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.5974023  -0.52522653 -0.9821581   0.15322316]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 3471. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.6747738  -0.6028697  -0.96396697  0.11605239]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3472. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.42620653 -0.76842743 -0.9723919   0.13953137]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3473. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.8910505  -0.805795   -0.8930323   0.01090169]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3474. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.86072636 -0.62645304 -0.9975231   0.04812956]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3475. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.70110995 -0.79046404 -0.846313    0.03045332]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3476. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.91033745 -0.8140261  -0.98909456 -0.01958627]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3477. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.7244764  -0.65894896 -0.9825748  -0.06381154]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 3478. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.7257585  -0.6626614  -0.98013204 -0.13156044]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3479. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.8318653  -0.7764253  -0.9897015  -0.13257807]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3480. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.7972062  -0.8040705  -0.996963   -0.11576939]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3481. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.8836895  -0.781045   -0.99371    -0.22073019]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3482. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.86118084 -0.8596433  -0.9948492  -0.16274202]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3483. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.8278514  -0.8173693  -0.9990399  -0.19829482]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3484. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.90659815 -0.84128845 -0.99376243 -0.19833302]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 3485. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9129595  -0.7458859  -0.99936926 -0.26002514]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3486. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9089658  -0.8262838  -0.99830675 -0.30827624]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 3487. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9388579  -0.87148273 -0.99102515 -0.326262  ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3488. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.8991912  -0.7396409  -0.99268144 -0.3793465 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3489. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9057028  -0.8338322  -0.9546958  -0.42284477]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 3490. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.96208715 -0.79866284 -0.9976452  -0.45418066]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 3491. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.94977784 -0.79715633 -0.9960508  -0.4736104 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3492. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.93571347 -0.655413   -0.9989305  -0.5274321 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3493. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.95764816 -0.8110081  -0.9956358  -0.5872404 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 3494. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.93462205 -0.8871945  -0.99900115 -0.5149822 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3495. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.97201955 -0.87689495 -0.979091   -0.5848877 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 3496. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9738358  -0.96952754 -0.97508097 -0.61472017]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 3497. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9798837 -0.8814776 -0.9982004 -0.6683675]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3498. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.95872474 -0.9302538  -0.93858856 -0.7658671 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3499. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.96374106 -0.9512883  -0.9963518  -0.7348183 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3500. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.95348525 -0.90161705 -0.94548863 -0.7344799 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3501. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.96134824 -0.8525555  -0.9946354  -0.7371503 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 3502. State = [[-0.1936332   0.06544456  0.03957523  1.        ]]. Action = [[-0.9741544  -0.913205   -0.96934664 -0.757749  ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3503. State = [[-0.26508346  0.06922495  0.11947304  1.        ]]. Action = [[-0.9599979  -0.77515477 -0.9996276  -0.7692423 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3504. State = [[-0.2650399   0.07633737  0.10657204  1.        ]]. Action = [[-0.91856086 -0.73129594 -0.9840143  -0.6065095 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 3505. State = [[-0.2650399   0.07633737  0.10657204  1.        ]]. Action = [[-0.9388405  -0.90487474 -0.9927092  -0.71088105]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3506. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.96409655 -0.62872237 -0.99917877 -0.67706335]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3507. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.95226794 -0.77053666 -0.9985721  -0.65461856]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3508. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.89732724 -0.66243327 -0.9940069  -0.7075635 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3509. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9531523  -0.47162616 -0.99962544 -0.64566165]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3510. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.91989243 -0.69046056 -0.99915034 -0.6724169 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3511. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.95549685 -0.76723236 -0.9991685  -0.6255703 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3512. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.93921673 -0.5402277  -0.99731493 -0.66318995]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3513. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.92813927 -0.70566994 -0.9986652  -0.68809813]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3514. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.93347067 -0.6784556  -0.9924363  -0.6499287 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3515. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.92035824 -0.41264105 -0.9871862  -0.6636901 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3516. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94717985 -0.46186495 -0.8663488  -0.69486505]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3517. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9487434 -0.5513591 -0.9987348 -0.6656331]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3518. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9642498  -0.6316321  -0.9966016  -0.69560343]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3519. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94853866 -0.58924073 -0.99290556 -0.6869991 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3520. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9381973 -0.4665339 -0.9897378 -0.7506178]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3521. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.95067704 -0.7385603  -0.99690896 -0.6528062 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3522. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94083035 -0.5144897  -0.99262536 -0.6787199 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3523. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9348415  -0.53860426 -0.98276013 -0.70331836]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3524. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.95439065 -0.53398603 -0.95931876 -0.65230066]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3525. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.945471   -0.54218525 -0.98916745 -0.6863914 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3526. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9522995  -0.45355868 -0.98713523 -0.72656155]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3527. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9567485  -0.5552105  -0.99593556 -0.7603772 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3528. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94822186 -0.594532   -0.97827435 -0.74310666]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3529. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.96678835 -0.4459765  -0.9953529  -0.78720397]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3530. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9534763  -0.61088824 -0.9552761  -0.728928  ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3531. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.95686203 -0.25010377 -0.97844    -0.76420677]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3532. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9319075  -0.48425758 -0.97635585 -0.78172237]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3533. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9801028  -0.5279063  -0.981962   -0.71026397]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3534. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.97851807 -0.52779067 -0.98545367 -0.722572  ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3535. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.93780303 -0.3214314  -0.9895349  -0.75011355]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3536. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.95319873 -0.3489492  -0.9831159  -0.68009585]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3537. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94631374 -0.44392025 -0.9916101  -0.72826296]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3538. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94934213 -0.34696364 -0.97519356 -0.69412017]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3539. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9467629  -0.43655002 -0.9758506  -0.7109812 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3540. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9371846  -0.38221025 -0.97869295 -0.67134154]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 3541. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.96247035 -0.4495119  -0.9923518  -0.7280597 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3542. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94465536 -0.48202413 -0.99748456 -0.6863047 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 3543. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.96234083 -0.26486212 -0.94360524 -0.7028981 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3544. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8830606 -0.419549  -0.9978229 -0.7575291]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3545. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9193158  -0.4498154  -0.9795707  -0.67311585]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3546. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8884212  -0.4506756  -0.9964059  -0.59892935]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3547. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9424899  -0.3107773  -0.90720713 -0.610091  ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 3548. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.91715676 -0.36616325 -0.992635   -0.5578629 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3549. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.94242644 -0.27145875 -0.98619497 -0.59199846]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3550. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9686813  -0.39870095 -0.9712008  -0.5170562 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3551. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9440003  -0.20326155 -0.93418926 -0.64476264]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3552. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8775664  -0.04884332 -0.9810491  -0.58231837]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3553. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9291562  -0.32271343 -0.9826225  -0.49618208]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3554. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.86325765 -0.2645023  -0.9953909  -0.39995968]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3555. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8840781  -0.37236714 -0.97402155 -0.59417206]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3556. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.9275503  -0.13303143 -0.96311665 -0.55002177]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3557. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8549927  -0.19038463 -0.99473107 -0.41495788]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3558. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.881033   -0.16029716 -0.94134325 -0.4972422 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3559. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.801814   -0.07468033 -0.97594327 -0.3740822 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 3560. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.83254063 -0.1584428  -0.98557967 -0.39313352]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 3561. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.870195   -0.11604822 -0.95126945 -0.40682536]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3562. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8267363  -0.2707296  -0.9691003  -0.45604014]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 3563. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8870151  -0.22755837 -0.8817086  -0.41660172]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3564. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8562467  -0.38963842 -0.964892   -0.40735364]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 3565. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8256291  -0.0197351  -0.9871736  -0.44207764]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3566. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.87866306 -0.3899021  -0.9796975  -0.44088578]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 3567. State = [[-0.26506388  0.07638246  0.10657205  1.        ]]. Action = [[-0.8724255  -0.23662889 -0.9669865  -0.31929398]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3568. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8771402  -0.22419953 -0.9031811  -0.4857875 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3569. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8953403  -0.44826734 -0.99317235 -0.52015114]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3570. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8880105  -0.36224192 -0.9797018  -0.47754306]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3571. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.91909546 -0.20848298 -0.9801439  -0.4671222 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3572. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.9229847  -0.24602628 -0.9554475  -0.5674065 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 3573. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.91273075 -0.3848244  -0.91318613 -0.4422183 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3574. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8704649  -0.22605848 -0.8692125  -0.5796978 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3575. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.920623   -0.37955034 -0.9736671  -0.5677911 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3576. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8638841  -0.22681755 -0.8349103  -0.5143218 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3577. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8659082 -0.2705158 -0.9744429 -0.6738521]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3578. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.9132421  -0.04166591 -0.9743938  -0.5917214 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3579. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.869003   -0.29146814 -0.9931562  -0.7097313 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 3580. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.84684193 -0.29459882 -0.97490406 -0.6475939 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3581. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8695029  -0.45306122 -0.9490818  -0.55535233]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3582. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8998565  -0.26994407 -0.9706142  -0.5934152 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3583. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.845329  -0.2689978 -0.909579  -0.483701 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3584. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.77768105 -0.23218197 -0.9803382  -0.5254156 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3585. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.7817069  -0.19374347 -0.9379163  -0.3545668 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3586. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.79459226 -0.1848296  -0.98698515 -0.3753835 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 3587. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.7767638  -0.32238328 -0.99285364 -0.4943033 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3588. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.8465518  -0.07685918 -0.98997855 -0.43822318]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 3589. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.7286407  -0.3455999  -0.9668562  -0.30220413]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3590. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.7333318  -0.2295134  -0.94747466 -0.33443034]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3591. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.73372793 -0.17660677 -0.9845061  -0.33370817]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 3592. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.77420783 -0.16164601 -0.9574461  -0.2585274 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 3593. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.77072227 -0.13098192 -0.9488681  -0.3738643 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3594. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.6989116  -0.00386715 -0.97916716 -0.34589112]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3595. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.5821651  -0.0213331  -0.93610764 -0.3263778 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 3596. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.6883109  -0.09147662 -0.96907943 -0.31922978]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3597. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.5563887  -0.10900706 -0.9884762  -0.28769565]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 3598. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.58016557 -0.10460937 -0.98571867 -0.33966732]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 3599. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.613725   -0.16269302 -0.9829239  -0.18852586]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3600. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.4372285  -0.00890905 -0.997726   -0.06891674]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3601. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.5580912  -0.21599352 -0.9517175  -0.03751063]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3602. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.62330943 -0.1941284  -0.9683031  -0.14539546]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3603. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-5.4856259e-01 -1.1305040e-01 -9.5758927e-01  9.4103813e-04]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 3604. State = [[-0.26508674  0.07637904  0.10653265  1.        ]]. Action = [[-0.55226576 -0.16142279 -0.95641375 -0.22173357]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3605. State = [[-0.25330725 -0.00232387  0.1231476   1.        ]]. Action = [[-0.63894296 -0.14020276 -0.82259136 -0.09280896]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3606. State = [[-0.24882296 -0.00332338  0.10794637  1.        ]]. Action = [[ 0.3383212   0.1240145  -0.18205404  0.21205688]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3607. State = [[-0.26368275  0.11770424  0.12453801  1.        ]]. Action = [[ 0.02656746  0.017941   -0.45918834 -0.06209821]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3608. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.3776363  -0.01259238 -0.9821847   0.10302424]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 3609. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5968423  -0.10918206 -0.92695993 -0.23437881]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3610. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.69629186 -0.15792239 -0.98141205 -0.3214981 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3611. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6557599  -0.20703489 -0.9866146  -0.26515353]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3612. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7643583  -0.12563664 -0.9879723  -0.1938026 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3613. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6645716  -0.29037964 -0.98791516 -0.3519534 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3614. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7433067  -0.24512553 -0.9664839  -0.41057438]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3615. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7080486  -0.25826347 -0.996373   -0.38004148]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3616. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6217292  -0.09585404 -0.9510737  -0.18697327]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3617. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.4978696  -0.15205842 -0.95397604 -0.39267677]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3618. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5292413  -0.12587553 -0.9129432  -0.2898081 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3619. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5466451  -0.16772544 -0.8076467  -0.2160368 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3620. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.66202086 -0.27731544 -0.9980592  -0.16780412]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3621. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6620264  -0.3359623  -0.90741634 -0.22287422]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3622. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6844389  -0.2101208  -0.9837393  -0.32783014]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3623. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.75063384 -0.2202493  -0.9236026  -0.16550744]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3624. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6588901  -0.18630558 -0.9978242  -0.2117582 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3625. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.74725884 -0.2660749  -0.879944   -0.2908126 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3626. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.74241567 -0.08321327 -0.9903322  -0.28366977]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3627. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7740276  -0.2335198  -0.90812945 -0.3126794 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3628. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.64085495 -0.24479538 -0.9917287  -0.21198511]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3629. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.70085764 -0.14545983 -0.9484094  -0.2955352 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3630. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7921059  -0.23351532 -0.95912194 -0.42070305]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3631. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.753862   -0.30459553 -0.9950986  -0.34427178]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3632. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.68917483 -0.31306314 -0.95377934 -0.358348  ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3633. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.730715   -0.14708567 -0.9359078  -0.3789425 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3634. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.73717123 -0.38036984 -0.99801886 -0.5410537 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3635. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.63764185 -0.3342036  -0.9810941  -0.5139013 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3636. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7621983  -0.41505128 -0.76253694 -0.30688852]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3637. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.75901175 -0.25351036 -0.926194   -0.51813054]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3638. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.64367354 -0.2829814  -0.97348845 -0.47780228]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3639. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.76787144 -0.30666047 -0.995134   -0.3755105 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3640. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7500413  -0.3893866  -0.9946056  -0.27140987]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3641. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.63254493 -0.3224188  -0.9967502  -0.25254536]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3642. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.4008242  -0.29643524 -0.84479105 -0.2892205 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3643. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5417171  -0.23358434 -0.9978808  -0.15670931]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3644. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.52843994 -0.2556733  -0.81895226 -0.02088571]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 3645. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.39215446 -0.0373463  -0.9948191  -0.2773136 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3646. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.49458766 -0.20219684 -0.98579025 -0.2590239 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 3647. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5821238  -0.33120537 -0.9903931  -0.11978388]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3648. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.49225163 -0.3004477  -0.9594095  -0.28658676]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3649. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.40964574 -0.16460776 -0.9780405  -0.23528671]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3650. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5015654  -0.1204052  -0.9732486  -0.31396282]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3651. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.59912026 -0.33180583 -0.9814159  -0.19123381]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 3652. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.65293264 -0.422453   -0.9844487  -0.13856924]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3653. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6632739  -0.24936926 -0.99336743 -0.30082095]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3654. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.7141033  -0.14358711 -0.9838042  -0.46879923]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3655. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6174152  -0.28372544 -0.9294616  -0.3817551 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3656. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.71020573 -0.27922577 -0.9774386  -0.25416744]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3657. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.6053167  -0.16299206 -0.9893154  -0.32943708]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3658. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.5487686  -0.31550246 -0.95003873 -0.20630491]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3659. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.46008897 -0.29755569 -0.9853459  -0.17699122]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3660. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.41457427 -0.09588426 -0.985688   -0.31325078]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3661. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.35356164 -0.00896752 -0.99238896 -0.08747393]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3662. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.2401123  -0.21028984 -0.974475   -0.07516658]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3663. State = [[-0.2601453   0.1308271   0.11267727  1.        ]]. Action = [[-0.28152615 -0.08922064 -0.9936461  -0.05761629]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 3664. State = [[-0.25654614 -0.03569292  0.11775216  1.        ]]. Action = [[-0.12109995 -0.15290189 -0.9663042  -0.13302445]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 3665. State = [[-0.25536478 -0.04031127  0.10478874  1.        ]]. Action = [[0.16368496 0.11945701 0.22454453 0.49606168]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3666. State = [[-0.2506064  -0.03996141  0.09985287  1.        ]]. Action = [[ 0.26975787  0.12993038 -0.6527046   0.44941592]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3667. State = [[-0.242172   -0.03409562  0.08438297  1.        ]]. Action = [[ 0.12245286  0.31425953 -0.36673915  0.5394598 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3668. State = [[-0.23967233 -0.02896594  0.07276777  1.        ]]. Action = [[ 0.11402082 -0.03977686 -0.2457484   0.5307274 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 3669. State = [[-0.23833306 -0.0311573   0.0588068   1.        ]]. Action = [[-0.09703362 -0.2725261  -0.72308344  0.53770006]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 3670. State = [[-0.2382805  -0.03271188  0.03934817  1.        ]]. Action = [[-0.5021201  -0.35161424 -0.76909834 -0.22670096]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3671. State = [[-0.23679873 -0.03281344  0.03781271  1.        ]]. Action = [[-0.838541   -0.30292475  0.48783064  0.12804377]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3672. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.46710062 -0.5159398  -0.7590981  -0.42601693]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3673. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.9447113  -0.33028007 -0.09494859 -0.66265893]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3674. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.7816763  -0.6516283  -0.59941024 -0.30992222]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3675. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.9691619  -0.49079317 -0.92189556 -0.47636402]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3676. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.99261487 -0.61173654  0.7733221  -0.44126916]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3677. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.9227437  -0.02460378 -0.51183945 -0.6864756 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3678. State = [[-0.23660366 -0.03289381  0.03783954  1.        ]]. Action = [[-0.7527267  -0.689342    0.13162076 -0.09984642]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3679. State = [[-0.2603489   0.0094096   0.12051753  1.        ]]. Action = [[ 0.04033947 -0.6517718   0.8988471  -0.22481304]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 3680. State = [[-0.26143318  0.01456584  0.09986146  1.        ]]. Action = [[ 0.18386209  0.26020348 -0.68596596  0.15933871]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3681. State = [[-0.25405446  0.01947735  0.0740049   1.        ]]. Action = [[ 0.43282616  0.12711775 -0.91566837  0.26024902]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3682. State = [[-0.24357842  0.02256947  0.04998392  1.        ]]. Action = [[-0.22032595 -0.26717222 -0.9494285  -0.1410402 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3683. State = [[-0.25434244 -0.10959896  0.1240177   1.        ]]. Action = [[ 0.02898955 -0.5913642  -0.186346   -0.6568873 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 3684. State = [[-0.251998   -0.1200294   0.11498047  1.        ]]. Action = [[0.33955514 0.2959926  0.7899947  0.7411587 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3685. State = [[-0.23876494 -0.11260222  0.13113649  1.        ]]. Action = [[0.7643585  0.4198557  0.81690335 0.83837867]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3686. State = [[-0.21822342 -0.10457599  0.1566827   1.        ]]. Action = [[0.31466293 0.04329348 0.3238094  0.7409127 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3687. State = [[-0.20594081 -0.09935004  0.1747754   1.        ]]. Action = [[0.18355703 0.1824801  0.5869839  0.7021103 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 3688. State = [[-0.19364914 -0.09219719  0.18522927  1.        ]]. Action = [[ 0.44582772  0.19828737 -0.47326833  0.48384333]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 3689. State = [[-0.18706849 -0.07939959  0.17471771  1.        ]]. Action = [[ 0.21027303  0.28756332 -0.94940126  0.46150374]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 3690. State = [[-0.17545049 -0.06785782  0.14485982  1.        ]]. Action = [[ 0.22787583  0.3690698  -0.8636407   0.5118005 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3691. State = [[-0.16690332 -0.06153089  0.12340688  1.        ]]. Action = [[ 0.19261456  0.22245705 -0.8061134   0.41089368]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 3692. State = [[-0.1661318  -0.05961376  0.12093911  1.        ]]. Action = [[ 0.1681683   0.16456378 -0.9093576   0.41950083]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 3693. State = [[-0.16606948 -0.05915076  0.12018743  1.        ]]. Action = [[ 0.28865337  0.0382328  -0.23109603  0.5160861 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 3694. State = [[-0.1660721  -0.05881375  0.11991686  1.        ]]. Action = [[ 0.11665356  0.2582016  -0.89246595  0.22931027]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 3695. State = [[-0.1660721  -0.05881375  0.11991686  1.        ]]. Action = [[ 0.19929743  0.28022695 -0.26452464  0.44106317]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 3696. State = [[-0.1660721  -0.05881375  0.11991686  1.        ]]. Action = [[ 0.14608204  0.17220008 -0.6971704   0.26602817]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 3697. State = [[-0.1660721  -0.05881375  0.11991686  1.        ]]. Action = [[ 0.17966855  0.24221921 -0.75627005  0.37881386]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 3698. State = [[-0.1660721  -0.05881375  0.11991686  1.        ]]. Action = [[ 0.28990197  0.05617869 -0.5036286   0.3366238 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 3699. State = [[-0.16606097 -0.05874738  0.11991251  1.        ]]. Action = [[ 0.15631008  0.4112029  -0.6414092   0.47401285]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 3700. State = [[-0.16606097 -0.05874738  0.11991251  1.        ]]. Action = [[ 0.47228432  0.11365223 -0.4490075   0.33607864]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 3701. State = [[-0.16606097 -0.05874738  0.11991251  1.        ]]. Action = [[ 0.30118096  0.16935432 -0.6148212   0.4804206 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 3702. State = [[-0.16606097 -0.05874738  0.11991251  1.        ]]. Action = [[ 0.27805495  0.22084546 -0.7571101   0.32387948]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 3703. State = [[-0.16606097 -0.05874738  0.11991251  1.        ]]. Action = [[ 0.2655759   0.196414   -0.74854195  0.5912123 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 3704. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.2716217   0.1253953  -0.6431887   0.31064463]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 3705. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.28636646  0.14110303 -0.6318667   0.42367232]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 3706. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.37985587  0.35197055 -0.9489069   0.4623505 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 3707. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.32573855  0.271024   -0.70020586  0.3567258 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 3708. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.4989189   0.39817166 -0.84067315  0.49923038]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 3709. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.28217208  0.2303859  -0.20958674  0.49896276]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 3710. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[0.14119923 0.275728   0.29345202 0.47591102]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 3711. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[-0.00500906 -0.04004222 -0.3396948   0.43040216]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 3712. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.38140154  0.33660817 -0.7806886   0.35165   ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 3713. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.30642176  0.2788551  -0.87918764  0.47849226]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 3714. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.23286271  0.14448786 -0.4367355   0.2856431 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 3715. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.15483165  0.09414625 -0.8022553   0.31274688]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 3716. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.16577137  0.20582974 -0.6522939   0.5633004 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 3717. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.3282808   0.58894575 -0.5866548   0.44575047]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 3718. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.16564822  0.30684626 -0.06689966  0.5248685 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 3719. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.03423512  0.46843696 -0.9282007   0.62161136]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 3720. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.15646017  0.18625832 -0.6525207   0.54510534]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 3721. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.27852297  0.35178924 -0.90775776  0.45600235]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 3722. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.3720169   0.38277984 -0.6878269   0.5825403 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 3723. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.18955493  0.31136703 -0.805853    0.35888672]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 3724. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.18940842  0.10453331 -0.9036447   0.49231803]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 3725. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[0.22057617 0.3069086  0.2907529  0.3080263 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 3726. State = [[-0.16603881 -0.05861506  0.11990385  1.        ]]. Action = [[ 0.21355832  0.39044094 -0.8595725   0.43226004]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 3727. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.20281363  0.22307551 -0.8706584   0.5305958 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 3728. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.32320964  0.14778137 -0.05910814  0.4824655 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 3729. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.18328106  0.34239495 -0.8443066   0.40330172]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 3730. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.22097635  0.40880704 -0.8689055   0.43331885]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 3731. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.22495985  0.24582851 -0.64498943  0.56561446]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 3732. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.33522964  0.33090854 -0.4991634   0.370888  ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 3733. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2823516   0.17551196 -0.7345977   0.40158522]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 3734. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2285366   0.14430237 -0.72694683  0.63899183]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 3735. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.05149543  0.04737055 -0.70395046  0.44562125]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 3736. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.39499342  0.23608196 -0.57387185  0.46476173]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 3737. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2453196   0.45386612 -0.80556226  0.49733102]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 3738. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.15869713  0.30440748 -0.90343964  0.3932228 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 3739. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.18200696  0.13542604 -0.5726905   0.5161698 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 3740. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.31483495  0.5431745  -0.6753392   0.57568693]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 3741. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.19470096  0.4159578  -0.91933143  0.31270826]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 3742. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.14860451  0.39665246 -0.94123197  0.49698222]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 3743. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.33000803  0.3883903  -0.7065599   0.5961497 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 3744. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.3382156   0.26375842 -0.93772525  0.457276  ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 3745. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.24742937  0.33717358 -0.7312653   0.4067595 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 3746. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.21942806  0.338982   -0.67882526  0.5031197 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 3747. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.40637517  0.26716495 -0.66954553  0.58710146]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 3748. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.3265723   0.5489037  -0.59740543  0.5185704 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 3749. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.34575474  0.48116922 -0.71466094  0.57970226]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 3750. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.34284377  0.4878744  -0.84346616  0.5233233 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 3751. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.25053763  0.39037883 -0.6207304   0.4094206 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 3752. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.5242126   0.380435   -0.960951    0.29788065]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 3753. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2735381   0.4456247  -0.4306944   0.45053887]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 3754. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.283216    0.17517316 -0.851198    0.64015687]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 3755. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.13278913  0.41566646 -0.44890714  0.5742526 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 3756. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.45590758  0.26132    -0.8609364   0.5908873 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 3757. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.31987357  0.46985888 -0.82115823  0.4626906 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 3758. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.3332771   0.45592594 -0.8216185   0.43792248]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 3759. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.36187458  0.3168007  -0.66812354  0.5536071 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 3760. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.4238552   0.19966114 -0.9149855   0.72788596]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 3761. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.3996359   0.34867787 -0.25866306  0.54559445]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 3762. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.4892695   0.37825632 -0.70125115  0.51042235]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 3763. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.21333373  0.29489958 -0.6631018   0.6065911 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 3764. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2643125   0.51484394 -0.7015002   0.47966373]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 3765. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.36369574  0.21562243 -0.69377816  0.47389817]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 3766. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2875042   0.17693484 -0.4368102   0.6765611 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 3767. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.51686835  0.27883077 -0.9330776   0.4150114 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 3768. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.24861717  0.4326043  -0.78703326  0.5599525 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 3769. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.33250523  0.29333103 -0.3059628   0.4963503 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 3770. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.37012172  0.5393083  -0.8781366   0.6037731 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 3771. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.41940117  0.33018553 -0.68436664  0.5154638 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 3772. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.30544758  0.36868703 -0.7762915   0.51850474]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 3773. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.22498858  0.36492896 -0.72907925  0.49509668]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 3774. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.38543582  0.4252205  -0.85875446  0.3952875 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 3775. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.17191792  0.28379405 -0.8461322   0.45783985]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 3776. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.3062147   0.33200932 -0.8613905   0.38318896]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 3777. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.23971832  0.4107951  -0.52572685  0.5474198 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 3778. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.41631842  0.3290422  -0.71095985  0.5745802 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 3779. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.48628533  0.35095477 -0.7419929   0.5046375 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 3780. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.31681037  0.16985822 -0.88281876  0.3949405 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 3781. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.3718804   0.24461937 -0.68462765  0.4576764 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 3782. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.2542088  0.3638549 -0.7263909  0.4321041]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 3783. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.25166202  0.4571607  -0.74692905  0.42231786]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 3784. State = [[-0.16602771 -0.05854867  0.11989951  1.        ]]. Action = [[ 0.28015447  0.17764604 -0.77909285  0.34769034]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 3785. State = [[-0.2645544   0.07771418  0.11324392  1.        ]]. Action = [[ 0.23413348  0.25196016 -0.81286454  0.5722709 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 3786. State = [[-0.26277766  0.08838913  0.09124011  1.        ]]. Action = [[ 0.19898129  0.21793628 -0.91476667  0.10825086]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3787. State = [[-0.25734022  0.09190216  0.06763591  1.        ]]. Action = [[-0.5522555  -0.12177014 -0.9681558  -0.35956025]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3788. State = [[-0.2570314   0.09251988  0.06471317  1.        ]]. Action = [[-0.95418364 -0.47864783 -0.9697428  -0.9019788 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3789. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.8731903  -0.5270994  -0.80733144 -0.7807842 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3790. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.9519669  -0.39687514 -0.97025466 -0.8422229 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3791. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.9124203  -0.40880775 -0.9367713  -0.901297  ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3792. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.8557348  -0.48742664 -0.984996   -0.75082177]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3793. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.8897111  -0.5515926  -0.92244136 -0.8992069 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3794. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.8275406 -0.4806552 -0.9796339 -0.8739403]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3795. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.8147691  -0.53065675 -0.91715574 -0.7979542 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3796. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.8554254  -0.60504895 -0.37242    -0.8912867 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3797. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.91052026 -0.48051548 -0.96635777 -0.7378135 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3798. State = [[-0.25688273  0.09260486  0.06470481  1.        ]]. Action = [[-0.9177139  -0.41771328 -0.9972334  -0.8616126 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3799. State = [[-0.25693384  0.09269629  0.06470484  1.        ]]. Action = [[-0.8534894 -0.5307388 -0.9095542 -0.852241 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3800. State = [[-0.25693384  0.09269629  0.06470484  1.        ]]. Action = [[-0.960493   -0.46247852 -0.9569159  -0.8127103 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3801. State = [[-0.25695926  0.09274178  0.06470486  1.        ]]. Action = [[-0.88719314 -0.41770232 -0.73949456 -0.9380702 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3802. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.90985614 -0.36024415 -0.9461306  -0.84125125]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3803. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.94301075 -0.48570222 -0.9161817  -0.7822922 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3804. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.93679506 -0.46507418 -0.99500716 -0.91928864]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3805. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9598614  -0.6252829  -0.7806574  -0.90048146]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3806. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9253193  -0.41958594 -0.36285168 -0.8916767 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3807. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.98533666 -0.48287332 -0.9686147  -0.929195  ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3808. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.94473666 -0.5196166  -0.85184765 -0.7857058 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3809. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9193611 -0.4240495 -0.8014979 -0.9263284]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3810. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.969845   -0.43073893 -0.8811748  -0.84523934]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3811. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9729366  -0.5178614  -0.99510306 -0.91884357]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3812. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.8964995  -0.41929555 -0.97358394 -0.93344474]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3813. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9426816  -0.48523986 -0.9782367  -0.819677  ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3814. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.920374   -0.47049344 -0.97300243 -0.8760437 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3815. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.96147275 -0.50910884 -0.9125523  -0.7157942 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3816. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9536153  -0.49835014 -0.90953356 -0.80689126]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3817. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.85390174 -0.54934007 -0.9594316  -0.86033344]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3818. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.91971177 -0.4738022  -0.85631716 -0.8228897 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3819. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9567434  -0.43020838 -0.98930633 -0.79768294]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3820. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.8786932  -0.27232134 -0.8840468  -0.86973476]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3821. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.97022456 -0.46639258 -0.5624984  -0.8514665 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3822. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9367067 -0.4174065 -0.9636289 -0.8901058]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 3823. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.96221966 -0.36140817 -0.6641251  -0.7619661 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3824. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.93914086 -0.49056864 -0.8545113  -0.8032566 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 3825. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9640419  -0.34711802 -0.97744346 -0.93089825]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3826. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9578775  -0.42836595 -0.56093377 -0.92697036]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3827. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.94694936 -0.47676492 -0.9350296  -0.91133946]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3828. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9844374  -0.58634907 -0.8085498  -0.84783006]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3829. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.93418646 -0.66099167 -0.8094403  -0.91673905]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 3830. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9568013 -0.3778301 -0.8043906 -0.9138438]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3831. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9056012  -0.6130374  -0.8546967  -0.85197157]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3832. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.90620923 -0.5214865  -0.99580234 -0.8882033 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3833. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9235881  -0.67875445 -0.21769452 -0.8874317 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3834. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9420012  -0.5714318  -0.99051934 -0.83825004]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3835. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.97809917 -0.5390488   0.3668673  -0.8118316 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3836. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9661849  -0.46484673 -0.97434187 -0.82849026]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3837. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9404101  -0.46819878 -0.9822144  -0.92157817]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3838. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.94150823 -0.54525167 -0.98056614 -0.91216826]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3839. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9547523  -0.45708632 -0.96610904 -0.912345  ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3840. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9357665  -0.502292   -0.92208976 -0.80444604]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3841. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.91109484 -0.47691154 -0.72204494 -0.8462854 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 3842. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.8495798  -0.47071064 -0.96708536 -0.8843823 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 3843. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.875542   -0.46622396 -0.9853265  -0.9237855 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3844. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.8924403 -0.4674725 -0.9446459 -0.8667594]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 3845. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.95144486 -0.56077594 -0.7350815  -0.88358366]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3846. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.83697677 -0.51514    -0.7055349  -0.8691647 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 3847. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.92609227 -0.48386204 -0.07692987 -0.82305795]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3848. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.91445106 -0.6447517  -0.94653136 -0.8930295 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 3849. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9237668  -0.54955846 -0.91472197 -0.8517427 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3850. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9334644  -0.33654046 -0.9764723  -0.73532915]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3851. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.88752997 -0.4468757  -0.9213496  -0.79768294]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3852. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.90324396 -0.60491526 -0.82681775 -0.8516071 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3853. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9392546  -0.41432667 -0.90054715 -0.8616223 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3854. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.95381016 -0.553848    0.24424243 -0.9339724 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 3855. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9594207  -0.56859016 -0.32059276 -0.95022273]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3856. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9598329  -0.61738104 -0.54528177 -0.93375325]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3857. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9602681  -0.5309932  -0.17522848 -0.9279839 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3858. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.8668435 -0.6402127 -0.9466474 -0.8664263]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3859. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9303058  -0.5600455  -0.5727348  -0.89513123]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3860. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.959749   -0.50258666 -0.78898215 -0.84530306]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3861. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.8919458 -0.6059212 -0.7570731 -0.6937101]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 3862. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.96059453 -0.5274641  -0.50353754 -0.83094645]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3863. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9511719  -0.44970167 -0.6907774  -0.9071168 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3864. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.94032353 -0.59870374 -0.9413996  -0.90240437]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3865. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.96747464 -0.64232236 -0.7982109  -0.8995729 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3866. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9541754  -0.642641   -0.98321414 -0.89798945]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3867. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9807837  -0.59890664 -0.8541128  -0.90694314]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3868. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9855637  -0.49926794 -0.5458086  -0.93927306]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 3869. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9645948 -0.6155027 -0.9073789 -0.8161995]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3870. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9529461  -0.45394588 -0.8805546  -0.9032698 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 3871. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9886485  -0.55015445 -0.96779954 -0.8479594 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3872. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9844195  -0.5698243  -0.97908616 -0.88429344]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3873. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9484689  -0.50397706 -0.63372535 -0.8661384 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 3874. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9119546  -0.66021043 -0.9665793  -0.8571016 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 3875. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.98450226 -0.6410877  -0.9368803  -0.9005139 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3876. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.99009466 -0.54078436 -0.66363776 -0.9021001 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3877. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9858914  -0.47469866 -0.6496821  -0.9142993 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 3878. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.90255356 -0.64524907 -0.8544716  -0.91808474]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3879. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.95070976 -0.5920576   0.5879681  -0.85725   ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 3880. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.98529595 -0.6897172  -0.35800254 -0.8553254 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 3881. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9377085 -0.706756  -0.5514569 -0.9199406]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3882. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9300885  -0.72907597  0.26008654 -0.8717091 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3883. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.93568707 -0.69659644 -0.21487617 -0.89776385]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3884. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.95283926 -0.6191361  -0.5967332  -0.8991708 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3885. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.9255149  -0.42415965 -0.2672962  -0.8871189 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 3886. State = [[-0.25688484  0.09275558  0.06471789  1.        ]]. Action = [[-0.92282826 -0.6888095  -0.42100024 -0.8679653 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3887. State = [[-0.26246035  0.17564078  0.12122675  1.        ]]. Action = [[-0.96200573 -0.7219817   0.69690704 -0.84606415]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3888. State = [[-0.2611056   0.19403216  0.10067322  1.        ]]. Action = [[ 0.19214582 -0.09033442 -0.9687582   0.25028253]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3889. State = [[-0.25508925  0.1942837   0.07446095  1.        ]]. Action = [[-0.62308156 -0.4333111  -0.30843294 -0.39358634]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3890. State = [[-0.25187758  0.19474345  0.07175817  1.        ]]. Action = [[-0.9570792  -0.66201955 -0.97666585 -0.9047854 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3891. State = [[-0.25194368  0.19454294  0.0716569   1.        ]]. Action = [[-0.92319506 -0.74877244 -0.43054426 -0.9087465 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3892. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.9779103  -0.7595103   0.04627001 -0.95364   ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3893. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.9769551  -0.7794768   0.45042813 -0.9279603 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3894. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.9599525  -0.81480956  0.17779267 -0.95090723]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3895. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.9405025  -0.75472176  0.4407915  -0.94716257]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3896. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.96586525 -0.77315366  0.77446246 -0.8955532 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 3897. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.90160286 -0.65665436 -0.06026387 -0.9481688 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3898. State = [[-0.2519212   0.19447596  0.07165809  1.        ]]. Action = [[-0.9809229  -0.77492833 -0.21911883 -0.96276486]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3899. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9865015  -0.80739266 -0.37810558 -0.9473635 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3900. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96709645 -0.7346215   0.6277081  -0.9457713 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3901. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9741454  -0.7890095   0.16450584 -0.9475195 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3902. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9335925  -0.713165    0.2981174  -0.94230306]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3903. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9084438 -0.7129742  0.76537   -0.9095976]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3904. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9629644  -0.7417916   0.14769793 -0.8520129 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3905. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9680217  -0.77296937  0.48071086 -0.89217585]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3906. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.93042576 -0.7882087  -0.3379693  -0.952486  ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3907. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9793223  -0.7560943   0.30513036 -0.92095387]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3908. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9729764  -0.840262    0.4924593  -0.94918066]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3909. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97813886 -0.8666546   0.05344355 -0.9091251 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3910. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98235416 -0.78910035  0.17163002 -0.90307903]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3911. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9723799  -0.845147    0.09461606 -0.9236095 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3912. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97451794 -0.8043557   0.24221528 -0.9234535 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3913. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9771173  -0.84160376  0.49024343 -0.9718037 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3914. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9791045  -0.81781524  0.03446543 -0.9161874 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3915. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9339622  -0.8619291   0.35026598 -0.94786173]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3916. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9673914  -0.8346877   0.21120667 -0.963096  ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3917. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9864545  -0.87397546  0.52274823 -0.9355636 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3918. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97640586 -0.8475778   0.23451173 -0.9511714 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3919. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9558874  -0.8502839   0.18223977 -0.92727107]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3920. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9524245  -0.8451826   0.03333318 -0.9408801 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3921. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96193826 -0.8396218   0.26928806 -0.9644655 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3922. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9891294  -0.8257242   0.03957665 -0.93609715]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3923. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9408425  -0.8684021   0.08648407 -0.9400737 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3924. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9808783  -0.8830022   0.25687742 -0.90483105]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 3925. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96193194 -0.8285875   0.10889268 -0.9384793 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3926. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98107475 -0.8096944   0.05906165 -0.91366017]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 3927. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.95325446 -0.867215   -0.02826363 -0.9174418 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3928. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96778154 -0.7940411   0.15852356 -0.9161512 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3929. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.88234216 -0.82054746  0.3281691  -0.86144775]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3930. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97438926 -0.82389635  0.18518794 -0.93438184]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3931. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9452775  -0.8530027  -0.07733649 -0.9414906 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 3932. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96904975 -0.83109146 -0.03787249 -0.95954597]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3933. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98305684 -0.89500743  0.1588366  -0.95456094]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3934. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98048985 -0.8699004   0.39540827 -0.94933474]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3935. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9817246  -0.891306    0.40162325 -0.981257  ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3936. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9913117  -0.89886814  0.546051   -0.9260203 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3937. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9741326  -0.84489393 -0.19053948 -0.93827605]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3938. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.950433   -0.83444756  0.1733104  -0.9267587 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3939. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9753072  -0.8513232   0.14306545 -0.8764882 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3940. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9367447  -0.84664595  0.45983446 -0.9363243 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3941. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9469679  -0.87803006  0.4382205  -0.9455788 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3942. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.92120486 -0.8606999   0.0882566  -0.92797375]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3943. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97665423 -0.8491386  -0.2978217  -0.920309  ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 3944. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96671605 -0.9289326   0.28512597 -0.90404993]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 3945. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9734849  -0.87224644  0.14270544 -0.9582303 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3946. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.95197093 -0.878446    0.6774769  -0.9195846 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 3947. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9379641  -0.8715982   0.25199938 -0.95952195]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3948. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97335696 -0.9028928   0.26681113 -0.932953  ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 3949. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97466624 -0.8213233  -0.04724807 -0.9265304 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3950. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9571876  -0.86037695  0.15906942 -0.91677654]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 3951. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97040707 -0.8849898   0.1420883  -0.8996215 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3952. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9792998  -0.8320688  -0.00650877 -0.93830687]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3953. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.95804197 -0.8258433  -0.02897811 -0.9157492 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3954. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9498307  -0.8990521  -0.71883756 -0.8582209 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3955. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9637908  -0.86898816 -0.5345286  -0.88586456]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3956. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.91494185 -0.90186316 -0.07144618 -0.8757483 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 3957. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9376823  -0.8913309   0.07292402 -0.9289007 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3958. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98245424 -0.9057295   0.45512593 -0.9035909 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3959. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.95924187 -0.8520533   0.70337486 -0.94860613]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3960. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.96568    -0.84085363  0.2766093  -0.93767065]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3961. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9762321 -0.8543364  0.7263057 -0.9664322]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3962. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9788282  -0.8838086   0.05705416 -0.9044231 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3963. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9829911  -0.9052837   0.16200078 -0.95820516]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 3964. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98158616 -0.8667207   0.42418027 -0.9417401 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3965. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.98254573 -0.8853497   0.13178146 -0.9461307 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3966. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.92918354 -0.8781038   0.3082688  -0.8647402 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3967. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.92575586 -0.90641266  0.61446404 -0.96422946]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3968. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9772484  -0.91150737  0.2609836  -0.9611142 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3969. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.97280985 -0.8319765   0.38612127 -0.9162545 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3970. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9541881  -0.86659974  0.23551297 -0.9290986 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 3971. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9096761 -0.8653616 -0.1958785 -0.9111946]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3972. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9643245  -0.8893067   0.1253041  -0.89651656]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 3973. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.975932   -0.9321578   0.02317214 -0.9271242 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3974. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9737895  -0.87834674  0.4551388  -0.9665547 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3975. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9709055  -0.90219456 -0.09788334 -0.89183134]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 3976. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9762298  -0.85510075  0.09745395 -0.92427593]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 3977. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9479433 -0.8745703  0.0404563 -0.9036955]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3978. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9219772  -0.90429825 -0.13474715 -0.89836556]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3979. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.8715364  -0.8775085   0.12921333 -0.7868373 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 3980. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9080156  -0.8763475  -0.10718274 -0.9156288 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3981. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9367938 -0.8313592 -0.0696227 -0.9110756]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 3982. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.93881005 -0.85853    -0.27230006 -0.9167968 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 3983. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.88329947 -0.8525682   0.09075832 -0.7680511 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3984. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9488759  -0.86364126  0.23913181 -0.8569442 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3985. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.95749164 -0.90867954  0.16756344 -0.96273166]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3986. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9567533  -0.8622743   0.11207139 -0.9457977 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3987. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.962965   -0.8671805   0.29735386 -0.958421  ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 3988. State = [[-0.25178364  0.19452727  0.07169943  1.        ]]. Action = [[-0.9370517  -0.8735576  -0.43271565 -0.9394696 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3989. State = [[-0.2559523  -0.07858659  0.11746716  1.        ]]. Action = [[-0.9897925  -0.89002824 -0.25282907 -0.8933485 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3990. State = [[-0.25379968 -0.08230941  0.10460846  1.        ]]. Action = [[0.2988397  0.4825747  0.26620412 0.78389335]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3991. State = [[-0.24172841 -0.06732852  0.11299356  1.        ]]. Action = [[0.61463714 0.7220144  0.77353334 0.81693506]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3992. State = [[-0.22576469 -0.04483763  0.12509386  1.        ]]. Action = [[ 0.3220154   0.46220577 -0.20821196  0.64476776]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3993. State = [[-0.21482483 -0.02963875  0.11986291  1.        ]]. Action = [[ 0.40205896  0.22067714 -0.7272427   0.23787653]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 3994. State = [[-0.20080476 -0.02052041  0.09930497  1.        ]]. Action = [[ 0.42559874  0.21895623 -0.92362505  0.18623781]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 3995. State = [[-0.18750852 -0.01275705  0.06659183  1.        ]]. Action = [[ 0.09009171  0.04517639 -0.89701754  0.2625147 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 3996. State = [[-0.1808495  -0.01106428  0.04343774  1.        ]]. Action = [[-0.06735325 -0.35491621 -0.80699503 -0.05157155]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 3997. State = [[-0.26106238  0.00639398  0.11979854  1.        ]]. Action = [[-0.90978575 -0.81479675 -0.3064326  -0.84780663]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 3998. State = [[-0.261       0.01146017  0.0981859   1.        ]]. Action = [[ 0.30166805  0.2685244  -0.93117386  0.26172388]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3999. State = [[-0.25521666  0.01437736  0.06661517  1.        ]]. Action = [[ 0.237988   -0.02866369 -0.87159836  0.07406247]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4000. State = [[-0.24821879  0.01497179  0.04066854  1.        ]]. Action = [[-0.9289866  -0.8693924  -0.4724028  -0.76406014]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 4001. State = [[-0.24721771  0.01500101  0.03976927  1.        ]]. Action = [[-0.9994379  -0.9885545   0.6452311  -0.96119136]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 4002. State = [[-0.2466491   0.01501514  0.03982555  1.        ]]. Action = [[-0.99756485 -0.9380944   0.85091233 -0.9780982 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4003. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.98683596 -0.97658914  0.71492887 -0.92422485]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4004. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.9995329  -0.9248943   0.80422235 -0.92732954]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4005. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.9869712  -0.9692753   0.83723307 -0.95692366]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4006. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.99576813 -0.9836018   0.8852494  -0.99065846]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4007. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.98818415 -0.94831973  0.46548164 -0.97246844]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4008. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.97941667 -0.9837638   0.7031672  -0.9878489 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4009. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.99867356 -0.9739455   0.8694519  -0.98706096]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4010. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.99394643 -0.9583476   0.64727724 -0.9848702 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4011. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.9990863  -0.91798407  0.4445548  -0.9713335 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4012. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.9975464  -0.97450495  0.6512661  -0.9767978 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4013. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.97536    -0.9724996   0.75426984 -0.97821766]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4014. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.9986691  -0.867716    0.78988385 -0.99260485]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4015. State = [[-0.2464472   0.0150146   0.03984421  1.        ]]. Action = [[-0.9183711  -0.97928     0.26882696 -0.989543  ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4016. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9819095 -0.9164894  0.7856512 -0.9771191]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4017. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.986872   -0.98486835  0.8504579  -0.95385617]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4018. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9960177  -0.98287326  0.8569968  -0.989066  ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4019. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9957716  -0.98007214  0.4612311  -0.9923087 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4020. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9934688  -0.98848844  0.6135688  -0.98998636]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4021. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99707115 -0.96186894  0.93608713 -0.9881754 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4022. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99710476 -0.94224834  0.86516213 -0.9901274 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4023. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9988812  -0.9784645   0.88481236 -0.9874376 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4024. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9913711  -0.9548093   0.6347064  -0.97570634]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4025. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9744467  -0.9108634   0.85018134 -0.92577744]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4026. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9852731  -0.97439724  0.64514184 -0.98984665]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4027. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9985155  -0.9659814   0.5609665  -0.98971015]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4028. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9819209  -0.96182674  0.8269787  -0.9602441 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4029. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9686715 -0.9607067  0.6305517 -0.971211 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4030. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9877321 -0.9612391  0.6394745 -0.9932392]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4031. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9974402  -0.9620773   0.60094035 -0.98882216]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4032. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9872075  -0.97205096  0.8631191  -0.9840278 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4033. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9847813 -0.9624377  0.5892464 -0.9960184]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4034. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9939277  -0.97818875  0.7067361  -0.9706511 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4035. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9948072  -0.97794664  0.7361753  -0.98536116]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4036. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.8870454  -0.9630845   0.654866   -0.93551403]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4037. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9960694  -0.9773828   0.76112545 -0.97814196]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4038. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9970063 -0.9796545  0.7770889 -0.9792941]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4039. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9978738  -0.9588474   0.88458014 -0.95104766]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4040. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99916726 -0.9086249   0.90058863 -0.9765908 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4041. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99098253 -0.97962326  0.8882022  -0.9929516 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4042. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9910662  -0.93001103  0.8652116  -0.98633724]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4043. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99618113 -0.94376516  0.83934784 -0.9670251 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4044. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9637615  -0.96353173  0.7712741  -0.9755282 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4045. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.98902506 -0.9761255   0.66111064 -0.97318184]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4046. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9965057  -0.9204429   0.90783894 -0.96188307]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4047. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99797773 -0.96833783  0.6797986  -0.9864083 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4048. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9870882  -0.9677715   0.84294343 -0.97379935]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4049. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9972538  -0.94888896  0.8302009  -0.9820331 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4050. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9867114 -0.9884791  0.5277544 -0.993324 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4051. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9613824  -0.9618745   0.82499576 -0.96159863]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4052. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9919668  -0.96159256  0.7184262  -0.97852147]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4053. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9681774  -0.97843     0.67838717 -0.98498577]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4054. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9633211  -0.9490734   0.40955687 -0.9936943 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4055. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9842768 -0.9228437  0.7558403 -0.9958276]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4056. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9907905  -0.96254677  0.5315206  -0.97409296]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4057. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.8763527  -0.8378958   0.68406343 -0.9863789 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4058. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9730013 -0.89254    0.6370814 -0.9717429]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4059. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9435449  -0.95778626  0.32509446 -0.9847698 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4060. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99607474 -0.9582324   0.7291708  -0.9937444 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4061. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9942725  -0.9844442   0.544809   -0.98732495]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4062. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9937987 -0.9742517  0.3547579 -0.9727252]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4063. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9914657  -0.92945355  0.60886383 -0.96226865]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4064. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.986396   -0.9865257   0.62652004 -0.9904894 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4065. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9974944  -0.9114982   0.72187805 -0.9611528 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4066. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9948964  -0.93947375  0.6637802  -0.99401265]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4067. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99698234 -0.9151556   0.823073   -0.98253345]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4068. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99711037 -0.9574516   0.9045259  -0.93566316]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4069. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9747085  -0.9577201   0.21275866 -0.94422835]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4070. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.79869914 -0.9779876   0.6001692  -0.94176614]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4071. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99187565 -0.97813755  0.73774624 -0.97567147]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4072. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.92918855 -0.97346604  0.65421057 -0.9847366 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4073. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.98630196 -0.98654157  0.47114182 -0.97340643]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4074. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99450386 -0.97528106  0.54735446 -0.98491335]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4075. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.98456514 -0.93448913  0.31659293 -0.98062545]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4076. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9886194  -0.9810045   0.44916737 -0.97182417]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4077. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9992565  -0.94093335  0.81870294 -0.9810781 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4078. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99952686 -0.81950545  0.5952289  -0.98498845]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4079. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.94131607 -0.9134691   0.603369   -0.99058187]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4080. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99892473 -0.96959764  0.7678175  -0.9978282 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4081. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99342394 -0.941951    0.54887843 -0.9933461 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4082. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99619395 -0.82322335  0.776958   -0.9827786 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4083. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.991209  -0.8990975  0.8513502 -0.9897777]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4084. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99961406 -0.9391481   0.7198883  -0.9902411 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4085. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9845234  -0.94561625  0.8100873  -0.99449325]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4086. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99053043 -0.97525674  0.8954215  -0.98833156]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4087. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99781525 -0.9643082   0.842602   -0.979167  ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4088. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9980638  -0.9867916   0.79666805 -0.9251112 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4089. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9976117  -0.9615113   0.67208195 -0.9801783 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4090. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9479659  -0.88129026  0.40587544 -0.94528466]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4091. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9864581  -0.9870043   0.49728584 -0.957007  ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4092. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9679852  -0.9618757   0.72212434 -0.9812272 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4093. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99785465 -0.9659853   0.7291012  -0.9735373 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4094. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9859605  -0.9486559   0.53110886 -0.98231673]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4095. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.989383   -0.94633347  0.6871871  -0.9866342 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4096. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9864373 -0.9405952  0.6153636 -0.939877 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4097. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.9788869  -0.9810221   0.50237954 -0.98211336]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4098. State = [[-0.24637052  0.01501595  0.03985173  1.        ]]. Action = [[-0.99915206 -0.97265184  0.73128176 -0.98933846]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4099. State = [[-0.26413408  0.13957675  0.12430749  1.        ]]. Action = [[-0.99713635 -0.96535486  0.87644887 -0.967967  ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4100. State = [[-0.26033613  0.15648909  0.10263162  1.        ]]. Action = [[ 0.29925418  0.15842497 -0.9674315   0.3441671 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4101. State = [[-0.25246027  0.15975176  0.077272    1.        ]]. Action = [[-0.34633124 -0.44338346 -0.85594034 -0.4244231 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 4102. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9782028  -0.9167243   0.08645356 -0.81381774]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 4103. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9387417  -0.9283506  -0.29089397 -0.9508765 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 4104. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9689174  -0.90615183  0.03572822 -0.8996982 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4105. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9790343  -0.79398197 -0.00104481 -0.9124098 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4106. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9702725  -0.906351    0.16020584 -0.86894053]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4107. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96098834 -0.8603239   0.0603081  -0.94357896]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4108. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.971966   -0.9017799   0.17267084 -0.8436402 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4109. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.95724684 -0.8374909  -0.41163242 -0.92385864]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4110. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.8451581  -0.80639255 -0.78907484 -0.8841579 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4111. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96474814 -0.86209106 -0.27861655 -0.8017166 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4112. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.8580519  -0.83240974 -0.6874914  -0.7557437 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4113. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.8821097  -0.81211996 -0.7784749  -0.77312696]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4114. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9142107  -0.65125084 -0.44294518 -0.65834755]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4115. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9473494 -0.8580314 -0.5924848 -0.7469013]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4116. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93967056 -0.68874955 -0.7167113  -0.8658396 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4117. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9355354  -0.8188886  -0.38403392 -0.75214255]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4118. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.8615413  -0.8170669  -0.46278155 -0.82669544]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4119. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.92176133 -0.6119303   0.04953992 -0.8946825 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4120. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9393435  -0.83777684 -0.41358078 -0.88786525]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4121. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96018094 -0.90210307  0.33425224 -0.754816  ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4122. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.97384804 -0.9132975  -0.2752496  -0.86880267]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4123. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9516687  -0.8387235  -0.14976609 -0.8769576 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4124. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9759862  -0.89458936 -0.1430316  -0.9296836 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4125. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93700004 -0.8598366  -0.03944647 -0.87138766]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4126. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.91486186 -0.88436776 -0.09248817 -0.9409082 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4127. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.92662305 -0.8100017   0.08279645 -0.9324827 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4128. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96221113 -0.89637697  0.09369898 -0.87670046]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4129. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9660976  -0.9076658  -0.27034807 -0.93846965]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4130. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9516389  -0.80687994 -0.3349874  -0.8872145 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4131. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.928584   -0.85506827 -0.50372237 -0.898438  ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4132. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9524562  -0.88756573 -0.55877745 -0.79982996]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4133. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.940865   -0.82380897 -0.32310295 -0.92323637]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4134. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.89721733 -0.928432   -0.03278154 -0.8227335 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4135. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.938616   -0.7089403  -0.34766412 -0.93875927]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4136. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.90498537 -0.8486118  -0.39767277 -0.8620291 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4137. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.8721702  -0.8407419   0.14294255 -0.8756239 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4138. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9699618  -0.78630143 -0.29181957 -0.84440565]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4139. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93608755 -0.7971448   0.13579488 -0.788223  ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4140. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96727663 -0.7538275  -0.2241323  -0.86377895]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4141. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9225183  -0.79949874  0.17245317 -0.8845306 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4142. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.917809   -0.87965435 -0.16047448 -0.9211614 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4143. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.92315507 -0.8970133  -0.48249125 -0.8787442 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4144. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.95026374 -0.93340486  0.07827604 -0.889091  ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4145. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.953936   -0.7979287  -0.522597   -0.88247186]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4146. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93717414 -0.95008135 -0.13227463 -0.95242244]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4147. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96123546 -0.86886424 -0.3452626  -0.8999662 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4148. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9610011  -0.9179445  -0.05660057 -0.88204086]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4149. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.95182395 -0.87617975 -0.10658544 -0.84977734]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4150. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.94972456 -0.88620776  0.3386488  -0.83561945]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4151. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9752984  -0.8961622  -0.41965675 -0.89002156]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4152. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9776072  -0.70732653 -0.56158525 -0.91863906]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4153. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9607322  -0.9222313  -0.38759863 -0.90808815]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4154. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.957254   -0.92894006  0.20860994 -0.9354956 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4155. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9311099  -0.88080966  0.06814325 -0.8995088 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4156. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9745737  -0.83440125 -0.24821734 -0.9293105 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4157. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9414213  -0.88458383  0.15893567 -0.9066537 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4158. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9779815  -0.90913755  0.06154203 -0.9176794 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4159. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9462614  -0.92809325  0.32143438 -0.873522  ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4160. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9386568  -0.7902062  -0.35141337 -0.91314954]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4161. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9646709  -0.8807549  -0.23300815 -0.90014565]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4162. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.98045707 -0.88431513  0.16717196 -0.89761543]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4163. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9604091  -0.8965559  -0.31395245 -0.85874844]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4164. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9323342  -0.9250781   0.05840635 -0.9136285 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4165. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.95289946 -0.90592504 -0.04710424 -0.9314217 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4166. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9712847  -0.9240406  -0.22436482 -0.9385916 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4167. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9842897  -0.89831936  0.3601128  -0.9229    ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4168. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9668107  -0.8996473   0.30029154 -0.96166486]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4169. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9571144  -0.862033   -0.20184529 -0.88057995]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4170. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96541995 -0.9157492   0.45820963 -0.91876745]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4171. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.91759    -0.88357073 -0.11802191 -0.8756111 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4172. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.94937325 -0.9051049  -0.34495366 -0.93217456]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4173. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9727362  -0.87740695  0.3613087  -0.94299185]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4174. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.94876677 -0.8059433  -0.2976395  -0.92309463]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4175. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9359964  -0.8970212  -0.43067998 -0.83256936]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4176. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.96608627 -0.77971876 -0.3718421  -0.9095339 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4177. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.86329794 -0.7170828  -0.0351693  -0.9072087 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4178. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9312603  -0.9190834  -0.42264855 -0.89093614]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4179. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9263714  -0.80063343 -0.49763072 -0.81618875]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4180. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.86709213 -0.91513574 -0.07897222 -0.87947214]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4181. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93793553 -0.8031308  -0.0273366  -0.8979397 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4182. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.94543105 -0.8578374  -0.01391685 -0.89451337]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4183. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9280057  -0.9575472  -0.3210882  -0.73757863]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4184. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9747457  -0.9164237   0.23190856 -0.82270616]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4185. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.95970297 -0.86892396  0.27453768 -0.9094339 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4186. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9562707  -0.93230927  0.41302347 -0.9431566 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4187. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9453485  -0.9111115   0.49206853 -0.9005111 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4188. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9825522  -0.94767857 -0.10963064 -0.887162  ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4189. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93083715 -0.78443676 -0.21555597 -0.89722157]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4190. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9655296  -0.958207    0.03114247 -0.9159083 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4191. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.91040856 -0.95414895 -0.079045   -0.9065217 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4192. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9407628  -0.90684116 -0.10940188 -0.88316226]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4193. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9609748  -0.8743877  -0.3120011  -0.86842126]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4194. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9352039  -0.8382539  -0.20828104 -0.8627839 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4195. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.8858175  -0.9042341  -0.49700475 -0.88001907]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4196. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.9406535 -0.8911787 -0.4681282 -0.7797525]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4197. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93782425 -0.86102754 -0.22611159 -0.89246005]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4198. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.95230794 -0.8864316  -0.2745201  -0.7886535 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4199. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.93176824 -0.86006    -0.03885198 -0.77622586]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4200. State = [[-0.24975157  0.16087593  0.07449606  1.        ]]. Action = [[-0.91300803 -0.91695863 -0.41558623 -0.8220048 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4201. State = [[-0.25369027 -0.13805139  0.11809003  1.        ]]. Action = [[-0.9771643  -0.8407278  -0.05572051 -0.9191877 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4202. State = [[-0.24399239 -0.14180726  0.11230641  1.        ]]. Action = [[0.862092   0.8805437  0.99147606 0.9919292 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4203. State = [[-0.21934304 -0.11926971  0.13335018  1.        ]]. Action = [[0.90294373 0.7368916  0.73863184 0.97471225]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4204. State = [[-0.18361297 -0.09393256  0.16469218  1.        ]]. Action = [[0.83087516 0.4641906  0.82892895 0.81303334]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 4205. State = [[-0.15832007 -0.07158118  0.18271919  1.        ]]. Action = [[ 0.18069625  0.6084924  -0.33407795  0.43512094]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4206. State = [[-0.15382749 -0.05993308  0.18381162  1.        ]]. Action = [[ 0.03038085  0.20605624 -0.7643835   0.22549951]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 4207. State = [[-0.15248297 -0.05817199  0.18462525  1.        ]]. Action = [[ 0.31167436  0.0881331  -0.917571    0.10238993]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 4208. State = [[-0.15153183 -0.05730953  0.18508402  1.        ]]. Action = [[ 0.17055845  0.26741695 -0.87262094  0.12929106]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 4209. State = [[-0.15122154 -0.05692136  0.18521047  1.        ]]. Action = [[ 0.22062624 -0.08283609 -0.9341767   0.19240808]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 4210. State = [[-0.15098615 -0.05607127  0.1852318   1.        ]]. Action = [[ 0.13027024 -0.01497024 -0.7053069   0.11970246]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 4211. State = [[-0.15100877 -0.05553072  0.18518329  1.        ]]. Action = [[ 0.18074     0.06009281 -0.89922136  0.17968237]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 4212. State = [[-0.15099277 -0.05526453  0.18516077  1.        ]]. Action = [[ 0.29167938  0.16400373 -0.9670545   0.19711304]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 4213. State = [[-0.15099277 -0.05526453  0.18516077  1.        ]]. Action = [[ 0.18006968  0.08278096 -0.8992077   0.267797  ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 4214. State = [[-0.15102592 -0.05512549  0.18514723  1.        ]]. Action = [[ 0.00973618  0.14497244 -0.8735539   0.15332437]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 4215. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.15601015  0.1594317  -0.9184667   0.12163711]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 4216. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.09784043 -0.02988648 -0.7752226   0.1009506 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 4217. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.31028783  0.06539941 -0.8775472   0.14675617]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 4218. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.21931672 -0.06904888 -0.9208271   0.13125837]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 4219. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[-0.07441211  0.05800378 -0.7727043   0.10303903]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 4220. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.07503629  0.0816896  -0.92847365  0.11840975]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 4221. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.02208471  0.04135299 -0.7508774   0.22998941]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 4222. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.09761417  0.10969508 -0.922296    0.1691227 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 4223. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.23065972  0.10920835 -0.87302303  0.22040081]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 4224. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.2044177   0.23564613 -0.9621251   0.10965669]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 4225. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.1496098   0.10608685 -0.9350098   0.1675713 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 4226. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.17859638  0.09637403 -0.6267803   0.14633858]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 4227. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.07939076  0.09988534 -0.9548667   0.22497869]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 4228. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.04840863 -0.03881699 -0.8814079   0.19325972]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 4229. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.21070814 -0.09849304 -0.89720416  0.18451059]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 4230. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.15003633 -0.01597691 -0.91891843  0.16636121]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 4231. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.27694237 -0.10149187 -0.8919439   0.17886758]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 4232. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.243945    0.11259639 -0.88275737  0.27462423]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 4233. State = [[-0.15101518 -0.05506023  0.18514204  1.        ]]. Action = [[ 0.0898397   0.1059041  -0.86701065  0.22208309]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 4234. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.32412934  0.05253029 -0.7897887   0.19093394]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 4235. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.3480848  -0.00421458 -0.9234201   0.24471474]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 4236. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.08223403 -0.0401656  -0.94833165  0.20446467]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 4237. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.2771001  -0.01813906 -0.7839231   0.15567005]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 4238. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.23684514 -0.16856241 -0.8033293   0.19277763]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 4239. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.24817932  0.12101042 -0.66556895  0.18330061]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 4240. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.21865106  0.0451771  -0.71663815  0.15661132]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 4241. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.21046221 -0.08582044 -0.9577278   0.1459868 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 4242. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.05867982 -0.03952539 -0.83918095  0.21964264]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 4243. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.226794    0.07977545 -0.9486923   0.32707012]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 4244. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.09338379  0.23083806 -0.95275134  0.2764982 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 4245. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.3545636   0.03681302 -0.8772411   0.25101566]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 4246. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.12598848  0.2511704  -0.57952255  0.18461525]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 4247. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.16046858  0.06984746 -0.8314148   0.21473014]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 4248. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[-0.04902595  0.1112932  -0.92664766  0.11271334]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 4249. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.09243596 -0.05356759 -0.8207658   0.18003893]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 4250. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.05692875  0.09744596 -0.9376342   0.2598257 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 4251. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.03430569  0.31783485 -0.9545722   0.17426908]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 4252. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.11859488  0.04866958 -0.7322763   0.15271246]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 4253. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.2901703   0.11785519 -0.8368626   0.12680483]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 4254. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.30552232  0.07074797 -0.7538611   0.12075591]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 4255. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.3127581   0.09727681 -0.6452848   0.298849  ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 4256. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.138479   -0.15430665 -0.77093995  0.2046423 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 4257. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.06749666  0.04633665 -0.93334585  0.18060768]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 4258. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.12292314  0.13012409 -0.68703294  0.13695681]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 4259. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.11001909 -0.07063192 -0.84867597  0.18129838]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 4260. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.06185365  0.08692682 -0.81344575  0.1364913 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 4261. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.13107419 -0.10170883 -0.52351356  0.18362212]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 4262. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.09525645  0.04858613 -0.57983243  0.17978787]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 4263. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.26028728 -0.02064955 -0.758548    0.05476189]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 4264. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.08971691  0.14508104 -0.8573969   0.22570682]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 4265. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 2.3603439e-05  2.7658415e-01 -8.7323922e-01  1.9546437e-01]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 4266. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.04587603  0.09137738 -0.778647    0.22267771]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 4267. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.11400199  0.1717776  -0.64059097  0.23173559]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 4268. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.21327245  0.21272922 -0.7975412   0.25845098]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 4269. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.30874956  0.08704758 -0.57784426  0.16323888]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 4270. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.2744348   0.12107944 -0.762167    0.23258519]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 4271. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.24259543  0.13693857 -0.55945575  0.27028608]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 4272. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.08029282  0.34891582 -0.75470996  0.2631079 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 4273. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.13599432  0.07614255 -0.21003062  0.26608992]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 4274. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.29954767  0.08808494 -0.8724455   0.28941333]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 4275. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.42873633  0.51244855 -0.45382726  0.39318407]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 4276. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.38313484  0.20877028 -0.8405948   0.28073204]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 4277. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.45654488  0.11750185 -0.45156205  0.3354206 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 4278. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.4807366   0.19008422 -0.22040695  0.43418407]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 4279. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.5184834   0.26208115 -0.4207257   0.40034366]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 4280. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.35415888  0.4451418  -0.4305216   0.4044168 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 4281. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.415529    0.2714709  -0.464684    0.49889576]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 4282. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.49093783  0.38736176 -0.10844481  0.44603062]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 4283. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.49484026 0.19802749 0.28223515 0.534945  ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 4284. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.5140698   0.33560014 -0.40293026  0.4733013 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 4285. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.5150194   0.31426072 -0.47290242  0.538347  ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 4286. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.6758617   0.2069236  -0.1951986   0.61696315]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 4287. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.24410152 0.25310814 0.296829   0.61163855]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 4288. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.3918718   0.4324684  -0.32764584  0.62288094]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 4289. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.52079713  0.6882504  -0.57537746  0.7186997 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 4290. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.6656172   0.5373819  -0.14564103  0.5715153 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 4291. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.4268198  0.77391434 0.06474197 0.7337041 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 4292. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.647136   0.7225745  0.12227333 0.6497023 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 4293. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.6124737  0.6763449 -0.2975452  0.6469332]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 4294. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.5216272  0.72570455 0.7641454  0.7877916 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 4295. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.57737815  0.46983314 -0.30344778  0.57606053]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 4296. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.5900049  0.5202166  0.30707216 0.80018616]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 4297. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.5826385  0.29396904 0.05121815 0.6746919 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 4298. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.60039294  0.4225452  -0.75607854  0.7191429 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 4299. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.5492635   0.5347742  -0.44662267  0.6479919 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 4300. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.733269    0.42682934 -0.660033    0.6941221 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 4301. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[ 0.6556319   0.45927417 -0.11351854  0.7642331 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 4302. State = [[-0.15100437 -0.05499452  0.18513681  1.        ]]. Action = [[0.23557925 0.55531645 0.01671886 0.7473533 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 4303. State = [[-0.2591983  -0.01047316  0.11478962  1.        ]]. Action = [[0.5821167  0.7644055  0.49778676 0.7003958 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 4304. State = [[-0.25202227 -0.0030784   0.10225207  1.        ]]. Action = [[ 0.6490934   0.7488966  -0.01199687  0.85374546]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4305. State = [[-0.23088461  0.01136239  0.10500894  1.        ]]. Action = [[0.77919555 0.4602821  0.50345147 0.8055575 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4306. State = [[-0.20962273  0.02456053  0.11001369  1.        ]]. Action = [[ 0.33960414  0.16506279 -0.26630777  0.4553672 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 4307. State = [[-0.19864142  0.03292866  0.09913003  1.        ]]. Action = [[ 0.19526732  0.2556224  -0.67225176  0.3714832 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4308. State = [[-0.18906945  0.03923704  0.07732911  1.        ]]. Action = [[ 0.32069206  0.1116724  -0.8260938   0.34098232]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 4309. State = [[-0.1762815   0.04215775  0.05119606  1.        ]]. Action = [[ 0.20226431 -0.13229191 -0.4053806   0.07990253]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 4310. State = [[-0.16973938  0.04267941  0.04071653  1.        ]]. Action = [[-0.84818774 -0.88239175 -0.5304025  -0.6508999 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4311. State = [[-0.2660453   0.04486783  0.12021214  1.        ]]. Action = [[-0.907654   -0.728074   -0.01157176 -0.86669147]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 4312. State = [[-0.26298285  0.05303614  0.10055614  1.        ]]. Action = [[ 0.3343358   0.21813834 -0.9410063   0.47326255]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4313. State = [[-0.25557762  0.06040917  0.06625875  1.        ]]. Action = [[ 0.14615929  0.26028    -0.87996864  0.33936155]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4314. State = [[-0.24947016  0.06689828  0.041211    1.        ]]. Action = [[-0.88322896 -0.8724806   0.01767623 -0.87633175]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 4315. State = [[-0.24794658  0.06879703  0.03871259  1.        ]]. Action = [[-0.9909556 -0.9864201  0.5676477 -0.9960802]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 4316. State = [[-0.24786451  0.06950133  0.03868555  1.        ]]. Action = [[-0.9448225  -0.99705344  0.78607726 -0.9781767 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4317. State = [[-0.24790928  0.06969986  0.03867964  1.        ]]. Action = [[-0.9673617  -0.99225914  0.41939998 -0.98733294]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4318. State = [[-0.24792658  0.069902    0.03867387  1.        ]]. Action = [[-0.623641   -0.95332104  0.43661547 -0.9902081 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4319. State = [[-0.24793237  0.06996953  0.03867196  1.        ]]. Action = [[-0.99341595 -0.95450383  0.46790195 -0.99396354]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4320. State = [[-0.24793237  0.06996953  0.03867196  1.        ]]. Action = [[-0.97761935 -0.8963287   0.647912   -0.98345935]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4321. State = [[-0.24793237  0.06996953  0.03867196  1.        ]]. Action = [[-0.99947506 -0.99608445  0.45739472 -0.9728875 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4322. State = [[-0.24793237  0.06996953  0.03867196  1.        ]]. Action = [[-0.99948114 -0.9670752   0.5904944  -0.9943252 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4323. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.997008   -0.99406123  0.41708243 -0.97681546]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4324. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9991667  -0.98998654  0.7201152  -0.9565909 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4325. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9966965 -0.9815347  0.6289332 -0.9977467]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4326. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9344868  -0.9909189   0.8088951  -0.99469376]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4327. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99831694 -0.9754127   0.76627135 -0.99859136]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4328. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9062905  -0.9810319   0.5904305  -0.99666345]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4329. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9833382  -0.9419084   0.73247564 -0.99078166]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4330. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9768661  -0.97066486  0.6628003  -0.99613714]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4331. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9997127  -0.948501    0.57582617 -0.9922114 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4332. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9953059  -0.9767501   0.58603406 -0.9652994 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4333. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99809724 -0.95361596  0.61372745 -0.9635107 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4334. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9766178 -0.9538847  0.3998834 -0.9940127]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4335. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9431098  -0.98717767  0.72199774 -0.9862905 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4336. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9823621  -0.9368624   0.32634914 -0.9917552 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4337. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9533188  -0.93273526  0.5933076  -0.9910677 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4338. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9934572  -0.9930227   0.56481826 -0.9928134 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4339. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.93884265 -0.9894225   0.6615901  -0.98576194]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4340. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99954903 -0.983851    0.6337609  -0.9964668 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4341. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.95470816 -0.9126481   0.68448615 -0.9934385 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4342. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9988445  -0.9829064   0.7010541  -0.96261585]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4343. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.95542884 -0.97974116  0.3912053  -0.99422294]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4344. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9772691  -0.8946376   0.84576845 -0.93585825]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4345. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9919181 -0.9723798  0.8329222 -0.9103732]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4346. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.97474    -0.9537277   0.54593205 -0.9934234 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4347. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9934014  -0.95111644  0.5781021  -0.98740697]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4348. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.969529  -0.9666807  0.6080662 -0.9966718]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4349. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.98021775 -0.98378783  0.6018255  -0.9956507 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4350. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9446017  -0.83260787  0.5370492  -0.9963346 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4351. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99455416 -0.9959591   0.31635618 -0.99488795]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4352. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9954194  -0.96249604  0.5839169  -0.9581395 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4353. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99801433 -0.95937735  0.5949516  -0.9619943 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4354. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9502342  -0.9879997   0.44568324 -0.95620936]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4355. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9862386  -0.96352565  0.5554199  -0.9277753 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4356. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99572897 -0.98456156  0.49073207 -0.98605126]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4357. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99924743 -0.89463276  0.8264619  -0.99120563]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4358. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.98542225 -0.9429383   0.7305999  -0.9347618 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4359. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9216687  -0.70171005  0.58975375 -0.9969127 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4360. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9531532  -0.980187    0.7141514  -0.94507694]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4361. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9897993  -0.954689    0.23382425 -0.9841688 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4362. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.8586993  -0.9790947   0.68079233 -0.9809435 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4363. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.997171  -0.962815   0.4963436 -0.9857942]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4364. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99671805 -0.9980997   0.56770086 -0.9816925 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4365. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9958981  -0.95707613  0.6338197  -0.9943155 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4366. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9978509 -0.8615064  0.1043613 -0.9660733]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4367. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9935332  -0.99794793  0.29301357 -0.99123955]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4368. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9942338 -0.9962858  0.5127704 -0.998507 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4369. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9638106  -0.9796576   0.65814185 -0.9643089 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4370. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.976239   -0.9538253   0.6485901  -0.99575055]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4371. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9967812 -0.9773724  0.5004635 -0.9923258]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4372. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9873684  -0.91501796  0.3460598  -0.9973551 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4373. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99742085 -0.96174455  0.76101863 -0.86734045]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4374. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99919873 -0.9832917   0.7677617  -0.97478837]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4375. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99921536 -0.9608946   0.6752064  -0.9134596 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4376. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99994206 -0.8716063   0.673754   -0.9907822 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4377. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9977727  -0.99619174  0.8435869  -0.98108256]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4378. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99810475 -0.9412336   0.87640357 -0.9909618 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4379. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99877983 -0.9707604   0.7954463  -0.96012115]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4380. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9881486  -0.9796807   0.63100135 -0.9887597 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4381. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9520517  -0.98006433  0.77874506 -0.9559786 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4382. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.95841795 -0.96759915  0.6074171  -0.98838633]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4383. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9906963 -0.9224426  0.385782  -0.9452719]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4384. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.988249  -0.9877687 -0.0186078 -0.9564776]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4385. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9947373  -0.96073043  0.2912979  -0.98181146]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4386. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99345654 -0.9876971   0.49193347 -0.8758318 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4387. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9806648  -0.90845376  0.5312269  -0.98817044]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4388. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9990493 -0.9919117  0.3716414 -0.919478 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4389. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9013001  -0.9798188   0.38149524 -0.9516332 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4390. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.94831556 -0.9634819   0.72677445 -0.9854607 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4391. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.97495455 -0.91851825  0.23804069 -0.96992403]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4392. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99707264 -0.97843754  0.46100783 -0.961537  ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4393. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9212321 -0.9105533  0.5821049 -0.9944869]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4394. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99634534 -0.95931     0.6399317  -0.9552755 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4395. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9385518  -0.9848272   0.37652087 -0.9804427 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4396. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9983139  -0.8800566   0.61391425 -0.99367577]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4397. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.98089993 -0.9848767   0.65884995 -0.93045527]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4398. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.91016376 -0.9876204   0.6046251  -0.9858229 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4399. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9983222  -0.9756637   0.57857513 -0.99498016]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4400. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9966282  -0.9841774   0.5032078  -0.95180035]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4401. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9983491  -0.96289265  0.74705195 -0.9657738 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4402. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.98146045 -0.8469696   0.80665326 -0.8299726 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4403. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.99477446 -0.99076945  0.70161617 -0.70583   ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4404. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.98294526 -0.9721189   0.8492329  -0.9903987 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4405. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.96014947 -0.99466497  0.39654315 -0.9768028 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4406. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9864374  -0.9596794   0.43378568 -0.94985104]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4407. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9912405 -0.9849742  0.7005913 -0.9829303]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4408. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.9673835 -0.9487224  0.4445597 -0.9409189]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4409. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.73463696 -0.83098304  0.5357828  -0.9018005 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4410. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.98755723 -0.9741811   0.31687033 -0.9290039 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4411. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.8621451  -0.9565761   0.42933154 -0.9628182 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4412. State = [[-0.24793817  0.07003706  0.03867006  1.        ]]. Action = [[-0.97131395 -0.8827414   0.6079185  -0.9747598 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4413. State = [[-0.258666    0.01504558  0.12148927  1.        ]]. Action = [[-0.98394233 -0.9728869   0.18579888 -0.9265036 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4414. State = [[-0.25319463  0.02716207  0.10835989  1.        ]]. Action = [[0.54410934 0.6072395  0.03023195 0.6269655 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4415. State = [[-0.23868418  0.04402201  0.10337104  1.        ]]. Action = [[ 0.6153202   0.26629996 -0.58847535  0.70980215]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4416. State = [[-0.22106169  0.05444941  0.07775823  1.        ]]. Action = [[ 0.3350581   0.23097813 -0.8845365   0.49684572]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 4417. State = [[-0.21029711  0.06026255  0.0464323   1.        ]]. Action = [[ 0.04614198  0.00959659 -0.85597324  0.21896088]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4418. State = [[-0.26128474 -0.01823333  0.11878964  1.        ]]. Action = [[-0.96316403 -0.8704272   0.03426206 -0.8140574 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 4419. State = [[-0.25043842 -0.0177841   0.11212125  1.        ]]. Action = [[0.8142824 0.3701043 0.7889099 0.7602093]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4420. State = [[-0.22689767 -0.01664813  0.13072208  1.        ]]. Action = [[ 0.74425006 -0.0857417   0.867007    0.818586  ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4421. State = [[-0.20241734 -0.01655066  0.1597998   1.        ]]. Action = [[0.29646063 0.03018117 0.64038837 0.78385687]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 4422. State = [[-0.18390858 -0.01047764  0.178508    1.        ]]. Action = [[0.6627407  0.39789844 0.03731167 0.63452625]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4423. State = [[-0.16955437 -0.00239356  0.18098512  1.        ]]. Action = [[ 0.19439948  0.14300728 -0.4881295   0.36648548]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 4424. State = [[-0.16165625  0.00239012  0.17350698  1.        ]]. Action = [[ 0.01353264 -0.13524008 -0.77701855  0.197258  ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 4425. State = [[-0.16126502  0.00345792  0.17271425  1.        ]]. Action = [[ 0.17383516 -0.18229997 -0.70126325  0.36024964]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 4426. State = [[-0.1612389   0.00392905  0.17268753  1.        ]]. Action = [[ 0.32816792  0.15133417 -0.96192557  0.087147  ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 4427. State = [[-0.1612263   0.00451877  0.1726576   1.        ]]. Action = [[ 0.25042892  0.29705334 -0.81809986  0.19570374]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 4428. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.31548274  0.18107033 -0.47396624  0.0358448 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 4429. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.3367138   0.04083645 -0.68037844  0.20431805]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 4430. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.25120008  0.17877102 -0.80616385  0.24102187]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 4431. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.01017165  0.13002884 -0.97617877  0.18669796]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 4432. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.131482    0.07111382 -0.44929773  0.26677823]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 4433. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.20474088  0.16037226 -0.9312838   0.27087045]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 4434. State = [[-0.16121554  0.00471872  0.1726465   1.        ]]. Action = [[ 0.25368083  0.166731   -0.9447319   0.26930213]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 4435. State = [[-0.16121197  0.00478552  0.1726428   1.        ]]. Action = [[ 0.28653908 -0.08765596 -0.7391151   0.34182692]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 4436. State = [[-0.16121197  0.00478552  0.1726428   1.        ]]. Action = [[ 0.41347325  0.03564501 -0.81287986  0.27718067]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 4437. State = [[-0.16121197  0.00478552  0.1726428   1.        ]]. Action = [[ 0.2391454   0.2790537  -0.7674281   0.32312727]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 4438. State = [[-0.16121197  0.00478552  0.1726428   1.        ]]. Action = [[ 0.26808846  0.00778496 -0.5873259   0.25758564]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 4439. State = [[-0.16121197  0.00478552  0.1726428   1.        ]]. Action = [[ 0.07698762  0.01538563 -0.81510186  0.2663808 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 4440. State = [[-0.16121197  0.00478552  0.1726428   1.        ]]. Action = [[ 0.07947338 -0.03470308 -0.31050038  0.19824207]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 4441. State = [[-0.16120839  0.00485232  0.1726391   1.        ]]. Action = [[ 0.10901177  0.1577177  -0.76229864  0.3218913 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 4442. State = [[-0.16120839  0.00485232  0.1726391   1.        ]]. Action = [[ 0.20561182  0.20755506 -0.8137639   0.19252717]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 4443. State = [[-0.16120839  0.00485232  0.1726391   1.        ]]. Action = [[-0.13589686 -0.12773567 -0.79506546  0.16744101]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 4444. State = [[-0.16120839  0.00485232  0.1726391   1.        ]]. Action = [[ 0.2825414   0.04833519 -0.28503817  0.17985642]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 4445. State = [[-0.16120839  0.00485232  0.1726391   1.        ]]. Action = [[ 0.00921464  0.05802131 -0.46134663  0.29750586]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 4446. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.03720307 -0.07397902 -0.6081042   0.34403348]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 4447. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[-0.02595955  0.08735561 -0.9498494   0.2938378 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 4448. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.5202391   0.30606186 -0.7182363   0.17774999]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 4449. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.2875042   0.2970922  -0.82089406  0.29656982]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 4450. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.38512623  0.21876824 -0.400316    0.35665917]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 4451. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.22219205  0.09720969 -0.6063588   0.26510596]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 4452. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[-0.09008563  0.02938008 -0.7342002   0.53722286]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 4453. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.3019364  -0.05312687 -0.62419677  0.38296652]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 4454. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.25476408  0.15163672 -0.68264395  0.37170768]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 4455. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.24989462  0.16305423 -0.82276547  0.31971812]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 4456. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.2375704  -0.15298438 -0.73051405  0.24492252]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 4457. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.5030856  -0.05462217 -0.06884098  0.19941223]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 4458. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.10613394  0.16398668 -0.8017971   0.38009465]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 4459. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.13868558  0.13757372 -0.69129753  0.3257184 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 4460. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.23867893  0.00919795 -0.7629862   0.40039468]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 4461. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.3985486  -0.05722046 -0.6487606   0.4146737 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 4462. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.21991181 -0.03536922 -0.67453045  0.17662156]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 4463. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.44523716  0.26921487 -0.9161591   0.40329158]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 4464. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.22968996  0.3459537  -0.8563722   0.28798532]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 4465. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.4514085   0.11612213 -0.6124058   0.3889413 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 4466. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.42629445  0.19317722 -0.27353954  0.4397341 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 4467. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.34750712  0.15821981 -0.9334036   0.3268677 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 4468. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.43844748  0.01568377 -0.32790232  0.32029796]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 4469. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.528674   -0.20884943 -0.86791384  0.29549742]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 4470. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.16427612  0.31637955 -0.4766692   0.4410746 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 4471. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.23949456  0.34806192 -0.71424997  0.41448462]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 4472. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.17388391  0.3037926  -0.42820907  0.46672773]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 4473. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.22682667  0.13511169 -0.63253325  0.17604578]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 4474. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.26885223  0.19422889 -0.88527495  0.3929937 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 4475. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.11779404  0.1006974  -0.67877734  0.29222083]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 4476. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.23496604  0.23812449 -0.8418536   0.37766218]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 4477. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.38896143  0.00553417 -0.8454993   0.43032324]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 4478. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.44616115  0.35167217 -0.53795964  0.5048052 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 4479. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.20469785  0.40542674 -0.34730327  0.30275917]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 4480. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.10545349  0.02978647 -0.78588635  0.27865422]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 4481. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.2153616  -0.07409853 -0.87780815  0.21973598]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 4482. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[-0.03296912  0.13299966 -0.9408858   0.3222282 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 4483. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.3948846   0.25497925 -0.5645159   0.25113428]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 4484. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.06805074  0.2890389  -0.97163343  0.28076863]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 4485. State = [[-0.16120481  0.00491912  0.1726354   1.        ]]. Action = [[ 0.26435518  0.19126725 -0.4499796   0.20162416]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 4486. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.03749442  0.00277567 -0.948949    0.38689005]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 4487. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.17166305  0.2281425  -0.77604854  0.18920541]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 4488. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.38116097  0.2224344  -0.8963484   0.19069242]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 4489. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.33974695  0.13674748 -0.5814136   0.32356215]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 4490. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.14580345  0.26359558 -0.8551965   0.22125828]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 4491. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.15695024  0.1300683  -0.73403716  0.09976196]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 4492. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.11491561  0.3803687  -0.9773735   0.42939067]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 4493. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.24555945  0.12964165 -0.96537834  0.15099418]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 4494. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[-0.17258722  0.15026617 -0.8133113   0.35648108]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 4495. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.02159572  0.22237849 -0.9545055   0.13777661]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 4496. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.20474052  0.30394578 -0.9261784   0.3461554 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 4497. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.3685665   0.29584837 -0.9545598   0.2677672 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 4498. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.37315392 -0.12354434 -0.9649769   0.3363707 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 4499. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.19356811  0.2793194  -0.6583024   0.21274889]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 4500. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.11514628  0.17792821 -0.8055967   0.29312062]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 4501. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.52171755  0.17749429 -0.9823085   0.30564475]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 4502. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.29359794  0.13043821 -0.9025446   0.3111081 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 4503. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.0956316   0.34550405 -0.7610876   0.34178293]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 4504. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.15684736  0.07035756 -0.94113696  0.1599456 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 4505. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.16662014  0.18461955 -0.96866405  0.3174634 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 4506. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.20315695  0.09692967 -0.79115295  0.25580382]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 4507. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.13303685 -0.01013696 -0.9777987   0.07562053]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 4508. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.22264254  0.11972058 -0.9392823   0.21818209]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 4509. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.24645495  0.26787806 -0.9768494   0.29273272]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 4510. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.27983046  0.21470118 -0.909693    0.17959344]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 4511. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.37970877  0.12510264 -0.83603483  0.18474042]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 4512. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.39581227  0.11544394 -0.6523325   0.14791143]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 4513. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.3658477   0.05398667 -0.97064006  0.22420001]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 4514. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.27768755  0.18611753 -0.9183149   0.32814503]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 4515. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.49223948 -0.03983682 -0.86059266  0.32240272]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 4516. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.06384504 -0.03006881 -0.92624044  0.18934369]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 4517. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.32293105  0.20899892 -0.13573503  0.24236608]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 4518. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.08999455 -0.01505184 -0.9060611   0.2410636 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 4519. State = [[-0.16120128  0.00498547  0.17263176  1.        ]]. Action = [[ 0.56831074  0.03568459 -0.90030354  0.262321  ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 4520. State = [[-0.26935333  0.17198153  0.12718871  1.        ]]. Action = [[ 0.42486012 -0.05766356 -0.29558885  0.24620306]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 4521. State = [[-0.26662755  0.1919497   0.10693635  1.        ]]. Action = [[ 0.18086791  0.12528801 -0.93763506  0.30993414]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4522. State = [[-0.26147634  0.1946977   0.08105862  1.        ]]. Action = [[-0.12230736 -0.05552346 -0.88364667  0.23808765]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 4523. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8868098  -0.31646073 -0.97006756 -0.47509152]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 4524. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.749399   -0.27906072 -0.95314515 -0.54484963]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 4525. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9082391 -0.2527361 -0.9903269 -0.5883731]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4526. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8746697  -0.176373   -0.9944263  -0.56219643]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4527. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.856332   -0.5755538  -0.90169424 -0.61650836]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4528. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9134692  -0.08667755 -0.8515772  -0.5633579 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4529. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.88706106 -0.35946608 -0.9531692  -0.57942486]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4530. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9274577  -0.2552172  -0.949022   -0.50489724]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4531. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9240426  -0.30281854 -0.95635176 -0.40317565]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4532. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8965992 -0.3016281 -0.9550599 -0.5491461]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4533. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.91689134 -0.19573486 -0.93140006 -0.507693  ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4534. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9090741  -0.2643184  -0.9797435  -0.47293425]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4535. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9022744  -0.44509625 -0.98096526 -0.4214865 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4536. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8491915  -0.31762135 -0.99223053 -0.71846676]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4537. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.83429533 -0.4010383  -0.93694603 -0.4269632 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4538. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.94343776 -0.3787623  -0.97238165 -0.34747577]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4539. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9411326  -0.17295289 -0.98678017 -0.61002254]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4540. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.932608   -0.29690623 -0.6399164  -0.36620307]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4541. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9361213  -0.31437063 -0.9573767  -0.63100284]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4542. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9203272  -0.24786246 -0.98821867 -0.37336648]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4543. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.92783725 -0.43992573 -0.91217965 -0.7122881 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4544. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.863589   -0.18319356 -0.95802337 -0.7142263 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4545. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9093804  -0.39696836 -0.98438776 -0.53719705]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4546. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.93622583 -0.42736912 -0.974598   -0.7817503 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4547. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9067663  -0.3112797  -0.97165066 -0.4568441 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4548. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9431209 -0.4318012 -0.9115122 -0.5588865]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4549. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9293948  -0.47396946 -0.94242126 -0.73066837]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4550. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8626425  -0.46669972 -0.97447604 -0.6146798 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4551. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9369455 -0.4332801 -0.9476653 -0.7421983]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4552. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9151294  -0.48827565 -0.98522806 -0.6639698 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4553. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9237604  -0.25375152 -0.9821275  -0.55564797]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4554. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9122074  -0.42682153 -0.9627199  -0.57210445]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4555. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96141624 -0.04136658 -0.9545517  -0.50178236]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4556. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.945465   -0.4521699  -0.92683285 -0.5720437 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4557. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9563551  -0.48417604 -0.96050173 -0.5894815 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4558. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8847512 -0.153148  -0.9444619 -0.5115676]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4559. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9073966  -0.21114933 -0.94092643 -0.55554205]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4560. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.88460374 -0.31380135 -0.9644478  -0.6012012 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4561. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.93743676 -0.37235594 -0.91546476 -0.28716087]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4562. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8792035  -0.32696378 -0.90587896 -0.53514075]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4563. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9238968  -0.39726317 -0.9260042  -0.6589974 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4564. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9543454  -0.33509147 -0.98169035 -0.7489741 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4565. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9387078  -0.47210574 -0.9494025  -0.46494406]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4566. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9301841  -0.39071476 -0.9121918  -0.64825755]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4567. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.88779825 -0.5611711  -0.9830278  -0.76378584]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4568. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9338251  -0.37964118 -0.97313267 -0.75829864]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4569. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.91409045 -0.5224889  -0.9812627  -0.60888004]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4570. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9504972  -0.41861993 -0.95691556 -0.62784004]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4571. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9426175  -0.21320677 -0.9861871  -0.45012438]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4572. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96277267 -0.33756453 -0.5435479  -0.792788  ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4573. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96494544 -0.44816804 -0.91035277 -0.54288983]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4574. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96639425 -0.2992006  -0.6148676  -0.40371656]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4575. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9213984  -0.48539776 -0.9565347  -0.7426166 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4576. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9457273 -0.536812  -0.9742492 -0.672218 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4577. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9638775  -0.4153831  -0.96104187 -0.66779494]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4578. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.93536425 -0.46557367 -0.9894948  -0.6676128 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4579. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9001702  -0.3926654  -0.97400606 -0.42630708]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4580. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96746755 -0.3341061  -0.94169635 -0.5679086 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4581. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.92427415 -0.39266992 -0.9884287  -0.7397445 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4582. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.95275223 -0.28880537 -0.9666224  -0.42123413]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4583. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9350738  -0.27383375 -0.9857766  -0.75831074]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4584. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8577357  -0.09991336 -0.9320201  -0.6822862 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4585. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9059991  -0.32062364 -0.96014327 -0.69818074]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4586. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9551081  -0.36250675 -0.9238481  -0.64765906]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4587. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96296346 -0.4815129  -0.9709529  -0.6536139 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4588. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9247496  -0.24465549 -0.9458155  -0.50275767]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4589. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8664065  -0.17904854 -0.95190537 -0.72696114]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4590. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96063215 -0.43433952 -0.9795028  -0.5241297 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4591. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8882953  -0.26355588 -0.91679245 -0.4947927 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4592. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.95800275 -0.5189145  -0.9829709  -0.5145578 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4593. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.8865532  -0.46378726 -0.9285122  -0.5090065 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4594. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9615885  -0.37855554 -0.9387933  -0.5172443 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4595. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9594733  -0.21427959 -0.95879984 -0.6636245 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4596. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.93578786 -0.4324658  -0.9844927  -0.70684636]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4597. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.96784973 -0.33076274 -0.9212386  -0.54771495]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4598. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9533199  -0.31780422 -0.9592636  -0.768883  ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4599. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.92940426 -0.517609   -0.96136403 -0.72310644]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4600. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.98213977 -0.22617662 -0.94539523 -0.6248935 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4601. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9747621  -0.38481832 -0.93463516 -0.7788393 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4602. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.97595876 -0.36205375 -0.98696876 -0.4223855 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4603. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9609307  -0.40003753 -0.9358612  -0.56670356]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4604. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.94317603 -0.27165622 -0.94421023 -0.57096803]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4605. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9830861  -0.37017393 -0.83974755 -0.57979923]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4606. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.94493264 -0.38649356 -0.98067915 -0.64279175]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4607. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9653224  -0.18246657 -0.96842134 -0.5653534 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4608. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9516453  -0.27735138 -0.9153063  -0.5898004 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4609. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9894463  -0.22569233 -0.9589318  -0.38398516]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4610. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9755705 -0.5413431 -0.9779303 -0.7431059]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4611. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9726738 -0.3832134 -0.9622527 -0.8050252]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4612. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9151606 -0.5199636 -0.9865227 -0.6842274]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4613. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9303154  -0.36357182 -0.98062027 -0.6282791 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4614. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9802265  -0.31622607 -0.98456943 -0.47192836]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4615. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9626219  -0.18289405 -0.9909401  -0.7389461 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4616. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.94182986 -0.31342214 -0.984447   -0.7479498 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4617. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.94486964 -0.40778238 -0.9699955  -0.67434496]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4618. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9863569  -0.35391867 -0.97281945 -0.7190119 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4619. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9798012  -0.39162934 -0.9852168  -0.7064139 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4620. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.83395433 -0.39540136 -0.95375437 -0.6714043 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4621. State = [[-0.25968927  0.19562209  0.07827041  1.        ]]. Action = [[-0.9754622 -0.5201886 -0.9909253 -0.5301068]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4622. State = [[-0.25843713 -0.04217948  0.11835823  1.        ]]. Action = [[-0.89624006 -0.4353171  -0.97716457 -0.873524  ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4623. State = [[-0.25256503 -0.0439281   0.1032248   1.        ]]. Action = [[ 0.52548087  0.3701918  -0.1824125   0.8214054 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4624. State = [[-0.23975751 -0.03690912  0.09258737  1.        ]]. Action = [[ 0.287825    0.3179065  -0.40026194  0.7838638 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4625. State = [[-0.2250788  -0.02650426  0.07977708  1.        ]]. Action = [[ 0.5675111   0.3307178  -0.27402568  0.5809152 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 4626. State = [[-0.2063603  -0.01584451  0.07449949  1.        ]]. Action = [[0.4295466  0.19522357 0.20363474 0.5367228 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4627. State = [[-0.19834167 -0.01176427  0.07590865  1.        ]]. Action = [[ 0.5416999   0.14321935 -0.9584922   0.50079226]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4628. State = [[-0.19677587 -0.01058357  0.07617048  1.        ]]. Action = [[ 0.4707408   0.08667207 -0.9609473   0.52789974]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4629. State = [[-0.19337773 -0.00671769  0.07164239  1.        ]]. Action = [[ 0.25508046  0.22346818 -0.40853626  0.6548226 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 4630. State = [[-0.185776    0.00335907  0.05643732  1.        ]]. Action = [[ 0.39735687  0.45736122 -0.85556376  0.5785897 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 4631. State = [[-0.1730372   0.01184964  0.03394892  1.        ]]. Action = [[-0.11048812 -0.26295614 -0.81794816  0.12739515]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4632. State = [[-0.17213897  0.01389793  0.0320491   1.        ]]. Action = [[-0.9599597  -0.04771256 -0.96167314 -0.48425752]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4633. State = [[-0.17188618  0.01437978  0.03177696  1.        ]]. Action = [[-0.8658279  -0.24756515 -0.891655   -0.43234593]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4634. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.59447944 -0.31070113 -0.9604731  -0.24485898]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4635. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.9215318  -0.20109886 -0.8996016  -0.31125206]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4636. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.874028   -0.28459716 -0.9006038  -0.38492358]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4637. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.8931433  -0.4116422  -0.78627634 -0.51990724]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4638. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.7560181  -0.48991424 -0.9711002  -0.35125554]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4639. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.8200474  -0.20686662 -0.9514083  -0.574615  ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4640. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.81964713 -0.1845631  -0.97938746 -0.3292539 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4641. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.9018573  -0.09600765 -0.93686146 -0.26737   ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4642. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.9476042  -0.1774416  -0.9318476  -0.36399567]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4643. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.8950413  -0.31842124 -0.95845515 -0.2525578 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4644. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.67174613 -0.25759578 -0.91531396 -0.53041357]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4645. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.9564011  -0.21460617 -0.8784783  -0.3034618 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4646. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.7247752  -0.28414154 -0.9136854  -0.30709088]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4647. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.8901622  -0.55257595 -0.92250043 -0.30586362]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4648. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.41904855 -0.27187842 -0.9307612  -0.48045647]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4649. State = [[-0.17155637  0.01445756  0.03184626  1.        ]]. Action = [[-0.87508523 -0.33172154 -0.8538919  -0.12898254]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4650. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.9330407  -0.27723753 -0.9277291  -0.44042927]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4651. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.89584225 -0.27121437 -0.93871874 -0.2591461 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4652. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.8452008  -0.21719176 -0.946995   -0.24588537]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4653. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.95800847 -0.37037432 -0.96499825 -0.31259167]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4654. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.76303077 -0.15389127 -0.8562549  -0.37225968]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4655. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.7999749  -0.0353139  -0.7288583  -0.06511712]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4656. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.86936474 -0.04080725 -0.96162874 -0.42035365]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4657. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.9070678  -0.458593   -0.9194508  -0.49472678]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4658. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.84678036 -0.28703237 -0.8696109  -0.5157247 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4659. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.6654581  -0.30246687 -0.9758993  -0.36617577]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4660. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.86082107 -0.2533192  -0.93947476 -0.380417  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4661. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.9469365  -0.19926786 -0.9143016  -0.3323015 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4662. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.49015105 -0.20364922 -0.9397422  -0.17391706]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4663. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.90723425 -0.26966804 -0.9109752  -0.39045155]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4664. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.90605915 -0.29325867 -0.9616141  -0.3290357 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4665. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.9346633  -0.23901296 -0.99007714 -0.50726736]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4666. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.7344436  -0.08007604 -0.91979516 -0.40159535]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4667. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.9286754  -0.26257843 -0.94620365 -0.38218117]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4668. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.89207715 -0.35719907 -0.967255   -0.23411459]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4669. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.9346062   0.01555693 -0.932173   -0.2735272 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4670. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.92190605 -0.23822504 -0.9848134  -0.60016507]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4671. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.73254937 -0.29731995 -0.93174654 -0.33696043]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4672. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.8859781 -0.3025099 -0.8994438 -0.3369518]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4673. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.8880034  -0.19436729 -0.89973474 -0.08246422]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4674. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.84285724 -0.2621354  -0.8147169  -0.34411335]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4675. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.3238064  -0.13970393 -0.898247   -0.35413802]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4676. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.6683183  -0.13220209 -0.87379986 -0.49047792]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4677. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.90106    -0.2859254  -0.89638555 -0.29615474]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4678. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.86180174 -0.14782095 -0.919644   -0.4611628 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4679. State = [[-0.17155409  0.01452923  0.03184454  1.        ]]. Action = [[-0.8365907  -0.11666268 -0.9499675  -0.3065337 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4680. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.8836802  -0.15329945 -0.9698927  -0.1452331 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4681. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.84756297 -0.23666859 -0.96751255 -0.5944618 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4682. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.74309826 -0.17462736 -0.9866118  -0.39250618]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4683. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.93391675 -0.19441986 -0.9602841  -0.18805206]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4684. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.94999075 -0.29028523 -0.9583517  -0.5493899 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4685. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.82264006 -0.2645409  -0.81434524 -0.6026749 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4686. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.9535762  -0.19148898 -0.9674797  -0.48754388]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4687. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.81881684 -0.22451973 -0.9720089  -0.5962373 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4688. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.83787686 -0.18313396 -0.89817166 -0.36432052]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4689. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.73986006 -0.36457705 -0.78306824 -0.58463365]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4690. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.7112773  -0.30783403 -0.9923295  -0.409693  ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4691. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.92295116 -0.09031194 -0.9511754  -0.49990737]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4692. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.5933362  -0.22654879 -0.87764835 -0.4115008 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4693. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.92513555 -0.21514541 -0.8066871  -0.41829264]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4694. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.6917714  -0.27614963 -0.96330845 -0.13047546]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4695. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.9564135  -0.16356182 -0.9348367  -0.31302792]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4696. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.621209   -0.22859085 -0.7037907  -0.570739  ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4697. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.7740499  -0.41627538 -0.96767443 -0.2049861 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4698. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.84657973  0.03166771 -0.9650341  -0.4794835 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4699. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.94135606 -0.23677367 -0.8921501  -0.24294055]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4700. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.56910443 -0.3028847  -0.9759472  -0.38009095]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4701. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.96954834 -0.1021533  -0.9510838  -0.11568391]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4702. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.8816372  -0.30762386 -0.98296756 -0.32948363]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4703. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.9509481  -0.1409685  -0.9512257  -0.19576418]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4704. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.93644863 -0.20492423 -0.9854811  -0.33124655]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4705. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.6841836  -0.14917749 -0.9532169  -0.28826922]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4706. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.80222666 -0.20520484 -0.9529594  -0.3447802 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4707. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.8492242  -0.13051319 -0.89108634 -0.17321229]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4708. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.6255865  -0.25809354 -0.8658179  -0.4295236 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4709. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.5888335  -0.40355635 -0.9546985  -0.13546795]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4710. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.85857975 -0.16256332 -0.90841794 -0.45250803]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4711. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.7064054  -0.10523188 -0.97042507 -0.35157645]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4712. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.57670134 -0.3024041  -0.8923179  -0.35278285]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4713. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.7902707  -0.13484728 -0.92825884 -0.27902317]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4714. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.93171155 -0.3571797  -0.9292795  -0.43427283]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4715. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.3021561  -0.2468009  -0.9211381  -0.25808966]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4716. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.9112661  -0.16895968 -0.9100446  -0.0813598 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4717. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.797462   -0.09106892 -0.9560846  -0.36428833]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4718. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.89802504 -0.34406656 -0.8915999  -0.35477877]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4719. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.44828498 -0.33807963 -0.98469925 -0.19572031]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4720. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.9405523  -0.1909079  -0.8606402  -0.25567448]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4721. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.8619426  -0.07190347 -0.90534943 -0.49758995]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4722. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.83312446 -0.20575827 -0.98309934 -0.10501271]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4723. State = [[-0.17155184  0.01460042  0.03184284  1.        ]]. Action = [[-0.65185165 -0.12925369 -0.8229965  -0.16990459]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4724. State = [[-0.26018864 -0.01409619  0.11870785  1.        ]]. Action = [[-0.7691342  -0.27906668 -0.9817114  -0.18215454]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4725. State = [[-0.2540533  -0.01265696  0.10042492  1.        ]]. Action = [[ 0.65558565  0.40279222 -0.4803269   0.7575693 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4726. State = [[-0.23447637 -0.00735658  0.08913952  1.        ]]. Action = [[0.55953217 0.19695556 0.07662785 0.55584764]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4727. State = [[-0.22692072 -0.00153975  0.08846062  1.        ]]. Action = [[-0.33456182  0.1739608   0.02036858  0.6346803 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 4728. State = [[-0.22856441  0.00653886  0.08163319  1.        ]]. Action = [[ 0.30165446  0.22490549 -0.93031853  0.61209464]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4729. State = [[-0.21846884  0.01117158  0.04972074  1.        ]]. Action = [[ 0.3481226  -0.05313951 -0.8576108   0.56119156]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 4730. State = [[-0.20877704  0.01167305  0.02694112  1.        ]]. Action = [[-0.15079993 -0.14911604 -0.8142678   0.11580145]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4731. State = [[-0.2075516   0.01170944  0.02478073  1.        ]]. Action = [[-0.98757285 -0.2632699  -0.865699   -0.51624364]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4732. State = [[-0.2073187   0.01171239  0.02481508  1.        ]]. Action = [[-0.6899829 -0.3351137 -0.8962814 -0.5921588]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4733. State = [[-0.2073187   0.01171239  0.02481508  1.        ]]. Action = [[-0.94906753 -0.37260634 -0.91209936 -0.5007034 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4734. State = [[-0.2073187   0.01171239  0.02481508  1.        ]]. Action = [[-0.85537285 -0.41554946 -0.88344955 -0.44047397]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4735. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91913617 -0.29880905 -0.94571    -0.564463  ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4736. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.98372865 -0.33667403 -0.9342392  -0.5537878 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4737. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.75575227 -0.37769043 -0.9594009  -0.64134485]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4738. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.94686645 -0.10575396 -0.8808948  -0.55481744]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4739. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.6205068  -0.3284421  -0.92227083 -0.562996  ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4740. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.95895666 -0.3645594  -0.9383167  -0.59657824]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4741. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.94501156 -0.26363373 -0.880359   -0.52749276]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4742. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.51681924 -0.2913885  -0.83791834 -0.24410307]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4743. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9750204  -0.24341321 -0.9245207  -0.5370969 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4744. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.98785096 -0.1824776  -0.9127952  -0.2077204 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4745. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9043414  -0.13521934 -0.93269986 -0.50295174]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4746. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91650045 -0.47442073 -0.8625117  -0.5371559 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4747. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.94510627 -0.41964686 -0.88413477 -0.30277824]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4748. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.89227563 -0.40720433 -0.742077   -0.4096992 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4749. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9288878  -0.42704928 -0.92593366 -0.54316354]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4750. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.97881526 -0.3725571  -0.9624313  -0.6370829 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4751. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.947139   -0.27996254 -0.8664302  -0.33848542]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4752. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.97500485 -0.21964204 -0.8603271  -0.6318133 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4753. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9717013  -0.45173824 -0.87111443 -0.65844035]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4754. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8939297 -0.36092   -0.8136893 -0.5152045]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4755. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.74453276 -0.27313685 -0.9521503  -0.5672949 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4756. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8612265 -0.440215  -0.8766141 -0.6944981]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4757. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91944975 -0.0874539  -0.8410353  -0.32230198]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4758. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.95944387 -0.31692594 -0.96069354 -0.46420366]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4759. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.85509753 -0.23451912 -0.8899805  -0.44973767]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4760. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.93487203 -0.38447738 -0.95562947 -0.7005713 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4761. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.97292084 -0.25876582 -0.9813988  -0.37382102]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4762. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91015637 -0.30683267 -0.933013   -0.39826053]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4763. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.89652497 -0.3797956  -0.9512034  -0.50160533]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4764. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.86844116 -0.1865775  -0.5527755  -0.6071212 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4765. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9877164  -0.1575998  -0.6981466  -0.36573362]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4766. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.89078754 -0.3225913  -0.87043434 -0.6481765 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4767. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8986947  -0.14809799 -0.58871317 -0.74729514]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4768. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9229121   0.03443778 -0.7917895  -0.33180642]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4769. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8482529  -0.18247211 -0.98671985  0.158391  ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4770. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.86067116 -0.09344798 -0.935608   -0.30317152]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4771. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8168948  -0.16284221 -0.9187578  -0.12139112]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4772. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.76800007 -0.26299775 -0.93846595 -0.29615784]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4773. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8972594  -0.3908177  -0.8004208  -0.22894299]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4774. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8590916  -0.27070415 -0.97363055 -0.48143232]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4775. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91521925 -0.39884418 -0.90857    -0.12141854]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4776. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9247939  -0.05490786 -0.8526116  -0.54643446]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4777. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91378784 -0.16091835 -0.9123952  -0.22848928]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4778. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8780624  -0.24070412 -0.7280337  -0.47316718]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4779. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.47355485 -0.21330214 -0.9705916  -0.4084499 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4780. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8793369  -0.29421157 -0.91127616 -0.5535856 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4781. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7955464  -0.12502682 -0.97648066 -0.5367469 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4782. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.935193   -0.16966498 -0.4329883  -0.35776293]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4783. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7300717  -0.17972964 -0.518379   -0.27529937]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4784. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.22367477 -0.19822335 -0.9587565  -0.3557539 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4785. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8778603  -0.33356166 -0.7915834  -0.24967194]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4786. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.95455134 -0.3484503  -0.9606593  -0.4221496 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4787. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.96963316 -0.2872777  -0.6691592  -0.26965725]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4788. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.5187099  -0.23312163 -0.8927984  -0.5071614 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4789. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9038593  -0.23282743 -0.9483443  -0.39598173]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4790. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.95353866 -0.18928987 -0.9219994  -0.35247833]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4791. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.91949934 -0.19858456 -0.87708557 -0.05882627]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4792. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8515408 -0.2373581 -0.9617835 -0.519702 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4793. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9382275  -0.18142515 -0.96642953 -0.13121736]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4794. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8117074  -0.28219658 -0.9532763  -0.04181999]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4795. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.70653754 -0.23414642 -0.7101813  -0.4656459 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4796. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8439307  -0.26328695 -0.8348754  -0.16243047]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4797. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7255032  -0.22304577 -0.92718744 -0.4799248 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4798. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7704012  -0.21550012 -0.8971601  -0.46921396]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4799. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7781763  -0.19365823 -0.92828643 -0.19223899]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4800. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.77749366 -0.41274965 -0.9563111  -0.4110316 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4801. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7926482  -0.34788537 -0.9735705  -0.5268699 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4802. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.67013115 -0.44735038 -0.96181375 -0.5921048 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4803. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8892344  -0.44566762 -0.9634     -0.4221077 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4804. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7674785  -0.296987   -0.935038   -0.32036018]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4805. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8924874  -0.16323388 -0.9576927  -0.45989215]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4806. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.46446192 -0.49197578 -0.9558219  -0.22173476]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4807. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.87678397 -0.400339   -0.97803926 -0.4152373 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4808. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8369872  -0.2713133  -0.9470004  -0.36044532]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4809. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8665483  -0.20422572 -0.95682126 -0.19706035]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4810. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.55602765 -0.27926308 -0.94888836 -0.14130378]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4811. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8241195  -0.28846967 -0.9666662  -0.3240878 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4812. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9062052 -0.4781679 -0.9722839 -0.1498496]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4813. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8160789  -0.42205507 -0.96476185 -0.13290691]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4814. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9177405  -0.34441817 -0.92173874 -0.26035547]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4815. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.60737026 -0.2633859  -0.963617   -0.05427641]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4816. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.45874947 -0.22653466 -0.9430694  -0.342916  ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4817. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.5802941  -0.14673948 -0.9496143  -0.18173236]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4818. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9090211  -0.30005097 -0.94752324 -0.27286625]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4819. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7721997  -0.4292531  -0.875189   -0.01170892]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4820. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.32630658 -0.31867576 -0.9767601  -0.17063534]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4821. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.82165724 -0.13136971 -0.9655175  -0.1630727 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4822. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.7525391  -0.25206697 -0.907646   -0.31772852]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4823. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.48941064 -0.31507707 -0.64351386 -0.4029078 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4824. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.9299948  -0.4320699  -0.92192125 -0.48903668]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4825. State = [[-0.20724311  0.01171335  0.02482625  1.        ]]. Action = [[-0.8543811   0.05469394 -0.94517016  0.42082977]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4826. State = [[-0.2688982   0.16839078  0.1247265   1.        ]]. Action = [[-0.86688405 -0.1320194  -0.8849969  -0.1008355 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4827. State = [[-0.26674148  0.18673025  0.10554961  1.        ]]. Action = [[ 0.20517683 -0.03052062 -0.99580693  0.2856785 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4828. State = [[-0.26118115  0.18893933  0.07704177  1.        ]]. Action = [[-0.7997777  -0.21792483 -0.990939   -0.3445285 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 4829. State = [[-0.26001453  0.18919803  0.07476211  1.        ]]. Action = [[-0.9916382  -0.34100783 -0.91001457 -0.89008904]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 4830. State = [[-0.25994465  0.18924317  0.0743202   1.        ]]. Action = [[-0.9928372  -0.20949328 -0.7647995  -0.9449891 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 4831. State = [[-0.2596107   0.1892857   0.07417254  1.        ]]. Action = [[-0.9802924  -0.34854126 -0.93481785 -0.98164064]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4832. State = [[-0.25919193  0.18937975  0.07401488  1.        ]]. Action = [[-0.9901613  -0.3432938  -0.91993517 -0.9349909 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4833. State = [[-0.25900164  0.18943919  0.07406504  1.        ]]. Action = [[-0.90078074 -0.37965107 -0.9661448  -0.90759444]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4834. State = [[-0.25900164  0.18943919  0.07406504  1.        ]]. Action = [[-0.9478611  -0.36203766 -0.9766331  -0.8359919 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4835. State = [[-0.25900164  0.18943919  0.07406504  1.        ]]. Action = [[-0.9930086  -0.39519274 -0.92275876 -0.9593221 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4836. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97869134 -0.13513434 -0.991247   -0.91565305]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4837. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98195004 -0.33868587 -0.9699031  -0.82819384]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4838. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9976058  -0.15387356 -0.98652554 -0.8161474 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4839. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.99714744 -0.37653708 -0.9924937  -0.9675945 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4840. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9934395  -0.32331192 -0.9887875  -0.8728904 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4841. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98571956 -0.2355727  -0.901894   -0.9105828 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4842. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.96595156 -0.14092779 -0.9820945  -0.7791323 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4843. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97699195 -0.2921002  -0.9126826  -0.92430687]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4844. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97780234 -0.23360336 -0.85447884 -0.6120966 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4845. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.99459344 -0.2865193  -0.9595678  -0.93152916]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4846. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.94104874 -0.26476848 -0.68642557 -0.86891174]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4847. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.92590725 -0.27858293 -0.93835    -0.9221064 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4848. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9739319  -0.29524338 -0.8600778  -0.8399124 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4849. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9957494  -0.08312136 -0.78928006 -0.92699564]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4850. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9960459  -0.37667876 -0.7479366  -0.7512345 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4851. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97100013 -0.33383644 -0.964002   -0.9478533 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4852. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9071213  -0.38426685 -0.9573124  -0.67995787]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4853. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9196266 -0.272182  -0.9899307 -0.9118415]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4854. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9785775  -0.32699227 -0.9957604  -0.86945087]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4855. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98543763 -0.3361497  -0.98065513 -0.8520507 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4856. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98325163 -0.27037692 -0.9911017  -0.89988893]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4857. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97921544 -0.38852793 -0.7255273  -0.89067787]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4858. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9673084 -0.403185  -0.8755555 -0.8515787]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4859. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9928257  -0.47538918 -0.9940091  -0.90415186]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4860. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9950016  -0.32323635 -0.9827399  -0.95809436]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4861. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98407257 -0.29970068 -0.93534905 -0.9540431 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4862. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9513742  -0.30942822 -0.9646998  -0.9150507 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4863. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9882647  -0.29199725 -0.9925921  -0.9266972 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4864. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9607134  -0.25180662 -0.9749977  -0.88606834]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4865. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9817146  -0.31566304 -0.9938754  -0.72501737]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4866. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.94710743 -0.33693945 -0.8812418  -0.74846536]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4867. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97730726 -0.24882126 -0.9846821  -0.85444564]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4868. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9900979  -0.31142688 -0.9270668  -0.8853821 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4869. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9722157  -0.29806077 -0.75395525 -0.80472356]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4870. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9553138 -0.268741  -0.7137834 -0.9102907]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4871. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.988738   -0.3058554  -0.95155346 -0.9296081 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4872. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9733395  -0.25676215 -0.81265676 -0.87761104]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4873. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9785591  -0.27745944 -0.9566398  -0.7694553 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4874. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.7519779 -0.26939   -0.9546948 -0.6538728]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4875. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97364897 -0.27925873 -0.9776869  -0.91268885]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4876. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9789329  -0.25598526 -0.44606233 -0.7655878 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4877. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9603405  -0.12702364 -0.9900265  -0.90336317]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4878. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9581677  -0.38187706 -0.98331827 -0.48580915]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4879. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9753709  -0.16531527 -0.92177767 -0.7658133 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4880. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9702201  -0.11785471 -0.9762945  -0.812565  ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4881. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9718542  -0.2603101  -0.96538275 -0.8723609 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4882. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.92654705 -0.25846517 -0.99624777 -0.7796091 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4883. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98607427 -0.22711372 -0.8974427  -0.7816022 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4884. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98777884 -0.25692654 -0.9825272  -0.9271996 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4885. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.96724665 -0.21925473 -0.98407394 -0.7572075 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4886. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.94754815 -0.22129166 -0.9261365  -0.76809806]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4887. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.90482587 -0.14995801 -0.95889914 -0.7726342 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4888. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.95337826 -0.35360408 -0.985096   -0.90286434]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4889. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9536034  -0.33671975 -0.93222463 -0.8623288 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4890. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9839678  -0.26307333 -0.9967372  -0.82957786]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4891. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9518493 -0.3049987 -0.9657189 -0.7884322]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4892. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.96470135 -0.2737972  -0.9835133  -0.9040508 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4893. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.7652478  -0.3281952  -0.9952324  -0.70601314]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4894. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98524344 -0.17364144 -0.9041502  -0.76020354]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4895. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97979975 -0.30245817 -0.9980883  -0.27267438]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4896. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9800199  -0.3488645  -0.91427106 -0.7870213 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4897. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9578134  -0.36715734 -0.99296254 -0.6325355 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 4898. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97024566 -0.35255134 -0.98691416 -0.8763127 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 4899. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.95664424 -0.20023525 -0.984758   -0.8599931 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 4900. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.98368365 -0.14114046 -0.7461696  -0.9444918 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 4901. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9762759 -0.200867  -0.9939276 -0.8996712]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 4902. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.95776296 -0.18403566 -0.9894761  -0.81705374]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 4903. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9868553  -0.40462375 -0.61840194 -0.89107716]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 4904. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.96259284 -0.31113255 -0.98946357 -0.9312314 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 4905. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.99035764 -0.27982342 -0.9791014  -0.86280733]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 4906. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9892663  -0.23631722 -0.87890357 -0.8218369 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 4907. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9912315  -0.45566827 -0.96694446 -0.92180324]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 4908. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97085184 -0.40221536 -0.9718233  -0.93034655]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 4909. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9671234  -0.25612003 -0.9246588  -0.9173599 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 4910. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97066814 -0.31494343 -0.9858338  -0.84565026]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 4911. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9967692  -0.29140162 -0.9720885  -0.8530848 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 4912. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9530326  -0.18460536 -0.9785567  -0.863775  ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 4913. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9890483  -0.16838682 -0.98066187 -0.856814  ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 4914. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9910864  -0.14750886 -0.99843174 -0.89302593]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 4915. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9597881  -0.29526323 -0.9712849  -0.8564355 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 4916. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9851517  -0.31437063 -0.96910334 -0.8498308 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 4917. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.95943296 -0.24627155 -0.97064316 -0.9361411 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 4918. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9416564  -0.27658224 -0.86812586 -0.70014465]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 4919. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.96543217 -0.25800693 -0.9857046  -0.60027784]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 4920. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9672198  -0.19211423 -0.9843872  -0.77611697]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 4921. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.89453423 -0.30123675 -0.9168011  -0.8131023 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 4922. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9270535  -0.33855063 -0.9494118  -0.8357446 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 4923. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.97494304 -0.23804629 -0.8938622  -0.8254795 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 4924. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.95706636 -0.19672477 -0.9815783  -0.63487726]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 4925. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9923857  -0.3160485  -0.989771   -0.67123854]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 4926. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.89397067 -0.27437055 -0.90664005 -0.78869766]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 4927. State = [[-0.25897995  0.18937282  0.07406575  1.        ]]. Action = [[-0.9314773  -0.31106472 -0.9295177  -0.70368767]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 4928. State = [[-0.26088566  0.07896792  0.11977139  1.        ]]. Action = [[-0.9509896  -0.13338381 -0.9777029  -0.5915666 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 4929. State = [[-0.25411692  0.08962862  0.10018346  1.        ]]. Action = [[ 0.541868    0.10525596 -0.83378243  0.32537794]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 4930. State = [[-0.24683422  0.09083581  0.06691559  1.        ]]. Action = [[-0.14902627 -0.2388668  -0.99184114  0.24200022]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 4931. State = [[-0.24611135  0.09020576  0.03930928  1.        ]]. Action = [[-0.3629446   0.01243865 -0.9885648   0.04178596]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 4932. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.88508093 -0.3291937  -0.9244399  -0.47860646]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 4933. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9235788  -0.3163746  -0.9887482  -0.50745565]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 4934. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.96513146 -0.3851152  -0.988473   -0.5874736 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 4935. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.5554884  -0.24453461 -0.99382126 -0.7515943 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 4936. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.92307174 -0.39761204 -0.988331   -0.7270671 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 4937. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9724311  -0.12180126 -0.9652083  -0.70833004]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 4938. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9757351  -0.3213197  -0.96941745 -0.6642766 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 4939. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9845406  -0.48900443 -0.95223224 -0.60311246]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 4940. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98412997 -0.3025958  -0.93534017 -0.75939506]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 4941. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9816645  -0.46673405 -0.96153057 -0.7617589 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 4942. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9590378  -0.32812595 -0.9621248  -0.68401265]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 4943. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9355224  -0.20156181 -0.95443857 -0.71944207]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 4944. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9532109 -0.3728047 -0.9537987 -0.7368119]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 4945. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.97288543 -0.37001675 -0.95113045 -0.68204397]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 4946. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9632754  -0.48392153 -0.9775892  -0.807252  ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 4947. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9744488  -0.40944934 -0.9649374  -0.60329163]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 4948. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9367439  -0.39746916 -0.9894957  -0.61496764]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 4949. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.96419066 -0.27697134 -0.991892   -0.66854185]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 4950. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9124299  -0.3655343  -0.91961294 -0.66974413]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 4951. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.86435163 -0.32981622 -0.90486944 -0.74682456]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 4952. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.97579837 -0.37929142 -0.98234373 -0.36726296]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 4953. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.910711   -0.46006668 -0.9787469  -0.9068981 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 4954. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98771745 -0.40554154 -0.94610834 -0.8407491 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 4955. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9611259  -0.40847486 -0.9598026  -0.6449802 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 4956. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.7692437  -0.27315986 -0.9889174  -0.64601386]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 4957. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9796425  -0.3209474  -0.99356836 -0.7953536 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 4958. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9201443  -0.41360736 -0.9736073  -0.7552311 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 4959. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.95762336 -0.30709553 -0.9913526  -0.6022319 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 4960. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8771783  -0.42326772 -0.9058365  -0.76336634]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 4961. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.82736784 -0.32843208 -0.9733938  -0.88633263]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 4962. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98156077 -0.24801743 -0.9916784  -0.7733379 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 4963. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9637155  -0.37957215 -0.9984934  -0.7902575 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 4964. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8552084  -0.30603957 -0.9004743  -0.6298095 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 4965. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.94733554 -0.34889126 -0.87135524 -0.7499619 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 4966. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.88095605 -0.32094204 -0.97488844 -0.32030368]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 4967. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9844265  -0.37555546 -0.9949344  -0.68252   ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 4968. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9794556  -0.20394307 -0.9750352  -0.70933497]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 4969. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9368965  -0.42822683 -0.99100775 -0.8537198 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 4970. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9703287  -0.44870937 -0.54950595 -0.63533807]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 4971. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9579018  -0.39377064 -0.9788126  -0.6877372 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 4972. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.93677443 -0.42727423 -0.86600477 -0.6207331 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 4973. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9774854  -0.39396918 -0.9872184  -0.67000854]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 4974. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9616087  -0.35262978 -0.9855328  -0.8691391 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 4975. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8825761  -0.34210712 -0.9924594  -0.64947397]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 4976. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9725587 -0.3721711 -0.9901066 -0.7181083]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 4977. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.88952416 -0.27226686 -0.930524   -0.49328685]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 4978. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.972737   -0.385095   -0.99284935 -0.71317977]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 4979. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.82421845 -0.50826216 -0.96794    -0.5077644 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 4980. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9443571  -0.46819913 -0.9899408  -0.5770834 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 4981. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.95152575 -0.41681498 -0.9938687  -0.6016155 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 4982. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.95522493 -0.3063798  -0.89781874 -0.6642556 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 4983. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9408996  -0.44466448 -0.973176   -0.59979594]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 4984. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9687121  -0.45046353 -0.9133916  -0.5159979 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 4985. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.89560413 -0.22485769 -0.9817738  -0.6737969 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 4986. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.93578    -0.4029776  -0.98955023 -0.74395955]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 4987. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9090929  -0.43542308 -0.9914254  -0.7011415 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 4988. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9547058  -0.2650758  -0.95477295 -0.5145    ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 4989. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9722566  -0.36664122 -0.9930513  -0.65127695]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 4990. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.93622583 -0.2899201  -0.9904016  -0.7941741 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 4991. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8254441  -0.27351952 -0.98771614 -0.5404867 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 4992. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98103654 -0.36666918 -0.9909298  -0.732689  ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 4993. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9174919  -0.32090032 -0.98484284 -0.8699295 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 4994. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9812554  -0.40669656 -0.9880659  -0.8954857 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 4995. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9799999  -0.23562527 -0.98104304 -0.62529165]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 4996. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9491099  -0.20960903 -0.9973844  -0.8236939 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 4997. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9247565  -0.45246124 -0.9598171  -0.8397995 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 4998. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.95269084 -0.35063595 -0.96771777 -0.68561864]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 4999. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9402963  -0.2487334  -0.96262246 -0.71530247]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 5000. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9687518  -0.24868578 -0.948905   -0.6834743 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 5001. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.95375925 -0.4126606  -0.9796565  -0.61000973]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 5002. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.948346   -0.3057245  -0.98398215 -0.7061051 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 5003. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.96301866 -0.21894306 -0.87803096 -0.5785746 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 5004. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.97727084 -0.38090026 -0.99003834 -0.6054275 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 5005. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8954567  -0.37141895 -0.98802006 -0.69552696]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 5006. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.87638503 -0.38609564 -0.9802017  -0.6137233 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 5007. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9946795  -0.14382398 -0.95810276 -0.44725907]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 5008. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.89010495 -0.19647771 -0.99447954 -0.6305283 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 5009. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8279303  -0.40944362 -0.99305415 -0.7230759 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 5010. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98836577 -0.43226057 -0.9858743  -0.8099328 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 5011. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8590854  -0.3491323  -0.97449183 -0.6128673 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 5012. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8972664  -0.25870156 -0.9951642  -0.74123186]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 5013. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9430566  -0.31379753 -0.951301   -0.66807395]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 5014. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.88926286 -0.4934473  -0.9775497  -0.698361  ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 5015. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9669723  -0.47411793 -0.9889564  -0.57216597]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 5016. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9832989  -0.2104873  -0.98718256 -0.12835872]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 5017. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.90643126 -0.32643843 -0.91553646 -0.49741274]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 5018. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.93796545 -0.35693783 -0.9854582  -0.7550005 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 5019. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9892221  -0.26797163 -0.97577775 -0.6776443 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5020. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9803702  -0.15399373 -0.9754892  -0.4090258 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5021. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9049405  -0.37779653 -0.98255837 -0.6440557 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5022. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98260844 -0.46313286 -0.9717241  -0.5675363 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5023. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9571556  -0.49513304 -0.9687967  -0.6202144 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5024. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8522496  -0.21512032 -0.9651069  -0.44425392]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5025. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.9823212  -0.43304563 -0.8743139  -0.52068543]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5026. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.98243827 -0.2872554  -0.9936952  -0.7750732 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 5027. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.93656677 -0.44933003 -0.97212636 -0.76717705]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 5028. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.8572844  -0.19322115 -0.99601    -0.75943816]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5029. State = [[-0.24600162  0.08982383  0.03812207  1.        ]]. Action = [[-0.96974397 -0.23671687 -0.9675381  -0.7273771 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 5030. State = [[-0.26191297  0.14435245  0.1225328   1.        ]]. Action = [[-0.96189636 -0.50295156 -0.97120243 -0.60929966]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5031. State = [[-0.26051483  0.15944746  0.10256013  1.        ]]. Action = [[ 0.0422374  -0.09708464 -0.99538213  0.27629745]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5032. State = [[-0.2602275   0.15922885  0.07489893  1.        ]]. Action = [[-0.22427297 -0.4038366  -0.9693501   0.24351406]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 5033. State = [[-0.25867143  0.15940955  0.07186031  1.        ]]. Action = [[-0.8487158  -0.37702793 -0.7893641  -0.46905   ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 5034. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.94223416 -0.38169426 -0.93663555 -0.59663165]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 5035. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.9725737  -0.23097473 -0.98550105 -0.7567429 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 5036. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.9347262  -0.26372486 -0.9920281  -0.5287815 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 5037. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.9601183  -0.47386414 -0.9887261  -0.6674317 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 5038. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.93857276 -0.38190162 -0.99604607 -0.5063285 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 5039. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.9122049  -0.35392857 -0.97261596 -0.5434903 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 5040. State = [[-0.2586973   0.15940212  0.07181569  1.        ]]. Action = [[-0.958907   -0.36705595 -0.9786067  -0.14370322]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 5041. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8477703  -0.33331144 -0.99843067 -0.6878274 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 5042. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.93522465 -0.4122494  -0.98530227 -0.69293255]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 5043. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9800067  -0.38377118 -0.9911008  -0.7135038 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 5044. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9625271  -0.3195145  -0.97788733 -0.581442  ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 5045. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9173973 -0.4113871 -0.9687031 -0.7450064]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 5046. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9704371  -0.36961412 -0.8800111  -0.73541427]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 5047. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94496864 -0.36150163 -0.98214716 -0.6438142 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 5048. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94718766 -0.31172264 -0.9661922  -0.45397997]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 5049. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9649674  -0.3187617  -0.9947885  -0.44564784]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 5050. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.96918774 -0.29629892 -0.993088   -0.71944946]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 5051. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.96119773 -0.2960685  -0.7643168  -0.7564416 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 5052. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.88725585 -0.50883424 -0.9892157  -0.49644494]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 5053. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.91362494 -0.17826271 -0.899695   -0.5365824 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 5054. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.88670367 -0.3247552  -0.97378063 -0.35041302]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 5055. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9576679  -0.39354837 -0.90546876 -0.49298036]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 5056. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.88976556 -0.42861354 -0.9755692  -0.58219576]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 5057. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8658668  -0.43946886 -0.9951232  -0.2836759 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 5058. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.84756446 -0.31937408 -0.8717854  -0.60314   ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 5059. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.89822614 -0.34750938 -0.9794742  -0.6443935 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 5060. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.6782281  -0.3257966  -0.9599114  -0.32036185]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 5061. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94100565 -0.19474983 -0.9569402  -0.60065883]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 5062. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8814332  -0.11174214 -0.9899591  -0.46043622]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 5063. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8359455 -0.2770086 -0.8684989 -0.546677 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 5064. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.86488724 -0.41293478 -0.9544245  -0.34578145]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 5065. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.93762803 -0.34495854 -0.98390526 -0.46535504]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 5066. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.88541424 -0.24773884 -0.99665993 -0.46116042]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 5067. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8704921  -0.3799324  -0.9455527  -0.48271185]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 5068. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.98011094 -0.23196918 -0.98253125 -0.2963755 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5069. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.7424467  -0.30657923 -0.9709553  -0.5558774 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 5070. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.83438337 -0.3881361  -0.98492676 -0.4612453 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 5071. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.97082543 -0.3699813  -0.99592    -0.58018863]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 5072. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8474695  -0.4162004  -0.9625642  -0.40889353]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 5073. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.984857   -0.4038086  -0.9894981  -0.48353797]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 5074. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9805985  -0.43043172 -0.9938107  -0.60792553]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 5075. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.85340345 -0.43999618 -0.9933464  -0.6789371 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 5076. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9626388  -0.10954511 -0.99583155 -0.56486106]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 5077. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.96023184 -0.22684067 -0.94912803 -0.63636637]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 5078. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9200756  -0.32072932 -0.9883797  -0.5622719 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 5079. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.97050047 -0.3473786  -0.99513483 -0.4648505 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 5080. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.95669097 -0.50940096 -0.99335265 -0.7681626 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 5081. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9430941  -0.44676727 -0.9643276  -0.7521358 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 5082. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8805278  -0.32268435 -0.9800081  -0.41492856]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 5083. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.7941755  -0.35218728 -0.99599206 -0.42924643]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 5084. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.90780735 -0.27360028 -0.9682579  -0.75425893]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 5085. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.91461265 -0.25090885 -0.90467703 -0.52522326]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 5086. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9124471  -0.37414587 -0.9491585  -0.418872  ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 5087. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.98775995 -0.36649698 -0.96304995 -0.53386194]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 5088. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.87785614 -0.43565464 -0.94898695 -0.24227399]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 5089. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8671881  -0.43030185 -0.97108424 -0.56194156]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 5090. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9269053  -0.48833895 -0.98420745 -0.62747175]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 5091. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.89627373 -0.24105263 -0.97416395 -0.43431604]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 5092. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94860035 -0.36620378 -0.9891367  -0.59394133]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 5093. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8708485  -0.36522007 -0.9885671  -0.45193487]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 5094. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9348876  -0.30863637 -0.99865925 -0.48289418]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 5095. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9408274  -0.49350554 -0.905053   -0.577645  ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 5096. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.932813   -0.40925473 -0.86064315 -0.5926132 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 5097. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9342171  -0.23698717 -0.97577906 -0.47648942]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 5098. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.92965657 -0.36379826 -0.99087673 -0.70259494]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 5099. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9436978  -0.49022615 -0.9196972  -0.5795197 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 5100. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.98257107 -0.44520515 -0.95505166 -0.5026959 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 5101. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8844809 -0.7210991 -0.9976795 -0.5688623]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 5102. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9926477  -0.39932024 -0.9888202  -0.5924137 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 5103. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.96885544 -0.22454262 -0.94849944 -0.49151856]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 5104. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.7222294  -0.56324804 -0.992863   -0.52140796]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 5105. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9034144  -0.3672198  -0.9614749  -0.51445377]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 5106. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9818938  -0.40292388 -0.9605225  -0.62610596]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 5107. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8511583  -0.35681254 -0.99071676 -0.7077416 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 5108. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9660404  -0.3122555  -0.9938794  -0.18696809]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 5109. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9556405  -0.23955286 -0.974767   -0.60360765]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 5110. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94570446 -0.1412844  -0.9948532  -0.6615113 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 5111. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9716063  -0.29659045 -0.963239   -0.555573  ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 5112. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9236456  -0.46329015 -0.99531865 -0.48818517]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 5113. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.907382   -0.53114945 -0.9883872  -0.5926482 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 5114. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9678282  -0.42779016 -0.98195356 -0.5822661 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 5115. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.93429726 -0.3915906  -0.918189   -0.540683  ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 5116. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8988035  -0.2223714  -0.9894405  -0.49692369]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 5117. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.8020779  -0.47384787 -0.8849333  -0.1828602 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 5118. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9373033  -0.36683738 -0.95452255 -0.42625266]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 5119. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.84987473 -0.28774726 -0.98815566 -0.6960088 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 5120. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94228417 -0.24340945 -0.9951051  -0.28003144]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 5121. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.69358945 -0.19866264 -0.98337185 -0.47769904]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5122. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9254525  -0.24790537 -0.98978436 -0.41010892]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5123. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.978205   -0.22980207 -0.9945868  -0.5840271 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5124. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.94537586 -0.2772019  -0.97403973 -0.4567507 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5125. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9449473  -0.36684716 -0.982282   -0.68892276]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5126. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9570905  -0.36901182 -0.978112   -0.5832192 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5127. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9738128  -0.23664904 -0.99331945 -0.66841274]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5128. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9653072 -0.2701887 -0.7552913 -0.4135219]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 5129. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9576954  -0.4438269  -0.9669424  -0.63139725]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 5130. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.98005056 -0.5270313  -0.94922763 -0.38508117]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5131. State = [[-0.2586793   0.1593357   0.07181721  1.        ]]. Action = [[-0.9672658  -0.32158208 -0.92407614 -0.66393095]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 5132. State = [[-0.2597398   0.07712058  0.12190714  1.        ]]. Action = [[-0.95662016 -0.47430474 -0.9583543  -0.42201614]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5133. State = [[-0.25631732  0.08470465  0.1017294   1.        ]]. Action = [[ 0.33841705 -0.221847   -0.74486536  0.33602512]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5134. State = [[-0.24509458  0.07758935  0.07268947  1.        ]]. Action = [[ 0.3536067  -0.4304636  -0.82660615  0.48963737]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5135. State = [[-0.22820632  0.07273097  0.0495196   1.        ]]. Action = [[ 0.56364024  0.11148834 -0.08283234  0.34273922]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5136. State = [[-0.21658969  0.07339753  0.04475169  1.        ]]. Action = [[ 0.16291058 -0.21072978 -0.9546837   0.30644107]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 5137. State = [[-0.21432951  0.07363637  0.04514139  1.        ]]. Action = [[-0.20910585 -0.19633776 -0.97975856  0.2604711 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 5138. State = [[-0.21355467  0.07375214  0.04530394  1.        ]]. Action = [[-0.21862441  0.04990745 -0.7639667   0.26930654]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 5139. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.13650846  0.05264282 -0.9489725   0.21380627]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 5140. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.14928126  0.01082933 -0.8248013   0.453871  ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 5141. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.0878458 -0.3303293 -0.970369   0.375991 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 5142. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.2524923  -0.20833004 -0.72383505  0.29962444]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 5143. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.46199167 -0.23658156 -0.9596088   0.20210552]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 5144. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.7169761  -0.11387473 -0.49343044  0.03357172]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 5145. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.16397119 -0.07170546 -0.9610778   0.21322715]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 5146. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.19997531 -0.5544527  -0.9415958   0.28724182]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 5147. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.23940289 -0.3311448  -0.91676605  0.34160757]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 5148. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.03139353  0.14916193 -0.9467354   0.4569621 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 5149. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.73361915 -0.19715941 -0.90083236  0.13188004]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 5150. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.18509984 -0.11293095 -0.707857    0.27778912]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 5151. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.21660513 -0.10101694 -0.74867123  0.32554436]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 5152. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.05718279 -0.44011414 -0.78404415  0.19228601]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 5153. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.42765588 -0.0728361  -0.58331853  0.071486  ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 5154. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.02453542 -0.3413422  -0.87621796  0.04746473]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 5155. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.00388515 -0.08326405 -0.8896983   0.33280694]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 5156. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.3779453  -0.31464237 -0.9299618   0.28296506]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 5157. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[-0.5885139   0.14678812 -0.8117224   0.15194929]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 5158. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.294582   -0.1790592  -0.6484368   0.24737465]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 5159. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.17857385 -0.14649725 -0.81932825  0.08265495]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 5160. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.564458   -0.68399423 -0.9461868   0.16969156]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 5161. State = [[-0.21318738  0.07380427  0.04538064  1.        ]]. Action = [[ 0.30844688 -0.38698602 -0.7750282   0.4400146 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 5162. State = [[-0.21287583  0.07005437  0.04336567  1.        ]]. Action = [[ 0.0658555  -0.25196052 -0.16647524  0.45349836]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 5163. State = [[-0.21205449  0.06466334  0.03799072  1.        ]]. Action = [[-0.40848207 -0.5040503  -0.98179126  0.40997624]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 5164. State = [[-0.21197549  0.063669    0.03802369  1.        ]]. Action = [[-0.12302816 -0.48464847 -0.88653463  0.20888972]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 5165. State = [[-0.21196932  0.06359917  0.0380287   1.        ]]. Action = [[-0.11814934 -0.3165909  -0.9873208   0.28545952]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 5166. State = [[-0.21196932  0.06359917  0.0380287   1.        ]]. Action = [[ 0.0678736  -0.4283163  -0.90526754  0.47415674]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 5167. State = [[-0.21196932  0.06359917  0.0380287   1.        ]]. Action = [[ 0.19713378 -0.01125306 -0.89656293  0.24769187]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 5168. State = [[-0.21005492  0.05847457  0.03864166  1.        ]]. Action = [[ 0.1109513  -0.28084671  0.25527728  0.501307  ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 5169. State = [[-0.20842056  0.05344069  0.0393253   1.        ]]. Action = [[ 0.41931796 -0.21398062 -0.85726273  0.5589509 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 5170. State = [[-0.20807034  0.05190016  0.03952239  1.        ]]. Action = [[ 0.67456627 -0.14932269 -0.5941089   0.74004745]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5171. State = [[-0.1987798   0.05063387  0.04127469  1.        ]]. Action = [[ 0.67480314 -0.06944495  0.03490818  0.5553045 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 5172. State = [[-0.18707244  0.05017853  0.04357647  1.        ]]. Action = [[ 0.7563211 -0.5199218 -0.6632609  0.6894641]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5173. State = [[-0.18392268  0.05013977  0.04447982  1.        ]]. Action = [[ 0.8182566  -0.13015795 -0.17514563  0.8209214 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 5174. State = [[-0.18326253  0.05008798  0.04469515  1.        ]]. Action = [[ 0.6822109 -0.7064887  0.7071445  0.7827755]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 5175. State = [[-0.18250796  0.05018941  0.04494862  1.        ]]. Action = [[0.74734867 0.05939043 0.8617306  0.6563308 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 5176. State = [[-0.1823697   0.05021016  0.04499978  1.        ]]. Action = [[0.77105236 0.8332269  0.22178984 0.7534573 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 5177. State = [[-0.18264584  0.05019339  0.04498339  1.        ]]. Action = [[0.65376306 0.11361527 0.69986033 0.70699143]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 5178. State = [[-0.18264584  0.05019339  0.04498339  1.        ]]. Action = [[0.62764454 0.06006205 0.7641597  0.6889081 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 5179. State = [[-0.18264584  0.05019339  0.04498339  1.        ]]. Action = [[ 0.863394   -0.06183517  0.28573942  0.59520125]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 5180. State = [[-0.18264584  0.05019339  0.04498339  1.        ]]. Action = [[0.56456006 0.07532322 0.02459645 0.8427839 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 5181. State = [[-0.18264584  0.05019339  0.04498339  1.        ]]. Action = [[0.760368   0.39734077 0.18325412 0.3541683 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 5182. State = [[-0.1777167   0.03394135  0.05422078  1.        ]]. Action = [[ 0.05822849 -0.9438284   0.9917191   0.80355906]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 5183. State = [[-0.17402071  0.01788017  0.06980769  1.        ]]. Action = [[ 0.8289809  -0.75741595  0.33444297  0.8956994 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 5184. State = [[-0.17366958  0.01556376  0.07062554  1.        ]]. Action = [[ 0.8697523 -0.8713259  0.8519776  0.9399284]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 5185. State = [[-0.17366724  0.0153601   0.07066402  1.        ]]. Action = [[0.55982435 0.544757   0.848932   0.83490276]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 5186. State = [[-0.17380439  0.01535015  0.07064921  1.        ]]. Action = [[ 0.9881424 -0.7914071  0.9411181  0.806427 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 5187. State = [[-0.17394413  0.01535751  0.07064945  1.        ]]. Action = [[0.3013625  0.5751445  0.98272085 0.88993645]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 5188. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.7283845  -0.9713909   0.54463625  0.8963125 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 5189. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.94724226 0.8288982  0.9819479  0.5198709 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 5190. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.9844723  -0.7529505   0.7961762   0.91129696]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 5191. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.80880165 0.8771281  0.99298024 0.9176104 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 5192. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.8556454  0.43353772 0.8783977  0.95736146]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 5193. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.9336864 0.7179942 0.7175305 0.7992145]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 5194. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.9899727  -0.48497558  0.9977951   0.61455214]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 5195. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.97884643 -0.28331327  0.9905561   0.6821327 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 5196. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.92527723 -0.97601104  0.9647219   0.73326194]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 5197. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.9673276  0.9546027  0.97269297 0.955333  ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 5198. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.59708285 0.096596   0.97410846 0.8960211 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 5199. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.9521854  -0.06863308  0.995584    0.95314527]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 5200. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.9793978  0.48324573 0.8943889  0.94879866]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 5201. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.88221526 -0.1999684   0.94823766  0.93970704]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 5202. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.8584963 -0.7751766  0.8948015  0.977746 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 5203. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.6102836 0.9768244 0.9615476 0.9019413]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 5204. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.8747364 0.9635701 0.9990225 0.5544529]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 5205. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.99194884 -0.6294613   0.8965316   0.8856468 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 5206. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.9799274  0.49848998 0.87813234 0.9635472 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 5207. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.8348675  -0.8892029   0.94721174  0.8356128 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 5208. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.84575367 0.37360072 0.9969684  0.9519466 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 5209. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[0.8134334  0.22576809 0.9723079  0.79617333]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 5210. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.9143238  -0.98612976  0.9739213   0.81858516]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 5211. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.69410896 -0.95695263  0.9261646   0.9083581 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 5212. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.85300636 -0.8647843   0.99872684  0.836424  ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 5213. State = [[-0.17397054  0.01528636  0.07065841  1.        ]]. Action = [[ 0.95200133 -0.658536    0.97876084  0.6959934 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 5214. State = [[-0.17131142  0.01009177  0.08372933  1.        ]]. Action = [[-0.33420908 -0.27412027  0.9902098   0.67428994]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 5215. State = [[-0.16984123  0.00458626  0.10647832  1.        ]]. Action = [[ 0.9181578  -0.86910224  0.95609224  0.8307259 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 5216. State = [[-0.1703707   0.00371442  0.1076862   1.        ]]. Action = [[ 0.860667   -0.02922767  0.998875    0.58948886]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 5217. State = [[-0.17124373  0.00359548  0.10775254  1.        ]]. Action = [[0.97505593 0.28624296 0.86249745 0.64984536]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 5218. State = [[-0.17218532  0.00352632  0.10758373  1.        ]]. Action = [[ 0.98786545 -0.99564844  0.9265549   0.87783396]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 5219. State = [[-0.17305821  0.00348897  0.10772112  1.        ]]. Action = [[0.6375382  0.595289   0.9664805  0.87748647]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 5220. State = [[-0.17386632  0.00344126  0.10795767  1.        ]]. Action = [[0.9458809  0.47989738 0.9707942  0.86463714]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 5221. State = [[-0.17431979  0.00344499  0.10859638  1.        ]]. Action = [[ 0.922758   -0.6331088   0.90766644  0.5553664 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 5222. State = [[-0.17493503  0.00343443  0.1096112   1.        ]]. Action = [[0.75285435 0.4272157  0.9540591  0.8737879 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 5223. State = [[-0.17510483  0.00344632  0.11026482  1.        ]]. Action = [[ 0.84540856 -0.84514785  0.9810914   0.8114567 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 5224. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[0.5566013  0.04743624 0.9853816  0.27949035]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 5225. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[0.8869394  0.27152765 0.98033094 0.90860367]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 5226. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[ 0.8704065  -0.10479164  0.9951227   0.612411  ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 5227. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[ 0.91596866 -0.9532417   0.9931457   0.8780749 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 5228. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[ 0.32665634 -0.5882255   0.9975774   0.82264566]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 5229. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[ 0.52386403 -0.13818139  0.99092245  0.71131134]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 5230. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[0.9915993  0.29779732 0.8503089  0.7709391 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 5231. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[ 0.991241  -0.475132   0.8547461  0.9710336]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 5232. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[0.97851086 0.00675845 0.97897696 0.9006994 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 5233. State = [[-0.17522891  0.00344311  0.11038048  1.        ]]. Action = [[ 0.98115885 -0.6575092   0.9839473   0.7237873 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 5234. State = [[-0.26324934  0.02921671  0.11849338  1.        ]]. Action = [[ 0.64875746 -0.82305676  0.9681859   0.8644905 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 5235. State = [[-0.24861297  0.0190748   0.11441582  1.        ]]. Action = [[ 0.81396663 -0.9780701   0.9931632   0.6699817 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5236. State = [[-0.22148664  0.01491544  0.13750924  1.        ]]. Action = [[0.9693742  0.61952376 0.9779079  0.8984268 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5237. State = [[-0.18436801  0.02088407  0.17518093  1.        ]]. Action = [[9.262724e-01 4.878044e-04 9.820725e-01 7.446599e-01]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5238. State = [[-0.1591577   0.02298728  0.19995815  1.        ]]. Action = [[ 0.9689081  -0.24722517  0.94930196  0.5265329 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 5239. State = [[-0.15418899  0.02346864  0.20383354  1.        ]]. Action = [[ 0.8884902  -0.04572678  0.45673072  0.51014256]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 5240. State = [[-0.15304828  0.02362741  0.20492873  1.        ]]. Action = [[ 0.7517214  -0.7177297  -0.15875787  0.5510886 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 5241. State = [[-0.152103    0.02377346  0.20569079  1.        ]]. Action = [[ 0.85201216 -0.12423527  0.4058876   0.33142877]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 5242. State = [[-0.15168227  0.02387725  0.20601413  1.        ]]. Action = [[ 0.6750202  -0.39658105  0.60593534  0.33871162]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 5243. State = [[-0.15156925  0.02388343  0.20611672  1.        ]]. Action = [[ 0.8305485  -0.3720625  -0.06156093  0.39673305]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 5244. State = [[-0.15156925  0.02388343  0.20611672  1.        ]]. Action = [[ 0.8173158  -0.72388226  0.32797468  0.3509425 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 5245. State = [[-0.14411852  0.01823834  0.21515536  1.        ]]. Action = [[ 0.4518789  -0.43722445  0.7001107   0.45556808]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 5246. State = [[-0.12646607  0.00607349  0.237687    1.        ]]. Action = [[ 0.54308236 -0.36213583  0.48383904  0.32592154]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 5247. State = [[-0.11201299 -0.00516694  0.25340563  1.        ]]. Action = [[ 0.22580683 -0.29750812 -0.03738523  0.07014775]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 5248. State = [[-0.1077865  -0.01003066  0.2576945   1.        ]]. Action = [[ 0.6583626  -0.27212965 -0.91604626  0.03819501]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 5249. State = [[-0.10759193 -0.01096153  0.25793773  1.        ]]. Action = [[-0.00168055 -0.19328439 -0.82642835  0.16602588]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 5250. State = [[-0.10405035 -0.01070741  0.257971    1.        ]]. Action = [[ 0.3677975   0.11721301 -0.06044638  0.15157938]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 5251. State = [[-0.09562906 -0.00991069  0.25765193  1.        ]]. Action = [[ 0.41812134 -0.15514982 -0.9846281   0.14873183]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Above hoop
Current timestep = 5252. State = [[-0.09514637 -0.00986587  0.25809598  1.        ]]. Action = [[ 0.10123372  0.02039623 -0.74370736  0.22796118]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Above hoop
Current timestep = 5253. State = [[-0.09514637 -0.00986587  0.25809598  1.        ]]. Action = [[ 0.24275517 -0.00712913 -0.6713407   0.15006244]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Above hoop
Current timestep = 5254. State = [[-0.09514637 -0.00986587  0.25809598  1.        ]]. Action = [[ 0.32370043 -0.2896844  -0.91116816  0.11074567]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Above hoop
Current timestep = 5255. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.22142398 -0.27649438 -0.94971997 -0.02349091]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Above hoop
Current timestep = 5256. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 6.3220644e-01 -4.6551228e-04 -7.8141081e-01  6.3551784e-02]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Above hoop
Current timestep = 5257. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.09295857 -0.47528434 -0.9271261   0.09763956]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Above hoop
Current timestep = 5258. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.5163964  -0.19417465 -0.8690074   0.1579032 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Above hoop
Current timestep = 5259. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.45861924 -0.18851697 -0.79985046  0.00658095]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Above hoop
Current timestep = 5260. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.19579029 -0.23490345 -0.6487431   0.07550776]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Above hoop
Current timestep = 5261. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.42103338 -0.19075894 -0.9323406   0.1576097 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Above hoop
Current timestep = 5262. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.5403552  -0.29575467 -0.95048666  0.13462102]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Above hoop
Current timestep = 5263. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.4478947  -0.18100172 -0.7898179   0.05997741]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Above hoop
Current timestep = 5264. State = [[-0.09522922 -0.00986197  0.25809124  1.        ]]. Action = [[ 0.54756474 -0.05880845 -0.85218453  0.17821252]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Above hoop
Current timestep = 5265. State = [[-0.09018797 -0.0145286   0.25488013  1.        ]]. Action = [[ 0.45181847 -0.33883005 -0.29888654  0.16568267]]. Reward = [0.]
Curr episode timestep = 30
Above hoop
Current timestep = 5266. State = [[-0.07962372 -0.01855759  0.24805048  1.        ]]. Action = [[ 0.37555838  0.10967612 -0.96054     0.05470729]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Above hoop
Current timestep = 5267. State = [[-0.07714047 -0.01995492  0.24807297  1.        ]]. Action = [[ 0.14397025 -0.2616074  -0.93056345  0.05557895]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Above hoop
Current timestep = 5268. State = [[-0.07696477 -0.02037311  0.24817926  1.        ]]. Action = [[ 0.302153   -0.06462616 -0.4624915   0.03184855]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Above hoop
Current timestep = 5269. State = [[-0.07703841 -0.02086019  0.24822886  1.        ]]. Action = [[ 0.41562223 -0.18510169 -0.6922662   0.06294012]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Above hoop
Current timestep = 5270. State = [[-0.07703787 -0.02106395  0.24824359  1.        ]]. Action = [[ 0.33037472 -0.00852293 -0.7674259   0.07999218]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Above hoop
Current timestep = 5271. State = [[-0.07704449 -0.02113387  0.24825095  1.        ]]. Action = [[ 0.2582016  -0.09235561 -0.85051405  0.08124685]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Above hoop
Current timestep = 5272. State = [[-0.07704449 -0.02113387  0.24825095  1.        ]]. Action = [[ 0.4193095  -0.10796028 -0.90103215  0.11118305]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Above hoop
Current timestep = 5273. State = [[-0.07704449 -0.02113387  0.24825095  1.        ]]. Action = [[ 0.4563756  -0.13519311 -0.88523024  0.1756655 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Above hoop
Current timestep = 5274. State = [[-0.07704449 -0.02113387  0.24825095  1.        ]]. Action = [[ 0.42290235 -0.12468904 -0.7109185   0.14506674]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Above hoop
Current timestep = 5275. State = [[-0.07704449 -0.02113387  0.24825095  1.        ]]. Action = [[ 0.3996042  -0.00673288 -0.9167396   0.10320544]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Above hoop
Current timestep = 5276. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.24681568 -0.01477033 -0.8281506   0.10203016]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Above hoop
Current timestep = 5277. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.25539184 -0.19307226 -0.5207447   0.07054341]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Above hoop
Current timestep = 5278. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.4263817  -0.20088714 -0.82637095  0.10667717]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Above hoop
Current timestep = 5279. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.23453617 -0.16160786 -0.90668064  0.04795074]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Above hoop
Current timestep = 5280. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.26746333 -0.09507549 -0.91861266  0.09145916]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Above hoop
Current timestep = 5281. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.3742411  -0.22614598 -0.9303216   0.02268374]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Above hoop
Current timestep = 5282. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.47798014 -0.06257647 -0.4766277   0.12107265]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Above hoop
Current timestep = 5283. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.08161783 -0.08103973 -0.822315   -0.05237669]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Above hoop
Current timestep = 5284. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.36040223 -0.24635446 -0.94351673  0.04789388]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Above hoop
Current timestep = 5285. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 2.4653685e-01 -1.8838340e-01 -5.9397608e-01 -2.7698278e-04]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Above hoop
Current timestep = 5286. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.07092643 -0.2782122  -0.7686097   0.09537292]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Above hoop
Current timestep = 5287. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.48564434 -0.174231   -0.9277333   0.00528252]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Above hoop
Current timestep = 5288. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.18932378 -0.05505717 -0.97370064  0.06937587]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Above hoop
Current timestep = 5289. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.33474398 -0.22733486 -0.9460929   0.01435268]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Above hoop
Current timestep = 5290. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.35630178 -0.15727085 -0.89016247  0.07817268]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Above hoop
Current timestep = 5291. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.20122993 -0.10766482 -0.8621035   0.02584636]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Above hoop
Current timestep = 5292. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.2741232  -0.251374   -0.84912586  0.09915566]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Above hoop
Current timestep = 5293. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.3395214  -0.04147929 -0.79276836  0.03041553]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Above hoop
Current timestep = 5294. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.4437183  -0.09956235 -0.38463622  0.02537525]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Above hoop
Current timestep = 5295. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.37292457  0.00582445 -0.87717223  0.05516779]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Above hoop
Current timestep = 5296. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.34851372 -0.10186428 -0.88112247  0.01317918]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Above hoop
Current timestep = 5297. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.2579645  -0.2894659  -0.9478863  -0.03417528]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Above hoop
Current timestep = 5298. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.30218554  0.05472946 -0.93843025 -0.04828775]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Above hoop
Current timestep = 5299. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.28152966  0.00945997 -0.8460404   0.02705622]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Above hoop
Current timestep = 5300. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.2428658  -0.06836557 -0.9724154   0.01651525]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Above hoop
Current timestep = 5301. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.32799292 -0.17127097 -0.97539127  0.04480529]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Above hoop
Current timestep = 5302. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.3220122  -0.05111587 -0.8911751   0.04403186]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Above hoop
Current timestep = 5303. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.41314507 -0.15748507 -0.89900047  0.08563995]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Above hoop
Current timestep = 5304. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.21838534 -0.0791406  -0.894224    0.06271005]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Above hoop
Current timestep = 5305. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.03374255 -0.18858194 -0.9126258   0.02861905]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Above hoop
Current timestep = 5306. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.10822022 -0.02870744 -0.99192053  0.00622547]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Above hoop
Current timestep = 5307. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.20618272  0.0965991  -0.93195873  0.04162681]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Above hoop
Current timestep = 5308. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.13831234  0.01287699 -0.92664504  0.0384028 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Above hoop
Current timestep = 5309. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.27760136 -0.00775373 -0.8559087   0.11190248]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Above hoop
Current timestep = 5310. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.11611331 -0.13328159 -0.60775375  0.03757906]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Above hoop
Current timestep = 5311. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.3310219  -0.21740389 -0.97122806  0.0337137 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Above hoop
Current timestep = 5312. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.20349073 -0.1795575  -0.9383296   0.02201545]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Above hoop
Current timestep = 5313. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.2869972  -0.15218979 -0.9098642   0.00167024]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Above hoop
Current timestep = 5314. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.14167202 -0.1214149  -0.8089682  -0.00757265]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Above hoop
Current timestep = 5315. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.27980304 -0.22881305 -0.91176826 -0.01679564]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Above hoop
Current timestep = 5316. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.22827554 -0.0408076  -0.81555736  0.04745662]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Above hoop
Current timestep = 5317. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.2192316  -0.08434385 -0.8398423   0.08348775]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Above hoop
Current timestep = 5318. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.07189631 -0.24186742 -0.70742106  0.05149567]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Above hoop
Current timestep = 5319. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.26542544 -0.08560055 -0.8930697   0.05620754]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Above hoop
Current timestep = 5320. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.2657249  -0.16719425 -0.9502978   0.07867408]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Above hoop
Current timestep = 5321. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.33656347 -0.00681198 -0.98451585  0.03606462]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Above hoop
Current timestep = 5322. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.27556312 -0.27641666 -0.5966297   0.01270247]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Above hoop
Current timestep = 5323. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.252326   -0.10009867 -0.8479219   0.07261133]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Above hoop
Current timestep = 5324. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.28965652 -0.01295537 -0.8459331   0.05916524]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Above hoop
Current timestep = 5325. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.215554    0.03334332 -0.69345206  0.06512022]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Above hoop
Current timestep = 5326. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.25191092  0.02516651 -0.88859904  0.02773428]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Above hoop
Current timestep = 5327. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.3621484  -0.08156753 -0.9312087   0.00263882]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Above hoop
Current timestep = 5328. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.32943892 -0.18135321 -0.92534137  0.03924632]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Above hoop
Current timestep = 5329. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.27824628  0.05697525 -0.9448611   0.05546737]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Above hoop
Current timestep = 5330. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.33876526 -0.32079852 -0.9344161   0.05470037]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Above hoop
Current timestep = 5331. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.23388803 -0.05841523 -0.94593906  0.03686738]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Above hoop
Current timestep = 5332. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.22480154 -0.30618054 -0.85220987  0.02507019]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Above hoop
Current timestep = 5333. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.22459435 -0.31311893 -0.9409994   0.03690982]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Above hoop
Current timestep = 5334. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.24883068 -0.1852727  -0.69062847  0.08049226]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Above hoop
Current timestep = 5335. State = [[-0.07705109 -0.02120331  0.24825826  1.        ]]. Action = [[ 0.26121902 -0.12106907 -0.7838287   0.07515132]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Above hoop
Current timestep = 5336. State = [[-0.27172548  0.10384911  0.16467507  1.        ]]. Action = [[ 0.22160983 -0.13084304 -0.51522726  0.07330656]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Above hoop
Current timestep = 5337. State = [[-0.26124775  0.09279232  0.17115955  1.        ]]. Action = [[ 0.6400006  -0.8336111   0.880901    0.24622858]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5338. State = [[-0.2382674   0.06659627  0.19334492  1.        ]]. Action = [[ 0.7400032 -0.7973487  0.9352586  0.2758119]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5339. State = [[-0.20761442  0.04201502  0.2294901   1.        ]]. Action = [[ 0.768661   -0.5821315   0.9843476   0.33652735]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5340. State = [[-0.17865066  0.01950968  0.2673762   1.        ]]. Action = [[ 0.62298894 -0.5732658   0.80621934  0.29311562]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5341. State = [[-0.15922579  0.00151719  0.2833281   1.        ]]. Action = [[ 0.416389   -0.38201463 -0.5760435   0.23257971]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5342. State = [[-0.1475754  -0.00920758  0.2727662   1.        ]]. Action = [[ 0.21381795 -0.08704931 -0.46168685  0.10652471]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 5343. State = [[-0.13711108 -0.01899541  0.2573834   1.        ]]. Action = [[ 0.29361045 -0.39099503 -0.5368638   0.15775466]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 5344. State = [[-0.12778239 -0.02657354  0.24648239  1.        ]]. Action = [[ 0.21806812 -0.26140314 -0.54451334  0.12291706]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 5345. State = [[-0.11814848 -0.03390027  0.2535433   1.        ]]. Action = [[ 0.40919805 -0.30956614  0.7478297   0.15459096]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 5346. State = [[-0.11057486 -0.03890324  0.26128772  1.        ]]. Action = [[ 0.23099673 -0.48737198 -0.8519364   0.1729039 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 5347. State = [[-0.11044589 -0.04009347  0.26193535  1.        ]]. Action = [[ 0.33399785  0.10626507 -0.75154     0.07666218]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 5348. State = [[-0.10772425 -0.04242526  0.2599518   1.        ]]. Action = [[ 0.33426785 -0.14330763 -0.3161443   0.04556775]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 5349. State = [[-0.10031585 -0.04484     0.25878352  1.        ]]. Action = [[ 0.3387959  -0.28299004 -0.6780167   0.00633895]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 5350. State = [[-0.09977099 -0.04556629  0.25911847  1.        ]]. Action = [[ 0.3204937  -0.20955217 -0.78384537  0.11153102]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 5351. State = [[-0.09809726 -0.05270107  0.25473773  1.        ]]. Action = [[ 0.18080151 -0.42001927 -0.3567654   0.08813274]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 5352. State = [[-0.09208381 -0.05997519  0.24868791  1.        ]]. Action = [[ 0.15929627 -0.17895412 -0.66080326  0.04361176]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Above hoop
Current timestep = 5353. State = [[-0.09178343 -0.06098935  0.24890076  1.        ]]. Action = [[ 0.15969598 -0.2632141  -0.3489799   0.0438596 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Above hoop
Current timestep = 5354. State = [[-0.09177367 -0.06105235  0.24890077  1.        ]]. Action = [[ 0.28236628 -0.43484306 -0.40750003  0.05281901]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Above hoop
Current timestep = 5355. State = [[-0.09177367 -0.06105235  0.24890077  1.        ]]. Action = [[ 0.18211484 -0.28121424 -0.6033883   0.08234894]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Above hoop
Current timestep = 5356. State = [[-0.09177367 -0.06105235  0.24890077  1.        ]]. Action = [[ 0.22957778 -0.2569121  -0.7089295   0.09052777]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Above hoop
Current timestep = 5357. State = [[-0.09177367 -0.06105235  0.24890077  1.        ]]. Action = [[ 0.17718935 -0.15942353 -0.9294442   0.02356005]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Above hoop
Current timestep = 5358. State = [[-0.09176402 -0.06111476  0.24890077  1.        ]]. Action = [[ 0.29491353  0.00258088 -0.6679881   0.0763154 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Above hoop
Current timestep = 5359. State = [[-0.09176402 -0.06111476  0.24890077  1.        ]]. Action = [[ 0.21025372  0.13047504 -0.9540834   0.05453837]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Above hoop
Current timestep = 5360. State = [[-0.08891867 -0.06557118  0.24609868  1.        ]]. Action = [[ 0.30085695 -0.23627847 -0.2236538   0.08638573]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Current timestep = 5361. State = [[-0.08384557 -0.07103764  0.23966107  1.        ]]. Action = [[ 0.18514907 -0.3057884  -0.82947713  0.07936776]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 5362. State = [[-0.08355141 -0.07193194  0.23889299  1.        ]]. Action = [[ 0.14425123 -0.26143473 -0.72609115  0.04847252]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 5363. State = [[-0.08344173 -0.07194136  0.23833667  1.        ]]. Action = [[ 0.24559772 -0.23710412 -0.46574903  0.03662097]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 5364. State = [[-0.08254559 -0.0719502   0.23874137  1.        ]]. Action = [[ 0.243464   -0.36535883 -0.9365494   0.10506058]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 5365. State = [[-0.08157916 -0.07195298  0.23911959  1.        ]]. Action = [[ 0.22203624 -0.27302027 -0.77304626  0.14411998]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 5366. State = [[-0.08100831 -0.07196989  0.23898236  1.        ]]. Action = [[ 0.07201326 -0.3996256  -0.8219666   0.10594165]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 5367. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.23626661 -0.16378164 -0.28406966  0.03963912]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 5368. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.00897622 -0.32171583 -0.28715122  0.07247198]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 5369. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.0528723  -0.21468538 -0.7917881   0.04322004]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 5370. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.33094907 -0.44101018 -0.7029172   0.02656758]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 5371. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.1475805  -0.13920188 -0.519911    0.09340274]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 5372. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.14143014 -0.26464218 -0.7989698   0.16499305]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 5373. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.28820872 -0.4606514  -0.76694244  0.00840271]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 5374. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.39591753 -0.29709697 -0.97285986  0.04755807]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 5375. State = [[-0.08103253 -0.07198387  0.23896366  1.        ]]. Action = [[ 0.21203244 -0.08472407 -0.8459162   0.13371646]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 5376. State = [[-0.0766279  -0.07186726  0.24202785  1.        ]]. Action = [[0.17499995 0.13587737 0.27808976 0.08066475]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 5377. State = [[-0.07349516 -0.07157924  0.24432287  1.        ]]. Action = [[ 0.24754024 -0.3726027  -0.92088777  0.00827503]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 5378. State = [[-0.07327847 -0.07159929  0.24450323  1.        ]]. Action = [[ 0.085145    0.12600076 -0.96405405  0.0143404 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 5379. State = [[-0.0732093  -0.07153047  0.24453232  1.        ]]. Action = [[ 0.15681446 -0.17831993 -0.8414518   0.07370138]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 5380. State = [[-0.0732093  -0.07153047  0.24453232  1.        ]]. Action = [[ 0.09245396  0.04283035 -0.72692937  0.11421442]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 5381. State = [[-0.0732093  -0.07153047  0.24453232  1.        ]]. Action = [[ 0.07292879  0.01343858 -0.8767888   0.03651154]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 5382. State = [[-0.0732093  -0.07153047  0.24453232  1.        ]]. Action = [[ 0.20286822 -0.22229815 -0.6922838   0.06456983]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 5383. State = [[-0.07319693 -0.07145754  0.24451703  1.        ]]. Action = [[ 0.13169515 -0.15588701 -0.7261569   0.13244748]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 5384. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.29924595 -0.19347322 -0.9276728   0.01762497]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 5385. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.35921216 -0.0839954  -0.9632355   0.0023421 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 5386. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.14595568 -0.03914917 -0.64055437  0.05850554]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 5387. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.13077497 -0.00132293 -0.95976627 -0.04230487]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 5388. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[-0.07785833 -0.05663484 -0.88981736  0.10089946]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 5389. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.05168676 -0.19050282 -0.9166095   0.11583745]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 5390. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.27729523 -0.25163376 -0.41676354  0.1777407 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 5391. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.09480727 -0.09700817 -0.8491242  -0.02546775]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 5392. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.25077307 -0.20047557 -0.93162143  0.10268152]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 5393. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.14294863 -0.21788967 -0.76328266  0.00711179]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 5394. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.24399436 -0.13459408 -0.96667874  0.09314048]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 5395. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[-0.03587312 -0.3197649  -0.8361094  -0.03228068]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 5396. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.23899949 -0.02832502 -0.73869646  0.11597669]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 5397. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.31145692 -0.13478178 -0.62083215  0.09147084]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 5398. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[-0.06036389 -0.28691554 -0.93327546  0.02405858]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 5399. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.1586219  -0.06016248 -0.88963735 -0.00453502]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 5400. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[-0.02783322 -0.06513017 -0.972445    0.10101497]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 5401. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[-0.08385265 -0.00434089 -0.89358014  0.11647391]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 5402. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.25947046 -0.06111717 -0.5309007   0.03556097]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 5403. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.30024195  0.00823176 -0.9544184   0.01427686]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 5404. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.14468062 -0.12618393 -0.62356573  0.00089192]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 5405. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.05987787 -0.47117937 -0.97086626  0.07428217]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 5406. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.26048815  0.01490533 -0.8750055  -0.02009755]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 5407. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.05530703 -0.2149725  -0.7739359   0.11090386]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 5408. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.00496614 -0.0167793  -0.9477205   0.040048  ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 5409. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.3067119   0.18839979 -0.7470387  -0.01966989]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 5410. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.13519895 -0.19076025 -0.8449501   0.02458811]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 5411. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 2.2333264e-01 -3.4977853e-02 -7.8741723e-01 -4.0870905e-04]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 5412. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.21779358 -0.05632764 -0.82423854  0.10081863]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 5413. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.19283462 -0.2122277  -0.9347252   0.09108567]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 5414. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.045784   -0.08537346 -0.7456369   0.07024908]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 5415. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[-0.0141992  -0.01057452 -0.96620035  0.11820424]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 5416. State = [[-0.07317363 -0.07132002  0.24448821  1.        ]]. Action = [[ 0.02040672 -0.05217886 -0.9139899   0.04069555]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 5417. State = [[-0.07284112 -0.07382286  0.24420197  1.        ]]. Action = [[ 0.06823635 -0.2742567  -0.17119312  0.15000749]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 5418. State = [[-0.07293618 -0.0757426   0.24282984  1.        ]]. Action = [[ 0.116431   -0.11047232 -0.86163807  0.06090415]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 5419. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[-0.04852557  0.138165   -0.9764938   0.02192891]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 5420. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[-0.09771591 -0.10841101 -0.9150189   0.04273653]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 5421. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.15474844 -0.13460636 -0.9095664   0.08489621]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 5422. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[-0.03214711  0.0664314  -0.6855579   0.09932959]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 5423. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.16556764 -0.17044973 -0.9162858   0.04593277]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 5424. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.07625782 -0.03723383 -0.9010774   0.05462575]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 5425. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.3607937   0.10537219 -0.4931885   0.08034873]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 5426. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.07011557  0.05360568 -0.80305     0.09606314]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 5427. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.16140378 -0.11424345 -0.9856081   0.10969102]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 5428. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[-0.02171963 -0.12952733 -0.91273683  0.07157099]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 5429. State = [[-0.07296732 -0.0757317   0.24250917  1.        ]]. Action = [[ 0.07195878 -0.02291721 -0.78395915  0.14858115]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 5430. State = [[-0.07295094 -0.07586098  0.24250919  1.        ]]. Action = [[ 0.17283595 -0.01981717 -0.855913   -0.02944005]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 5431. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[ 0.15591896  0.23862648 -0.9748412   0.09101057]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 5432. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[ 0.26693225 -0.16567773 -0.9605554   0.08648968]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 5433. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[ 0.28040648 -0.33613294 -0.96802783  0.04214418]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 5434. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[-0.02112067 -0.10479283 -0.94119686  0.1014843 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 5435. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[ 0.16773689  0.03166664 -0.6519009   0.13834167]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 5436. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[-0.14884639  0.05228817 -0.89767283  0.03282857]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 5437. State = [[-0.07294272 -0.07592594  0.2425092   1.        ]]. Action = [[ 0.3226664  -0.13769025 -0.894287    0.08598208]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 5438. State = [[-2.6520768e-01 -8.8156573e-04  1.1741827e-01  1.0000000e+00]]. Action = [[ 0.03859365 -0.2543071  -0.7339877   0.14131808]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 5439. State = [[-0.25960547 -0.01771348  0.11156043  1.        ]]. Action = [[ 0.43446898 -0.9328154   0.9566095   0.24038756]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5440. State = [[-0.24798338 -0.05000279  0.13311131  1.        ]]. Action = [[ 0.4981339  -0.9362735   0.9902489   0.18019748]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5441. State = [[-0.22999577 -0.08314916  0.17103913  1.        ]]. Action = [[ 0.44511616 -0.83769566  0.9866061   0.2274859 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5442. State = [[-0.21293497 -0.11078727  0.20810771  1.        ]]. Action = [[ 0.3412212  -0.63677174  0.9835789   0.22259188]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5443. State = [[-0.19359232 -0.14259392  0.24998268  1.        ]]. Action = [[ 0.5677128  -0.8874977   0.9233396   0.32835507]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5444. State = [[-0.17096353 -0.17448823  0.28502032  1.        ]]. Action = [[ 0.71899366 -0.7230958   0.4344045   0.32670712]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 5445. State = [[-0.14813654 -0.19892481  0.31488836  1.        ]]. Action = [[ 0.55308604 -0.4952637   0.9829035   0.4336257 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 5446. State = [[-0.13039944 -0.21969189  0.33379298  1.        ]]. Action = [[ 0.3450513  -0.53623635 -0.42320073  0.47429848]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 5447. State = [[-0.11877732 -0.23680158  0.3174352   1.        ]]. Action = [[ 0.2847823  -0.26256442 -0.88938344  0.47742057]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 5448. State = [[-0.10443964 -0.24479495  0.2930749   1.        ]]. Action = [[ 0.3651328  -0.03362966 -0.5946717   0.57169914]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 5449. State = [[-0.08616001 -0.2504864   0.28501928  1.        ]]. Action = [[ 0.38315153 -0.21895367  0.29141867  0.55740523]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 5450. State = [[-0.07036488 -0.2561953   0.2841636   1.        ]]. Action = [[ 0.69014955 -0.21105671 -0.15413761  0.4989469 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 5451. State = [[-0.05051119 -0.2605553   0.2715326   1.        ]]. Action = [[ 0.5741291  -0.06435335 -0.7508596   0.59568095]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 5452. State = [[-0.03152157 -0.26651254  0.24725254  1.        ]]. Action = [[ 0.23116338 -0.14516258 -0.5294296   0.56400347]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 5453. State = [[-0.01634701 -0.268991    0.22442965  1.        ]]. Action = [[ 0.6995466  -0.02070779 -0.8849099   0.5207851 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 5454. State = [[ 0.00241096 -0.27700534  0.19587749  1.        ]]. Action = [[-0.1692496  -0.23264825 -0.25742102  0.69977117]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 5455. State = [[ 0.00928577 -0.28185943  0.18120906  1.        ]]. Action = [[ 0.66320753 -0.14725089 -0.7286198   0.6249386 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 5456. State = [[ 0.03276126 -0.2825206   0.15466611  1.        ]]. Action = [[ 0.7487364   0.3000021  -0.40803397  0.61643267]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 5457. State = [[ 0.05832699 -0.28202587  0.12966463  1.        ]]. Action = [[ 0.7019031  -0.28883374 -0.7636952   0.748814  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 5457 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 5457 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5457 of -1
Current timestep = 5458. State = [[ 0.08681908 -0.28603497  0.10402977  1.        ]]. Action = [[ 0.75962114 -0.23870093 -0.87611103  0.76291656]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 5459. State = [[ 0.08681908 -0.28603497  0.10402977  1.        ]]. Action = [[ 0.5697     -0.24975473 -0.57333106  0.7167473 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 5460. State = [[ 0.08679271 -0.2860434   0.10400859  1.        ]]. Action = [[ 0.84924316 -0.18371642 -0.91166466  0.7729964 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 5461. State = [[ 0.08681908 -0.28603497  0.10402977  1.        ]]. Action = [[ 0.8213091  -0.03065896 -0.31046772  0.70347595]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 5462. State = [[ 0.08679598 -0.28603014  0.10395557  1.        ]]. Action = [[ 0.84779966 -0.12129748 -0.9701745   0.78754044]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 5463. State = [[ 0.0867467  -0.28603375  0.10386089  1.        ]]. Action = [[ 0.66205263 -0.30122173 -0.18602753  0.7593124 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 5464. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.8065684  -0.11112761 -0.02256238  0.64945173]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 5465. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.42213607  0.1166594  -0.82373655  0.7594414 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 5466. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.7499659 -0.4042328 -0.9605806  0.7425978]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 5467. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.77583206  0.03436852 -0.8910831   0.6915102 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 5468. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.81722236  0.21183681 -0.4430588   0.7183244 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 5469. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.26412106  0.18342113 -0.94652313  0.76458454]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 5470. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[0.5191214  0.01959622 0.46394145 0.7612083 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 5471. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.64346886 -0.22978264 -0.9962203   0.7463677 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 5472. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.8175937  -0.27719003 -0.75818944  0.7404516 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 5473. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.73246706 -0.31444168 -0.8975836   0.7380059 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 5474. State = [[ 0.08675016 -0.28602052  0.10380858  1.        ]]. Action = [[ 0.80330944 -0.26535785 -0.48436886  0.74693274]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 5475. State = [[ 0.09098859 -0.28760797  0.11096059  1.        ]]. Action = [[ 0.23916113 -0.06028146  0.57439625  0.78071094]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 5476. State = [[ 0.09773314 -0.28952506  0.12391125  1.        ]]. Action = [[ 0.8657882  -0.00568068 -0.6571351   0.7029047 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5477. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.43507278 -0.05724585 -0.96755654  0.7723186 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 5478. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.8723502   0.18269515 -0.8395461   0.66959107]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 5479. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[0.7103398  0.06515038 0.1486193  0.6139542 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 5480. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.46759617  0.14344501 -0.9254287   0.7763804 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 5481. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.56941986 -0.04450691 -0.66096675  0.7115005 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 5482. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[0.50408363 0.21794462 0.25578642 0.75664616]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 5483. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.58209515  0.3184173  -0.68558747  0.57136154]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 5484. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.4274584   0.19775605 -0.88961625  0.74698734]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 5485. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.6791012   0.1012758  -0.73470324  0.80449605]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 5486. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.66017294 -0.11581099 -0.9950959   0.8136635 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 5487. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.88414526  0.04785514 -0.9153026   0.78083956]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 5488. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.48818183  0.01733184 -0.9869282   0.81976676]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 5489. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.6815734  -0.07326573 -0.98073816  0.8108107 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 5490. State = [[ 0.09903706 -0.28945234  0.12520567  1.        ]]. Action = [[ 0.73446465 -0.07019585  0.7355995   0.70978546]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 5491. State = [[ 0.09901629 -0.289448    0.12513088  1.        ]]. Action = [[0.7156484  0.12016642 0.55638456 0.7166437 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 5492. State = [[ 0.09901629 -0.289448    0.12513088  1.        ]]. Action = [[ 0.6127871  -0.04618365  0.26690507  0.79717445]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 5493. State = [[ 0.09901629 -0.289448    0.12513088  1.        ]]. Action = [[ 0.78735924 -0.09250456 -0.9859497   0.7455857 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 5494. State = [[ 0.09901629 -0.289448    0.12513088  1.        ]]. Action = [[ 0.8717058   0.20187879 -0.98986685  0.7825093 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 5495. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.7315229   0.18454051 -0.7781089   0.7547549 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 5496. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.8617674  -0.21439028 -0.8920716   0.73579335]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 5497. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.93073034  0.12368321 -0.99247867  0.7724016 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 5498. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.34391677 -0.1471529  -0.93371266  0.752789  ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 5499. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 6.7665923e-01 -1.9550323e-05  4.6767163e-01  6.1762643e-01]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 5500. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.86654377  0.25382483 -0.31754732  0.69400835]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 5501. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.8073914   0.17302823 -0.9990058   0.747918  ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 5502. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.84731615 -0.24779284 -0.8697216   0.8091965 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 5503. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.8039504  -0.13261604 -0.3893078   0.73021066]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 5504. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.55957365  0.12255192 -0.9499697   0.70716846]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 5505. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.89112735 -0.07494706 -0.99874187  0.75126934]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 5506. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[0.86559534 0.09494662 0.6848147  0.83965325]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 5507. State = [[ 0.09900241 -0.28953075  0.12513092  1.        ]]. Action = [[ 0.9019275   0.29919875 -0.97541654  0.80872035]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 5508. State = [[ 0.09897902 -0.28951582  0.12512934  1.        ]]. Action = [[ 0.8252146   0.24841368 -0.34487075  0.7809684 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 5509. State = [[ 0.09897902 -0.28951582  0.12512934  1.        ]]. Action = [[ 0.850888    0.57800937 -0.9856318   0.77226067]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 5510. State = [[ 0.09897902 -0.28951582  0.12512934  1.        ]]. Action = [[ 0.87666893  0.18993199 -0.68190837  0.868634  ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 5511. State = [[ 0.09897902 -0.28951582  0.12512934  1.        ]]. Action = [[ 0.6897819   0.26712215 -0.98479956  0.80160713]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 5512. State = [[ 0.09897902 -0.28951582  0.12512934  1.        ]]. Action = [[ 0.8888657   0.16366208 -0.6118654   0.81120694]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 5513. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.85703707  0.03799379 -0.88315827  0.8606777 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 5514. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.91855884  0.02574551 -0.9025804   0.73698735]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 5515. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[0.65736365 0.4978963  0.5327499  0.8135421 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 5516. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.32304597  0.4263295  -0.25460196  0.82252944]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 5517. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.7576128   0.57505155 -0.9802923   0.7323214 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 5518. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.84421134  0.35019636 -0.8607864   0.87309444]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 5519. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.8569839  -0.05150259 -0.9759909   0.77165556]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 5520. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.8339199  -0.15258056 -0.99377245  0.86601496]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 5521. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.9188926  -0.05677617 -0.99975765  0.8147571 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 5522. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.85963976 -0.09971285  0.43973792  0.8671056 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 5523. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.62032557  0.3529036  -0.9992105   0.8645135 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 5524. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.75701666  0.16855288 -0.9804051   0.86973786]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 5525. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[0.7324904  0.29002333 0.99993694 0.88923573]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 5526. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[0.77824926 0.3000933  0.9949602  0.90088   ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 5527. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.8106251   0.1291132  -0.43792886  0.87129927]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 5528. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.93274224  0.14542115 -0.72439307  0.78975904]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 5529. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.9427749   0.3611548  -0.86248785  0.8363664 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5530. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.8470603  0.5462265 -0.9888441  0.8955462]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5531. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[0.8619493  0.28341436 0.98117375 0.79088247]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5532. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[0.69502985 0.2399832  0.8439367  0.9101627 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5533. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.89344144  0.12453043 -0.8002479   0.87174845]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5534. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[0.9149511  0.36349154 0.92841434 0.95654356]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5535. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.85072446  0.10916138 -0.9364518   0.9070189 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5536. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.9233165   0.44210577 -0.8576531   0.9339783 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 5537. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.7436907   0.58497024 -0.48130286  0.93251777]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 5538. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.96113753  0.72904193 -0.99984026  0.9211743 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5539. State = [[ 0.09890909 -0.2894712   0.12512463  1.        ]]. Action = [[ 0.90117455  0.37406814 -0.9285697   0.88074064]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 5540. State = [[-0.2698899   0.16261496  0.11147141  1.        ]]. Action = [[0.94256926 0.46465278 0.6406963  0.92521167]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5541. State = [[-0.26160666  0.1699051   0.10363951  1.        ]]. Action = [[-0.02936006 -0.8484234   0.7185253   0.26322484]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5542. State = [[-0.25766563  0.15730214  0.10953185  1.        ]]. Action = [[-0.26045144 -0.9326948  -0.44472504  0.2641362 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 5543. State = [[-0.25178364  0.1422114   0.11778956  1.        ]]. Action = [[ 0.20940733 -0.7821876   0.58163905  0.2613628 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5544. State = [[-0.23893796  0.1141098   0.14141609  1.        ]]. Action = [[ 0.56102264 -0.97782385  0.9489802   0.2938305 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5545. State = [[-0.22334428  0.0788186   0.17867136  1.        ]]. Action = [[ 0.57708883 -0.9196668   0.99093986  0.20052576]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5546. State = [[-0.20144376  0.04349107  0.21830396  1.        ]]. Action = [[ 0.7439754  -0.9578475   0.9942794   0.49285507]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 5547. State = [[-0.17367059  0.01992235  0.25652108  1.        ]]. Action = [[ 0.8148179  -0.04653621  0.78053284  0.52489305]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 5548. State = [[-1.4217822e-01 -8.3779177e-04  2.9585233e-01  1.0000000e+00]]. Action = [[ 0.8839092  -0.87938553  0.9810009   0.5907836 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 5549. State = [[-0.11286172 -0.0189103   0.33385566  1.        ]]. Action = [[ 0.79246664 -0.13761127  0.820475    0.5899584 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 5550. State = [[-0.0841494  -0.02249605  0.36814535  1.        ]]. Action = [[0.571453   0.09697354 0.89101124 0.6007912 ]]. Reward = [0.]
Curr episode timestep = 9
Above hoop
Current timestep = 5551. State = [[-0.067317   -0.02130751  0.38744614  1.        ]]. Action = [[ 0.38534546  0.062024   -0.30315685  0.58977914]]. Reward = [0.]
Curr episode timestep = 10
Above hoop
Current timestep = 5552. State = [[-0.05188725 -0.01909212  0.38442802  1.        ]]. Action = [[0.08880818 0.02994549 0.00430894 0.31862497]]. Reward = [0.]
Curr episode timestep = 11
Above hoop
Current timestep = 5553. State = [[-0.04326749 -0.01780011  0.39145958  1.        ]]. Action = [[ 0.2315458  -0.00652629  0.43142366  0.2847998 ]]. Reward = [0.]
Curr episode timestep = 12
Above hoop
Scene graph at timestep 5553 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 5553 is tensor(9.0899e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5553 of 1
Current timestep = 5554. State = [[-0.0381417  -0.01528795  0.39650512  1.        ]]. Action = [[ 0.35225463  0.18790448 -0.6807252   0.34869337]]. Reward = [0.]
Curr episode timestep = 13
Above hoop
Scene graph at timestep 5554 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 5554 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5554 of -1
Current timestep = 5555. State = [[-0.02508552 -0.0134262   0.37071773  1.        ]]. Action = [[ 0.23643887 -0.06560349 -0.9771394   0.28141832]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Scene graph at timestep 5555 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 5555 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5555 of -1
Current timestep = 5556. State = [[-0.01051892 -0.01237154  0.33747667  1.        ]]. Action = [[ 0.12856996  0.18147326 -0.51314926  0.22078073]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Current timestep = 5557. State = [[-0.00338441 -0.01223442  0.3251685   1.        ]]. Action = [[ 0.3650844  -0.13626498 -0.01656359  0.3720504 ]]. Reward = [0.]
Curr episode timestep = 16
Above hoop
Current timestep = 5558. State = [[ 0.00600337 -0.00877395  0.31531742  1.        ]]. Action = [[ 0.3158214   0.15660596 -0.86394405  0.22396815]]. Reward = [0.]
Curr episode timestep = 17
Above hoop
Current timestep = 5559. State = [[ 0.01423321 -0.00978736  0.27920875  1.        ]]. Action = [[-0.10451615 -0.1790002  -0.9209784   0.50786877]]. Reward = [0.]
Curr episode timestep = 18
Above hoop
Current timestep = 5560. State = [[ 0.01922394 -0.01103693  0.25534633  1.        ]]. Action = [[ 0.48796844  0.14462507 -0.9733124   0.43464315]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Above hoop
Current timestep = 5561. State = [[ 0.02640649 -0.01411663  0.26474723  1.        ]]. Action = [[ 0.33958495 -0.12993824  0.9577739   0.37236404]]. Reward = [0.]
Curr episode timestep = 20
Above hoop
Current timestep = 5562. State = [[ 0.03435176 -0.01699321  0.2775911   1.        ]]. Action = [[ 0.2830341   0.40265334 -0.8890603   0.36258864]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Above hoop
Current timestep = 5563. State = [[ 0.03689502 -0.01603353  0.2787409   1.        ]]. Action = [[ 0.39325738  0.08547819 -0.07441205  0.442675  ]]. Reward = [0.]
Curr episode timestep = 22
Above hoop
Current timestep = 5564. State = [[ 0.0463451  -0.01169575  0.29036695  1.        ]]. Action = [[0.6137576  0.14581466 0.62169266 0.33625138]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Current timestep = 5565. State = [[ 0.06233909 -0.00323755  0.29326105  1.        ]]. Action = [[ 0.45632696  0.42621994 -0.69105965  0.37992644]]. Reward = [0.]
Curr episode timestep = 24
Above hoop
Current timestep = 5566. State = [[ 8.0449402e-02 -7.3981693e-04  2.6634783e-01  1.0000000e+00]]. Action = [[ 0.5948603  -0.26184523 -0.91233116  0.39725327]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 5567. State = [[ 0.10399183 -0.00302371  0.24181126  1.        ]]. Action = [[ 0.5234747  -0.01671106  0.70061064  0.53899264]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 5568. State = [[ 0.10968027 -0.00208937  0.23979707  1.        ]]. Action = [[ 0.4751737  -0.03867424 -0.8646467   0.3627751 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5569. State = [[ 0.10986901 -0.00211079  0.24013723  1.        ]]. Action = [[-0.11680645 -0.05280393 -0.7929691   0.23783886]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5570. State = [[ 0.11100199 -0.00156628  0.24135537  1.        ]]. Action = [[ 0.04338861  0.26625323 -0.67868465  0.24084365]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5571. State = [[ 0.11101997 -0.00154591  0.2412538   1.        ]]. Action = [[ 0.5503527   0.12510717 -0.8863786   0.1037631 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5572. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.4046178   0.53453183 -0.270221    0.20827878]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5573. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.64989805  0.32296562 -0.2238242   0.375772  ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5574. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.32632232 -0.07946432 -0.5445223   0.1730398 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5575. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.42658472  0.21775293 -0.9716419   0.2646134 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5576. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.46609366  0.02329612 -0.93489206  0.50054026]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5577. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[0.3812207  0.39136052 0.34153926 0.3423133 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 5578. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[0.32191324 0.03536427 0.1823529  0.46111286]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5579. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5573127   0.5029948  -0.44610798  0.505008  ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5580. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5315521  0.2910086 -0.8209396  0.5150225]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5581. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5394943  -0.47297084 -0.868822    0.6969731 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5582. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.76726365  0.29685903 -0.57280207  0.47544646]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5583. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.6387379   0.05466688 -0.8958058   0.5230994 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5584. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.65536     0.4506855  -0.8296653   0.18226504]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5585. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.6754658  -0.3554722   0.46473956  0.47687304]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 5586. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.44207108  0.44750214 -0.47787505  0.39118814]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5587. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5441878  -0.2224868  -0.28185153  0.19880533]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5588. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5408237   0.02908218 -0.7065501   0.39432442]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5589. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[0.5947881  0.602509   0.18887496 0.4135878 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 5590. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5171068  -0.00106144 -0.92978793  0.55939054]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5591. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.3525343   0.39708972 -0.9921275   0.5513135 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5592. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.6825721   0.14462185 -0.61585027  0.32710242]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5593. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.5120828  -0.2644279  -0.5806323   0.52268803]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5594. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.43796766 -0.46443093 -0.8016715   0.426342  ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5595. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.64380276  0.42948294 -0.28283054  0.59497833]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5596. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[0.63817227 0.0550698  0.7430731  0.3434595 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 5597. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.7853123  -0.14557868  0.38488567  0.5935736 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 5598. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.6693039   0.22641683 -0.7740115   0.4614905 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5599. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.6945262   0.19703138 -0.84460706  0.63546586]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5600. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[0.5330825  0.28935146 0.2532301  0.4342028 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 5601. State = [[ 0.11101066 -0.00154732  0.24117668  1.        ]]. Action = [[ 0.62416446 -0.14129454  0.52755356  0.5273318 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 5602. State = [[ 0.11013053 -0.00402262  0.2498251   1.        ]]. Action = [[-0.6038261  -0.03895569  0.63264513  0.5170522 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 5603. State = [[ 0.10814354 -0.00760295  0.2572111   1.        ]]. Action = [[ 0.66314673  0.05886602 -0.19828069  0.57997286]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 5604. State = [[ 0.10755553 -0.00744231  0.2578614   1.        ]]. Action = [[ 0.56644726  0.28031301 -0.27368438  0.28175282]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 5605. State = [[ 0.10756305 -0.00744114  0.25793734  1.        ]]. Action = [[-0.29219323  0.48904335 -0.87500644  0.6205883 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 5606. State = [[ 0.10756305 -0.00744114  0.25793734  1.        ]]. Action = [[ 0.5177176  -0.03513986  0.8744056   0.650193  ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 5607. State = [[ 0.10756305 -0.00744114  0.25793734  1.        ]]. Action = [[ 0.6753583  -0.2723912   0.0652982   0.72965074]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 5608. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[ 0.6478679  -0.51322895 -0.3255496   0.6822516 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 5609. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[ 0.79954004  0.4978342  -0.8768442   0.72308016]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5610. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[ 0.6321194  -0.02273762 -0.8827308   0.4878124 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5611. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[-0.1389175   0.4541502   0.704638    0.52802086]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 5612. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[ 0.5551727  -0.2447412  -0.54288477  0.7452915 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5613. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[0.02119112 0.10562384 0.30808294 0.53157425]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 5614. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[ 0.57383    -0.29288363  0.9453242   0.2224549 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 5615. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[ 0.18272686 -0.15294993 -0.7102267   0.74425936]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5616. State = [[ 0.10760067 -0.00743526  0.2583185   1.        ]]. Action = [[-0.05203503  0.32104683  0.8974949   0.5630574 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 5617. State = [[ 0.10757351 -0.00742957  0.2583173   1.        ]]. Action = [[0.9275253  0.11527395 0.7108691  0.61295176]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 5618. State = [[ 0.10757351 -0.00742957  0.2583173   1.        ]]. Action = [[0.6290622  0.16343117 0.865345   0.6641791 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 5619. State = [[ 0.10757351 -0.00742957  0.2583173   1.        ]]. Action = [[ 0.19098318  0.01013041 -0.9099833   0.59982264]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5620. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[-0.08630717  0.21752346  0.8763323   0.77153397]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 5621. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.7000897  -0.32578087  0.30446815  0.52363634]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 5622. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[-0.4859619  -0.28448796 -0.65968335  0.58411455]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 5623. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[0.8229091  0.515314   0.31697953 0.7430322 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 5624. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.91489196 -0.35892618 -0.18394572  0.6005776 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 5625. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[0.7981436  0.4261595  0.3303001  0.65908575]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 5626. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[0.46306133 0.15907645 0.3509674  0.69612575]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 5627. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.6516495  -0.06430924  0.86423254  0.5771276 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 5628. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.7401931   0.61414266 -0.08080429  0.70929205]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 5629. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.52273333  0.7584629  -0.295555    0.77591276]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 5630. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.9082819  -0.17664516  0.9416201   0.68409824]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 5631. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.6693585   0.03164053 -0.270226    0.8085916 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5632. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.15260303 -0.16533172  0.11464679  0.7399788 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5633. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.6155468  -0.10228479  0.7930273   0.4523623 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5634. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.21117806 -0.20764053  0.8944534   0.72810316]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5635. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[0.07200849 0.3221599  0.5550132  0.7951431 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5636. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.69915533  0.11355138 -0.17399144  0.410748  ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5637. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[0.79576814 0.45727873 0.530926   0.53599095]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5638. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.3433857   0.0815022  -0.77108824  0.6637144 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5639. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.5709568  -0.13527888 -0.8431017   0.61167145]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5640. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.49179733 -0.03483325  0.86845803  0.839005  ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5641. State = [[ 0.10758105 -0.00742839  0.25839394  1.        ]]. Action = [[ 0.62154806  0.03787816 -0.56462806  0.684602  ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5642. State = [[-0.2661644   0.02255029  0.10673502  1.        ]]. Action = [[0.85901904 0.01277769 0.18215513 0.5166397 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5643. State = [[-0.25846478  0.01273138  0.10219551  1.        ]]. Action = [[ 0.5778611  -0.9617203   0.9909123   0.11948466]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5644. State = [[-0.24016285 -0.01657257  0.12457881  1.        ]]. Action = [[ 0.5789552  -0.96193224  0.98227787  0.08912098]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5645. State = [[-0.21517037 -0.05001834  0.16259505  1.        ]]. Action = [[ 0.6247628  -0.93528986  0.98990893  0.23685133]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5646. State = [[-0.19006972 -0.08081266  0.20108226  1.        ]]. Action = [[ 0.5184711  -0.61891884  0.96495557  0.20125687]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5647. State = [[-0.16534019 -0.10917135  0.24087839  1.        ]]. Action = [[ 0.67515886 -0.7524853   0.9739778   0.49539125]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5648. State = [[-0.14200726 -0.12978908  0.28123087  1.        ]]. Action = [[ 0.41247237 -0.14299572  0.95010054  0.8287717 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 5649. State = [[-0.11789587 -0.14196765  0.31945056  1.        ]]. Action = [[ 0.8308015  -0.38428217  0.98497343  0.8332145 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 5650. State = [[-0.09672826 -0.14837016  0.36142802  1.        ]]. Action = [[0.20548177 0.09199667 0.936093   0.9226103 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 5651. State = [[-0.08383912 -0.15859649  0.39755118  1.        ]]. Action = [[ 0.2539053  -0.5579767   0.74555063  0.8735112 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 5652. State = [[-0.07936036 -0.16690844  0.41959953  1.        ]]. Action = [[ 0.44537508 -0.16740447  0.9476211   0.86622906]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 5653. State = [[-0.07938301 -0.16770224  0.4204246   1.        ]]. Action = [[-0.03430539  0.13231301  0.9954722   0.9575037 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 5654. State = [[-0.07943339 -0.16782318  0.42045283  1.        ]]. Action = [[ 0.9590783  -0.07173067  0.18978775  0.9306787 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 5655. State = [[-0.06915022 -0.16855295  0.418194    1.        ]]. Action = [[ 0.9807645  -0.0904516  -0.22688735  0.95867205]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 5656. State = [[-0.04675892 -0.17008938  0.41553557  1.        ]]. Action = [[0.74468243 0.34405148 0.98292506 0.95750475]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 5657. State = [[-0.04248565 -0.16993648  0.41763985  1.        ]]. Action = [[0.48143065 0.67978954 0.9994452  0.9705255 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 5658. State = [[-0.04213115 -0.17053366  0.4172754   1.        ]]. Action = [[0.9771416  0.75974035 0.83770823 0.9820726 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 5659. State = [[-0.04213115 -0.17053366  0.4172754   1.        ]]. Action = [[0.96107376 0.67270756 0.61043024 0.87683666]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 5660. State = [[-0.04213115 -0.17053366  0.4172754   1.        ]]. Action = [[0.6305727  0.47261536 0.77598965 0.9533689 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 5661. State = [[-0.04213115 -0.17053366  0.4172754   1.        ]]. Action = [[ 0.91881585  0.71764314 -0.00794905  0.8948511 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 5662. State = [[-0.04213115 -0.17053366  0.4172754   1.        ]]. Action = [[0.99122846 0.5347929  0.8732321  0.75216556]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 5663. State = [[-0.04213115 -0.17053366  0.4172754   1.        ]]. Action = [[-0.00191808  0.12472045  0.9952464   0.9786303 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 5664. State = [[-0.03820848 -0.15877002  0.40794393  1.        ]]. Action = [[ 0.4867314  0.8074889 -0.9046215  0.8846202]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 5665. State = [[-0.02480484 -0.14927953  0.389844    1.        ]]. Action = [[-0.93378097  0.62728703  0.35370874  0.9351095 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 5666. State = [[-0.01636771 -0.141097    0.37786376  1.        ]]. Action = [[ 0.7500222   0.26840746 -0.87945056  0.8457999 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 5667. State = [[-2.9406016e-04 -1.3494964e-01  3.5171795e-01  1.0000000e+00]]. Action = [[-0.15972918  0.11568737 -0.23837852  0.9427967 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 5668. State = [[ 0.00844157 -0.12949526  0.3562813   1.        ]]. Action = [[0.38054907 0.23321903 0.70623326 0.96147656]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 5669. State = [[ 0.02006661 -0.11972653  0.375174    1.        ]]. Action = [[0.30972838 0.3149104  0.80436397 0.95755816]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 5670. State = [[ 0.02872602 -0.11603222  0.3856444   1.        ]]. Action = [[ 0.87530804 -0.3653878  -0.9405753   0.91184247]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 5671. State = [[ 0.04543437 -0.11539042  0.3573894   1.        ]]. Action = [[-0.60584605  0.31643832 -0.4514448   0.7969804 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 5672. State = [[ 0.04327752 -0.11697918  0.36349505  1.        ]]. Action = [[-0.9969357  -0.07284915  0.8998828   0.8980651 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 5673. State = [[ 0.036163   -0.11306361  0.37671268  1.        ]]. Action = [[ 0.9699421   0.11328244 -0.25338936  0.8692223 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 5674. State = [[ 0.03557862 -0.09788973  0.36098176  1.        ]]. Action = [[-0.8615172   0.8586962  -0.89608157  0.9371002 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 5675. State = [[ 0.02942407 -0.07916486  0.36269054  1.        ]]. Action = [[-0.08850896  0.12033963  0.960233    0.95450866]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 5676. State = [[ 0.02899021 -0.06894954  0.38342327  1.        ]]. Action = [[0.46729255 0.25512648 0.84747314 0.80129266]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 5677. State = [[ 0.03048808 -0.06377613  0.39522788  1.        ]]. Action = [[-0.17503673  0.4741398   0.7353947   0.8531916 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Above hoop
Current timestep = 5678. State = [[ 0.03150636 -0.06253327  0.39915112  1.        ]]. Action = [[0.33461916 0.7573416  0.65339994 0.7441467 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Above hoop
Current timestep = 5679. State = [[ 0.0316768  -0.05827168  0.40450367  1.        ]]. Action = [[-0.18675351  0.23563683  0.25420928  0.8785033 ]]. Reward = [0.]
Curr episode timestep = 36
Above hoop
Current timestep = 5680. State = [[ 0.03029097 -0.05419085  0.40700364  1.        ]]. Action = [[-0.5206486   0.24964142  0.7940788   0.7462889 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Above hoop
Current timestep = 5681. State = [[ 0.03011079 -0.05360341  0.40727565  1.        ]]. Action = [[ 0.72765124 -0.09569216  0.85913706  0.8255279 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Above hoop
Current timestep = 5682. State = [[ 0.03011079 -0.05360341  0.40727565  1.        ]]. Action = [[-0.5470876  0.5364096  0.9333508  0.7593849]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Above hoop
Current timestep = 5683. State = [[ 0.03011079 -0.05360341  0.40727565  1.        ]]. Action = [[0.8699596  0.07233942 0.9286623  0.64551425]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Above hoop
Current timestep = 5684. State = [[ 0.03194714 -0.04925811  0.40996855  1.        ]]. Action = [[0.46462476 0.2810377  0.08942258 0.8457339 ]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Current timestep = 5685. State = [[ 0.03526963 -0.0455289   0.41569227  1.        ]]. Action = [[0.5994711  0.3817073  0.28674865 0.7806108 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Above hoop
Current timestep = 5686. State = [[ 0.03523794 -0.04486396  0.41645536  1.        ]]. Action = [[ 0.74808097 -0.1454882   0.7020631   0.50580025]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Above hoop
Current timestep = 5687. State = [[ 0.03520957 -0.04475291  0.41651514  1.        ]]. Action = [[ 0.5659199  -0.266836    0.68031275  0.7423115 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Above hoop
Current timestep = 5688. State = [[ 0.03499131 -0.04462308  0.41669053  1.        ]]. Action = [[0.75307953 0.45002985 0.9603672  0.6939565 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Above hoop
Current timestep = 5689. State = [[ 0.03488488 -0.0445995   0.4166847   1.        ]]. Action = [[-0.35001445 -0.3518076   0.69531417  0.68890285]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Above hoop
Current timestep = 5690. State = [[ 0.03464162 -0.04454558  0.41667166  1.        ]]. Action = [[ 0.6783304  -0.10084611  0.6897969   0.8405262 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Above hoop
Current timestep = 5691. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.5433905   0.25459576  0.9760679   0.57228804]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Above hoop
Current timestep = 5692. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[ 0.58889985 -0.3753841   0.8741909   0.71034265]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Above hoop
Current timestep = 5693. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.2447145   0.11366081  0.971828    0.7758193 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Above hoop
Current timestep = 5694. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[0.15568614 0.44019687 0.6872829  0.66721725]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Above hoop
Current timestep = 5695. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.13014579 -0.05019265  0.27826095  0.8585546 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Above hoop
Current timestep = 5696. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.48211527  0.52437544  0.8532684   0.7733288 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Above hoop
Current timestep = 5697. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[ 0.10909402 -0.65542245  0.9838295   0.7351861 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Above hoop
Current timestep = 5698. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[0.8253622  0.39191723 0.90660214 0.51802635]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Above hoop
Current timestep = 5699. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[ 0.71846914 -0.04790223  0.91980386  0.68162966]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Above hoop
Current timestep = 5700. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[ 0.6784229  -0.16848004  0.6762955   0.75658846]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Above hoop
Current timestep = 5701. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.544145   -0.24116331  0.734944    0.6663207 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Above hoop
Current timestep = 5702. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.74998826 -0.14600706  0.9598025   0.77195466]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Above hoop
Current timestep = 5703. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[ 0.75649905 -0.44179654  0.41935647  0.81326866]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Above hoop
Current timestep = 5704. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[ 0.374449  -0.1863271  0.7060964  0.7830297]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Above hoop
Current timestep = 5705. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.27397692  0.2822554   0.66748023  0.46649086]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Above hoop
Current timestep = 5706. State = [[ 0.03456049 -0.0445276   0.4166674   1.        ]]. Action = [[-0.42860478  0.42458677  0.9050412   0.73826075]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Above hoop
Current timestep = 5707. State = [[ 0.03883006 -0.04325976  0.415413    1.        ]]. Action = [[ 0.863094   -0.03178066 -0.06447148  0.7609551 ]]. Reward = [0.]
Curr episode timestep = 64
Above hoop
Current timestep = 5708. State = [[ 0.05545485 -0.03854557  0.4124936   1.        ]]. Action = [[ 0.34641433 -0.21218216  0.6511177   0.7907183 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Above hoop
Current timestep = 5709. State = [[ 0.06136107 -0.03711598  0.41334605  1.        ]]. Action = [[-0.04335362  0.42724395  0.9380058   0.68875885]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Above hoop
Current timestep = 5710. State = [[ 0.06139806 -0.03687525  0.4139155   1.        ]]. Action = [[0.00807548 0.07525122 0.9280077  0.62827706]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Above hoop
Current timestep = 5711. State = [[ 0.06303015 -0.04204333  0.41085264  1.        ]]. Action = [[ 0.31820595 -0.24778622 -0.50932807  0.65510845]]. Reward = [0.]
Curr episode timestep = 68
Above hoop
Current timestep = 5712. State = [[ 0.0720761  -0.04547756  0.39516765  1.        ]]. Action = [[0.08181036 0.34686732 0.9606755  0.7806667 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Above hoop
Current timestep = 5713. State = [[ 0.07505716 -0.04583007  0.39538503  1.        ]]. Action = [[ 0.70487297  0.20743906 -0.02189332  0.7244415 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Above hoop
Current timestep = 5714. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.58890414 0.39563942 0.6990731  0.82146263]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Above hoop
Current timestep = 5715. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.7914822  0.05363858 0.91382515 0.6629474 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Above hoop
Current timestep = 5716. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.93611383 0.25372863 0.4922757  0.6903515 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Above hoop
Current timestep = 5717. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[ 0.74994993 -0.24156398  0.87009835  0.8401793 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Above hoop
Current timestep = 5718. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.7412846  0.3083756  0.14594257 0.8444549 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Above hoop
Current timestep = 5719. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.2808429  0.44968426 0.8323293  0.77955663]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Above hoop
Current timestep = 5720. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.23061919 0.07194519 0.8780072  0.80956316]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Above hoop
Current timestep = 5721. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.90002203 0.04573631 0.45588636 0.7007687 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Above hoop
Current timestep = 5722. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.07133532 0.0601573  0.802269   0.6633768 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Above hoop
Current timestep = 5723. State = [[ 0.07506009 -0.04596341  0.3954025   1.        ]]. Action = [[0.49096155 0.0298245  0.86878216 0.71338725]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Above hoop
Current timestep = 5724. State = [[ 0.07517313 -0.04514062  0.394768    1.        ]]. Action = [[ 0.09011471  0.21200919 -0.02535349  0.69507694]]. Reward = [0.]
Curr episode timestep = 81
Above hoop
Current timestep = 5725. State = [[ 0.07614558 -0.04374453  0.39304513  1.        ]]. Action = [[-0.10888875 -0.4595697   0.8307663   0.6478431 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Above hoop
Current timestep = 5726. State = [[ 0.0771994  -0.04289558  0.39245436  1.        ]]. Action = [[0.58670497 0.57729673 0.24781394 0.6754383 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Above hoop
Current timestep = 5727. State = [[ 0.07767513 -0.04271393  0.3920176   1.        ]]. Action = [[-0.5395402   0.44459224  0.88631606  0.6527833 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Above hoop
Current timestep = 5728. State = [[ 0.07767513 -0.04271393  0.3920176   1.        ]]. Action = [[ 0.27108455 -0.18243814  0.8229306   0.6838604 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Above hoop
Current timestep = 5729. State = [[ 0.07768125 -0.04271343  0.3919454   1.        ]]. Action = [[0.4772259  0.06136286 0.9123781  0.7487719 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Above hoop
Current timestep = 5730. State = [[ 0.07985752 -0.04642257  0.38984942  1.        ]]. Action = [[ 0.3944379  -0.3137601  -0.13022089  0.706501  ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 5731. State = [[ 0.09395722 -0.05722415  0.39290506  1.        ]]. Action = [[ 0.27829313 -0.41048574  0.45547354  0.8245394 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 5732. State = [[ 0.10248699 -0.06513562  0.40214455  1.        ]]. Action = [[0.44890702 0.5929086  0.6425903  0.8773446 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 5733. State = [[ 0.10214843 -0.06624331  0.40359166  1.        ]]. Action = [[0.6037841  0.18739533 0.59483266 0.7125623 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5734. State = [[ 0.10216817 -0.0662993   0.40361857  1.        ]]. Action = [[-0.38953733 -0.12887782  0.98786426  0.82994485]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5735. State = [[ 0.10206033 -0.06627415  0.40361297  1.        ]]. Action = [[0.47574377 0.29616475 0.49092877 0.82748973]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5736. State = [[ 0.10206033 -0.06627415  0.40361297  1.        ]]. Action = [[ 0.9037268   0.6333822  -0.00388074  0.79431903]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5737. State = [[ 0.10206033 -0.06627415  0.40361297  1.        ]]. Action = [[-0.05396998 -0.13023746  0.40731275  0.8406217 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5738. State = [[ 0.10206033 -0.06627415  0.40361297  1.        ]]. Action = [[ 0.5494158   0.50278246 -0.29607677  0.78617644]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5739. State = [[ 0.10206033 -0.06627415  0.40361297  1.        ]]. Action = [[ 0.37272418 -0.04890627 -0.57996446  0.8281174 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5740. State = [[ 0.10206033 -0.06627415  0.40361297  1.        ]]. Action = [[0.83947587 0.42752755 0.9651531  0.64339733]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 5741. State = [[ 0.10208549 -0.06625892  0.40366858  1.        ]]. Action = [[-0.07030773  0.36611712  0.5723045   0.81165206]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 5742. State = [[ 0.10208549 -0.06625892  0.40366858  1.        ]]. Action = [[0.7868861  0.22653747 0.87067914 0.67720914]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5743. State = [[ 0.10208549 -0.06625892  0.40366858  1.        ]]. Action = [[0.45914507 0.11414862 0.6466112  0.78731656]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 5744. State = [[-0.2600017  -0.1364438   0.11137319  1.        ]]. Action = [[-0.1791035  -0.33532393  0.63305974  0.85169864]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5745. State = [[-0.25622302 -0.16614312  0.10090394  1.        ]]. Action = [[ 0.4070419  -0.98056686  0.64208066  0.04125047]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5746. State = [[-0.24592873 -0.2020377   0.11736399  1.        ]]. Action = [[ 0.34136128 -0.93524235  0.86359715  0.1787231 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5747. State = [[-0.23435782 -0.23838294  0.14638022  1.        ]]. Action = [[ 0.18738341 -0.9078831   0.7029598   0.26877093]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5748. State = [[-0.22032253 -0.25218225  0.16399358  1.        ]]. Action = [[0.65413904 0.44555235 0.00173604 0.59852815]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5749. State = [[-0.19919279 -0.2580612   0.17783424  1.        ]]. Action = [[ 0.42789507 -0.54839206  0.7176869   0.6322651 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5750. State = [[-0.1744227  -0.26192927  0.20268832  1.        ]]. Action = [[0.8398855  0.35280132 0.6813803  0.7253797 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 5751. State = [[-0.14592414 -0.2562385   0.22584507  1.        ]]. Action = [[0.6508143  0.32085383 0.37021065 0.82701945]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 5752. State = [[-0.12186808 -0.24635755  0.24595939  1.        ]]. Action = [[0.5124526  0.26399088 0.54090285 0.7448547 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 5753. State = [[-0.09686965 -0.22630008  0.26912597  1.        ]]. Action = [[0.5578728  0.9156792  0.6629205  0.57803106]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 5754. State = [[-0.08819873 -0.19999954  0.28086507  1.        ]]. Action = [[-0.85883266  0.52032995 -0.41763246  0.92741764]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 5755. State = [[-0.08912756 -0.17906828  0.2812106   1.        ]]. Action = [[0.9037869  0.44152832 0.14594495 0.8835697 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 5756. State = [[-0.07571516 -0.16340141  0.28696296  1.        ]]. Action = [[0.5443692  0.24964905 0.15776968 0.8168181 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 5757. State = [[-0.06616916 -0.15946482  0.30407566  1.        ]]. Action = [[-0.32115555 -0.12227905  0.9562166   0.8639934 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 5758. State = [[-0.05374104 -0.1547066   0.33369935  1.        ]]. Action = [[0.9832549  0.301319   0.85292435 0.8736503 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 5759. State = [[-0.03127807 -0.14910965  0.3717626   1.        ]]. Action = [[0.5731305  0.05588138 0.9757154  0.9794774 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 5760. State = [[-0.00896851 -0.13878657  0.40582833  1.        ]]. Action = [[0.6212332  0.43544042 0.59993696 0.88516474]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 5761. State = [[ 0.00706297 -0.13153273  0.42383718  1.        ]]. Action = [[0.38331485 0.41496336 0.9367461  0.924006  ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 5762. State = [[ 0.00982033 -0.13050711  0.4261156   1.        ]]. Action = [[ 0.67811537 -0.2265498   0.99223924  0.9524714 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 5763. State = [[ 0.00979208 -0.13005625  0.42610693  1.        ]]. Action = [[-0.27615857  0.6662488   0.8117249   0.90470743]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 5764. State = [[ 0.00983086 -0.12959488  0.42598948  1.        ]]. Action = [[-0.65687984  0.60267615  0.75653386  0.8526237 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 5765. State = [[ 0.01000176 -0.12944616  0.42637727  1.        ]]. Action = [[0.68046427 0.33463955 0.9459193  0.77395535]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 5766. State = [[ 0.01001145 -0.12944734  0.42631286  1.        ]]. Action = [[0.80278873 0.32901692 0.6524317  0.9667369 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 5767. State = [[ 0.01001145 -0.12944734  0.42631286  1.        ]]. Action = [[0.830294   0.49008727 0.88123417 0.9262954 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 5768. State = [[ 0.00990593 -0.12942003  0.42630693  1.        ]]. Action = [[0.08604145 0.6340251  0.915032   0.9326333 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 5769. State = [[ 0.00982968 -0.12938137  0.42640284  1.        ]]. Action = [[0.85474336 0.75274944 0.81854796 0.8021445 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 5770. State = [[ 0.00982968 -0.12938137  0.42640284  1.        ]]. Action = [[ 0.7490401  -0.40140343  0.5256847   0.93989015]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 5771. State = [[ 0.01277275 -0.11918657  0.4204294   1.        ]]. Action = [[ 0.82683384  0.4823444  -0.7752123   0.48438656]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 5772. State = [[ 0.03745141 -0.11214954  0.40190497  1.        ]]. Action = [[0.2566483  0.19706404 0.643558   0.91180575]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 5773. State = [[ 0.04373139 -0.11171408  0.3995085   1.        ]]. Action = [[-0.5425007   0.01573431  0.7071544   0.84022164]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 5774. State = [[ 0.04715352 -0.11174873  0.39964405  1.        ]]. Action = [[-0.03276116  0.5666976   0.38383305  0.94090414]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 5775. State = [[ 0.04739442 -0.11152441  0.39864254  1.        ]]. Action = [[0.8405585  0.07973003 0.38419676 0.98227465]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 5776. State = [[ 0.04751672 -0.11153334  0.39861     1.        ]]. Action = [[-0.24102896  0.5439553   0.8438177   0.84635925]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 5777. State = [[ 0.04947348 -0.10244521  0.40163115  1.        ]]. Action = [[0.2614746  0.5367937  0.25708127 0.8311932 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 5778. State = [[ 0.05287787 -0.09133503  0.4078368   1.        ]]. Action = [[0.47389328 0.66707253 0.49167705 0.9040704 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 5779. State = [[ 0.05267788 -0.08985823  0.40780506  1.        ]]. Action = [[0.5981606 0.6703584 0.8287492 0.814687 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 5780. State = [[ 0.05240364 -0.08935006  0.4076672   1.        ]]. Action = [[0.667078   0.42719173 0.9050455  0.9548631 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 5781. State = [[ 0.05237727 -0.08926819  0.40759683  1.        ]]. Action = [[0.72140765 0.20665383 0.69396234 0.6108568 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 5782. State = [[ 0.05237727 -0.08926819  0.40759683  1.        ]]. Action = [[ 0.40783238 -0.6999623   0.8431182   0.84302604]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5783. State = [[ 0.05703661 -0.07578583  0.41012275  1.        ]]. Action = [[0.5343256  0.7711592  0.14901209 0.8730012 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 5784. State = [[ 0.07184455 -0.06139565  0.41084963  1.        ]]. Action = [[0.43448293 0.37338018 0.54790163 0.9151132 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Above hoop
Current timestep = 5785. State = [[ 0.07919361 -0.05894922  0.41411492  1.        ]]. Action = [[ 0.61772275 -0.20183992  0.7520716   0.7627598 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Above hoop
Current timestep = 5786. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.64271355 0.6270416  0.8963437  0.8833351 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Above hoop
Current timestep = 5787. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.45835817 0.25080633 0.32066178 0.9192004 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Above hoop
Current timestep = 5788. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[-0.6123197   0.2841786   0.83697534  0.8546159 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Above hoop
Current timestep = 5789. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.55614173 0.50386095 0.9494605  0.8699205 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Above hoop
Current timestep = 5790. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.7346144  0.02431691 0.84731674 0.77273977]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Above hoop
Current timestep = 5791. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.61327314 -0.18549645  0.8007871   0.41038406]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Above hoop
Current timestep = 5792. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.88596344 0.61819935 0.88138103 0.8219781 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Above hoop
Current timestep = 5793. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.8142526  0.77503    0.21876347 0.8482727 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Above hoop
Current timestep = 5794. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.7745435  -0.02597868  0.79086816  0.6992037 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Above hoop
Current timestep = 5795. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.520368   0.5297811  0.91643226 0.8444669 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Above hoop
Current timestep = 5796. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.9432454  0.39718175 0.20517719 0.78211164]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Above hoop
Current timestep = 5797. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.9142885  0.70823836 0.957052   0.8479018 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Above hoop
Current timestep = 5798. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[-0.04763973  0.50215554  0.97513604  0.8128551 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Above hoop
Current timestep = 5799. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.5977242  0.40286827 0.38199186 0.72777176]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Above hoop
Current timestep = 5800. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.9603975  -0.34905976  0.86267626  0.74608994]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Above hoop
Current timestep = 5801. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.39270377 -0.44173598  0.8678762   0.79246676]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Above hoop
Current timestep = 5802. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.5714915  0.0554688  0.75558734 0.84266067]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Above hoop
Current timestep = 5803. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.06621647 -0.02615196  0.9823985   0.88000095]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Above hoop
Current timestep = 5804. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.5275855  0.49215245 0.76808167 0.6756054 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Above hoop
Current timestep = 5805. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.933028   0.68592155 0.8417766  0.79023695]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Above hoop
Current timestep = 5806. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.6801194 0.2506312 0.8593509 0.661957 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Above hoop
Current timestep = 5807. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.7175844  0.05675495 0.93700457 0.72755885]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Above hoop
Current timestep = 5808. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.48719907 0.5081415  0.98633194 0.75111055]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Above hoop
Current timestep = 5809. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[-0.57501954  0.6924964   0.78125     0.6605141 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Above hoop
Current timestep = 5810. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.8528235  0.16126347 0.7618909  0.49567318]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Above hoop
Current timestep = 5811. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.56943655 0.7131171  0.9463866  0.7359829 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Above hoop
Current timestep = 5812. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.02929568 0.81134415 0.5899011  0.8640492 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Above hoop
Current timestep = 5813. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.5776106 0.7700689 0.9002465 0.7516146]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Above hoop
Current timestep = 5814. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.28101623 0.38484108 0.57522106 0.9116175 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Above hoop
Current timestep = 5815. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.8801689  -0.18250507  0.23326874  0.88354063]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Above hoop
Current timestep = 5816. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.4265604  -0.04876572  0.8843701   0.7293618 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Above hoop
Current timestep = 5817. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.36141372 0.4072628  0.27168822 0.8227998 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Above hoop
Current timestep = 5818. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.69665647  0.15100098 -0.647811    0.67281103]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Above hoop
Current timestep = 5819. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.8525677  0.52291965 0.7826141  0.78109646]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Above hoop
Current timestep = 5820. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.68339825 0.16524696 0.56092274 0.832212  ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Above hoop
Current timestep = 5821. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.06027532 0.1328721  0.7976438  0.34467506]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Above hoop
Current timestep = 5822. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.2586863  0.56962776 0.5701306  0.84151125]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Above hoop
Current timestep = 5823. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.3300699  0.5765538  0.83652925 0.6375575 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Above hoop
Current timestep = 5824. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[ 0.5173173  -0.40145367  0.221017    0.7755637 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Above hoop
Current timestep = 5825. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[-0.512737    0.6317314   0.85645497  0.71056914]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Above hoop
Current timestep = 5826. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.6532953  0.29781437 0.52119696 0.8738973 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Above hoop
Current timestep = 5827. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[-0.18507367  0.13470244  0.96734357  0.8097379 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Above hoop
Current timestep = 5828. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.6427176  0.5564003  0.61058235 0.796512  ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Above hoop
Current timestep = 5829. State = [[ 0.07952111 -0.05848436  0.41458666  1.        ]]. Action = [[0.8713511  0.19669843 0.80080783 0.75614166]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Above hoop
Current timestep = 5830. State = [[ 0.07883172 -0.05144805  0.40843672  1.        ]]. Action = [[ 0.26005554  0.40895724 -0.77820563  0.7711861 ]]. Reward = [0.]
Curr episode timestep = 85
Above hoop
Current timestep = 5831. State = [[ 0.08729043 -0.04430016  0.38781264  1.        ]]. Action = [[-0.11148196  0.45758247  0.88176966  0.83377624]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 5832. State = [[ 0.09340071 -0.04316791  0.38465518  1.        ]]. Action = [[0.74998677 0.19897556 0.87890005 0.56784177]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 5833. State = [[ 0.09387264 -0.04275562  0.38517496  1.        ]]. Action = [[0.3806436  0.58224154 0.83101976 0.67363906]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 5834. State = [[ 0.09425602 -0.04462179  0.3883307   1.        ]]. Action = [[-0.13667792 -0.11050314  0.31311274  0.83983135]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 5835. State = [[ 0.0941405  -0.04531851  0.3900091   1.        ]]. Action = [[ 0.54344237 -0.35928392  0.8923111   0.78625345]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5836. State = [[ 0.09410325 -0.04554917  0.39023462  1.        ]]. Action = [[0.18842053 0.19797647 0.2498368  0.810956  ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5837. State = [[ 0.09410325 -0.04554917  0.39023462  1.        ]]. Action = [[ 0.00689459 -0.06866604  0.95098567  0.6875895 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5838. State = [[ 0.0939929  -0.04554018  0.39022893  1.        ]]. Action = [[ 0.70356536  0.15879822 -0.6045407   0.8111079 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5839. State = [[ 0.0939929  -0.04554018  0.39022893  1.        ]]. Action = [[0.5960734  0.69324195 0.5655844  0.8520379 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5840. State = [[ 0.0939929  -0.04554018  0.39022893  1.        ]]. Action = [[ 0.41633785 -0.39212513  0.59778416  0.7678615 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5841. State = [[ 0.0939929  -0.04554018  0.39022893  1.        ]]. Action = [[0.8717178 0.3666159 0.7064141 0.7830546]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5842. State = [[ 0.0939929  -0.04554018  0.39022893  1.        ]]. Action = [[ 0.4516083   0.41188097 -0.7737071   0.81193876]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 5843. State = [[ 0.0939929  -0.04554018  0.39022893  1.        ]]. Action = [[0.8930321 0.3362254 0.7627275 0.6968684]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 5844. State = [[ 0.09391007 -0.04553343  0.39022473  1.        ]]. Action = [[0.66329014 0.89481807 0.9643339  0.8028755 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5845. State = [[ 0.09391007 -0.04553343  0.39022473  1.        ]]. Action = [[0.04345131 0.27065063 0.876086   0.837553  ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 5846. State = [[-0.2655145  -0.00302494  0.11585451  1.        ]]. Action = [[ 0.7758727  -0.25692838  0.6154901   0.7293227 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5847. State = [[-0.25905615 -0.02073214  0.10951436  1.        ]]. Action = [[ 0.52002704 -0.94169056  0.86452436  0.12358463]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5848. State = [[-0.24680834 -0.05292563  0.12963386  1.        ]]. Action = [[ 0.49563766 -0.98508996  0.8726249   0.16481745]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5849. State = [[-0.2248265  -0.08837348  0.16296418  1.        ]]. Action = [[ 0.62135553 -0.8698845   0.98557436  0.0805732 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5850. State = [[-0.19697426 -0.12330191  0.2027932   1.        ]]. Action = [[ 0.7275945  -0.88080263  0.94257045  0.22687149]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5851. State = [[-0.17078613 -0.15739235  0.23881039  1.        ]]. Action = [[ 0.53743434 -0.84398925  0.7692318   0.6861533 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5852. State = [[-0.14939296 -0.17416033  0.2665633   1.        ]]. Action = [[0.63164353 0.24605262 0.49831784 0.8201518 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 5853. State = [[-0.12319179 -0.1792729   0.2975667   1.        ]]. Action = [[ 0.5575254  -0.26947427  0.95848954  0.84569025]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 5854. State = [[-0.10793533 -0.17586632  0.3323621   1.        ]]. Action = [[-0.19290262  0.5262525   0.787554    0.9034791 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 5855. State = [[-0.0979149  -0.15698512  0.35240248  1.        ]]. Action = [[0.6993755  0.8389621  0.00733161 0.9234338 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 5856. State = [[-0.08673921 -0.12672645  0.37193272  1.        ]]. Action = [[0.09733963 0.7994945  0.92554307 0.9882903 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 5857. State = [[-0.07890869 -0.10894897  0.3912382   1.        ]]. Action = [[0.6027088  0.5002105  0.84811044 0.9722085 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 5858. State = [[-0.0736362  -0.10283527  0.39836502  1.        ]]. Action = [[0.47684526 0.20258462 0.24340248 0.9691601 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 5859. State = [[-0.06593487 -0.09878007  0.4063081   1.        ]]. Action = [[0.5159757  0.93841624 0.9209912  0.8802645 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 5860. State = [[-0.06587856 -0.09775161  0.4064253   1.        ]]. Action = [[ 0.513927  -0.1592074  0.9536357  0.878839 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 5861. State = [[-0.06580844 -0.0974906   0.40637803  1.        ]]. Action = [[0.65341115 0.6973902  0.92134833 0.97200096]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 5862. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[0.89074695 0.5814812  0.6562444  0.96650743]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 5863. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[ 0.5973654  -0.4965133   0.7088349   0.80579114]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 5864. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[-0.27616405 -0.45357037  0.46477246  0.67930734]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 5865. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[0.7698343  0.4155562  0.45891142 0.87968874]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 5866. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[0.9040134  0.5451366  0.6530485  0.72829497]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 5867. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[-0.7397006   0.82436705  0.9901016   0.84740067]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 5868. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[-0.30235958  0.34614503  0.7027762   0.9201299 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 5869. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[ 0.8262706  -0.80423623  0.40031505  0.84830856]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 5870. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[0.6905571  0.70301616 0.5143192  0.83642006]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 5871. State = [[-0.06591415 -0.09745768  0.40637234  1.        ]]. Action = [[-0.1955418  -0.54676515  0.97020316  0.8811623 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 5872. State = [[-0.05586599 -0.09245183  0.40858683  1.        ]]. Action = [[0.758749   0.27931857 0.17030382 0.8764634 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 5873. State = [[-0.03887694 -0.08800598  0.41370735  1.        ]]. Action = [[0.25414002 0.6502085  0.85466766 0.8744571 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 5874. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.24146116 0.25555527 0.5422323  0.8921691 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 5875. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.9403045  0.9223988  0.61631143 0.92278004]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 5876. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.4510877  0.5625093  0.96287274 0.8652891 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 5877. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.8806281  0.6017208  0.93940544 0.6666852 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 5878. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.6510941  0.95523894 0.8492291  0.9425125 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 5879. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.17539573 0.6150651  0.6967907  0.88922226]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 5880. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.04502416 0.873489   0.69538426 0.91826105]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 5881. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.2378465  0.73233056 0.6761955  0.851951  ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 5882. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[ 0.5902107  -0.05746239  0.9265728   0.8234019 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 5883. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[-0.01948351  0.60026217  0.97859335  0.77825856]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 5884. State = [[-0.03495873 -0.08727267  0.4157899   1.        ]]. Action = [[0.43666112 0.11256027 0.9104643  0.7082516 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5885. State = [[-0.03331001 -0.09367438  0.41016546  1.        ]]. Action = [[ 0.4174701  -0.5177488  -0.48187172  0.7151867 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 5886. State = [[-0.02182181 -0.0995279   0.4003162   1.        ]]. Action = [[0.5853082  0.47597945 0.6756053  0.78511965]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 5887. State = [[-0.02167901 -0.09989203  0.4004321   1.        ]]. Action = [[-0.3555308  0.8444836  0.7593539  0.8762908]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 5888. State = [[-0.02168954 -0.10002528  0.40044564  1.        ]]. Action = [[0.49831545 0.5715442  0.7593701  0.9424089 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 5889. State = [[-0.02170574 -0.10008897  0.40045917  1.        ]]. Action = [[0.26281095 0.83226025 0.6683233  0.8030536 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 5890. State = [[-0.02170574 -0.10008897  0.40045917  1.        ]]. Action = [[-0.2629171   0.34939575  0.7679405   0.861801  ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 5891. State = [[-0.01842729 -0.09122387  0.39562115  1.        ]]. Action = [[ 0.9076929  0.5022886 -0.6651295  0.9507301]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 5892. State = [[ 0.00585161 -0.0852046   0.37733972  1.        ]]. Action = [[0.855114   0.22375417 0.6728209  0.6091602 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 5893. State = [[ 0.02164892 -0.07800522  0.38180605  1.        ]]. Action = [[0.74562025 0.41319227 0.5382414  0.8172957 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 5894. State = [[ 0.04419676 -0.06461823  0.39340752  1.        ]]. Action = [[0.65550494 0.44944143 0.33011115 0.6727103 ]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Current timestep = 5895. State = [[ 0.06461582 -0.05498116  0.40144876  1.        ]]. Action = [[ 0.92725074 -0.6915218   0.64691734  0.5718651 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Above hoop
Current timestep = 5896. State = [[ 0.07140909 -0.05233796  0.4060995   1.        ]]. Action = [[0.7685623  0.23379004 0.9757186  0.8246782 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Above hoop
Current timestep = 5897. State = [[ 0.07146104 -0.05218627  0.40628105  1.        ]]. Action = [[0.9577602 0.657959  0.8095298 0.8121054]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Above hoop
Current timestep = 5898. State = [[ 0.07174895 -0.05207654  0.40590113  1.        ]]. Action = [[0.494357   0.18819547 0.6708077  0.8488476 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Above hoop
Current timestep = 5899. State = [[ 0.0718584  -0.05198266  0.4051487   1.        ]]. Action = [[-0.4634409  -0.2741292   0.26095033  0.8876023 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Above hoop
Current timestep = 5900. State = [[ 0.07194825 -0.05193138  0.4045799   1.        ]]. Action = [[0.2175417  0.18299496 0.6459358  0.9241984 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Above hoop
Current timestep = 5901. State = [[ 0.07212873 -0.05180329  0.40419382  1.        ]]. Action = [[0.7791015  0.42595458 0.48848665 0.73257184]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Above hoop
Current timestep = 5902. State = [[ 0.07235679 -0.05165105  0.40408233  1.        ]]. Action = [[ 0.59691954 -0.09311074  0.94232774  0.7371379 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Above hoop
Current timestep = 5903. State = [[ 0.07252072 -0.05169954  0.4039523   1.        ]]. Action = [[-0.16702902  0.68042696  0.56117296  0.9589517 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Above hoop
Current timestep = 5904. State = [[ 0.07252072 -0.05169954  0.4039523   1.        ]]. Action = [[-0.16553485  0.21619534  0.8041495   0.7234764 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Above hoop
Current timestep = 5905. State = [[ 0.07252072 -0.05169954  0.4039523   1.        ]]. Action = [[0.3743943  0.4594915  0.9865327  0.93026567]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Above hoop
Current timestep = 5906. State = [[ 0.07252072 -0.05169954  0.4039523   1.        ]]. Action = [[0.74674666 0.1099323  0.80171824 0.80436385]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Above hoop
Current timestep = 5907. State = [[ 0.07252072 -0.05169954  0.4039523   1.        ]]. Action = [[0.7189429  0.14323604 0.8888103  0.94956994]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Above hoop
Current timestep = 5908. State = [[ 0.07252072 -0.05169954  0.4039523   1.        ]]. Action = [[0.4125842 0.5177939 0.3362925 0.8731543]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Above hoop
Current timestep = 5909. State = [[ 0.07252809 -0.05169934  0.40388098  1.        ]]. Action = [[0.66717386 0.3375938  0.959975   0.8255248 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Above hoop
Current timestep = 5910. State = [[ 0.07252809 -0.05169934  0.40388098  1.        ]]. Action = [[-0.74940455  0.36550272  0.81347084  0.7681663 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Above hoop
Current timestep = 5911. State = [[ 0.07252809 -0.05169934  0.40388098  1.        ]]. Action = [[-0.70850563 -0.69663066  0.6335492   0.6355816 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Above hoop
Current timestep = 5912. State = [[ 0.07252809 -0.05169934  0.40388098  1.        ]]. Action = [[-0.07600802 -0.28076202  0.25033212  0.82803416]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Above hoop
Current timestep = 5913. State = [[ 0.07252809 -0.05169934  0.40388098  1.        ]]. Action = [[ 0.80690956 -0.12361014  0.7765858   0.7139584 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Above hoop
Current timestep = 5914. State = [[ 0.07482668 -0.05080559  0.4032553   1.        ]]. Action = [[0.3079338  0.11786354 0.0132072  0.66077495]]. Reward = [0.]
Curr episode timestep = 67
Above hoop
Current timestep = 5915. State = [[ 0.08348001 -0.04761009  0.4057216   1.        ]]. Action = [[0.9593437  0.12314439 0.81917644 0.6967621 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 5916. State = [[ 0.08443674 -0.04738338  0.40546668  1.        ]]. Action = [[0.75044084 0.9630903  0.9703305  0.8020675 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 5917. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[ 0.6021857  -0.5864648   0.85469246  0.81426716]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 5918. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[0.89219785 0.12565362 0.3978758  0.82335484]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 5919. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[0.7433295  0.40146697 0.85121655 0.86295843]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 5920. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[ 0.977721   -0.05630308  0.8870709   0.7563467 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 5921. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[0.33696234 0.87279344 0.49502206 0.90047765]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 5922. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[0.9116602  0.27410078 0.74275434 0.9221101 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 5923. State = [[ 0.08444426 -0.04738317  0.40539414  1.        ]]. Action = [[0.92155623 0.3709389  0.58423924 0.831738  ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 5924. State = [[ 0.08519202 -0.03956673  0.40661582  1.        ]]. Action = [[0.22558737 0.5114509  0.12521207 0.886312  ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 5925. State = [[ 0.09194582 -0.03104291  0.4088018   1.        ]]. Action = [[0.28165877 0.13504875 0.8628185  0.78840566]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 5926. State = [[ 0.09584174 -0.02850104  0.41124544  1.        ]]. Action = [[ 0.06413972 -0.20978397  0.21454978  0.82472324]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 5927. State = [[ 0.0960873  -0.02838712  0.4107905   1.        ]]. Action = [[0.6159079  0.6570755  0.7611878  0.87832856]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 5928. State = [[ 0.09599362 -0.02837283  0.4106361   1.        ]]. Action = [[0.5654087 0.0587064 0.3149544 0.810783 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 5929. State = [[ 0.09597316 -0.02834206  0.4106615   1.        ]]. Action = [[ 0.6549995 -0.2784884  0.7365469  0.8772402]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 5930. State = [[ 0.09588362 -0.0283592   0.41063032  1.        ]]. Action = [[0.24037778 0.13804042 0.4020989  0.8020301 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 5931. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[0.57368004 0.19536424 0.09032404 0.7942302 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 5932. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[0.61607146 0.6016973  0.9096571  0.69593453]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 5933. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[0.6785364  0.03557396 0.6135069  0.80849314]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 5934. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[0.09551346 0.6196307  0.98234034 0.9467542 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 5935. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[ 0.5334568   0.8470429  -0.21063048  0.7344414 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 5936. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[-0.6589098   0.80464196  0.89982057  0.885978  ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 5937. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[0.5883167 0.4188671 0.92505   0.7365514]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 5938. State = [[ 0.09589882 -0.02833149  0.41058257  1.        ]]. Action = [[ 0.9115262   0.7647095  -0.658893    0.91957664]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 5939. State = [[ 0.09590686 -0.02833116  0.41050866  1.        ]]. Action = [[-0.36673057  0.5091772   0.60740757  0.8527937 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 5940. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[ 0.50714445  0.4696331  -0.04677892  0.78722465]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 5941. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[ 0.9195595   0.63763905 -0.5810901   0.8591833 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 5942. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[0.69848657 0.4218328  0.7379446  0.81032467]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 5943. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[ 0.66264343 -0.24565423  0.28117454  0.8804176 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 5944. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[0.88909984 0.44026816 0.0068152  0.806     ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 5945. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[0.41065657 0.83712244 0.68344176 0.88625073]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 5946. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[-0.05804807  0.44235396  0.7364006   0.58370495]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 5947. State = [[ 0.09591489 -0.02833082  0.41043475  1.        ]]. Action = [[0.13217747 0.90236616 0.8055929  0.9251578 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 5948. State = [[-0.26271898 -0.01991531  0.11381618  1.        ]]. Action = [[0.6962831  0.32534206 0.9994395  0.9423264 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 5949. State = [[-0.25296557 -0.0385304   0.10900567  1.        ]]. Action = [[ 0.64468026 -0.9065022   0.96557045  0.14692044]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 5950. State = [[-0.23815525 -0.0710137   0.13135608  1.        ]]. Action = [[ 0.37424183 -0.8955788   0.98225427  0.18464756]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 5951. State = [[-0.22001843 -0.10557822  0.16788311  1.        ]]. Action = [[ 0.48135638 -0.97428703  0.9576293   0.14325833]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 5952. State = [[-0.19849467 -0.14204946  0.20726494  1.        ]]. Action = [[ 0.46699882 -0.8007446   0.93434596  0.28870893]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 5953. State = [[-0.18136536 -0.16656025  0.24220479  1.        ]]. Action = [[ 0.2812786  -0.2891767   0.72141695  0.63779616]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5954. State = [[-0.17108838 -0.17766534  0.26016435  1.        ]]. Action = [[ 0.6852398   0.75954986 -0.23153102  0.7840233 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 5955. State = [[-0.17014107 -0.17966577  0.2626509   1.        ]]. Action = [[ 0.91410923  0.54211783 -0.5582044   0.8320372 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 5956. State = [[-0.17037097 -0.18001343  0.26274943  1.        ]]. Action = [[ 0.8226783   0.5458571  -0.62243026  0.6789744 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 5957. State = [[-0.17049621 -0.17986728  0.26282623  1.        ]]. Action = [[ 0.35075366  0.5527644  -0.69223404  0.91840494]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 5958. State = [[-0.17738217 -0.1836733   0.2594598   1.        ]]. Action = [[-0.64230734 -0.21479106 -0.61607915  0.93634593]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 5959. State = [[-0.17534447 -0.17815581  0.25235504  1.        ]]. Action = [[ 0.8813571   0.5898119  -0.29332095  0.8440659 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 5960. State = [[-0.15804815 -0.16587842  0.24885303  1.        ]]. Action = [[0.71821487 0.20995617 0.19539094 0.8705182 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 5961. State = [[-0.13818952 -0.16706105  0.24586406  1.        ]]. Action = [[ 0.37202144 -0.43898857 -0.23380286  0.75338066]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 5962. State = [[-0.12631753 -0.17086656  0.23998833  1.        ]]. Action = [[ 0.7018192   0.793314   -0.67580515  0.8016074 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 5963. State = [[-0.12457494 -0.17132029  0.24069141  1.        ]]. Action = [[-0.02676117  0.4931723  -0.9707918   0.5893154 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 5964. State = [[-0.12451528 -0.17133304  0.24073137  1.        ]]. Action = [[ 0.6971351  0.2889248 -0.7123503  0.7836168]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 5965. State = [[-0.12451528 -0.17133304  0.24073137  1.        ]]. Action = [[ 0.70074594  0.66629434 -0.2276296   0.8420428 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 5966. State = [[-0.11604473 -0.17078814  0.24395518  1.        ]]. Action = [[0.708277   0.08246171 0.1693703  0.83834314]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 5967. State = [[-0.09194111 -0.17617087  0.24897352  1.        ]]. Action = [[ 0.8128259  -0.48505116  0.1630075   0.80153656]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 5968. State = [[-0.07067847 -0.18302074  0.25087413  1.        ]]. Action = [[ 0.96708155  0.7957835  -0.42152393  0.8736234 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 5969. State = [[-0.06736948 -0.18370134  0.25312808  1.        ]]. Action = [[ 0.12862039  0.7571633  -0.7539477   0.99143314]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 5970. State = [[-0.06718866 -0.18422367  0.25314116  1.        ]]. Action = [[ 0.9205725  0.8272097 -0.8777456  0.8806565]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 5971. State = [[-0.06718866 -0.18422367  0.25314116  1.        ]]. Action = [[ 0.6766989   0.911468   -0.47058034  0.96105933]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 5972. State = [[-0.06275592 -0.17421474  0.25421476  1.        ]]. Action = [[0.2908957  0.77585316 0.07466459 0.84756196]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 5973. State = [[-0.05609974 -0.16573188  0.25721604  1.        ]]. Action = [[ 0.79524183  0.90344703 -0.9665518   0.8900244 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 5974. State = [[-0.05433388 -0.16290681  0.25717175  1.        ]]. Action = [[ 0.56022406 -0.02963948 -0.72660667  0.82518744]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 5975. State = [[-0.04376204 -0.1471158   0.25527015  1.        ]]. Action = [[ 0.96306884  0.9208448  -0.11985362  0.97596025]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 5976. State = [[-0.02923577 -0.13103789  0.2566683   1.        ]]. Action = [[-0.79173356  0.06870139  0.5167148   0.9061115 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 5977. State = [[-0.0326607  -0.1262384   0.26124218  1.        ]]. Action = [[ 0.9822104   0.70338154 -0.9534894   0.90890384]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 5978. State = [[-0.03445687 -0.12608354  0.26222804  1.        ]]. Action = [[ 0.661857   -0.01478326 -0.9804582   0.75077105]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 5979. State = [[-0.02818762 -0.12637347  0.2664673   1.        ]]. Action = [[ 0.8479638  -0.09172541  0.08760655  0.71347845]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 5980. State = [[-0.00807344 -0.1200064   0.2831884   1.        ]]. Action = [[0.76542854 0.3934219  0.5678394  0.9442966 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 5981. State = [[ 0.01903176 -0.10978324  0.29351285  1.        ]]. Action = [[ 0.9702606   0.22509968 -0.16641337  0.36563337]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 5982. State = [[ 0.05002219 -0.09631895  0.28236672  1.        ]]. Action = [[ 0.80212617  0.440902   -0.6483692   0.95284486]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 5983. State = [[ 0.0787869  -0.07462002  0.2538754   1.        ]]. Action = [[ 0.69774246  0.7854948  -0.9727225   0.8788073 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 5984. State = [[ 0.10474502 -0.0597754   0.22626267  1.        ]]. Action = [[0.5547384  0.5251291  0.84577703 0.87946224]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 5985. State = [[ 0.11654466 -0.05680683  0.22431383  1.        ]]. Action = [[ 0.9859369  -0.04096484  0.36850214  0.84784555]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 5986. State = [[ 0.11812636 -0.05625885  0.2240103   1.        ]]. Action = [[ 0.7366996  -0.07594401  0.5609962   0.457507  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 5987. State = [[ 0.11813148 -0.05624804  0.22358838  1.        ]]. Action = [[-0.44678038  0.9566264  -0.11149848  0.8108871 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 5988. State = [[ 0.11806974 -0.05630633  0.22322018  1.        ]]. Action = [[ 0.61911345  0.29297054 -0.40869403  0.91996014]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5989. State = [[ 0.11806974 -0.05630633  0.22322018  1.        ]]. Action = [[0.9564252  0.29721928 0.2548417  0.9087174 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5990. State = [[ 0.11806974 -0.05630633  0.22322018  1.        ]]. Action = [[ 0.11355186  0.72532344 -0.28208566  0.8538437 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5991. State = [[ 0.11806974 -0.05630633  0.22322018  1.        ]]. Action = [[0.10883236 0.7566714  0.02928579 0.93484664]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5992. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.79869986  0.25938594 -0.9194703   0.86033356]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5993. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[-0.71354544  0.87402606 -0.87244684  0.83918417]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 5994. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[0.85988975 0.7281561  0.66220605 0.87917125]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 5995. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.14119673  0.9006419  -0.47579205  0.8473425 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5996. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.2937863  0.7533252 -0.2786286  0.9215641]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5997. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.8348596   0.94961894 -0.8505721   0.8786768 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5998. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.8945993  -0.51764226 -0.8577211   0.8369535 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 5999. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[0.8632101  0.13152778 0.52418005 0.7224853 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 6000. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.11643386 -0.8114798  -0.971249    0.65564847]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6001. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.2502663  0.9383272 -0.5450875  0.536046 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6002. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.74385905 -0.17859757 -0.70064896  0.84279764]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6003. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.5019584   0.5151285  -0.9959751   0.92237234]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6004. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[-0.07646459  0.4384508  -0.72319144  0.94422126]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6005. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.77577865  0.8889897  -0.84523964  0.8651632 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6006. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[0.77873874 0.7752621  0.19081843 0.91138506]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6007. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[0.6824963  0.70038974 0.13879931 0.7804564 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6008. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.9235122   0.7340443  -0.49924266  0.91078687]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6009. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.95624554  0.5253835  -0.9374257   0.9019494 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6010. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.3705244   0.02207398 -0.46042317  0.93675625]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6011. State = [[ 0.11805847 -0.05630617  0.22314398  1.        ]]. Action = [[ 0.2736559   0.95011616 -0.8491839   0.8628299 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6012. State = [[ 0.11803611 -0.05630584  0.222993    1.        ]]. Action = [[ 0.41940808  0.4337163  -0.7769547   0.83234787]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6013. State = [[ 0.11803611 -0.05630584  0.222993    1.        ]]. Action = [[ 0.7499474   0.81516564 -0.7085782   0.95052385]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6014. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.9715214   0.96494436 -0.24324167  0.86148596]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6015. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.9124572  0.6746974  0.6531539  0.90587354]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6016. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[-0.15241516  0.7200606  -0.6696716   0.9684472 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6017. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.7366214   0.72324944 -0.6357229   0.93841124]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6018. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.17048967  0.25639904 -0.9752812   0.9074888 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6019. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.36666787 0.94236684 0.07665515 0.8934221 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6020. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.11638737  0.18940592 -0.8292436   0.89984167]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6021. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.9697859  0.4888153  0.29327202 0.92567205]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6022. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.9298773  0.85068464 0.37074053 0.93531775]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6023. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.3331442   0.93497133 -0.6612667   0.9366107 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6024. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.95885575  0.28029013 -0.8279715   0.8786051 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6025. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[-0.63343924  0.6292951  -0.08040428  0.920169  ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 6026. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.810554    0.98695517 -0.83986485  0.77139044]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6027. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.8418615   0.95952106 -0.94300115  0.9383838 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6028. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.8131151   0.86178064 -0.2652158   0.86333036]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6029. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.9171232  0.63606405 0.5892353  0.7394335 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6030. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.83369946 0.907356   0.04298818 0.69772565]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6031. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.6905997   0.6975179  -0.96472967  0.914927  ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6032. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.73173654 0.5266447  0.7572156  0.87316895]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6033. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.22894347  0.7201197  -0.88539624  0.9359641 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6034. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.74214697  0.8647059  -0.9696175   0.8626895 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6035. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[-0.40828073  0.7909119  -0.84871835  0.86765623]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 6036. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.7968328   0.13119268 -0.49403727  0.77232075]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6037. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.21236861 0.63596034 0.21059644 0.9483106 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6038. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.9008392  -0.05773127  0.59954166  0.8708675 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6039. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.90647864  0.52619743 -0.9809093   0.8120358 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6040. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[0.72175    0.7222848  0.36711705 0.65473676]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6041. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.06032205  0.7959454  -0.7928044   0.79303074]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6042. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.8272095   0.51714194 -0.6673985   0.8864062 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6043. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.70644784  0.30162597 -0.60473925  0.7321665 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6044. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.29602635  0.8494723  -0.587672    0.7982085 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6045. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.7016132   0.10273647 -0.7044059   0.8541992 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6046. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.8754275   0.43681204 -0.9417226   0.83263195]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6047. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.96144605  0.88764405 -0.04957366  0.74818397]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6048. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.7208928  0.8761492 -0.9801906  0.797006 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6049. State = [[ 0.11802482 -0.05630568  0.2229168   1.        ]]. Action = [[ 0.5732826   0.8070457  -0.95118773  0.8276422 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6050. State = [[-0.2631843   0.00250105  0.11312199  1.        ]]. Action = [[ 0.25466204 -0.35166228 -0.7796064   0.92129326]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6051. State = [[-0.25701022 -0.01212221  0.10632167  1.        ]]. Action = [[ 0.5046586  -0.9767081   0.8812871   0.16260874]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6052. State = [[-0.24321766 -0.04273802  0.12673514  1.        ]]. Action = [[ 0.5501263  -0.8478191   0.95634675  0.19144201]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6053. State = [[-0.22090682 -0.07422066  0.16349722  1.        ]]. Action = [[ 0.4971547  -0.78714466  0.986259    0.29857683]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6054. State = [[-0.20033543 -0.10444327  0.19756736  1.        ]]. Action = [[ 0.43717408 -0.79734594  0.6170964   0.2562828 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6055. State = [[-0.18777338 -0.12256896  0.21656685  1.        ]]. Action = [[0.66312337 0.41627264 0.70432603 0.43163168]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 6056. State = [[-0.18727525 -0.12013501  0.22959827  1.        ]]. Action = [[-0.22941065  0.49566388  0.7721255   0.5989032 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6057. State = [[-0.17976171 -0.11777841  0.25196025  1.        ]]. Action = [[ 0.7841865  -0.19397545  0.46054912  0.78289056]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6058. State = [[-0.15529676 -0.12049895  0.280003    1.        ]]. Action = [[ 0.6111462  -0.08923852  0.97028387  0.7135184 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6059. State = [[-0.12984858 -0.11142158  0.30716944  1.        ]]. Action = [[0.6340009  0.73302794 0.12804449 0.44441724]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6060. State = [[-0.10689896 -0.08745959  0.3297579   1.        ]]. Action = [[0.75632954 0.79815626 0.9261544  0.81903696]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6061. State = [[-0.08315501 -0.06144733  0.3621603   1.        ]]. Action = [[0.50267076 0.5410913  0.6536956  0.9238988 ]]. Reward = [0.]
Curr episode timestep = 10
Above hoop
Current timestep = 6062. State = [[-0.06836142 -0.04856043  0.3757906   1.        ]]. Action = [[ 0.04389226  0.0301069  -0.23559844  0.8656635 ]]. Reward = [0.]
Curr episode timestep = 11
Above hoop
Current timestep = 6063. State = [[-0.06470487 -0.04642713  0.3765818   1.        ]]. Action = [[-0.13797933  0.5058439   0.8609339   0.7901499 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Above hoop
Current timestep = 6064. State = [[-0.05806754 -0.04602949  0.36936793  1.        ]]. Action = [[ 0.8125601  -0.10509533 -0.48118472  0.72440314]]. Reward = [0.]
Curr episode timestep = 13
Above hoop
Current timestep = 6065. State = [[-0.02961885 -0.04264944  0.35632795  1.        ]]. Action = [[ 0.7536068   0.24557376 -0.1490137   0.88129306]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Current timestep = 6066. State = [[-0.00891577 -0.03429719  0.34427458  1.        ]]. Action = [[ 0.45046103  0.29252017 -0.6984206   0.825737  ]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Current timestep = 6067. State = [[ 0.01024981 -0.01334984  0.31913698  1.        ]]. Action = [[ 0.59346414  0.93564904 -0.79788333  0.8890331 ]]. Reward = [0.]
Curr episode timestep = 16
Above hoop
Current timestep = 6068. State = [[0.03329474 0.00375421 0.28925297 1.        ]]. Action = [[ 0.73095894 -0.06455034 -0.8023226   0.81162405]]. Reward = [0.]
Curr episode timestep = 17
Above hoop
Current timestep = 6069. State = [[0.05822072 0.01596909 0.26161155 1.        ]]. Action = [[-0.18854678  0.5371673  -0.15433669  0.8205793 ]]. Reward = [0.]
Curr episode timestep = 18
Above hoop
Current timestep = 6070. State = [[0.06846572 0.02676853 0.26263723 1.        ]]. Action = [[0.44323397 0.0131067  0.482347   0.58097565]]. Reward = [0.]
Curr episode timestep = 19
Above hoop
Current timestep = 6071. State = [[0.07483204 0.02907459 0.27267078 1.        ]]. Action = [[ 0.817348    0.28788555 -0.66762465  0.7808387 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Action ignored: No entry zone
Above hoop
Current timestep = 6072. State = [[0.07495842 0.02910672 0.27265838 1.        ]]. Action = [[ 0.88363373 -0.09773648 -0.36434048  0.23621035]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Above hoop
Current timestep = 6073. State = [[0.07495235 0.02910622 0.27258587 1.        ]]. Action = [[ 0.8573904  -0.33098114  0.8931613   0.6494132 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Above hoop
Current timestep = 6074. State = [[0.08103109 0.02739573 0.28444943 1.        ]]. Action = [[ 0.40521634 -0.1593889   0.77653146  0.58644104]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 6075. State = [[0.09093627 0.02690907 0.30015233 1.        ]]. Action = [[ 0.39652538 -0.6261171   0.18496811  0.60191965]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 6076. State = [[0.09323864 0.02587084 0.3039213  1.        ]]. Action = [[ 0.73707306  0.70559263 -0.15967607  0.50500345]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 6077. State = [[0.09319185 0.02511066 0.30324572 1.        ]]. Action = [[ 0.8249006  0.6970377 -0.7390164  0.7497878]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6078. State = [[0.09293745 0.03276279 0.31302425 1.        ]]. Action = [[-0.3156399   0.4488821   0.68971956  0.7736224 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 6079. State = [[0.09321043 0.04082324 0.32727587 1.        ]]. Action = [[ 0.810045   -0.74270797 -0.254516    0.8726995 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 6080. State = [[0.09360405 0.04149628 0.32878327 1.        ]]. Action = [[ 0.49900424  0.88148355 -0.4024452   0.7652941 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6081. State = [[0.09342679 0.04198411 0.32876512 1.        ]]. Action = [[0.54788494 0.43409228 0.49524748 0.66752267]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6082. State = [[0.09342679 0.04198411 0.32876512 1.        ]]. Action = [[ 0.8917105  -0.51474005  0.7498007   0.7389102 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 6083. State = [[0.09218389 0.03991786 0.33544335 1.        ]]. Action = [[-0.23196101 -0.09041154  0.4843533   0.78564477]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 6084. State = [[0.09071322 0.03902549 0.3446846  1.        ]]. Action = [[ 0.48056412 -0.64290756 -0.06709611  0.81858253]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 6085. State = [[0.09182018 0.0395987  0.3483066  1.        ]]. Action = [[0.74065375 0.51538014 0.53360724 0.63015056]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 6086. State = [[0.0918     0.03968069 0.3483066  1.        ]]. Action = [[0.73483276 0.692521   0.6656239  0.65409327]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 6087. State = [[0.0918     0.03968069 0.3483066  1.        ]]. Action = [[ 0.9564409  -0.01695609  0.9315851   0.69147396]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 6088. State = [[0.09136629 0.048282   0.35879186 1.        ]]. Action = [[0.08898175 0.49480295 0.7044816  0.5278306 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 6089. State = [[0.09020212 0.05687328 0.37416467 1.        ]]. Action = [[ 0.20880067 -0.4396876   0.72750854  0.4398036 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 6090. State = [[0.09017142 0.05324535 0.3707508  1.        ]]. Action = [[-0.1011622 -0.4180932 -0.7573959  0.6996832]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 6091. State = [[0.0908704  0.04643001 0.36584765 1.        ]]. Action = [[-0.0115062  -0.13896972  0.46901846  0.5695723 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 6092. State = [[0.09125419 0.04228249 0.37086877 1.        ]]. Action = [[ 0.4725176  -0.32803786 -0.48196554  0.6745243 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6093. State = [[0.09140576 0.04161041 0.37212434 1.        ]]. Action = [[ 0.9751861  -0.54723966 -0.08371413  0.6745746 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 6094. State = [[0.09016941 0.04258637 0.3817639  1.        ]]. Action = [[-0.15309238  0.15376353  0.63257706  0.7335931 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 6095. State = [[0.08596549 0.03811067 0.39855042 1.        ]]. Action = [[-0.50563765 -0.39437187  0.30338705  0.6518866 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 6096. State = [[0.08108592 0.03347505 0.41217938 1.        ]]. Action = [[ 0.53784275  0.36409664 -0.5383533   0.8223355 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 6097. State = [[0.07981171 0.03179999 0.41515058 1.        ]]. Action = [[ 0.45374107  0.9173554  -0.73270494  0.86276376]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 6098. State = [[0.07905211 0.03170454 0.4158568  1.        ]]. Action = [[ 0.9404099  -0.42567688  0.65058756  0.8964915 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Above hoop
Current timestep = 6099. State = [[0.07905211 0.03170454 0.4158568  1.        ]]. Action = [[ 0.7865181   0.85577273 -0.5577205   0.8536322 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Above hoop
Current timestep = 6100. State = [[0.07904343 0.03155682 0.41590488 1.        ]]. Action = [[0.4740541  0.39798617 0.29075515 0.6459966 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Above hoop
Current timestep = 6101. State = [[0.07904343 0.03155682 0.41590488 1.        ]]. Action = [[0.6662061  0.38134587 0.60361385 0.7500316 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Above hoop
Current timestep = 6102. State = [[0.07904343 0.03155682 0.41590488 1.        ]]. Action = [[0.7763951  0.34531415 0.16851997 0.6446829 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Above hoop
Current timestep = 6103. State = [[0.07904343 0.03155682 0.41590488 1.        ]]. Action = [[-0.17905104  0.01827192  0.40967417  0.6295358 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Above hoop
Current timestep = 6104. State = [[0.07904343 0.03155682 0.41590488 1.        ]]. Action = [[0.20958745 0.06811202 0.08063138 0.85634446]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Above hoop
Current timestep = 6105. State = [[0.07748468 0.0450751  0.4064252  1.        ]]. Action = [[ 0.23265505  0.91361046 -0.68287575  0.6200737 ]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Current timestep = 6106. State = [[0.07506951 0.05866685 0.3858637  1.        ]]. Action = [[-0.14378071 -0.3051406  -0.6770686   0.77529216]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Current timestep = 6107. State = [[0.07635677 0.05804619 0.37447587 1.        ]]. Action = [[ 0.93282187 -0.6962886   0.11995935  0.7238406 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Above hoop
Current timestep = 6108. State = [[0.07618915 0.05789671 0.37338674 1.        ]]. Action = [[0.8099172  0.0134325  0.43158424 0.7055298 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Above hoop
Current timestep = 6109. State = [[0.07470258 0.06379987 0.3738239  1.        ]]. Action = [[0.00062656 0.38145483 0.32573628 0.55837655]]. Reward = [0.]
Curr episode timestep = 58
Above hoop
Current timestep = 6110. State = [[0.07357588 0.07054225 0.36663193 1.        ]]. Action = [[ 0.22932601  0.16570735 -0.76597196  0.7479241 ]]. Reward = [0.]
Curr episode timestep = 59
Above hoop
Current timestep = 6111. State = [[0.0762296  0.07339972 0.35199735 1.        ]]. Action = [[ 0.5190761  -0.11184108  0.7700529   0.589569  ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Above hoop
Current timestep = 6112. State = [[0.07630152 0.07404397 0.35042676 1.        ]]. Action = [[ 0.7408109 -0.7197699  0.887936   0.2708416]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 6113. State = [[0.07633363 0.07413433 0.34997758 1.        ]]. Action = [[ 0.6001679 -0.5851361 -0.5501967  0.5285946]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 6114. State = [[0.07575747 0.08185793 0.349058   1.        ]]. Action = [[0.24921119 0.5095606  0.14247394 0.66177416]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 6115. State = [[0.07513581 0.10334608 0.35088572 1.        ]]. Action = [[-0.2302078   0.7528708   0.26733685  0.6137159 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 6116. State = [[0.06969462 0.12042864 0.3521613  1.        ]]. Action = [[ 0.701071   -0.2358383   0.37889898  0.37926137]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 6117. State = [[0.06904465 0.122279   0.35218734 1.        ]]. Action = [[ 0.7143986  -0.09450674  0.86231136  0.5191655 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6118. State = [[0.07296115 0.10945661 0.3546797  1.        ]]. Action = [[ 0.57609236 -0.8927168   0.12526369  0.5343691 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 6119. State = [[0.08354679 0.08398151 0.3652313  1.        ]]. Action = [[ 0.20472836 -0.66158     0.6620747   0.48721337]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 6120. State = [[0.09084394 0.07125066 0.37664166 1.        ]]. Action = [[ 0.33808982 -0.1296475   0.18399441  0.6643238 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 6121. State = [[0.09117804 0.06877106 0.37762287 1.        ]]. Action = [[0.7706741  0.21872234 0.02557242 0.37672627]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 6122. State = [[0.09095848 0.06831986 0.3777828  1.        ]]. Action = [[ 0.40685773 -0.6108864  -0.1305998   0.33273697]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6123. State = [[0.09085482 0.06802668 0.37839785 1.        ]]. Action = [[0.43237042 0.6111703  0.11100268 0.28071237]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6124. State = [[0.09115331 0.06715448 0.3796576  1.        ]]. Action = [[ 0.4223957  -0.07113737  0.5335823   0.5888529 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6125. State = [[0.09116446 0.06709156 0.37964192 1.        ]]. Action = [[ 0.492198   -0.25561643  0.74099505  0.5693457 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6126. State = [[0.09125133 0.06704067 0.38006547 1.        ]]. Action = [[ 0.7416191   0.6789924  -0.22721475  0.57703376]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 6127. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.9146898  -0.5990896   0.15492249  0.68770385]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6128. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.6373681   0.0895952  -0.6719072   0.56458235]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6129. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.4957478   0.57745516 -0.9273647   0.64347446]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6130. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.69639814 -0.5255007  -0.480797    0.73349273]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6131. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.64394784 -0.11454576  0.42794073  0.6199217 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6132. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.38238883  0.0267874  -0.08520067  0.58496475]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 6133. State = [[0.09130774 0.06687006 0.38045976 1.        ]]. Action = [[ 0.8563633   0.01314259 -0.06890321  0.48099363]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6134. State = [[0.09002876 0.0640207  0.38760373 1.        ]]. Action = [[-0.34984076 -0.17755705  0.49830735  0.71437883]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 6135. State = [[0.089115   0.05892505 0.39441976 1.        ]]. Action = [[ 0.35040128  0.22277737 -0.20441955  0.619118  ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6136. State = [[0.08918943 0.05886519 0.39530626 1.        ]]. Action = [[ 0.5612823   0.10502553 -0.14964771  0.770314  ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6137. State = [[0.08920126 0.05887665 0.39537728 1.        ]]. Action = [[ 0.31109023  0.43563986 -0.42929876  0.6946949 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 6138. State = [[0.08920024 0.05882927 0.39539665 1.        ]]. Action = [[ 0.5625074  -0.47496295  0.37351477  0.69622624]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6139. State = [[0.08919831 0.05874026 0.3954331  1.        ]]. Action = [[ 0.42491674  0.87410545 -0.21148592  0.56696033]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6140. State = [[0.08919831 0.05874026 0.3954331  1.        ]]. Action = [[ 0.64002585 -0.3627318  -0.8974152   0.73083997]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6141. State = [[0.08919831 0.05874026 0.3954331  1.        ]]. Action = [[ 0.488253    0.11025107 -0.42136854  0.81389713]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6142. State = [[0.08919439 0.05856167 0.39550632 1.        ]]. Action = [[0.34575546 0.3993852  0.26486075 0.78522086]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6143. State = [[0.08919042 0.05838341 0.39557952 1.        ]]. Action = [[ 0.6237037   0.29499614 -0.1496638   0.6176666 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 6144. State = [[0.08919042 0.05838341 0.39557952 1.        ]]. Action = [[ 0.90961933  0.76175857 -0.57814103  0.78028464]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6145. State = [[0.08776772 0.06507343 0.39113668 1.        ]]. Action = [[ 0.01856136  0.40770698 -0.48332644  0.6602776 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 6146. State = [[0.08660555 0.06986108 0.38891333 1.        ]]. Action = [[0.7079239  0.50547385 0.7814121  0.80015635]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 6147. State = [[0.08642621 0.07053704 0.38893962 1.        ]]. Action = [[ 0.5060848  -0.62642574  0.3034191   0.6882986 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6148. State = [[0.08633331 0.07086043 0.3889396  1.        ]]. Action = [[ 0.8439262   0.10016024 -0.1587286   0.6153203 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6149. State = [[0.08633331 0.07086043 0.3889396  1.        ]]. Action = [[ 0.34129095 -0.31768775 -0.0695461   0.7479209 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6150. State = [[0.08633331 0.07086043 0.3889396  1.        ]]. Action = [[ 0.91087115 -0.27104354  0.10343206  0.6862494 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 6151. State = [[0.08633331 0.07086043 0.3889396  1.        ]]. Action = [[ 0.6094134  -0.29761863  0.13395083  0.56743   ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 6152. State = [[-0.2619915  -0.06547169  0.10782117  1.        ]]. Action = [[ 0.20806909  0.23161876 -0.22272158  0.63031983]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 6153. State = [[-0.25686666 -0.08901288  0.10208318  1.        ]]. Action = [[ 0.4266641 -0.9476326  0.9909791  0.052351 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6154. State = [[-0.25107303 -0.12361757  0.1265623   1.        ]]. Action = [[ 0.07334745 -0.9307107   0.98997855  0.18830776]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6155. State = [[-0.24044879 -0.1579432   0.1605254   1.        ]]. Action = [[ 0.51426387 -0.84931487  0.8132405   0.2196163 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6156. State = [[-0.21790646 -0.1938164   0.18891704  1.        ]]. Action = [[ 0.6284698  -0.940199    0.45927334  0.45474613]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6157. State = [[-0.19115733 -0.2205782   0.20672001  1.        ]]. Action = [[ 0.8342476  -0.36630428 -0.0563978   0.8004142 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 6158. State = [[-0.17527384 -0.23026188  0.21314722  1.        ]]. Action = [[ 0.43767703  0.9900172  -0.02364641  0.8575797 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 6159. State = [[-0.1615636  -0.2403611   0.21616392  1.        ]]. Action = [[ 0.9877994  -0.6375429   0.04550529  0.77415395]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6160. State = [[-0.125243   -0.24445379  0.22589977  1.        ]]. Action = [[0.94349504 0.7568729  0.7345114  0.86132383]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6161. State = [[-0.09461266 -0.22238752  0.23389319  1.        ]]. Action = [[ 0.8886366   0.85809135 -0.96249413  0.8527372 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6162. State = [[-0.05604902 -0.1916336   0.20859186  1.        ]]. Action = [[ 0.9732456   0.8853023  -0.3087449   0.96415055]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6163. State = [[-0.03148387 -0.17300124  0.19844668  1.        ]]. Action = [[ 0.9702866   0.62406564 -0.95598185  0.98745966]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 6164. State = [[-0.02562576 -0.17027648  0.19731465  1.        ]]. Action = [[0.97250986 0.977237   0.17777574 0.9520006 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 6165. State = [[-0.0232045  -0.16975062  0.1984944   1.        ]]. Action = [[ 0.63115263  0.9913628  -0.29050112  0.97253776]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 6166. State = [[-0.02314644 -0.16976213  0.19853741  1.        ]]. Action = [[ 0.7766204  0.8924546 -0.9983402  0.9351591]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 6167. State = [[-0.02317348 -0.1697363   0.19843532  1.        ]]. Action = [[ 0.8831036  0.644127  -0.9342598  0.9682157]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 6168. State = [[-0.02317348 -0.1697363   0.19843532  1.        ]]. Action = [[ 0.9195397   0.27278197 -0.00819832  0.7496152 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 6169. State = [[-0.02317348 -0.1697363   0.19843532  1.        ]]. Action = [[ 0.8694849   0.94349873 -0.6289441   0.8999499 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 6170. State = [[-0.0231992  -0.16972591  0.19843368  1.        ]]. Action = [[ 0.913759    0.8860059  -0.95446736  0.858058  ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 6171. State = [[-0.02281225 -0.17465767  0.19370805  1.        ]]. Action = [[ 0.59471655 -0.519027   -0.98762584  0.8324392 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 6172. State = [[-0.00629776 -0.18301468  0.16671532  1.        ]]. Action = [[ 0.9061538   0.9035666  -0.37694275  0.8833072 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 6173. State = [[ 0.00134537 -0.18526907  0.1593072   1.        ]]. Action = [[ 0.6929467   0.6689279  -0.8688894   0.82884645]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 6174. State = [[ 0.00102786 -0.18536548  0.15857571  1.        ]]. Action = [[ 0.402516   0.680511  -0.9347531  0.7866932]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 6175. State = [[ 9.928262e-04 -1.853583e-01  1.584495e-01  1.000000e+00]]. Action = [[ 0.92004716  0.898895   -0.91808504  0.9039941 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 6176. State = [[ 0.00824252 -0.18354204  0.15612489  1.        ]]. Action = [[ 0.96069956  0.03226376 -0.52721137  0.9170506 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 6177. State = [[ 0.03119471 -0.18599275  0.13845937  1.        ]]. Action = [[0.6447252  0.7136916  0.0422734  0.93073773]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 6178. State = [[ 0.03901393 -0.1871117   0.13722809  1.        ]]. Action = [[ 0.9374937   0.68678474 -0.97086     0.95257616]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 6179. State = [[ 0.03904845 -0.1870183   0.13722757  1.        ]]. Action = [[ 0.9317312   0.93921626 -0.90862435  0.9555092 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 6180. State = [[ 0.0391095  -0.18699865  0.13733098  1.        ]]. Action = [[ 0.6846297   0.88239694 -0.54451334  0.9619005 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 6181. State = [[ 0.0391095  -0.18699865  0.13733098  1.        ]]. Action = [[ 0.94649506  0.8690908  -0.9908461   0.8983054 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 6182. State = [[ 0.0391095  -0.18699865  0.13733098  1.        ]]. Action = [[ 0.97781825  0.51139414 -0.618754    0.9336605 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 6183. State = [[ 0.03908375 -0.1869887   0.13732938  1.        ]]. Action = [[ 0.9635923   0.87958515 -0.8145609   0.93948364]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 6184. State = [[ 0.03908375 -0.1869887   0.13732938  1.        ]]. Action = [[ 0.6100297   0.8744724  -0.7731436   0.85439825]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 6185. State = [[ 0.03908375 -0.1869887   0.13732938  1.        ]]. Action = [[ 0.61290133  0.9966514  -0.9499099   0.9011626 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 6186. State = [[ 0.03908375 -0.1869887   0.13732938  1.        ]]. Action = [[ 0.86888456  0.48604894 -0.84235376  0.87931013]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 6187. State = [[ 0.03908375 -0.1869887   0.13732938  1.        ]]. Action = [[ 0.95497847  0.95773625 -0.9890317   0.89135337]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 6188. State = [[ 0.03999626 -0.18738222  0.13448428  1.        ]]. Action = [[ 0.8315878  -0.26453984 -0.90795016  0.90969   ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 6189. State = [[ 0.05916562 -0.19225419  0.10679117  1.        ]]. Action = [[ 0.8733554  0.9982703 -0.9832208  0.876549 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Scene graph at timestep 6189 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 6189 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6189 of -1
Current timestep = 6190. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.97066736  0.9636098  -0.74102515  0.96534705]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6191. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.8978679   0.9441397  -0.9756532   0.94282615]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6192. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.9434558  0.9971378 -0.6902417  0.739383 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6193. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.98483896  0.9674237  -0.8649756   0.963835  ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6194. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.9657326  0.9843849 -0.9590778  0.8766153]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6195. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.5707476   0.99268126 -0.89092827  0.9536023 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6196. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.9907081   0.9726987  -0.70419496  0.95670867]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6197. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.32204747  0.98676515 -0.9704694   0.93224597]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 6198. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.95084     0.8939729  -0.99191135  0.89038026]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6199. State = [[ 0.07456008 -0.19568801  0.10042755  1.        ]]. Action = [[ 0.97172904  0.7974963  -0.89215446  0.9444771 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6200. State = [[ 0.07478508 -0.19572267  0.10061297  1.        ]]. Action = [[ 0.9976195   0.8877475  -0.9810302   0.95523334]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6201. State = [[ 0.07478508 -0.19572267  0.10061297  1.        ]]. Action = [[ 0.42551863  0.9614259  -0.97108096  0.9390435 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 6202. State = [[ 0.07478508 -0.19572267  0.10061297  1.        ]]. Action = [[ 0.97709274  0.9954742  -0.5286316   0.97165346]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6203. State = [[ 0.07478508 -0.19572267  0.10061297  1.        ]]. Action = [[ 0.8624356   0.99000573 -0.8778246   0.9250338 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6204. State = [[ 0.07478508 -0.19572267  0.10061297  1.        ]]. Action = [[ 0.91795325  0.44023442 -0.97639096  0.951704  ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6205. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.9619372   0.8488239  -0.84179646  0.9427712 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6206. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.912307    0.98333967 -0.5637545   0.86965823]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6207. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.8923602   0.99148774 -0.92637634  0.9000156 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6208. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.9848839   0.8575504  -0.9992316   0.84178233]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6209. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.98999524  0.99024343 -0.97632074  0.93803525]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6210. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.9334614  0.9178666 -0.8069138  0.9665326]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6211. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.8196628   0.82065034 -0.79239464  0.8910086 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6212. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.9524405   0.7895839  -0.9475093   0.97643256]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6213. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.9369807   0.96406674 -0.97894025  0.9585302 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6214. State = [[ 0.07470718 -0.1956938   0.10060824  1.        ]]. Action = [[ 0.9003911   0.8945401  -0.88859063  0.9113629 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6215. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9003515   0.57411647 -0.90187675  0.9292674 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6216. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9747572   0.7419796  -0.75524485  0.94484663]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6217. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.82790995  0.9789643  -0.7057988   0.96259165]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6218. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9215081   0.83427167 -0.99401784  0.91691685]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6219. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.93330073  0.8436725  -0.93192697  0.97268236]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6220. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.96392846  0.98395133 -0.9450302   0.85708857]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6221. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.8662834   0.95794725 -0.9370397   0.8825952 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6222. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.75200033  0.85161304 -0.943063    0.9435433 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6223. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9396517  0.9633672 -0.9369814  0.946337 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6224. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.94393003  0.9020798  -0.30562615  0.9425955 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6225. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.4603572   0.86855316 -0.9811548   0.92189455]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 6226. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.942314    0.89651775 -0.93630433  0.9490739 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6227. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9750371   0.95947886 -0.9929465   0.9276494 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6228. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.91023874  0.6566615  -0.9926356   0.97939336]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6229. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9874289   0.9928775  -0.91990185  0.93390393]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6230. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.8906698  0.9687097 -0.9773839  0.9591056]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6231. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.99743414  0.6657655  -0.7418887   0.8977479 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6232. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9836502   0.9706216  -0.22129524  0.9079137 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6233. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9410671   0.99929595 -0.9452513   0.8801806 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6234. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.8135123   0.90247345 -0.79207397  0.88208413]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6235. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.7148173   0.98354983 -0.9951307   0.77749515]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6236. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.81688106  0.9862493  -0.9961131   0.7389896 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6237. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9810599   0.9871205  -0.94505584  0.8977045 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6238. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[0.95350766 0.9760356  0.48737836 0.85425425]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6239. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.89276576  0.9852581  -0.54036796  0.9725001 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6240. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[-0.4492464   0.662436   -0.60981095  0.9685737 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 6241. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.8617506   0.92729616 -0.65135485  0.9850941 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6242. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.8728293   0.98195267 -0.7206293   0.95263875]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6243. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9862828  0.8641553 -0.6597397  0.953585 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6244. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.7988286   0.99050426 -0.97362983  0.90083635]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6245. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.8192656   0.9660486  -0.8461199   0.91551757]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6246. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.7283602   0.95529056 -0.99867785  0.9202838 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6247. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.94815314  0.44897687 -0.5693248   0.9365494 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6248. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.94145036  0.6308665  -0.9858804   0.86734533]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6249. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9544444   0.83221793 -0.9838257   0.97412777]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6250. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9717572   0.9620948  -0.88676304  0.8950286 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6251. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9709883   0.73487616 -0.716378    0.95688486]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6252. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.9245987   0.72661734 -0.96875507  0.9503306 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6253. State = [[ 0.07468371 -0.19568896  0.10053715  1.        ]]. Action = [[ 0.97687304  0.9798937  -0.9768205   0.9298024 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6254. State = [[-0.260823   -0.11987229  0.11263818  1.        ]]. Action = [[ 0.6778017   0.9733362  -0.9450309   0.96086764]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6255. State = [[-0.25830892 -0.14832118  0.10607838  1.        ]]. Action = [[ 0.28307867 -0.9029217   0.9733548   0.09646058]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6256. State = [[-0.25376293 -0.17989072  0.12810157  1.        ]]. Action = [[ 0.1167798  -0.9431098   0.9643345   0.11431515]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6257. State = [[-0.24518186 -0.21219604  0.16423064  1.        ]]. Action = [[ 0.4484558  -0.6718906   0.9938872   0.23670733]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6258. State = [[-0.22883065 -0.22753812  0.19683078  1.        ]]. Action = [[0.23746371 0.14323795 0.50806856 0.5992787 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6259. State = [[-0.21213551 -0.22620414  0.22172968  1.        ]]. Action = [[0.74598885 0.2403189  0.6267867  0.7072239 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 6260. State = [[-0.18253791 -0.21488252  0.24536611  1.        ]]. Action = [[0.93614984 0.7499927  0.58013654 0.94939137]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6261. State = [[-0.15914087 -0.20054603  0.2636669   1.        ]]. Action = [[ 0.54204166  0.93418753 -0.52664864  0.8696772 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 6262. State = [[-0.14404961 -0.1890359   0.27280107  1.        ]]. Action = [[0.8544688  0.47420382 0.39513803 0.84064007]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6263. State = [[-0.12330477 -0.16524547  0.28904822  1.        ]]. Action = [[0.1541555  0.94121325 0.3287636  0.9540061 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6264. State = [[-0.11101012 -0.13084535  0.30274504  1.        ]]. Action = [[0.24113989 0.92543817 0.4196725  0.8020158 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6265. State = [[-0.10098809 -0.09426999  0.31018615  1.        ]]. Action = [[ 0.2381568  0.8915591 -0.3689376  0.9421601]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 6266. State = [[-0.09915051 -0.06464054  0.2987884   1.        ]]. Action = [[ 0.37180614  0.5559102  -0.68008006  0.742754  ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 6267. State = [[-0.0786566  -0.04522067  0.29901165  1.        ]]. Action = [[0.46971166 0.35242176 0.9906262  0.7204555 ]]. Reward = [0.]
Curr episode timestep = 12
Above hoop
Current timestep = 6268. State = [[-0.06251862 -0.02946082  0.31059745  1.        ]]. Action = [[ 0.51287055  0.35167432 -0.37136483  0.58902216]]. Reward = [0.]
Curr episode timestep = 13
Above hoop
Current timestep = 6269. State = [[-0.04462869 -0.03339293  0.31739503  1.        ]]. Action = [[ 0.21466923 -0.7249662   0.8704629   0.43277788]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Scene graph at timestep 6269 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 6269 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6269 of 1
Current timestep = 6270. State = [[-0.03041058 -0.05583426  0.34835398  1.        ]]. Action = [[ 0.37714767 -0.6542922   0.75710535  0.5340102 ]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Scene graph at timestep 6270 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 6270 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6270 of -1
Current timestep = 6271. State = [[-0.0177446  -0.05417525  0.37497956  1.        ]]. Action = [[0.4893924  0.9807317  0.25704277 0.8579229 ]]. Reward = [0.]
Curr episode timestep = 16
Above hoop
Current timestep = 6272. State = [[-0.00874305 -0.04054037  0.37291327  1.        ]]. Action = [[-0.20743239 -0.09428287 -0.51667464  0.83915436]]. Reward = [0.]
Curr episode timestep = 17
Above hoop
Current timestep = 6273. State = [[-0.00440159 -0.04941391  0.37267652  1.        ]]. Action = [[ 0.43484712 -0.7338255   0.4787693   0.67599535]]. Reward = [0.]
Curr episode timestep = 18
Above hoop
Current timestep = 6274. State = [[-0.00139462 -0.04939988  0.38646877  1.        ]]. Action = [[-0.3972093   0.7020116   0.6161492   0.52820265]]. Reward = [0.]
Curr episode timestep = 19
Above hoop
Current timestep = 6275. State = [[ 0.00137369 -0.02525655  0.39040604  1.        ]]. Action = [[ 0.69461465  0.98894167 -0.76935095  0.70728743]]. Reward = [0.]
Curr episode timestep = 20
Above hoop
Current timestep = 6276. State = [[0.02166353 0.00732    0.37496078 1.        ]]. Action = [[ 0.8861165   0.81824565 -0.17620814  0.47938657]]. Reward = [0.]
Curr episode timestep = 21
Above hoop
Current timestep = 6277. State = [[0.05048231 0.02515929 0.3595541  1.        ]]. Action = [[ 0.62040997 -0.01128036 -0.3319683   0.71962786]]. Reward = [0.]
Curr episode timestep = 22
Above hoop
Current timestep = 6278. State = [[0.07543088 0.02581647 0.34555292 1.        ]]. Action = [[ 0.34426093 -0.28567976 -0.34654772  0.5789423 ]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Current timestep = 6279. State = [[0.09072848 0.02491789 0.33919472 1.        ]]. Action = [[ 0.9636376  -0.45598555  0.8671546   0.7125516 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 6280. State = [[0.09415393 0.02472903 0.3389663  1.        ]]. Action = [[ 0.60970175 -0.6179481   0.7493987   0.75313413]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 6281. State = [[0.09487455 0.0248649  0.34150645 1.        ]]. Action = [[0.70649993 0.3342371  0.34496045 0.50034714]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6282. State = [[0.09471968 0.02448801 0.34117067 1.        ]]. Action = [[ 0.9491668  -0.05499965  0.6182244   0.5114279 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 6283. State = [[0.09327444 0.01301559 0.33439296 1.        ]]. Action = [[-0.12472606 -0.71422225 -0.99324065  0.80152285]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 6284. State = [[0.09641925 0.00189103 0.31067473 1.        ]]. Action = [[ 0.47822452 -0.41809177 -0.66731757  0.67061496]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6285. State = [[9.6717961e-02 6.4819858e-07 3.0888855e-01 1.0000000e+00]]. Action = [[ 0.42069674  0.56530094 -0.4442746   0.7560222 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6286. State = [[0.09441231 0.00912036 0.31926903 1.        ]]. Action = [[-0.79556024  0.6374136   0.6986289   0.78058136]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 6287. State = [[0.08944044 0.02027174 0.33210152 1.        ]]. Action = [[ 0.197986    0.4202795  -0.57766306  0.88308215]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 6288. State = [[0.08837549 0.02138013 0.33589026 1.        ]]. Action = [[0.20876598 0.0442338  0.74405086 0.831053  ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 6289. State = [[0.08789068 0.02127883 0.3364317  1.        ]]. Action = [[ 0.5711019  0.793962  -0.4153459  0.7445121]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 6290. State = [[0.08789112 0.02127886 0.3363576  1.        ]]. Action = [[ 0.37447798  0.01017392 -0.01200265  0.46703362]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 6291. State = [[0.08706629 0.01120469 0.32981536 1.        ]]. Action = [[ 0.13564038 -0.7120258  -0.5691722   0.66095436]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 6292. State = [[0.08807454 0.00244624 0.3226206  1.        ]]. Action = [[ 0.59921217 -0.5050202   0.31340647  0.6627836 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 6293. State = [[8.8037089e-02 9.4215159e-04 3.2062018e-01 1.0000000e+00]]. Action = [[0.5889287  0.49870396 0.36676323 0.753659  ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 6294. State = [[ 0.08686536 -0.01017605  0.32007074  1.        ]]. Action = [[-0.15476787 -0.587705   -0.07187742  0.79153824]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 6295. State = [[ 0.0848043  -0.02087312  0.31922898  1.        ]]. Action = [[0.9672531  0.4776857  0.01865065 0.83313394]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 6296. State = [[ 0.08460536 -0.02261242  0.31951898  1.        ]]. Action = [[0.849164   0.67846084 0.39442682 0.5745449 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6297. State = [[ 0.08163387 -0.01597274  0.31041363  1.        ]]. Action = [[ 0.02172315  0.5912709  -0.81231034  0.46680498]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 6298. State = [[ 0.07814172 -0.00310041  0.28013888  1.        ]]. Action = [[-0.1634584   0.35711408 -0.698239    0.85993207]]. Reward = [0.]
Curr episode timestep = 43
Above hoop
Current timestep = 6299. State = [[0.07373273 0.00618574 0.260188   1.        ]]. Action = [[-0.6152058   0.01808572  0.05179846  0.49716568]]. Reward = [0.]
Curr episode timestep = 44
Above hoop
Current timestep = 6300. State = [[0.07154796 0.00255112 0.26437268 1.        ]]. Action = [[ 0.41969693 -0.45115972  0.7204635   0.5678531 ]]. Reward = [0.]
Curr episode timestep = 45
Above hoop
Current timestep = 6301. State = [[ 0.07414716 -0.00286295  0.2695059   1.        ]]. Action = [[ 0.52007353  0.10499918 -0.38670158  0.4868436 ]]. Reward = [0.]
Curr episode timestep = 46
Above hoop
Current timestep = 6302. State = [[ 0.07385097 -0.0014879   0.2609377   1.        ]]. Action = [[-0.14773178  0.16207325 -0.617106    0.5261172 ]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Current timestep = 6303. State = [[ 0.07371898 -0.00114157  0.24778771  1.        ]]. Action = [[ 0.3510139  -0.0911808  -0.54417366  0.5965545 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Above hoop
Current timestep = 6304. State = [[ 0.07408426 -0.00404572  0.24717194  1.        ]]. Action = [[-0.20414829 -0.31428856  0.03773332  0.7705691 ]]. Reward = [0.]
Curr episode timestep = 49
Above hoop
Current timestep = 6305. State = [[0.07665811 0.00426533 0.25658467 1.        ]]. Action = [[0.36814284 0.7455199  0.8615465  0.12608624]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Current timestep = 6306. State = [[0.07763556 0.01380792 0.26279318 1.        ]]. Action = [[ 0.7781615  -0.07430255  0.08120966  0.4617033 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Above hoop
Current timestep = 6307. State = [[0.08302499 0.01370127 0.27800763 1.        ]]. Action = [[ 0.20989013 -0.14228195  0.8600602   0.5110116 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 6308. State = [[0.08558778 0.00122711 0.2911276  1.        ]]. Action = [[-0.00777119 -0.74404204 -0.5131697   0.7508044 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 6309. State = [[ 0.08715699 -0.00826113  0.2905972   1.        ]]. Action = [[ 0.79727197 -0.7709981   0.14285707  0.6828685 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 6310. State = [[ 0.08187746 -0.00783392  0.27867436  1.        ]]. Action = [[-0.2994523   0.278103   -0.90397626  0.41428137]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 6311. State = [[ 0.08083381 -0.00717084  0.26063588  1.        ]]. Action = [[ 0.84704757 -0.57113606  0.49887538  0.6360514 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 6312. State = [[ 0.08091861 -0.00722669  0.25931382  1.        ]]. Action = [[ 0.51711583 -0.11154336  0.17864561  0.5221386 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 6313. State = [[ 0.08091861 -0.00722669  0.25931382  1.        ]]. Action = [[0.813431   0.07962954 0.4980321  0.8796879 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 6314. State = [[ 0.08091861 -0.00722669  0.25931382  1.        ]]. Action = [[ 0.71017337 -0.58175856  0.33059764  0.7099047 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 6315. State = [[ 0.08091861 -0.00722669  0.25931382  1.        ]]. Action = [[0.67235994 0.9394779  0.10420477 0.5328499 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 6316. State = [[ 0.08091122 -0.00722778  0.2592406   1.        ]]. Action = [[-0.39726734 -0.9475921  -0.57103026  0.57562304]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 6317. State = [[0.07850403 0.00410237 0.262497   1.        ]]. Action = [[-0.6378277   0.63814425  0.3373561   0.5225711 ]]. Reward = [0.]
Curr episode timestep = 62
Above hoop
Current timestep = 6318. State = [[0.07386278 0.01843067 0.26587382 1.        ]]. Action = [[0.6525259  0.13910174 0.9122182  0.70090115]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Above hoop
Current timestep = 6319. State = [[0.07366037 0.01308658 0.27802578 1.        ]]. Action = [[-0.07711351 -0.511771    0.65149593  0.47681153]]. Reward = [0.]
Curr episode timestep = 64
Above hoop
Current timestep = 6320. State = [[ 0.06803818 -0.00219325  0.2819929   1.        ]]. Action = [[-0.0036217  -0.4634087  -0.61219573  0.60652506]]. Reward = [0.]
Curr episode timestep = 65
Above hoop
Current timestep = 6321. State = [[ 0.06470621 -0.01225542  0.26725534  1.        ]]. Action = [[ 0.574345   -0.07567179 -0.46545148  0.5136663 ]]. Reward = [0.]
Curr episode timestep = 66
Above hoop
Current timestep = 6322. State = [[ 0.06921951 -0.00636     0.25925207  1.        ]]. Action = [[0.14970481 0.65353394 0.29316533 0.75061095]]. Reward = [0.]
Curr episode timestep = 67
Above hoop
Current timestep = 6323. State = [[ 0.06871045 -0.00119119  0.259726    1.        ]]. Action = [[ 0.8739666  -0.3146224   0.12024021  0.4892912 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Above hoop
Current timestep = 6324. State = [[0.07255543 0.01248765 0.26254576 1.        ]]. Action = [[0.39475596 0.83743787 0.12971902 0.68103707]]. Reward = [0.]
Curr episode timestep = 69
Above hoop
Current timestep = 6325. State = [[0.0803784  0.01701577 0.26106313 1.        ]]. Action = [[ 0.39709115 -0.90473056 -0.15735048  0.58777106]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 6326. State = [[0.09015171 0.00711263 0.2558896  1.        ]]. Action = [[0.55634737 0.38236177 0.87641025 0.5075755 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6327. State = [[0.09054286 0.00610425 0.25600982 1.        ]]. Action = [[ 0.42275846 -0.4189129   0.89504266  0.63488436]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6328. State = [[0.09054286 0.00610425 0.25600982 1.        ]]. Action = [[ 0.636691   -0.25285125  0.8579725   0.58875895]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6329. State = [[0.09054286 0.00610425 0.25600982 1.        ]]. Action = [[ 0.5490284  -0.06562352  0.342255    0.7069919 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6330. State = [[0.09054177 0.00600004 0.25605676 1.        ]]. Action = [[ 0.3922634   0.07292247 -0.4890989   0.60821605]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6331. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[0.7204919  0.25592387 0.50632715 0.62041163]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6332. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[0.49837518 0.45471168 0.47719038 0.5834336 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6333. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[0.9114007  0.30120397 0.28544796 0.6286808 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6334. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[0.68528223 0.1673671  0.3450917  0.38843477]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6335. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[ 0.5774394  -0.5234906   0.48189294  0.4123249 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6336. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[ 0.83669436 -0.7232421  -0.5099272   0.5250459 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6337. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[ 0.88472223 -0.78840715 -0.586637    0.6991687 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6338. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[ 0.9482902  -0.06160909  0.6261513   0.54893804]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6339. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[0.54256535 0.28958988 0.8065491  0.65108585]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6340. State = [[0.09054123 0.00594794 0.25608024 1.        ]]. Action = [[0.86456573 0.02124941 0.66279626 0.8030746 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6341. State = [[ 0.0925796  -0.00354473  0.26458296  1.        ]]. Action = [[-0.10199058 -0.5303406   0.6545272   0.39948678]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 6342. State = [[ 0.09473985 -0.01301757  0.2751399   1.        ]]. Action = [[ 0.19717455 -0.23653817  0.97211266  0.69280815]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6343. State = [[ 0.09421752 -0.01566771  0.27653235  1.        ]]. Action = [[0.6300385  0.31770384 0.39053345 0.58564603]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6344. State = [[ 0.09386674 -0.01563051  0.27653995  1.        ]]. Action = [[0.1487112  0.91681755 0.7747805  0.45474434]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6345. State = [[ 0.09386674 -0.01563051  0.27653995  1.        ]]. Action = [[ 0.7930939   0.50194323 -0.6873707   0.5432737 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6346. State = [[ 0.09386674 -0.01563051  0.27653995  1.        ]]. Action = [[ 0.95248663  0.656147   -0.5495522   0.41395724]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6347. State = [[ 0.09207263 -0.01650494  0.287636    1.        ]]. Action = [[-0.56053376  0.05319905  0.63431764  0.7091044 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 6348. State = [[ 0.08931427 -0.0176218   0.29958737  1.        ]]. Action = [[ 0.3480152   0.9370792  -0.08572859  0.64447653]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6349. State = [[ 0.08542025 -0.02690973  0.30931604  1.        ]]. Action = [[-0.5587488  -0.55146027  0.27800083  0.6255183 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 6350. State = [[ 0.07916968 -0.03528458  0.31848     1.        ]]. Action = [[0.82963955 0.86130714 0.19468439 0.69830537]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Above hoop
Current timestep = 6351. State = [[ 0.07840718 -0.03554572  0.32024676  1.        ]]. Action = [[ 0.63546324  0.06298053 -0.3844959   0.6443409 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Above hoop
Current timestep = 6352. State = [[ 0.07801411 -0.03549478  0.3213163   1.        ]]. Action = [[0.8927492  0.15809512 0.11866462 0.7303666 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Above hoop
Current timestep = 6353. State = [[ 0.0778125  -0.03545648  0.32261744  1.        ]]. Action = [[0.75372505 0.59200406 0.19749177 0.37852   ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Above hoop
Current timestep = 6354. State = [[ 0.07779793 -0.03556044  0.32315028  1.        ]]. Action = [[ 0.4699086   0.68669915 -0.14002693  0.8088834 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Above hoop
Current timestep = 6355. State = [[ 0.07779793 -0.03556044  0.32315028  1.        ]]. Action = [[ 0.712754    0.12493289 -0.8219201   0.81499386]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Above hoop
Current timestep = 6356. State = [[-0.2686644   0.07718846  0.10707337  1.        ]]. Action = [[ 0.75348866 -0.22772503 -0.31870806  0.4993068 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Above hoop
Current timestep = 6357. State = [[-0.25722826  0.07594683  0.10081951  1.        ]]. Action = [[ 0.5936278  -0.64285034  0.80781484  0.05400276]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6358. State = [[-0.23705046  0.05406998  0.1206075   1.        ]]. Action = [[ 0.6645247  -0.87101686  0.97253036  0.10609651]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6359. State = [[-0.21219134  0.02327066  0.15146168  1.        ]]. Action = [[ 0.5931926  -0.9072785   0.56726265  0.19298267]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6360. State = [[-0.18476015 -0.00955755  0.1797926   1.        ]]. Action = [[ 0.7324892  -0.91922134  0.88402593  0.26386094]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6361. State = [[-0.16708936 -0.02942567  0.20113197  1.        ]]. Action = [[ 0.43215644 -0.7430626   0.94836783  0.28952873]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 6362. State = [[-0.15243432 -0.0472668   0.2199843   1.        ]]. Action = [[ 0.7314553  -0.86088216  0.9940394   0.63915384]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6363. State = [[-0.12679258 -0.07792939  0.2563363   1.        ]]. Action = [[ 0.75806713 -0.78951347  0.7627648   0.5458007 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6364. State = [[-0.1065946  -0.10572354  0.28617683  1.        ]]. Action = [[ 0.42822278 -0.77708113  0.4503529   0.68238616]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6365. State = [[-0.09014915 -0.11939788  0.3106397   1.        ]]. Action = [[0.1694119  0.42420185 0.8648896  0.8354126 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6366. State = [[-0.07262027 -0.11883355  0.32750186  1.        ]]. Action = [[ 0.6597972  -0.19385064 -0.17631555  0.7182082 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6367. State = [[-0.05005527 -0.11643893  0.34130493  1.        ]]. Action = [[0.8788912  0.26342165 0.87821245 0.83212173]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 6368. State = [[-0.02186237 -0.10087324  0.37010032  1.        ]]. Action = [[0.57666075 0.7947769  0.59577024 0.88525534]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 6369. State = [[-8.3344209e-04 -7.4115045e-02  3.9399344e-01  1.0000000e+00]]. Action = [[0.13096142 0.96718526 0.45689833 0.74586403]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 6370. State = [[ 0.00310035 -0.05415386  0.40698388  1.        ]]. Action = [[ 0.09769857 -0.03610182  0.6616219   0.6365745 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Above hoop
Current timestep = 6371. State = [[ 0.00564793 -0.060091    0.40674895  1.        ]]. Action = [[ 0.26938605 -0.66262835 -0.49327266  0.7115886 ]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Current timestep = 6372. State = [[ 0.01176934 -0.06557541  0.38219047  1.        ]]. Action = [[ 0.29047585  0.09918475 -0.861569    0.848413  ]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Current timestep = 6373. State = [[ 0.03104509 -0.07499332  0.35345212  1.        ]]. Action = [[ 0.9826671 -0.6366769 -0.6191961  0.7077334]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 6374. State = [[ 0.06546515 -0.09023461  0.32717553  1.        ]]. Action = [[ 0.86006594 -0.15293288 -0.51743215  0.7454958 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 6375. State = [[ 0.09222696 -0.10818198  0.2973977   1.        ]]. Action = [[ 0.2844361  -0.8640791  -0.90770566  0.6586889 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 6376. State = [[ 0.11139133 -0.12568922  0.2766493   1.        ]]. Action = [[ 0.45501947  0.83364654 -0.96828085  0.703532  ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 6376 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 6376 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6376 of -1
Current timestep = 6377. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.6816058  -0.11827672 -0.91236746  0.8425126 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6378. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.9698627   0.6768992  -0.8570515   0.84870887]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6379. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.20003891 -0.04315114 -0.99134284  0.7912077 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6380. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.24348462  0.84572077 -0.9790289   0.9229908 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6381. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.8887918   0.49732065 -0.9491052   0.90453434]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6382. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.52930224  0.48347974 -0.95327014  0.92491317]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6383. State = [[ 0.11426991 -0.12785421  0.2734624   1.        ]]. Action = [[ 0.88356566  0.9911305  -0.5541163   0.8778014 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6384. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.73732996  0.4763292  -0.6987121   0.84629786]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 6385. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.61850595  0.45997262 -0.75098705  0.8629799 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 6386. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.80418515 -0.17986071 -0.5647667   0.4419092 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6387. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.97762203 -0.31387526  0.0562427   0.79767   ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6388. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.77909446  0.9545915  -0.95144945  0.88539505]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6389. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.6713488   0.21035421 -0.9959149   0.7086315 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6390. State = [[ 0.1142639  -0.12785414  0.2733872   1.        ]]. Action = [[ 0.9754348   0.18692362 -0.88826865  0.63550115]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6391. State = [[ 0.11069486 -0.13954578  0.27471712  1.        ]]. Action = [[-0.8767423  -0.62780434 -0.00698733  0.87394047]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 6392. State = [[ 0.10755087 -0.15071991  0.2775981   1.        ]]. Action = [[ 0.01787758  0.9637039  -0.998582    0.90487885]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6393. State = [[ 0.10722844 -0.15229365  0.27746865  1.        ]]. Action = [[0.90100265 0.99019504 0.30940604 0.80824304]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 6394. State = [[ 0.10724029 -0.15254325  0.27763343  1.        ]]. Action = [[ 0.307415   0.997545  -0.7369317  0.92557  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 6395. State = [[ 0.10026233 -0.14056076  0.26937264  1.        ]]. Action = [[-0.88654315  0.9922409  -0.63775206  0.8528867 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 6396. State = [[ 0.08786336 -0.12442469  0.25639796  1.        ]]. Action = [[ 0.85613275  0.9866036  -0.47016704  0.9537995 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 6397. State = [[ 0.0856912  -0.12301809  0.25347042  1.        ]]. Action = [[ 0.9045787   0.89071167 -0.5948098   0.78199804]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6398. State = [[ 0.08510732 -0.12268275  0.25313812  1.        ]]. Action = [[ 0.584389   -0.09321982 -0.16000807  0.8941575 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6399. State = [[ 0.08483429 -0.12278441  0.25301254  1.        ]]. Action = [[ 0.9453392  -0.27937722  0.70903444  0.8303449 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 6400. State = [[ 0.08486049 -0.12279364  0.25301233  1.        ]]. Action = [[0.366148   0.8574698  0.20685291 0.67348814]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 6401. State = [[ 0.08486049 -0.12279364  0.25301233  1.        ]]. Action = [[ 0.92857385 -0.5352707  -0.9558235   0.652591  ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6402. State = [[ 0.0848686  -0.1227934   0.25308466  1.        ]]. Action = [[ 0.42406082  0.1470375  -0.7874083   0.7524463 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6403. State = [[ 0.0848686  -0.1227934   0.25308466  1.        ]]. Action = [[ 0.94135976  0.8243437  -0.9009743   0.6087762 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6404. State = [[ 0.0852946  -0.12077804  0.25799623  1.        ]]. Action = [[0.10463333 0.11798382 0.35497093 0.7409215 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 6405. State = [[ 0.08565157 -0.11883539  0.26273817  1.        ]]. Action = [[ 0.9614277   0.78666127 -0.9075088   0.92985296]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6406. State = [[ 0.08568898 -0.11828356  0.26280656  1.        ]]. Action = [[ 0.9775801  -0.448532   -0.73123926  0.80746365]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6407. State = [[ 0.08568898 -0.11828356  0.26280656  1.        ]]. Action = [[0.93154085 0.7514782  0.4319042  0.58022356]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 6408. State = [[ 0.08568898 -0.11828356  0.26280656  1.        ]]. Action = [[ 0.93237615 -0.07003844 -0.9697976   0.78929245]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6409. State = [[ 0.08568898 -0.11828356  0.26280656  1.        ]]. Action = [[ 0.9323087  -0.05308568  0.52687883  0.86638045]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 6410. State = [[ 0.08568898 -0.11828356  0.26280656  1.        ]]. Action = [[ 0.80054915  0.93719864 -0.60420424  0.8124304 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6411. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.12663269  0.99505186 -0.99170434  0.896966  ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 6412. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.93584347 -0.4181037   0.4562682   0.7956197 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 6413. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.4181441   0.65250635 -0.6277704   0.84065986]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6414. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.76056755 -0.01536542 -0.9811598   0.7508762 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6415. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.81553674 -0.02006191 -0.98724157  0.80986357]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6416. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.8085754   0.92616546 -0.50582975  0.8485484 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 6417. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[0.814693  0.8555523 0.6714784 0.7792959]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 6418. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.78630173  0.2029053  -0.21277577  0.76128006]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 6419. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.71356606 -0.19055331  0.7977574   0.6794417 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 6420. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.65426505  0.9261918  -0.770606    0.8085216 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6421. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[0.89258766 0.862133   0.42627573 0.7944045 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 6422. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[0.8017924  0.99335265 0.83741355 0.88062453]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 6423. State = [[ 0.08569608 -0.1182833   0.262879    1.        ]]. Action = [[ 0.90706134 -0.04988819 -0.3123864   0.86436176]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6424. State = [[ 0.08310506 -0.12656629  0.26739213  1.        ]]. Action = [[-0.33337224 -0.5308705   0.18084073  0.7792797 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 6425. State = [[ 0.07501552 -0.13382311  0.27454296  1.        ]]. Action = [[ 0.768883    0.5799124  -0.09276605  0.93080735]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 6426. State = [[ 0.07437631 -0.1356853   0.2757867   1.        ]]. Action = [[ 0.9693074  -0.20225793 -0.9719706   0.7583444 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 6427. State = [[ 0.07521188 -0.12359665  0.28369522  1.        ]]. Action = [[0.04264641 0.88827765 0.6204158  0.85803056]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 6428. State = [[ 0.07342254 -0.10026009  0.28992572  1.        ]]. Action = [[ 0.4418682   0.27845216 -0.47631264  0.7816055 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 6429. State = [[ 0.07299695 -0.09380843  0.28616202  1.        ]]. Action = [[ 0.9403204  0.5111418 -0.8258222  0.6013169]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6430. State = [[ 0.07293733 -0.09312644  0.28534594  1.        ]]. Action = [[ 0.5972619  -0.22316247  0.643368    0.70446205]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6431. State = [[ 0.07285946 -0.07809816  0.28468692  1.        ]]. Action = [[-0.26141107  0.8846731   0.10948884  0.8011764 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 6432. State = [[ 0.07349716 -0.06984244  0.29315525  1.        ]]. Action = [[ 0.37542558 -0.6833548   0.56939125  0.41262698]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 6433. State = [[ 0.07523759 -0.0764531   0.3013963   1.        ]]. Action = [[ 0.527477   -0.7918489   0.31668627  0.61517346]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6434. State = [[ 0.07569157 -0.07675     0.30237043  1.        ]]. Action = [[ 0.70574594  0.87727785 -0.998311    0.8433585 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6435. State = [[ 0.07188032 -0.09064489  0.31918627  1.        ]]. Action = [[-0.726529   -0.70123124  0.8739302   0.5536082 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 6436. State = [[ 0.06320155 -0.10418608  0.33787894  1.        ]]. Action = [[ 0.8027475  -0.96829134 -0.7066855   0.6814766 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6437. State = [[ 0.06470777 -0.10769112  0.34921446  1.        ]]. Action = [[ 0.6477616  -0.11365372  0.8617281   0.7786579 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 6438. State = [[ 0.06626804 -0.12436859  0.37456435  1.        ]]. Action = [[ 0.31341267 -0.94680464  0.6891024   0.5111631 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 6439. State = [[ 0.06893901 -0.14169435  0.3909706   1.        ]]. Action = [[ 0.81486773  0.40378988 -0.19097388  0.83673906]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6440. State = [[ 0.07152992 -0.1485401   0.3959519   1.        ]]. Action = [[ 0.1553489  -0.25529438  0.15647447  0.84583676]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 6441. State = [[ 0.07403494 -0.15403971  0.4017953   1.        ]]. Action = [[ 0.9462261  -0.10252029 -0.5233652   0.80566454]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6442. State = [[ 0.07462225 -0.1544846   0.40253326  1.        ]]. Action = [[ 0.7459748  -0.13375515  0.805012    0.95686126]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6443. State = [[ 0.07462946 -0.15448423  0.40246215  1.        ]]. Action = [[ 0.6279433 -0.9510301 -0.6116779  0.8741205]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 6444. State = [[ 0.07462953 -0.15456368  0.40246218  1.        ]]. Action = [[ 0.98479223  0.43131876 -0.1296829   0.5961008 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6445. State = [[ 0.07460549 -0.15454985  0.4024606   1.        ]]. Action = [[ 0.9957504  -0.3068456  -0.65193176  0.82462263]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6446. State = [[ 0.07460549 -0.15454985  0.4024606   1.        ]]. Action = [[ 0.95075893  0.93095565 -0.8318655   0.8672675 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6447. State = [[ 0.07460549 -0.15454985  0.4024606   1.        ]]. Action = [[ 0.9178839   0.80213416 -0.83019257  0.90935946]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6448. State = [[ 0.07460549 -0.15454985  0.4024606   1.        ]]. Action = [[ 0.6664655  -0.693001    0.44630957  0.8488922 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6449. State = [[ 0.07460555 -0.15463005  0.40246063  1.        ]]. Action = [[-0.9172735   0.93570065  0.70187473  0.7773433 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 6450. State = [[ 0.07460555 -0.15463005  0.40246063  1.        ]]. Action = [[0.30071294 0.29827785 0.9779773  0.8205366 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6451. State = [[ 0.07460555 -0.15463005  0.40246063  1.        ]]. Action = [[0.42719793 0.6640849  0.85084987 0.87454486]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 6452. State = [[ 0.07701923 -0.14563033  0.4067824   1.        ]]. Action = [[0.08488059 0.65166974 0.24190581 0.83739316]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 6453. State = [[ 0.07940209 -0.13817716  0.41436896  1.        ]]. Action = [[0.8140744  0.9893484  0.13370264 0.74217796]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6454. State = [[ 0.0793075  -0.1377394   0.41438186  1.        ]]. Action = [[ 0.8903239  -0.64817953 -0.91158587  0.8196373 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6455. State = [[ 0.0793075  -0.1377394   0.41438186  1.        ]]. Action = [[ 0.39611506 -0.73262095  0.12089729  0.6151564 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6456. State = [[ 0.07508112 -0.1484233   0.40972456  1.        ]]. Action = [[-0.717141   -0.73964757 -0.5095462   0.65937614]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 6457. State = [[ 0.06997568 -0.15689316  0.40414116  1.        ]]. Action = [[ 0.5881189   0.21442568 -0.80937654  0.89263916]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 6458. State = [[-0.26819494  0.1344691   0.10435163  1.        ]]. Action = [[ 0.9957988  0.7904029 -0.6013904  0.6716881]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 6459. State = [[-0.25097317  0.13712363  0.09798405  1.        ]]. Action = [[ 0.79392266 -0.8493901   0.9831188   0.18316627]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6460. State = [[-0.22882706  0.11191918  0.12181202  1.        ]]. Action = [[ 0.6777519  -0.72821486  0.90638685  0.2591474 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6461. State = [[-0.19928925  0.08390926  0.1580195   1.        ]]. Action = [[ 0.70397353 -0.90487736  0.9903183   0.22531033]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6462. State = [[-0.18192287  0.06558719  0.18295376  1.        ]]. Action = [[ 0.68810606 -0.7561401   0.86569285  0.17504084]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 6463. State = [[-0.17813268  0.06383523  0.18677056  1.        ]]. Action = [[ 0.80306053 -0.74676484  0.86974     0.2844584 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 6464. State = [[-0.17798795  0.0631951   0.18723714  1.        ]]. Action = [[ 0.74928474 -0.78053594  0.8911909   0.32921016]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 6465. State = [[-0.17788601  0.06277271  0.18742692  1.        ]]. Action = [[ 0.87973785 -0.27556115  0.838732    0.25085223]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 6466. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.3364041  -0.6339451   0.6828886   0.30874503]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 6467. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.76519275 -0.9175305   0.9491794   0.32809126]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 6468. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.75295675 -0.6932175   0.9582561   0.20008612]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 6469. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8222475  -0.9513945   0.7267219   0.27834845]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 6470. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.6584412  -0.88836676  0.71372426  0.3034513 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 6471. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.9299376  -0.34624535  0.7391747   0.40998125]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 6472. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.50867105 -0.6706446   0.96878386  0.32026863]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 6473. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.39358795 -0.08285785  0.752651    0.22518337]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 6474. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8883827 -0.9704828  0.8926827  0.3256886]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 6475. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.91166115 -0.97134     0.9976504   0.19900537]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 6476. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.6582148  -0.9203209   0.97032595  0.36203325]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 6477. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.6007683  -0.8322938   0.8633398   0.22278559]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 6478. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.40275466 -0.95825493  0.6334528   0.24960434]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 6479. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[0.7657018  0.19865227 0.9226308  0.19590867]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 6480. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.4696802  -0.53454816  0.9776213   0.37091458]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 6481. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8179972  -0.9281956   0.96534276  0.2914089 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 6482. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.5487511  -0.98383063  0.8086102   0.30626178]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 6483. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.78148913 -0.6399013   0.93973184  0.21100831]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 6484. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.35251832 -0.91572803  0.70625806  0.24009717]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 6485. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8079035  -0.9260767   0.9017618   0.28751278]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 6486. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.4326129  -0.79124665  0.95216775  0.1482513 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 6487. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.63426816 -0.9286417   0.7090348   0.30855465]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 6488. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8288859  -0.46297228  0.97859716  0.35945916]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 6489. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.590361   -0.6419932   0.8802233   0.19805372]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 6490. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8247967  -0.8015888   0.9643662   0.24308062]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 6491. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.82894874 -0.54183674  0.90006876  0.18582857]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 6492. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[0.8275436  0.09209061 0.72827387 0.3135841 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 6493. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8775625  -0.7281027   0.83526754  0.3720721 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 6494. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.4758811  -0.94821787  0.9064295   0.21558666]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 6495. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.6184497  -0.3229146   0.35073113  0.32362938]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 6496. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8134619  -0.8426889   0.68787396  0.2730186 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 6497. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.8325312  -0.9267545   0.9318795   0.21621084]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 6498. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.91408765 -0.8102508   0.9279696   0.26462817]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 6499. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.30547023 -0.8360995   0.51199436  0.304358  ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 6500. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.7341658  -0.27626878  0.96029365  0.26442266]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 6501. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.80866075 -0.9396814   0.8756776   0.2426486 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 6502. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.71184444 -0.78347844  0.9590825   0.42385697]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 6503. State = [[-0.1779401   0.06277704  0.18752171  1.        ]]. Action = [[ 0.53405666 -0.71454686  0.84828186  0.34038234]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 6504. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.7768657  -0.874967    0.9698094   0.37299407]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 6505. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.5236604  -0.7161921   0.968547    0.21774125]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 6506. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.675771   -0.84876186  0.98389137  0.31742358]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 6507. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.6706867  -0.71790874  0.94758916  0.32595742]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 6508. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.5049542  -0.34214735  0.15109849  0.4168284 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 6509. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.3396982  -0.45063084  0.95373523  0.28512788]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 6510. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.76977384 -0.8894311   0.95849514  0.3536209 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 6511. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.73102    -0.5283489   0.91743803  0.36687303]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 6512. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.66638505 -0.721954    0.81461966  0.29568434]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 6513. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.7533486  -0.25436282  0.91699266  0.3626293 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 6514. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.6576055  -0.68150586  0.98629475  0.26933742]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 6515. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.73105955 -0.9573146   0.8454075   0.38459003]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 6516. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.60449314 -0.75475526  0.5621306   0.3589319 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 6517. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.84469867 -0.86403334  0.27573216  0.40263128]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 6518. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.75379884 -0.89137477  0.92447054  0.3867333 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 6519. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.22010136 -0.5529626   0.9798672   0.38130975]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 6520. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.32302523 -0.73650455  0.480623    0.3506167 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 6521. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.91405344 -0.7114239   0.8458302   0.41950965]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 6522. State = [[-0.17792575  0.0627808   0.18757096  1.        ]]. Action = [[ 0.6483214  -0.2308302   0.9489827   0.34443307]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 6523. State = [[-0.17493397  0.04667364  0.19732635  1.        ]]. Action = [[-0.08581787 -0.9681321   0.63569367  0.2869134 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 6524. State = [[-0.16364224  0.0180812   0.22413155  1.        ]]. Action = [[ 0.84093356 -0.70279557  0.98428035  0.35017443]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 6525. State = [[-0.14069209 -0.01007774  0.26017275  1.        ]]. Action = [[ 0.5781543 -0.609152   0.7331209  0.558687 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 6526. State = [[-0.12001748 -0.03993417  0.29604635  1.        ]]. Action = [[ 0.5571158  -0.91835177  0.8584546   0.3486153 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 6527. State = [[-0.10786679 -0.0673851   0.32418597  1.        ]]. Action = [[-0.02540654 -0.44260573  0.3630433   0.48898613]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 6528. State = [[-0.09687588 -0.09259807  0.34925273  1.        ]]. Action = [[ 0.7471552  -0.9181057   0.8451283   0.43970132]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 6529. State = [[-0.0728128  -0.11942959  0.3792366   1.        ]]. Action = [[ 0.5447211  -0.3756541   0.7535453   0.40335405]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 6530. State = [[-0.05964991 -0.12991594  0.40063074  1.        ]]. Action = [[ 0.513474   -0.87438107  0.8279209   0.5149083 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6531. State = [[-0.06322521 -0.14822735  0.40933096  1.        ]]. Action = [[-0.5218663  -0.93576914  0.25281     0.5391902 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 6532. State = [[-0.07033055 -0.16384456  0.4140483   1.        ]]. Action = [[ 0.20455909 -0.9699532   0.3132317   0.5762439 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6533. State = [[-0.07109265 -0.16707991  0.4147317   1.        ]]. Action = [[ 0.6018952  -0.4199193   0.32175636  0.64405704]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6534. State = [[-0.07134696 -0.16797537  0.41497636  1.        ]]. Action = [[0.8068261  0.03076732 0.98505163 0.56617856]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 6535. State = [[-0.07151792 -0.16815554  0.41504446  1.        ]]. Action = [[ 0.5974498 -0.7375355  0.9683883  0.3871988]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6536. State = [[-0.07175492 -0.16814448  0.41505402  1.        ]]. Action = [[ 0.53695226 -0.84632134  0.9061961   0.58667696]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6537. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[-0.19633019  0.7399647   0.96154547  0.45033026]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6538. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[-0.42973304 -0.35471004  0.9259809   0.61382127]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6539. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.7009599  -0.9400864   0.94411576  0.6155752 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6540. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.5038583 0.5886278 0.9256246 0.6981689]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 6541. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.35252213 -0.8661738   0.94071186  0.5191386 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6542. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.8993349  -0.0710184   0.9231911   0.69150364]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6543. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.7011721  -0.99159163  0.948092    0.67718256]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6544. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.30318415 -0.4773305   0.9738625   0.5921178 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6545. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.5934967 -0.8948661  0.565256   0.7009586]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 6546. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.60298157 -0.48076618  0.9273257   0.7383852 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6547. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.5246613  0.2424357  0.9917834  0.36638045]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6548. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.5551772  -0.55941707  0.6729151   0.68921065]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6549. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.57683563 0.48674154 0.8376138  0.5792835 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6550. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.4107262  0.5032661  0.8576796  0.40397656]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6551. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.56794715 -0.8774925   0.9371307   0.61526704]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 6552. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.2182399  -0.6238462   0.69817495  0.37765336]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6553. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.7926667  0.03030562 0.9263762  0.7019963 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 6554. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.18910313 0.8434377  0.10725415 0.73539734]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 6555. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[-0.26768756 -0.40505445  0.8454313   0.6606989 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6556. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[0.22927308 0.90901613 0.8626609  0.53609097]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6557. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.57655776 -0.0833177   0.76505816  0.6585603 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6558. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[ 0.42422724 -0.07765627  0.73835945  0.652686  ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 6559. State = [[-0.07182368 -0.1681012   0.41504925  1.        ]]. Action = [[-0.0092274  -0.93266875  0.7955415   0.7077465 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 6560. State = [[-0.25702262 -0.05658972  0.10675545  1.        ]]. Action = [[0.316303   0.87416553 0.8374609  0.49907255]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 6561. State = [[-0.24660309 -0.07715619  0.10034283  1.        ]]. Action = [[ 0.74492204 -0.86322427  0.91169906  0.14764035]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6562. State = [[-0.2247062  -0.10843959  0.12129536  1.        ]]. Action = [[ 0.5789945  -0.849791    0.88209677  0.21693504]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6563. State = [[-0.20553342 -0.14189252  0.15463911  1.        ]]. Action = [[ 0.27046227 -0.7999254   0.9271686   0.16742897]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6564. State = [[-0.18429165 -0.16550192  0.19172503  1.        ]]. Action = [[ 0.7257936  -0.35567582  0.90699303  0.36168838]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6565. State = [[-0.16035593 -0.19136642  0.22631633  1.        ]]. Action = [[ 0.4443382  -0.91739136  0.8846562   0.48908138]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 6566. State = [[-0.13677679 -0.21157055  0.26091224  1.        ]]. Action = [[ 0.8029777  -0.00341004  0.7038965   0.5699117 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6567. State = [[-0.10728905 -0.22652657  0.29290083  1.        ]]. Action = [[ 0.83387256 -0.74236315  0.49109817  0.8165264 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6568. State = [[-0.08578954 -0.23045921  0.32027182  1.        ]]. Action = [[-0.06664866  0.7200594   0.90722775  0.68352437]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6569. State = [[-0.08497684 -0.2382983   0.3381997   1.        ]]. Action = [[-0.28632653 -0.86067337 -0.08654118  0.81232107]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6570. State = [[-0.09234569 -0.23551601  0.3531907   1.        ]]. Action = [[-0.39772797  0.99814105  0.8580859   0.71250796]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6571. State = [[-0.08907584 -0.21029149  0.3813624   1.        ]]. Action = [[0.73816097 0.62023616 0.6808076  0.86078954]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 6572. State = [[-0.07901036 -0.19690317  0.40101132  1.        ]]. Action = [[-0.48432994  0.07077205  0.67011476  0.46963954]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 6573. State = [[-0.07779831 -0.19526432  0.40320337  1.        ]]. Action = [[0.38614762 0.98963165 0.85811996 0.66831195]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 6574. State = [[-0.07783393 -0.19505562  0.40321365  1.        ]]. Action = [[ 0.24120569 -0.87367177  0.5947981   0.7188451 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 6575. State = [[-0.07790667 -0.1950194   0.4032093   1.        ]]. Action = [[ 0.82994604 -0.8089968   0.98226523  0.6992719 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 6576. State = [[-0.07790667 -0.1950194   0.4032093   1.        ]]. Action = [[0.3342656  0.79809606 0.8754995  0.45316672]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 6577. State = [[-0.0779315  -0.19500703  0.40320784  1.        ]]. Action = [[-0.22873372 -0.23729408  0.9020963   0.7631339 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 6578. State = [[-0.0780306  -0.19495766  0.40320206  1.        ]]. Action = [[-0.2060973  -0.91701454  0.9901383   0.7159183 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 6579. State = [[-0.0780306  -0.19495766  0.40320206  1.        ]]. Action = [[0.16484213 0.95227456 0.39862633 0.58248425]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 6580. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.57161105 -0.05467033  0.9170166   0.4547267 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 6581. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.745291  -0.7738455  0.8243029  0.6874447]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 6582. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.7701107  0.8877206  0.84882903 0.61897194]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 6583. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.86744547  0.69200325  0.9522183   0.730896  ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 6584. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.62108326 0.16368032 0.9547019  0.60553277]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 6585. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.30075014 -0.84703094  0.98033905  0.64575374]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 6586. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.36780465 0.94226515 0.8991568  0.6579881 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 6587. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.21277797 -0.9924122   0.92341936  0.59497   ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6588. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.5103051   0.7063612   0.98521245  0.63065636]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 6589. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.17268914  0.8503294   0.94637895  0.70841885]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 6590. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.7918892  0.22082973 0.42236757 0.72137403]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6591. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.5652727  0.8531263  0.9743345  0.74961686]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6592. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.5150354   0.53897405  0.85319746  0.51080215]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 6593. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.5827639  0.28305662 0.5170145  0.66872203]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 6594. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.24466586 -0.65108186  0.8257346   0.8579849 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 6595. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.12472498 -0.91480565  0.7403954   0.7544656 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 6596. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.1945613  -0.9568597   0.63199615  0.47206795]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 6597. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.52775776 0.7106675  0.9002744  0.64271235]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 6598. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.63272667 -0.86177903  0.9628974   0.5255543 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 6599. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.7350018 -0.9584521  0.6332176  0.6753154]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 6600. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.19137168 -0.845878    0.84443665  0.7501602 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 6601. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.10069668 -0.761824    0.96916974  0.72216654]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 6602. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.5489737  0.58557975 0.8819667  0.6830375 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6603. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.55459833 -0.81878066  0.8290074   0.68552125]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 6604. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.30729675 -0.2592194   0.82558763  0.63203   ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 6605. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.6874902  0.92007995 0.94464445 0.64786077]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 6606. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.39114594 0.9065535  0.44088125 0.6865809 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 6607. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.44327855 -0.24495912  0.8863013   0.6902623 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 6608. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.28618622 -0.25165665  0.76865757  0.39749575]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 6609. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.41189575 -0.1349327   0.35143352  0.71967006]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 6610. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.14558125 0.96616673 0.7989569  0.6991223 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 6611. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.73041296 -0.9842911   0.8285774   0.77157784]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 6612. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.5985259  -0.29452157  0.5617249   0.7175007 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 6613. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.27656674 0.9231603  0.59427226 0.7986958 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 6614. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.06038809 0.7204807  0.9442129  0.80863047]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 6615. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.9200356  0.5043808  0.67468286 0.59428906]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 6616. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.75394464 0.1907146  0.55182624 0.63513017]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 6617. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.01179188 -0.24414384  0.8331559   0.60699606]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 6618. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.21253788 0.5235131  0.3633132  0.6535847 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 6619. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.14061868 -0.6595572   0.8994725   0.6728132 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 6620. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.02380931 -0.7336854   0.4451673   0.7530656 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 6621. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[-0.13071269 -0.856223    0.88913846  0.6324086 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 6622. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.26364613 -0.97519624  0.9224111   0.7118896 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 6623. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.428447   0.42830575 0.9392812  0.6814097 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 6624. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.7785876  0.10678899 0.76031184 0.69001603]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 6625. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.14510095 0.28046632 0.78226364 0.6731696 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 6626. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[0.8507805  0.79629755 0.72072816 0.49301875]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 6627. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.39602935 -0.9408301   0.8251257   0.70318127]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6628. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.12204766 -0.69322515  0.3089192   0.5362663 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 6629. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.5610907  -0.10471243  0.5011854   0.6475111 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 6630. State = [[-0.07810499 -0.19492061  0.40319777  1.        ]]. Action = [[ 0.17208219 -0.8707133   0.27290916  0.5921707 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 6631. State = [[-0.08256344 -0.20473485  0.40551755  1.        ]]. Action = [[-0.44218063 -0.62394804  0.13847244  0.518674  ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 6632. State = [[-0.08841414 -0.21377224  0.40785697  1.        ]]. Action = [[ 0.23986602 -0.628631    0.57512736  0.68220246]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6633. State = [[-0.08959391 -0.21529774  0.40821135  1.        ]]. Action = [[ 0.2903092  -0.1117245   0.67692614  0.47732234]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6634. State = [[-0.09020681 -0.21601558  0.4083892   1.        ]]. Action = [[0.07952476 0.47765362 0.87726474 0.6906264 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6635. State = [[-0.09039927 -0.21620928  0.408439    1.        ]]. Action = [[0.6254902  0.75625193 0.83726025 0.6716819 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6636. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[ 0.44614983 -0.41997713  0.7956872   0.61059904]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 6637. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[0.29525006 0.22513366 0.7388017  0.7362156 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6638. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[0.54287696 0.66697466 0.8564271  0.69706726]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6639. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[ 0.26819825 -0.91868883  0.7902019   0.6757798 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6640. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[-0.28592443 -0.5273534   0.60071135  0.6077745 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6641. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[-0.17588747 -0.43974388  0.5969224   0.77475333]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6642. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[-8.42273235e-04 -1.03040695e-01  8.77834439e-01  5.77167630e-01]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 6643. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[ 0.46783495 -0.9661781   0.7095244   0.6214354 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6644. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[0.66625    0.41099656 0.96144164 0.5536685 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6645. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[ 0.6904125  -0.5405551   0.75665593  0.6546881 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6646. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[ 0.1716497  -0.84209853  0.90649986  0.6621096 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6647. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[-0.20005256  0.46668494  0.8364644   0.8024678 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 6648. State = [[-0.09060942 -0.2166192   0.40853304  1.        ]]. Action = [[ 0.5756993 -0.0279941  0.5563643  0.6300931]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6649. State = [[-0.09066958 -0.21673638  0.40855995  1.        ]]. Action = [[ 0.25916874 -0.5199121   0.7786219   0.48106265]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6650. State = [[-0.09066958 -0.21673638  0.40855995  1.        ]]. Action = [[-0.27207488  0.4880327   0.7813766   0.69009304]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6651. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[ 0.1224103  -0.59386396  0.97414136  0.68192387]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6652. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[ 0.5532658  -0.46221602  0.7162361   0.73060036]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6653. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[ 0.717371   -0.08101976  0.960395    0.68071055]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 6654. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[ 0.25753593 -0.5250837   0.8997979   0.64623475]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6655. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[0.83288646 0.8215058  0.9534154  0.5088819 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 6656. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[ 0.01023161 -0.98962367  0.48679876  0.7747586 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 6657. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[-0.3196218   0.64030004  0.91945577  0.6698942 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6658. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[-0.6673529  -0.05529726  0.8808384   0.7191237 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6659. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[ 0.8808236  -0.38255048  0.7839196   0.7182803 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6660. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[-0.3365445   0.46891177  0.86571145  0.75765586]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 6661. State = [[-0.09069967 -0.21679497  0.40857342  1.        ]]. Action = [[-0.13200897 -0.0144968   0.32965386  0.7139983 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 6662. State = [[-0.2551054  -0.06517866  0.10811075  1.        ]]. Action = [[ 0.51508164 -0.3780372   0.8822119   0.7456219 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 6663. State = [[-0.24752156 -0.0870069   0.10058121  1.        ]]. Action = [[ 0.5280645  -0.86248195  0.86609113  0.16897428]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6664. State = [[-0.23184341 -0.11183347  0.11972754  1.        ]]. Action = [[ 0.53972054 -0.53011787  0.89634037  0.18940437]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6665. State = [[-0.20891607 -0.13310494  0.1541337   1.        ]]. Action = [[ 0.62283874 -0.44241214  0.86368585  0.17720497]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6666. State = [[-0.18077739 -0.15747783  0.1891116   1.        ]]. Action = [[ 0.7421832  -0.82308245  0.81385994  0.34376657]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6667. State = [[-0.16244914 -0.17560132  0.20978977  1.        ]]. Action = [[0.83493614 0.03691924 0.38017905 0.501178  ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 6668. State = [[-0.1551338  -0.19297919  0.21813986  1.        ]]. Action = [[ 0.27909565 -0.9233432   0.12373161  0.31392848]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6669. State = [[-0.14865683 -0.20739655  0.22638698  1.        ]]. Action = [[0.15535545 0.25306952 0.39168096 0.49445248]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6670. State = [[-0.13750577 -0.20829092  0.2446816   1.        ]]. Action = [[0.13271642 0.05593348 0.73010874 0.58605814]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6671. State = [[-0.12994047 -0.21968415  0.26905406  1.        ]]. Action = [[ 0.2846619  -0.748774    0.54504704  0.81984127]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6672. State = [[-0.11377843 -0.23212606  0.29545224  1.        ]]. Action = [[ 0.6546619  -0.03585029  0.72034     0.7301836 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6673. State = [[-0.09541176 -0.24320956  0.32657883  1.        ]]. Action = [[ 0.32683945 -0.44998765  0.8134754   0.725121  ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 6674. State = [[-0.0776451  -0.24204527  0.3560459   1.        ]]. Action = [[0.5657649  0.6831949  0.51817906 0.8532145 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 6675. State = [[-0.05662252 -0.22564553  0.3854029   1.        ]]. Action = [[0.6432109  0.5159552  0.8688173  0.67894673]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 6676. State = [[-0.04405501 -0.21480973  0.40821648  1.        ]]. Action = [[0.79891074 0.97836053 0.8570626  0.7219231 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 6677. State = [[-0.0407433  -0.21357843  0.41156933  1.        ]]. Action = [[0.9541974 0.7438934 0.8647642 0.778178 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 6678. State = [[-0.04069389 -0.21286786  0.41175574  1.        ]]. Action = [[0.77333    0.10264266 0.90435386 0.7581625 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 6679. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.17149258  0.7987838   0.8344215   0.67390835]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 6680. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.3323319  0.18349504 0.7739563  0.7034738 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 6681. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.94028234 -0.11351001  0.58156276  0.6885637 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 6682. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.282022   0.60470414 0.5737505  0.68548894]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 6683. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.8119528  -0.5106165   0.7578788   0.57152605]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 6684. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.24593997 0.52346087 0.5883678  0.68399596]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 6685. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.6255152  -0.7294917   0.8750174   0.72413146]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 6686. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.79776275 0.4303459  0.8861332  0.75412035]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 6687. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.9336815  0.4948038  0.8306339  0.8167231]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 6688. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.1169132  -0.8664477   0.36851597  0.78052235]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 6689. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.41726065 0.8959626  0.88325644 0.76789594]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6690. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.53248036 -0.20125574  0.7122171   0.65074766]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 6691. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.01613486 0.6226752  0.7468176  0.69593596]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 6692. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.76346517 0.3552022  0.87575173 0.7501228 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6693. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.08458805 0.09522104 0.4243269  0.69411194]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6694. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.25334454 0.5212555  0.5903373  0.80822396]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 6695. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.43471658 -0.27992898  0.56524694  0.7824638 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 6696. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.6528406  0.41994405 0.9569936  0.72974384]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 6697. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.91538906 -0.9594625   0.60058224  0.70975614]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 6698. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.7289815  -0.66143715  0.6778629   0.762274  ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 6699. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.39289534 -0.78550565  0.9777341   0.47395968]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 6700. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.35842466 0.03923678 0.9840019  0.8557589 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 6701. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.36351848 -0.05331928  0.44166481  0.64643836]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 6702. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.7460618  -0.81955194  0.86783934  0.7166779 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 6703. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.50748754 0.3165655  0.7841047  0.6159177 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 6704. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.09646696 -0.8233246   0.82252586  0.652477  ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6705. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.6404705  -0.77792466  0.43694222  0.7169874 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 6706. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.775215   0.34586132 0.8110974  0.7676134 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 6707. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.09191996  0.6648351   0.818398    0.72131324]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 6708. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.44064045 -0.4005053   0.7475772   0.80034316]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 6709. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.61909246 -0.9832719   0.9437516   0.8369026 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 6710. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.01018512 0.69791746 0.7104747  0.83062077]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 6711. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.3929479  0.85274184 0.92498577 0.74929786]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 6712. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.20966494 0.9028959  0.66867566 0.73961234]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 6713. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.24184978 0.6344197  0.79286134 0.64422035]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 6714. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.20455813 0.77923536 0.44111907 0.52222836]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 6715. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.71267605 0.9977565  0.92660904 0.67217565]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 6716. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.71054053 -0.3018409   0.84205186  0.6616423 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 6717. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.00513899 -0.61026317  0.81731117  0.74874234]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 6718. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.89517426 0.48203444 0.3681506  0.7704475 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 6719. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.17465585  0.21643686  0.7049439   0.6606405 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 6720. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.76318026 -0.1938808   0.7465985   0.63954544]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 6721. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.5807431  -0.8163304   0.8495383   0.73881006]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 6722. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.51137257 -0.43314242  0.4495859   0.8747523 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 6723. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.5160105  -0.9594442   0.76749206  0.6443747 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 6724. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.8437934  -0.85169214  0.8392345   0.76873183]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 6725. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.81703615 0.25495768 0.9238286  0.72597384]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 6726. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.64903677 0.8444427  0.6592387  0.78331494]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 6727. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.904214   -0.35179436  0.87498903  0.61921227]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 6728. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.4597633  0.9081762  0.8527515  0.725824 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 6729. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.4924755  -0.48332763  0.9249716   0.7146735 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6730. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.48699617 -0.8714499   0.52162135  0.7961514 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 6731. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.25710857 0.01692379 0.86234164 0.32626247]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 6732. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.30525136 0.20514369 0.8179498  0.7476113 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 6733. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.22722018 -0.784053    0.5229757   0.69177055]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 6734. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.40477943 -0.84579736  0.8438432   0.7949631 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6735. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.41264892 -0.62984824  0.7902925   0.8331207 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6736. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.6720911  -0.18385553  0.80681384  0.77401197]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6737. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.74901557 -0.9168668   0.8800731   0.6371398 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6738. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.09928322 0.2842437  0.8262676  0.81809235]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 6739. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.06051862 0.76890373 0.15233803 0.66323376]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6740. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.3754598 0.8511882 0.8944974 0.7317381]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6741. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.7368343 -0.6777698  0.8689282  0.6829424]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6742. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.42953396 -0.96816987  0.9074936   0.7005906 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6743. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.6060822  -0.8854244   0.54219544  0.59021974]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6744. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.560748   -0.69751     0.7000232   0.74110985]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 6745. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.8018503  0.3654859  0.87072873 0.67883873]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6746. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.7687161  -0.81303644  0.6748686   0.7452121 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6747. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.01996785 -0.77759105  0.86287594  0.7200495 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6748. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.20481157 0.4914546  0.6816139  0.59337616]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6749. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.763807   -0.83293486  0.7566581   0.74834657]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 6750. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.34966898 -0.4504249   0.7146766   0.6727245 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6751. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.64061105 -0.854719    0.89857423  0.7141626 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6752. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.8002603  -0.997911    0.47934878  0.66687834]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6753. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.33491015 0.85261977 0.5748377  0.79516053]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6754. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.43324506  0.26798105  0.76415884  0.61320615]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6755. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[-0.5568459  -0.79811585  0.35526168  0.82853174]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 6756. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[0.83303225 0.7855334  0.49600804 0.5588883 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6757. State = [[-0.04066635 -0.21280743  0.41174373  1.        ]]. Action = [[ 0.3811369  -0.75150454  0.707206    0.58975434]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 6758. State = [[-0.04075757 -0.2114879   0.41173026  1.        ]]. Action = [[-0.0665651   0.12197232  0.08125138  0.86441636]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 6759. State = [[-0.04094815 -0.21004719  0.41169748  1.        ]]. Action = [[-5.3042173e-04 -9.4473749e-01  7.7894258e-01  8.2958782e-01]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6760. State = [[-0.04130458 -0.20937695  0.41167364  1.        ]]. Action = [[ 0.34603703 -0.5585802   0.66162944  0.6850287 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6761. State = [[-0.04133181 -0.2094374   0.41168562  1.        ]]. Action = [[-0.09628868  0.8849764   0.43023622  0.6859889 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6762. State = [[-0.04132625 -0.20936984  0.4116856   1.        ]]. Action = [[0.18660116 0.6822679  0.6933881  0.866678  ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 6763. State = [[-0.04129903 -0.2093094   0.4116736   1.        ]]. Action = [[0.2009759  0.4907136  0.96499753 0.7438005 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 6764. State = [[-0.26010883 -0.17335294  0.11241204  1.        ]]. Action = [[0.60835826 0.9251499  0.6511426  0.67155886]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 6765. State = [[-0.25300995 -0.20289761  0.10417565  1.        ]]. Action = [[ 0.58760464 -0.6890098   0.8253658   0.24279547]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6766. State = [[-0.23903514 -0.22629811  0.12097537  1.        ]]. Action = [[ 0.37322915 -0.6917151   0.93595386  0.22028911]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6767. State = [[-0.2191171  -0.24765913  0.15780076  1.        ]]. Action = [[ 0.52546096 -0.3258928   0.9320402   0.3787918 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6768. State = [[-0.20051096 -0.26649398  0.19378752  1.        ]]. Action = [[ 0.27631724 -0.5750681   0.8482703   0.5082927 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6769. State = [[-0.1894119  -0.28631002  0.22666267  1.        ]]. Action = [[ 0.0820843 -0.2778734  0.7528143  0.515985 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 6770. State = [[-0.1781023  -0.29754868  0.2549451   1.        ]]. Action = [[ 0.46153045 -0.13363189  0.66918635  0.7071916 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6771. State = [[-0.15467441 -0.30411097  0.28788677  1.        ]]. Action = [[ 0.9027822  -0.08766216  0.9615016   0.7018062 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6772. State = [[-0.13455458 -0.30978632  0.3144995   1.        ]]. Action = [[ 0.7844398  -0.5876034   0.870916    0.57814336]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 6773. State = [[-0.12008108 -0.3033056   0.32588866  1.        ]]. Action = [[0.7771809  0.40127993 0.49441016 0.8399912 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6774. State = [[-0.10565542 -0.28173214  0.3372708   1.        ]]. Action = [[-0.14842826  0.9997804  -0.14302838  0.3720187 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6775. State = [[-0.08968202 -0.24765487  0.3443191   1.        ]]. Action = [[0.9978088  0.7953081  0.49570942 0.9200189 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 6776. State = [[-0.06643917 -0.2145262   0.36813247  1.        ]]. Action = [[0.44922614 0.84396553 0.8996155  0.60822976]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 6777. State = [[-0.04498959 -0.20405285  0.40073156  1.        ]]. Action = [[ 0.8003442  -0.6354248   0.6911671   0.49360013]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 6778. State = [[-0.02702474 -0.20982082  0.41960806  1.        ]]. Action = [[0.32222462 0.27698994 0.749624   0.7730434 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 6779. State = [[-0.02413257 -0.21093972  0.42111638  1.        ]]. Action = [[ 0.8361685  -0.08363664  0.96966314  0.6481931 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 6780. State = [[-0.02411773 -0.21102887  0.4215795   1.        ]]. Action = [[ 0.8640411  -0.9432563   0.7984439   0.79491854]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 6781. State = [[-0.02117213 -0.21170521  0.4241716   1.        ]]. Action = [[ 0.5176599  -0.44796968  0.7061653   0.6604506 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 6782. State = [[-0.02106636 -0.21155263  0.42458183  1.        ]]. Action = [[-0.2113806   0.79071045  0.9671693   0.67526877]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 6783. State = [[-0.02106636 -0.21155263  0.42458183  1.        ]]. Action = [[0.04942524 0.93727183 0.6902888  0.69474554]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 6784. State = [[-0.02106636 -0.21155263  0.42458183  1.        ]]. Action = [[-0.15881461  0.94829     0.9534714   0.4980662 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 6785. State = [[-0.02116826 -0.21150906  0.42457572  1.        ]]. Action = [[ 0.6856694  -0.4262358   0.8872156   0.66587496]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 6786. State = [[-0.02116826 -0.21150906  0.42457572  1.        ]]. Action = [[ 0.84213936 -0.79490626  0.8607874   0.5770302 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 6787. State = [[-0.02116826 -0.21150906  0.42457572  1.        ]]. Action = [[-0.7859046   0.8040979   0.41601372  0.78282046]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 6788. State = [[-0.02124454 -0.21147645  0.42457125  1.        ]]. Action = [[-0.6151681 -0.9221018  0.4662156  0.7786579]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 6789. State = [[-0.02115049 -0.21149072  0.42467964  1.        ]]. Action = [[0.7761116  0.73658633 0.43436372 0.83295727]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 6790. State = [[-0.02127767 -0.21143636  0.42467222  1.        ]]. Action = [[0.53663933 0.8794539  0.90733504 0.7308214 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 6791. State = [[-0.02127767 -0.21143636  0.42467222  1.        ]]. Action = [[-0.36687696  0.02572632  0.9459243   0.69855154]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6792. State = [[-0.02120656 -0.21145104  0.42478445  1.        ]]. Action = [[-0.05813795 -0.31512916  0.6861627   0.66693425]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 6793. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.4875692   0.67480624  0.9767182   0.72703266]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 6794. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[0.6639832  0.7602643  0.60189843 0.7332181 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6795. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[0.38899088 0.21592331 0.44122815 0.7364284 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6796. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.6804204  -0.9285713   0.6340239   0.62064064]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 6797. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[0.10358894 0.1801809  0.71529627 0.8094995 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 6798. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.87114066  0.48197126  0.5728407   0.7620392 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 6799. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.5044577   0.81169367  0.96772456  0.77169764]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 6800. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.42600548 -0.05780452  0.6125071   0.61867213]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 6801. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.63875926 -0.9597543   0.55872464  0.73403645]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 6802. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.62834495 -0.19688123  0.5847899   0.70598054]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 6803. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.15591824  0.15703893  0.9428339   0.67508984]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 6804. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.01152736  0.63571286  0.92070913  0.7151277 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 6805. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.44078767 -0.6477052   0.7741072   0.79229486]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 6806. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.04858923 -0.1496104   0.8752732   0.56975603]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6807. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[0.07364011 0.93359756 0.7319368  0.67901754]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 6808. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[-0.74995136 -0.9756808   0.8264357   0.7978461 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 6809. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.93624544 -0.58389014  0.88990974  0.7501308 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 6810. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.94258237 -0.05242342  0.9007027   0.66733   ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 6811. State = [[-0.02115236 -0.21146248  0.4247179   1.        ]]. Action = [[ 0.25442958 -0.34907544  0.7030139   0.8055937 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 6812. State = [[-0.02194535 -0.22413927  0.41912395  1.        ]]. Action = [[ 0.22926903 -0.839068   -0.35559374  0.7515665 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 6813. State = [[-0.0141246  -0.2382155   0.41186592  1.        ]]. Action = [[0.65675056 0.96965814 0.7687144  0.8318329 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 6814. State = [[-0.01452708 -0.24005114  0.4115157   1.        ]]. Action = [[-0.47227466 -0.34302342  0.70017743  0.7614536 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 6815. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.89076495 0.755291   0.55743086 0.7778909 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 6816. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.4565606   0.98561907  0.56351805  0.7340071 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 6817. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.7713773  0.4664054  0.23713362 0.8875551 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 6818. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.77662015 0.98477507 0.92336607 0.8876151 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 6819. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[ 0.6608343 -0.6830161  0.9627247  0.7214606]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 6820. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.05569243 0.23235643 0.5908499  0.8948605 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 6821. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.4590839 -0.7571666  0.8901886  0.8725493]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 6822. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.56409055  0.1959902   0.7718259   0.72436345]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 6823. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[ 0.65854347 -0.32352734  0.664896    0.74009967]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 6824. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.7984959  0.99469995 0.86182725 0.65253854]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 6825. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.15240443  0.9544718   0.7969339   0.8976276 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 6826. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.13342929  0.96156955  0.9028735   0.7218394 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 6827. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.1969797   0.9746065   0.47802532  0.71576166]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 6828. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.7112137  0.6805419  0.48566628 0.60915494]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 6829. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.64626455 0.29488826 0.43897414 0.72246075]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 6830. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.65300333  0.71368575  0.17310297  0.5771532 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 6831. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[0.7338953  0.27392554 0.32039428 0.8353667 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6832. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.33788294  0.9683745   0.62715304  0.9116225 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 6833. State = [[-0.01467324 -0.24072379  0.41154656  1.        ]]. Action = [[-0.07376003 -0.81832767  0.28267813  0.8094604 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 6834. State = [[-0.01229703 -0.2538089   0.4120739   1.        ]]. Action = [[ 0.3346784  -0.82130057  0.07625127  0.77453923]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 6835. State = [[ 0.00644653 -0.261036    0.41158026  1.        ]]. Action = [[ 0.7655524   0.5795226  -0.04652137  0.7743031 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 6836. State = [[ 0.02383677 -0.258701    0.41147175  1.        ]]. Action = [[0.32708466 0.8102597  0.21414709 0.9575449 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6837. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.649632   0.9985256  0.73517394 0.88777137]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6838. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.6248641 0.9770942 0.7697363 0.5935185]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6839. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.49842644 0.654192   0.95515513 0.89031935]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6840. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[-0.75573486 -0.72565174  0.7725558   0.80487084]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 6841. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[ 0.8773651  -0.91172457  0.59346604  0.7074239 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6842. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[-0.5914505   0.9963454   0.46054602  0.84545946]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6843. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[-0.9766472  -0.30720544  0.7027924   0.85294247]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6844. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.9069694  0.8853824  0.56716394 0.8105407 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6845. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.5765512  0.7919638  0.42905235 0.9006164 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6846. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[ 0.86192083 -0.98664963  0.8906821   0.9213493 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 6847. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.5629797  0.99980855 0.71917987 0.8700068 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6848. State = [[ 0.02918509 -0.25874257  0.41276684  1.        ]]. Action = [[0.62944484 0.9444542  0.2446574  0.68912697]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6849. State = [[ 0.03152002 -0.26762596  0.41289064  1.        ]]. Action = [[ 0.8729696  -0.8429138  -0.08533061  0.63952017]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 6850. State = [[ 0.05286933 -0.28072593  0.40588742  1.        ]]. Action = [[ 0.8580034  -0.5954673   0.16943538  0.9040208 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6851. State = [[ 0.06364523 -0.27315232  0.40606344  1.        ]]. Action = [[-0.2540717   0.87571096 -0.20949924  0.91837895]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 6852. State = [[ 0.06667434 -0.2700948   0.40309858  1.        ]]. Action = [[ 0.6421995  -0.71984947  0.02421129  0.8160949 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 6853. State = [[ 0.07856842 -0.27695867  0.399592    1.        ]]. Action = [[0.9784709  0.8990582  0.09963882 0.7098868 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6854. State = [[ 0.08231223 -0.27793208  0.40118095  1.        ]]. Action = [[ 0.6675854  0.8951857 -0.4486239  0.9065485]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6855. State = [[ 0.08171104 -0.2665663   0.39525306  1.        ]]. Action = [[-0.63360333  0.97749627 -0.73808753  0.6735544 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 6856. State = [[ 0.07566502 -0.24264768  0.39075693  1.        ]]. Action = [[-0.9390827   0.8114468  -0.01403785  0.8902501 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 6857. State = [[ 0.05751558 -0.22831027  0.38447973  1.        ]]. Action = [[-0.20571405 -0.23354578 -0.16579223  0.4818715 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 6858. State = [[ 0.05286214 -0.21749093  0.38676646  1.        ]]. Action = [[0.16470373 0.6771591  0.5422096  0.9015217 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 6859. State = [[ 0.05108815 -0.20383604  0.39662763  1.        ]]. Action = [[ 0.41277826 -0.3500415   0.8149649   0.68172574]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 6860. State = [[ 0.05070134 -0.20182653  0.39896527  1.        ]]. Action = [[0.03052056 0.3408351  0.8001423  0.6909267 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 6861. State = [[ 0.05090186 -0.2016435   0.399351    1.        ]]. Action = [[-0.31839406  0.08590221  0.85458827  0.34603858]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6862. State = [[ 0.05090186 -0.2016435   0.399351    1.        ]]. Action = [[-0.44721925 -0.40072912  0.8806772   0.74711144]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6863. State = [[ 0.05090186 -0.2016435   0.399351    1.        ]]. Action = [[ 0.7841984  -0.39036041  0.63795424  0.82398677]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6864. State = [[ 0.05090186 -0.2016435   0.399351    1.        ]]. Action = [[ 0.16479278 -0.78910476  0.8516115   0.6175823 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 6865. State = [[ 0.05207747 -0.21058737  0.4007654   1.        ]]. Action = [[ 0.7973962  -0.7984223   0.26828218  0.85504603]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 6866. State = [[-0.27120528  0.11933033  0.11424923  1.        ]]. Action = [[ 0.45004058 -0.27824718  0.69545555  0.7405913 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 6867. State = [[-0.25667313  0.12298045  0.10764674  1.        ]]. Action = [[ 0.81409645 -0.70992666  0.910848    0.1675812 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6868. State = [[-0.22856914  0.10382818  0.13092573  1.        ]]. Action = [[ 0.80502915 -0.5165631   0.9414909   0.10223925]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 6869. State = [[-0.19940141  0.08000891  0.16574231  1.        ]]. Action = [[ 0.6160364  -0.8955521   0.9175024   0.27537656]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6870. State = [[-0.17567483  0.05045055  0.19631822  1.        ]]. Action = [[ 0.43595552 -0.7012724   0.39885306  0.39870226]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6871. State = [[-0.16428912  0.0366042   0.20830145  1.        ]]. Action = [[ 0.77192354 -0.5416051   0.5720761   0.23770893]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 6872. State = [[-0.1570943   0.02559171  0.22070618  1.        ]]. Action = [[ 0.29662728 -0.43801224  0.7753167   0.35383797]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6873. State = [[-0.14111693  0.00150398  0.25227347  1.        ]]. Action = [[ 0.4145534 -0.8577889  0.9255452  0.3590814]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6874. State = [[-0.12419202 -0.02501995  0.28254604  1.        ]]. Action = [[ 0.5901239  -0.5821046   0.35898757  0.27037168]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 6875. State = [[-0.10857359 -0.05071903  0.31000137  1.        ]]. Action = [[ 0.21373796 -0.66665477  0.9560709   0.33838725]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 6876. State = [[-0.09432667 -0.07853108  0.3453059   1.        ]]. Action = [[ 0.52686405 -0.78753585  0.8367944   0.41296363]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 6877. State = [[-0.08157376 -0.10836985  0.3801643   1.        ]]. Action = [[ 0.27878952 -0.8021582   0.8912295   0.35000944]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 6878. State = [[-0.07241292 -0.12648138  0.4027      1.        ]]. Action = [[ 0.7316872 -0.6856516  0.7526411  0.5100727]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 6879. State = [[-0.07039773 -0.12845865  0.4062661   1.        ]]. Action = [[ 0.4707452  -0.59501606  0.7006941   0.4587282 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 6880. State = [[-0.07048047 -0.12854092  0.4063756   1.        ]]. Action = [[0.3829534  0.01983631 0.8251853  0.5214257 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 6881. State = [[-0.07048047 -0.12854092  0.4063756   1.        ]]. Action = [[ 0.3406421  -0.74492365  0.5395714   0.5544982 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 6882. State = [[-0.07048047 -0.12854092  0.4063756   1.        ]]. Action = [[ 0.11604404 -0.8829361   0.91414523  0.45623183]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 6883. State = [[-0.07048047 -0.12854092  0.4063756   1.        ]]. Action = [[ 0.00347352 -0.7881769   0.83033717  0.45569885]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 6884. State = [[-0.07048047 -0.12854092  0.4063756   1.        ]]. Action = [[ 0.4744302  -0.8119222   0.85415757  0.5867095 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 6885. State = [[-0.070555   -0.12850459  0.40637034  1.        ]]. Action = [[-0.06375217 -0.74483347  0.93718827  0.40668368]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 6886. State = [[-0.070555   -0.12850459  0.40637034  1.        ]]. Action = [[ 0.02326477 -0.66990554  0.87580967  0.48124993]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 6887. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.3107226  -0.97159183  0.9043417   0.4866675 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 6888. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.12177348 -0.96567804  0.9096246   0.42211223]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 6889. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.3148346  -0.9449357   0.50442576  0.46967673]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 6890. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.03841656 -0.58960164  0.8401587   0.32923102]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 6891. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.15409362 -0.78529465  0.8535466   0.39819062]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 6892. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.10567844 -0.64052814  0.78506756  0.48889744]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 6893. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.10800993 -0.78101355  0.9503586   0.37028074]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 6894. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.21872729 -0.6775961   0.506305    0.25465918]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 6895. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.012963    0.28613782  0.79397225  0.47393417]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 6896. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.00979888 -0.95036966  0.83988845  0.44074988]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 6897. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.11201704 -0.47600067  0.5789325   0.39656162]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 6898. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.6750617  -0.38272     0.89395976  0.39323747]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 6899. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.19039083 -0.4336112   0.88708913  0.47139907]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 6900. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.29927647 -0.2670995   0.86276174  0.4123423 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 6901. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.56627417 -0.01921308  0.8513174   0.40261936]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 6902. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.01090264 -0.9705541   0.8113482   0.34259868]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 6903. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.0214349  -0.9594979   0.9225218   0.41596925]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 6904. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[0.39974082 0.33013308 0.83239603 0.18787539]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 6905. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.34250784 -0.6053614   0.9078524   0.4898008 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 6906. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.00797433 -0.760542    0.90337896  0.4780922 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 6907. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.4840151  -0.47229797  0.8119426   0.5462973 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 6908. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.0459348  -0.7058859   0.88519716  0.39993513]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 6909. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.0360257  -0.81871194  0.9324217   0.54016685]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 6910. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.39450192 -0.8590397   0.8178499   0.4576639 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 6911. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.3273474  -0.50293595  0.75169444  0.4668677 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 6912. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.58286464 -0.60962325  0.68045497  0.49008536]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 6913. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.17123961 -0.37239778  0.94161475  0.41099238]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 6914. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.29837918 -0.18868971  0.8204392   0.52289414]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 6915. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.39725506 -0.93107074  0.89843535  0.4595642 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 6916. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.29974174 -0.82289463  0.7028668   0.5556054 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 6917. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.03638256 -0.7143533   0.7013333   0.47165608]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 6918. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.4641279  -0.50523674  0.9026475   0.4709065 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 6919. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.39083004 -0.9480259   0.7631626   0.49337184]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 6920. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.00633782 -0.12798369  0.9212407   0.471542  ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 6921. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.4441707  -0.43427968  0.91044664  0.5146394 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 6922. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.01459146 -0.15143847  0.9706917   0.589085  ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 6923. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.52141595 -0.8599473   0.8380399   0.49419022]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 6924. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.1011883  -0.6966971   0.8506987   0.43977308]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 6925. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.481786   -0.5257131   0.89996266  0.4903834 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 6926. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.58333945 -0.73235583  0.8629292   0.41158354]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 6927. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.02685368 -0.8367078   0.6146672   0.5035417 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 6928. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.6586633  -0.45179677  0.7573421   0.22835934]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 6929. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.23984313 -0.30612665  0.90338945  0.50716496]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 6930. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.15162206 -0.7912415   0.8672402   0.55405855]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 6931. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.07833457 -0.6992068   0.64941716  0.54980063]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 6932. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.4657657  -0.59686476  0.8884845   0.4828453 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 6933. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.34775865 -0.92805344  0.43405855  0.474841  ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 6934. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.25511217 -0.66844153  0.7110641   0.35472023]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 6935. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.03034472 -0.62663126  0.6227031   0.39168644]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 6936. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.08753884 -0.8298067   0.65536594  0.51804066]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 6937. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.28818643 -0.5807949   0.87955666  0.48222458]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 6938. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.2576065  -0.6707018   0.81062555  0.424253  ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 6939. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.12905288 -0.9593945   0.8506222   0.45643377]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 6940. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.41269696 -0.35765535  0.8733363   0.5395678 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 6941. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.30061853 -0.83955544  0.9323852   0.39981675]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 6942. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.09546483 -0.9071275   0.8233974   0.41945803]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 6943. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.10642076 -0.8250521   0.60579085  0.48453188]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 6944. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.29754174 -0.68145835  0.8344481   0.48539388]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 6945. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.285591   -0.81769353  0.7564707   0.3997208 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 6946. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.34460115 -0.9344361   0.8684428   0.4624138 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 6947. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.33348614 -0.8458355   0.8469846   0.37782156]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 6948. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.06691605 -0.29493123  0.6774684   0.4782406 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 6949. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.5498184  -0.74579144  0.88005185  0.38290846]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 6950. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.5815909  -0.95177937  0.78215873  0.52722204]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 6951. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.2677486  -0.72804755  0.82227254  0.5469458 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 6952. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.62360597 -0.63643616  0.78868663  0.5848005 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 6953. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.22637522 -0.9324983   0.8843076   0.38735127]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 6954. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.6222162  -0.85341966  0.85519767  0.41304898]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 6955. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.3281603  -0.69621193  0.8881674   0.4053048 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 6956. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.71516466 -0.6954233   0.5205488   0.39318657]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 6957. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.07968313 -0.6113301   0.89027643  0.42111278]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 6958. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.50036156 -0.0709902   0.92174506  0.44217443]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 6959. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.22107542 -0.85328954  0.703642    0.45180035]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 6960. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.21463847 -0.89868516  0.83852386  0.43678188]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 6961. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.49275136 -0.8510288   0.90701437  0.35932326]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 6962. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[0.20336127 0.08089614 0.9266634  0.43532693]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 6963. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.09138608 -0.23877978  0.9380727   0.43910742]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 6964. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.62539077 -0.8370141   0.9173206   0.46897495]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 6965. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.09827757 -0.9710567   0.7408166   0.49677658]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 6966. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[-0.18974245 -0.74015963  0.9531512   0.5083606 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 6967. State = [[-0.07057992 -0.12849243  0.4063686   1.        ]]. Action = [[ 0.3385141 -0.4231087  0.9441894  0.5122838]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 6968. State = [[-0.26633912  0.17399988  0.11422119  1.        ]]. Action = [[-0.265625   0.1034739  0.829136   0.5032823]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 6969. State = [[-0.2553428   0.18452774  0.10069387  1.        ]]. Action = [[ 0.40482473 -0.64687663  0.44276977  0.30704045]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 6970. State = [[-0.24896365  0.17316942  0.10261881  1.        ]]. Action = [[-0.76872504 -0.8892092  -0.7738887   0.32195902]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 6971. State = [[-0.24988613  0.15832394  0.09556275  1.        ]]. Action = [[-0.30458838 -0.9362683  -0.83776504  0.45123804]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 6972. State = [[-0.24754156  0.1302605   0.08711061  1.        ]]. Action = [[ 0.0305649  -0.89549994  0.27123976  0.352283  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 6973. State = [[-0.23455708  0.09763708  0.09822059  1.        ]]. Action = [[ 0.68992734 -0.88716006  0.88488364  0.22998261]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 6974. State = [[-0.21440937  0.06444909  0.12292433  1.        ]]. Action = [[ 0.72604156 -0.8954738   0.9560385   0.15258145]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 6975. State = [[-0.18979177  0.03571553  0.1570645   1.        ]]. Action = [[ 0.74649704 -0.3892898   0.76290107  0.16820991]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 6976. State = [[-0.17486565  0.02385448  0.17690544  1.        ]]. Action = [[ 0.79419804 -0.57274354  0.8588586   0.37602854]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 6977. State = [[-0.17180344  0.02303394  0.17927542  1.        ]]. Action = [[ 0.61832607 -0.4557672   0.739496    0.3373226 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 6978. State = [[-0.17165077  0.02306547  0.17942104  1.        ]]. Action = [[ 0.40808392 -0.72202027  0.8462858   0.4949113 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 6979. State = [[-0.17137921  0.02299837  0.17960934  1.        ]]. Action = [[ 0.5058547  -0.6727227   0.6274183   0.39825308]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 6980. State = [[-0.17137921  0.02299837  0.17960934  1.        ]]. Action = [[ 0.25401187 -0.42935365  0.6322975   0.527894  ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 6981. State = [[-0.17140676  0.02293319  0.17961997  1.        ]]. Action = [[ 0.9572183 -0.4673766  0.7467948  0.4712901]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 6982. State = [[-0.17140676  0.02293319  0.17961997  1.        ]]. Action = [[ 0.8426256  -0.50565857  0.8625903   0.37343764]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 6983. State = [[-0.17140676  0.02293319  0.17961997  1.        ]]. Action = [[ 0.6571357  -0.11302131  0.6394938   0.3229692 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 6984. State = [[-0.17140676  0.02293319  0.17961997  1.        ]]. Action = [[ 0.80744314 -0.2153132   0.8859037   0.38928175]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 6985. State = [[-0.17034078  0.0197277   0.18580896  1.        ]]. Action = [[-0.01739836 -0.1482861   0.5244608   0.48330545]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 6986. State = [[-0.16834891  0.01532967  0.19616774  1.        ]]. Action = [[ 0.5493345  -0.65551186  0.8845607   0.5136486 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 6987. State = [[-0.16745456  0.01482029  0.19695815  1.        ]]. Action = [[ 0.73254347 -0.4114753   0.65625334  0.5165764 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 6988. State = [[-0.16683213  0.01493492  0.19738366  1.        ]]. Action = [[ 0.779937   -0.75727934  0.5166726   0.383875  ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 6989. State = [[-0.16632335  0.01492263  0.1978677   1.        ]]. Action = [[ 0.3430251  -0.48281503  0.8396442   0.39118063]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 6990. State = [[-0.15594584  0.01616946  0.21241559  1.        ]]. Action = [[0.7049935  0.04509783 0.95534563 0.48358905]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 6991. State = [[-0.1415282   0.01775826  0.23456042  1.        ]]. Action = [[ 0.6201196  -0.6092802   0.16346669  0.48854542]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 6992. State = [[-0.13698173  0.01170739  0.24455008  1.        ]]. Action = [[ 0.22724509 -0.30318856  0.44426775  0.3848598 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 6993. State = [[-0.12154671 -0.00748236  0.26792815  1.        ]]. Action = [[ 0.5869565 -0.8014874  0.8054956  0.3964188]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 6994. State = [[-0.10735568 -0.0370835   0.29753882  1.        ]]. Action = [[ 0.16723478 -0.8111426   0.5022216   0.36037612]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 6995. State = [[-0.09667577 -0.06649451  0.3184079   1.        ]]. Action = [[ 0.61174464 -0.73316926  0.28197885  0.3843304 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 6996. State = [[-0.07884773 -0.08442162  0.3344306   1.        ]]. Action = [[ 0.43872714 -0.05311507  0.45755315  0.4624076 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 6997. State = [[-0.0607357 -0.0972837  0.3582837  1.       ]]. Action = [[ 0.71119225 -0.61312616  0.87261367  0.32138944]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 6998. State = [[-0.03829474 -0.12599073  0.39043194  1.        ]]. Action = [[ 0.53492427 -0.9298187   0.7234824   0.44647813]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 6999. State = [[-0.02014349 -0.14548573  0.40760654  1.        ]]. Action = [[-0.37484992 -0.5574819   0.8111408   0.3999803 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 7000. State = [[-0.01766744 -0.14834473  0.41022688  1.        ]]. Action = [[ 0.00946963 -0.19016778  0.88988745  0.6870272 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 7001. State = [[-0.01764172 -0.14877753  0.41020632  1.        ]]. Action = [[ 0.31411314 -0.97060204  0.86334515  0.6502209 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 7002. State = [[-0.0176875  -0.14882429  0.41022545  1.        ]]. Action = [[0.28444123 0.01160741 0.85427606 0.6319736 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 7003. State = [[-0.01781136 -0.14876282  0.4102174   1.        ]]. Action = [[0.18699968 0.15645254 0.8533697  0.48137498]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 7004. State = [[-0.01781136 -0.14876282  0.4102174   1.        ]]. Action = [[ 0.28504968 -0.9305173   0.82654667  0.5343325 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 7005. State = [[-0.01781136 -0.14876282  0.4102174   1.        ]]. Action = [[ 0.19289553 -0.74902225  0.620402    0.6258006 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 7006. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.5403669  -0.99751985  0.47980142  0.53462505]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 7007. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.36529684 -0.5203403   0.8411441   0.525156  ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 7008. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.31739122 -0.22303706  0.7152431   0.6165433 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 7009. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.28397346 0.3612709  0.7472261  0.6113796 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 7010. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.28214705 -0.75058115  0.70685387  0.67417073]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 7011. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.03741676  0.7187464   0.8805771   0.5812086 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 7012. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.6298442 -0.2969582  0.892496   0.7076454]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 7013. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.34245765 -0.10635179  0.57638645  0.61282384]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 7014. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.20230794 0.02493012 0.9744401  0.5798402 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 7015. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.8605498  -0.42959404  0.86499774  0.6292163 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 7016. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.20374     0.32811654  0.8372679   0.43849516]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 7017. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.70327044 -0.16420358  0.7870401   0.4595697 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 7018. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.706062   -0.8670969   0.91868544  0.539346  ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 7019. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.20442605 -0.9073558   0.7963841   0.6057826 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 7020. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.55515385 0.66577506 0.71708655 0.44885242]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 7021. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.4131344  -0.38497448  0.9534185   0.5573534 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 7022. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.16739058 -0.6615178   0.48787546  0.45044374]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 7023. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.16103601 -0.8744254   0.87679434  0.46549475]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 7024. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.6869252 0.1279105 0.7174065 0.4986899]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 7025. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.0835253  -0.740765    0.97637796  0.6865146 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 7026. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.3834535  0.7545117  0.947217   0.65699077]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 7027. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.33972263 -0.03062814  0.5244967   0.6437099 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 7028. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.04401219 -0.16734564  0.9503585   0.5090121 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 7029. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.43511653 -0.76244867  0.8041761   0.7075952 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 7030. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.6382375  -0.6458187   0.90215826  0.6122587 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 7031. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.30244708 -0.8541527   0.82490134  0.40749192]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 7032. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.06477219 -0.23394442  0.7970135   0.65851355]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 7033. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.20773125 0.23396933 0.9349184  0.49502468]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 7034. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.06066859 -0.9110157   0.80610085  0.442461  ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 7035. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.36341882 0.41973615 0.5892905  0.53593504]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 7036. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.45279336 -0.36333722  0.6850152   0.49902773]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 7037. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.4811529  -0.25649923  0.9131491   0.41036582]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 7038. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.24281108 -0.68854886  0.9314871   0.5481994 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 7039. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.5343151  -0.04488015  0.922122    0.55997705]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 7040. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.44623005 -0.6762198   0.80209446  0.64575124]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 7041. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.214234   -0.36437684  0.3768053   0.61203456]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 7042. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.5301328  -0.4376546   0.7577549   0.41130865]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 7043. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.05841398  0.44961417  0.86178064  0.55199695]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 7044. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.73734975 -0.18280262  0.70477784  0.69495225]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 7045. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.07269239 -0.9792188   0.6967261   0.70547557]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 7046. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.05753005 -0.827824    0.7992691   0.4941002 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 7047. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.76313305 -0.502842    0.89942694  0.6542487 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 7048. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.12500727  0.21045768  0.79729664  0.64636827]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 7049. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.0328241   0.15649426  0.8837323   0.6163522 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 7050. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.27607    0.04256606 0.7644011  0.5987086 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 7051. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.05501992 -0.6600393   0.6306789   0.55830216]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 7052. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.76860607 -0.80826974  0.7460358   0.62950706]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 7053. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.16421354 -0.7883754   0.815922    0.67634535]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 7054. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.60646915 -0.7988199   0.9113703   0.6170654 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 7055. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.48429048 -0.24136746  0.8291271   0.5424863 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 7056. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.12338066 -0.8683127   0.68106866  0.55472374]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 7057. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.19843256  0.64845276  0.77909374  0.70009995]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 7058. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.503001   -0.9646356   0.84854984  0.6483078 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 7059. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.24852276 -0.41075885  0.6810936   0.47071087]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 7060. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.64550614  0.35841107  0.8019829   0.50236106]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 7061. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.07705986 -0.5074189   0.90022826  0.57583475]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 7062. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[0.52774906 0.4856652  0.8716762  0.7154119 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 7063. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.18423551 -0.68704015  0.5564306   0.5199938 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 7064. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.03335339 -0.5644824   0.89847183  0.49212086]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 7065. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.00635076 -0.5504741   0.618083    0.53348994]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 7066. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[-0.44984722 -0.34569407  0.6540146   0.61040163]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 7067. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.700536   -0.5969686   0.44375992  0.5457448 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 7068. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.45994973 -0.8747908   0.92617345  0.57774425]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 7069. State = [[-0.01788578 -0.1487259   0.41021264  1.        ]]. Action = [[ 0.61995256 -0.682529    0.52279377  0.3682847 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 7070. State = [[-0.2658952   0.06453746  0.11370324  1.        ]]. Action = [[-0.10938179 -0.35329878  0.9472344   0.6554978 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 7071. State = [[-0.2536064   0.06143803  0.10725139  1.        ]]. Action = [[ 0.7126931  -0.7843569   0.8571806   0.07689834]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 7072. State = [[-0.23067866  0.03629438  0.12606166  1.        ]]. Action = [[ 0.6424569  -0.851957    0.9231827   0.11254179]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 7073. State = [[-0.20210102  0.02311799  0.16212356  1.        ]]. Action = [[0.80019283 0.11904252 0.96794903 0.23950386]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 7074. State = [[-0.17399968  0.00934131  0.19544736  1.        ]]. Action = [[ 0.46122098 -0.73963624  0.6716132   0.3653189 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 7075. State = [[-0.16067867 -0.00302371  0.21387592  1.        ]]. Action = [[ 0.7461703  -0.1330021   0.81564164  0.31830823]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 7076. State = [[-0.15700147 -0.00646231  0.21775523  1.        ]]. Action = [[ 0.3254032   0.10667121 -0.09878659  0.5196254 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 7077. State = [[-0.15046358 -0.00734258  0.22441603  1.        ]]. Action = [[ 0.42857695 -0.03455102  0.42560685  0.31866074]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 7078. State = [[-0.13581474 -0.02082753  0.24150068  1.        ]]. Action = [[ 0.35494518 -0.80102205  0.5307553   0.46519887]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 7079. State = [[-0.12251028 -0.03756755  0.26038393  1.        ]]. Action = [[ 0.32028854 -0.14418983  0.30124247  0.4950819 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 7080. State = [[-0.11476527 -0.05686212  0.2813214   1.        ]]. Action = [[-0.14821404 -0.8088702   0.7856693   0.4446838 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 7081. State = [[-0.10858546 -0.07198692  0.3042514   1.        ]]. Action = [[0.4018147  0.12969148 0.4328102  0.45048726]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 7082. State = [[-0.10705424 -0.08161151  0.32771865  1.        ]]. Action = [[-0.68224984 -0.510369    0.75023174  0.48573756]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 7083. State = [[-0.11435506 -0.10100183  0.36046213  1.        ]]. Action = [[ 0.19238818 -0.53041977  0.9401908   0.412879  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 7084. State = [[-0.10982945 -0.12202922  0.39808425  1.        ]]. Action = [[ 0.36332703 -0.60128915  0.9615612   0.3967471 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 7085. State = [[-0.10562162 -0.13353936  0.42443705  1.        ]]. Action = [[-3.5253733e-01 -4.5359135e-04  8.1256413e-01  6.1648941e-01]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 7086. State = [[-0.10555419 -0.13588968  0.427998    1.        ]]. Action = [[-0.24356294 -0.09438068  0.7022313   0.52542806]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 7087. State = [[-0.10593791 -0.13685456  0.42832008  1.        ]]. Action = [[ 0.30838525 -0.07495672  0.8778379   0.64933515]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 7088. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.19094145  0.38254702  0.94032407  0.4999975 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 7089. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.59257245 -0.10265857  0.9165435   0.5498593 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 7090. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.26799798 0.03530347 0.8878267  0.3883041 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 7091. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.7411568  -0.4660324   0.8554871   0.45368326]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 7092. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.04690862 -0.12902415  0.76694953  0.46998942]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 7093. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.16487771 -0.68056273  0.73236585  0.505456  ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 7094. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.03855217 -0.8281327   0.8884766   0.5490447 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 7095. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.8267814 -0.5687703  0.7948015  0.5376705]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 7096. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.02631193 -0.73327774  0.7235899   0.59905744]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 7097. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.08333766 0.6066841  0.5846665  0.536729  ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 7098. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.16469276 -0.38745284  0.967314    0.5757344 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 7099. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.60152394 -0.05306417  0.6612431   0.45579112]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 7100. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.5644787   0.4310348   0.6673881   0.49460018]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 7101. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.57839084 -0.86277723  0.6795609   0.5410805 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 7102. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.00409687 -0.3019653   0.8930025   0.53704226]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 7103. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.3063171 -0.5493818  0.7221565  0.5874988]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 7104. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.44543302 -0.35318524  0.7401422   0.61219203]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 7105. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.33854628 0.5657029  0.16692495 0.6047752 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 7106. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.08457887 -0.32094216  0.88388216  0.5336776 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 7107. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.4893633 -0.8565067  0.9117484  0.5289724]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 7108. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.83622545 -0.91234803  0.81987274  0.47827864]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 7109. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.18853056 -0.81044954  0.86944723  0.49893117]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 7110. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.5309529  -0.30464023  0.46998382  0.4956255 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 7111. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.17247128 -0.22894835  0.9682007   0.63486886]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 7112. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.10239649 0.03966057 0.8963603  0.48700964]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 7113. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.65744984 -0.5544443   0.7746514   0.6315396 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 7114. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.5257331  -0.6571361   0.7529048   0.62599134]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 7115. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.07249045 -0.48338115  0.42863226  0.77225494]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 7116. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.52795684  0.35660124  0.9338896   0.5791521 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 7117. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.30032992  0.34439707  0.9262446   0.37591004]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 7118. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.04294908 -0.00903529  0.87952435  0.50269485]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 7119. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.40941143 0.23362362 0.9248508  0.6055758 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 7120. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.56649125 -0.5268471   0.86893797  0.7181916 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 7121. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.24500144 -0.70458835  0.90659     0.6450336 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 7122. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.6669123  -0.54537654  0.9411638   0.7120197 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 7123. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.5795088  -0.48181164  0.89919114  0.74921656]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 7124. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.5581639  -0.03982282  0.80078053  0.58242905]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 7125. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.02973533 -0.5850004   0.59681284  0.5177624 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 7126. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.28170168 -0.05252218  0.9229193   0.66300154]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 7127. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.12571996 -0.14228427  0.8685814   0.41616702]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 7128. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.10921049 -0.14890271  0.71594894  0.28353262]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 7129. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.36833107 -0.8647723   0.83195066  0.52536094]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 7130. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.47076547  0.32741797  0.71024346  0.5724089 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 7131. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.25476193 -0.523433    0.7633047   0.67645216]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 7132. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.2969693  -0.6990712   0.9290837   0.71020985]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 7133. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.59598994 0.10206914 0.9676137  0.5391028 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 7134. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.28381693 -0.06642669  0.89945483  0.45659065]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 7135. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.4077108  0.07717645 0.89984477 0.44079208]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 7136. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.04905272 -0.2016651   0.8560666   0.5987184 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 7137. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.20418978 -0.9569545   0.886744    0.7184    ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 7138. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.43833637 0.1824193  0.8829889  0.55854666]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 7139. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.19527578 -0.26083136  0.98424673  0.6234393 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 7140. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.09018767  0.7246659   0.7959399   0.3464055 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 7141. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.41517818 -0.6865838   0.9451858   0.5738884 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 7142. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.1576922  -0.6885632   0.88046217  0.73099804]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 7143. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.01919293 -0.64669734  0.8913003   0.64046645]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 7144. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.02387166  0.15906096  0.8815      0.46583092]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 7145. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.06671548 -0.27893376  0.9202417   0.6397836 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 7146. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.4131722  -0.1785819   0.85396457  0.54025733]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 7147. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.23401737  0.1940285   0.55861497  0.4571141 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 7148. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.7060384 -0.4945469  0.849406   0.6154121]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 7149. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.480649   -0.41194558  0.89536357  0.5879135 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 7150. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.4814793  -0.70226234  0.8553269   0.57035494]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 7151. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.05327052 -0.27922451  0.85226536  0.62361383]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 7152. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.05720526 -0.1214422   0.892061    0.686425  ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 7153. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.34518242 0.7921536  0.8862349  0.49617016]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 7154. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.15521765 -0.00702268  0.60608876  0.65285015]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 7155. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.38101733 0.20049095 0.4151814  0.5361713 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 7156. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.6151897  -0.9392293   0.7828028   0.79078054]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 7157. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.42564994 -0.79160136  0.8278091   0.607919  ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 7158. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.34718347 -0.01245528  0.8529612   0.5753813 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 7159. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.9241725 0.326483  0.8222183 0.5154917]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 7160. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.09249246 -0.9798745   0.90365076  0.6332829 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 7161. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.71851325 -0.6829659   0.7855005   0.63859475]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 7162. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.5955725  0.7532103  0.6702713  0.66404307]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 7163. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.07602489 0.2801051  0.7501986  0.49168837]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 7164. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.12067145  0.59607816  0.8794601   0.514871  ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 7165. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.0604744   0.12921786  0.4262395   0.7642946 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 7166. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.08317292 -0.61868495  0.61881626  0.5277097 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 7167. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.85636127 -0.14541698  0.6121242   0.65795064]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 7168. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.647338  -0.657976   0.5915637  0.6672238]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 7169. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[ 0.62263846 -0.75430816  0.82637334  0.5391687 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 7170. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[-0.04695845 -0.5262651   0.87185013  0.79699445]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 7171. State = [[-0.10603163 -0.13709526  0.42838743  1.        ]]. Action = [[0.8513911  0.40968442 0.772712   0.47299397]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 7172. State = [[-0.25524992 -0.10260639  0.10852288  1.        ]]. Action = [[-0.2215029 -0.9081124  0.8918271  0.5795857]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 7173. State = [[-0.24849595 -0.12576288  0.10004581  1.        ]]. Action = [[ 0.49650216 -0.67326146  0.84014606  0.26773775]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 7174. State = [[-0.23056903 -0.15244685  0.1196348   1.        ]]. Action = [[ 0.69193864 -0.79309815  0.9259615   0.24426007]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 7175. State = [[-0.20956266 -0.18279736  0.15123926  1.        ]]. Action = [[ 0.39970767 -0.8605638   0.74012005  0.37601936]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 7176. State = [[-0.20196931 -0.2148339   0.1750879   1.        ]]. Action = [[-0.40434396 -0.6291012   0.30514538  0.72548044]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 7177. State = [[-0.19186226 -0.23178646  0.19477539  1.        ]]. Action = [[ 0.9389992  -0.17295188  0.64160466  0.65203905]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 7178. State = [[-0.16940698 -0.22550514  0.20512185  1.        ]]. Action = [[ 0.42242754  0.8575697  -0.53955567  0.7425597 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 7179. State = [[-0.16149427 -0.20059104  0.20506638  1.        ]]. Action = [[-0.22738338  0.95249176  0.457039    0.7663126 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 7180. State = [[-0.1546676  -0.16893224  0.22103493  1.        ]]. Action = [[0.3837986  0.71787584 0.8061421  0.7100377 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 7181. State = [[-0.13574612 -0.14349563  0.25146544  1.        ]]. Action = [[0.9273385  0.46478724 0.8836738  0.5379784 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 7182. State = [[-0.11597078 -0.1284408   0.2856229   1.        ]]. Action = [[-0.32981235  0.23390317  0.7545891   0.66786516]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 7183. State = [[-0.119098   -0.11527506  0.3132407   1.        ]]. Action = [[-0.06048864  0.3971609   0.6600301   0.65453196]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 7184. State = [[-0.12252992 -0.11646841  0.3411219   1.        ]]. Action = [[-0.27853274 -0.59749633  0.6943226   0.6450522 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 7185. State = [[-0.1287932  -0.1399497   0.37256393  1.        ]]. Action = [[-0.11567825 -0.8403104   0.8364477   0.5282121 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 7186. State = [[-0.1313472  -0.15647645  0.39423838  1.        ]]. Action = [[0.14611816 0.35471535 0.80562425 0.5184227 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 7187. State = [[-0.1319957  -0.15726194  0.39860937  1.        ]]. Action = [[-0.12302113  0.2055198   0.08943605  0.7097323 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 7188. State = [[-0.13449453 -0.1547303   0.399544    1.        ]]. Action = [[-0.32430732  0.9074379   0.65314794  0.73505974]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 7189. State = [[-0.13466664 -0.15435064  0.3995779   1.        ]]. Action = [[0.6100285 0.8363247 0.8861754 0.7597649]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 7190. State = [[-0.13471806 -0.15447415  0.39959785  1.        ]]. Action = [[0.23654258 0.67377543 0.68301845 0.8456578 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 7191. State = [[-0.13471806 -0.15447415  0.39959785  1.        ]]. Action = [[0.23567116 0.76130366 0.80863714 0.5687051 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 7192. State = [[-0.13471806 -0.15447415  0.39959785  1.        ]]. Action = [[-0.12454623  0.03536701  0.68048286  0.67996335]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 7193. State = [[-0.1370367  -0.14824611  0.4030793   1.        ]]. Action = [[-0.27284718  0.35733497  0.15873837  0.8043964 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 7194. State = [[-0.14088161 -0.14171456  0.4102082   1.        ]]. Action = [[-0.65788907  0.08959854  0.89394903  0.5818062 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 7195. State = [[-0.1413408  -0.14112218  0.4102185   1.        ]]. Action = [[-0.6098787   0.4346516   0.7748053   0.55141747]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 7196. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[0.6797794  0.6474123  0.34918213 0.5987041 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 7197. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.42844605  0.7849951   0.82118404  0.6114893 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 7198. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[ 0.6519041  -0.02201223  0.7835647   0.75008845]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 7199. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.8298049   0.49021924  0.7786207   0.5551286 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 7200. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[0.00262439 0.2958405  0.8767178  0.5673003 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 7201. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.4939648   0.8760271   0.72278154  0.62749434]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 7202. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.24327868  0.51649964  0.9521656   0.78642774]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 7203. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.10738868  0.5886693   0.8251331   0.44801807]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 7204. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.6179657   0.6664001   0.79307985  0.64084554]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 7205. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.49233425 -0.3199209   0.44157696  0.34623873]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 7206. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.47519374  0.421044    0.49829602  0.717438  ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 7207. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.05292237  0.43214476  0.7331041   0.66985166]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 7208. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.01666504 -0.7526246   0.9066855   0.6515739 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 7209. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[0.49763966 0.6569903  0.4650705  0.5739325 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 7210. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[0.16207945 0.07669425 0.84072673 0.68316853]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 7211. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[-0.03724754  0.42917633  0.69531155  0.68276525]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 7212. State = [[-0.1414968  -0.14098369  0.41024318  1.        ]]. Action = [[ 0.6675873  -0.04856455  0.6987847   0.60322165]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 7213. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.5749308   0.842569    0.13722086  0.73781013]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 7214. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.12445945  0.81625676  0.3014053   0.5007343 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 7215. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.48841107 -0.3865441   0.8515775   0.7249172 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 7216. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.45714343  0.04399383  0.93007565  0.6710248 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 7217. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[ 0.2828374  -0.0105626   0.77516544  0.7437961 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 7218. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.0610249  0.5173662  0.73064375 0.72065496]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 7219. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[ 0.18339741 -0.94809824  0.349599    0.5656518 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 7220. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.75042236 0.5546222  0.8562293  0.51545954]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 7221. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.15797436 0.93152297 0.82312226 0.49785936]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 7222. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.03662926  0.82968116  0.6754631   0.525759  ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 7223. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.4278921  0.31134725 0.9348893  0.6353016 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 7224. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.31634307 0.21838188 0.8132336  0.82562184]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 7225. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.8125013  0.03145194 0.9244052  0.7279918 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 7226. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.05637324  0.4114188   0.8446765   0.65826344]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 7227. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.30154395 0.13481033 0.8626604  0.7017721 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 7228. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.10915422 0.37146604 0.9098444  0.39253414]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 7229. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.17061329 0.5603374  0.69684565 0.7794701 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 7230. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[ 0.66543865 -0.46313703  0.68011284  0.6179899 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 7231. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.12542641 0.883556   0.7729993  0.51476264]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 7232. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.5785641  -0.17269349  0.8107681   0.6955923 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 7233. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.664653   -0.64267665  0.81320024  0.58432555]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 7234. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.17865473  0.39117384  0.6935301   0.43072534]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 7235. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[ 0.07183552 -0.22281933  0.8533561   0.58165836]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 7236. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[ 0.24381208 -0.7497651   0.75461364  0.85482883]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 7237. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.38326317 -0.8699648   0.5228293   0.61338186]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 7238. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.33350492 0.74496365 0.51370776 0.6625395 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 7239. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.11793375  0.79795694  0.68811727  0.4612466 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 7240. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.10348964 0.9323025  0.7339171  0.7154057 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 7241. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.3290006   0.5646665   0.8268826   0.68436086]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 7242. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.37445563  0.60260606  0.5872456   0.70204043]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 7243. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.7641484   0.01540792  0.7225195   0.75930023]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 7244. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.6197064  0.76535034 0.6225581  0.7370832 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 7245. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.18376124  0.14926124  0.625134    0.88262963]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 7246. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.6085203   0.8503585   0.9338217   0.78791666]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 7247. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.57941955  0.6405246   0.8150717   0.43553042]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 7248. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.28664482 0.9473933  0.6403978  0.71209455]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 7249. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[0.71377635 0.5217756  0.646909   0.57064295]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 7250. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.26429713 -0.17778623  0.83147097  0.58417344]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 7251. State = [[-0.14152063 -0.14104362  0.41025254  1.        ]]. Action = [[-0.19908994  0.7208953   0.8711158   0.7240865 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 7252. State = [[-0.13813779 -0.14086255  0.4129014   1.        ]]. Action = [[ 0.65919685 -0.0474956   0.08733165  0.71759677]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 7253. State = [[-0.13398324 -0.1402853   0.41700962  1.        ]]. Action = [[-0.4451505  0.6041604  0.9353602  0.5966661]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 7254. State = [[-0.1326227  -0.13973984  0.41874486  1.        ]]. Action = [[-0.25585747  0.34469128  0.7586522   0.665452  ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 7255. State = [[-0.1323579  -0.139526    0.41892606  1.        ]]. Action = [[-0.20422661  0.33153677  0.9015558   0.6064949 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 7256. State = [[-0.13231075 -0.13940483  0.41890734  1.        ]]. Action = [[0.4266733  0.38234878 0.9526601  0.6759207 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 7257. State = [[-0.13231075 -0.13940483  0.41890734  1.        ]]. Action = [[ 0.45138    -0.0349071   0.54748046  0.72447276]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 7258. State = [[-0.13231075 -0.13940483  0.41890734  1.        ]]. Action = [[-0.34695804 -0.03029299  0.69224656  0.5233877 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 7259. State = [[-0.13231075 -0.13940483  0.41890734  1.        ]]. Action = [[-0.58028114  0.56906724  0.9376049   0.5009161 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 7260. State = [[-0.13231075 -0.13940483  0.41890734  1.        ]]. Action = [[-0.00525743 -0.5618037   0.7756107   0.586298  ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 7261. State = [[-0.13231075 -0.13940483  0.41890734  1.        ]]. Action = [[-0.2831074  0.5988723  0.7038665  0.7388047]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 7262. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.5761048  -0.6671514   0.16518319  0.80021834]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 7263. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[0.31976223 0.6977949  0.6078348  0.40246975]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 7264. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[ 0.44124103 -0.24561214  0.92232394  0.7527745 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 7265. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[0.21299446 0.6013979  0.9171697  0.80788374]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 7266. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.54674196  0.3885306   0.21985042  0.7499534 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 7267. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[0.5164726  0.04338157 0.71046495 0.58255434]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 7268. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.5384309   0.73029387  0.90719986  0.62918043]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 7269. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.55403304  0.57786155  0.39483225  0.5856117 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 7270. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[ 0.04143977 -0.6640141   0.7354704   0.6871445 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 7271. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.00766939 -0.33492142  0.7432234   0.7920742 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 7272. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.7191311   0.30141532  0.92255366  0.58983266]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 7273. State = [[-0.13228717 -0.13934423  0.418898    1.        ]]. Action = [[-0.02819049  0.74240017  0.9486917   0.7847576 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 7274. State = [[-0.25195754 -0.12305289  0.11020827  1.        ]]. Action = [[-0.34598303  0.3274305   0.77425766  0.71646214]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 7275. State = [[-0.2432003  -0.14228977  0.10137573  1.        ]]. Action = [[ 0.73764634 -0.4062903   0.835649    0.26262844]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 7276. State = [[-0.2203205  -0.15259151  0.12098433  1.        ]]. Action = [[ 0.61845946 -0.13651997  0.9223187   0.15811706]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 7277. State = [[-0.19181083 -0.16563061  0.1565071   1.        ]]. Action = [[ 0.8535588 -0.5368087  0.8582089  0.4800856]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 7278. State = [[-0.17320877 -0.17510508  0.18484937  1.        ]]. Action = [[-0.2692107   0.22097111  0.48788905  0.47548628]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 7279. State = [[-0.17972596 -0.16384035  0.20457605  1.        ]]. Action = [[-0.79267347  0.83079207  0.4546348   0.6743978 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 7280. State = [[-0.18621635 -0.1462715   0.22866607  1.        ]]. Action = [[0.5848999  0.19898391 0.7980647  0.31773627]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 7281. State = [[-0.1730088  -0.13097104  0.2576302   1.        ]]. Action = [[0.5941162  0.5373733  0.72640836 0.4733597 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 7282. State = [[-0.16045158 -0.12802525  0.28023377  1.        ]]. Action = [[-0.19084036 -0.51007915  0.3386135   0.6061797 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 7283. State = [[-0.15503137 -0.13423406  0.30072904  1.        ]]. Action = [[ 0.48871326 -0.11865026  0.66690445  0.5092354 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 7284. State = [[-0.13442096 -0.12942262  0.3281705   1.        ]]. Action = [[0.6427741  0.55976963 0.64203167 0.6602156 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 7285. State = [[-0.11251854 -0.11744733  0.35552514  1.        ]]. Action = [[0.7609843  0.21996939 0.4423281  0.7406478 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 7286. State = [[-0.09529151 -0.1103727   0.37715486  1.        ]]. Action = [[0.06989276 0.19483376 0.61370146 0.55727816]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 7287. State = [[-0.09054461 -0.11418325  0.39408144  1.        ]]. Action = [[-0.21823496 -0.6182852   0.13072872  0.5597248 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 7288. State = [[-0.09319652 -0.12290987  0.3961058   1.        ]]. Action = [[-0.3932864   0.85684514  0.64377     0.53148675]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 7289. State = [[-0.09343245 -0.12450397  0.39701793  1.        ]]. Action = [[ 0.30086315 -0.06975329  0.755353    0.742838  ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 7290. State = [[-0.0927726  -0.12514116  0.398458    1.        ]]. Action = [[0.7172463  0.34403133 0.88411856 0.4046471 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 7291. State = [[-0.09278686 -0.12516622  0.398724    1.        ]]. Action = [[-0.06450212  0.69972515  0.55826175  0.7684758 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 7292. State = [[-0.09255265 -0.12517901  0.3990144   1.        ]]. Action = [[0.0542115  0.84129405 0.94200563 0.53793335]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 7293. State = [[-0.09246144 -0.12521377  0.39909494  1.        ]]. Action = [[0.2254827  0.6831863  0.86861706 0.6407292 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 7294. State = [[-0.09246144 -0.12521377  0.39909494  1.        ]]. Action = [[-0.01277691  0.56318927  0.92226315  0.731385  ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 7295. State = [[-0.09246144 -0.12521377  0.39909494  1.        ]]. Action = [[-0.267133   -0.5407338   0.8409885   0.53861403]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 7296. State = [[-0.09495148 -0.13247162  0.40185067  1.        ]]. Action = [[-0.29136318 -0.46609032  0.23807716  0.51923203]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 7297. State = [[-0.09787203 -0.14164846  0.40706798  1.        ]]. Action = [[ 0.63657033 -0.5897276   0.8003175   0.5727391 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 7298. State = [[-0.09881893 -0.14340436  0.40857914  1.        ]]. Action = [[-0.16701347 -0.2430619   0.9679264   0.7727002 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 7299. State = [[-0.09896751 -0.14431933  0.40922213  1.        ]]. Action = [[-0.83759564  0.9341469   0.6485727   0.6224859 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 7300. State = [[-0.09910742 -0.14467023  0.40930083  1.        ]]. Action = [[0.67719686 0.41395998 0.84255266 0.4342594 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 7301. State = [[-0.0994638  -0.14482358  0.40931076  1.        ]]. Action = [[0.01250136 0.5298443  0.83342814 0.67366636]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 7302. State = [[-0.09948746 -0.14482394  0.40931854  1.        ]]. Action = [[0.16328537 0.74400103 0.70491767 0.5654311 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 7303. State = [[-0.09948746 -0.14482394  0.40931854  1.        ]]. Action = [[0.15899765 0.8121575  0.43145967 0.8858968 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 7304. State = [[-0.09964673 -0.14519493  0.40937334  1.        ]]. Action = [[-0.5099892   0.81223655  0.61039543  0.67374873]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 7305. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.56834084 -0.8770842   0.8650975   0.64695203]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 7306. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.39113903  0.3578924   0.564057    0.6255076 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 7307. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.5496749  -0.14556468  0.85597277  0.61962175]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 7308. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.35904884 -0.0060972   0.8670263   0.5936055 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 7309. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.28210986  0.44473326  0.44637203  0.29529452]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 7310. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.22740746 0.11381543 0.41980684 0.56208086]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 7311. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.08700037 0.27257824 0.84931207 0.5145825 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 7312. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.3491261  0.50774944 0.7481587  0.51631546]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 7313. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.11112571  0.09730184  0.9695003   0.6702354 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 7314. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[ 0.44698346 -0.54803693  0.90786505  0.6373044 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 7315. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.16675675 0.09244275 0.26671565 0.5999954 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 7316. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.05717289 0.75822794 0.8587234  0.6868675 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 7317. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.7840405  0.3614031  0.4715786  0.6418991]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 7318. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.25653732 -0.08935857  0.43955612  0.8207958 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 7319. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.63326246  0.7827798   0.6253035   0.72926176]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 7320. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.04178679 -0.47619802  0.87316203  0.80390525]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 7321. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.6744966  -0.75327086  0.63387704  0.76010084]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 7322. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.23848152 0.36239934 0.6324016  0.6430582 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 7323. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.7597914  0.05111492 0.68913865 0.8159796 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 7324. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.65800095 0.2268728  0.82230365 0.7276459 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 7325. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[-0.45468372  0.8482635   0.672735    0.6246327 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 7326. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.7764063  0.36778986 0.76094985 0.70774746]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 7327. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.06485164 0.6173861  0.46699    0.83879566]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 7328. State = [[-0.0997408  -0.14532325  0.40940523  1.        ]]. Action = [[0.1163975  0.22207296 0.83696747 0.5331106 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 7329. State = [[-0.09887985 -0.13253252  0.4090321   1.        ]]. Action = [[-0.01931471  0.8634362  -0.00249916  0.7765331 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 7330. State = [[-0.09980217 -0.11598888  0.40915132  1.        ]]. Action = [[-0.5696871  -0.34389782  0.6037984   0.6012852 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 7331. State = [[-0.0998547  -0.11386233  0.4091524   1.        ]]. Action = [[ 0.24140346 -0.4602036   0.94192326  0.6661024 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 7332. State = [[-0.09988035 -0.11349182  0.40915227  1.        ]]. Action = [[ 0.04888713 -0.5002461   0.8959284   0.66213655]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 7333. State = [[-0.09988035 -0.11349182  0.40915227  1.        ]]. Action = [[-0.10587823 -0.12742412  0.8450124   0.67828643]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 7334. State = [[-0.09988035 -0.11349182  0.40915227  1.        ]]. Action = [[0.2419132  0.16482556 0.8332701  0.5109801 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 7335. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[ 0.1583072  -0.36147225  0.8766235   0.671648  ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 7336. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[-0.5104595   0.554134    0.6335144   0.63838625]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 7337. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[0.39409816 0.7871785  0.7855654  0.57541347]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 7338. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[-0.22605377 -0.5014442   0.80863667  0.7309272 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 7339. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[-0.7702973 -0.2087065  0.6715207  0.5043373]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 7340. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[-0.11907995  0.7606678   0.5624211   0.42777014]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 7341. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[-0.0114603   0.64170516  0.89842784  0.6601317 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 7342. State = [[-0.09988463 -0.11343026  0.40915227  1.        ]]. Action = [[ 0.3772918  -0.19512647  0.72179604  0.7133963 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 7343. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[0.2232368  0.7806437  0.2053299  0.56114006]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 7344. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[0.28085256 0.651114   0.83015084 0.57934475]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 7345. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[-0.48576742 -0.13548326  0.86603093  0.759567  ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 7346. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[0.4473232  0.7261263  0.82160974 0.5147816 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 7347. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[-0.17373967 -0.24016416  0.86216605  0.6968937 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 7348. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[-0.00361294  0.15344477  0.9583974   0.6614654 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 7349. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[ 0.30377388 -0.05909139  0.95259356  0.61190367]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 7350. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[ 0.05194128 -0.65779996  0.8893347   0.6073878 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 7351. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[-0.261253   0.5830147  0.6363759  0.6764569]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 7352. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[0.07948387 0.22585034 0.8873539  0.55601597]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 7353. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[-0.12914163  0.911602    0.93450904  0.8161967 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 7354. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[ 0.04145837 -0.34867287  0.8241322   0.462265  ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 7355. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[ 0.19143915 -0.24093682  0.95334196  0.26033735]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 7356. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[ 0.5824566 -0.895347   0.8832493  0.657622 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 7357. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[ 0.27984595 -0.58245254  0.9327278   0.70186746]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 7358. State = [[-0.09986541 -0.11336722  0.4091447   1.        ]]. Action = [[-0.349724   -0.93061715  0.91936255  0.6954193 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 7359. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.9136467   0.50361395  0.67809653  0.5483303 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 7360. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.49966604  0.58766985  0.7772703   0.4780413 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 7361. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[ 0.1531887  -0.5864352   0.88460994  0.6825323 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 7362. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[0.10491109 0.40970993 0.96033883 0.7300582 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 7363. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.57542765 -0.05147147  0.58798933  0.6766    ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 7364. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.06228286 -0.34262145  0.7256179   0.69451   ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 7365. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.51571673  0.0272646   0.9668716   0.5760677 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 7366. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[ 0.08123422 -0.24829215  0.8218851   0.46416068]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 7367. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.45578945 -0.8006326   0.6603261   0.7496935 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 7368. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.06513053  0.47214615  0.59861374  0.6402774 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 7369. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.49167383  0.09346938  0.6850647   0.62675095]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 7370. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.28209507 -0.13084668  0.7838578   0.7714176 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 7371. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[-0.29309154 -0.7790083   0.4690516   0.6844268 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 7372. State = [[-0.0998462  -0.11330418  0.40913713  1.        ]]. Action = [[0.00600314 0.61115336 0.7684455  0.7738292 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 7373. State = [[-0.09971516 -0.10389824  0.40839198  1.        ]]. Action = [[-0.03410035  0.55962324 -0.21064866  0.75936365]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 7374. State = [[-0.1005761  -0.09310731  0.40841612  1.        ]]. Action = [[-0.32852125 -0.4798199   0.65684175  0.5382161 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 7375. State = [[-0.10063141 -0.09168064  0.40838635  1.        ]]. Action = [[-0.40907228 -0.8473868   0.6625149   0.36217308]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 7376. State = [[-0.259373    0.00210687  0.10862505  1.        ]]. Action = [[-0.11018825  0.3318143   0.54660034  0.6774459 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 7377. State = [[-0.24940962 -0.00579262  0.10166564  1.        ]]. Action = [[ 0.7470107  -0.4696834   0.78322625  0.10452771]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 7378. State = [[-0.22342364 -0.02702866  0.11977895  1.        ]]. Action = [[ 0.7528541  -0.68088466  0.9780792   0.12984991]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 7379. State = [[-0.19889522 -0.04497227  0.15088378  1.        ]]. Action = [[ 0.41732192 -0.33482718  0.5851879   0.25599682]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 7380. State = [[-0.17943703 -0.0518153   0.16741212  1.        ]]. Action = [[ 0.53411245  0.1301502  -0.06712991  0.5710318 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 7381. State = [[-0.16904461 -0.05223478  0.17334577  1.        ]]. Action = [[0.82809985 0.21953297 0.13075793 0.5532768 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 7382. State = [[-0.16785109 -0.05230165  0.1740476   1.        ]]. Action = [[ 0.191329    0.2973709  -0.21732199  0.4018339 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 7383. State = [[-0.16792008 -0.05229691  0.17401722  1.        ]]. Action = [[0.5486889  0.43291974 0.24977255 0.5644629 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 7384. State = [[-0.16729367 -0.05234898  0.17433369  1.        ]]. Action = [[-0.01512498 -0.6239048   0.42831516  0.33909082]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 7385. State = [[-0.16719925 -0.05234282  0.17437755  1.        ]]. Action = [[ 0.53460884 -0.61331636  0.79764223  0.7433648 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 7386. State = [[-0.16906695 -0.04867528  0.17635384  1.        ]]. Action = [[-0.4861369   0.27118695  0.36304867  0.5607891 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 7387. State = [[-0.17011705 -0.04656434  0.18091816  1.        ]]. Action = [[ 0.57271934 -0.5000602   0.8706579   0.46878278]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 7388. State = [[-0.17024447 -0.04622498  0.18114303  1.        ]]. Action = [[ 0.28879166 -0.8622578   0.8162854   0.6280353 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 7389. State = [[-0.17379285 -0.04636467  0.18913344  1.        ]]. Action = [[-0.41436625 -0.03902727  0.57264876  0.64220476]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 7390. State = [[-0.18098548 -0.04592595  0.20187181  1.        ]]. Action = [[ 0.18029821  0.4814334  -0.41107488  0.63549685]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 7391. State = [[-0.17900124 -0.04670811  0.21448031  1.        ]]. Action = [[ 0.50966454 -0.09879041  0.7885778   0.6262628 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 7392. State = [[-0.16387111 -0.0498999   0.24368805  1.        ]]. Action = [[ 0.6688235  -0.20673788  0.80379856  0.59417343]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 7393. State = [[-0.142275   -0.05464887  0.2702258   1.        ]]. Action = [[ 0.40099728 -0.13853562  0.4678768   0.74315417]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 7394. State = [[-0.12097071 -0.066146    0.29681173  1.        ]]. Action = [[ 0.7391534  -0.49110246  0.75887394  0.6025349 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 7395. State = [[-0.10682056 -0.07347308  0.31542638  1.        ]]. Action = [[ 0.01439655  0.14434195 -0.26000714  0.40391445]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 7396. State = [[-0.10042592 -0.06580686  0.32515922  1.        ]]. Action = [[0.2872424  0.534569   0.7948744  0.46398604]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 7397. State = [[-0.09088958 -0.05973353  0.354914    1.        ]]. Action = [[-0.25740552 -0.10525328  0.9560466   0.46870375]]. Reward = [0.]
Curr episode timestep = 20
Above hoop
Current timestep = 7398. State = [[-0.08347732 -0.06356345  0.38646764  1.        ]]. Action = [[ 0.5772357 -0.3191064  0.7110001  0.5424106]]. Reward = [0.]
Curr episode timestep = 21
Above hoop
Current timestep = 7399. State = [[-0.07538448 -0.06788749  0.40802577  1.        ]]. Action = [[-0.19558638  0.35587645  0.753952    0.5300989 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 7400. State = [[-0.07491297 -0.06889372  0.41003516  1.        ]]. Action = [[-0.30045795  0.09989107  0.90228176  0.6437161 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 7401. State = [[-0.07498161 -0.06922064  0.41007018  1.        ]]. Action = [[ 0.5419259  -0.91347885  0.8284416   0.5753603 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 7402. State = [[-0.07499527 -0.06928558  0.41007712  1.        ]]. Action = [[-0.28773248  0.72484946  0.7767694   0.43731952]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 7403. State = [[-0.07499527 -0.06928558  0.41007712  1.        ]]. Action = [[ 0.03220177 -0.73634636  0.7867738   0.3944347 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 7404. State = [[-0.07499527 -0.06928558  0.41007712  1.        ]]. Action = [[ 0.38686132 -0.8607717   0.3237759   0.63095546]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 7405. State = [[-0.07364802 -0.0840632   0.4132479   1.        ]]. Action = [[ 0.23101044 -0.934837    0.09022415  0.35817504]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 7406. State = [[-0.07132249 -0.09940314  0.41732275  1.        ]]. Action = [[-0.22500163  0.79915214  0.7983612   0.57459974]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 7407. State = [[-0.07183244 -0.10274359  0.41889793  1.        ]]. Action = [[-0.06433749 -0.7373255   0.21430838  0.77358985]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 7408. State = [[-0.07204628 -0.10310107  0.41894543  1.        ]]. Action = [[-0.34828782 -0.1494444   0.88700235  0.32333767]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 7409. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.19798756 -0.05615473  0.22239387  0.72301674]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 7410. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.61920875 -0.36679387  0.811746    0.53658533]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 7411. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[0.2528429 0.5945399 0.6011517 0.5641477]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 7412. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.3723446  -0.96094555  0.7031143   0.59622836]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 7413. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.4490043  0.1833576  0.9301038  0.7421496]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 7414. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[0.20167518 0.95332456 0.7328758  0.64532495]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 7415. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[ 0.08048344 -0.07296151  0.57541823  0.5821172 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 7416. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.63370997 -0.45964408  0.88358593  0.7430675 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 7417. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.07303119  0.6431382   0.704813    0.83197904]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 7418. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.6065101  -0.8736659   0.78251314  0.44638884]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 7419. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.00244242 -0.57654893  0.9498706   0.7470014 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 7420. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.46750462 -0.27345276  0.91308784  0.7410221 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 7421. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[ 0.2951982  -0.55870885  0.947211    0.57230794]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 7422. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[ 0.14720654 -0.16797137  0.8819504   0.7401221 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 7423. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[ 0.3069706  -0.42364377  0.82586884  0.68974495]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 7424. State = [[-0.07207302 -0.10309423  0.4189438   1.        ]]. Action = [[-0.33941472 -0.02484244  0.68195677  0.6132356 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 7425. State = [[-0.07206979 -0.10257813  0.41892564  1.        ]]. Action = [[-0.13919312  0.1604402  -0.15340525  0.8502574 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 7426. State = [[-0.07203723 -0.10186893  0.4188895   1.        ]]. Action = [[0.02362466 0.8706287  0.77990127 0.62334514]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 7427. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.40033448 0.3916334  0.42041373 0.49648774]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 7428. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.5107251  -0.23534513  0.71068335  0.76572084]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 7429. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.02617174  0.285455    0.87372947  0.78881216]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 7430. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.5475923  -0.7253957   0.8691194   0.49349594]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 7431. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.14758307  0.15214038  0.93317676  0.69351864]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 7432. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.02909303 -0.41543913  0.94914174  0.6170111 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 7433. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.2425549  -0.9089251   0.8973987   0.60815907]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 7434. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.24544048  0.37549114  0.7888446   0.55059576]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 7435. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.07610196 -0.99457145  0.35566258  0.57701373]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 7436. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.13424182 0.5882007  0.9696007  0.66322684]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 7437. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.39032876 0.473917   0.68073833 0.49172235]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 7438. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.426188   -0.7277875   0.97667193  0.5883248 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 7439. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.2359358   0.31787062  0.90368164  0.4903885 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 7440. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.39602834  0.33034658  0.6982508   0.680599  ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 7441. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.06498671 -0.8310769   0.33091307  0.7296275 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 7442. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.46256506 -0.78122115  0.9601226   0.5836787 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 7443. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.03022206 0.40695286 0.8877293  0.6246598 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 7444. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.26922643  0.6486602   0.94547236  0.57160866]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 7445. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.08656526 -0.7674497   0.9679489   0.63853574]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 7446. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.38098943 -0.4131335   0.84134245  0.6738844 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 7447. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.3634894  -0.4262247   0.75659955  0.6527035 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 7448. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.15478396 0.5483513  0.49947333 0.7444923 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 7449. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.16714251  0.9772663   0.6281221   0.7179971 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 7450. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.02717477 -0.1265961   0.77899337  0.66318035]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 7451. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.16730809 0.6547053  0.86440825 0.6235378 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 7452. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.48037863 -0.9454443   0.80116415  0.5951681 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 7453. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.4534552  0.09004021 0.9118805  0.7073928 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 7454. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.6445092  0.76489305 0.91360927 0.51323485]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 7455. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.40889    -0.1484682   0.96287274  0.77281666]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 7456. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.02971554 -0.46479076  0.9374676   0.7656307 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 7457. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.3308339  -0.37075973  0.9177029   0.70530057]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 7458. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.3514408  -0.04876626  0.9091203   0.68034923]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 7459. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.06765169  0.2502178   0.6620965   0.66080904]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 7460. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.06327319 -0.12713122  0.72646403  0.66118395]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 7461. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.0510428  0.7470654  0.7177559  0.67404807]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 7462. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.3135215  -0.45091438  0.8540915   0.85968614]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 7463. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.27381837 -0.33159518  0.8576795   0.3166256 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 7464. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[ 0.49566114 -0.8591225   0.5214248   0.78388095]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 7465. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.31242323 -0.4897275   0.830703    0.7381619 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 7466. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[-0.32947928 -0.9155363   0.28462386  0.76727915]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 7467. State = [[-0.07208242 -0.10172001  0.4188666   1.        ]]. Action = [[0.5026586  0.4997791  0.86015344 0.6035361 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 7468. State = [[-0.0715206  -0.08988231  0.4157605   1.        ]]. Action = [[ 0.01053774  0.7243482  -0.15200979  0.68007696]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 7469. State = [[-0.07159275 -0.07902403  0.41063112  1.        ]]. Action = [[-0.02558643 -0.13512748  0.5350441   0.70241714]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 7470. State = [[-0.07146432 -0.07766291  0.40992916  1.        ]]. Action = [[ 0.21267593 -0.86080277  0.7716744   0.64899063]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 7471. State = [[-0.07132517 -0.07675515  0.40983212  1.        ]]. Action = [[ 0.35089755 -0.5363522   0.6904187   0.69609404]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 7472. State = [[-0.07119238 -0.07617068  0.40975314  1.        ]]. Action = [[ 0.2829249  -0.48039794  0.6262877   0.65454054]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 7473. State = [[-0.07105948 -0.07558249  0.40967402  1.        ]]. Action = [[0.1714375  0.58018744 0.38568473 0.59353685]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 7474. State = [[-0.07104483 -0.07551746  0.40966532  1.        ]]. Action = [[ 0.04427814 -0.90555674  0.68678534  0.61910045]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 7475. State = [[-0.07098632 -0.07525734  0.40963048  1.        ]]. Action = [[0.18534839 0.8203945  0.91440725 0.5356591 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 7476. State = [[-0.07088434 -0.07480247  0.4095697   1.        ]]. Action = [[ 0.15082276 -0.8945459   0.87802577  0.52984774]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 7477. State = [[-0.07086989 -0.07473785  0.4095611   1.        ]]. Action = [[-0.0250771   0.26350582  0.84743714  0.555243  ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 7478. State = [[-0.26669315  0.15406314  0.11957408  1.        ]]. Action = [[-0.44011354  0.21636128  0.7483206   0.6423378 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 7479. State = [[-0.25064486  0.16062075  0.11421356  1.        ]]. Action = [[ 0.82848406 -0.70480525  0.92740965  0.26832867]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 7480. State = [[-0.22370073  0.14057027  0.1349255   1.        ]]. Action = [[ 0.6576836  -0.6318447   0.8098817   0.28638744]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 7481. State = [[-0.1951384   0.12060343  0.16776806  1.        ]]. Action = [[ 0.7777865  -0.4983641   0.9848273   0.29231548]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 7482. State = [[-0.17700936  0.11064174  0.19191621  1.        ]]. Action = [[ 0.9287119  -0.52056545  0.9005921   0.40380037]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 7483. State = [[-0.16894507  0.09471568  0.20351562  1.        ]]. Action = [[ 0.04563606 -0.902509    0.5504521   0.34083033]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 7484. State = [[-0.1668786   0.07915552  0.21613552  1.        ]]. Action = [[ 0.6825247  -0.44825304  0.5633613   0.27964962]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 7485. State = [[-0.15639414  0.06723741  0.23056413  1.        ]]. Action = [[ 0.60272956 -0.48032612  0.87032354  0.3687985 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 7486. State = [[-0.14452732  0.0479764   0.25917053  1.        ]]. Action = [[-0.0514726  -0.5292458   0.6032715   0.32455516]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 7487. State = [[-0.13757105  0.02905341  0.28043118  1.        ]]. Action = [[ 0.33858037 -0.4833429   0.344643    0.5007882 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 7488. State = [[-0.1279437   0.01379483  0.29311728  1.        ]]. Action = [[ 0.35370278 -0.27270257 -0.04512894  0.4583869 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 7489. State = [[-0.11395553 -0.00815742  0.3067896   1.        ]]. Action = [[ 0.7266656  -0.8764204   0.58033967  0.42368245]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 7490. State = [[-0.09479409 -0.03900481  0.32966584  1.        ]]. Action = [[ 0.25569713 -0.76619774  0.79336834  0.56266546]]. Reward = [0.]
Curr episode timestep = 11
Above hoop
Current timestep = 7491. State = [[-0.07757159 -0.05692853  0.35948956  1.        ]]. Action = [[ 0.48724365 -0.05001438  0.83367634  0.5370691 ]]. Reward = [0.]
Curr episode timestep = 12
Above hoop
Current timestep = 7492. State = [[-0.06096151 -0.06692171  0.3914632   1.        ]]. Action = [[ 0.43238258 -0.39387846  0.7646359   0.30252838]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 7493. State = [[-0.05101924 -0.07503415  0.41298974  1.        ]]. Action = [[ 0.5748346  -0.9297895   0.7698581   0.51506066]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 7494. State = [[-0.0487539  -0.07606373  0.41769812  1.        ]]. Action = [[-0.55398417 -0.40295267  0.64713836  0.35083354]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 7494 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 7494 is tensor(1.9438e-05, grad_fn=<MseLossBackward0>)
