Current timestep = 0. State = [[-0.23638739 -0.0033772   0.24678276  1.        ]]. Action = [[ 0.67526853 -0.29342616  0.86052704  0.00105596]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3910, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 0 is 1
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.20887403 -0.02876336  0.2677234   1.        ]]. Action = [[ 0.8013251  -0.98566765 -0.15809613  0.15498888]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3650, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1 is 1
Human Feedback received at timestep 1 of 1
Current timestep = 2. State = [[-0.25063193  0.00250755  0.23252864  1.        ]]. Action = [[ 0.23432362  0.98530793  0.9645784  -0.2529018 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3224, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 2 is -1
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.25085354  0.00289675  0.23240192  1.        ]]. Action = [[-0.8121231   0.45546997 -0.2053265   0.16357863]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 3 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 3 is tensor(0.3187, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 3 is 1
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.24913053 -0.01500256  0.23362121  1.        ]]. Action = [[ 0.14765131 -0.9528199   0.21166015  0.9620966 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2888, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 4 is 1
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.25048524 -0.03547778  0.23563443  1.        ]]. Action = [[-0.5498909   0.52845335 -0.8729589   0.00535595]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2516, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 5 is -1
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.25006035  0.00282493  0.23295553  1.        ]]. Action = [[ 0.29844713  0.787086   -0.23500085 -0.8910757 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 6 is 1
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.2461953   0.00156156  0.21839592  1.        ]]. Action = [[-0.12091005 -0.1024462  -0.8749125   0.23090112]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.2077, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 7 is 1
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-2.4558382e-01  1.7030824e-04  1.9179291e-01  1.0000000e+00]]. Action = [[-0.58507     0.5453137   0.36722112  0.51638556]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1929, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 8 is -1
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.24969637  0.01357044  0.18623182  1.        ]]. Action = [[-0.24977028  0.75513804 -0.17117214  0.076401  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 9 is tensor(0.1738, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 9 is 1
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.2556046   0.02757672  0.18052238  1.        ]]. Action = [[-0.8658427  -0.6610322  -0.14427602  0.6479529 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1413, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 10 is 1
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.25652272  0.0296905   0.17977981  1.        ]]. Action = [[-0.7205855   0.490273    0.7050111  -0.06820601]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1300, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 11 is 1
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.2562724   0.02994294  0.17980786  1.        ]]. Action = [[-0.29200697 -0.80013937  0.6726345  -0.04104286]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 12 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 12 is tensor(0.0921, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 12 is -1
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.2563687   0.0299252   0.17977181  1.        ]]. Action = [[-0.74973124  0.8585057   0.09285617 -0.17833841]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.1028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 13 is -1
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.25636792  0.02998297  0.17976943  1.        ]]. Action = [[-0.2428534  -0.32339722  0.9663112   0.03013682]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0711, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 14 is 1
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.25069556  0.00234598  0.2324952   1.        ]]. Action = [[ 0.46194768  0.5461836  -0.9254214  -0.01615554]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 15 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 15 is tensor(0.0600, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 15 is 1
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.23636569  0.00454506  0.24097973  1.        ]]. Action = [[0.941241   0.14308655 0.54124594 0.18996775]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 16 is 1
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.25118837  0.0022574   0.23244782  1.        ]]. Action = [[ 0.6886356   0.6605015   0.11815274 -0.11219466]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0568, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 17 is 1
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.25123745  0.00217863  0.23243691  1.        ]]. Action = [[-0.6691619 -0.6361498  0.1982956 -0.5688487]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 18 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 18 is tensor(0.0287, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 18 is 1
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.24748951 -0.0069039   0.22980124  1.        ]]. Action = [[ 0.3819245  -0.46860832 -0.18775052  0.6580316 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0339, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 19 is 1
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.25047076  0.00319882  0.23269677  1.        ]]. Action = [[ 0.23487735 -0.05958319 -0.8082132  -0.01771724]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 20 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 20 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 20 is 1
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-2.5053513e-01  5.1422243e-04  2.3318328e-01  1.0000000e+00]]. Action = [[-0.3524971  -0.07806814  0.86799264 -0.86760926]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 21 is 1
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.25125784  0.00218107  0.23220369  1.        ]]. Action = [[ 0.5754981   0.70706725  0.42448342 -0.42570448]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0276, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 22 is 1
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.2512095   0.00207126  0.23220092  1.        ]]. Action = [[-0.9578795   0.77309346  0.43095207  0.52088976]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 23 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 23 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 23 is -1
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.2512291   0.00196148  0.23216729  1.        ]]. Action = [[-0.7855486  -0.4388275   0.49920225 -0.28199053]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0145, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 24 is 1
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.25272876  0.0091754   0.23759888  1.        ]]. Action = [[-0.14846861  0.34375668  0.47226024  0.48059487]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0289, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 25 is 1
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.24414505  0.01399338  0.24743183  1.        ]]. Action = [[ 0.77578604 -0.10888386  0.13673103  0.52451026]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 26 is 1
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.22997364  0.01360565  0.25679064  1.        ]]. Action = [[-0.81783247  0.40189552 -0.43300325 -0.32865816]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 27 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 27 is tensor(0.0320, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 27 is 1
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.2506861   0.00269139  0.23273061  1.        ]]. Action = [[ 0.01708639 -0.63976526 -0.6298627  -0.22812319]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 28 is 1
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.24031036 -0.01507555  0.24188109  1.        ]]. Action = [[ 0.5801945 -0.7923447  0.7760657  0.8856802]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 29 is 1
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.22735846 -0.05049931  0.25748456  1.        ]]. Action = [[ 0.09924817 -0.8977632   0.19335032  0.16530204]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0390, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 30 is 1
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.25089315  0.0026626   0.23269606  1.        ]]. Action = [[ 0.45454955 -0.23208463 -0.7076062  -0.6294656 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 31 is 1
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-2.5117594e-01  3.5708136e-04  2.3208238e-01  1.0000000e+00]]. Action = [[-0.7596453   0.4490974  -0.99366194  0.38386095]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0407, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 32 is -1
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.25027514  0.00264313  0.23272641  1.        ]]. Action = [[ 0.46825993 -0.6918804  -0.5983207  -0.6328882 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 33 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 33 is tensor(0.0372, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 33 is 1
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-2.4971253e-01  3.8664919e-04  2.3325902e-01  1.0000000e+00]]. Action = [[-0.62501156  0.26333976  0.70811296  0.29315126]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0522, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 34 is 1
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.25235206  0.00860117  0.22759265  1.        ]]. Action = [[ 0.01300669  0.5321075  -0.5586199   0.88893616]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0520, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 35 is 1
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.25036088  0.0023442   0.2325412   1.        ]]. Action = [[ 0.61414194  0.48643506  0.35969114 -0.31898403]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0702, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 36 is 1
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.2563159   0.0121819   0.22129332  1.        ]]. Action = [[-0.2879275   0.5285053  -0.9506846   0.76799226]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0542, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 37 is -1
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.26007798  0.02342703  0.19907089  1.        ]]. Action = [[-0.24560654 -0.79861814  0.8790674  -0.995838  ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 38 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 38 is tensor(0.0400, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 38 is 1
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.25073096  0.00219384  0.23244034  1.        ]]. Action = [[ 0.00323629 -0.28349864  0.24838614 -0.38298   ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 39 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 39 is tensor(0.0863, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 39 is 1
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.25388896  0.01727205  0.23590437  1.        ]]. Action = [[-0.1314671   0.8460264   0.23042297  0.93335366]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 40 is -1
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.25899604  0.0348811   0.2438364   1.        ]]. Action = [[-0.00558472 -0.11551803  0.33919263  0.80083513]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0849, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 41 is 1
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.25108352  0.0388164   0.24938907  1.        ]]. Action = [[ 0.6965703   0.22587097 -0.48133922  0.06136703]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 42 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 42 is tensor(0.0892, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 42 is 1
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.25071388  0.00270944  0.23264346  1.        ]]. Action = [[ 0.75030446 -0.805394   -0.5977951  -0.6221811 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0675, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 43 is -1
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.25006038  0.00259728  0.23272008  1.        ]]. Action = [[ 0.91565037  0.87789476 -0.44434184 -0.739008  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 44 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 44 is tensor(0.0668, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 44 is 1
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.24940522 -0.015791    0.24350083  1.        ]]. Action = [[-0.03116262 -0.9849577   0.7452836   0.68479514]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 45 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 45 is tensor(0.0750, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 45 is 1
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.25449106 -0.03949908  0.26059425  1.        ]]. Action = [[-0.9095679  -0.7411175   0.6970029  -0.50994736]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0649, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 46 is 1
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.2428149 -0.0531991  0.2745774  1.       ]]. Action = [[ 0.71420205 -0.51646364  0.75133085  0.29537904]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 47 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 47 is tensor(0.0900, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 47 is 1
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.22570588 -0.0668552   0.29749316  1.        ]]. Action = [[-0.52673846  0.33191848  0.27590454  0.44944513]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 48 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 48 is tensor(0.1016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 48 is 1
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.250697    0.0025269   0.23270579  1.        ]]. Action = [[ 0.92998385  0.64310145 -0.9443287  -0.73269707]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 49 is 1
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.25054568  0.0027696   0.23266596  1.        ]]. Action = [[ 0.4323789  -0.03353196 -0.8935434  -0.45029122]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 50 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 50 is tensor(0.0918, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 50 is 1
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.23511218 -0.01448914  0.23931442  1.        ]]. Action = [[ 0.92570496 -0.8056152   0.196154    0.13297415]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 51 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 51 is tensor(0.0881, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 51 is 1
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.25072154  0.00263159  0.23255184  1.        ]]. Action = [[ 0.9028673  -0.18718207  0.6812005  -0.6969623 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0900, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 52 is 1
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.25034338  0.00238663  0.23270616  1.        ]]. Action = [[ 0.03126061 -0.9076636  -0.98988646 -0.9255802 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0778, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 53 is -1
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.2509302  0.0026178  0.2322935  1.       ]]. Action = [[ 0.7720407   0.36913204 -0.26008618 -0.08889049]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 54 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 54 is tensor(0.1054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 54 is 1
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.25225204  0.00207434  0.23247202  1.        ]]. Action = [[-0.6672975  -0.8918634   0.90138555 -0.5093127 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0692, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 55 is 1
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.25064784  0.00230337  0.23365375  1.        ]]. Action = [[-0.00670487  0.2303319   0.70395446 -0.16040611]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.1106, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 56 is 1
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.2536793   0.00996921  0.25327277  1.        ]]. Action = [[-0.9337238  -0.98427874 -0.02614534  0.4365524 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.0807, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 57 is 1
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.24488375  0.0243349   0.26799545  1.        ]]. Action = [[0.7641499  0.64944506 0.34614325 0.68773806]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 58 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 58 is tensor(0.0952, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 58 is -1
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.25134066  0.00233148  0.23220153  1.        ]]. Action = [[ 0.42917097  0.1512922  -0.6185076  -0.6079167 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 59 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 59 is tensor(0.1034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 59 is -1
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.24113698  0.01082652  0.23642664  1.        ]]. Action = [[0.64316463 0.44687188 0.36911452 0.87939084]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.0955, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 60 is -1
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.22943664  0.01907568  0.2415558   1.        ]]. Action = [[-0.73552984 -0.7712975   0.7031146  -0.7910906 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 61 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 61 is tensor(0.0802, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 61 is 1
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.21964473  0.01868159  0.23719156  1.        ]]. Action = [[ 0.9378774  -0.07057828 -0.9045523   0.4816537 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 62 is -1
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.17447467  0.00983144  0.22053216  1.        ]]. Action = [[ 0.7400893  -0.5558557   0.6841366   0.27132797]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 63 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 63 is tensor(0.1039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 63 is 1
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.2503834   0.00275011  0.23278001  1.        ]]. Action = [[ 0.8394606  -0.30589938  0.8246108  -0.4571089 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.1047, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 64 is 1
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-2.5355273e-01 -5.4349378e-04  2.3166072e-01  1.0000000e+00]]. Action = [[-0.44770658 -0.69961303  0.31759405  0.01097429]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1073, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 65 is 1
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.25066882  0.00240579  0.23252423  1.        ]]. Action = [[-0.10096151 -0.736405   -0.7496133  -0.57347345]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 66 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 66 is tensor(0.1115, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 66 is 1
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.23729916 -0.01010235  0.22944996  1.        ]]. Action = [[ 0.8848071  -0.64089096 -0.5109055   0.96564066]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.0943, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 67 is 1
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.21377349 -0.00551799  0.23133451  1.        ]]. Action = [[0.15928662 0.93141484 0.7688428  0.08892512]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1080, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 68 is 1
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.20905834  0.00751942  0.25256452  1.        ]]. Action = [[-0.4627235  -0.21406883  0.5989392   0.44375134]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1117, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 69 is 1
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.2178316  -0.00485698  0.2692221   1.        ]]. Action = [[-0.25702715 -0.5143102  -0.17199731  0.8799107 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.1137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 70 is -1
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.25067592  0.00240298  0.23253083  1.        ]]. Action = [[ 0.5142219  -0.44015455  0.30537105 -0.13945109]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.1243, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 71 is 1
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.25085935  0.00186611  0.23251988  1.        ]]. Action = [[ 0.5924152  -0.11475831  0.4026693  -0.12137216]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.1231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 72 is 1
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.2508086   0.00219565  0.23250431  1.        ]]. Action = [[ 0.6316204   0.80981576 -0.7827492  -0.59270626]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.0986, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 73 is 1
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.23840111  0.01629318  0.2401766   1.        ]]. Action = [[0.9407339  0.72723436 0.41192758 0.7952175 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.0976, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 74 is -1
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.2142863   0.01727982  0.26527214  1.        ]]. Action = [[ 0.29837883 -0.775756    0.8776572   0.5133462 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 75 is -1
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.1885052  -0.01486387  0.28189424  1.        ]]. Action = [[ 0.7412647  -0.9666023  -0.49790204  0.5339669 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.1022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 76 is 1
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.25094035  0.00263268  0.23266594  1.        ]]. Action = [[ 0.5696299  -0.48432058  0.5684475  -0.02318233]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.1177, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 77 is 1
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.2421216   0.00300648  0.24357276  1.        ]]. Action = [[0.6230259  0.06170988 0.93488574 0.28110123]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.1054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 78 is -1
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.25065044  0.00244273  0.23415413  1.        ]]. Action = [[ 0.83332956  0.20573997  0.64912724 -0.12807214]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 79 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 79 is tensor(0.1135, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 79 is 1
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.25074905  0.0014695   0.23421189  1.        ]]. Action = [[-0.46236074 -0.7412813  -0.45054376 -0.35059416]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 80 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 80 is tensor(0.1200, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 80 is 1
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.25087562  0.0010221   0.23419961  1.        ]]. Action = [[-0.6858405   0.01980066 -0.7670051   0.15141487]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.1158, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 81 is -1
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-2.5089011e-01  8.5781817e-04  2.3420602e-01  1.0000000e+00]]. Action = [[-0.73193336 -0.78294903 -0.12229377 -0.7799927 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.1063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 82 is -1
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.24788539 -0.01497796  0.22596863  1.        ]]. Action = [[ 0.41417682 -0.7980453  -0.5247107   0.15462422]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 83 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 83 is tensor(0.1148, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 83 is 1
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.2503241   0.0027464   0.23274814  1.        ]]. Action = [[-0.36408335  0.35196018  0.39102292 -0.93904555]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.1078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 84 is 1
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.25438446  0.00682722  0.22501522  1.        ]]. Action = [[-0.11937863  0.3935709  -0.7081135   0.2643875 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 85 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 85 is tensor(0.1156, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 85 is 1
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.2506612   0.00230253  0.23248157  1.        ]]. Action = [[ 0.6917925   0.92128277  0.27247405 -0.42993426]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 86 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 86 is tensor(0.1040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 86 is -1
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.25041434  0.00240759  0.2335457   1.        ]]. Action = [[-0.27052468 -0.8363931   0.32230592 -0.86655694]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.1010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 87 is 1
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.25070712  0.00237047  0.23365635  1.        ]]. Action = [[-0.17871827 -0.49805754  0.6249889  -0.12114388]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.1137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 88 is 1
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25234133 -0.01680683  0.25165093  1.        ]]. Action = [[-0.82276446  0.7629883   0.44564795 -0.15365916]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 89 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 89 is tensor(0.1045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 89 is 1
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.25395694 -0.02318629  0.25720844  1.        ]]. Action = [[-0.4798481   0.81762576  0.47670412  0.49346876]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.1034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 90 is 1
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.25148535  0.00236197  0.23222503  1.        ]]. Action = [[-0.26467788  0.0359205  -0.17015451 -0.8383101 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.1133, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 91 is 1
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25150898  0.00234688  0.23220998  1.        ]]. Action = [[-0.8814712  -0.84749806  0.4938171  -0.6047823 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.0951, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 92 is 1
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.2540156  -0.00968924  0.22117306  1.        ]]. Action = [[-0.18363607 -0.6683712  -0.94303614  0.6224687 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.1090, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 93 is 1
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.24596071 -0.02251066  0.20315798  1.        ]]. Action = [[0.54258883 0.07588613 0.75110817 0.01556623]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.1120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 94 is 1
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.23113932 -0.02808522  0.20778477  1.        ]]. Action = [[ 0.5739722  -0.23374969 -0.13132113  0.38042188]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.1185, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 95 is 1
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.21760812 -0.03198923  0.20874758  1.        ]]. Action = [[-0.7949124  -0.73544055 -0.610774   -0.52164084]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.1129, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 96 is -1
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.25048792  0.00220987  0.2322922   1.        ]]. Action = [[-0.85026574 -0.06056684  0.6347673  -0.2114619 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.1083, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 97 is 1
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.25090164 -0.00135936  0.23312756  1.        ]]. Action = [[-0.47009134 -0.8044998  -0.41923034 -0.554912  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 98 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 98 is tensor(0.1150, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 98 is 1
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.25008163  0.00258553  0.23272227  1.        ]]. Action = [[ 0.9260433 -0.9489704 -0.2032373 -0.7275634]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.0941, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 99 is 1
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.25015685  0.00163606  0.23274277  1.        ]]. Action = [[-0.47030365 -0.31617653 -0.8550422   0.23942137]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 100 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 100 is tensor(0.1190, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 100 is 1
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-2.4956879e-01  8.3297683e-04  2.3249641e-01  1.0000000e+00]]. Action = [[ 0.6640911  -0.5316442   0.1904943  -0.42300433]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.1141, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 101 is 1
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.22390606 -0.02676523  0.2403153   1.        ]]. Action = [[ 0.71710277 -0.5085399   0.01648998  0.08950078]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 102 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 102 is tensor(0.1147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 102 is 1
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.25063276  0.00270957  0.23259386  1.        ]]. Action = [[ 0.555163   -0.09276855 -0.27227908 -0.76623106]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.1131, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 103 is 1
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-2.5102094e-01 -3.3483069e-04  2.3286138e-01  1.0000000e+00]]. Action = [[-0.68032146  0.79106593  0.893433   -0.70381296]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0996, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 104 is 1
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.2520889   0.0013776   0.23146762  1.        ]]. Action = [[-0.06322318 -0.30628157 -0.7580262  -0.73179936]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.1172, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 105 is 1
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.25505617  0.00651373  0.22061594  1.        ]]. Action = [[0.14780617 0.9682157  0.3393457  0.5595572 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 106 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 106 is tensor(0.1088, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 106 is -1
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.25840244  0.02206855  0.2214913   1.        ]]. Action = [[-0.49909508 -0.96153885 -0.59464324  0.5670359 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.1131, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 107 is 1
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.25886053  0.02421157  0.22156277  1.        ]]. Action = [[-0.28170586  0.9089304  -0.08219796 -0.9421316 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.1069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 108 is 1
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.25887525  0.02436767  0.22155829  1.        ]]. Action = [[-0.8931701   0.17114997 -0.85274833  0.8612492 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.1017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 109 is -1
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.25453857  0.03871588  0.22463085  1.        ]]. Action = [[0.25449443 0.6790662  0.28268003 0.72746813]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.1137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 110 is -1
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.24453677  0.04982323  0.23478733  1.        ]]. Action = [[ 0.16041017 -0.32004344  0.48890054  0.04575241]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.1321, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 111 is 1
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.25067186  0.00255287  0.23257388  1.        ]]. Action = [[ 0.5344392  -0.61852616  0.75100446 -0.18264377]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.1168, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 112 is 1
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.25097096  0.00125744  0.23206055  1.        ]]. Action = [[-0.42263454  0.34990072  0.92042804 -0.57121176]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.1242, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 113 is 1
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.24950036  0.00112799  0.2327159   1.        ]]. Action = [[ 0.35305095 -0.2553864  -0.24195325 -0.723815  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.1256, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 114 is 1
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.23802328 -0.02070323  0.23166594  1.        ]]. Action = [[ 0.2857417  -0.63968134  0.49394345  0.7052736 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.1236, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 115 is 1
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.25008819  0.00259741  0.23271996  1.        ]]. Action = [[ 0.49030542  0.28099728 -0.47126472 -0.23934126]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.1286, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 116 is -1
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.25270987  0.01156181  0.22713439  1.        ]]. Action = [[ 0.17258978  0.5281837  -0.56867784  0.5424726 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.1226, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 117 is 1
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.25455517  0.02042778  0.2185008   1.        ]]. Action = [[-0.7309051  -0.69876665 -0.5652732  -0.9500494 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.1195, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 118 is -1
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.25077733  0.00236102  0.23244125  1.        ]]. Action = [[-0.04557586 -0.47597206 -0.25169098 -0.39406806]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.1381, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 119 is 1
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.25013998 -0.00458143  0.22560284  1.        ]]. Action = [[-0.13768047 -0.34190178 -0.7589864   0.82057035]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.1282, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 120 is 1
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.25088805 -0.01148599  0.21037333  1.        ]]. Action = [[-0.5133378   0.32564938  0.81777453  0.19267249]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 121 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 121 is tensor(0.1351, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 121 is 1
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.25049263  0.00233375  0.23230827  1.        ]]. Action = [[-0.0308919 -0.7128745  0.8468702 -0.9079206]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 122 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 122 is tensor(0.1184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 122 is 1
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.24771906  0.00160478  0.23419553  1.        ]]. Action = [[-0.5163396  -0.91418654 -0.04670995 -0.613902  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.1253, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 123 is 1
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.2511908   0.00245354  0.2326459   1.        ]]. Action = [[ 0.7821269  -0.3089273   0.41700077 -0.43225294]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.1254, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 124 is 1
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.23861262  0.00306126  0.23572083  1.        ]]. Action = [[0.9076507  0.08849502 0.14342284 0.65028405]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.1244, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 125 is 1
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.25121143  0.00232419  0.2321913   1.        ]]. Action = [[ 0.02748442 -0.7550078   0.82428193 -0.63171774]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.1189, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 126 is 1
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.2508162   0.00238048  0.23249722  1.        ]]. Action = [[ 0.653988   -0.5916711  -0.97144943 -0.7535853 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 127 is tensor(0.1086, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 127 is -1
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.23913963  0.00978033  0.23594666  1.        ]]. Action = [[0.8552697 0.5519657 0.0070703 0.0208317]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.1267, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 128 is -1
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.22194418  0.0108135   0.24019945  1.        ]]. Action = [[ 0.04952431 -0.48486263  0.31486344  0.06772709]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.1366, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 129 is 1
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.251333    0.00227335  0.232235    1.        ]]. Action = [[ 0.15120804  0.8755299   0.41752446 -0.8924897 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.1233, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 130 is -1
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.25156516  0.0028656   0.2322371   1.        ]]. Action = [[-0.36754704  0.20094025  0.24853647 -0.6206266 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.1356, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 131 is 1
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.24552913 -0.00415898  0.23040026  1.        ]]. Action = [[ 0.55560493 -0.41388303 -0.2379756   0.9392772 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.1203, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 132 is 1
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.22879061 -0.01778112  0.21950929  1.        ]]. Action = [[ 0.6924956  -0.30217457 -0.8279022   0.07292438]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.1216, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 133 is 1
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.20588347 -0.039657    0.18971868  1.        ]]. Action = [[ 0.0985868  -0.67077523 -0.13553715  0.43915892]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.1317, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 134 is 1
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.2067416  -0.04849707  0.18962352  1.        ]]. Action = [[-0.8262571   0.40824246  0.67407274  0.645942  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.1161, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 135 is 1
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.25004137  0.00264228  0.23275813  1.        ]]. Action = [[ 0.46120024  0.6780188   0.2536391  -0.24374336]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.1299, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 136 is 1
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.25061747  0.0026621   0.2325319   1.        ]]. Action = [[ 0.92581224 -0.837107    0.24321532 -0.7628998 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.1043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 137 is 1
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.25033584  0.00259854  0.23271908  1.        ]]. Action = [[ 0.47031927 -0.8147678  -0.4291277  -0.5417511 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 138 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.1180, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 138 is 1
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.24911797  0.0016302   0.23339434  1.        ]]. Action = [[-0.73579794 -0.19133162 -0.65885407 -0.9348973 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.1257, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 139 is -1
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.2494646   0.00107605  0.23289429  1.        ]]. Action = [[ 0.4230895  -0.36344445 -0.9645113  -0.62026817]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.1180, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 140 is 1
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.25053746  0.00267259  0.2327103   1.        ]]. Action = [[ 0.722378   -0.38466763 -0.00322098 -0.6431058 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.1193, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 141 is 1
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.24923004  0.00105404  0.2328315   1.        ]]. Action = [[ 0.63329375 -0.49687243 -0.5171608  -0.77165955]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 142 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.1149, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 142 is 1
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.24973463  0.00268838  0.2328909   1.        ]]. Action = [[ 0.28921723 -0.5536756  -0.24288368 -0.81274515]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 143 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.1217, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 143 is 1
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-2.5085580e-01 -1.1283189e-05  2.3328191e-01  1.0000000e+00]]. Action = [[-0.45701885 -0.13621563 -0.35666704  0.45409417]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 144 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.1325, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 144 is 1
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.24124804  0.01697552  0.24350387  1.        ]]. Action = [[0.7757447  0.9546044  0.6036502  0.06880903]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.1121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 145 is -1
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.25119963  0.00243295  0.23229508  1.        ]]. Action = [[-0.1963371   0.45837796 -0.08955079 -0.25696492]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.1375, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 146 is 1
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.25124076  0.00267308  0.2322534   1.        ]]. Action = [[ 0.86887336 -0.26065814  0.64730847 -0.8507582 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 147 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 147 is tensor(0.1106, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 147 is -1
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.25109807  0.00234557  0.23228127  1.        ]]. Action = [[ 0.02243078  0.80336165  0.94499135 -0.04151702]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 148 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 148 is tensor(0.1183, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 148 is -1
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.2430884  -0.01481582  0.23888291  1.        ]]. Action = [[ 0.24927163 -0.8604908   0.5310824   0.6367998 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.1275, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 149 is 1
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.25091085  0.0027678   0.23243389  1.        ]]. Action = [[-0.2781294  -0.09124827 -0.11772835 -0.12401074]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.1479, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 150 is 1
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.24182639  0.00359457  0.24418572  1.        ]]. Action = [[0.48733366 0.16219151 0.8594538  0.32002604]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.1222, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 151 is 1
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.25112972  0.00236146  0.23229401  1.        ]]. Action = [[ 0.6534623  -0.644747   -0.09735066 -0.12294465]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.1375, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 152 is 1
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.2519598   0.004574    0.23149423  1.        ]]. Action = [[-0.05140769  0.18542862 -0.05412924  0.07640886]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 153 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 153 is tensor(0.1473, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 153 is 1
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.25068203  0.00226211  0.23246208  1.        ]]. Action = [[ 0.01698828  0.51792717 -0.8448461  -0.7130638 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 154 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 154 is tensor(0.1398, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 154 is 1
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.2502113   0.00268915  0.23261267  1.        ]]. Action = [[ 0.4492395  -0.9193721  -0.19192398 -0.44319087]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.1363, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 155 is 1
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-2.4982385e-01  5.3753552e-04  2.3252703e-01  1.0000000e+00]]. Action = [[ 0.61303616 -0.31113625 -0.79380107 -0.30492026]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.1429, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 156 is 1
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.23517185 -0.00965212  0.20869906  1.        ]]. Action = [[-0.7866848  -0.87182     0.44128668  0.06404138]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.1287, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 157 is 1
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.21476983 -0.03221651  0.20990942  1.        ]]. Action = [[ 0.5319431  -0.9417598   0.8766333   0.11728537]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.1325, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 158 is 1
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.20363615 -0.04392589  0.22764045  1.        ]]. Action = [[-0.2867427   0.5839422   0.32985473  0.25280857]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.1505, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 159 is 1
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.2506283   0.00254104  0.23272868  1.        ]]. Action = [[ 0.19188273  0.1923616  -0.88487124 -0.36750478]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.1567, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 160 is 1
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.251089    0.00148567  0.23257293  1.        ]]. Action = [[-0.5822526   0.29285812  0.64296365  0.0220294 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.1470, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 161 is 1
Human Feedback received at timestep 161 of 1
Current timestep = 162. State = [[-2.5127184e-01  8.5700426e-04  2.3257184e-01  1.0000000e+00]]. Action = [[-0.66274303  0.22029674 -0.63788384  0.6113559 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.1454, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 162 is -1
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.25266227  0.00240756  0.23170969  1.        ]]. Action = [[-0.23977554 -0.10149837 -0.20746458 -0.9292373 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.1523, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 163 is 1
Human Feedback received at timestep 163 of 1
Current timestep = 164. State = [[-0.25623965 -0.00192116  0.23046601  1.        ]]. Action = [[-0.5243812  -0.8692303  -0.35626626 -0.57711333]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.1540, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 164 is 1
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.257735   -0.00391062  0.22974496  1.        ]]. Action = [[-0.3472848   0.4113183   0.17533934  0.450629  ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.1564, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 165 is 1
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.25279164  0.00267152  0.23125155  1.        ]]. Action = [[ 0.17836142 -0.10578638 -0.6572817  -0.98430043]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.1488, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 166 is 1
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.2516595   0.00273819  0.23208264  1.        ]]. Action = [[-0.6300345   0.43704998  0.39875948 -0.24739605]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.1538, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 167 is 1
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.24954629  0.00633171  0.24471779  1.        ]]. Action = [[0.00145853 0.17901409 0.8723569  0.45154858]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.1463, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 168 is 1
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.25119632  0.00247341  0.23235214  1.        ]]. Action = [[ 0.93315077  0.22265708 -0.31762433 -0.06656754]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.1505, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 169 is -1
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.2513428   0.00174451  0.23110312  1.        ]]. Action = [[-0.51938546 -0.00466943 -0.78713465 -0.2847473 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.1597, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 170 is 1
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.25030088  0.00183018  0.23979355  1.        ]]. Action = [[-0.07956874  0.02838147  0.86631227  0.43299413]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.1475, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 171 is -1
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.25022453  0.0014627   0.25523224  1.        ]]. Action = [[-0.5484702   0.18105912  0.61136675  0.5295589 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.1505, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 172 is 1
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.2504747   0.00268373  0.23400334  1.        ]]. Action = [[ 0.03811061 -0.983312    0.88873863 -0.463852  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 173 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 173 is tensor(0.1452, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 173 is 1
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.25061324  0.00260078  0.23392965  1.        ]]. Action = [[-0.4895171  -0.8962077  -0.37093252  0.43331695]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.1509, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 174 is 1
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.25061324  0.00260078  0.23392965  1.        ]]. Action = [[-0.65453726 -0.16313648 -0.5885108  -0.42238945]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 175 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.1591, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 175 is 1
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.25061324  0.00260078  0.23392965  1.        ]]. Action = [[-0.37126863  0.85883236 -0.8133418   0.59767246]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 176 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 176 is tensor(0.1340, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 176 is -1
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.25061324  0.00260078  0.23392965  1.        ]]. Action = [[-0.43143773 -0.7943326  -0.46452892  0.57153463]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.1497, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 177 is 1
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.24360406 -0.00422886  0.23383708  1.        ]]. Action = [[ 0.5648086  -0.37142497 -0.01655066  0.44764662]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.1558, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 178 is 1
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.23594162 -0.01038652  0.23381166  1.        ]]. Action = [[-0.9386069  -0.59098023  0.20936668 -0.7063814 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 179 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 179 is tensor(0.1457, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 179 is 1
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.24995635  0.00272042  0.23280711  1.        ]]. Action = [[ 0.8248426  -0.8913809  -0.35910863 -0.51928896]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 180 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.1397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 180 is 1
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.25034866 -0.00111557  0.23296602  1.        ]]. Action = [[-0.5825971   0.3340143  -0.39366692 -0.7179714 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 181 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.1458, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 181 is 1
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.2422726  -0.02204642  0.23173773  1.        ]]. Action = [[ 0.6910219  -0.9908315  -0.24993134  0.2216711 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 182 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 182 is tensor(0.1401, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 182 is 1
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.25010967  0.00261252  0.2327731   1.        ]]. Action = [[ 0.82262945 -0.25780523 -0.94233716 -0.75857395]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 183 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.1314, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 183 is 1
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-2.4231754e-01  2.6318553e-04  2.3220839e-01  1.0000000e+00]]. Action = [[ 0.690343   -0.0326184  -0.21211326  0.02258646]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 184 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.1417, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 184 is 1
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-2.3121655e-01 -4.1745781e-04  2.2950786e-01  1.0000000e+00]]. Action = [[-0.68830943  0.06262994  0.36614442  0.00866318]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 185 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.1367, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 185 is 1
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.22555496 -0.0178507   0.22402956  1.        ]]. Action = [[ 0.33750153 -0.83803326 -0.4340157   0.8774712 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 186 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.1327, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 186 is 1
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.2507481   0.00258845  0.2326069   1.        ]]. Action = [[ 0.932044   -0.4498173   0.44964325 -0.49330378]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 187 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.1279, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 187 is 1
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-2.5120193e-01  5.2564948e-05  2.3282582e-01  1.0000000e+00]]. Action = [[-0.39979696 -0.86197495  0.9641652   0.26583576]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 188 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 188 is tensor(0.1169, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 188 is 1
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.25167412 -0.00159487  0.24538808  1.        ]]. Action = [[-0.13869876 -0.03705144  0.89773154  0.7161534 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 189 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.1208, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 189 is 1
Human Feedback received at timestep 189 of 1
Current timestep = 190. State = [[-0.2507447   0.0026104   0.23384383  1.        ]]. Action = [[-0.09047991 -0.7440281  -0.03527421 -0.6306557 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 190 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.1368, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 190 is 1
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.25003365  0.00268849  0.23276518  1.        ]]. Action = [[ 0.65953267 -0.6489188  -0.44943553 -0.9151937 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 191 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.1232, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 191 is 1
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.2493085  -0.01428189  0.22952868  1.        ]]. Action = [[ 0.23987126 -0.7779497  -0.36275542  0.65675676]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 192 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.1331, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 192 is 1
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.25003606  0.00296088  0.23297112  1.        ]]. Action = [[-0.07754409 -0.31026584 -0.5666812  -0.0039466 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 193 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.1451, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 193 is 1
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-2.4053378e-01  4.3575067e-04  2.4225532e-01  1.0000000e+00]]. Action = [[ 0.36572826 -0.12439704  0.4796486   0.68763745]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 194 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.1318, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 194 is 1
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.25074887  0.00253327  0.2326518   1.        ]]. Action = [[ 0.15640807  0.458493   -0.51224744 -0.23211467]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 195 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.1390, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 195 is 1
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.25067854  0.0027194   0.23274064  1.        ]]. Action = [[ 0.4846735   0.4910307  -0.55994016 -0.3596096 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 196 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.1344, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 196 is 1
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.25126758  0.00304013  0.23288184  1.        ]]. Action = [[-0.68635184 -0.02618021 -0.34194303 -0.1980502 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 197 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.1389, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 197 is 1
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.2512843   0.00307737  0.2329597   1.        ]]. Action = [[-0.8665843   0.6405945  -0.9719705  -0.11534762]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 198 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.1276, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 198 is -1
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.25138614  0.0028231   0.23297682  1.        ]]. Action = [[-0.6156324  0.5179405 -0.7290353  0.5771959]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 199 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.1189, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 199 is -1
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.25139993  0.00265858  0.23298328  1.        ]]. Action = [[-0.85301495 -0.25780666  0.08276522 -0.9570263 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 200 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.1404, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 200 is 1
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.25183257 -0.01053481  0.2423836   1.        ]]. Action = [[-0.17614621 -0.7083137   0.7217134   0.90825033]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 201 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.1219, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 201 is 1
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.2561509  -0.02727372  0.26008436  1.        ]]. Action = [[-0.8453762  -0.45004678 -0.225286   -0.8933217 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 202 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.1407, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 202 is 1
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.25708082 -0.03017841  0.26128095  1.        ]]. Action = [[-0.8526393  -0.23145998  0.1365509   0.06478214]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 203 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.1323, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 203 is 1
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.25151855 -0.04862602  0.26622555  1.        ]]. Action = [[ 0.5006107  -0.9220635   0.08250666  0.4344964 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 204 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.1370, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 204 is 1
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.25140366 -0.07556809  0.27534708  1.        ]]. Action = [[-0.24044526 -0.29612303  0.335953    0.63349724]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 205 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.1400, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 205 is 1
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.2525065  -0.08803146  0.28073725  1.        ]]. Action = [[-0.82371944 -0.89264566 -0.15704793 -0.20650095]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 206 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.1353, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 206 is -1
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.24998575 -0.10561811  0.2860889   1.        ]]. Action = [[ 0.16270864 -0.8126478  -0.0463931   0.7049377 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 207 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.1401, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 207 is -1
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.25070775  0.00272381  0.2326988   1.        ]]. Action = [[ 0.67261374 -0.69642496 -0.03848767 -0.8774466 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 208 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.1393, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 208 is 1
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.25016943  0.0045451   0.23263021  1.        ]]. Action = [[ 0.38980746  0.23175049 -0.0754143  -0.5933417 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 209 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.1415, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 209 is 1
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.24704954  0.01872852  0.22620466  1.        ]]. Action = [[-0.16998076  0.46785712  0.05702174  0.7397487 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 210 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.1223, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 210 is 1
Human Feedback received at timestep 210 of 1
Current timestep = 211. State = [[-0.24872407  0.03000095  0.22448301  1.        ]]. Action = [[-0.6204778   0.03832293  0.0543977  -0.9392759 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 211 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.1387, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 211 is 1
Human Feedback received at timestep 211 of 1
Current timestep = 212. State = [[-0.24688421  0.03076906  0.21692808  1.        ]]. Action = [[ 0.22862029 -0.01158655 -0.54831296  0.934687  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 212 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.1211, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 212 is -1
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.2506848   0.00228755  0.2324665   1.        ]]. Action = [[-0.15085399  0.5427215   0.07605696 -0.8058226 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 213 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.1370, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 213 is -1
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.23425484 -0.00652211  0.24394545  1.        ]]. Action = [[ 0.9321772  -0.57709736  0.65193796  0.08978236]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 214 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.1239, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 214 is 1
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.25118157  0.0023154   0.23226276  1.        ]]. Action = [[-0.3901965   0.94630635 -0.5425233  -0.80990916]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 215 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.1337, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 215 is -1
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.2422952  -0.00188499  0.2235287   1.        ]]. Action = [[ 0.6703739  -0.27940118 -0.5858616   0.5099819 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 216 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.1333, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 216 is -1
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.25110698  0.00225613  0.2323856   1.        ]]. Action = [[ 0.73832774  0.9504993   0.89523554 -0.83961606]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 217 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.1194, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 217 is 1
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.2512001   0.00265499  0.23338394  1.        ]]. Action = [[-0.49073124  0.3575859   0.2923596   0.12553239]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 218 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.1335, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 218 is 1
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.25116417  0.00260018  0.23353766  1.        ]]. Action = [[-0.9022639 -0.7237127  0.7218399  0.6258999]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 219 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.1102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 219 is 1
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.25029808  0.00100361  0.23362854  1.        ]]. Action = [[ 0.55214477 -0.48847628  0.38101995 -0.6619879 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 220 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.1344, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 220 is 1
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.23829286 -0.01277769  0.2432448   1.        ]]. Action = [[-0.8504373   0.65808725  0.55256057 -0.25996995]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 221 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.1305, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 221 is 1
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.22996546 -0.01730774  0.24503171  1.        ]]. Action = [[ 0.3477974   0.00689745 -0.3063404   0.5096432 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 222 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.1393, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 222 is 1
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.2183404  -0.02312408  0.2405463   1.        ]]. Action = [[ 0.37656856 -0.28618574 -0.23620749  0.60685205]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 223 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.1432, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 223 is 1
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.20796493 -0.03689941  0.24566728  1.        ]]. Action = [[-0.32519126 -0.37364793  0.90711045  0.32723594]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 224 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.1385, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 224 is 1
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.22118707 -0.04949886  0.25237617  1.        ]]. Action = [[-0.73800325 -0.00444484 -0.87915725  0.4370253 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 225 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.1284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 225 is 1
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.23645242 -0.04978443  0.24269299  1.        ]]. Action = [[-0.93136823  0.02015686  0.941509    0.4162631 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 226 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.1309, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 226 is -1
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.2433296  -0.05963627  0.23931676  1.        ]]. Action = [[-0.26684737 -0.43215477 -0.16895664  0.83694243]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 227 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.1468, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 227 is 1
Human Feedback received at timestep 227 of 1
Current timestep = 228. State = [[-0.25111213 -0.06933384  0.23670834  1.        ]]. Action = [[-0.8247321  -0.33730042 -0.1545887   0.8611038 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 228 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.1341, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 228 is 1
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.25078058  0.0025546   0.23275688  1.        ]]. Action = [[ 0.7788267   0.7206373  -0.10871553 -0.38296652]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 229 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.1429, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 229 is -1
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.2508206   0.00310903  0.23346911  1.        ]]. Action = [[ 0.218508    0.18862009  0.20163393 -0.915016  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 230 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 230 is tensor(0.1394, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 230 is 1
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.24219039  0.00696439  0.25016186  1.        ]]. Action = [[ 0.08183312 -0.1694746   0.92987967  0.17536402]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 231 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.1365, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 231 is 1
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.25104713  0.00237029  0.23228692  1.        ]]. Action = [[-0.24143708  0.44509292  0.37153363 -0.26856744]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 232 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 232 is tensor(0.1418, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 232 is 1
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.25116643  0.00173338  0.2322043   1.        ]]. Action = [[-0.85366136  0.913326   -0.79006565  0.5049994 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 233 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 233 is tensor(0.1110, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 233 is -1
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.25119665  0.00161513  0.23221062  1.        ]]. Action = [[-0.5529937  -0.7155891   0.30001628  0.3138399 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 234 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 234 is tensor(0.1278, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 234 is 1
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.24879843 -0.00701827  0.24130371  1.        ]]. Action = [[ 0.12068534 -0.41571057  0.8532095   0.70611346]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 235 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 235 is tensor(0.1210, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 235 is 1
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.24996781 -0.01721409  0.25619596  1.        ]]. Action = [[-0.4579481  -0.07059461  0.92933893  0.46857142]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 236 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 236 is tensor(0.1200, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 236 is 1
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.24366975 -0.02236669  0.2541638   1.        ]]. Action = [[ 0.59912264 -0.20139766 -0.47404575  0.8372619 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 237 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 237 is tensor(0.1331, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 237 is 1
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[-0.2506582   0.00258503  0.23250687  1.        ]]. Action = [[-0.24370712 -0.6957254   0.1759615  -0.8351911 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 238 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 238 is tensor(0.1227, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 238 is 1
Human Feedback received at timestep 238 of 1
Current timestep = 239. State = [[-0.2507683   0.00225195  0.23221552  1.        ]]. Action = [[-0.90346277  0.7833977  -0.19376498  0.01603568]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 239 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 239 is tensor(0.1141, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 239 is -1
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.2507574   0.00251585  0.23386414  1.        ]]. Action = [[ 0.33848727 -0.6742573   0.6162611  -0.85798776]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 240 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 240 is tensor(0.1138, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 240 is 1
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.23776028  0.00268516  0.24421982  1.        ]]. Action = [[0.63909435 0.06184196 0.6825712  0.94685817]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 241 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 241 is tensor(0.1105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 241 is 1
Human Feedback received at timestep 241 of 1
Current timestep = 242. State = [[-0.22017646 -0.00495575  0.2609885   1.        ]]. Action = [[ 0.0246774  -0.4453267   0.25593162  0.9304061 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 242 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 242 is tensor(0.1211, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 242 is 1
Human Feedback received at timestep 242 of 1
Current timestep = 243. State = [[-0.21692316 -0.02550971  0.26558188  1.        ]]. Action = [[ 0.08235979 -0.6094293  -0.49472904  0.59040976]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 243 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 243 is tensor(0.1309, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 243 is 1
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.20189647 -0.03224691  0.26543492  1.        ]]. Action = [[0.934644   0.48398447 0.18657124 0.10787261]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 244 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 244 is tensor(0.1287, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 244 is -1
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.2512626   0.00244086  0.23234892  1.        ]]. Action = [[-0.1378128   0.8666899   0.04683399 -0.3850975 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 245 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 245 is tensor(0.1302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 245 is 1
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.25092134  0.00240766  0.23240794  1.        ]]. Action = [[ 0.01522434 -0.5443092  -0.833018   -0.13850772]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 246 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 246 is tensor(0.1216, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 246 is 1
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.25082254  0.00236604  0.23359217  1.        ]]. Action = [[-0.00836951 -0.40661848  0.537889   -0.18145955]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 247 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 247 is tensor(0.1234, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 247 is 1
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.24312225 -0.00501219  0.26045227  1.        ]]. Action = [[0.41274202 0.28228033 0.65831685 0.05228996]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 248 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 248 is tensor(0.1242, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 248 is 1
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.2216417   0.00227596  0.29935363  1.        ]]. Action = [[0.65727365 0.35318446 0.84903574 0.4600811 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 249 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 249 is tensor(0.1160, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 249 is -1
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.25130856  0.00245401  0.2321934   1.        ]]. Action = [[ 0.880932  -0.7615609 -0.2370075 -0.4355381]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 250 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 250 is tensor(0.1089, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 250 is 1
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[-0.25214216 -0.00959928  0.24113052  1.        ]]. Action = [[-0.281214  -0.640615   0.8600699  0.8292674]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 251 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 251 is tensor(0.1077, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 251 is 1
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.25191644 -0.03088778  0.26565272  1.        ]]. Action = [[ 0.41180396 -0.35043067  0.44194746  0.8713305 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 252 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 252 is tensor(0.1194, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 252 is 1
Human Feedback received at timestep 252 of 1
Current timestep = 253. State = [[-0.24660902 -0.04184176  0.27790704  1.        ]]. Action = [[-0.7455453  -0.41080976 -0.8936297   0.946105  ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 253 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 253 is tensor(0.1153, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 253 is -1
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.24556737 -0.05886921  0.28267118  1.        ]]. Action = [[-0.07350695 -0.7740674   0.05182838  0.6193285 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 254 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 254 is tensor(0.1286, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 254 is 1
Human Feedback received at timestep 254 of 1
Current timestep = 255. State = [[-0.24928772 -0.07808824  0.28579414  1.        ]]. Action = [[-0.54491955 -0.5250805  -0.10437858 -0.89157456]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 255 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 255 is tensor(0.1316, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 255 is 1
Human Feedback received at timestep 255 of 1
Current timestep = 256. State = [[-0.2423315  -0.06903029  0.27753466  1.        ]]. Action = [[ 0.7172439   0.61110306 -0.887442    0.3890221 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 256 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 256 is tensor(0.1350, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 256 is -1
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.22923376 -0.04170214  0.2599027   1.        ]]. Action = [[-0.42237037  0.91594625  0.27029634  0.7762275 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 257 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 257 is tensor(0.1313, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 257 is -1
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.23307744 -0.03276305  0.26586425  1.        ]]. Action = [[-0.11497098 -0.7504367   0.33288944  0.0707711 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 258 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 258 is tensor(0.1386, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 258 is 1
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[-0.2346273  -0.04375506  0.27286863  1.        ]]. Action = [[-0.8415777   0.34781504  0.85454965  0.74336207]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 259 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 259 is tensor(0.1238, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 259 is -1
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.22631137 -0.05043491  0.2845985   1.        ]]. Action = [[ 0.57132506 -0.27499592  0.5858506   0.28775036]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 260 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 260 is tensor(0.1433, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 260 is -1
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.25072324  0.00271006  0.23266962  1.        ]]. Action = [[ 0.01640427 -0.5278267   0.38337088 -0.5776171 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 261 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 261 is tensor(0.1458, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 261 is 1
Human Feedback received at timestep 261 of 1
Current timestep = 262. State = [[-0.25028953  0.00226116  0.23381282  1.        ]]. Action = [[ 0.44421637 -0.37911457  0.61891556 -0.4293772 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 262 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.1390, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 262 is 1
Human Feedback received at timestep 262 of 1
Current timestep = 263. State = [[-0.23596387 -0.02257439  0.25885674  1.        ]]. Action = [[ 0.10995209 -0.5861813   0.93800807  0.48334372]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 263 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.1299, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 263 is 1
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.2283116  -0.04759131  0.29958287  1.        ]]. Action = [[ 0.07785511 -0.39470983  0.65415597  0.07403183]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 264 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.1451, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 264 is 1
Human Feedback received at timestep 264 of 1
Current timestep = 265. State = [[-0.2313171  -0.07647241  0.3325449   1.        ]]. Action = [[-0.60673064 -0.78580606  0.48716426  0.78306437]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 265 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 265 is tensor(0.1297, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 265 is 1
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.25106034 -0.08591784  0.3595819   1.        ]]. Action = [[-0.36234093  0.61647415  0.5179362   0.46855903]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 266 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 266 is tensor(0.1499, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 266 is -1
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.25872856 -0.07624216  0.37541896  1.        ]]. Action = [[-0.64236087  0.6033776   0.8511416   0.43165326]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 267 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 267 is tensor(0.1406, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 267 is -1
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.25919148 -0.0752139   0.37848267  1.        ]]. Action = [[-0.74881756  0.2742529   0.7012086  -0.52950895]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 268 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.1479, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 268 is -1
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.2507795   0.0027702   0.23269889  1.        ]]. Action = [[-0.01853299  0.48935342 -0.6610768  -0.88204473]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 269 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.1293, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 269 is -1
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.2510625   0.00291265  0.23238179  1.        ]]. Action = [[-0.8929416  -0.66351354  0.13353992 -0.6746746 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 270 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.1281, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 270 is 1
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.23974454 -0.00756878  0.24135177  1.        ]]. Action = [[ 0.63616705 -0.51916426  0.74368453  0.01480985]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 271 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.1269, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 271 is 1
Human Feedback received at timestep 271 of 1
Current timestep = 272. State = [[-0.2295567  -0.01797946  0.2517884   1.        ]]. Action = [[-0.9489167  -0.8110487   0.04094064 -0.75474435]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 272 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 272 is tensor(0.1219, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 272 is 1
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.22988503 -0.03733823  0.25157237  1.        ]]. Action = [[-0.19864106 -0.91714895 -0.34209967  0.5769576 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 273 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.1215, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 273 is 1
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.25071615  0.00249402  0.23247364  1.        ]]. Action = [[-0.05337989  0.03712618  0.5540409  -0.38928205]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 274 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 274 is tensor(0.1302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 274 is 1
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.2522663   0.00191478  0.231542    1.        ]]. Action = [[-0.2843166  -0.47597206 -0.17352188 -0.4755398 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 275 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 275 is tensor(0.1203, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 275 is 1
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.25736648 -0.01445594  0.22844592  1.        ]]. Action = [[-0.8288114   0.5633675  -0.74351645 -0.43698764]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 276 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.1003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 276 is -1
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.25849938 -0.01949994  0.22853081  1.        ]]. Action = [[-0.5683017  -0.35526383 -0.06021494 -0.5911535 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 277 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.1127, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 277 is 1
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.25867426 -0.02025582  0.22854665  1.        ]]. Action = [[-0.8189966   0.40235293  0.915357    0.24192011]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 278 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 278 is tensor(0.0988, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 278 is -1
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.250592    0.0023488   0.23229201  1.        ]]. Action = [[ 0.71576977 -0.2927974   0.51045763 -0.2017718 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 279 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 279 is tensor(0.1112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 279 is 1
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.2482011   0.00111974  0.23349127  1.        ]]. Action = [[-0.4447567  -0.4307971  -0.42615438  0.58928823]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 280 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 280 is tensor(0.1087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 280 is 1
Human Feedback received at timestep 280 of 1
Current timestep = 281. State = [[-0.23252678 -0.00147224  0.24070552  1.        ]]. Action = [[ 0.9412041  -0.12276351  0.3324015   0.8834306 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 281 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 281 is tensor(0.1054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 281 is 1
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.25115925  0.0024541   0.23240781  1.        ]]. Action = [[ 0.7496028   0.60752463  0.62589574 -0.82249975]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 282 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 282 is tensor(0.0967, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 282 is 1
Human Feedback received at timestep 282 of 1
Current timestep = 283. State = [[-0.25128776  0.00217545  0.23239242  1.        ]]. Action = [[-0.6229091  -0.01565021 -0.6215142  -0.5961401 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 283 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 283 is tensor(0.0998, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 283 is 1
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.2571175   0.00811556  0.22038746  1.        ]]. Action = [[-0.27786994  0.3827405  -0.89056563  0.63318765]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 284 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 284 is tensor(0.0988, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 284 is -1
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.2593367   0.01478693  0.19579498  1.        ]]. Action = [[-0.74972206 -0.3578099  -0.43750858  0.46794724]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 285 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 285 is tensor(0.1049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 285 is 1
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.25842488  0.00344556  0.18753427  1.        ]]. Action = [[ 0.14885843 -0.70604885 -0.2400679   0.25959265]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 286 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 286 is tensor(0.1145, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 286 is 1
Human Feedback received at timestep 286 of 1
Current timestep = 287. State = [[-0.24930269 -0.0190901   0.17245077  1.        ]]. Action = [[ 0.75664246 -0.55622524 -0.7197566   0.6156244 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 287 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 287 is tensor(0.1114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 287 is 1
Human Feedback received at timestep 287 of 1
Current timestep = 288. State = [[-0.25005314  0.00274392  0.2327561   1.        ]]. Action = [[ 0.83432364 -0.2575189  -0.55773    -0.70246136]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 288 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 288 is tensor(0.1006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 288 is 1
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[-0.25138187  0.0012769   0.2341483   1.        ]]. Action = [[-0.49710602 -0.56080353 -0.33865672  0.39169097]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 289 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 289 is tensor(0.1196, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 289 is 1
Human Feedback received at timestep 289 of 1
Current timestep = 290. State = [[-0.25139534  0.00124666  0.23446614  1.        ]]. Action = [[-0.8020026   0.17066061  0.0780952  -0.6964479 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 290 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 290 is tensor(0.1129, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 290 is 1
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.25140014  0.00119182  0.23446827  1.        ]]. Action = [[-0.614478   0.9137089  0.9401281 -0.592614 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 291 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 291 is tensor(0.1115, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 291 is -1
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.2515365  -0.01131207  0.22611985  1.        ]]. Action = [[ 0.20689118 -0.65094453 -0.7105693   0.07330656]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 292 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 292 is tensor(0.1217, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 292 is 1
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.25251493 -0.02402652  0.21127959  1.        ]]. Action = [[-0.583671   -0.07091004 -0.61825776 -0.71447754]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 293 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 293 is tensor(0.1144, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 293 is 1
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[-0.25256434 -0.02619286  0.2109669   1.        ]]. Action = [[-0.82736194  0.27685332 -0.9681246  -0.27478337]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 294 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 294 is tensor(0.1089, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 294 is -1
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.25068772  0.00234908  0.23244372  1.        ]]. Action = [[ 0.34250784  0.36332965 -0.06838471 -0.97836953]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 295 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 295 is tensor(0.1100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 295 is 1
Human Feedback received at timestep 295 of 1
Current timestep = 296. State = [[-0.24904405  0.00256634  0.23329253  1.        ]]. Action = [[-0.6599832  -0.2783091  -0.63984334 -0.3894806 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 296 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 296 is tensor(0.1181, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 296 is 1
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[-0.24911132  0.00259771  0.23327726  1.        ]]. Action = [[-0.47580892  0.28317177  0.2695923  -0.4959501 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 297 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 297 is tensor(0.1206, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 297 is 1
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.24911132  0.00259771  0.23327726  1.        ]]. Action = [[-0.77981716 -0.92058164 -0.02924639 -0.8418101 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 298 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 298 is tensor(0.1203, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 298 is 1
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[-0.24110068  0.00157857  0.2369829   1.        ]]. Action = [[ 0.47460854 -0.13453472  0.16417944  0.34352553]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 299 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 299 is tensor(0.1225, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 299 is 1
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.22588445 -0.01283649  0.23556815  1.        ]]. Action = [[ 0.52742696 -0.65526897 -0.46006024  0.6596321 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 300 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 300 is tensor(0.1121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 300 is 1
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.20956546 -0.02689424  0.22586133  1.        ]]. Action = [[-0.9616688   0.37625766  0.42841983  0.36565077]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 301 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 301 is tensor(0.1097, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 301 is 1
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.20795718 -0.04160635  0.22927503  1.        ]]. Action = [[-0.3432989  -0.5535158   0.33887887  0.18461132]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 302 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 302 is tensor(0.1212, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 302 is 1
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.1996209  -0.0528433   0.24700022  1.        ]]. Action = [[0.6923468  0.22508693 0.8853375  0.7880678 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 303 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 303 is tensor(0.1044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 303 is 1
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.1961052  -0.03752951  0.27183202  1.        ]]. Action = [[-0.826194    0.7604097   0.03536689  0.36988723]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 304 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 304 is tensor(0.1120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 304 is -1
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.2034097  -0.01421505  0.28431323  1.        ]]. Action = [[0.4235623  0.23570955 0.36714613 0.5274453 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 305 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 305 is tensor(0.1173, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 305 is -1
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.25065148  0.00285585  0.23256771  1.        ]]. Action = [[ 0.15686858  0.11960077  0.75619304 -0.21811545]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 306 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 306 is tensor(0.1147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 306 is 1
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.25195765 -0.00525239  0.23217884  1.        ]]. Action = [[-0.11627823 -0.39727604  0.2238648   0.05864406]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 307 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 307 is tensor(0.1122, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 307 is 1
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[-0.25076666  0.00244641  0.23248044  1.        ]]. Action = [[ 0.7333522  -0.86780924  0.7685561  -0.3710723 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 308 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 308 is tensor(0.1027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 308 is 1
Human Feedback received at timestep 308 of 1
Current timestep = 309. State = [[-0.25073496  0.00248062  0.2324019   1.        ]]. Action = [[-0.7627044  -0.8160037   0.12695551  0.5271559 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 309 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 309 is tensor(0.0947, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 309 is 1
Human Feedback received at timestep 309 of 1
Current timestep = 310. State = [[-0.25062338  0.00229272  0.23367152  1.        ]]. Action = [[-0.20483178  0.6459285   0.782763   -0.38121748]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 310 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 310 is tensor(0.1093, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 310 is 1
Human Feedback received at timestep 310 of 1
Current timestep = 311. State = [[-0.25027287  0.00141322  0.23262967  1.        ]]. Action = [[ 0.80065346 -0.11648518  0.29255652 -0.788942  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 311 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 311 is tensor(0.1008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 311 is 1
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-2.5029680e-01  3.7038527e-04  2.3270041e-01  1.0000000e+00]]. Action = [[-0.5546314  -0.6981447   0.97128487  0.90849614]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 312 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 312 is tensor(0.0942, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 312 is 1
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-2.5032133e-01  9.5755509e-05  2.3271103e-01  1.0000000e+00]]. Action = [[-0.5884264  -0.29392743  0.8399937  -0.5978395 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 313 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 313 is tensor(0.1120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 313 is 1
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-2.5034106e-01 -1.2356327e-04  2.3271957e-01  1.0000000e+00]]. Action = [[-0.43624258 -0.5522271   0.42704535 -0.74113303]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 314 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 314 is tensor(0.1111, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 314 is 1
Human Feedback received at timestep 314 of 1
Current timestep = 315. State = [[-0.25015432  0.00177188  0.233834    1.        ]]. Action = [[ 0.16427195 -0.03132045  0.7429451  -0.08996385]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 315 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 315 is tensor(0.1107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 315 is 1
Human Feedback received at timestep 315 of 1
Current timestep = 316. State = [[-0.24600562 -0.00125595  0.25005272  1.        ]]. Action = [[-0.96393347 -0.17564839  0.41458917  0.64391816]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 316 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 316 is tensor(0.0943, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 316 is 1
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.2507104   0.0022573   0.23381685  1.        ]]. Action = [[-0.22220367  0.37833428  0.58635914 -0.693801  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 317 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 317 is tensor(0.1142, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 317 is 1
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.25063038  0.00129448  0.23391825  1.        ]]. Action = [[-0.46266448  0.7938905  -0.49681222  0.72135997]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 318 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 318 is tensor(0.0936, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 318 is -1
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.25360167 -0.01601943  0.22446372  1.        ]]. Action = [[-0.31858504 -0.8212349  -0.6381866   0.36287856]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 319 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 319 is tensor(0.1010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 319 is 1
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.25656527 -0.03406241  0.20935114  1.        ]]. Action = [[-0.37826788  0.9001868  -0.42824578  0.917675  ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 320 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 320 is tensor(0.0937, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 320 is 1
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.25734925 -0.02949983  0.19803974  1.        ]]. Action = [[ 0.11001074  0.4933926  -0.65253264  0.24583507]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 321 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 321 is tensor(0.1078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 321 is 1
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.25087225  0.00246264  0.23243904  1.        ]]. Action = [[ 0.35731864 -0.43522364 -0.0007714  -0.2021352 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 322 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 322 is tensor(0.1190, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 322 is 1
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.25123644  0.00279386  0.23241094  1.        ]]. Action = [[ 0.8978145   0.26342106  0.7462909  -0.18526125]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 323 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 323 is tensor(0.1113, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 323 is -1
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.25023827  0.00273933  0.2326625   1.        ]]. Action = [[ 0.147668   -0.7086144  -0.24740553 -0.32299304]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 324 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 324 is tensor(0.1128, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 324 is 1
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.24884722  0.00229555  0.23334154  1.        ]]. Action = [[-0.8604387  -0.6874556  -0.6414291   0.61103284]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 325 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 325 is tensor(0.0940, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 325 is 1
Human Feedback received at timestep 325 of 1
