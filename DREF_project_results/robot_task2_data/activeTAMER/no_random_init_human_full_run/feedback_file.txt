Current timestep = 0. State = [[-0.23765382 -0.00279957  0.2465479   1.        ]]. Action = [[ 0.6752615  -0.29341793  0.8605341   0.00105047]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3912, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.22263288 -0.01247829  0.26852596  1.        ]]. Action = [[-0.59485996  0.05296361 -0.18381423  0.49439228]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3764, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.23033343 -0.01122938  0.26571897  1.        ]]. Action = [[-0.9711298   0.6804943   0.86174965  0.14710009]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3470, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.23087956 -0.01114838  0.26546744  1.        ]]. Action = [[-0.67392194  0.44761992 -0.68308794 -0.7500095 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 3 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 3 is tensor(0.2986, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.25126898  0.00225477  0.23230526  1.        ]]. Action = [[-0.5223242  -0.16691697 -0.30217642 -0.58392674]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 5. State = [[-0.25135195  0.00241016  0.23180151  1.        ]]. Action = [[-0.79289174  0.23680508 -0.01268464  0.2284652 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2676, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.2513381   0.00257482  0.23179533  1.        ]]. Action = [[-0.566736   -0.9292673   0.0677433  -0.71812606]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.2513381   0.00257482  0.23179533  1.        ]]. Action = [[-0.7814915  -0.97644293 -0.7663156  -0.34021574]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.1928, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.2513335   0.00262971  0.23179325  1.        ]]. Action = [[-0.8044337  -0.4483317  -0.07611799 -0.45475912]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1811, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.25071353  0.00259471  0.23268788  1.        ]]. Action = [[ 0.18446076 -0.6272002  -0.59545875 -0.7866302 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 10. State = [[-0.23561382  0.00227198  0.23283398  1.        ]]. Action = [[ 0.9096682   0.12657928 -0.20449197  0.30071545]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1264, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.20147195  0.00348443  0.23077513  1.        ]]. Action = [[ 0.68636906 -0.03022236  0.06927025  0.64467347]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.25117755  0.00241652  0.23240575  1.        ]]. Action = [[ 0.67828405  0.97765183  0.52441597 -0.85412085]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 13. State = [[-0.25199044  0.00211     0.23230839  1.        ]]. Action = [[-0.61418784 -0.498262    0.18281162  0.82428944]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.0814, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.25349548 -0.00280299  0.24461402  1.        ]]. Action = [[-0.13342792 -0.18414807  0.91438675  0.73453987]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.2511731   0.00246126  0.23223633  1.        ]]. Action = [[ 0.6526842   0.92281675 -0.05454743 -0.11635375]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 16. State = [[-0.24093704 -0.00961425  0.24356002  1.        ]]. Action = [[ 0.36724484 -0.6185168   0.8698156   0.9886198 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.22031115 -0.01410264  0.25593695  1.        ]]. Action = [[ 0.9273362   0.61193943 -0.91611636  0.11575425]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.2510174   0.00216452  0.2324478   1.        ]]. Action = [[-0.46792114  0.28804302 -0.19824743 -0.6597493 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 19. State = [[-0.2511549   0.00225595  0.2324219   1.        ]]. Action = [[-0.82449937 -0.8555407   0.24256194  0.05282938]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0219, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.25096813  0.0023931   0.23347098  1.        ]]. Action = [[-0.1844598  -0.84560674  0.4150021  -0.38918304]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 21. State = [[-0.24483368 -0.00265098  0.24262857  1.        ]]. Action = [[ 0.3512807  -0.21550995  0.7326212   0.96245706]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0191, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.23769821  0.00511037  0.2617055   1.        ]]. Action = [[-0.08252013  0.820081    0.10700035  0.30802774]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of 0
Current timestep = 23. State = [[-0.25128818  0.00237795  0.23235303  1.        ]]. Action = [[ 0.5961709  -0.48976338 -0.76291376 -0.31196302]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 24. State = [[-0.2514065   0.0010039   0.23240578  1.        ]]. Action = [[-0.8163695   0.87910104 -0.00991642 -0.12798452]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0339, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.24825522 -0.00986914  0.24348703  1.        ]]. Action = [[ 0.14613605 -0.48164225  0.9786848   0.5880091 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.24190739 -0.0232621   0.26063275  1.        ]]. Action = [[ 0.65515375  0.09339809 -0.5515066   0.29745412]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.2510801   0.00226927  0.23225583  1.        ]]. Action = [[-0.46379077  0.43320978  0.7571857  -0.38971615]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 28. State = [[-0.25118494  0.00227315  0.2321524   1.        ]]. Action = [[-0.3277756  -0.2986958   0.10330117  0.9460349 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0427, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.25439462 -0.0124906   0.22522181  1.        ]]. Action = [[-0.21056008 -0.74812746 -0.4505328   0.8374251 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.2488101  -0.03103847  0.21346112  1.        ]]. Action = [[ 0.81656575  0.07841492 -0.1533956   0.3982451 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0623, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 0
Current timestep = 31. State = [[-0.21534082 -0.03062598  0.2158431   1.        ]]. Action = [[0.93755984 0.11749291 0.8577161  0.323475  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0534, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.179133   -0.01623548  0.23986833  1.        ]]. Action = [[0.40340722 0.5545534  0.12842858 0.7932608 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.25131968  0.00240832  0.23236978  1.        ]]. Action = [[-0.36184132  0.4615351   0.93410015 -0.66917545]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 34. State = [[-0.2513865   0.0025522   0.23289779  1.        ]]. Action = [[-0.3742752  -0.67245626 -0.8529875  -0.7864564 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0596, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.2451107  -0.01173527  0.23758921  1.        ]]. Action = [[ 0.47932243 -0.7244334   0.29488277  0.8404639 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0678, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.24395506 -0.02274183  0.25639424  1.        ]]. Action = [[-0.4502716   0.4024955   0.83698916  0.76285875]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.2513569  -0.03330864  0.27960813  1.        ]]. Action = [[ 0.2429744  -0.99734676 -0.5088804   0.39337158]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0714, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.25076386  0.00249592  0.23253086  1.        ]]. Action = [[ 0.07655537 -0.7718028  -0.10104138 -0.28336734]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 39. State = [[-0.25017852  0.00132588  0.23390302  1.        ]]. Action = [[ 0.55797625 -0.24334365  0.66054463 -0.7645132 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 40. State = [[-0.21992113 -0.01510844  0.24515548  1.        ]]. Action = [[ 0.92866397 -0.44533432 -0.12447447  0.17463756]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.1914689  -0.02020411  0.24641913  1.        ]]. Action = [[-0.36219108  0.5565162  -0.3872999   0.98324597]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0845, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.2511454   0.00240794  0.23241435  1.        ]]. Action = [[ 0.05948544 -0.30467433 -0.38516718 -0.39526677]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 43. State = [[-0.2427679   0.00362035  0.22489327  1.        ]]. Action = [[ 0.9629618   0.22355258 -0.7777437   0.49505472]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0814, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 0
Current timestep = 44. State = [[-0.25045022  0.00292165  0.23249893  1.        ]]. Action = [[ 0.70645213 -0.64351594 -0.5842763  -0.25726533]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 45. State = [[-0.2512035   0.00226939  0.23222463  1.        ]]. Action = [[ 0.8189639   0.91972506  0.6120149  -0.6651824 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 46. State = [[-0.2510574   0.00294408  0.22116712  1.        ]]. Action = [[ 0.15533113  0.13277912 -0.80303264  0.47244644]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0974, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.25089335  0.00220069  0.2324574   1.        ]]. Action = [[ 0.20891035 -0.27370363  0.69923115 -0.13574117]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 48. State = [[-0.2511272   0.00223926  0.23222579  1.        ]]. Action = [[ 0.83248544  0.77429295  0.5415633  -0.623214  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 49. State = [[-0.24481155 -0.00489907  0.23719959  1.        ]]. Action = [[ 0.2595501  -0.3865764   0.47675645  0.6983713 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.1032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.25058     0.00246138  0.23274975  1.        ]]. Action = [[ 0.8491819  -0.53142774 -0.21529055 -0.3964668 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 51. State = [[-0.25055018  0.00250177  0.23246068  1.        ]]. Action = [[-0.07872182 -0.73050314 -0.33340514 -0.26321173]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 52. State = [[-0.23966962 -0.00899457  0.23909418  1.        ]]. Action = [[ 0.62242603 -0.5463098   0.603101    0.2825787 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.1058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.20955244 -0.0231998   0.26241773  1.        ]]. Action = [[0.96561635 0.01622403 0.71954393 0.22467864]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.1049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.25110528  0.00225723  0.23244779  1.        ]]. Action = [[-0.10920691  0.827106   -0.6945285  -0.07208109]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 55. State = [[-0.24138324  0.00206393  0.24380475  1.        ]]. Action = [[ 0.69581056 -0.00501686  0.9482119   0.821964  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.1021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.23246598  0.01369563  0.27738595  1.        ]]. Action = [[-0.739562    0.57701397  0.7374544   0.8478067 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.0978, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.25208643  0.02551342  0.32079214  1.        ]]. Action = [[-0.10650587 -0.25043976  0.91831064  0.81275845]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.1091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 0
Current timestep = 58. State = [[-0.25097874  0.00240793  0.23235822  1.        ]]. Action = [[ 0.30604684 -0.77435815 -0.351175   -0.6418995 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 59. State = [[-0.25117585  0.0023779   0.2322258   1.        ]]. Action = [[ 0.9189062   0.5406196   0.90750444 -0.18333286]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 60. State = [[-0.25119802  0.00199866  0.23220061  1.        ]]. Action = [[-0.689214   -0.4389462   0.05068231  0.8953495 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.1124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.2509645   0.00250915  0.23252904  1.        ]]. Action = [[ 0.6368493  -0.8067091   0.43627453 -0.15809989]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 62. State = [[-0.23821773 -0.00201743  0.22935107  1.        ]]. Action = [[ 0.85971045 -0.14478469 -0.32545573  0.59548783]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1188, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.25051767  0.00240907  0.23246068  1.        ]]. Action = [[-0.54648954  0.83699524 -0.11747611 -0.34753346]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 64. State = [[-0.23969132  0.01443928  0.23951492  1.        ]]. Action = [[0.85583496 0.5536891  0.45924366 0.8367591 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.1204, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.22895585  0.03681742  0.24472773  1.        ]]. Action = [[-0.36239207  0.4246651  -0.5395635   0.6916194 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1185, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.25121146  0.00240145  0.23235252  1.        ]]. Action = [[ 0.07452238  0.95475674 -0.17697674 -0.08365119]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 67. State = [[-0.24658619  0.01833934  0.22811855  1.        ]]. Action = [[ 0.6215421  0.7714093 -0.2725954  0.9554901]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.1178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 0
Current timestep = 68. State = [[-0.22227764  0.04340562  0.22197895  1.        ]]. Action = [[0.7984998  0.33831334 0.04959607 0.28382647]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1347, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.19783373  0.05222819  0.21814741  1.        ]]. Action = [[ 0.6312381  -0.02805173  0.17813432  0.7687328 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1364, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.20430233  0.04060833  0.22147992  1.        ]]. Action = [[-0.9686029  -0.8312803   0.4476235   0.43815517]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.1035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-0.22128849  0.01834508  0.23108588  1.        ]]. Action = [[ 0.98467803  0.1576035  -0.86020887  0.72357285]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.1126, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.22857217  0.0083299   0.24379109  1.        ]]. Action = [[-0.62863994 -0.48163974  0.83420146  0.84622157]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.1109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.25052857 -0.00680303  0.26273346  1.        ]]. Action = [[ 0.46806633 -0.17193806 -0.83636206  0.51282096]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.1231, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.24639645 -0.00874868  0.24544743  1.        ]]. Action = [[-0.60162866  0.6964433   0.13664067  0.2183373 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.1311, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.23917507 -0.00117376  0.24167924  1.        ]]. Action = [[ 0.5486696   0.49619722 -0.25480068  0.4623015 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1318, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.20766309 -0.0067422   0.23152986  1.        ]]. Action = [[ 0.8686755  -0.76548976 -0.08605003  0.5913913 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.1119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.18522353 -0.04121682  0.22774881  1.        ]]. Action = [[-0.4182163  -0.9662117   0.01437128  0.12751079]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.1179, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.19289772 -0.08842149  0.234387    1.        ]]. Action = [[-0.17280644 -0.94375914  0.0123179   0.67833376]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.1201, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.19683462 -0.11620697  0.23490623  1.        ]]. Action = [[ 0.5124748  -0.14475667 -0.5406102  -0.23805165]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Scene graph at timestep 79 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 79 is tensor(0.1193, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.25078133  0.00259342  0.2326329   1.        ]]. Action = [[ 0.9928442  -0.36770976  0.7393184  -0.58618015]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 81. State = [[-0.24332875 -0.01191587  0.24393633  1.        ]]. Action = [[ 0.4999032  -0.60606956  0.79437315  0.77887726]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.1067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.22648428 -0.04245445  0.26208624  1.        ]]. Action = [[ 0.6444347  -0.6379753  -0.21400058  0.85244894]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.1011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.25074327  0.00258515  0.23251364  1.        ]]. Action = [[ 0.38768208  0.5834644   0.51539993 -0.02657324]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 84. State = [[-0.25105417  0.00289453  0.23154774  1.        ]]. Action = [[-0.81329286 -0.32627475  0.22430348  0.8034226 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0955, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.25137287  0.00236202  0.23228939  1.        ]]. Action = [[ 0.40839696  0.8009546   0.44232917 -0.5308269 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 86. State = [[-0.25068754  0.00188336  0.23376486  1.        ]]. Action = [[ 0.07290888 -0.62762046  0.307958   -0.59291255]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 87. State = [[-0.24622726 -0.01685477  0.23841377  1.        ]]. Action = [[-0.37211144 -0.44379127  0.69805    -0.66303116]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0948, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.23170336 -0.01908379  0.25319016  1.        ]]. Action = [[0.8185067  0.3013811  0.78589606 0.77632976]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0877, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25118417  0.00240801  0.23225586  1.        ]]. Action = [[ 0.16445994 -0.86314183 -0.32680523 -0.72505367]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 90. State = [[-0.23480125 -0.01093625  0.2420293   1.        ]]. Action = [[ 0.92632306 -0.6102678   0.86316466  0.47601962]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0852, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.19361117 -0.03454131  0.2788825   1.        ]]. Action = [[ 0.812099   -0.31890655  0.98512673  0.9008708 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0810, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25110084  0.00247105  0.23243687  1.        ]]. Action = [[ 0.7265029   0.16468763 -0.0411216  -0.10999811]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 93. State = [[-2.4962717e-01 -1.6214432e-04  2.4349433e-01  1.0000000e+00]]. Action = [[ 0.24577641 -0.14934766  0.9603336   0.8712901 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0882, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.23895591 -0.00847792  0.28132463  1.        ]]. Action = [[ 0.6121671  -0.26362586  0.90029573  0.8960576 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0898, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.22113807 -0.02907136  0.3257059   1.        ]]. Action = [[-0.29915392 -0.6375022   0.82718444  0.9314251 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0925, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 0
Current timestep = 96. State = [[-0.21970394 -0.06240521  0.3724682   1.        ]]. Action = [[ 0.6442158  -0.73988223  0.8222687   0.75257087]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0991, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.20382906 -0.09163693  0.3934455   1.        ]]. Action = [[ 0.28467226 -0.43681026 -0.9503134   0.4499923 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.1088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.19275057 -0.10302389  0.369083    1.        ]]. Action = [[-0.8203524   0.6590769   0.94977653 -0.0832212 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 98 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 98 is tensor(0.1075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.18517253 -0.10117075  0.361329    1.        ]]. Action = [[ 0.7715061   0.09241259 -0.5891142   0.83020556]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.1090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.25070104  0.00261784  0.23263177  1.        ]]. Action = [[ 0.54386497 -0.97574073  0.553658   -0.61373305]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 101. State = [[-0.24323846  0.00142389  0.22264276  1.        ]]. Action = [[ 0.85233533  0.02256036 -0.66224545  0.903314  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.0940, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.25075516  0.00236113  0.23251009  1.        ]]. Action = [[-0.5568254   0.5067549  -0.82701874 -0.7882035 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 103. State = [[-0.25085485  0.00204801  0.23292077  1.        ]]. Action = [[-0.44726187  0.48754025  0.06348431  0.6199484 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.1134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.25100136  0.00183191  0.23301572  1.        ]]. Action = [[-0.8504953  -0.90980864 -0.08676189  0.84453416]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0932, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.25100136  0.00183191  0.23301572  1.        ]]. Action = [[-0.32657665 -0.6535437   0.4468547   0.716398  ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.1084, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.25061834  0.00274449  0.23243941  1.        ]]. Action = [[ 0.50752425  0.06191325 -0.42810988 -0.82406074]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 107. State = [[-0.24276896  0.00297212  0.22449042  1.        ]]. Action = [[-0.38339955 -0.88746595  0.9307325   0.9387692 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0878, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.22643186  0.01976795  0.21721157  1.        ]]. Action = [[ 0.8291347   0.86923957 -0.3023764   0.79788756]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.0972, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.2015338   0.0409003   0.20528008  1.        ]]. Action = [[ 0.8453915  0.5555124 -0.7116574  0.5833367]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.0979, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.20573355  0.03604321  0.19548072  1.        ]]. Action = [[-0.31907886 -0.3938399  -0.79818267  0.791278  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.1056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.20730968  0.02996238  0.17202826  1.        ]]. Action = [[ 0.9165857  -0.04664361  0.8683975   0.957618  ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0951, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.19871703  0.01612319  0.18320608  1.        ]]. Action = [[ 0.47358465 -0.63787156  0.97921705  0.78761446]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0951, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.18908307 -0.00238792  0.20162712  1.        ]]. Action = [[0.6730063  0.01492405 0.19751239 0.96373844]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.1057, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.1890026  -0.00238395  0.20189771  1.        ]]. Action = [[ 0.5815605  -0.41558832 -0.06465918  0.5375209 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.1119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.1890026  -0.00238395  0.20189771  1.        ]]. Action = [[ 0.96972966  0.39092147 -0.79510933  0.01539314]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0913, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-1.9512285e-01 -3.3362579e-05  1.9496658e-01  1.0000000e+00]]. Action = [[-0.4771402   0.18870473 -0.6945049   0.26218212]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.1038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.20104338  0.00875919  0.18938988  1.        ]]. Action = [[-0.02473086  0.27192163  0.417593    0.42745674]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.1123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.20307353  0.0156921   0.19034718  1.        ]]. Action = [[ 0.9058645   0.7868302  -0.06801653  0.29130447]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.1006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.20791392  0.0297374   0.18950275  1.        ]]. Action = [[-0.37447536  0.6647937  -0.01159084  0.17271793]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.1050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.2176774   0.04710424  0.19449195  1.        ]]. Action = [[-0.15413451 -0.2495144   0.32237136  0.8522055 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0977, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 0
Current timestep = 121. State = [[-0.2507676   0.00263381  0.23272824  1.        ]]. Action = [[ 0.40827584  0.84439373  0.36605215 -0.33623385]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 122. State = [[-0.2511865   0.00225717  0.23226497  1.        ]]. Action = [[ 0.95119405  0.91227174  0.24386859 -0.1822865 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 123. State = [[-0.25084475 -0.01377288  0.24350455  1.        ]]. Action = [[-0.31216335 -0.86590374  0.9358101   0.9098849 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0701, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 0
Current timestep = 124. State = [[-0.2453091  -0.05600525  0.26909274  1.        ]]. Action = [[ 0.8779588  -0.92018    -0.08478421  0.62614775]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0765, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.22194786 -0.07394356  0.28154778  1.        ]]. Action = [[0.3222083  0.32718027 0.7452128  0.94034827]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0772, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.1945314  -0.05663455  0.2956042   1.        ]]. Action = [[ 0.8868878   0.70498395 -0.7473417   0.8831208 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0793, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.25073335  0.00259993  0.23260935  1.        ]]. Action = [[ 0.36762393 -0.64113927  0.700608   -0.74100924]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 128. State = [[-0.25247955  0.00599052  0.22032887  1.        ]]. Action = [[ 0.40392208  0.24437869 -0.9046391   0.4393531 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0826, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 0
Current timestep = 129. State = [[-0.25134227  0.01906145  0.18276186  1.        ]]. Action = [[-0.03843319  0.42605495 -0.4527322   0.87477875]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0791, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.23489925  0.02684495  0.17811397  1.        ]]. Action = [[ 0.967746   -0.22842407  0.8265289   0.73746824]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.2053385   0.02339662  0.19432737  1.        ]]. Action = [[ 0.84434175 -0.8554403   0.24611712  0.51233983]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0758, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of -1
Current timestep = 132. State = [[-0.2044123   0.03376167  0.18607062  1.        ]]. Action = [[ 0.37533522  0.6120267  -0.9788542   0.7450135 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0691, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.18295509  0.06234678  0.1740412   1.        ]]. Action = [[0.26813078 0.71687174 0.94931066 0.7003834 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.17251477  0.08528437  0.19204241  1.        ]]. Action = [[ 0.5738441  -0.86272424  0.093279    0.5637274 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0828, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.17251477  0.08528437  0.19204241  1.        ]]. Action = [[ 0.7320254  -0.89347035 -0.5659839   0.5251622 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0714, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.17251477  0.08528437  0.19204241  1.        ]]. Action = [[ 0.9074389  -0.9975869   0.34320867  0.02741098]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0733, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of -1
Current timestep = 137. State = [[-0.15735078  0.07897642  0.20817475  1.        ]]. Action = [[ 0.8018353  -0.35652697  0.86183405  0.9826362 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0655, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.2507353   0.00270981  0.2326385   1.        ]]. Action = [[ 0.6411481  -0.8582714   0.93851423 -0.26953918]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 139. State = [[-0.26085973  0.01059499  0.23826051  1.        ]]. Action = [[-0.29255575  0.54957247  0.50437963  0.8099481 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0702, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.2760082   0.02572487  0.25150037  1.        ]]. Action = [[ 0.01140416 -0.3480845  -0.4716184   0.09105301]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.0926, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.26317865  0.01714085  0.26412717  1.        ]]. Action = [[ 0.8453045  -0.49206316  0.82014215  0.84646845]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0617, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.23460907  0.01560058  0.28784803  1.        ]]. Action = [[ 0.6398418   0.6642742  -0.42466718  0.60263526]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 142 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0678, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.21030466  0.02791757  0.2925767   1.        ]]. Action = [[ 0.25709248 -0.20216984  0.78623104  0.3680135 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 143 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0800, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.18355039  0.03769217  0.3193207   1.        ]]. Action = [[0.95263076 0.6197767  0.22617102 0.81139755]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 144 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0614, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.14318481  0.04530023  0.3466043   1.        ]]. Action = [[ 0.822402   -0.40754318  0.7962142   0.96048045]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.11614458  0.02312371  0.3789929   1.        ]]. Action = [[ 0.00280595 -0.7244373   0.41487515  0.24508214]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0855, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.25082296  0.00250862  0.23264268  1.        ]]. Action = [[-0.5409056  -0.45524198 -0.25135732 -0.65003306]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 148. State = [[-0.2500662   0.00165946  0.2338705   1.        ]]. Action = [[ 0.69685507 -0.3952533   0.62949336 -0.32737356]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 149. State = [[-0.23549674 -0.00932213  0.24292937  1.        ]]. Action = [[-0.43442905 -0.04070222  0.03989506  0.74741983]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0713, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.2280665  -0.00982558  0.23925878  1.        ]]. Action = [[ 0.3136953   0.34545016 -0.7159855   0.9328182 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0540, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.22273085  0.00949831  0.22557063  1.        ]]. Action = [[-0.3796233   0.66380596  0.12173593  0.46587825]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0705, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.22628441  0.03958452  0.21657373  1.        ]]. Action = [[ 0.58270097  0.49854207 -0.9142552   0.2632711 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.20813736  0.05107116  0.18673223  1.        ]]. Action = [[ 0.9705975   0.9744154   0.17338836 -0.43674302]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 153 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 153 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.20815495  0.05163732  0.18671423  1.        ]]. Action = [[ 0.73462665 -0.92474616  0.76265025 -0.3989979 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 154 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 154 is tensor(0.0596, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.19248311  0.06525791  0.19938602  1.        ]]. Action = [[0.92715144 0.6635735  0.94684327 0.93048096]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.16186343  0.08520962  0.22602421  1.        ]]. Action = [[ 0.25962186 -0.42602944  0.22612631  0.9369004 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0653, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.17011465  0.09353892  0.23773307  1.        ]]. Action = [[-0.8874594   0.25576043  0.7473247   0.37344348]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.19118467  0.10408702  0.2595567   1.        ]]. Action = [[ 0.6082647  -0.541021   -0.8269805   0.63383985]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.2020464   0.10504067  0.26577595  1.        ]]. Action = [[-0.8638131  -0.07633322  0.30774212  0.39587772]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0639, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.2196681   0.08659326  0.2706671   1.        ]]. Action = [[ 0.8043196 -0.7324687 -0.5228815  0.7156211]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.21992925  0.08007841  0.2552408   1.        ]]. Action = [[-0.918591    0.63146544 -0.4445567   0.9696567 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.23635888  0.09994854  0.23377247  1.        ]]. Action = [[ 0.35273743  0.36651075 -0.4436332   0.66621757]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.23251587  0.10834842  0.20858198  1.        ]]. Action = [[ 0.22670472 -0.04422832 -0.6938676   0.9033146 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0552, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.22092721  0.10903671  0.1870441   1.        ]]. Action = [[ 0.25755763 -0.088727    0.2411573   0.46160722]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0734, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of 0
Current timestep = 165. State = [[-0.19835736  0.10241893  0.2037696   1.        ]]. Action = [[ 0.77805734 -0.26692063  0.84052134  0.85006523]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.17576279  0.09474339  0.2250968   1.        ]]. Action = [[ 0.7663369  -0.39694786  0.03441501  0.8803053 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0550, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.17511684  0.08691777  0.23000464  1.        ]]. Action = [[-0.16410601 -0.39772105  0.2841928   0.9465116 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0614, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.16584374  0.09114732  0.25062358  1.        ]]. Action = [[0.6167681 0.7714517 0.8310218 0.9348023]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0404, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.13596754  0.1003198   0.29067156  1.        ]]. Action = [[ 0.6833031  -0.50788325  0.35164106  0.983925  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0528, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.13038994  0.10661153  0.31811398  1.        ]]. Action = [[-0.80730367  0.75967     0.8148031   0.47734094]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0545, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.14558457  0.11797032  0.3326268   1.        ]]. Action = [[-0.01974958 -0.4037782  -0.57335657  0.6421256 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of 1
Current timestep = 172. State = [[-0.13485731  0.11111388  0.3314025   1.        ]]. Action = [[0.9287218  0.09376907 0.39508712 0.8710797 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.25098798  0.00260069  0.23250252  1.        ]]. Action = [[ 0.8074057  -0.25288224 -0.85936654 -0.2056933 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 174. State = [[-2.5535449e-01  4.2311847e-04  2.3416385e-01  1.0000000e+00]]. Action = [[-0.2404474  -0.00874037  0.28521216  0.47746313]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0675, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of 0
Current timestep = 175. State = [[-0.2477223   0.0121816   0.24917226  1.        ]]. Action = [[0.93488455 0.86245275 0.8383646  0.8552325 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 175 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0335, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.25114307  0.00240804  0.2324078   1.        ]]. Action = [[ 0.36358786  0.62944984  0.48643804 -0.3684981 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 177. State = [[-0.24231455 -0.00752837  0.22491263  1.        ]]. Action = [[ 0.7443638  -0.52523214 -0.50735253  0.79150844]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0463, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.218039   -0.0110552   0.20568733  1.        ]]. Action = [[ 0.4828018   0.50542235 -0.19300568  0.67490923]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0545, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.2510921   0.00221109  0.232334    1.        ]]. Action = [[-0.22192085 -0.6181649   0.9884907  -0.24931192]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 180. State = [[-2.5085393e-01  1.9892515e-04  2.3416832e-01  1.0000000e+00]]. Action = [[-0.45429337 -0.8381346  -0.7196336   0.9173869 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 180 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0438, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-2.3652266e-01 -2.2690791e-04  2.4617752e-01  1.0000000e+00]]. Action = [[0.92900753 0.16754651 0.6456295  0.7494036 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 181 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0507, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.25129467  0.00243285  0.23219253  1.        ]]. Action = [[-0.75389254  0.64323103  0.96378446 -0.191387  ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 183. State = [[-0.25091845  0.00151122  0.23364368  1.        ]]. Action = [[ 0.07934296 -0.05393827  0.25835013  0.98519015]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 183 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0557, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-2.5024515e-01 -8.5116766e-04  2.3687132e-01  1.0000000e+00]]. Action = [[-0.38998342 -0.81734794  0.00644207  0.9708139 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 184 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0511, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.23774274 -0.00136142  0.2521894   1.        ]]. Action = [[ 0.6598239  -0.02481741  0.92840886  0.9309279 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 185 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0462, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.21792115  0.01298833  0.2939822   1.        ]]. Action = [[0.13312459 0.8236766  0.8959024  0.35687613]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 186 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0542, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.20985176  0.03261803  0.322856    1.        ]]. Action = [[ 0.2204808  -0.23083001 -0.33976817  0.77542484]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 187 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0623, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-0.25132686  0.00241644  0.23219137  1.        ]]. Action = [[-0.14262027 -0.00088298  0.840358   -0.02756613]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 189. State = [[-0.25146025  0.00161561  0.23211513  1.        ]]. Action = [[-0.7663739  -0.48272264  0.6191021   0.93991256]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 189 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0486, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.24300171 -0.00609341  0.22887501  1.        ]]. Action = [[ 0.7008245  -0.36611807 -0.29142296  0.5710132 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 190 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 0
Current timestep = 191. State = [[-0.2275673  -0.01366348  0.21348165  1.        ]]. Action = [[-0.02420068  0.19530737 -0.4955724   0.606812  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 191 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.21414289 -0.01888626  0.19716959  1.        ]]. Action = [[ 0.7892666  -0.45037472 -0.13500655  0.91938555]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 192 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0521, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 0
Current timestep = 193. State = [[-0.19126649 -0.0276297   0.18919685  1.        ]]. Action = [[0.9285686  0.85371566 0.3836162  0.9726212 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 193 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0435, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.19118528 -0.02789866  0.18926045  1.        ]]. Action = [[0.4543264  0.24186039 0.50424314 0.84220123]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 194 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.18328674 -0.03724763  0.19812673  1.        ]]. Action = [[ 0.30588102 -0.42031306  0.7204819   0.8596673 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 195 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0600, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.15608902 -0.06500222  0.22457445  1.        ]]. Action = [[ 0.97539985 -0.81118625  0.6349342   0.5696969 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 196 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0515, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.11944708 -0.08562206  0.26391527  1.        ]]. Action = [[0.3755573 0.1723131 0.7071792 0.9519416]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 197 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.10485154 -0.07860497  0.29930162  1.        ]]. Action = [[-0.29438317  0.29462028  0.93882287  0.8739829 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 198 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 0
Current timestep = 199. State = [[-0.10604934 -0.08113103  0.33702663  1.        ]]. Action = [[ 0.4333179  -0.53703845  0.48854542  0.15201187]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 199 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0848, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.09476801 -0.07539444  0.35567328  1.        ]]. Action = [[ 0.12270701  0.9236553  -0.14266181  0.7095864 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 200 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0726, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.08568309 -0.04945383  0.34735805  1.        ]]. Action = [[ 0.8341458   0.09182525 -0.79801863  0.83417606]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 201 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0650, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.04586412 -0.03304479  0.3185024   1.        ]]. Action = [[0.38820255 0.7282293  0.03980541 0.50769186]]. Reward = [0.]
Curr episode timestep = 13
Above hoop
Scene graph at timestep 202 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0765, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of 0
Current timestep = 203. State = [[-0.02599828 -0.01040031  0.32849666  1.        ]]. Action = [[0.56189466 0.17939413 0.5212357  0.23190236]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Scene graph at timestep 203 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0826, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 0
Current timestep = 204. State = [[-0.01031935 -0.02307947  0.3533189   1.        ]]. Action = [[-0.5108467  -0.8981288   0.42935133  0.626946  ]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Scene graph at timestep 204 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0648, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 0
Current timestep = 205. State = [[-0.01860889 -0.06273825  0.35437617  1.        ]]. Action = [[ 0.04298747 -0.8553514  -0.7799859   0.88486886]]. Reward = [0.]
Curr episode timestep = 16
Above hoop
Scene graph at timestep 205 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0576, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.0245287  -0.09277136  0.33276123  1.        ]]. Action = [[-0.52858067 -0.5306801  -0.5212748   0.92446256]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 206 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0649, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.0402273  -0.10681193  0.3238892   1.        ]]. Action = [[-0.39388776  0.16277683  0.5458875   0.4519434 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 207 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0851, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.04550605 -0.10310893  0.34439924  1.        ]]. Action = [[-0.20028955  0.1695826   0.5122266   0.93333197]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 208 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0738, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.06493533 -0.11064196  0.37262994  1.        ]]. Action = [[-0.76663125 -0.4724049   0.91068697  0.84071016]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 209 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0610, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.09658685 -0.13556206  0.39520487  1.        ]]. Action = [[-0.16074407 -0.6558051  -0.8674759   0.61109424]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 210 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0679, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.10517851 -0.15391353  0.36918977  1.        ]]. Action = [[ 0.44532645 -0.24404645 -0.43635988  0.58081317]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 211 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0839, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.09897175 -0.17254631  0.3369203   1.        ]]. Action = [[-0.3853464 -0.4704588 -0.9023739  0.5341679]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 212 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0712, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.11499697 -0.17192622  0.31347543  1.        ]]. Action = [[-0.8216204   0.92002416  0.49085307  0.46087515]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 213 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0759, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.14236796 -0.13732311  0.3216813   1.        ]]. Action = [[-0.22801596  0.6585274   0.14075196  0.95697   ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 214 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.0746, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.14141026 -0.12676755  0.33981317  1.        ]]. Action = [[ 0.8711312  -0.6293747   0.8414366   0.92288804]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 215 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of 0
Current timestep = 216. State = [[-0.13416114 -0.13041814  0.34798503  1.        ]]. Action = [[-0.20577484  0.505865   -0.9974188   0.7909987 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 216 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.0583, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 0
Current timestep = 217. State = [[-0.13937767 -0.10740238  0.33165428  1.        ]]. Action = [[-0.79018545  0.8928523   0.38032138  0.15449572]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 217 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.0707, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 0
Current timestep = 218. State = [[-0.17141005 -0.08772873  0.3370041   1.        ]]. Action = [[-0.70247304 -0.5391948  -0.16695762  0.6976485 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 218 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.0606, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.18526638 -0.10371193  0.33656037  1.        ]]. Action = [[ 0.8660157  -0.18292546  0.2515049   0.820035  ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 219 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.0697, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.1646773  -0.11046067  0.33926925  1.        ]]. Action = [[ 0.9098754  -0.305947   -0.24114627  0.4444692 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 220 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.0693, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.12730235 -0.12440671  0.32158157  1.        ]]. Action = [[ 0.882745   -0.31748414 -0.8398023   0.5932139 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 221 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.0595, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.07400464 -0.12784386  0.30308095  1.        ]]. Action = [[0.94462    0.38273787 0.92178655 0.75528955]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 222 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.0657, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.0479443  -0.10584313  0.33109608  1.        ]]. Action = [[-0.1152764   0.6875446   0.14224792  0.9440222 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 223 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.0674, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.03799397 -0.09447044  0.334197    1.        ]]. Action = [[ 0.84024763 -0.29075146 -0.20879054  0.5056498 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 224 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.0750, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.01610981 -0.11409295  0.33726504  1.        ]]. Action = [[-0.33957338 -0.815341    0.51356053  0.6413282 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 225 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.0625, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.03196899 -0.13866198  0.33848912  1.        ]]. Action = [[-0.87709564 -0.15543818 -0.89327097  0.3875035 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 226 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.0579, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.04760635 -0.15073855  0.3223601   1.        ]]. Action = [[ 0.07349896 -0.29973865  0.01632297  0.86600745]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 227 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.0717, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.05853106 -0.14629544  0.31621936  1.        ]]. Action = [[-0.95698273  0.8585415  -0.26396537  0.982847  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 228 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of 0
Current timestep = 229. State = [[-0.0789616  -0.1337759   0.29556724  1.        ]]. Action = [[ 0.6519525  -0.47804213 -0.5023317   0.72787166]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 229 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of 0
Current timestep = 230. State = [[-0.06114274 -0.12268931  0.2770748   1.        ]]. Action = [[ 0.9868059  0.9897816 -0.5626852  0.6709684]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 231. State = [[-0.03316373 -0.11728632  0.26634154  1.        ]]. Action = [[ 0.12669265 -0.7713328   0.621264    0.51345026]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 231 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.25043452  0.00274706  0.2329221   1.        ]]. Action = [[ 0.3067248   0.7951691   0.8120179  -0.14126283]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 233. State = [[-0.24953479  0.00556577  0.24174379  1.        ]]. Action = [[0.23240483 0.17236269 0.89493155 0.8852242 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 234. State = [[-0.24972346  0.00737875  0.2553316   1.        ]]. Action = [[-0.47702932 -0.14920628  0.5606502   0.84275675]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 235. State = [[-0.24300985 -0.00264086  0.26083642  1.        ]]. Action = [[ 0.44702995 -0.5766389   0.13975143  0.6854601 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 236. State = [[-0.22041921 -0.0105762   0.2839905   1.        ]]. Action = [[0.7295079  0.21164727 0.98154926 0.9817765 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 237. State = [[-0.18599507  0.00676624  0.3224318   1.        ]]. Action = [[0.7998309  0.8332192  0.47816753 0.8916873 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 238. State = [[-0.15123503  0.03535041  0.34892142  1.        ]]. Action = [[0.81930363 0.47804892 0.2652278  0.19228733]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 239. State = [[-0.13521565  0.05449425  0.34691912  1.        ]]. Action = [[-0.7410239   0.16323173 -0.70387816  0.5253974 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 240. State = [[-0.13216671  0.06665702  0.33712062  1.        ]]. Action = [[0.80468965 0.32824564 0.07979035 0.28752065]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 241. State = [[-0.10887563  0.06460872  0.33438972  1.        ]]. Action = [[ 0.44502258 -0.51583916  0.04491484  0.18327022]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 242. State = [[-0.08327582  0.0552778   0.33270466  1.        ]]. Action = [[ 0.90893006 -0.06362462 -0.07507914  0.6047356 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 243. State = [[-0.05180543  0.03520564  0.34660962  1.        ]]. Action = [[ 0.15048826 -0.8751977   0.83690166  0.20406842]]. Reward = [0.]
Curr episode timestep = 10
Above hoop
Current timestep = 244. State = [[-0.04587942  0.02963225  0.38688505  1.        ]]. Action = [[-0.37457305  0.70238006  0.9810178   0.91498613]]. Reward = [0.]
Curr episode timestep = 11
Above hoop
Scene graph at timestep 244 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 244 is tensor(0.0416, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 0
Current timestep = 245. State = [[-0.05067256  0.04203216  0.4158704   1.        ]]. Action = [[ 0.7715287  -0.8336827   0.12448764  0.8315264 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Above hoop
Current timestep = 246. State = [[-0.05067256  0.04203216  0.4158704   1.        ]]. Action = [[ 0.4422449  -0.37849575  0.26993513  0.20751226]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Above hoop
Current timestep = 247. State = [[-0.05067256  0.04203216  0.4158704   1.        ]]. Action = [[0.13232017 0.6737275  0.7474253  0.50522304]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Above hoop
Current timestep = 248. State = [[-0.25081527  0.00250209  0.2326638   1.        ]]. Action = [[ 0.9251063   0.73621726 -0.526383   -0.05656469]]. Reward = [1000.]
Curr episode timestep = 15
Above hoop
Current timestep = 249. State = [[-0.24926311  0.01852857  0.23331045  1.        ]]. Action = [[0.58104753 0.98056483 0.02364492 0.6942196 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 250. State = [[-0.24482621  0.03805935  0.23455372  1.        ]]. Action = [[-0.74205554 -0.99123615 -0.38042593  0.82058966]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 251. State = [[-0.23968095  0.02526311  0.2294833   1.        ]]. Action = [[ 0.28806663 -0.8751174  -0.38050973  0.64251184]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 252. State = [[-0.21633077  0.00974863  0.21923774  1.        ]]. Action = [[ 0.8941319  -0.02736998 -0.12782973  0.8158672 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 253. State = [[-0.20360333 -0.00358379  0.2012387   1.        ]]. Action = [[-0.8239128  -0.4878682  -0.71386003  0.739231  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 254. State = [[-0.22720449 -0.00867518  0.17188835  1.        ]]. Action = [[-0.78731567  0.37444496 -0.6229891   0.06689215]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 255. State = [[-2.4723969e-01 -1.0023512e-04  1.5523936e-01  1.0000000e+00]]. Action = [[0.1315124  0.2925558  0.29767525 0.91559577]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 256. State = [[-0.24408355  0.02022548  0.147455    1.        ]]. Action = [[ 0.6319988   0.68670654 -0.8680407   0.90059614]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 257. State = [[-0.23787868  0.02907447  0.11249197  1.        ]]. Action = [[-0.31737417 -0.41476715 -0.8877135   0.70153165]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 258. State = [[-0.23848264  0.01757417  0.09440044  1.        ]]. Action = [[ 0.05973172 -0.40144962  0.86546016  0.6881671 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 259. State = [[-0.22028925 -0.00819564  0.1038807   1.        ]]. Action = [[ 0.954586   -0.8454122  -0.25755054  0.34442544]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 260. State = [[-0.19379091 -0.03115728  0.09446763  1.        ]]. Action = [[ 0.74448085 -0.06196243 -0.9090808   0.56737876]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 261. State = [[-0.17018935 -0.03644911  0.06633213  1.        ]]. Action = [[ 0.97629    -0.68518245 -0.70878726  0.9263594 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 262. State = [[-0.16735718 -0.03668024  0.06315111  1.        ]]. Action = [[ 0.6763942  -0.6037155  -0.17553008  0.04620862]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 263. State = [[-0.16710821 -0.03668335  0.06321733  1.        ]]. Action = [[ 0.5639026  -0.36683083 -0.59399533  0.9266238 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 264. State = [[-0.16647404 -0.03671227  0.0632654   1.        ]]. Action = [[ 0.55783844 -0.94167364  0.60062504  0.3781805 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 265. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7649237   0.15439022 -0.7525509   0.88415813]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 266. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9645765   0.89275837 -0.57948655  0.96424985]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 267. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9137485  -0.10321051  0.4125688   0.52348995]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 268. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.65110254 -0.5321882   0.88215494  0.7501507 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 269. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8035307   0.8530872  -0.06499553  0.70955443]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 270. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[0.5840063  0.09414351 0.29196084 0.97820663]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 271. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7320807  -0.38261795 -0.8412553   0.71692693]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 272. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7476523  0.9376905 -0.2298705  0.8425653]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 273. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7509891  -0.49055684 -0.5446807   0.95007443]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 274. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9127934   0.5166857  -0.79860616  0.31035924]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 275. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.90363145  0.85810304 -0.9098178   0.9323591 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 276. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.949849    0.69462526 -0.6558844   0.9325646 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 277. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9275596   0.51904726 -0.63575965  0.12472796]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 278. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.18389988  0.56752706 -0.8967721  -0.5551845 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 279. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.878562    0.8322023  -0.9948629   0.05436337]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 280. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9117031  -0.07578421 -0.50496775 -0.3486666 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 281. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.82251227  0.6739402  -0.3609848   0.9886749 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 282. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7759528   0.12536597 -0.8016708   0.9149052 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 283. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.24188507  0.76221013 -0.80428076  0.2124598 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 284. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9284867  0.8676084 -0.7102201  0.2530043]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 285. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9228575   0.85196435 -0.99651253  0.32785034]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 286. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.92350113  0.78827786 -0.9345594   0.598853  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 287. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.57783973  0.93377864 -0.8277865   0.46216202]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 288. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.967312    0.96513057 -0.76542723 -0.01485252]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 289. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8017647   0.94708276 -0.97271794 -0.22185141]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 290. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.89174247  0.71465635 -0.81759804 -0.6140877 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 291. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8562896   0.88726795 -0.711204   -0.06032348]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 292. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9496194   0.9639356  -0.81588006 -0.32895708]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 293. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.89890254  0.8273361  -0.9501157   0.23144698]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 294. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8668119  -0.25893927 -0.7334671  -0.35313743]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 295. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.80131936  0.8635317  -0.86342824 -0.08597279]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 296. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7363167   0.64075065 -0.95392066 -0.6927161 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 297. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8922099   0.9831152  -0.97722554 -0.3641256 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 298. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9300624   0.5131793   0.09897947 -0.05280018]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 299. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9486525   0.52718186 -0.8964052   0.5384073 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 300. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.99601936  0.8331269  -0.96457154 -0.45517212]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 301. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.6746807   0.5238317  -0.88157725 -0.2550863 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 302. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.67521024  0.4012779  -0.5165287  -0.30374163]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 303. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9691217  0.9570613 -0.6028658 -0.508383 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 304. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.97949195  0.92742467 -0.96109694 -0.61948377]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 305. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8218286   0.890416   -0.56341016 -0.7551506 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 306. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8756702   0.9090488  -0.97698814 -0.7381003 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 307. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.928329    0.85321844 -0.7904146   0.07963455]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 308. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.6392455   0.9497657  -0.24389237 -0.86625737]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 309. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9089316   0.87340236 -0.8327922  -0.22271806]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 310. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.471614    0.7956197  -0.8822697  -0.73906225]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 311. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.54948163  0.8428774  -0.8700526  -0.5077127 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 312. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9729781   0.75403905 -0.9734724  -0.07439262]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 313. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.82203627  0.77984345 -0.68394864 -0.26090527]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 314. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9657414   0.9189075  -0.96248317 -0.04455835]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 315. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.94772744  0.8806999  -0.91118497 -0.50595033]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 316. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.96365666  0.95194864 -0.8747201  -0.09443742]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 317. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9532151   0.8265226  -0.90664494 -0.0510754 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 318. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.94113374  0.95857143 -0.98210317 -0.45259392]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 319. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.93594074  0.9802426  -0.8014436  -0.03724593]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 320. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.7518188   0.9617878  -0.80438125  0.4771278 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 321. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9494152   0.93395567 -0.9341118  -0.04416752]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 322. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.95275426  0.8854989  -0.9922409  -0.55896753]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 323. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.97455144  0.7680962  -0.9848563   0.2130748 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 324. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.94135153  0.32521474 -0.7137069   0.35765183]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 325. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.96046877  0.8363781  -0.9273543  -0.15357608]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 326. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.896266    0.67391074 -0.9899825  -0.1245302 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 327. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.92697096  0.95770216 -0.7446581   0.1090728 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 328. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8280963   0.926286   -0.971967   -0.20309472]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 329. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9402554   0.53211904 -0.89905816 -0.35817993]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 330. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9707309   0.8624859  -0.96136343 -0.5812111 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 331. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.57066643  0.95763254 -0.90818983  0.05335486]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 332. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9669043  0.8555393 -0.9498769 -0.2777847]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 333. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.860075    0.9680445  -0.9159982  -0.10304326]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 334. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.969807    0.8667772  -0.9141492   0.05344319]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 335. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.942634    0.8786235  -0.98646367  0.02179897]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 336. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8759136   0.97363174 -0.85803616  0.36693847]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 337. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9699917   0.82967496 -0.9142457  -0.11307096]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 338. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9647733   0.98069596 -0.69886726 -0.35708725]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 339. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8883462   0.7760452  -0.94311607 -0.09563977]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 340. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.81659603  0.66564727 -0.7647186  -0.09380543]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 341. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.96318054  0.9720359  -0.7518018  -0.2647869 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 342. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.82501173  0.7975793  -0.7129858  -0.5270723 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 343. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.99146736  0.98616064 -0.8534192   0.11508989]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 344. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9496156   0.9731587  -0.84498245 -0.13367718]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 345. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.97093225  0.7558881  -0.83840835 -0.13130552]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 346. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9501054   0.8942367  -0.9038281  -0.35832083]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 347. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.955873    0.58821    -0.9478668  -0.07470858]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 348. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.6501639   0.91330826 -0.7461229  -0.30523443]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 349. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9708086   0.94790435 -0.84113353  0.12011313]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 350. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9240272   0.97039175 -0.60759157 -0.26253223]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 351. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.88338447  0.9614048  -0.9112012  -0.09106106]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 352. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8534043   0.5227586  -0.9609847  -0.03207642]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 353. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9429108   0.9778192  -0.77612627 -0.00568092]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 354. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.93580294  0.8275292  -0.8794015  -0.14726877]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 355. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9868592   0.77993536 -0.8587047   0.23491943]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 356. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.96203303  0.7552594  -0.92551404  0.1720897 ]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 357. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.8791121   0.82657003 -0.8886067  -0.3350219 ]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 358. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9890764   0.986832   -0.91573095 -0.2256478 ]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 359. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9429815   0.8578682  -0.91452897 -0.18293053]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 360. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.98074996  0.9165709  -0.94398737  0.13684976]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 361. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9589231   0.6439426  -0.3525597   0.17035043]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: No entry zone
Current timestep = 362. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9843526   0.896533   -0.8039852  -0.02315331]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 363. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.879158    0.9689052  -0.5389473  -0.23663074]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: No entry zone
Current timestep = 364. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.98093724  0.66606987 -0.9377871   0.1084758 ]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 365. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9457381   0.9218123  -0.47558355  0.3728869 ]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: No entry zone
Current timestep = 366. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9755366   0.9204799  -0.84513295  0.01292515]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 367. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.96094155  0.86314094 -0.89914614  0.08353925]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 368. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9875666   0.8233352  -0.05728084  0.16616929]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: No entry zone
Current timestep = 369. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9815632   0.79312027 -0.87802577  0.11391699]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 370. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9837086   0.6301007  -0.9136853   0.08422256]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 371. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.79816985  0.90123224 -0.8452027   0.10590184]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 372. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.9128505   0.82889724 -0.71949846  0.00593996]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 373. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.98522115  0.9526911  -0.74778247  0.05996227]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 374. State = [[-0.1660304  -0.03672713  0.06338255  1.        ]]. Action = [[ 0.90640855  0.9486866  -0.6236568   0.10837233]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 375. State = [[-0.2505355   0.00261525  0.23256974  1.        ]]. Action = [[ 0.98085105  0.81785774 -0.7958867   0.33293986]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 376. State = [[-0.250989    0.00236143  0.2324788   1.        ]]. Action = [[ 0.96939385  0.9636059   0.01569879 -0.04432833]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 377. State = [[-0.25054204  0.00224118  0.23242912  1.        ]]. Action = [[ 0.94929075  0.9255719  -0.7866535  -0.0813418 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 378. State = [[-0.2402199   0.01522204  0.23294598  1.        ]]. Action = [[ 0.8874599   0.6756748  -0.15775883  0.2937956 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 379. State = [[-0.21625425  0.04664174  0.22333279  1.        ]]. Action = [[ 0.8971126   0.90866625 -0.94330966  0.21898973]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 380. State = [[-0.18484218  0.07094442  0.19317572  1.        ]]. Action = [[ 0.9544256   0.94950783 -0.55880266  0.34434927]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Current timestep = 381. State = [[-0.17785618  0.07587646  0.18760777  1.        ]]. Action = [[ 0.85373485  0.76399314 -0.8917243  -0.1422981 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 382. State = [[-0.17772253  0.07625109  0.18696451  1.        ]]. Action = [[ 0.9418918   0.84678006 -0.9140581  -0.05957329]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 383. State = [[-0.17770104  0.07626817  0.18698038  1.        ]]. Action = [[ 0.9612758   0.92696226 -0.41258115  0.21577632]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 384. State = [[-0.17770104  0.07626817  0.18698038  1.        ]]. Action = [[ 0.906988    0.96771264 -0.9109668   0.15518057]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 385. State = [[-0.17770104  0.07626817  0.18698038  1.        ]]. Action = [[ 0.9947387   0.95962906 -0.61202264  0.22964394]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 386. State = [[-0.17777252  0.07642949  0.18698041  1.        ]]. Action = [[ 0.8441169   0.87766623 -0.7249042   0.04244339]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 387. State = [[-0.17777252  0.07642949  0.18698041  1.        ]]. Action = [[ 0.9105451   0.9170613  -0.7237635  -0.00405347]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 388. State = [[-0.17751071  0.07644567  0.18714043  1.        ]]. Action = [[ 0.95036614  0.82520545 -0.5542467   0.10989666]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 389. State = [[-0.17751071  0.07644567  0.18714043  1.        ]]. Action = [[ 0.9878695   0.8356148  -0.78632617  0.01471281]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 390. State = [[-0.17748924  0.07646275  0.18715633  1.        ]]. Action = [[ 0.8789171   0.759372   -0.17531651  0.089728  ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 391. State = [[-0.17730986  0.07646961  0.18723689  1.        ]]. Action = [[ 0.93258405  0.93739045 -0.71413463 -0.21923053]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 392. State = [[-0.17717566  0.0764856   0.18730137  1.        ]]. Action = [[ 0.9932933   0.9232185  -0.94266367 -0.01075035]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 393. State = [[-0.17717566  0.0764856   0.18730137  1.        ]]. Action = [[ 0.8190712   0.95185065 -0.8889072   0.08682692]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 394. State = [[-0.17717566  0.0764856   0.18730137  1.        ]]. Action = [[ 0.8993304   0.9564483  -0.8508552   0.11657131]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 395. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.970801    0.7018089  -0.7629733   0.02173042]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 396. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.9785652   0.72723794 -0.87456757 -0.16020733]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 397. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.82968044  0.96973383 -0.24087113  0.33012748]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 398. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.9132782   0.91451204 -0.8859894   0.30366588]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 399. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.97029674  0.7892214  -0.5073557   0.08867347]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 400. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.9479811   0.40887153 -0.78033096  0.13963318]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 401. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.914551    0.94255304 -0.7436295   0.22208595]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 402. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.91675687  0.89119005 -0.38984883  0.18633103]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 403. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.98592436  0.9724101  -0.7444245   0.05975127]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 404. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.95815635  0.838827   -0.83752924 -0.09104866]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 405. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.9907694   0.9055451  -0.8039865   0.03883553]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 406. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.94632745  0.80078316 -0.61408406  0.08065736]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 407. State = [[-0.17710857  0.0764936   0.18733363  1.        ]]. Action = [[ 0.9317944   0.75113535 -0.7763925   0.13889062]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 408. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.95547104  0.2838112  -0.70583373 -0.00320107]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 409. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9889711   0.79904735 -0.87959814 -0.14884502]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 410. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.83276534  0.912128   -0.82042235  0.28171444]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 411. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9472995   0.9400792  -0.57368934  0.17241943]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 412. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9810698   0.77283835 -0.8181124   0.0458287 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 413. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9757401   0.63237345 -0.6578929  -0.07977211]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 414. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97539425  0.71837354 -0.5158517   0.33734214]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 415. State = [[-0.17711088  0.07656434  0.18734956  1.        ]]. Action = [[ 0.9873831   0.88554335 -0.9194786   0.23085403]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 416. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9352462   0.9017472  -0.71253467  0.05376995]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 417. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9640733   0.89588714 -0.7394549  -0.09519446]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 418. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9169049   0.6314757  -0.6781489  -0.25556183]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 419. State = [[-0.17711088  0.07656434  0.18734956  1.        ]]. Action = [[ 0.9033241   0.85262275 -0.6009526  -0.11239499]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 420. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97636783  0.91344905 -0.59698534  0.01604033]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 421. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.93491673  0.87386405 -0.60852396  0.03645122]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 422. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9793315   0.7415292  -0.4449494   0.22813177]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 423. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9536071   0.81509674 -0.4045545   0.04026091]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 424. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.92136717  0.9668684  -0.01879138  0.01675749]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 425. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.89608574  0.8958719  -0.49314976 -0.26257497]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 426. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98482347  0.9240844  -0.10523665  0.275064  ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 427. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9334452  0.9162736 -0.4169165 -0.1555059]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 428. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.89564955  0.8332896  -0.6363588   0.32048726]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 429. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9917215   0.8649783  -0.4547646  -0.06490493]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 430. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9478421   0.60625935 -0.6944037   0.10281241]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 431. State = [[-0.17711088  0.07656434  0.18734956  1.        ]]. Action = [[ 0.9930301   0.07134759 -0.5760451   0.08188915]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 432. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97727704  0.4554317  -0.7646555   0.02397215]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 433. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9653578   0.6858076  -0.474082   -0.05232102]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 434. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9758966   0.5556127  -0.59318304  0.10751021]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 435. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98464465  0.83912814 -0.51722145 -0.17071629]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 436. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98355985  0.6559696  -0.5042505   0.04207337]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 437. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9585129   0.7173939  -0.68783057  0.17077339]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 438. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.7780845   0.72756433 -0.4764694  -0.19934636]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 439. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.8867419   0.7330673  -0.6122226   0.07663906]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 440. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.91699624  0.81708694 -0.6732785   0.02948904]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 441. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.96281576  0.86377    -0.41903216  0.00099444]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 442. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97267604  0.87474513 -0.36708516  0.26465273]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 443. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9780204   0.8644099  -0.6644642   0.29897666]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 444. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97549176  0.92428005 -0.28095412  0.0166105 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 445. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9528425   0.80846965 -0.6504757  -0.17746007]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 446. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 9.875896e-01  9.241687e-01 -5.682761e-01 -5.429387e-04]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 447. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.90948653  0.57930875 -0.52686256  0.05473328]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 448. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.985417    0.85535157 -0.5069309   0.15198052]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 449. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9804472   0.80145144 -0.53806776  0.15662849]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 450. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99435735  0.7587911  -0.44754446 -0.05658889]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 451. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9936764   0.84407306 -0.3009994   0.08046603]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 452. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.96316874  0.83545303 -0.42767382 -0.17221779]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 453. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99084544  0.7615261  -0.5749139  -0.17648774]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 454. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98373985  0.62156963 -0.54471964  0.08228385]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 455. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98560834  0.6484418  -0.41562665 -0.0959512 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 456. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9543686   0.8750485  -0.48566133  0.02515054]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 457. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99161327  0.7002425  -0.6491846   0.15419424]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 458. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.95373774  0.8877742  -0.667397    0.10082698]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 459. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9933535   0.6540477  -0.4488473   0.18371725]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 460. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9810134   0.88799274 -0.32271814  0.02557564]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 461. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99127007  0.57432294 -0.3042376   0.04515231]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 462. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.75746465  0.7473569  -0.5982618   0.08125508]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 463. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9483526   0.7145121  -0.16409862  0.07149529]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 464. State = [[-0.17711088  0.07656434  0.18734956  1.        ]]. Action = [[ 0.97301126  0.8421899  -0.17597306  0.23571491]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 465. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.8499992   0.86407816 -0.20939708  0.08198035]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 466. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9582081   0.43564403 -0.27528167  0.31134188]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 467. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9744332   0.50537515 -0.38842052 -0.00136787]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 468. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9759908   0.7574433  -0.3632902  -0.10221386]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 469. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.95939887  0.74561    -0.13145155 -0.09046441]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 470. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.951807    0.8553364  -0.39500642 -0.04170209]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 471. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97445536  0.74239445 -0.70988506 -0.2019527 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 472. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9580035   0.69335246 -0.5641804   0.08715868]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 473. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.97646093  0.65980434 -0.63765144  0.03704274]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 474. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98307395  0.9032097  -0.5211367   0.17121863]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 475. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99376607  0.8260145  -0.68816143 -0.09993356]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 476. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.96892047  0.3309927  -0.26199418  0.00344968]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 477. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99820983  0.67255175 -0.6690065  -0.02512008]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 478. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.95680976  0.10671616 -0.5501368  -0.16963685]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 479. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9791732   0.74278486 -0.5970312  -0.10115957]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 480. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9332317   0.18268156 -0.29216003 -0.02492958]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: No entry zone
Current timestep = 481. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.95773864  0.588238   -0.6168522   0.12007761]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: No entry zone
Current timestep = 482. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9909024   0.61626756 -0.48696864  0.13286829]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: No entry zone
Current timestep = 483. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98593116  0.6863625  -0.67088693  0.03787816]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: No entry zone
Current timestep = 484. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9954419   0.5359266  -0.5631232  -0.02611673]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: No entry zone
Current timestep = 485. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.989429    0.6529032  -0.34307003 -0.17135346]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: No entry zone
Current timestep = 486. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.95515513  0.77715516 -0.4922983   0.18956542]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: No entry zone
Current timestep = 487. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9774492   0.89224505 -0.5347929   0.04493785]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: No entry zone
Current timestep = 488. State = [[-0.17711088  0.07656434  0.18734956  1.        ]]. Action = [[ 0.95632744  0.72933555 -0.47434652 -0.229473  ]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: No entry zone
Current timestep = 489. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.8212788   0.707602   -0.27730072 -0.1914326 ]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: No entry zone
Current timestep = 490. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99315345  0.7852447  -0.24861777  0.05759192]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: No entry zone
Current timestep = 491. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98393345  0.79283226 -0.6408844   0.04429638]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: No entry zone
Current timestep = 492. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9860375   0.69143176 -0.40300345  0.00252223]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: No entry zone
Current timestep = 493. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.989506    0.58636785 -0.5120633   0.24570549]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: No entry zone
Current timestep = 494. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9576936   0.00130546 -0.32172972 -0.14432967]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: No entry zone
Current timestep = 495. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.98933244  0.4625075  -0.75229317 -0.0409202 ]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: No entry zone
Current timestep = 496. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9420874   0.60430753 -0.45826232  0.04384315]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: No entry zone
Current timestep = 497. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.93142533  0.83458745 -0.24409902 -0.1264072 ]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: No entry zone
Current timestep = 498. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.99397874  0.6114681  -0.31480527 -0.10406017]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: No entry zone
Current timestep = 499. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9793844   0.66557336 -0.5466816   0.01227379]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: No entry zone
Current timestep = 500. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9761214   0.7228615  -0.460073   -0.19244051]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: No entry zone
Current timestep = 501. State = [[-0.17711088  0.07656434  0.18734956  1.        ]]. Action = [[ 0.97995734  0.46555257 -0.19396013 -0.10495508]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: No entry zone
Current timestep = 502. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.9677665   0.6739676  -0.25798333  0.13884926]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: No entry zone
Current timestep = 503. State = [[-0.17713235  0.07654726  0.18733364  1.        ]]. Action = [[ 0.8814136   0.6883292  -0.30534083 -0.15001589]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: No entry zone
Current timestep = 504. State = [[-0.25052038  0.00220684  0.23246469  1.        ]]. Action = [[ 0.9246757   0.7804656  -0.45177305 -0.16681123]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: No entry zone
Current timestep = 505. State = [[-0.2504261   0.00226649  0.23252483  1.        ]]. Action = [[ 0.9799745   0.7786126  -0.5711033  -0.38381463]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 506. State = [[-0.23911645  0.00986707  0.22830354  1.        ]]. Action = [[ 0.9952332   0.42484987 -0.47147453  0.25677776]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 507. State = [[-0.25052968  0.00236058  0.23244199  1.        ]]. Action = [[ 0.9300194   0.7642603  -0.3502438  -0.07155907]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 508. State = [[-0.25041538  0.00222163  0.2324801   1.        ]]. Action = [[ 0.9534639   0.6856148  -0.5912617  -0.06354541]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 509. State = [[-0.25043222  0.00234544  0.23246528  1.        ]]. Action = [[ 0.88204813  0.72252965 -0.40049648 -0.01295996]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 510. State = [[-0.25011843  0.00244391  0.23270468  1.        ]]. Action = [[ 0.98482466  0.8122479  -0.44470716 -0.1666913 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 511. State = [[-0.24011819  0.01529622  0.23042749  1.        ]]. Action = [[ 0.94386065  0.6913954  -0.37849593  0.10053611]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 512. State = [[-0.25097483  0.0026413   0.23266399  1.        ]]. Action = [[ 0.9637182   0.7792027  -0.30189633 -0.02690101]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 513. State = [[-0.24006572  0.01176504  0.23039398  1.        ]]. Action = [[ 0.89674187  0.47727215 -0.3735124   0.24662018]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 514. State = [[-0.20919834  0.03414923  0.2197526   1.        ]]. Action = [[ 0.94891036  0.6875106  -0.36636877  0.13379538]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 515. State = [[-0.18071906  0.05245847  0.20615506  1.        ]]. Action = [[ 0.9812392   0.78003716 -0.4801017  -0.00612092]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Current timestep = 516. State = [[-0.17547397  0.05539539  0.20404047  1.        ]]. Action = [[ 0.98216796  0.5470773  -0.41876787  0.33447695]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 517. State = [[-0.17431429  0.05615366  0.20438169  1.        ]]. Action = [[ 0.9940989   0.53592205 -0.8265742  -0.16386169]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 518. State = [[-0.17362738  0.05639265  0.20475696  1.        ]]. Action = [[ 0.9934993   0.6792153  -0.5373787   0.07789719]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 519. State = [[-0.17358246  0.05638193  0.20477475  1.        ]]. Action = [[ 0.97869813  0.7077687  -0.68627715  0.27443886]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 520. State = [[-0.17343087  0.05647538  0.2048581   1.        ]]. Action = [[ 0.9921285   0.46937585 -0.42708075  0.07368588]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 521. State = [[-0.17322797  0.05649343  0.20496273  1.        ]]. Action = [[ 0.9196218   0.7256391  -0.58090144 -0.11906147]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 522. State = [[-0.1732304   0.05655893  0.2049614   1.        ]]. Action = [[ 0.9674573   0.23722899 -0.2631539  -0.1325056 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 523. State = [[-0.1732304   0.05655893  0.2049614   1.        ]]. Action = [[ 0.94276595  0.8265325  -0.5373812  -0.30413508]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 524. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.96517754  0.69644284 -0.33287472 -0.49066484]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 525. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9832151   0.80382264 -0.36215806 -0.13104552]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 526. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9821793   0.53613305 -0.6690681  -0.0941211 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 527. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.94782066  0.74350035 -0.5917147  -0.36202025]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 528. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9406998   0.7239013  -0.541841   -0.32983434]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 529. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.8179369   0.85263443 -0.45476723 -0.22800791]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 530. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9952949   0.542449   -0.34976423 -0.31820476]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 531. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9816165   0.8117949  -0.17803109  0.10750043]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 532. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.991348    0.65073395 -0.38323408  0.00977075]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 533. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9893178   0.8604486  -0.3864503  -0.19367617]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 534. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9806967   0.85129714 -0.6201322   0.2978623 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 535. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9656918   0.83832264 -0.6097238  -0.4308948 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 536. State = [[-0.17323284  0.05662443  0.20496008  1.        ]]. Action = [[ 0.9443667   0.5793841  -0.6502793  -0.13314193]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 537. State = [[-0.173167    0.0566303   0.20499405  1.        ]]. Action = [[ 0.96741295  0.78332853 -0.81759256  0.20797682]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 538. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.9745593   0.41548157 -0.6590944  -0.11461174]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 539. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.9425111   0.5903518  -0.5144968  -0.00154281]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 540. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.9864042   0.5475297  -0.66959465 -0.08602965]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 541. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.98913264  0.8088908  -0.67594975  0.01524389]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 542. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.9490571   0.6673198  -0.6809602  -0.03730106]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 543. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.88194203  0.8339484  -0.5950561  -0.10515714]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 544. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.93152595  0.5732701  -0.56718355 -0.2169851 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 545. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.9815843   0.7920052  -0.70798385 -0.03447849]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 546. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.9745538   0.85757685 -0.34025526 -0.12632215]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 547. State = [[-0.17316946  0.05669581  0.20499273  1.        ]]. Action = [[ 0.99431634  0.50208426 -0.6285592  -0.30379295]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 548. State = [[-0.17317192  0.05676131  0.20499142  1.        ]]. Action = [[ 0.95113003  0.76776075 -0.7029914   0.22308183]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 549. State = [[-0.17317192  0.05676131  0.20499142  1.        ]]. Action = [[ 0.84759283  0.35138142 -0.5552126   0.10938013]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 550. State = [[-0.17317192  0.05676131  0.20499142  1.        ]]. Action = [[ 0.9839978   0.69122624  0.09267533 -0.07701731]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 551. State = [[-0.17317192  0.05676131  0.20499142  1.        ]]. Action = [[ 0.9873626   0.87354255 -0.22813922 -0.4899087 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 552. State = [[-0.1731744   0.05682682  0.2049901   1.        ]]. Action = [[ 0.9844215   0.832968   -0.43285024 -0.3100049 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 553. State = [[-0.1731744   0.05682682  0.2049901   1.        ]]. Action = [[ 0.8078077   0.6960573  -0.16307986 -0.03327322]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 554. State = [[-0.1731744   0.05682682  0.2049901   1.        ]]. Action = [[ 0.90591574  0.79412055 -0.32221568 -0.17470032]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 555. State = [[-0.1731744   0.05682682  0.2049901   1.        ]]. Action = [[ 0.6923063  0.4205954 -0.5478766  0.3383026]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 556. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.94295144  0.58020735 -0.6020269   0.20575464]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 557. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9491596   0.579643   -0.38857096 -0.17837572]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 558. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9465785  0.8034954 -0.7180674 -0.1267305]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 559. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.97583604  0.78406703 -0.7482984  -0.21330869]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 560. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9870554   0.35711873 -0.30115736  0.23955858]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 561. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9670472   0.56337285 -0.607149   -0.24120194]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 562. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9140551   0.6422534  -0.6673534  -0.36300743]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 563. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9824133  0.8913431 -0.5770822 -0.3149597]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 564. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9833163   0.7966614  -0.6756968  -0.02426738]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 565. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9753115   0.81854355 -0.6489038  -0.19126022]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 566. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.9565165   0.6200154  -0.7598875  -0.47421366]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 567. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.99215925  0.78163075 -0.63406235 -0.16482043]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 568. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.84551024  0.6623235  -0.24691403 -0.42180192]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 569. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.97895026  0.8498862  -0.33766276 -0.17509699]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 570. State = [[-0.17317685  0.05689188  0.2049888   1.        ]]. Action = [[ 0.94264996  0.6786132  -0.7088386  -0.04026449]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 571. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.95661616  0.6284323  -0.4691645  -0.5378412 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 572. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.98797464  0.7697078  -0.48387265 -0.17126298]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 573. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99192524  0.74135983 -0.5361302   0.00523973]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 574. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9975283   0.67026854 -0.33683157 -0.2248261 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 575. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9905596   0.68058586 -0.68076634 -0.34280455]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 576. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.98028874  0.7286937  -0.49087453  0.07511389]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 577. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.96743226  0.90763855 -0.85441816 -0.14652598]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 578. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99117947  0.75242066 -0.61224467 -0.38975668]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 579. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.94303393  0.7719917  -0.66569364 -0.11448938]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 580. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9324641  0.8494489 -0.7514501  0.3939917]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 581. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9827876   0.8250605  -0.4872142  -0.40019345]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 582. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.92166054  0.83325315 -0.75473404 -0.4299872 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 583. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99562955  0.81946063 -0.6852935  -0.11031646]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 584. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.8919618   0.88289714 -0.8099566  -0.34335184]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 585. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99557996  0.6980295  -0.43620694  0.10705388]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 586. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9548087   0.50903726 -0.6829255   0.1970917 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 587. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9455935   0.7664492  -0.49806857 -0.19130754]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 588. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9683324   0.82244396 -0.4521116  -0.60889643]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 589. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.94666123  0.76343155 -0.47785658 -0.04935473]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 590. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.98594046  0.81161857 -0.7429439  -0.04567999]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 591. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.96539736  0.8483746  -0.6596629  -0.01964271]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 592. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9122286   0.90418375 -0.68856347 -0.2987554 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 593. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.974753    0.7973523  -0.09101582  0.04300618]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 594. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.96039486  0.64655066 -0.73948705  0.07000303]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 595. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99945855  0.8847586  -0.47316313 -0.2051403 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 596. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.98855865  0.7586975  -0.42676842  0.09586954]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 597. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9979527   0.73802114 -0.21150196 -0.06957161]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 598. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.8260164   0.72238374 -0.44877088  0.05378914]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 599. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.8120525   0.5385846  -0.870783   -0.25127256]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 600. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.92721546  0.8568679  -0.7704439  -0.33380234]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 601. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9641311   0.6663575  -0.69879717 -0.5259778 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 602. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9603194   0.640533   -0.6719578  -0.23547018]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 603. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9163711   0.7552084  -0.32287198 -0.14655733]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 604. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9440019   0.6813568  -0.49327338 -0.43660104]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 605. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.95565724  0.7958859  -0.7582149  -0.35465997]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 606. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9856169   0.6109841  -0.8089247  -0.28285813]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 607. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.8919071   0.7772175  -0.81931865 -0.12590104]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 608. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.95547366  0.8849971  -0.6354495  -0.04762828]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 609. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.95693827  0.63519096 -0.64664704 -0.03881943]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 610. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.970603    0.6822709  -0.64195156 -0.04039788]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 611. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.97770643  0.33785295 -0.8581602  -0.01614195]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 612. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9632871   0.7103448  -0.77167267 -0.080993  ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 613. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.95655346  0.83342576 -0.17875719 -0.0965836 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 614. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.97143424  0.79693294 -0.5010884  -0.18417001]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 615. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9619098  0.8639574 -0.2015531  0.1866101]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: No entry zone
Current timestep = 616. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9903619   0.8273268  -0.8401158  -0.19722277]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: No entry zone
Current timestep = 617. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 9.7967172e-01  8.7884736e-01 -5.6257731e-01 -1.9681454e-04]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: No entry zone
Current timestep = 618. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.98237777  0.74260783 -0.54230374 -0.3278346 ]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: No entry zone
Current timestep = 619. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99309397  0.7604667  -0.710175   -0.22807252]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: No entry zone
Current timestep = 620. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99295616  0.90099573 -0.63273007 -0.171296  ]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: No entry zone
Current timestep = 621. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9872371   0.7656944  -0.18770593  0.11655128]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: No entry zone
Current timestep = 622. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9466871   0.68284607 -0.4987719  -0.0872348 ]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: No entry zone
Current timestep = 623. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9497287   0.7425873  -0.44353342  0.05695271]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: No entry zone
Current timestep = 624. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.9698143   0.6743053  -0.60538554  0.20527768]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: No entry zone
Current timestep = 625. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.96052027  0.90095806 -0.64160526 -0.16513348]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: No entry zone
Current timestep = 626. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.8762028  0.7836379 -0.3607337 -0.3257612]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: No entry zone
Current timestep = 627. State = [[-0.17317934  0.05695739  0.20498751  1.        ]]. Action = [[ 0.99175894  0.62838936 -0.53564453  0.00828099]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: No entry zone
Current timestep = 628. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9630827   0.8459735  -0.75373197 -0.10236776]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: No entry zone
Current timestep = 629. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.95269036  0.578933   -0.8593722   0.28484678]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: No entry zone
Current timestep = 630. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9753567   0.8814969  -0.45515186 -0.11444652]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: No entry zone
Current timestep = 631. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.97593606  0.76303244 -0.65436894 -0.02372414]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: No entry zone
Current timestep = 632. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9541105   0.61137867 -0.5367819  -0.16877663]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: No entry zone
Current timestep = 633. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.8940027   0.38185406 -0.18273145  0.13809562]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: No entry zone
Current timestep = 634. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9295428   0.6455693  -0.5781243  -0.09503955]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: No entry zone
Current timestep = 635. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.97261834  0.8914106  -0.42136204  0.10770738]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: No entry zone
Current timestep = 636. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9856534   0.7957959  -0.83052707 -0.12802178]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: No entry zone
Current timestep = 637. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9570501   0.8966875  -0.305601   -0.21002638]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: No entry zone
Current timestep = 638. State = [[-0.17318183  0.05702289  0.20498621  1.        ]]. Action = [[ 0.9892726   0.8390174  -0.7060111  -0.13601732]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: No entry zone
Current timestep = 639. State = [[-0.25142625  0.00230073  0.23236819  1.        ]]. Action = [[ 0.9856951   0.81444955 -0.46476007 -0.29905057]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: No entry zone
Current timestep = 640. State = [[-0.25044     0.00229814  0.23250993  1.        ]]. Action = [[ 0.89740646  0.8274021  -0.7183986  -0.0407908 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 641. State = [[-0.25042638  0.0021483   0.23242946  1.        ]]. Action = [[ 0.90885556  0.68056965 -0.8787203  -0.25025117]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 642. State = [[-0.250598    0.00231897  0.23241413  1.        ]]. Action = [[ 0.9059181   0.5656568  -0.59342194 -0.05264777]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 643. State = [[-0.25053355  0.00229284  0.23268642  1.        ]]. Action = [[ 0.9664296   0.45437443 -0.26413673 -0.0272302 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 644. State = [[-0.23988488  0.01337293  0.23144636  1.        ]]. Action = [[ 0.9659233   0.639019   -0.27728766  0.06123602]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 645. State = [[-0.20710482  0.03899992  0.22463796  1.        ]]. Action = [[ 0.9361737   0.6982286  -0.15053004  0.09615803]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 646. State = [[-0.18011536  0.05740019  0.21648258  1.        ]]. Action = [[ 0.9243481   0.37575936 -0.6806832  -0.33409983]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Current timestep = 647. State = [[-0.17353263  0.06039743  0.21713647  1.        ]]. Action = [[ 0.9690027   0.81889474 -0.4689415   0.04772007]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 648. State = [[-0.17341541  0.06081656  0.21735318  1.        ]]. Action = [[ 0.9909699   0.83000386 -0.78202647 -0.0766046 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 649. State = [[-0.17342712  0.06107781  0.21734628  1.        ]]. Action = [[ 0.957798    0.8252249  -0.46442807 -0.12532961]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 650. State = [[-0.17342311  0.06107807  0.21744373  1.        ]]. Action = [[ 0.9462594   0.86932445 -0.7581521   0.34857142]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 651. State = [[-0.17342606  0.06114348  0.21744202  1.        ]]. Action = [[ 0.9248624   0.40954828 -0.72670615  0.20110536]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 652. State = [[-0.17342606  0.06114348  0.21744202  1.        ]]. Action = [[ 0.94210887  0.8783355  -0.86668146 -0.23465449]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 653. State = [[-0.17342606  0.06114348  0.21744202  1.        ]]. Action = [[ 0.97761583  0.82146    -0.62470007 -0.1074121 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 654. State = [[-0.17342606  0.06114348  0.21744202  1.        ]]. Action = [[ 0.9510672   0.85591054 -0.3446666  -0.1363762 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 655. State = [[-0.17342903  0.0612089   0.2174403   1.        ]]. Action = [[ 0.9660653   0.82923794 -0.28489268 -0.28852516]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 656. State = [[-0.17342903  0.0612089   0.2174403   1.        ]]. Action = [[ 0.9840851   0.8341447  -0.61464894 -0.14903724]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 657. State = [[-0.17343199  0.06127431  0.21743861  1.        ]]. Action = [[ 0.9928951   0.85245466 -0.60299987 -0.41900468]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 658. State = [[-0.17343199  0.06127431  0.21743861  1.        ]]. Action = [[ 0.9439206   0.7946522  -0.4942248   0.16040719]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 659. State = [[-0.17343199  0.06127431  0.21743861  1.        ]]. Action = [[ 0.9550407   0.74645114 -0.02186906 -0.50109124]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 660. State = [[-0.17343199  0.06127431  0.21743861  1.        ]]. Action = [[ 0.9915068   0.7705059  -0.66776633 -0.14374882]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 661. State = [[-0.17343199  0.06127431  0.21743861  1.        ]]. Action = [[ 0.9950334   0.7428417  -0.665425   -0.36757827]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 662. State = [[-0.17343199  0.06127431  0.21743861  1.        ]]. Action = [[ 0.98694324  0.84066784 -0.7431075   0.05229056]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 663. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.91837776  0.87016296 -0.5373481  -0.20452225]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 664. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.99289227  0.842675   -0.4955464  -0.22295684]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 665. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9691186   0.5890341  -0.35418797 -0.05986273]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 666. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9869623   0.7097912  -0.44432187  0.18385577]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 667. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9407122  0.7552011 -0.4207703 -0.2643804]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 668. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.96177876  0.592945   -0.57209104 -0.23284417]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 669. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.96529055  0.71390843 -0.12103176  0.01567411]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 670. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.99233973  0.7310724  -0.6344414  -0.00196052]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 671. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.95494556  0.8448185  -0.75704813 -0.30091196]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 672. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9600285   0.3805194  -0.8106428  -0.17175746]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 673. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9877509   0.78399634 -0.6814255   0.03706503]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 674. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.99117875  0.5236565  -0.71449816  0.32533622]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 675. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9168041   0.84394467 -0.5081182   0.16838217]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 676. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9416857   0.92808867 -0.7432586   0.05777073]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 677. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.99009657  0.6819699  -0.7284886   0.35859203]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 678. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.87814593  0.711195   -0.70148677 -0.07236719]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 679. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.93683994  0.61616266 -0.58993125  0.3590243 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 680. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.98534465  0.40592384 -0.3940394  -0.14473891]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 681. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9781921   0.67081046 -0.6638215   0.11504972]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 682. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[0.9269829  0.5921092  0.0318923  0.02238941]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 683. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.95737314  0.8876091  -0.66752845  0.09923291]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 684. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.98163724  0.6104082  -0.6593446   0.2718072 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 685. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9433713   0.20670176 -0.6133682  -0.00735581]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 686. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9840478   0.6720381  -0.7532997   0.20906305]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 687. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.8837086   0.74768806 -0.6304466  -0.1879046 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 688. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9882674   0.6726613  -0.3873539   0.34369075]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 689. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9891095   0.83146596 -0.34757042  0.07966161]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 690. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.984161    0.932264   -0.6936942  -0.00472224]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 691. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.980392    0.43554485 -0.09135538 -0.0527671 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 692. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.8036461   0.6697444  -0.5376287   0.24650502]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 693. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.8362422   0.8911066  -0.63197255 -0.27877963]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 694. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.97608566  0.86209905 -0.7438816  -0.43600827]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 695. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.90193343  0.85410523 -0.68439436 -0.2530955 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 696. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9925151   0.7621014  -0.6643966   0.31812918]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 697. State = [[-0.17343497  0.06133973  0.21743691  1.        ]]. Action = [[ 0.9955704   0.6272682  -0.44502622 -0.2912439 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 698. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9769138   0.570892   -0.43838692 -0.21816695]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 699. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9563724   0.7770779  -0.326621    0.08832526]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 700. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.99026597  0.83476806 -0.5428519   0.0150584 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 701. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9698944  0.9617419 -0.711507   0.2355622]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 702. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.99678826  0.69125247 -0.39622265  0.21350992]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 703. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9913238   0.39607346 -0.08002096  0.11524689]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 704. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9440948   0.7245867  -0.60669214 -0.22463632]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 705. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9734969   0.81845975 -0.1171754   0.42194116]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 706. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9939245   0.78196406 -0.7404281   0.22174   ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 707. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9904094   0.728127   -0.7104743  -0.14293784]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 708. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.96471334  0.38717103 -0.48431504 -0.03261721]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 709. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9872906   0.8469281  -0.598788   -0.04073197]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 710. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9612961   0.6124083  -0.67761415 -0.4504068 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 711. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.8478062   0.46008086 -0.5631118  -0.09976602]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 712. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.99712837  0.64692664 -0.5067358  -0.29499817]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 713. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.90795183  0.7758324  -0.6971886  -0.33770955]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 714. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9928386   0.85457873 -0.5985732   0.27094662]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 715. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.98389137  0.6299119  -0.63787234 -0.03919852]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 716. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.98353565  0.769938   -0.36192644  0.04328024]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 717. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9915631   0.6701224  -0.33689177 -0.07223308]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 718. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.98313355  0.79323864 -0.7660635   0.01136708]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 719. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9691638   0.81600714 -0.52591777 -0.07245636]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 720. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.8708041   0.7954788  -0.5854624  -0.16786438]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Current timestep = 721. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9965695   0.81191564 -0.69428897  0.00799096]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Current timestep = 722. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9692924  0.9366958 -0.6660893 -0.2429645]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Current timestep = 723. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.99015284  0.5963769  -0.6139737   0.10931098]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 724. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.994382    0.6275592  -0.54246664  0.2627399 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 725. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9961796   0.7483592  -0.37770593  0.07399642]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 726. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.45544147  0.7463616  -0.3702225   0.11788869]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 727. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9294598   0.65684485 -0.5597965  -0.13577777]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 728. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.97191525  0.633157   -0.23920149 -0.05404013]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 729. State = [[-0.17343794  0.0614047   0.21743523  1.        ]]. Action = [[ 0.9853959   0.2702278  -0.34015787  0.17709303]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 730. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.93557334  0.87220526 -0.58438534  0.05823326]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 731. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9972334   0.85910416 -0.7809186   0.15908134]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 732. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.95713305  0.77528214 -0.38041866  0.14901507]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 733. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9043274   0.407225   -0.40616196  0.0740279 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 734. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9560095   0.48765516 -0.50421387  0.00528038]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 735. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.969543    0.3538096  -0.32668614  0.1404531 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 736. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9659722   0.35827923 -0.11505473  0.10053456]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 737. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.97553015  0.6742966  -0.6599392   0.08249629]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 738. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9518297   0.5485277  -0.5058549   0.07391608]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 739. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9831066   0.48857677 -0.58992374  0.06503224]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 740. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.8483026   0.2510097  -0.48865438  0.3176669 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 741. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.98843336  0.5495875  -0.20483953  0.06504321]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 742. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.99510765  0.61254454 -0.7139637   0.27830815]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 743. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.9783163   0.7374444  -0.4163133   0.02832592]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 744. State = [[-0.17344092  0.06147011  0.21743353  1.        ]]. Action = [[ 0.97471     0.712095   -0.44755924 -0.01995236]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 745. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9685674   0.40479004 -0.68707246  0.12342227]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 746. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.97655106  0.47460103 -0.17275572  0.01914835]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: No entry zone
Current timestep = 747. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.93317556  0.56939745 -0.07649946 -0.14067578]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: No entry zone
Current timestep = 748. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9928024   0.50457907 -0.36662555  0.11862791]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: No entry zone
Current timestep = 749. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9491432   0.7706897  -0.28725612 -0.05943412]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: No entry zone
Current timestep = 750. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.95483565  0.81127024 -0.6416737   0.14253104]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: No entry zone
Current timestep = 751. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9835954   0.4104339  -0.85769755  0.05081558]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: No entry zone
Current timestep = 752. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9485651   0.6607926  -0.26413453 -0.12430316]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: No entry zone
Current timestep = 753. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.97354865  0.74237573 -0.5270423   0.01936746]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: No entry zone
Current timestep = 754. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9587122   0.741385   -0.66661274  0.40670896]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: No entry zone
Current timestep = 755. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.92643404  0.6801603  -0.42428553 -0.06787324]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: No entry zone
Current timestep = 756. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9877187   0.61026156 -0.36000323  0.2743411 ]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: No entry zone
Current timestep = 757. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9470955   0.590232   -0.35648084  0.26069164]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: No entry zone
Current timestep = 758. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.9493685  0.7772722 -0.4397344  0.3379873]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: No entry zone
Current timestep = 759. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.8858458   0.46890247 -0.5789259   0.06329179]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: No entry zone
Current timestep = 760. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.7905526   0.29363644 -0.44696498  0.33245635]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: No entry zone
Current timestep = 761. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.7966716   0.8533881  -0.23860991  0.15573597]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: No entry zone
Current timestep = 762. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.98407483  0.29172766 -0.25259423  0.6297684 ]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: No entry zone
Current timestep = 763. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.992502    0.49570632 -0.30925524  0.4659052 ]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: No entry zone
Current timestep = 764. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.96447504  0.27850747 -0.45949638  0.54877114]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: No entry zone
Current timestep = 765. State = [[-0.17344393  0.06153552  0.21743184  1.        ]]. Action = [[ 0.87296593  0.36288202 -0.35499585 -0.10185248]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: No entry zone
Current timestep = 766. State = [[-0.17344694  0.06160093  0.21743016  1.        ]]. Action = [[ 0.9496045   0.10288894 -0.69438493  0.42946196]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: No entry zone
Current timestep = 767. State = [[-0.17344694  0.06160093  0.21743016  1.        ]]. Action = [[ 0.93639445 -0.08441854 -0.34389526  0.46363318]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: No entry zone
Current timestep = 768. State = [[-0.17344694  0.06160093  0.21743016  1.        ]]. Action = [[ 0.9651418   0.67364717 -0.55042654  0.37796438]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: No entry zone
Current timestep = 769. State = [[-0.17344694  0.06160093  0.21743016  1.        ]]. Action = [[ 0.9721396   0.60648286 -0.5277253   0.25570357]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: No entry zone
Current timestep = 770. State = [[-0.25099918  0.00236143  0.23244098  1.        ]]. Action = [[ 0.542663   -0.09535414 -0.02746141  0.5667844 ]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: No entry zone
Current timestep = 771. State = [[-0.23777269  0.0037188   0.24006525  1.        ]]. Action = [[0.9641887  0.13819063 0.45084321 0.5327554 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 772. State = [[-0.20888154  0.00395815  0.24668454  1.        ]]. Action = [[ 0.93401885 -0.12145311 -0.4710455   0.38524818]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 773. State = [[-0.17153822  0.00523129  0.23794965  1.        ]]. Action = [[0.63939893 0.14989734 0.07504046 0.51647735]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 774. State = [[-0.14862075  0.00741     0.23913097  1.        ]]. Action = [[ 0.9456917  -0.20777708 -0.11798102  0.36816287]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 775. State = [[-0.14419995  0.00827667  0.24002777  1.        ]]. Action = [[ 0.92435944  0.27048802 -0.30449045  0.2980441 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 776. State = [[-0.1307877   0.01397407  0.24613921  1.        ]]. Action = [[0.8414992  0.31880283 0.23764384 0.32399416]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 777. State = [[-0.09815879  0.03075147  0.25063327  1.        ]]. Action = [[ 0.6722119   0.5090797  -0.10330456  0.25292575]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 778. State = [[-0.06008848  0.03464128  0.2521722   1.        ]]. Action = [[ 0.98942363 -0.5029105   0.13654065  0.3849386 ]]. Reward = [0.]
Curr episode timestep = 7
Above hoop
Current timestep = 779. State = [[-0.02081036  0.03290786  0.25456455  1.        ]]. Action = [[ 0.8931742   0.2716744  -0.17320156  0.5072973 ]]. Reward = [0.]
Curr episode timestep = 8
Above hoop
Scene graph at timestep 779 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 779 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[0.03360277 0.0421091  0.24670789 1.        ]]. Action = [[ 0.9085747   0.28171885 -0.16396105  0.50842893]]. Reward = [0.]
Curr episode timestep = 9
Above hoop
Scene graph at timestep 780 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 780 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 780 of -1
Current timestep = 781. State = [[0.07459083 0.05119325 0.24019171 1.        ]]. Action = [[ 0.995986    0.1267134  -0.06393003  0.4793434 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 781 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 781 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 781 of -1
Current timestep = 782. State = [[0.07459083 0.05119325 0.24019171 1.        ]]. Action = [[ 0.9711063  -0.06810808  0.402722    0.29871547]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 782 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 782 is tensor(0.0315, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of -1
Current timestep = 783. State = [[0.07459083 0.05119325 0.24019171 1.        ]]. Action = [[ 0.9502275  -0.25417995 -0.0403831   0.4391166 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 783 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 783 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 783 of -1
Current timestep = 784. State = [[0.07459083 0.05119325 0.24019171 1.        ]]. Action = [[0.9959576  0.17228985 0.0159111  0.5710459 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 784 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 784 is tensor(0.0458, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of -1
Current timestep = 785. State = [[0.07463528 0.0511954  0.24024461 1.        ]]. Action = [[ 0.92463636 -0.0663662   0.2178123   0.56254125]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 785 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 785 is tensor(0.0384, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 785 of -1
Current timestep = 786. State = [[0.07461391 0.05127563 0.2402446  1.        ]]. Action = [[0.95406115 0.13617063 0.26735854 0.53626525]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 786 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 786 is tensor(0.0443, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of -1
Current timestep = 787. State = [[0.07461391 0.05127563 0.2402446  1.        ]]. Action = [[0.9512074  0.15193224 0.14708626 0.44755042]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 787 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 787 is tensor(0.0487, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of -1
Current timestep = 788. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9975133 0.259825  0.2543553 0.6248138]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 788 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 788 is tensor(0.0566, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 788 of -1
Current timestep = 789. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9309981  -0.17746252  0.39987803  0.57280016]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 789 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 789 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of -1
Current timestep = 790. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9930794  -0.10309935  0.22934985  0.6553316 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 790 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 790 is tensor(0.0449, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of -1
Current timestep = 791. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9677832  0.40028572 0.01968813 0.5942023 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 791 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 791 is tensor(0.0583, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 791 of -1
Current timestep = 792. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.6764877  -0.2346927   0.2752024   0.56798744]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 792 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 792 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of -1
Current timestep = 793. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.94719815 0.10263467 0.15459418 0.5602751 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 793 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 793 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 793 of -1
Current timestep = 794. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9711697  -0.08318096  0.26038074  0.66397   ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 794 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 794 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of -1
Current timestep = 795. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.94553113 0.20366657 0.40275884 0.6539848 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 796. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.98288536 0.08189249 0.18360841 0.57433915]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 797. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9168372  -0.05120718  0.31362534  0.67245054]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 798. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9845059  0.07097483 0.3483348  0.5056871 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 799. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.97781396 -0.09027016  0.4669547   0.7788274 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 800. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9351227  0.56546974 0.10782528 0.6539993 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 801. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9438293  0.52029645 0.42232394 0.5422369 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 802. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.99603856 0.06250298 0.41677678 0.6910236 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 803. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.96885884 -0.42894918  0.17250896  0.64612794]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 804. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9121765  -0.18439859  0.07141626  0.60028386]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 805. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.95870423 0.25739813 0.19600606 0.49622703]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 806. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9744718   0.14791942 -0.01510507  0.6626667 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 807. State = [[0.07461558 0.05129958 0.24019817 1.        ]]. Action = [[ 0.9971349   0.25926352 -0.4157294   0.358963  ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 808. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9696479  0.25359917 0.44126296 0.44237494]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 809. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.95858717  0.21243036 -0.01376021  0.36076963]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 810. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9219837  0.17944598 0.07729197 0.47164214]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 811. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9552858  0.5812539  0.3319174  0.48702574]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 812. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.95948124 0.31476903 0.3718921  0.37596726]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 813. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.81606615 0.18166733 0.5469134  0.43247712]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 814. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.8815373   0.57010376 -0.19028896  0.31182182]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 815. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.9102955  0.12266254 0.33475697 0.29973733]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 816. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.87322974  0.05057597 -0.16214085  0.3195567 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 817. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[0.94737494 0.19846916 0.01594925 0.2315185 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 818. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.95888305  0.37056625 -0.06258136  0.23028052]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 819. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9517132   0.7123499  -0.14595044  0.09448457]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 820. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.8695705   0.6780751  -0.48103464  0.18271136]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 821. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9129474   0.75352514 -0.2514736   0.04110408]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 822. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.97905946  0.6671517   0.1481061  -0.01510644]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 823. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.95896506  0.49527395 -0.319979   -0.09633046]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 824. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98433435  0.7838447  -0.4453913   0.01583767]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 825. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9563379   0.7484206  -0.46485877 -0.02965975]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 826. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.51747584  0.66463256 -0.6166903  -0.07432568]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 827. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9924432   0.78241825 -0.64959973 -0.09757173]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 828. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.8667791   0.7520473  -0.20540762  0.05587959]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 829. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.97518563  0.7760496  -0.14124131  0.10621524]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 830. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9946258   0.7417619  -0.33902538  0.01191962]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 831. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.7458694   0.74980724 -0.6547801  -0.01755047]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 832. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.97736573  0.634351   -0.10982251  0.0211705 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 833. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9940331   0.80232596 -0.18908453 -0.04417312]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 834. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.95333123  0.724432   -0.37435758 -0.02616197]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 835. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.94936347  0.6839533  -0.56526864 -0.15036875]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 836. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9464536   0.7717209  -0.43377012 -0.1902749 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 837. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9786916   0.636004   -0.41489732 -0.10885817]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 838. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.96699286  0.5821905  -0.31112832 -0.22506309]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 839. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.8328006   0.7708392  -0.2326256  -0.14598525]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 840. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9802866   0.8011515  -0.36120743 -0.1192925 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 841. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98991203  0.6871202  -0.63176525 -0.2512163 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 842. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.89607763  0.6731653  -0.30945933 -0.16619867]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 843. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.90552247  0.8159518  -0.25650775 -0.20397633]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 844. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.8896079   0.64621496 -0.22310889 -0.20463574]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 845. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9833462   0.79879355 -0.41978276 -0.14603162]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 846. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9821949   0.7762219  -0.49311507 -0.19402152]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 847. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9980849   0.713922   -0.5616076  -0.27398145]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 848. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.97744036  0.79534054 -0.46993124 -0.24514371]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 849. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99686074  0.84046865 -0.5721011  -0.2369169 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 850. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9909167   0.8797772  -0.58829737 -0.28343773]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 851. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9822111   0.8449352  -0.51522684 -0.20130318]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 852. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9896512   0.88735557 -0.6042028  -0.2511903 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 853. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9882624   0.8354771  -0.5459417  -0.23038393]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 854. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99185574  0.8316276  -0.69700813 -0.2259677 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 855. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99126184  0.7704618  -0.61484826 -0.14090681]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 856. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9932498   0.8721398  -0.567965   -0.13187182]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 857. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9900892   0.8130932  -0.54582685 -0.12458318]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 858. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98613834  0.8837683  -0.51668155 -0.18076032]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 859. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9902282   0.8625922  -0.5771348  -0.01645899]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 860. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.957281    0.6818147  -0.5473661  -0.10749245]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 861. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9861357   0.8277745  -0.59448516 -0.04569525]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 862. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98313904  0.84733117 -0.48538935 -0.1443541 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 863. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9752135   0.79971457 -0.49479878 -0.03888631]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 864. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9887283   0.86726534 -0.5173671  -0.1161527 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 865. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98856735  0.878759   -0.5488653  -0.11525154]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 866. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99416363  0.7529268  -0.6608679  -0.13519293]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 867. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.988027    0.86760235 -0.5236376  -0.162997  ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 868. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9776329   0.8849182  -0.4298302  -0.05112606]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 869. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9825778   0.8949292  -0.71856797 -0.22097623]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 870. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9886223   0.82328963 -0.6299094  -0.12314367]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 871. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.96327186  0.79795873 -0.5635556  -0.00751638]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 872. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99076915  0.844959   -0.43787366 -0.08289731]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 873. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9795537   0.872185   -0.53077334 -0.06131911]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 874. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99300075  0.82863843 -0.64231175 -0.14711511]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 875. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9956231   0.8844967  -0.61747134 -0.20727456]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 876. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9861345   0.8873019  -0.68603235 -0.15049088]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 877. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99422765  0.8131566  -0.584861   -0.16135895]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 878. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.990458    0.8580198  -0.5669804  -0.20424211]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 879. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.993536    0.8814044  -0.60907185 -0.15158498]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 880. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99057484  0.81886744 -0.52389985 -0.0110693 ]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 881. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98842597  0.8410938  -0.47914535 -0.02499545]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 882. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.98152447  0.84147954 -0.59083503 -0.03280562]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 883. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9683871   0.8289008  -0.41193932  0.01486349]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 884. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99442124  0.8912964  -0.47412705 -0.10260999]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 885. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.95901394  0.7279632  -0.517996   -0.0816052 ]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 886. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9882219   0.8636731  -0.49279428 -0.08644021]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 887. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9897604   0.83210576 -0.5395794  -0.1953612 ]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 888. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99550855  0.8852509  -0.54621154 -0.07919943]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 889. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9889761   0.8269299  -0.5273475  -0.08019799]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 890. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9850681   0.8665426  -0.43292868  0.00184286]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 891. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99399984  0.8094542  -0.46290278 -0.03296328]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 892. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99708056  0.7402139  -0.6439813  -0.02241838]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 893. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.988446    0.8753524  -0.4756316   0.02987039]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 894. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.9791986   0.8409233  -0.6227505  -0.00385529]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 895. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.993201    0.87336075 -0.7044611  -0.04854923]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 896. State = [[0.0746045  0.05127516 0.24017246 1.        ]]. Action = [[ 0.99636006  0.84911895 -0.52723104 -0.12733483]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 897. State = [[-0.25081655  0.0025469   0.23267573  1.        ]]. Action = [[ 0.99249554  0.93011355 -0.59850764 -0.01754028]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 898. State = [[-0.24164055 -0.01715166  0.24197257  1.        ]]. Action = [[ 0.94899786 -0.8242855   0.79140997  0.9426303 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 899. State = [[-0.21694492 -0.05582456  0.2691967   1.        ]]. Action = [[ 0.5136645 -0.935247   0.6027951  0.8483956]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 900. State = [[-0.18727268 -0.09279937  0.30263492  1.        ]]. Action = [[ 0.90804005 -0.5255461   0.5358827   0.80402267]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 901. State = [[-0.14913636 -0.1134173   0.3219649   1.        ]]. Action = [[ 0.98864865 -0.23336905 -0.08376026  0.69461286]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 902. State = [[-0.10267633 -0.10915142  0.31974912  1.        ]]. Action = [[ 0.96849656  0.6880977  -0.14237303  0.37969506]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 903. State = [[-0.25077704  0.00263953  0.23270895  1.        ]]. Action = [[ 0.9049959   0.50262356 -0.42177987 -0.06081641]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 904. State = [[-0.2371965  -0.01742822  0.24557304  1.        ]]. Action = [[ 0.98983836 -0.9840989   0.91278493  0.7570269 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 905. State = [[-0.2050334  -0.04568462  0.28025442  1.        ]]. Action = [[ 0.9934497  -0.29661167  0.9305508   0.88946295]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 906. State = [[-0.16436374 -0.05445271  0.31838018  1.        ]]. Action = [[0.9745743  0.16250432 0.29471838 0.77872   ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 907. State = [[-0.12018543 -0.04094977  0.33972272  1.        ]]. Action = [[0.96788716 0.68569136 0.31274998 0.5391016 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 908. State = [[-0.07625692 -0.02220888  0.34794018  1.        ]]. Action = [[ 0.94743896  0.21809638 -0.14540923  0.48193455]]. Reward = [0.]
Curr episode timestep = 4
Above hoop
Current timestep = 909. State = [[-3.4741707e-02  6.9401425e-04  3.3956453e-01  1.0000000e+00]]. Action = [[ 0.85792613  0.7391784  -0.491794    0.00517917]]. Reward = [0.]
Curr episode timestep = 5
Above hoop
Scene graph at timestep 909 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 909 is tensor(0.0225, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 909 of 1
Current timestep = 910. State = [[-0.25055832  0.00244624  0.23269688  1.        ]]. Action = [[ 0.9945829   0.8370433  -0.4130203  -0.09797251]]. Reward = [1000.]
Curr episode timestep = 6
Above hoop
Current timestep = 911. State = [[-0.24022813 -0.01743251  0.24300155  1.        ]]. Action = [[ 0.89700127 -0.93141     0.7840395   0.9504876 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 912. State = [[-0.21000117 -0.05692314  0.27438566  1.        ]]. Action = [[ 0.9633858  -0.97743     0.90410376  0.9005234 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 913. State = [[-0.17060353 -0.08377324  0.31704676  1.        ]]. Action = [[ 0.9264238  -0.0338732   0.56076133  0.861989  ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 914. State = [[-0.12761179 -0.08103384  0.34203184  1.        ]]. Action = [[0.9851227  0.51509404 0.15784156 0.7017369 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 915. State = [[-0.08140184 -0.06168321  0.34760255  1.        ]]. Action = [[ 0.9680352   0.60176325 -0.12024313  0.23299778]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 916. State = [[-0.03949481 -0.03167795  0.34151265  1.        ]]. Action = [[ 0.964219    0.8078873  -0.44016552  0.10836267]]. Reward = [0.]
Curr episode timestep = 5
Above hoop
Scene graph at timestep 916 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 916 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 916 of 1
Current timestep = 917. State = [[-0.25056827  0.00272455  0.23276077  1.        ]]. Action = [[ 0.976702   0.9096005 -0.6866091 -0.0831393]]. Reward = [1000.]
Curr episode timestep = 6
Above hoop
Current timestep = 918. State = [[-0.23914003 -0.01187116  0.24435882  1.        ]]. Action = [[ 0.9541311  -0.67039406  0.87449265  0.9590099 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 919. State = [[-0.2050237  -0.04600425  0.27593383  1.        ]]. Action = [[ 0.9919133  -0.97400576  0.6870601   0.6313715 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 920. State = [[-0.16629168 -0.07154285  0.31387022  1.        ]]. Action = [[ 0.97362924 -0.09286004  0.5656847   0.8276589 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 921. State = [[-0.12122249 -0.07821531  0.34386006  1.        ]]. Action = [[0.9755013  0.03049231 0.5325575  0.52236986]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 922. State = [[-0.07695618 -0.06714111  0.35598767  1.        ]]. Action = [[ 0.98316526  0.65620875 -0.35513234  0.2758683 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 923. State = [[-0.03098347 -0.03799663  0.3421772   1.        ]]. Action = [[ 0.9896743   0.85696626 -0.41878748  0.03782606]]. Reward = [0.]
Curr episode timestep = 5
Above hoop
Scene graph at timestep 923 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 923 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 923 of 1
Current timestep = 924. State = [[-0.25066975  0.00264483  0.23273061  1.        ]]. Action = [[ 0.9692683  0.9482205 -0.6743261 -0.2040624]]. Reward = [1000.]
Curr episode timestep = 6
Above hoop
Current timestep = 925. State = [[-0.24049245 -0.00629371  0.24459645  1.        ]]. Action = [[ 0.86000586 -0.37912834  0.89486563  0.88285613]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 926. State = [[-0.21177268 -0.01913667  0.27489454  1.        ]]. Action = [[ 0.8381169  -0.24029768  0.7119206   0.9726281 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 927. State = [[-0.17202479 -0.04294405  0.3123743   1.        ]]. Action = [[ 0.98850024 -0.857803    0.5251105   0.7604244 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 928. State = [[-0.13041632 -0.04851673  0.3419903   1.        ]]. Action = [[0.9849136 0.7693908 0.4445727 0.7243755]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 929. State = [[-0.08741235 -0.02470786  0.35048285  1.        ]]. Action = [[ 0.9710028   0.6983298  -0.45618647  0.36593366]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 930. State = [[-0.04458122  0.00609215  0.33287466  1.        ]]. Action = [[ 0.99388146  0.6962187  -0.5550986   0.04700339]]. Reward = [0.]
Curr episode timestep = 5
Above hoop
Current timestep = 931. State = [[-0.00110489  0.04072087  0.30866733  1.        ]]. Action = [[ 0.98152757  0.82839966 -0.508847    0.10683763]]. Reward = [0.]
Curr episode timestep = 6
Above hoop
Current timestep = 932. State = [[-0.25085652  0.00272514  0.23265865  1.        ]]. Action = [[ 0.91443825  0.89705205 -0.5819778  -0.06365705]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 933. State = [[-0.23973326 -0.00887511  0.23861697  1.        ]]. Action = [[ 0.9715171  -0.47522235  0.5221745   0.9147718 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 934. State = [[-0.2206625  -0.03421207  0.26111308  1.        ]]. Action = [[ 0.08367813 -0.7777218   0.93270206  0.88100195]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 935. State = [[-0.1961254  -0.06088625  0.29984215  1.        ]]. Action = [[ 0.9921012  -0.23652327  0.68512464  0.7622386 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 936. State = [[-0.16254897 -0.08649756  0.3374056   1.        ]]. Action = [[ 0.64919305 -0.8707676   0.4935956   0.29511726]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 937. State = [[-0.12899664 -0.09691435  0.3572879   1.        ]]. Action = [[0.798017  0.6823518 0.0889287 0.3879993]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 938. State = [[-0.09148556 -0.07043362  0.35532013  1.        ]]. Action = [[ 0.96556187  0.9010917  -0.57308364  0.14076436]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 939. State = [[-0.25078335  0.00270992  0.23256283  1.        ]]. Action = [[ 0.9948063   0.95633614 -0.47008663 -0.14739591]]. Reward = [1000.]
Curr episode timestep = 6
Above hoop
Current timestep = 940. State = [[-0.24087179 -0.00223914  0.24453494  1.        ]]. Action = [[ 0.774542   -0.24512553  0.9105437   0.8408965 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 941. State = [[-0.21060234 -0.00923437  0.27512786  1.        ]]. Action = [[ 0.96955657 -0.10223705  0.6206515   0.88092244]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 942. State = [[-1.7300810e-01 -1.9286288e-04  3.0992430e-01  1.0000000e+00]]. Action = [[0.8563565 0.6769879 0.6077539 0.9121704]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 943. State = [[-0.1330144   0.01730141  0.33609805  1.        ]]. Action = [[0.96200013 0.32136035 0.19589877 0.4906342 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 944. State = [[-0.08989275  0.04025374  0.34364146  1.        ]]. Action = [[ 0.8958888   0.65110373 -0.09070837  0.56266165]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 945. State = [[-0.05230175  0.07014723  0.3365139   1.        ]]. Action = [[ 0.98729277  0.66696715 -0.62821203  0.32562995]]. Reward = [0.]
Curr episode timestep = 5
Above hoop
Current timestep = 946. State = [[-0.01202048  0.10296015  0.31176373  1.        ]]. Action = [[ 0.995908    0.85312676 -0.46705687  0.24040377]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 947. State = [[0.03126    0.14394003 0.28796256 1.        ]]. Action = [[ 0.9844793   0.97051096 -0.24715531  0.22534859]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 947 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 947 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9483886   0.9531505  -0.3041985   0.07100129]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 948 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 948 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of -1
Current timestep = 949. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99098504  0.97733545 -0.5098412  -0.2070756 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 949 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 949 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 949 of -1
Current timestep = 950. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98828244  0.96348417 -0.5412751  -0.20729828]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 950 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 950 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of -1
Current timestep = 951. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99760914  0.972479   -0.6098026  -0.23651272]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 951 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 951 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98340034  0.9703845  -0.5235174  -0.13333923]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 952 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 952 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 952 of -1
Current timestep = 953. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9769584   0.95844674 -0.65487254 -0.22677767]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 953 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 953 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 953 of -1
Current timestep = 954. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9978845   0.9858222  -0.72089565 -0.06572294]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 954 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 954 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of -1
Current timestep = 955. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9648018   0.9478471  -0.656077   -0.08315551]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 955 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 955 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 955 of -1
Current timestep = 956. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99783266  0.9804044  -0.7219559  -0.10329008]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 956 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 956 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.998039    0.96083367 -0.6627356  -0.2162466 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 957 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 957 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of -1
Current timestep = 958. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9706397   0.9623399  -0.54568547 -0.13838458]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 958 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 958 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 958 of -1
Current timestep = 959. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.987509    0.9251609  -0.6616409  -0.17967474]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 959 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 959 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98940825  0.9772924  -0.6477145  -0.14688975]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 961. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9878156   0.99155474 -0.6949434  -0.20772219]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 962. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.96046495  0.99207914 -0.5706574  -0.05559665]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 963. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.961375    0.97526145 -0.6905958  -0.14418006]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 964. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9410536   0.9853941  -0.56366134 -0.20585644]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 965. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99093175  0.97794807 -0.37177658 -0.11545753]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 966. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98936594  0.9362817  -0.6866107  -0.04657584]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 967. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9949659   0.92674756 -0.5997616  -0.12478036]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 968. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9168656   0.9752736  -0.5944765  -0.06465816]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 969. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.73367     0.98002565 -0.5009821   0.00296164]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 970. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9968009   0.91815054 -0.65312594 -0.02310807]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 971. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9829118   0.91756344 -0.41280675 -0.03510189]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 972. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99417233  0.9440924  -0.6692032  -0.15320486]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 973. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9897895   0.97939706 -0.5664682  -0.13949764]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 974. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.97871184  0.97702    -0.54774195 -0.24905568]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 975. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9950746   0.95778596 -0.5879317  -0.16826749]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 976. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9704857   0.9554508  -0.46309853 -0.21072805]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 977. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.97803223  0.96511483 -0.60797846 -0.15716612]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 978. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.961313    0.9406359  -0.5647867  -0.14383894]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 979. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9902154   0.9714675  -0.48924375 -0.11347651]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 980. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98283887  0.9694617  -0.6316189  -0.1018042 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 981. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9897146   0.92828727 -0.66288155 -0.1307832 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 982. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98866034  0.94515324 -0.58161247 -0.17348576]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 983. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.95806336  0.91838574 -0.5516008  -0.24741632]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 984. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.97711515  0.95966196 -0.65842587 -0.19260097]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 985. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99520755  0.9679618  -0.5186721  -0.18173707]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 986. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9802027   0.980845   -0.6640775  -0.17325675]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 987. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.96139884  0.8839712  -0.48845232 -0.09384936]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 988. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98988605  0.96377575 -0.4446993  -0.09940207]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 989. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.95983887  0.9118717  -0.3750832  -0.12059939]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 990. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9898944   0.860219   -0.4297005  -0.06616735]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 991. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.96096945  0.9055666  -0.35143328 -0.11766416]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 992. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9757087   0.9612229  -0.53010625 -0.09830028]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 993. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9944155   0.93648195 -0.5220015  -0.13952398]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 994. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9947107   0.92711425 -0.644228   -0.16041124]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 995. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9814501   0.97755957 -0.51725817 -0.10967869]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 996. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9962511   0.83699155 -0.51413065 -0.08759522]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 997. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9964783   0.93282485 -0.23808748 -0.15173131]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 998. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9931843   0.92682934 -0.51545614 -0.14959443]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 999. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.96798825  0.96009064 -0.60515285 -0.07624614]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 1000. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99698925  0.9287257  -0.4870199  -0.15656936]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 1001. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9942012   0.9654031  -0.6169063  -0.10443813]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 1002. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9984294   0.9560958  -0.6104797  -0.05964935]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 1003. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99744844  0.9499557  -0.6344193  -0.21386659]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 1004. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.96939385  0.96235013 -0.511347   -0.03962225]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 1005. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.99063325  0.8916843  -0.37125653 -0.06406707]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 1006. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.94537807  0.93913984 -0.29062712 -0.06128633]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 1007. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9866595   0.9551158  -0.6773901  -0.03806853]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 1008. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9820976   0.86742735 -0.45409667 -0.08030403]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 1009. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.8850285   0.9190701  -0.29884297 -0.1145699 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 1010. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9937401  0.9511367 -0.2618103 -0.1625911]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 1011. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.95901823  0.9553964  -0.3738798  -0.13780415]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 1012. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9021783   0.92350435 -0.2795893  -0.11089981]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 1013. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9771701   0.9367275  -0.46438617 -0.18057156]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 1014. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9371462   0.9675479  -0.36394906 -0.1243394 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 1015. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.96274924  0.8400278  -0.4543308  -0.17540652]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 1016. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.98592067  0.97226405 -0.43160206 -0.15937775]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 1017. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.9751904   0.8764336  -0.4518717  -0.10801893]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 1018. State = [[0.09650575 0.1827997  0.28241256 1.        ]]. Action = [[ 0.97888637  0.96211696 -0.7115148  -0.13633746]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 1019. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9821168   0.9463663  -0.50908077 -0.07655495]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 1020. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.97606575  0.8928852  -0.55429316 -0.0094226 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 1021. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98342013  0.9556401  -0.5114451  -0.04888558]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 1022. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9777098   0.96915174 -0.45570463 -0.11920959]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 1023. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98896194  0.94412947 -0.4272648  -0.13825929]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 1024. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99141395  0.9317689  -0.39199793 -0.07974452]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 1025. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98516846  0.9566364  -0.50549275 -0.17381155]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 1026. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.96115303  0.96385694 -0.4976207  -0.19559306]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 1027. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99395585  0.92867863 -0.2119788  -0.13930029]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 1028. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9348078  0.9245094 -0.2861166 -0.1204766]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 1029. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9538629   0.9804714  -0.35922062 -0.06619453]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 1030. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98392344  0.9879384  -0.3758396  -0.05857283]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 1031. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9899633   0.9735279  -0.23912573 -0.24911225]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 1032. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99102044  0.9031613  -0.45994866 -0.05932623]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 1033. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98480237  0.925637   -0.5048684  -0.12342161]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 1034. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9892148   0.8906231  -0.45484656  0.00187492]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 1035. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99383354  0.9621942  -0.5834084  -0.07997155]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 1036. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.990209    0.94659615 -0.5538127  -0.09686482]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 1037. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9930408   0.86821604 -0.24613577  0.0345515 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 1038. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.92320657  0.9501909  -0.30699408 -0.12344462]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 1039. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9902425   0.9506999  -0.46247303 -0.13593501]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 1040. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.8145524   0.95871365 -0.25043726  0.03842664]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 1041. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.8725176   0.8065975  -0.4117092  -0.14787263]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 1042. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.94567704  0.9744942  -0.2628985  -0.15186512]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: Workspace boundary
Current timestep = 1043. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.95930994  0.9681413  -0.45488107 -0.16188729]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Current timestep = 1044. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9947573   0.9549583  -0.42535365 -0.15459675]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Current timestep = 1045. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98676765  0.9700427  -0.6271523  -0.19260812]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: Workspace boundary
Current timestep = 1046. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9912393   0.97805    -0.53946036 -0.14656961]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Current timestep = 1047. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 9.8458529e-01  9.8238349e-01 -5.8367479e-01  9.1922283e-04]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Current timestep = 1048. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99088407  0.9743998  -0.471915   -0.0722568 ]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: Workspace boundary
Current timestep = 1049. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9903393   0.9613538  -0.41289186 -0.10727084]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: Workspace boundary
Current timestep = 1050. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9825343   0.98583364 -0.58036125 -0.09481031]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: Workspace boundary
Current timestep = 1051. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98490894  0.90798163 -0.6252779  -0.16659254]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: Workspace boundary
Current timestep = 1052. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99254847  0.9293847  -0.6251539  -0.13965303]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: Workspace boundary
Current timestep = 1053. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9838252   0.95713544 -0.4551922  -0.16346824]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Current timestep = 1054. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98914456  0.97820234 -0.46052843 -0.14806682]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Current timestep = 1055. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9166796   0.96775675 -0.26735997 -0.09505951]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Current timestep = 1056. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9646311   0.8377075  -0.35788798 -0.0743258 ]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: Workspace boundary
Current timestep = 1057. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.94362783  0.9634149  -0.35603666 -0.07115656]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Current timestep = 1058. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99058986  0.95566344 -0.37397146 -0.15738034]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: Workspace boundary
Current timestep = 1059. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.99050546  0.9617903  -0.37912953 -0.09332168]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: Workspace boundary
Current timestep = 1060. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98317266  0.9670265  -0.35766882 -0.11158931]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Current timestep = 1061. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9583931   0.96280444 -0.42572564 -0.18187249]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: Workspace boundary
Current timestep = 1062. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.971606    0.97229195 -0.5294071  -0.13818336]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Current timestep = 1063. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9953182   0.98542905 -0.45511448 -0.04498595]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Current timestep = 1064. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.98911095  0.95897555 -0.33255363 -0.06181884]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: Workspace boundary
Current timestep = 1065. State = [[0.09650069 0.18279873 0.2823333  1.        ]]. Action = [[ 0.9663582   0.96189237 -0.4850844  -0.12942493]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Current timestep = 1066. State = [[-0.2510285   0.00250487  0.23258641  1.        ]]. Action = [[ 0.997123    0.98106146 -0.48831856 -0.00858837]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: Workspace boundary
Current timestep = 1067. State = [[-0.22938836 -0.04273629  0.2252455   1.        ]]. Action = [[ 0.9699743 -0.9554616  0.7804878  0.8630167]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1068. State = [[-0.1889928  -0.08630574  0.2488328   1.        ]]. Action = [[ 0.9194088  -0.66756135  0.88604593  0.8646959 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1069. State = [[-0.14581603 -0.11971785  0.27853444  1.        ]]. Action = [[ 0.99200463 -0.64788884  0.2045232   0.76015913]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1070. State = [[-0.09873504 -0.12296024  0.28321186  1.        ]]. Action = [[ 0.9874983   0.8496783  -0.22557753  0.2467339 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1071. State = [[-0.05531654 -0.09205052  0.27082077  1.        ]]. Action = [[ 0.9492092   0.960268   -0.6362774   0.02978003]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1072. State = [[-0.02204711 -0.06885213  0.25160894  1.        ]]. Action = [[ 0.9691758   0.99240756 -0.7223366  -0.14299262]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 1073. State = [[-0.0149635  -0.06429095  0.25132182  1.        ]]. Action = [[ 0.96608794  0.97350144 -0.5918736  -0.13029444]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Above hoop
Current timestep = 1074. State = [[-0.01493615 -0.06366549  0.2507175   1.        ]]. Action = [[ 0.9661403   0.9831008  -0.6153621  -0.28717047]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Above hoop
Current timestep = 1075. State = [[-0.01505651 -0.06345197  0.2498445   1.        ]]. Action = [[ 0.92440104  0.9861238  -0.740409   -0.18011248]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Above hoop
Current timestep = 1076. State = [[-0.01516697 -0.06323685  0.24909501  1.        ]]. Action = [[ 0.99636436  0.9862716  -0.53665775 -0.06311095]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Above hoop
Current timestep = 1077. State = [[-0.01520119 -0.06323566  0.24884741  1.        ]]. Action = [[ 0.99210894  0.9408164  -0.5465866  -0.12047684]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Above hoop
Current timestep = 1078. State = [[-0.01499822 -0.06322732  0.24881028  1.        ]]. Action = [[ 0.9918419   0.9176868  -0.72137314 -0.14391243]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Above hoop
Current timestep = 1079. State = [[-0.01472209 -0.0631872   0.2489464   1.        ]]. Action = [[ 0.89955664  0.9644679  -0.62837887  0.03625715]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Above hoop
Current timestep = 1080. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.9569142   0.9913602  -0.44405365 -0.1250847 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Above hoop
Current timestep = 1081. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.8314674   0.9570999  -0.68157446 -0.22662115]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Above hoop
Current timestep = 1082. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.93880475  0.99051666 -0.693174   -0.12224036]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Above hoop
Current timestep = 1083. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.9311235   0.9816966  -0.5942273  -0.20983887]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Above hoop
Current timestep = 1084. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.95323753  0.97327816 -0.7737052  -0.17827117]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Above hoop
Current timestep = 1085. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.98208356  0.9813632  -0.6055128  -0.07815886]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Above hoop
Current timestep = 1086. State = [[-0.01466805 -0.06318956  0.24899349  1.        ]]. Action = [[ 0.91961193  0.87516475 -0.6653842  -0.1780318 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Above hoop
Current timestep = 1087. State = [[-0.01470232 -0.06318836  0.24874568  1.        ]]. Action = [[ 0.92848635  0.962029   -0.6384464  -0.15696847]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Above hoop
Current timestep = 1088. State = [[-0.01471941 -0.06318776  0.24862236  1.        ]]. Action = [[ 0.99124265  0.9757359  -0.6595297  -0.11982077]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Above hoop
Current timestep = 1089. State = [[-0.01474517 -0.06318686  0.24843681  1.        ]]. Action = [[ 0.96990514  0.9880537  -0.7329368  -0.13581294]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Above hoop
Current timestep = 1090. State = [[-0.01476241 -0.06318627  0.24831292  1.        ]]. Action = [[ 0.94641006  0.9416784  -0.74245006 -0.08777618]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Above hoop
Current timestep = 1091. State = [[-0.01476241 -0.06318627  0.24831292  1.        ]]. Action = [[ 0.9843823  0.9706712 -0.6226561 -0.2418142]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Above hoop
Current timestep = 1092. State = [[-0.01476241 -0.06318627  0.24831292  1.        ]]. Action = [[ 0.9872844   0.92349815 -0.50458    -0.1880886 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Above hoop
Current timestep = 1093. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.92790484  0.9738488  -0.6436572  -0.14052683]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Above hoop
Current timestep = 1094. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.97642493  0.8109096  -0.6239985  -0.1393007 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Above hoop
Current timestep = 1095. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.86609364  0.95172465 -0.4122088  -0.14636171]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Above hoop
Current timestep = 1096. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.9503238   0.97815263 -0.6381324  -0.1193918 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Above hoop
Current timestep = 1097. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.8844123   0.9516754  -0.6293339  -0.00563073]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Above hoop
Current timestep = 1098. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.9828203   0.9831312  -0.52593815 -0.1640706 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Above hoop
Current timestep = 1099. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.9499061  0.9572661 -0.6573168 -0.0911997]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Above hoop
Current timestep = 1100. State = [[-0.01477108 -0.06318596  0.2482507   1.        ]]. Action = [[ 0.9832512  0.9466679 -0.6323221 -0.2178238]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Above hoop
Current timestep = 1101. State = [[-0.01478827 -0.06318536  0.24812739  1.        ]]. Action = [[ 0.93957114  0.9508941  -0.4161513  -0.16940027]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Above hoop
Current timestep = 1102. State = [[-0.01478827 -0.06318536  0.24812739  1.        ]]. Action = [[ 0.9758663   0.97380424 -0.643961   -0.16577822]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Above hoop
Current timestep = 1103. State = [[-0.01478827 -0.06318536  0.24812739  1.        ]]. Action = [[ 0.96089876  0.9567226  -0.5944512  -0.09085083]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Above hoop
Current timestep = 1104. State = [[-0.01478827 -0.06318536  0.24812739  1.        ]]. Action = [[ 0.9718542   0.98262    -0.69197506 -0.18432248]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Above hoop
Current timestep = 1105. State = [[-0.01478827 -0.06318536  0.24812739  1.        ]]. Action = [[ 0.9798348   0.9415026  -0.5897367  -0.16356575]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Above hoop
Current timestep = 1106. State = [[-0.01478827 -0.06318536  0.24812739  1.        ]]. Action = [[ 0.9887903   0.9823923  -0.6697147  -0.19806576]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Above hoop
Current timestep = 1107. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99568725  0.9558208  -0.65396166 -0.12224036]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Above hoop
Current timestep = 1108. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.97946155  0.988654   -0.5327583  -0.18761599]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Above hoop
Current timestep = 1109. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.97801375  0.96374726 -0.5104912  -0.17242408]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Above hoop
Current timestep = 1110. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9908521   0.9256954  -0.6822753  -0.09324211]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Above hoop
Current timestep = 1111. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9695828   0.9576993  -0.6721283  -0.10825425]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Above hoop
Current timestep = 1112. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9765489   0.9797597  -0.66505027 -0.1259141 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Above hoop
Current timestep = 1113. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9883754   0.9756012  -0.76501924 -0.14990169]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Above hoop
Current timestep = 1114. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.8977783   0.8970084  -0.7542968  -0.22359955]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Above hoop
Current timestep = 1115. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98060846  0.9840311  -0.5415578  -0.15103775]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Above hoop
Current timestep = 1116. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9615083   0.8673558  -0.59197205 -0.10733938]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Above hoop
Current timestep = 1117. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9738327   0.8416902  -0.55858594 -0.16186053]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Above hoop
Current timestep = 1118. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9776132   0.9748906  -0.7229516  -0.09117681]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Above hoop
Current timestep = 1119. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.97640026  0.9788617  -0.60352004 -0.06431949]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Above hoop
Current timestep = 1120. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98951125  0.9490557  -0.7400234  -0.27667975]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Above hoop
Current timestep = 1121. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98410773  0.9750161  -0.65512824 -0.04399717]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Above hoop
Current timestep = 1122. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9726839   0.9836743  -0.6844261  -0.14461744]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Above hoop
Current timestep = 1123. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.92830205  0.8728621  -0.668029   -0.20856488]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Above hoop
Current timestep = 1124. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.978765    0.99665034 -0.6178079  -0.18694937]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Above hoop
Current timestep = 1125. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.970279    0.9929173  -0.6046686  -0.11449635]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Above hoop
Current timestep = 1126. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9849212   0.956975   -0.6924273  -0.13975155]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Above hoop
Current timestep = 1127. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98424697  0.99108875 -0.64641774 -0.19665104]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Above hoop
Current timestep = 1128. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98669696  0.9595909  -0.7098812  -0.13739371]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Above hoop
Current timestep = 1129. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98598146  0.9862155  -0.63512653 -0.15896475]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Above hoop
Current timestep = 1130. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9708445  0.9896319 -0.6036553 -0.1422171]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Above hoop
Current timestep = 1131. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9815891   0.9434285  -0.6979267  -0.12170029]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Above hoop
Current timestep = 1132. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9849019   0.9798199  -0.72966516 -0.14310306]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Above hoop
Current timestep = 1133. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9391248   0.9782758  -0.6687973  -0.19962841]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Above hoop
Current timestep = 1134. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9855299   0.97322035 -0.68272465 -0.04631013]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Above hoop
Current timestep = 1135. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99267817  0.95367134 -0.7114051  -0.04181123]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Above hoop
Current timestep = 1136. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9910879   0.8945296  -0.5817153  -0.22131681]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Above hoop
Current timestep = 1137. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9890362   0.934788   -0.6160431  -0.10652822]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Above hoop
Current timestep = 1138. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9628074   0.96855783 -0.683327   -0.16458142]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Above hoop
Current timestep = 1139. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.93028855  0.99602795 -0.7199203  -0.13927644]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Above hoop
Current timestep = 1140. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9404032   0.92494845 -0.5923945  -0.19963431]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Above hoop
Current timestep = 1141. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9892726   0.9797014  -0.7302519  -0.18682921]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Above hoop
Current timestep = 1142. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9856713   0.9437661  -0.63759685 -0.27428472]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Above hoop
Current timestep = 1143. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9761723   0.7647462  -0.64748466 -0.03064489]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Above hoop
Current timestep = 1144. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9910879   0.9662831  -0.65196264 -0.16721004]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: No entry zone
Above hoop
Current timestep = 1145. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9915278   0.9327209  -0.64907104 -0.20538771]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Above hoop
Current timestep = 1146. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9930928   0.95397556 -0.60927355 -0.12898904]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Above hoop
Current timestep = 1147. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98226666  0.9684174  -0.6156743  -0.17339325]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Above hoop
Current timestep = 1148. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98374844  0.92725706 -0.5542242  -0.2566024 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Above hoop
Current timestep = 1149. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.967674    0.9494884  -0.6867536  -0.08721209]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Above hoop
Current timestep = 1150. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9906094   0.97981024 -0.6284349  -0.17349851]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Above hoop
Current timestep = 1151. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9565716   0.9629917  -0.58242095 -0.15759021]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Above hoop
Current timestep = 1152. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.92578435  0.8409059  -0.5253954  -0.17969131]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Above hoop
Current timestep = 1153. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9802408   0.97886646 -0.6988221  -0.21229303]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Above hoop
Current timestep = 1154. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.97475696  0.98893857 -0.67644906 -0.00977987]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Above hoop
Current timestep = 1155. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9930129   0.95730424 -0.53821385 -0.13542128]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Above hoop
Current timestep = 1156. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9606657   0.93871784 -0.59449756 -0.07600528]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Above hoop
Current timestep = 1157. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9855294   0.99400306 -0.6703461  -0.06088483]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Above hoop
Current timestep = 1158. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9516523   0.9916637  -0.7223396  -0.18656313]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Above hoop
Current timestep = 1159. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9627043   0.9813335  -0.66336864 -0.03378439]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Above hoop
Current timestep = 1160. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99142885  0.9724351  -0.6151721  -0.19705474]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Above hoop
Current timestep = 1161. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9939953   0.9494021  -0.7077877  -0.10699522]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Above hoop
Current timestep = 1162. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9602339   0.9285389  -0.54269576 -0.28788233]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Above hoop
Current timestep = 1163. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9870627   0.91551125 -0.6126296  -0.0823608 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Above hoop
Current timestep = 1164. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99583435  0.9841131  -0.5516274  -0.13757181]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Above hoop
Current timestep = 1165. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.885587    0.75410247 -0.6720443  -0.21057045]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Above hoop
Current timestep = 1166. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9567044   0.9568744  -0.61760384 -0.18047178]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Above hoop
Current timestep = 1167. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.96228385  0.9766023  -0.5600464  -0.20078582]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Above hoop
Current timestep = 1168. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.83757687  0.9587492  -0.42692882 -0.07310432]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Above hoop
Current timestep = 1169. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9755504   0.94131374 -0.5059446  -0.1533392 ]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: No entry zone
Above hoop
Current timestep = 1170. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9823737   0.95838284 -0.67884225 -0.16997719]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: No entry zone
Above hoop
Current timestep = 1171. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9216553   0.96144414 -0.546061   -0.16168177]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: No entry zone
Above hoop
Current timestep = 1172. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98190975  0.97414756 -0.5396001  -0.07190931]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: No entry zone
Above hoop
Current timestep = 1173. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.87728524  0.8937502  -0.63125587 -0.11838448]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: No entry zone
Above hoop
Current timestep = 1174. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.92780364  0.93895745 -0.56150997 -0.20449764]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: No entry zone
Above hoop
Current timestep = 1175. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.97274864  0.93189216 -0.53724116 -0.14367032]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: No entry zone
Above hoop
Current timestep = 1176. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9786346   0.97422063 -0.6740397  -0.23839545]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: No entry zone
Above hoop
Current timestep = 1177. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99088764  0.93239117 -0.55810016 -0.10649395]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: No entry zone
Above hoop
Current timestep = 1178. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9937141   0.9655845  -0.6868027  -0.09157681]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: No entry zone
Above hoop
Current timestep = 1179. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.97135925  0.97742295 -0.62505573 -0.15813076]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: No entry zone
Above hoop
Current timestep = 1180. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9930339   0.997126   -0.5049691  -0.13998824]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: No entry zone
Above hoop
Current timestep = 1181. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99178386  0.98573554 -0.4033029  -0.24540532]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: No entry zone
Above hoop
Current timestep = 1182. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.978588    0.97429216 -0.5336574  -0.12805998]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: No entry zone
Above hoop
Current timestep = 1183. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9767004   0.95991564 -0.6112628  -0.19376016]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: No entry zone
Above hoop
Current timestep = 1184. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9627999   0.934911   -0.5851876  -0.06021625]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: No entry zone
Above hoop
Current timestep = 1185. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.99630713  0.9443966  -0.45873463 -0.2594821 ]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: No entry zone
Above hoop
Current timestep = 1186. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.94652367  0.94618785 -0.61357754 -0.1622935 ]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: No entry zone
Above hoop
Current timestep = 1187. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9625337   0.8870623  -0.570473   -0.25169432]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: No entry zone
Above hoop
Current timestep = 1188. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.98005307  0.96618736 -0.6308433  -0.20552647]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: No entry zone
Above hoop
Current timestep = 1189. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.95724154  0.97378063 -0.5087672  -0.03451645]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: No entry zone
Above hoop
Current timestep = 1190. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9639776   0.97722363 -0.677352   -0.07734364]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: No entry zone
Above hoop
Current timestep = 1191. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.9782765   0.94996834 -0.6127588  -0.14757693]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: No entry zone
Above hoop
Current timestep = 1192. State = [[-0.01479695 -0.06318506  0.24806516  1.        ]]. Action = [[ 0.95095897  0.95500994 -0.6051809  -0.21818972]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: No entry zone
Above hoop
Current timestep = 1193. State = [[-0.2508699   0.00263071  0.23267779  1.        ]]. Action = [[ 0.9749372   0.96840143 -0.50631845 -0.06347066]]. Reward = [0.]
Curr episode timestep = 126
Action ignored: No entry zone
Above hoop
Current timestep = 1194. State = [[-0.23939513 -0.01620567  0.24372911  1.        ]]. Action = [[ 0.99682164 -0.9186078   0.86837816  0.9443735 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1195. State = [[-0.2065596  -0.05532034  0.27628636  1.        ]]. Action = [[ 0.9734609  -0.91752565  0.75466096  0.93194306]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1196. State = [[-0.16923052 -0.07163941  0.31187353  1.        ]]. Action = [[0.9325671  0.43988633 0.37358403 0.7133033 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1197. State = [[-0.12707542 -0.05321121  0.322663    1.        ]]. Action = [[ 0.9863584   0.8218682  -0.3208881   0.43428147]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1198. State = [[-0.25069645  0.00261432  0.23277895  1.        ]]. Action = [[ 0.9909005   0.9591191  -0.47961283 -0.04687554]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1199. State = [[-0.24019751 -0.01490092  0.2444324   1.        ]]. Action = [[ 0.9160603  -0.8705619   0.9103179   0.93251634]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1200. State = [[-0.20797013 -0.05276835  0.27904814  1.        ]]. Action = [[ 0.97363746 -0.8909722   0.8381703   0.8862655 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1201. State = [[-0.16841789 -0.07250224  0.31173044  1.        ]]. Action = [[0.99262    0.20264673 0.05020738 0.6618937 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1202. State = [[-0.12277098 -0.05581489  0.31851715  1.        ]]. Action = [[ 0.989846    0.95798826 -0.00154245  0.2956047 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1203. State = [[-0.07893179 -0.01758608  0.31665105  1.        ]]. Action = [[ 0.9800763   0.905313   -0.35471946  0.07931733]]. Reward = [0.]
Curr episode timestep = 4
Above hoop
Current timestep = 1204. State = [[-0.03683077  0.02296541  0.30033976  1.        ]]. Action = [[ 0.92368317  0.804883   -0.5019719   0.02917182]]. Reward = [0.]
Curr episode timestep = 5
Above hoop
Current timestep = 1205. State = [[0.00298319 0.06342298 0.27738935 1.        ]]. Action = [[ 0.9692559   0.8957292  -0.5319181   0.01165187]]. Reward = [0.]
Curr episode timestep = 6
Above hoop
Current timestep = 1206. State = [[0.04567826 0.104389   0.2537415  1.        ]]. Action = [[ 0.9564276   0.86833644 -0.12682217  0.14803421]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1207. State = [[0.08410402 0.13187844 0.24323204 1.        ]]. Action = [[ 0.9945892   0.8434925  -0.1103285   0.20720744]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 1208. State = [[0.10071494 0.13852386 0.24398719 1.        ]]. Action = [[0.9589306  0.72480416 0.16818476 0.1470877 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 1209. State = [[0.10865297 0.13986646 0.24600084 1.        ]]. Action = [[ 0.95045424  0.86877704 -0.1943323   0.17857134]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1210. State = [[0.11231752 0.14086439 0.24755733 1.        ]]. Action = [[ 0.99065995  0.9079726  -0.18751246  0.09909654]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 1211. State = [[0.11293398 0.13945393 0.249229   1.        ]]. Action = [[ 0.98886406  0.8975136  -0.4178748  -0.04717958]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 1212. State = [[0.11228858 0.138794   0.24908294 1.        ]]. Action = [[ 0.91743517  0.9094484  -0.36586988 -0.02600795]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 1213. State = [[0.11179709 0.13919452 0.24734631 1.        ]]. Action = [[ 0.97029173  0.8951957  -0.04841715  0.09029484]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 1214. State = [[0.11163591 0.13925457 0.24639204 1.        ]]. Action = [[ 0.968472   0.8551953 -0.3640107 -0.014332 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1215. State = [[0.11143401 0.13946748 0.24602175 1.        ]]. Action = [[ 0.9959364   0.7919651  -0.28203744  0.0242157 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1216. State = [[0.11136286 0.13947889 0.24577704 1.        ]]. Action = [[ 0.9835901   0.7763537  -0.12961566  0.03995359]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 1217. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.9910388   0.8727796  -0.27742207  0.04840314]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1218. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.9754039   0.92278886 -0.47482026  0.0260936 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1219. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.9904516   0.6933234  -0.35688156  0.09092951]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 1220. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.9808737   0.81881595 -0.25444996  0.09830427]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 1221. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.94451785  0.91440105 -0.295942    0.14940858]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1222. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.9762     0.8330436 -0.3682798  0.0496515]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1223. State = [[0.1112746  0.13975719 0.24580361 1.        ]]. Action = [[ 0.96095324  0.8068392  -0.32738227  0.09485424]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 1224. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9496927   0.8539994  -0.25360334  0.02669144]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1225. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9130423   0.96198916 -0.32089877  0.08142388]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1226. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9798372   0.923553   -0.40009832  0.07743025]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1227. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9787046   0.8208921  -0.09581727 -0.07599562]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 1228. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9304826   0.90948653 -0.40676248  0.13679814]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 1229. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.97808313  0.77977836 -0.19457912 -0.08027589]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 1230. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.976763    0.7786459  -0.17885494  0.06610811]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 1231. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.99405     0.45963    -0.16938776  0.04367304]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 1232. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.95867157  0.9168339  -0.36677355  0.00931346]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 1233. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.986135    0.959121   -0.47426116  0.05695641]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 1234. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9838724   0.899047   -0.20575905  0.12647688]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 1235. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.98020387  0.95568633 -0.18350524 -0.0365023 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 1236. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.99174094  0.914078   -0.15936494  0.1169678 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 1237. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.95814824  0.9373753  -0.5235621   0.06707585]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 1238. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9893091   0.94434094 -0.34657335  0.10874259]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 1239. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.96608067  0.81019735 -0.23551023  0.19439924]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 1240. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9365026   0.8701879  -0.15719467  0.15456283]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 1241. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.98633456  0.7576928  -0.08250237  0.08134568]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 1242. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[5.0169218e-01 8.0147982e-01 6.4587593e-04 1.7315888e-01]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 1243. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9730456   0.7631445  -0.07091188  0.24120426]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 1244. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.98207974  0.57724    -0.04246324  0.2237618 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 1245. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.9736358  -0.09625405 -0.14130437  0.25380254]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 1246. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.98495495  0.39299297 -0.15745276  0.2105887 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 1247. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.98569405  0.9069582  -0.1737997   0.24364471]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 1248. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.97002244  0.7549802  -0.15370697  0.2925738 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 1249. State = [[0.11126582 0.13975555 0.24572375 1.        ]]. Action = [[ 0.99393773  0.7627282  -0.01657844  0.31131363]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
