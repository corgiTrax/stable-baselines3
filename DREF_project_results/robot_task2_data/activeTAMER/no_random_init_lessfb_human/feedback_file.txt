Current timestep = 0. State = [[-0.2373754  -0.00387666  0.2469127   1.        ]]. Action = [[ 0.67527044 -0.29339707  0.8605387   0.00103998]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3912, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.22379787 -0.01252942  0.26796022  1.        ]]. Action = [[-0.59492844  0.0529573  -0.183864    0.49441886]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3765, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.2310584  -0.01152431  0.26552075  1.        ]]. Action = [[-0.97113353  0.6805415   0.8617672   0.14707565]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3471, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.2310584  -0.01152431  0.26552075  1.        ]]. Action = [[-0.6739342   0.44766045 -0.68309885 -0.7500126 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 3 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 3 is tensor(0.2986, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.25121456  0.00220822  0.23234336  1.        ]]. Action = [[-0.5223324  -0.16690701 -0.30219412 -0.58392966]]. Reward = [-1.]
Curr episode timestep = 4
Current timestep = 5. State = [[-0.2513252   0.00230946  0.23177218  1.        ]]. Action = [[-0.79287994  0.23679519 -0.01269174  0.22850132]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2676, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of 0
Current timestep = 6. State = [[-0.25133213  0.00246492  0.23176806  1.        ]]. Action = [[-0.56670713 -0.9292649   0.06773877 -0.718104  ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.25133213  0.00246492  0.23176806  1.        ]]. Action = [[-0.7814601  -0.9764394  -0.7663123  -0.34010804]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.1928, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25133213  0.00246492  0.23176806  1.        ]]. Action = [[-0.80438954 -0.44826365 -0.07604492 -0.45460457]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1811, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.2508175   0.00254011  0.23242418  1.        ]]. Action = [[ 0.18450093 -0.62709576 -0.5953502  -0.78654885]]. Reward = [-1.]
Curr episode timestep = 4
Current timestep = 10. State = [[-0.23531993  0.00261261  0.23261717  1.        ]]. Action = [[ 0.9096589   0.12677908 -0.20415407  0.30131626]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1264, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.20147437  0.00378969  0.23103167  1.        ]]. Action = [[ 0.6864784  -0.02989852  0.06986582  0.6452986 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.25127628  0.00230157  0.23234779  1.        ]]. Action = [[ 0.67849946  0.9777051   0.5252397  -0.8540842 ]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 13. State = [[-0.2530101   0.00150255  0.23184945  1.        ]]. Action = [[-0.61435467 -0.49757272  0.18424249  0.82494664]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.0814, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.25357774 -0.00231713  0.24417146  1.        ]]. Action = [[-0.13399327 -0.18244505  0.91490257  0.7356007 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.2511038   0.00236028  0.23223181  1.        ]]. Action = [[ 0.6523763   0.9229255  -0.05214882 -0.11390603]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 16. State = [[-0.24081631 -0.00807892  0.24393907  1.        ]]. Action = [[ 0.36632955 -0.61740077  0.8706914   0.9887402 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.21908341 -0.01247968  0.25777173  1.        ]]. Action = [[ 0.92714787  0.6127815  -0.91569066  0.11988103]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0335, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.25118104  0.00216464  0.23238526  1.        ]]. Action = [[-0.4697556   0.28987563 -0.1944657  -0.6575795 ]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 19. State = [[-0.2513067   0.00237237  0.2323723   1.        ]]. Action = [[-0.8255829  -0.85454166  0.2473135   0.05864108]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0218, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.2510032   0.0022415   0.23347421  1.        ]]. Action = [[-0.18845522 -0.8443801   0.4198599  -0.3837663 ]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 21. State = [[-0.24424262 -0.00281231  0.24353354  1.        ]]. Action = [[ 0.3467312  -0.21233618  0.73589444  0.9631727 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0191, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.23789385  0.00547267  0.26130262  1.        ]]. Action = [[-0.08899796  0.82044744  0.11400104  0.3163129 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0401, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.25119162  0.00251745  0.23241022  1.        ]]. Action = [[ 0.5912447  -0.4865477  -0.76022416 -0.30319458]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 24. State = [[-2.5138557e-01  7.8475854e-04  2.3244561e-01  1.0000000e+00]]. Action = [[-0.81947875  0.87904453 -0.00134301 -0.11702555]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0338, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.24888358 -0.01059788  0.24385771  1.        ]]. Action = [[ 0.13519168 -0.4781931   0.9791565   0.5958451 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.24307333 -0.02364011  0.25914866  1.        ]]. Action = [[ 0.64750147  0.09618723 -0.5454247   0.30904055]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.25110734  0.00248587  0.23226671  1.        ]]. Action = [[-0.4752952   0.43420637  0.7617662  -0.37716806]]. Reward = [-1.]
Curr episode timestep = 3
Current timestep = 28. State = [[-0.25124845  0.00241839  0.23220727  1.        ]]. Action = [[-0.342723   -0.29615307  0.11372995  0.9473549 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0425, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.25373828 -0.01050653  0.22666515  1.        ]]. Action = [[-0.22871965 -0.7462586  -0.44171494  0.84167576]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.25026363 -0.03142776  0.21493849  1.        ]]. Action = [[ 0.80908227  0.07778907 -0.14181161  0.4128294 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0625, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.21640596 -0.03128332  0.21669024  1.        ]]. Action = [[0.9345461  0.11559129 0.86070764 0.34043813]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.18058544 -0.0175292   0.24120644  1.        ]]. Action = [[0.3819393  0.55042124 0.14108694 0.79957175]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.25114486  0.00246159  0.23241267  1.        ]]. Action = [[-0.38433397  0.45619202  0.9350982  -0.6542307 ]]. Reward = [-1.]
Curr episode timestep = 5
Current timestep = 34. State = [[-0.25253546  0.00158153  0.23169683  1.        ]]. Action = [[-0.3975854  -0.6743959  -0.8475208  -0.77425194]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0595, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.24677353 -0.01249924  0.23592272  1.        ]]. Action = [[ 0.4556502  -0.7270066   0.30582833  0.8450537 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0681, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.2458237  -0.02418042  0.25374186  1.        ]]. Action = [[-0.47369778  0.3920169   0.83890724  0.77045107]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 0
Current timestep = 37. State = [[-0.2544161  -0.03501107  0.278762    1.        ]]. Action = [[ 0.21169126 -0.99734837 -0.49338496  0.41419256]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0715, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.25100252  0.00252695  0.23245455  1.        ]]. Action = [[ 0.04297519 -0.77607214 -0.08252573 -0.25217056]]. Reward = [-1.]
Curr episode timestep = 4
Current timestep = 39. State = [[-0.25084874  0.00267348  0.23338772  1.        ]]. Action = [[ 0.53281176 -0.25845373  0.667047   -0.74621254]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 40. State = [[-0.2375102  -0.00754818  0.23412834  1.        ]]. Action = [[ 0.92273617 -0.4591024  -0.10248893  0.20546281]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.22168168 -0.01061837  0.22729464  1.        ]]. Action = [[-0.3900293   0.5390346  -0.36420435  0.983456  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0849, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 0
Current timestep = 42. State = [[-0.2504067   0.00236048  0.23247337  1.        ]]. Action = [[ 0.02515876 -0.3250258  -0.3609832  -0.3575322 ]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 43. State = [[-0.24334385  0.0030742   0.22591917  1.        ]]. Action = [[ 0.9595628   0.19617236 -0.76226234  0.52026296]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0812, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.25054336  0.00259212  0.23247315  1.        ]]. Action = [[ 0.68682265 -0.6574774  -0.5617721  -0.21187627]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 45. State = [[-0.2512396   0.00240811  0.2322559   1.        ]]. Action = [[ 0.80557346  0.91231227  0.62061167 -0.6307984 ]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 46. State = [[-0.25109243  0.00263591  0.22189143  1.        ]]. Action = [[ 0.12617111  0.09660614 -0.7882495   0.50342846]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0977, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.2508357   0.0023652   0.23234929  1.        ]]. Action = [[ 0.1826328  -0.30558687  0.70449567 -0.08262312]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 48. State = [[-0.25124153  0.00213458  0.23220369  1.        ]]. Action = [[ 0.822248   0.7523432  0.5507935 -0.5801595]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 49. State = [[-0.24676678 -0.00583634  0.23600821  1.        ]]. Action = [[ 0.2387073  -0.41685748  0.48650432  0.7196964 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.1026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.25118563  0.00246321  0.23239408  1.        ]]. Action = [[ 0.8421521  -0.5552225  -0.19626284 -0.33543354]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 51. State = [[-0.25053138  0.00252968  0.23247208  1.        ]]. Action = [[-0.09100807 -0.74322873 -0.3182149  -0.19441688]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 52. State = [[-0.23928888 -0.00943886  0.2394805   1.        ]]. Action = [[ 0.61473083 -0.57006127  0.60400105  0.3426789 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.1049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.2089704  -0.02530074  0.26302394  1.        ]]. Action = [[ 0.96468925 -0.03201771  0.71697915  0.29298353]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.1042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.18256032 -0.01241762  0.27993345  1.        ]]. Action = [[-0.10855973  0.80196905 -0.6940402   0.01383352]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 54 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 54 is tensor(0.1010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.16764359  0.00511186  0.2781879   1.        ]]. Action = [[ 0.6959957  -0.05451792  0.9463136   0.8419659 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.1051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.15301034  0.01650785  0.31485042  1.        ]]. Action = [[-0.7326635   0.531176    0.728775    0.86554337]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.1043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.16876984  0.02713333  0.35076946  1.        ]]. Action = [[-0.09437156 -0.2885058   0.9142144   0.83688104]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.1123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 0
Current timestep = 58. State = [[-0.2512827   0.00251766  0.23236562  1.        ]]. Action = [[ 0.3167584  -0.7816182  -0.38230765 -0.557761  ]]. Reward = [-1.]
Curr episode timestep = 6
Current timestep = 59. State = [[-0.25126427  0.00241629  0.23211572  1.        ]]. Action = [[ 0.92014754  0.49968028  0.9008868  -0.0668171 ]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 60. State = [[-0.2513443   0.00199875  0.23210038  1.        ]]. Action = [[-0.6793621  -0.46286488  0.00216591  0.90983903]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.1092, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.2512482   0.00237914  0.2322851   1.        ]]. Action = [[ 0.6455848  -0.8118178   0.3910817  -0.03244996]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 62. State = [[-0.23883    -0.00247561  0.22826754  1.        ]]. Action = [[ 0.8637866 -0.1751728 -0.3845657  0.6594286]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.25050047  0.00224121  0.23246016  1.        ]]. Action = [[-0.53536063  0.82088923 -0.18933016 -0.21881133]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 64. State = [[-0.24057138  0.01245913  0.23852433  1.        ]]. Action = [[0.86098146 0.5276753  0.3971598  0.8628987 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.1108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.23067793  0.033156    0.24193104  1.        ]]. Action = [[-0.34855002  0.4003954  -0.6007255   0.74382293]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.23948257  0.06110964  0.22733633  1.        ]]. Action = [[ 0.09317684  0.9506724  -0.26505166  0.05658066]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 66 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 66 is tensor(0.1124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.22842978  0.10428276  0.21379934  1.        ]]. Action = [[ 0.6347592   0.7587347  -0.36266166  0.9618434 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.1077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.20662813  0.1264407   0.2000641   1.        ]]. Action = [[ 0.80685115  0.32246184 -0.05281323  0.39869785]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 68 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1246, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.19673087  0.12718798  0.20235375  1.        ]]. Action = [[ 0.6445968  -0.03600943  0.07368147  0.8097689 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 69 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1244, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 0
Current timestep = 70. State = [[-0.18531272  0.11461799  0.2026131   1.        ]]. Action = [[-0.9680563  -0.82793653  0.35471177  0.5374044 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.0950, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-0.20376134  0.09262639  0.20831285  1.        ]]. Action = [[ 0.98561835  0.15394628 -0.88823813  0.7751181 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.0964, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.21001056  0.0832212   0.22095042  1.        ]]. Action = [[-0.62181455 -0.47624946  0.7930119   0.87443376]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.0997, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 0
Current timestep = 73. State = [[-0.22945035  0.06660059  0.23829734  1.        ]]. Action = [[ 0.48186874 -0.16624802 -0.87115633  0.60635567]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.1055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of 0
Current timestep = 74. State = [[-0.23218492  0.07555321  0.21509875  1.        ]]. Action = [[-0.5990861   0.6950817   0.00319779  0.35895717]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.1159, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.23783319  0.10112827  0.2095136   1.        ]]. Action = [[ 0.5587646   0.49849987 -0.3792771   0.56797147]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.21384513  0.09937729  0.19718446  1.        ]]. Action = [[ 0.87345505 -0.7551327  -0.22555465  0.67460656]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.0910, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.19297199  0.06548929  0.18817939  1.        ]]. Action = [[-0.42719227 -0.96332127 -0.13232934  0.28394628]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.1002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 0
Current timestep = 78. State = [[-0.20198661  0.02062979  0.18328725  1.        ]]. Action = [[-0.18654013 -0.9389793  -0.13826394  0.7440002 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.0973, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of 0
Current timestep = 79. State = [[-0.2501454   0.002644    0.23256795  1.        ]]. Action = [[ 0.5082257  -0.12450993 -0.642867   -0.06138206]]. Reward = [-1.]
Curr episode timestep = 15
Current timestep = 80. State = [[-0.25103718  0.00240796  0.23224439  1.        ]]. Action = [[ 0.99336195 -0.3402455   0.6602948  -0.42934275]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 81. State = [[-0.24289809 -0.00845829  0.2405737   1.        ]]. Action = [[ 0.4828689  -0.5834032   0.72718847  0.8244145 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.0864, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.22306456 -0.0360623   0.25281918  1.        ]]. Action = [[ 0.6292772  -0.61481607 -0.36627936  0.8808744 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.0800, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.20089789 -0.04093067  0.25375938  1.        ]]. Action = [[0.34607518 0.59280455 0.3836776  0.15580225]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 83 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 83 is tensor(0.1042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.19671956 -0.03690954  0.26556212  1.        ]]. Action = [[-0.8495893  -0.29696405  0.0601269   0.8421198 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0823, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.25078025  0.00248047  0.23249078  1.        ]]. Action = [[ 0.3490932   0.80222535  0.29964352 -0.3564676 ]]. Reward = [-1.]
Curr episode timestep = 4
Current timestep = 86. State = [[-2.5036472e-01  6.0027093e-04  2.3251688e-01  1.0000000e+00]]. Action = [[-0.02228105 -0.6026963   0.15301836 -0.43699747]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 87. State = [[-0.25235277 -0.01796547  0.23307547  1.        ]]. Action = [[-0.47678673 -0.4163946   0.60875857 -0.5226193 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0845, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 0
Current timestep = 88. State = [[-0.24040459 -0.02067621  0.2442722   1.        ]]. Action = [[0.79570544 0.31570923 0.7205646  0.81818867]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0730, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25117382  0.00240802  0.23229402  1.        ]]. Action = [[ 0.04243016 -0.85120773 -0.46667278 -0.60354704]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 90. State = [[-0.23643833 -0.00976538  0.24104133  1.        ]]. Action = [[ 0.9176018  -0.59104216  0.8227377   0.57689786]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0701, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.19881797 -0.03146115  0.27537975  1.        ]]. Action = [[ 0.77892137 -0.3015042   0.98112917  0.9163163 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0677, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.1621131  -0.03817685  0.30660385  1.        ]]. Action = [[ 0.6704515   0.17229319 -0.19312513  0.06213808]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.0998, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.13937019 -0.0374243   0.32012123  1.        ]]. Action = [[ 0.10308146 -0.13854241  0.9496505   0.89411974]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0832, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.1196776  -0.0454702   0.36269233  1.        ]]. Action = [[ 0.52273893 -0.2539084   0.87360096  0.91413987]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0874, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.10523383 -0.05146537  0.39302495  1.        ]]. Action = [[-0.45562088 -0.62719053  0.7819878   0.94292784]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0917, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 0
Current timestep = 96. State = [[-0.10542432 -0.05143978  0.3930171   1.        ]]. Action = [[ 0.5526817  -0.7327574   0.77637887  0.8067751 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0953, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 0
Current timestep = 97. State = [[-0.10455289 -0.05963885  0.3822171   1.        ]]. Action = [[ 0.12424135 -0.43418968 -0.9647402   0.5726304 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.1006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 0
Current timestep = 98. State = [[-0.10536823 -0.06009839  0.37080002  1.        ]]. Action = [[-0.8826895  0.6530938  0.9375249  0.1175251]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 98 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 98 is tensor(0.0983, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.12200919 -0.04166579  0.38573387  1.        ]]. Action = [[ 0.7070689   0.08850312 -0.68117285  0.86935234]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.0960, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.25071996  0.00272503  0.23260903  1.        ]]. Action = [[ 0.4218899  -0.9749442   0.45528674 -0.41873884]]. Reward = [-1.]
Curr episode timestep = 10
Current timestep = 101. State = [[-2.4645922e-01  8.1928342e-04  2.2304310e-01  1.0000000e+00]]. Action = [[ 0.81515443  0.01921403 -0.7380448   0.92461383]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.0766, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 0
Current timestep = 102. State = [[-0.2507546   0.00230192  0.23243806  1.        ]]. Action = [[-0.67571706  0.5052018  -0.87032664 -0.64140123]]. Reward = [-1.]
Curr episode timestep = 1
Current timestep = 103. State = [[-0.25071645  0.00190604  0.23279642  1.        ]]. Action = [[-0.58465296  0.4898622  -0.07069343  0.7216332 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.0920, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of 0
Current timestep = 104. State = [[-0.25074944  0.00152227  0.23281105  1.        ]]. Action = [[-0.90146273 -0.90795714 -0.21869826  0.8823154 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0764, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of 0
Current timestep = 105. State = [[-0.25074944  0.00152227  0.23281105  1.        ]]. Action = [[-0.47885597 -0.6446475   0.34350348  0.78803706]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.0883, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of 0
Current timestep = 106. State = [[-0.2508645   0.00279959  0.23231596  1.        ]]. Action = [[ 0.39827847  0.0844537  -0.5322387  -0.706572  ]]. Reward = [-1.]
Curr episode timestep = 3
Current timestep = 107. State = [[-0.24695237  0.00301998  0.21979633  1.        ]]. Action = [[-0.52941895 -0.881058    0.9168116   0.94898045]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0681, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.23478797  0.01986617  0.20945667  1.        ]]. Action = [[ 0.7969426   0.87803817 -0.4101193   0.8365123 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.0769, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.20968522  0.04152505  0.19402033  1.        ]]. Action = [[ 0.81852376  0.5852089  -0.76885784  0.6591196 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.0771, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 0
Current timestep = 110. State = [[-0.2157701   0.03867824  0.18346478  1.        ]]. Action = [[-0.47223735 -0.349797   -0.83863264  0.82288826]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.0851, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.21880662  0.03187762  0.15717372  1.        ]]. Action = [[0.9066551  0.01010823 0.84571874 0.96133494]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0732, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.2126257   0.01935546  0.16882275  1.        ]]. Action = [[ 0.37172496 -0.59881145  0.9760355   0.81262326]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0747, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.1997334  0.0039595  0.1910524  1.       ]]. Action = [[0.6128496  0.08071935 0.10909843 0.9661186 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.0853, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.1850826  0.0047775  0.1996564  1.       ]]. Action = [[ 0.5029378  -0.35308647 -0.153045    0.5905038 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.0926, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of 0
Current timestep = 115. State = [[-0.1850826  0.0047775  0.1996564  1.       ]]. Action = [[ 0.96871257  0.44923425 -0.82690275  0.11422348]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0716, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of 0
Current timestep = 116. State = [[-0.1932773   0.00977411  0.18925412  1.        ]]. Action = [[-0.6020305   0.25936842 -0.7368875   0.34919906]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.0832, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.20155323  0.02223058  0.17601751  1.        ]]. Action = [[-0.1615622  0.3390168  0.3502705  0.5028939]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.0926, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.20501381  0.03251548  0.17985757  1.        ]]. Action = [[ 0.8972528   0.80948913 -0.14644396  0.38914752]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.0796, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.21293688  0.04573436  0.17793566  1.        ]]. Action = [[-0.48744404  0.69944584 -0.09110522  0.29407144]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.0851, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.22753626  0.06606316  0.18048495  1.        ]]. Action = [[-0.2663281  -0.17114198  0.24766159  0.8742583 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0801, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of -1
Current timestep = 121. State = [[-0.2508093   0.00261607  0.23272714  1.        ]]. Action = [[ 0.33582306  0.85890174  0.29106688 -0.16881126]]. Reward = [-1.]
Curr episode timestep = 14
Current timestep = 122. State = [[-0.23879823  0.01979901  0.23914793  1.        ]]. Action = [[0.9488869  0.91930985 0.16072619 0.00287938]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 122 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 122 is tensor(0.0738, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.21903154  0.02862043  0.259203    1.        ]]. Action = [[-0.4005419  -0.83738804  0.9225216   0.92415965]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0556, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.21537696 -0.0078537   0.28599274  1.        ]]. Action = [[ 0.8684778  -0.901394   -0.17666852  0.70682454]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0609, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.18890844 -0.02615585  0.29302907  1.        ]]. Action = [[0.25874543 0.38473797 0.69657564 0.9489286 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0640, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.16620752 -0.00596955  0.30461702  1.        ]]. Action = [[ 0.87954426  0.7306309  -0.7869708   0.90355337]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0656, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.25098813  0.00272092  0.23245212  1.        ]]. Action = [[ 0.313879   -0.58879244  0.6416199  -0.59926367]]. Reward = [-1.]
Curr episode timestep = 5
Current timestep = 128. State = [[-0.25352603  0.00544694  0.22067809  1.        ]]. Action = [[ 0.35936522  0.29906726 -0.9201013   0.55548334]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0695, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.25418818  0.02022732  0.18659589  1.        ]]. Action = [[-0.10269886  0.46938217 -0.5282964   0.8932102 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0650, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.24125198  0.03072281  0.1781865   1.        ]]. Action = [[ 0.96861756 -0.16818464  0.78807473  0.78204465]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0592, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.21155694  0.02822095  0.1949369   1.        ]]. Action = [[ 0.8410983  -0.83408433  0.14671433  0.5980699 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0617, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 0
Current timestep = 132. State = [[-0.21113725  0.03956178  0.18557875  1.        ]]. Action = [[ 0.33941162  0.64297414 -0.98196805  0.78278947]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0562, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.19152346  0.07012961  0.17270991  1.        ]]. Action = [[0.22703826 0.7399732  0.9366424  0.74344444]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0531, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 0
Current timestep = 134. State = [[-0.18396097  0.09309456  0.19244018  1.        ]]. Action = [[ 5.5666673e-01 -8.4257466e-01  1.1718273e-04  6.2751579e-01]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 0
Current timestep = 135. State = [[-0.18368758  0.09345334  0.19291636  1.        ]]. Action = [[ 0.72699    -0.87698793 -0.6180188   0.5923579 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0579, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of 0
Current timestep = 136. State = [[-0.18368758  0.09345334  0.19291636  1.        ]]. Action = [[ 0.91055334 -0.9970663   0.26433325  0.15158737]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0571, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 0
Current timestep = 137. State = [[-0.16773699  0.08819883  0.2082271   1.        ]]. Action = [[ 0.8028891  -0.29865044  0.835292    0.9823859 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.25090316  0.00248376  0.23265488  1.        ]]. Action = [[ 0.6353142 -0.8367016  0.9258679 -0.1343373]]. Reward = [-1.]
Curr episode timestep = 10
Current timestep = 139. State = [[-2.5322926e-01  7.4741436e-04  2.3168626e-01  1.0000000e+00]]. Action = [[-0.34017265  0.5836401   0.44849455  0.83085966]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0566, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 0
Current timestep = 140. State = [[-0.2554939  -0.004331    0.22618452  1.        ]]. Action = [[-0.02142119 -0.29123068 -0.5075292   0.2163961 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.0781, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.24433012 -0.02110989  0.22633381  1.        ]]. Action = [[ 0.85227656 -0.44142818  0.79439163  0.86428344]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.22001408 -0.02113985  0.23469982  1.        ]]. Action = [[ 0.64654565  0.6895658  -0.45332485  0.667537  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 142 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0566, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.19566227 -0.00918632  0.23728469  1.        ]]. Action = [[ 0.25758004 -0.14184713  0.7600167   0.48242784]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 143 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0622, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.1687039   0.00247311  0.26119062  1.        ]]. Action = [[0.95788455 0.64927554 0.18660426 0.843143  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 144 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0482, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.12352456  0.01277226  0.2883154   1.        ]]. Action = [[ 0.83585453 -0.34694123  0.7731228   0.9643172 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0448, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.09880764 -0.0060246   0.32077116  1.        ]]. Action = [[ 0.02033675 -0.68305784  0.38204062  0.40779626]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0682, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.25108805  0.00226948  0.2324028   1.        ]]. Action = [[-0.53358996 -0.38941467 -0.26488698 -0.47062862]]. Reward = [-1.]
Curr episode timestep = 8
Current timestep = 148. State = [[-0.25077778  0.00226947  0.23358679  1.        ]]. Action = [[ 0.72381663 -0.325238    0.60464907 -0.09720945]]. Reward = [-1.]
Curr episode timestep = 0
Current timestep = 149. State = [[-0.2381177  -0.00959273  0.24684462  1.        ]]. Action = [[-0.40677965  0.03627789  0.02411234  0.80177057]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0569, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.2290056  -0.00876733  0.24540025  1.        ]]. Action = [[ 0.35667825  0.40995407 -0.70712316  0.94211257]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0454, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.2231162  0.0136275  0.2313982  1.       ]]. Action = [[-0.34864652  0.70355296  0.10594642  0.5785279 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0561, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.22598542  0.04519707  0.22270826  1.        ]]. Action = [[ 0.61423135  0.5588491  -0.9064716   0.40120924]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0436, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.20760718  0.05946737  0.19627738  1.        ]]. Action = [[ 0.97418094  0.97795653  0.15408707 -0.2778834 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 153 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 153 is tensor(0.0445, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.251082    0.00227315  0.23229931  1.        ]]. Action = [[ 0.7527406  -0.9072959   0.7391312  -0.25913697]]. Reward = [-1.]
Curr episode timestep = 5
Current timestep = 155. State = [[-0.23897162  0.01385258  0.24703412  1.        ]]. Action = [[0.9336071 0.7163613 0.9365493 0.9354254]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.21090552  0.02602141  0.2772644   1.        ]]. Action = [[ 0.27741718 -0.33064497  0.19565535  0.9404267 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0551, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.21000521  0.02683259  0.2967154   1.        ]]. Action = [[-0.8902974   0.3584082   0.71605515  0.4519955 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0517, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.22760086  0.02788553  0.31207788  1.        ]]. Action = [[ 0.61682034 -0.4518249  -0.8227858   0.6779151 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0511, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.21829145  0.01746571  0.2949142   1.        ]]. Action = [[-0.86992514  0.04098618  0.26071548  0.47516334]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0542, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 0
Current timestep = 160. State = [[-0.20860463  0.00356913  0.2890319   1.        ]]. Action = [[ 0.8083277 -0.6728159 -0.5370551  0.74972  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0476, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.19202818 -0.00296021  0.25668705  1.        ]]. Action = [[-0.9250993   0.6935594  -0.46733636  0.97000873]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.20663466  0.02234323  0.23808913  1.        ]]. Action = [[ 0.34597278  0.45805013 -0.4704777   0.7155217 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0592, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.20260817  0.03514981  0.21217667  1.        ]]. Action = [[ 0.21827114  0.06722105 -0.70677906  0.91349745]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0531, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.19321954  0.03715986  0.18846557  1.        ]]. Action = [[0.25192487 0.02147019 0.17501163 0.56807315]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0665, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of 0
Current timestep = 165. State = [[-0.1871172   0.03795704  0.19112134  1.        ]]. Action = [[ 0.783705   -0.16137272  0.8060653   0.87458324]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0463, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 0
Current timestep = 166. State = [[-0.1871172   0.03795704  0.19112134  1.        ]]. Action = [[ 0.7733426  -0.29926986 -0.037902    0.89994836]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 0
Current timestep = 167. State = [[-0.18679594  0.03320307  0.19212952  1.        ]]. Action = [[-0.16888076 -0.30216467  0.20305884  0.9525399 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0530, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of 0
Current timestep = 168. State = [[-0.17691787  0.04302924  0.20674784  1.        ]]. Action = [[0.6277809  0.8076582  0.78927255 0.94341207]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.14861841  0.055331    0.23644868  1.        ]]. Action = [[ 0.69498956 -0.42743313  0.26453972  0.9840746 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0453, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.13905007  0.06305289  0.26247     1.        ]]. Action = [[-0.8146253  0.7964667  0.7670796  0.5963036]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.15484677  0.07923364  0.277421    1.        ]]. Action = [[-0.01940805 -0.32595122 -0.6117124   0.7118995 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0505, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of 0
Current timestep = 172. State = [[-0.14391395  0.07512366  0.27450034  1.        ]]. Action = [[0.9335923  0.17499554 0.30630207 0.8863878 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0402, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.12452567  0.07884789  0.27604967  1.        ]]. Action = [[ 0.81476057 -0.18301988 -0.8666447  -0.06178975]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Scene graph at timestep 173 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 173 is tensor(0.0411, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 173 of 0
Current timestep = 174. State = [[-0.12522075  0.07938955  0.27749842  1.        ]]. Action = [[-0.24856275  0.05867183  0.19612324  0.5431733 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0546, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.11571343  0.09432698  0.29169825  1.        ]]. Action = [[0.93779993 0.8775835  0.79461646 0.86447275]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 175 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.25085708  0.00265549  0.23264556  1.        ]]. Action = [[ 0.36368418  0.66233087  0.39913785 -0.2697839 ]]. Reward = [-1.]
Curr episode timestep = 21
Current timestep = 177. State = [[-0.24652864 -0.00721469  0.22429755  1.        ]]. Action = [[ 0.7514168  -0.49035376 -0.5554386   0.80437136]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0377, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of 0
Current timestep = 178. State = [[-0.22602092 -0.01141806  0.20456833  1.        ]]. Action = [[ 0.49060118  0.5402484  -0.2753508   0.7018378 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0452, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.2511179   0.00245409  0.23232718  1.        ]]. Action = [[-0.21961933 -0.59149814  0.98276234 -0.14201492]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 180. State = [[-2.5099105e-01  3.1156815e-04  2.3394264e-01  1.0000000e+00]]. Action = [[-0.44995975 -0.8251507  -0.75326216  0.9222131 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 180 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0328, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 0
Current timestep = 181. State = [[-2.3856319e-01  3.2627734e-04  2.4277499e-01  1.0000000e+00]]. Action = [[0.93351364 0.20821023 0.55116534 0.7817867 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 181 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0421, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.25126883  0.00244793  0.23220098  1.        ]]. Action = [[-0.7450739   0.665606    0.9461422  -0.02570826]]. Reward = [-1.]
Curr episode timestep = 2
Current timestep = 183. State = [[-0.25081274  0.00231751  0.23242985  1.        ]]. Action = [[ 0.11345625 -0.0113377   0.1186589   0.98545814]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 183 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 0
Current timestep = 184. State = [[-0.25066435  0.00213769  0.23255263  1.        ]]. Action = [[-0.35623848 -0.7995417  -0.13679755  0.9730406 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 184 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0402, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of 0
Current timestep = 185. State = [[-0.2368364   0.00211784  0.24692391  1.        ]]. Action = [[0.6846293  0.01694989 0.8923992  0.94056416]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 185 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0394, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.21276052  0.01760074  0.28550398  1.        ]]. Action = [[0.17847359 0.8334174  0.8444991  0.4991448 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 186 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0455, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of 0
Current timestep = 187. State = [[-0.20502369  0.03729976  0.3087487   1.        ]]. Action = [[ 0.26304126 -0.1895526  -0.46832561  0.8200115 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 187 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0500, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.20035443  0.03488104  0.31402627  1.        ]]. Action = [[-0.10017407  0.0389955   0.7630379   0.15574753]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 188 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 188 is tensor(0.0611, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.200337    0.0269297   0.34039974  1.        ]]. Action = [[-0.7470692  -0.44574702  0.48102784  0.9474226 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 189 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of 0
Current timestep = 190. State = [[-0.21279599  0.00813946  0.3553829   1.        ]]. Action = [[ 0.7194835  -0.3264823  -0.43425053  0.65301037]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 190 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0505, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.20426857  0.00352589  0.33874047  1.        ]]. Action = [[ 0.01231658  0.23008668 -0.6042475   0.6770804 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 191 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0523, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-1.8982035e-01 -4.8283109e-04  3.1445605e-01  1.0000000e+00]]. Action = [[ 0.8005061  -0.41962785 -0.29466617  0.927536  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 192 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0450, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.15256213  0.00751262  0.30545798  1.        ]]. Action = [[0.9323529  0.8573003  0.21780515 0.9734191 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 193 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-0.11611273  0.03158389  0.3139631   1.        ]]. Action = [[0.47113025 0.25731385 0.35445142 0.859427  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 194 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0510, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.09209919  0.03178898  0.33764923  1.        ]]. Action = [[ 0.32024717 -0.4052447   0.6132884   0.87442136]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 195 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0509, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.06817318  0.00892398  0.36711138  1.        ]]. Action = [[ 0.9766364  -0.8024684   0.50962186  0.63783   ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 196 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0452, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of 0
Current timestep = 197. State = [[-0.03409526 -0.01238044  0.38417724  1.        ]]. Action = [[0.38635898 0.16686356 0.5984211  0.95517397]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 197 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0570, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 0
Current timestep = 198. State = [[-0.03417804 -0.01237296  0.38417318  1.        ]]. Action = [[-0.29138184  0.27898633  0.90620697  0.890481  ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 198 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 0
Current timestep = 199. State = [[-0.02897099 -0.02364736  0.3903878   1.        ]]. Action = [[ 0.43927145 -0.54098904  0.3469746   0.31159735]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 199 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0683, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.01699955 -0.02204634  0.39448112  1.        ]]. Action = [[ 0.11813831  0.9120742  -0.2734642   0.7620264 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 200 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0562, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.00599776  0.00192762  0.38425103  1.        ]]. Action = [[ 0.8359343   0.05181944 -0.82706183  0.859779  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 201 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0535, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[0.03750245 0.01763456 0.34528807 1.        ]]. Action = [[ 0.37716174  0.69262266 -0.09418792  0.6048615 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 202 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0610, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[0.06282056 0.03709034 0.34893355 1.        ]]. Action = [[0.54972637 0.12576962 0.39851952 0.37682867]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 203 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[0.081624   0.02607168 0.36827263 1.        ]]. Action = [[-0.54557073 -0.8996016   0.30332088  0.69145894]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 204 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[ 0.07766092 -0.01295689  0.36987957  1.        ]]. Action = [[-0.0041123 -0.8607646 -0.8020518  0.8958583]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 205 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0512, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[ 0.07384184 -0.04403869  0.34358025  1.        ]]. Action = [[-0.5791214  -0.5609905  -0.5782056   0.92786336]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 206 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0530, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[ 0.05733085 -0.06431058  0.33020207  1.        ]]. Action = [[-0.45862687  0.10468638  0.43810523  0.51112866]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 207 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0682, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[ 0.04194505 -0.06159669  0.35048574  1.        ]]. Action = [[-0.278237    0.11504865  0.40463638  0.9332502 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 208 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0605, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[ 0.02417351 -0.07063475  0.3784774   1.        ]]. Action = [[-0.80999976 -0.5021404   0.8733237   0.84597206]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 209 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0486, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.02064897 -0.09490178  0.39641067  1.        ]]. Action = [[-0.25395322 -0.67197114 -0.8758291   0.63591385]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 210 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0520, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.03625736 -0.1162202   0.35822278  1.        ]]. Action = [[ 0.37724113 -0.2750342  -0.4979549   0.60768485]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 211 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0613, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.03641719 -0.13448177  0.33060253  1.        ]]. Action = [[-0.47508174 -0.4891554  -0.90745026  0.56814075]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 212 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.06061342 -0.13628952  0.3016977   1.        ]]. Action = [[-0.86015713  0.9099374   0.3865806   0.51062405]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 213 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.08435479 -0.10282592  0.30844694  1.        ]]. Action = [[-0.3183353   0.6339042   0.03246486  0.9580791 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 215. State = [[-0.08420204 -0.09719805  0.32449287  1.        ]]. Action = [[ 0.8611028  -0.63104093  0.78726244  0.9284024 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 216. State = [[-0.08429675 -0.09716151  0.32303172  1.        ]]. Action = [[-0.28553367  0.4841733  -0.9970787   0.8173095 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 217. State = [[-0.09113076 -0.07644503  0.31210944  1.        ]]. Action = [[-0.82691866  0.88348734  0.27322972  0.27527308]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 218. State = [[-0.12009346 -0.06296661  0.30946013  1.        ]]. Action = [[-0.74463725 -0.5346748  -0.2526449   0.74419284]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 219. State = [[-0.13213328 -0.07065168  0.3057982   1.        ]]. Action = [[ 0.86439335 -0.18383652  0.1474483   0.84493387]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 220. State = [[-0.11781695 -0.07937814  0.30382407  1.        ]]. Action = [[ 0.91140175 -0.30163443 -0.31528938  0.5337707 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 221. State = [[-0.08657243 -0.09290682  0.2815992   1.        ]]. Action = [[ 0.885188   -0.31071186 -0.8470639   0.65398633]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 222. State = [[-0.04032433 -0.09559509  0.2689614   1.        ]]. Action = [[0.94764626 0.37885046 0.8903301  0.78547955]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 223. State = [[-0.01258868 -0.07542695  0.28540727  1.        ]]. Action = [[-0.1468705   0.68174386  0.04874444  0.94654584]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 224. State = [[-0.00114058 -0.06581026  0.29064995  1.        ]]. Action = [[ 0.8436918  -0.27971476 -0.27739215  0.56644547]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 225. State = [[ 0.01505562 -0.08341339  0.28634265  1.        ]]. Action = [[-0.37707222 -0.80484354  0.42148829  0.6802621 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 226. State = [[ 0.003571   -0.10319959  0.28150168  1.        ]]. Action = [[-0.89508414 -0.14361161 -0.8925189   0.45357752]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 227. State = [[-0.01012033 -0.11546804  0.26487303  1.        ]]. Action = [[ 0.0381856  -0.28506064 -0.06621194  0.8739704 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 228. State = [[-0.0211082  -0.11134689  0.2533989   1.        ]]. Action = [[-0.96580106  0.85122585 -0.32104468  0.9822371 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 229. State = [[-0.04142492 -0.09532408  0.24074206  1.        ]]. Action = [[ 0.64341664 -0.46148545 -0.5327092   0.7522552 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 230. State = [[-0.04345725 -0.0938122   0.23882934  1.        ]]. Action = [[ 0.9881182  0.9879904 -0.5851747  0.7039609]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 231. State = [[-0.04332852 -0.10772011  0.24918447  1.        ]]. Action = [[ 0.08072722 -0.75634074  0.54626215  0.5704472 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 232. State = [[-0.25081265  0.00250863  0.23268087  1.        ]]. Action = [[ 0.2691909   0.781456    0.76117754 -0.02252758]]. Reward = [-1.]
Curr episode timestep = 49
Current timestep = 233. State = [[-0.25200012  0.00394903  0.24037693  1.        ]]. Action = [[0.19443333 0.15841115 0.8605231  0.8946042 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 234. State = [[-0.2520717   0.00549456  0.2558638   1.        ]]. Action = [[-0.5320439  -0.15616816  0.48860812  0.85673976]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 235. State = [[-0.24677029 -0.00423476  0.2593611   1.        ]]. Action = [[ 0.4156053  -0.5726818   0.070822    0.71877027]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 236. State = [[-0.2259849  -0.01218297  0.2807742   1.        ]]. Action = [[0.7173073  0.18275404 0.97306514 0.9810071 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 237. State = [[-0.1930757   0.00402908  0.3164577   1.        ]]. Action = [[0.79140234 0.8122122  0.4059993  0.89553344]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 238. State = [[-0.15852214  0.03045695  0.33974072  1.        ]]. Action = [[0.81121016 0.44097185 0.19117653 0.25587583]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 239. State = [[-0.14270596  0.04696263  0.33600274  1.        ]]. Action = [[-0.78518194  0.12381816 -0.71480244  0.5572438 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 240. State = [[-0.1409481   0.05808066  0.32634562  1.        ]]. Action = [[0.79398394 0.2841648  0.00397837 0.33738017]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 241. State = [[-0.12120582  0.05611827  0.31944132  1.        ]]. Action = [[ 0.4008925  -0.53654164 -0.03519517  0.243029  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 242. State = [[-0.09591248  0.04498176  0.3147892   1.        ]]. Action = [[ 0.906363   -0.1081661  -0.15543675  0.63143706]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 243. State = [[-0.06493358  0.02417031  0.3223501   1.        ]]. Action = [[ 0.08544123 -0.8785585   0.783728    0.27019155]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 244. State = [[-0.06031944  0.01603442  0.35768232  1.        ]]. Action = [[-0.44709468  0.6644287   0.9706788   0.91759014]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 245. State = [[-0.05919249  0.00810293  0.3864941   1.        ]]. Action = [[ 0.7586961  -0.840859    0.00476348  0.8428943 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 246. State = [[-0.03998156 -0.01476732  0.3952675   1.        ]]. Action = [[ 0.39978874 -0.41425514  0.13780868  0.28922927]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 247. State = [[-0.026483   -0.02637753  0.40126118  1.        ]]. Action = [[0.0698694  0.637185   0.6561736  0.56226814]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 248. State = [[-0.02136743 -0.01550368  0.39472446  1.        ]]. Action = [[ 0.9256711   0.7063873  -0.60676783  0.05537903]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 249. State = [[0.01585002 0.01557524 0.37169278 1.        ]]. Action = [[ 0.554456    0.9761592  -0.13077366  0.74125004]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 250. State = [[0.0351151  0.02145855 0.35728317 1.        ]]. Action = [[-0.78703237 -0.9906405  -0.4952196   0.8425454 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 251. State = [[ 0.03146443 -0.00980834  0.3468891   1.        ]]. Action = [[ 0.23001266 -0.87611085 -0.49518883  0.68897915]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 252. State = [[ 0.04186889 -0.0342297   0.32504702  1.        ]]. Action = [[ 0.8910167  -0.06179142 -0.2697935   0.83429074]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 253. State = [[ 0.05669099 -0.05035917  0.29749694  1.        ]]. Action = [[-0.8677857  -0.50562006 -0.76394534  0.7641721 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 254. State = [[ 0.04762111 -0.05561671  0.27278215  1.        ]]. Action = [[-0.8445999   0.3307078  -0.6811876   0.14984536]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 255. State = [[ 0.02531074 -0.04856388  0.24657436  1.        ]]. Action = [[0.00403082 0.24178958 0.1802324  0.91762996]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 256. State = [[ 0.02117589 -0.04322643  0.24589802  1.        ]]. Action = [[ 0.5564915   0.6389911  -0.8747587   0.90139496]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 257. State = [[ 0.021042   -0.04247642  0.24591328  1.        ]]. Action = [[-0.5050416  -0.45176232 -0.88574785  0.7120371 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 258. State = [[ 0.02129165 -0.05163557  0.26073325  1.        ]]. Action = [[-0.17815685 -0.44556844  0.8402214   0.69616747]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 259. State = [[ 0.01800945 -0.0759479   0.28011876  1.        ]]. Action = [[ 0.94264007 -0.85131127 -0.23432451  0.37237203]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 260. State = [[ 0.02513728 -0.09710516  0.2761571   1.        ]]. Action = [[ 0.61045134 -0.16249156 -0.88236284  0.572773  ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 261. State = [[ 0.02997043 -0.10942609  0.27185377  1.        ]]. Action = [[ 0.96836853 -0.72417134 -0.6260911   0.91740155]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 262. State = [[ 0.06121194 -0.1467598   0.24654925  1.        ]]. Action = [[ 0.42410064 -0.66892445 -0.01575619  0.07883513]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 262 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.0492, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[ 0.08892123 -0.17723924  0.23191637  1.        ]]. Action = [[ 0.17121315 -0.49915558 -0.41153353  0.91264606]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 263 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.0482, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[ 0.1004902  -0.19261718  0.21922222  1.        ]]. Action = [[ 0.08239818 -0.9554062   0.7326565   0.3616507 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 264 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.0512, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[ 0.10062207 -0.1926832   0.21937595  1.        ]]. Action = [[ 0.43504238 -0.11528236 -0.5315509   0.857774  ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 266. State = [[ 0.10062207 -0.1926832   0.21937595  1.        ]]. Action = [[ 0.9336021   0.79733276 -0.21473444  0.9496002 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 267. State = [[ 0.10062207 -0.1926832   0.21937595  1.        ]]. Action = [[ 0.7738955  -0.44252706  0.73981595  0.47737217]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 268. State = [[ 0.09976397 -0.21033394  0.23779605  1.        ]]. Action = [[-0.15797311 -0.7799149   0.96531105  0.7052721 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 269. State = [[ 0.0966187  -0.22980078  0.25874445  1.        ]]. Action = [[0.18365324 0.6850281  0.5793772  0.6702895 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 270. State = [[ 0.09168446 -0.24550776  0.27905452  1.        ]]. Action = [[-0.59232515 -0.4573592   0.8280859   0.9645833 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 271. State = [[ 0.07479955 -0.27519184  0.298262    1.        ]]. Action = [[-0.41374075 -0.8267359  -0.3529353   0.69362724]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 272. State = [[ 0.06439669 -0.2846852   0.3121898   1.        ]]. Action = [[-0.50430655  0.8597679   0.7132478   0.81699944]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 273. State = [[ 0.04907515 -0.2705715   0.32909802  1.        ]]. Action = [[-0.59308267 -0.9322709   0.5342798   0.9274478 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 273 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.0389, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[ 0.04790805 -0.27207452  0.3324112   1.        ]]. Action = [[ 0.30061054 -0.33285683  0.11926138  0.45604157]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 275. State = [[ 0.04932567 -0.26539513  0.32832074  1.        ]]. Action = [[ 0.17983842  0.51436603 -0.33961272  0.9097383 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 276. State = [[ 0.05951219 -0.25722414  0.3350312   1.        ]]. Action = [[ 0.6910579  -0.15493512  0.6427424   0.91170335]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 276 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[ 0.07180281 -0.25296202  0.34407243  1.        ]]. Action = [[ 0.49983096 -0.5930686   0.71154785  0.46170783]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 277 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[ 0.06151779 -0.2671079   0.345729    1.        ]]. Action = [[-0.9626957  -0.56811434 -0.11384588  0.04736423]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 278 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 278 is tensor(0.0359, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[ 0.04642581 -0.281216    0.33222133  1.        ]]. Action = [[ 0.22967744  0.1802355  -0.98367673  0.50570107]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 280. State = [[ 0.05002801 -0.27982166  0.3077029   1.        ]]. Action = [[ 0.54194427 -0.95256686  0.84465003  0.3151877 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 280 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 280 is tensor(0.0374, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[ 0.0515844  -0.28785414  0.31927267  1.        ]]. Action = [[ 0.06740201 -0.44129467  0.9027817   0.9828737 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 282. State = [[ 0.05077869 -0.29688755  0.33348748  1.        ]]. Action = [[-0.0873003 -0.9154816  0.3716811  0.9390738]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 283. State = [[ 0.05054484 -0.29757887  0.33415586  1.        ]]. Action = [[-0.86595607 -0.24076384  0.36152554  0.6772609 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 284. State = [[ 0.06113498 -0.29176995  0.3463442   1.        ]]. Action = [[0.71077466 0.20284247 0.61723185 0.7097683 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 285. State = [[ 0.07279873 -0.28433537  0.3615754   1.        ]]. Action = [[ 0.6637554   0.07065308 -0.9911532   0.75500894]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 286. State = [[ 0.07424708 -0.28341088  0.36374867  1.        ]]. Action = [[ 0.64374065 -0.26673633 -0.41901034  0.86456466]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 287. State = [[ 0.07272975 -0.2773718   0.37505317  1.        ]]. Action = [[-0.6958619   0.5363773   0.35617232  0.83903575]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 288. State = [[ 0.0669273  -0.27124298  0.38901505  1.        ]]. Action = [[0.88078   0.7758509 0.591468  0.6863742]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 289. State = [[ 0.06336269 -0.26061365  0.38060334  1.        ]]. Action = [[-0.3010947   0.5849736  -0.7788966   0.60433114]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 290. State = [[ 0.0534573  -0.2537674   0.37308416  1.        ]]. Action = [[ 0.1725707  -0.6049048   0.48839593  0.26725137]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 291. State = [[ 0.05223842 -0.25873366  0.37748063  1.        ]]. Action = [[-0.1684736   0.02925122  0.74423623  0.7311338 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 292. State = [[ 0.06173872 -0.24696404  0.38676143  1.        ]]. Action = [[0.65333223 0.6995975  0.51217484 0.6022402 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 293. State = [[ 0.06823915 -0.24024315  0.3871958   1.        ]]. Action = [[ 0.07497883 -0.3022678  -0.47094     0.8731468 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 294. State = [[ 0.07032288 -0.24036463  0.37901807  1.        ]]. Action = [[-0.21078229 -0.98143214  0.6902132   0.63213015]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 295. State = [[ 0.06752411 -0.24434571  0.38429603  1.        ]]. Action = [[-0.54474735 -0.10660529  0.24116242  0.7996669 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 296. State = [[ 0.0536814  -0.26250258  0.38162768  1.        ]]. Action = [[-0.708377   -0.68724746 -0.5639401   0.22906888]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 297. State = [[ 0.03776538 -0.26452485  0.35859013  1.        ]]. Action = [[ 0.0490855   0.8855605  -0.83281523  0.67015505]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 297 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 297 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[ 0.03489773 -0.25764745  0.34991875  1.        ]]. Action = [[ 0.45460427 -0.78709644  0.975301    0.84277534]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 299. State = [[ 0.0412737  -0.28201008  0.35715136  1.        ]]. Action = [[ 0.6622927  -0.77542937 -0.10973835  0.9676266 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 300. State = [[ 0.06132893 -0.3074249   0.3575516   1.        ]]. Action = [[ 0.99551415 -0.18667531 -0.7361844   0.60897946]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 300 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 300 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[ 0.06419907 -0.31028733  0.3587475   1.        ]]. Action = [[-0.6964387  -0.7837398  -0.05744231  0.7596848 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 301 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 301 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[ 0.06420178 -0.3102876   0.35867622  1.        ]]. Action = [[-0.68066335 -0.8627721   0.807359    0.7256776 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 303. State = [[ 0.06420178 -0.3102876   0.35867622  1.        ]]. Action = [[0.8590882 0.6659415 0.7358326 0.5363172]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 304. State = [[ 0.06420178 -0.3102876   0.35867622  1.        ]]. Action = [[ 0.92651105  0.38089108 -0.7342515   0.36714208]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 305. State = [[ 0.06190186 -0.31112042  0.37302426  1.        ]]. Action = [[-0.23877513  0.06314814  0.79277587  0.05219173]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 306. State = [[ 0.06006474 -0.30826133  0.37702665  1.        ]]. Action = [[ 0.06174803  0.1851964  -0.8753984   0.108477  ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 307. State = [[ 0.06163289 -0.3072007   0.36401975  1.        ]]. Action = [[ 0.47837996 -0.22398514  0.44839764  0.89078   ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 308. State = [[ 0.06181479 -0.3071884   0.36085704  1.        ]]. Action = [[-0.76881266  0.54860973  0.9529048  -0.34000647]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 309. State = [[ 0.06186351 -0.30719337  0.35965005  1.        ]]. Action = [[ 0.23452902 -0.15427417  0.3495171   0.78388155]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 310. State = [[ 0.06186913 -0.30719393  0.35950738  1.        ]]. Action = [[-0.9158825  -0.525523    0.08870816  0.14399016]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 311. State = [[ 0.06186913 -0.30719393  0.35950738  1.        ]]. Action = [[-0.8953769  -0.3555603   0.21319222  0.57330227]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 312. State = [[ 0.06186913 -0.30719393  0.35950738  1.        ]]. Action = [[ 0.8386047  -0.6411033  -0.8202002   0.85769737]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 313. State = [[ 0.06186913 -0.30719393  0.35950738  1.        ]]. Action = [[-0.5184346  -0.57339525  0.80959845  0.77479887]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 314. State = [[ 0.06186913 -0.30719393  0.35950738  1.        ]]. Action = [[ 0.73719287  0.18530929 -0.67344147  0.8654463 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 315. State = [[ 0.06186913 -0.30719393  0.35950738  1.        ]]. Action = [[ 0.48364997 -0.10891616 -0.03797162  0.5305731 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 316. State = [[ 0.06218172 -0.3070085   0.3594927   1.        ]]. Action = [[0.6914716  0.5531955  0.27168787 0.83455837]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 317. State = [[ 0.06235816 -0.30690315  0.3595031   1.        ]]. Action = [[ 0.5382626  -0.3586254   0.01826513  0.84393775]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 318. State = [[ 0.06843354 -0.2934891   0.34882587  1.        ]]. Action = [[ 0.36481965  0.63726914 -0.88569456  0.5036496 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 319. State = [[ 0.08844348 -0.26580593  0.33582813  1.        ]]. Action = [[0.28984356 0.8637643  0.59534705 0.828424  ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 320. State = [[ 0.09131689 -0.22888066  0.35446018  1.        ]]. Action = [[-0.7728865  0.6729634  0.5819284  0.9645109]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 321. State = [[ 0.07725272 -0.20586944  0.36974454  1.        ]]. Action = [[ 0.4655738   0.37514925 -0.27141786  0.791962  ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 322. State = [[ 0.07634749 -0.2037908   0.3738375   1.        ]]. Action = [[ 0.5052178  -0.04665703 -0.9664476   0.04298127]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 323. State = [[ 0.07623008 -0.2036195   0.3746886   1.        ]]. Action = [[ 0.8060777  -0.5655825  -0.900716    0.89715564]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 324. State = [[ 0.07623008 -0.2036195   0.3746886   1.        ]]. Action = [[ 0.31227505 -0.9338474   0.77064514  0.9402394 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 325. State = [[ 0.07623008 -0.2036195   0.3746886   1.        ]]. Action = [[ 0.5906458  -0.38127512 -0.17682165  0.65469384]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 326. State = [[ 0.06785826 -0.21652521  0.36193666  1.        ]]. Action = [[-0.24743682 -0.7698744  -0.94494665  0.6916239 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 327. State = [[ 0.06210813 -0.22020972  0.35194767  1.        ]]. Action = [[0.06608188 0.5599257  0.70957017 0.87043023]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 328. State = [[ 0.05505155 -0.21150042  0.34977704  1.        ]]. Action = [[-0.6618712   0.17444432 -0.74004817  0.6343181 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 329. State = [[ 0.03881481 -0.21818742  0.33567673  1.        ]]. Action = [[ 0.20940125 -0.8961777   0.08869684  0.39427996]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 329 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 329 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.25081778  0.0025043   0.23261943  1.        ]]. Action = [[ 0.7017422  -0.35646772 -0.5981433  -0.20952672]]. Reward = [-1.]
Curr episode timestep = 97
Current timestep = 331. State = [[-0.2552872   0.00195444  0.21906056  1.        ]]. Action = [[-0.94862473  0.45151722  0.05315828  0.88033736]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 332. State = [[-0.2492923  -0.0064656   0.20751625  1.        ]]. Action = [[ 0.6502595  -0.48930782 -0.4170428   0.5253866 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 333. State = [[-0.24142784 -0.01417335  0.19193546  1.        ]]. Action = [[-0.5499176   0.61237335 -0.04090846  0.74581003]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 334. State = [[-0.22732787 -0.02463891  0.1898375   1.        ]]. Action = [[ 0.6985948  -0.43126762 -0.04131818  0.8589809 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 335. State = [[-0.2098059  -0.04231694  0.17767216  1.        ]]. Action = [[ 0.25364172 -0.35885167 -0.916102    0.81485057]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 336. State = [[-0.20246658 -0.04122211  0.15284328  1.        ]]. Action = [[-0.4440108   0.711933    0.32843137  0.96617365]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 337. State = [[-0.2055535  -0.02974088  0.1528694   1.        ]]. Action = [[ 0.713151   -0.559616   -0.11520529  0.5944313 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 338. State = [[-0.20572738 -0.0288527   0.15287553  1.        ]]. Action = [[ 0.64369726  0.82110405  0.74504423 -0.01243198]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 339. State = [[-0.21081746 -0.03978766  0.14718872  1.        ]]. Action = [[-0.3009084  -0.6781458  -0.47683978  0.63248324]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 340. State = [[-0.22388598 -0.06871759  0.1453133   1.        ]]. Action = [[-0.66214424 -0.828124    0.60103905  0.656935  ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 341. State = [[-0.2346887  -0.07818434  0.16487357  1.        ]]. Action = [[0.62376785 0.71499157 0.624208   0.35124707]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 342. State = [[-0.23009309 -0.0703357   0.1791052   1.        ]]. Action = [[-0.6581734  -0.62937474  0.6993141  -0.40276653]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 343. State = [[-0.21483468 -0.05054943  0.18647397  1.        ]]. Action = [[0.969249   0.89748263 0.23175585 0.8995688 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 344. State = [[-0.18401623 -0.0130046   0.20257846  1.        ]]. Action = [[0.37229228 0.726261   0.26979828 0.66081786]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 345. State = [[-0.1704237   0.00697633  0.21294414  1.        ]]. Action = [[ 0.7223501 -0.7361525  0.2942629  0.6463462]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 346. State = [[-0.16709568  0.01011104  0.21517456  1.        ]]. Action = [[ 0.36893654 -0.21180356 -0.18163276  0.06954122]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 347. State = [[-0.16720815  0.01121624  0.21511742  1.        ]]. Action = [[ 0.46839845 -0.8893349  -0.63485193  0.67777956]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 348. State = [[-0.17542946  0.0122548   0.22427104  1.        ]]. Action = [[-0.92538154 -0.04290456  0.604403    0.14006424]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 349. State = [[-0.18093126  0.02023286  0.2427247   1.        ]]. Action = [[0.7207236  0.34659588 0.2382865  0.8545649 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 350. State = [[-0.17319784  0.03916087  0.26307794  1.        ]]. Action = [[-0.04633278  0.66322374  0.8225746   0.23110878]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 351. State = [[-0.17854981  0.06989045  0.28277713  1.        ]]. Action = [[-0.47130537  0.5271063  -0.29010838  0.6108365 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 352. State = [[-0.19508825  0.06881007  0.27233335  1.        ]]. Action = [[-0.6553634 -0.9168283 -0.7823761  0.7102587]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 353. State = [[-0.21062234  0.06547274  0.2643052   1.        ]]. Action = [[0.13654315 0.76474285 0.56064916 0.75478685]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 354. State = [[-0.212085    0.06715383  0.27007467  1.        ]]. Action = [[-0.02131546 -0.5986489   0.04128182  0.5428655 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 355. State = [[-0.19990389  0.04214263  0.27817574  1.        ]]. Action = [[ 0.90831804 -0.7296674   0.21190298  0.931005  ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 356. State = [[-0.17869033  0.00815694  0.27978906  1.        ]]. Action = [[ 0.4104154  -0.77697664 -0.38381064  0.8978846 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 357. State = [[-0.25088474  0.00250864  0.23264767  1.        ]]. Action = [[-0.63304275 -0.61026365 -0.02196407 -0.05373466]]. Reward = [-1.]
Curr episode timestep = 26
Current timestep = 358. State = [[-0.24119559  0.0160383   0.22870865  1.        ]]. Action = [[ 0.9305068   0.89348054 -0.29844618  0.2391175 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 359. State = [[-0.22161888  0.02396758  0.2136755   1.        ]]. Action = [[ 0.0722369  -0.47268105 -0.31622422  0.29694796]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 360. State = [[-0.20433821  0.01895165  0.19775327  1.        ]]. Action = [[ 0.81393313 -0.0235908  -0.64374423  0.81343126]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 361. State = [[-0.17191187  0.00135724  0.1906395   1.        ]]. Action = [[ 0.41009068 -0.84829706  0.9353728   0.82969785]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 362. State = [[-0.1581114  -0.01878867  0.20397575  1.        ]]. Action = [[ 0.8707     -0.11050111  0.31521273  0.56121457]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 363. State = [[-0.16066913 -0.0093554   0.21952653  1.        ]]. Action = [[-0.53881985  0.71916366  0.8246074   0.03721106]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 364. State = [[-0.16867916  0.00555459  0.24311392  1.        ]]. Action = [[ 0.8121996  -0.7904175  -0.66967744  0.7618568 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 365. State = [[-0.16728929  0.01184957  0.26120803  1.        ]]. Action = [[0.13902533 0.19113898 0.8475585  0.9478178 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 366. State = [[-0.15679531  0.01873511  0.28507632  1.        ]]. Action = [[ 0.6972642   0.18298483 -0.02111077  0.65724254]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 367. State = [[-0.1373496   0.01881741  0.28621814  1.        ]]. Action = [[ 0.3455813  -0.28543472 -0.40060163  0.76512206]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 368. State = [[-0.10884582  0.0058955   0.29643604  1.        ]]. Action = [[ 0.8881575  -0.5046065   0.96819973  0.8549087 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 369. State = [[-0.0779059  -0.01760285  0.31935748  1.        ]]. Action = [[ 0.7449169 -0.6302986 -0.2441765  0.8062048]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 370. State = [[-0.04516995 -0.052039    0.3075886   1.        ]]. Action = [[ 0.77567756 -0.8735851  -0.5323479   0.75905395]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 371. State = [[-0.02483362 -0.07723849  0.2908324   1.        ]]. Action = [[-0.92654884 -0.04693681 -0.05133682  0.7690084 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 372. State = [[-0.03651888 -0.09723873  0.29956377  1.        ]]. Action = [[-0.6209356  -0.49591613  0.49333894  0.57607746]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 373. State = [[-0.04273006 -0.09798592  0.31604597  1.        ]]. Action = [[0.8007097  0.58022916 0.38403356 0.6597642 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 374. State = [[-0.04325337 -0.0772111   0.33722264  1.        ]]. Action = [[-0.62575704  0.5427091   0.6740825   0.72528446]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 375. State = [[-0.04561639 -0.06963751  0.3599729   1.        ]]. Action = [[ 0.71232533 -0.4603377   0.10222924  0.9246168 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 376. State = [[-0.03878004 -0.07253125  0.36641547  1.        ]]. Action = [[0.36604846 0.730512   0.9765475  0.43767083]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 377. State = [[-0.03889328 -0.06851032  0.36736044  1.        ]]. Action = [[-0.09038401  0.3382864   0.10664177  0.34145868]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 378. State = [[-0.03831077 -0.06504643  0.36888653  1.        ]]. Action = [[-0.69568133 -0.73276174  0.9458947   0.91531897]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 379. State = [[-0.04336269 -0.0585907   0.35955516  1.        ]]. Action = [[-0.63168156  0.2511878  -0.8734257   0.86619186]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 380. State = [[-0.04947017 -0.04021768  0.35522953  1.        ]]. Action = [[0.06534004 0.659348   0.6706573  0.94076157]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 381. State = [[-0.0611565  -0.03200991  0.35365194  1.        ]]. Action = [[-0.78908587 -0.47944242 -0.65554637  0.18782187]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 382. State = [[-0.07994895 -0.04031107  0.32911965  1.        ]]. Action = [[-0.18466294 -0.12420905 -0.784801    0.4103781 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 383. State = [[-0.08917265 -0.03511216  0.31931603  1.        ]]. Action = [[0.16617322 0.48346555 0.8278966  0.8652636 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 384. State = [[-0.09824193 -0.00928314  0.31745616  1.        ]]. Action = [[-0.6187354  0.8458407 -0.7889801  0.8117075]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 385. State = [[-0.10205812  0.02894984  0.31718305  1.        ]]. Action = [[0.96554303 0.78225136 0.56973207 0.8856958 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 386. State = [[-0.09972076  0.05180606  0.32551804  1.        ]]. Action = [[-0.8678121   0.04978013  0.21285248  0.6723089 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 387. State = [[-0.11840007  0.06925119  0.3382018   1.        ]]. Action = [[-0.6544256   0.38944435  0.22268379  0.60050535]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 388. State = [[-0.13937984  0.07317973  0.35930705  1.        ]]. Action = [[-0.19629908 -0.28995502  0.7059853   0.7994716 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 389. State = [[-0.14281501  0.06326012  0.37526336  1.        ]]. Action = [[ 0.82421696 -0.23669428 -0.09464025  0.6638441 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 390. State = [[-0.13620812  0.05778446  0.37551606  1.        ]]. Action = [[-0.7943366  -0.55167764  0.95725274  0.77197087]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 391. State = [[-0.25092363  0.00265612  0.23267895  1.        ]]. Action = [[-0.44655454  0.62038016  0.2621249  -0.01507759]]. Reward = [-1.]
Curr episode timestep = 33
Current timestep = 392. State = [[-0.24368231  0.00690059  0.21858785  1.        ]]. Action = [[ 0.946723    0.43263316 -0.93641156  0.5789114 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 393. State = [[-0.22156213  0.01194118  0.18215662  1.        ]]. Action = [[-0.8709053   0.72317517 -0.7361095   0.7248019 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 394. State = [[-0.22740027  0.02854919  0.16745564  1.        ]]. Action = [[-0.6191254  0.7694986 -0.5372881  0.7483127]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 395. State = [[-0.23120719  0.03371819  0.15258743  1.        ]]. Action = [[ 0.442505   -0.73366994 -0.0264219   0.579015  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 396. State = [[-0.21655156  0.0102762   0.14472103  1.        ]]. Action = [[ 0.64975333 -0.68624705 -0.6934928   0.08704877]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 397. State = [[-0.20895855  0.00782851  0.13079453  1.        ]]. Action = [[-0.8217876   0.8804772   0.93724406  0.9301665 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 398. State = [[-0.23168586  0.03089186  0.13463706  1.        ]]. Action = [[-0.4828509  0.3477254 -0.7703921  0.9180014]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 399. State = [[-0.23369247  0.03109542  0.13354103  1.        ]]. Action = [[ 0.42825615 -0.52179205  0.7498703   0.7118139 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 400. State = [[-0.2294182   0.00380452  0.14208014  1.        ]]. Action = [[-0.08203053 -0.9464079  -0.19816023  0.7937021 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 401. State = [[-0.23913456 -0.00542469  0.14163487  1.        ]]. Action = [[-0.52582437  0.6232066  -0.00162482  0.8838804 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 402. State = [[-2.4994132e-01 -5.7918846e-04  1.3931003e-01  1.0000000e+00]]. Action = [[-0.52320254  0.09946966  0.86360264  0.8622116 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 403. State = [[-0.2419495   0.01742616  0.14064953  1.        ]]. Action = [[ 0.7989943   0.8913033  -0.04446149  0.7151092 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 404. State = [[-0.23075424  0.03374965  0.13563208  1.        ]]. Action = [[ 0.05579293 -0.28827262 -0.59168     0.38178658]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 405. State = [[-0.21312855  0.03751692  0.11606823  1.        ]]. Action = [[ 0.90161014  0.23294437 -0.42405784  0.6773522 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 406. State = [[-0.19015728  0.0340334   0.10515477  1.        ]]. Action = [[-0.21892387 -0.45886958  0.45394897  0.7436154 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 407. State = [[-0.19327375  0.01442209  0.10544538  1.        ]]. Action = [[-0.4522556  -0.6165322  -0.29916197  0.8204876 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 408. State = [[-0.20215625 -0.01896038  0.10518695  1.        ]]. Action = [[-0.09535259 -0.96338433  0.08151841  0.601501  ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 409. State = [[-0.20227091 -0.04371142  0.1055982   1.        ]]. Action = [[ 0.8450519  -0.4643613  -0.8190894   0.22724974]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 410. State = [[-0.21866196 -0.04354008  0.09714456  1.        ]]. Action = [[-0.9055803   0.32336998 -0.55321485  0.9413372 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 411. State = [[-0.24884203 -0.02988666  0.09251595  1.        ]]. Action = [[-0.3366369   0.59497905  0.5866635   0.8821392 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 412. State = [[-0.25459778 -0.02629429  0.09391183  1.        ]]. Action = [[ 0.57655907 -0.6133006  -0.5221725   0.74246335]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 413. State = [[-0.24357793 -0.05219297  0.09136245  1.        ]]. Action = [[ 0.3752153  -0.86178887  0.35569596  0.4702916 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 414. State = [[-0.2270844  -0.08905891  0.10407145  1.        ]]. Action = [[ 0.3554095  -0.7668235   0.73456264  0.95929646]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 415. State = [[-0.20561609 -0.1120919   0.11347335  1.        ]]. Action = [[ 0.7729888  -0.0386529  -0.95325696  0.90390015]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 416. State = [[-0.19276145 -0.11603506  0.09199946  1.        ]]. Action = [[-0.49623227  0.15066671 -0.09478611  0.68062615]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 417. State = [[-0.19557801 -0.11033438  0.08319128  1.        ]]. Action = [[ 0.09871161  0.1172992  -0.36339074  0.28609753]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 418. State = [[-0.25017974  0.00259777  0.2326817   1.        ]]. Action = [[-0.5519566  -0.8304399  -0.18931198 -0.24197698]]. Reward = [-1.]
Curr episode timestep = 26
Current timestep = 419. State = [[-0.25022358  0.00177581  0.2329636   1.        ]]. Action = [[-0.5679389  -0.21077824  0.02625763  0.25787735]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 420. State = [[-0.2432607   0.00667348  0.2350704   1.        ]]. Action = [[ 0.6216445   0.3376565  -0.09959859  0.5890956 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 421. State = [[-0.23778823  0.0120341   0.23278281  1.        ]]. Action = [[-0.1736759   0.05448508 -0.2766112   0.63974464]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 422. State = [[-0.22663009  0.00571374  0.23126853  1.        ]]. Action = [[ 0.7131165  -0.52436876  0.21864963  0.8750528 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 423. State = [[-0.20934832 -0.00479165  0.23622322  1.        ]]. Action = [[ 0.12455225 -0.18367702  0.27329326  0.6541736 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 424. State = [[-0.20744196  0.00622961  0.25385025  1.        ]]. Action = [[-0.4091164   0.89482784  0.891029    0.60395646]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 425. State = [[-0.25122598  0.00225554  0.23234278  1.        ]]. Action = [[-0.6527102   0.44812727 -0.10786003 -0.19151646]]. Reward = [-1.]
Curr episode timestep = 6
Current timestep = 426. State = [[-0.24023089  0.01440407  0.24277616  1.        ]]. Action = [[0.78922915 0.6613612  0.8630762  0.91132784]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 427. State = [[-0.23153041  0.04264418  0.26341042  1.        ]]. Action = [[-0.4511968   0.6501162   0.30380094  0.14361632]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 428. State = [[-0.2374631   0.06187393  0.27388492  1.        ]]. Action = [[-0.7849807   0.19491053 -0.57251215  0.94572854]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 429. State = [[-0.22346693  0.07235227  0.2859378   1.        ]]. Action = [[0.9050479  0.42365217 0.38093865 0.46649528]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 430. State = [[-0.21216094  0.07118923  0.29144672  1.        ]]. Action = [[-0.46504593 -0.54015976 -0.69900197  0.8096663 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 431. State = [[-0.20083545  0.04495781  0.2812332   1.        ]]. Action = [[ 0.9163761  -0.94474036  0.01534832  0.79959047]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 432. State = [[-0.1813483   0.00780678  0.26531288  1.        ]]. Action = [[ 0.26728344 -0.7491026  -0.8666159   0.7313976 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 433. State = [[-0.16977973 -0.01805005  0.24764597  1.        ]]. Action = [[-0.1991654  -0.22948813  0.6298784   0.5967634 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 434. State = [[-0.16707674 -0.0397695   0.25614148  1.        ]]. Action = [[ 0.18230677 -0.5860697   0.07125401  0.85370135]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 435. State = [[-0.15556213 -0.04458274  0.26929983  1.        ]]. Action = [[0.5834272  0.577561   0.45126987 0.2884233 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 436. State = [[-0.13195442 -0.04294606  0.29108799  1.        ]]. Action = [[ 0.5572512  -0.31695485  0.44616342  0.77484274]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 437. State = [[-0.11897127 -0.04584577  0.29578492  1.        ]]. Action = [[-0.2417559  -0.06687546 -0.6476297   0.89300585]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 438. State = [[-0.12854038 -0.05060821  0.29070967  1.        ]]. Action = [[-0.9517915  -0.05018032  0.38013542  0.13367724]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 439. State = [[-0.1559552  -0.05633274  0.2893376   1.        ]]. Action = [[-0.78691983 -0.07007486 -0.43525165  0.7794502 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 440. State = [[-0.18999071 -0.05116347  0.26826292  1.        ]]. Action = [[-0.5928096   0.41764688 -0.74371886  0.711921  ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 441. State = [[-0.21057627 -0.02826327  0.25279114  1.        ]]. Action = [[0.11679244 0.6787021  0.31012094 0.6763098 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 442. State = [[-0.20907724  0.00385745  0.25963166  1.        ]]. Action = [[0.4058001  0.71678305 0.43403506 0.93626595]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 443. State = [[-0.20098937  0.03626259  0.2576209   1.        ]]. Action = [[ 0.55651045  0.63240933 -0.80666834  0.9524324 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 444. State = [[-0.17700922  0.07220007  0.25337273  1.        ]]. Action = [[0.48415637 0.9239224  0.6242446  0.74589205]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 445. State = [[-0.16965006  0.09795657  0.25387797  1.        ]]. Action = [[-0.06617779  0.14031923 -0.80161625  0.33546245]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 446. State = [[-0.16396113  0.10655948  0.23388827  1.        ]]. Action = [[ 0.79569745  0.92131245 -0.5378701   0.72394586]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 447. State = [[-0.17011964  0.09355259  0.22752239  1.        ]]. Action = [[-0.61364156 -0.8313415  -0.3224851   0.79281604]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 448. State = [[-0.17794117  0.07980232  0.22168687  1.        ]]. Action = [[ 0.7219187   0.5529114  -0.15764874  0.88093424]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 449. State = [[-0.1798762   0.07746531  0.22046667  1.        ]]. Action = [[ 0.5514486   0.16778302 -0.24271947  0.8833463 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 450. State = [[-0.16840829  0.07455681  0.22515203  1.        ]]. Action = [[ 0.94273865 -0.06663674  0.31523657  0.6066351 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 451. State = [[-0.13978249  0.0821826   0.24204172  1.        ]]. Action = [[0.9282541  0.5880642  0.8038564  0.82864666]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 452. State = [[-0.11341934  0.1045714   0.2718173   1.        ]]. Action = [[-0.07935661  0.5897374   0.5144768   0.3324989 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 453. State = [[-0.09757108  0.12245239  0.28519022  1.        ]]. Action = [[ 0.84686923  0.19479477 -0.2133739   0.30849838]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 454. State = [[-0.06541102  0.12280854  0.2814074   1.        ]]. Action = [[ 0.5884907  -0.42352754 -0.00982231  0.82726884]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 455. State = [[-0.03538795  0.11286901  0.29266414  1.        ]]. Action = [[ 0.65185    -0.27665794  0.5990828   0.4816724 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 456. State = [[-0.01858547  0.12293041  0.3139179   1.        ]]. Action = [[-0.36496472  0.83847785  0.26504087  0.70690966]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 457. State = [[-0.01761927  0.13593072  0.3186822   1.        ]]. Action = [[ 0.85811853  0.0134722  -0.67804843  0.8555472 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 457 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 457 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[0.01438921 0.1621819  0.27955002 1.        ]]. Action = [[-0.35401392  0.87543535 -0.7762208   0.78522325]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 458 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 458 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[0.02182963 0.18059191 0.2593214  1.        ]]. Action = [[ 0.9097643  -0.22605693  0.23376274  0.86924124]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 459 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 459 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[0.05283266 0.19398734 0.2751596  1.        ]]. Action = [[0.48605156 0.8655914  0.65394616 0.6682215 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 460 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 460 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[0.0734462  0.2225401  0.29523948 1.        ]]. Action = [[ 0.8495859  -0.5259099   0.65768147  0.7212477 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 461 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 461 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[0.06694913 0.23094586 0.28603092 1.        ]]. Action = [[-0.96351737  0.19174385 -0.70451057  0.7927581 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 462 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 462 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[0.05674589 0.24202608 0.29352534 1.        ]]. Action = [[-0.40299064 -0.00514591  0.8625779   0.79804754]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 464. State = [[0.04247448 0.25590923 0.32472375 1.        ]]. Action = [[0.19553089 0.6834247  0.8437984  0.93378663]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 465. State = [[0.03473547 0.26605862 0.34082577 1.        ]]. Action = [[-0.9005154  0.7789619  0.8024492  0.8252282]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 466. State = [[0.03611881 0.2541091  0.36175644 1.        ]]. Action = [[-0.2334544 -0.8395039  0.6920681  0.9646132]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 467. State = [[0.03557885 0.225796   0.39260012 1.        ]]. Action = [[ 0.20045257 -0.75594425  0.33505     0.7118335 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 468. State = [[0.03665756 0.20617376 0.4035579  1.        ]]. Action = [[0.2093035  0.22921395 0.52835476 0.48980737]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 469. State = [[0.03660646 0.20101917 0.40596652 1.        ]]. Action = [[-0.33097553  0.18183672  0.958102    0.5265136 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 470. State = [[0.03666202 0.19970441 0.40627146 1.        ]]. Action = [[-0.50535667  0.8008113   0.5936675   0.6520994 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 471. State = [[0.03345941 0.19955622 0.3916126  1.        ]]. Action = [[ 0.03819358  0.19953787 -0.9131718   0.19523704]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 472. State = [[0.03204212 0.20265687 0.369872   1.        ]]. Action = [[-0.42857748 -0.0758999  -0.2135554   0.8670417 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 473. State = [[0.02805266 0.1974958  0.34969676 1.        ]]. Action = [[ 0.13096261 -0.24064153 -0.6368812   0.8168385 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 474. State = [[0.03064548 0.20733108 0.3305313  1.        ]]. Action = [[0.4488393  0.9459672  0.12189043 0.9286144 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 475. State = [[0.0345424  0.2328753  0.31250593 1.        ]]. Action = [[ 0.90149856  0.69643307 -0.8362999   0.5638331 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 476. State = [[0.05674471 0.23916395 0.2968312  1.        ]]. Action = [[ 0.00577402 -0.90938824  0.89625275  0.7586278 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 477. State = [[0.06578675 0.22576728 0.3128712  1.        ]]. Action = [[ 0.99101305 -0.2014184  -0.82134265  0.70419335]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 478. State = [[0.06718767 0.20737563 0.30856848 1.        ]]. Action = [[-0.22290558 -0.9731134  -0.37571537  0.36315703]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 479. State = [[0.07286134 0.18601628 0.2969893  1.        ]]. Action = [[ 0.44227934  0.15364456 -0.65626794  0.53932416]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 480. State = [[0.07815504 0.16734295 0.2891826  1.        ]]. Action = [[-0.52693635 -0.959296    0.6800935   0.684412  ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 481. State = [[0.07322354 0.13502394 0.29037547 1.        ]]. Action = [[-0.16426861 -0.5645152  -0.77261055  0.8541434 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 482. State = [[0.07039585 0.11799581 0.27516744 1.        ]]. Action = [[ 0.8389845  -0.48788255 -0.23372614  0.86305666]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 482 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 482 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of 1
Current timestep = 483. State = [[0.06911615 0.11338063 0.27141842 1.        ]]. Action = [[ 0.6693735  -0.21122026 -0.89837074  0.7717465 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Action ignored: No entry zone
Scene graph at timestep 483 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 483 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 0
Current timestep = 484. State = [[0.06932489 0.11318827 0.27147174 1.        ]]. Action = [[ 0.94985557 -0.69696754 -0.6040703   0.68507576]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 484 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 484 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of 0
Current timestep = 485. State = [[0.0692997  0.11314552 0.27143094 1.        ]]. Action = [[ 0.78178096 -0.34021866  0.5020186   0.3804065 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 486. State = [[0.06609401 0.11959821 0.26931402 1.        ]]. Action = [[-0.29556715  0.35671592 -0.23574698  0.90435123]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 487. State = [[0.06125636 0.14033282 0.25854117 1.        ]]. Action = [[ 0.33165574  0.89864135 -0.43824267  0.78290725]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 487 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 487 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[0.05546663 0.16703041 0.23917373 1.        ]]. Action = [[-0.2679515   0.15400362 -0.05661476  0.21209323]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 489. State = [[0.0472796  0.17709452 0.25431123 1.        ]]. Action = [[-0.929963    0.07291234  0.7866833   0.3178197 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 490. State = [[0.02357175 0.18314508 0.26986334 1.        ]]. Action = [[0.9078064  0.5334083  0.8512752  0.79408455]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 491. State = [[0.02009066 0.19200106 0.26209965 1.        ]]. Action = [[ 0.6131916   0.6009791  -0.78263026  0.78301585]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 492. State = [[0.03251596 0.20624325 0.24610348 1.        ]]. Action = [[0.69089186 0.13777268 0.5020888  0.733058  ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 493. State = [[0.05002141 0.20677665 0.24773557 1.        ]]. Action = [[ 0.80487466 -0.2894631  -0.05875397  0.9321934 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 494. State = [[0.07887322 0.18086961 0.25963315 1.        ]]. Action = [[-0.25376606 -0.96976805  0.79638934  0.46943116]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 495. State = [[0.08355165 0.16076419 0.27972803 1.        ]]. Action = [[ 0.77169335 -0.62275714 -0.965375    0.66849923]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 496. State = [[0.08226787 0.1539813  0.29467845 1.        ]]. Action = [[-0.5801456  -0.18859929  0.350394    0.7894263 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 497. State = [[0.07115443 0.16277757 0.32492715 1.        ]]. Action = [[-0.6906507   0.8399956   0.92295766  0.5194123 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 498. State = [[0.04490671 0.18458255 0.34952664 1.        ]]. Action = [[ 0.91105664 -0.24708462  0.8388982   0.58905363]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 499. State = [[0.04297526 0.18636984 0.3501788  1.        ]]. Action = [[ 0.35738742 -0.04987508 -0.2519558   0.7915909 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 500. State = [[0.04458176 0.18935733 0.3494232  1.        ]]. Action = [[0.25417197 0.21791565 0.26210093 0.45570612]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 501. State = [[0.04998424 0.17924248 0.36669582 1.        ]]. Action = [[ 0.43136585 -0.7894669   0.9331136   0.65469503]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 502. State = [[0.06204726 0.15805791 0.38761207 1.        ]]. Action = [[ 0.03996527 -0.1667934   0.85121775  0.90758085]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 503. State = [[0.0642939  0.15555693 0.3898626  1.        ]]. Action = [[-0.8301765  -0.11220384  0.7318809   0.5743182 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 504. State = [[0.05930348 0.16444393 0.39514148 1.        ]]. Action = [[-0.6046995   0.4282503   0.05169225  0.5377848 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 505. State = [[-0.2507064   0.00260019  0.23268078  1.        ]]. Action = [[ 0.5590205   0.40689838 -0.64873415 -0.06612206]]. Reward = [-1.]
Curr episode timestep = 79
Current timestep = 506. State = [[-0.24017867 -0.01265351  0.21676916  1.        ]]. Action = [[ 0.97366977 -0.87280893 -0.09912622  0.9476261 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 507. State = [[-0.22479607 -0.0244421   0.21103725  1.        ]]. Action = [[-0.24738479  0.2188437   0.4936862   0.7235385 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 508. State = [[-0.22474964 -0.02706338  0.20817094  1.        ]]. Action = [[ 0.13607001 -0.18352878 -0.6850065   0.7184808 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 509. State = [[-0.23141353 -0.02959743  0.19917099  1.        ]]. Action = [[-0.60862976  0.05677211  0.28416383  0.7657106 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 510. State = [[-0.22719763 -0.02039834  0.20387144  1.        ]]. Action = [[0.81908464 0.59299016 0.09322965 0.5465281 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 511. State = [[-0.21755962 -0.00925666  0.21174978  1.        ]]. Action = [[-0.02320576  0.01145732  0.44760966  0.8378452 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 512. State = [[-0.20770203  0.00457017  0.23327142  1.        ]]. Action = [[0.3234123  0.50975597 0.72802305 0.70843256]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 513. State = [[-0.20389211  0.00677723  0.26456124  1.        ]]. Action = [[-0.5579235  -0.580317    0.55740273  0.89899683]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 514. State = [[-0.21233426  0.00174699  0.2954451   1.        ]]. Action = [[-0.0360167   0.20333719  0.6367192   0.8240346 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 515. State = [[-0.20821814  0.01595514  0.32123458  1.        ]]. Action = [[0.66742563 0.6439624  0.28123498 0.685246  ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 516. State = [[-0.18419768  0.02602251  0.34849316  1.        ]]. Action = [[ 0.66202474 -0.27349424  0.6185026   0.92658305]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 517. State = [[-0.15843357  0.01790365  0.35724634  1.        ]]. Action = [[ 0.94067323 -0.269323   -0.98056495  0.4385935 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 518. State = [[-0.10984495  0.01869675  0.33539912  1.        ]]. Action = [[0.92191267 0.34369886 0.2061708  0.77764773]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 519. State = [[-0.07608549  0.0333141   0.32345772  1.        ]]. Action = [[ 0.4232267  0.4813969 -0.6382103  0.911847 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 520. State = [[-0.04239389  0.03501164  0.32074383  1.        ]]. Action = [[ 0.8643398  -0.48075438  0.73497427  0.80825996]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 521. State = [[-0.02627436  0.04095211  0.33923396  1.        ]]. Action = [[-0.7139521   0.5323529   0.08753967  0.63073087]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 522. State = [[-0.02989548  0.03182251  0.36027673  1.        ]]. Action = [[-0.10367161 -0.9018888   0.9485502   0.6515875 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 523. State = [[-0.04427019  0.03077949  0.3937189   1.        ]]. Action = [[-0.5834701   0.84858775  0.37514257  0.3956436 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 524. State = [[-0.05758918  0.04828177  0.40799284  1.        ]]. Action = [[-0.21357775  0.1625297   0.8998015  -0.06368965]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 525. State = [[-0.05896809  0.05076973  0.40837833  1.        ]]. Action = [[0.44856274 0.6886066  0.85462534 0.75851583]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 526. State = [[-0.0562282   0.03598686  0.40110365  1.        ]]. Action = [[ 0.42644703 -0.7354195  -0.5411249   0.8101648 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 527. State = [[-0.05517559  0.02526995  0.39223436  1.        ]]. Action = [[-0.4531331   0.14269078 -0.1393084   0.43006873]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 528. State = [[-0.06198124  0.02325751  0.3932632   1.        ]]. Action = [[-0.4959433 -0.0847165  0.0839169  0.5182706]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 529. State = [[-0.07156902  0.02105591  0.3951136   1.        ]]. Action = [[-0.90993315  0.7391825   0.45131946  0.68348193]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 530. State = [[-0.073061    0.02017631  0.3953554   1.        ]]. Action = [[ 0.9485612  -0.83940357  0.7353041   0.53722286]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 531. State = [[-0.07314112  0.02026319  0.3954702   1.        ]]. Action = [[0.56337476 0.33693516 0.9310334  0.9139912 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 532. State = [[-0.07314112  0.02026319  0.3954702   1.        ]]. Action = [[ 0.8662534 -0.6720838  0.6856731  0.8595276]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 533. State = [[-0.07314112  0.02026319  0.3954702   1.        ]]. Action = [[0.8084686 0.6169007 0.7174468 0.6580504]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 534. State = [[-0.07169943  0.03042768  0.38831288  1.        ]]. Action = [[ 0.50584483  0.523227   -0.27020562  0.95641685]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 535. State = [[-0.07005026  0.04804606  0.37785298  1.        ]]. Action = [[-0.01651943  0.4086684  -0.12363791  0.10012066]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 536. State = [[-0.07321445  0.0438292   0.37110597  1.        ]]. Action = [[-0.4811607 -0.831386  -0.3033563  0.6817739]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 537. State = [[-0.08215925  0.03076586  0.34775686  1.        ]]. Action = [[-0.03724325  0.01976252 -0.9436827   0.9285207 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 538. State = [[-0.08161488  0.00922686  0.3162265   1.        ]]. Action = [[ 0.1829288  -0.9312063  -0.22584784  0.6964793 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 539. State = [[-0.08290313 -0.02581181  0.31527582  1.        ]]. Action = [[-0.5830654  -0.715669    0.62362456  0.81987333]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 540. State = [[-0.09035452 -0.06321777  0.32490954  1.        ]]. Action = [[ 0.6459894  -0.74601704 -0.21204603  0.74863386]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 541. State = [[-0.07553563 -0.07571452  0.315655    1.        ]]. Action = [[ 0.7529249   0.46687388 -0.23810107  0.84525836]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 542. State = [[-0.06234575 -0.07625299  0.30404252  1.        ]]. Action = [[-0.5404092  -0.2616285  -0.26685888  0.80940294]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 543. State = [[-0.07509909 -0.06785882  0.30361924  1.        ]]. Action = [[-0.89718723  0.66300654  0.27904236  0.7499113 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 544. State = [[-0.1030442  -0.06685179  0.31579858  1.        ]]. Action = [[-0.6839708  -0.51557326  0.3981501   0.61535597]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 545. State = [[-0.12128601 -0.0672321   0.3189196   1.        ]]. Action = [[ 0.48682785  0.45754004 -0.48329937  0.82135475]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 546. State = [[-0.11428156 -0.04478187  0.3215417   1.        ]]. Action = [[0.2666198  0.757439   0.87807965 0.73602796]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 547. State = [[-0.10094053 -0.03523439  0.3321986   1.        ]]. Action = [[ 0.93951094 -0.6290949  -0.11628741  0.46513522]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 548. State = [[-0.0848298  -0.03788541  0.3232121   1.        ]]. Action = [[-0.24963379  0.34056318 -0.5836769   0.9303571 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 549. State = [[-0.09379375 -0.05121309  0.31655854  1.        ]]. Action = [[-0.8620593  -0.8117942   0.12729967  0.87615824]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 550. State = [[-0.09683795 -0.06785794  0.33304632  1.        ]]. Action = [[ 0.677932   -0.03752196  0.98792386  0.72742224]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 551. State = [[-0.25088474  0.00250864  0.23264767  1.        ]]. Action = [[ 0.773916    0.79658484  0.8850825  -0.09244537]]. Reward = [-1.]
Curr episode timestep = 45
Current timestep = 552. State = [[-0.24371378  0.0125799   0.23738147  1.        ]]. Action = [[0.76300395 0.5275644  0.5617132  0.3781911 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 553. State = [[-0.23286146  0.02316708  0.24576226  1.        ]]. Action = [[-0.8799912  -0.22343671  0.93471396  0.7514212 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 554. State = [[-0.23220499  0.02402363  0.24629156  1.        ]]. Action = [[-0.6347244   0.24833024  0.82253885  0.5669576 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 555. State = [[-0.23205018  0.0241044   0.24637581  1.        ]]. Action = [[-0.9666105  -0.87115186  0.22662425  0.94830275]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 556. State = [[-0.23446666  0.0117007   0.24591944  1.        ]]. Action = [[-0.40735734 -0.7102254   0.00728977  0.90913725]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 557. State = [[-0.24081682 -0.01668471  0.25886402  1.        ]]. Action = [[-0.38098395 -0.73959386  0.8363999   0.59192777]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 558. State = [[-0.25918353 -0.03331744  0.27450475  1.        ]]. Action = [[-0.47755992  0.19406319 -0.57804215  0.69487786]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 559. State = [[-0.27016932 -0.03268417  0.25798732  1.        ]]. Action = [[ 0.2182684   0.05320323 -0.69912434  0.6231488 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 560. State = [[-0.25844023 -0.04920185  0.25532922  1.        ]]. Action = [[ 0.67552423 -0.9478095   0.9527379   0.9518695 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 561. State = [[-0.24910027 -0.08640621  0.27287525  1.        ]]. Action = [[-0.10903972 -0.79693264  0.28231394  0.6819823 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 562. State = [[-0.25061136 -0.10911179  0.28180233  1.        ]]. Action = [[-0.7869768  -0.6204772  -0.10998273  0.5401263 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 563. State = [[-0.24253598 -0.09783941  0.2900146   1.        ]]. Action = [[0.46595    0.8173878  0.44770873 0.6486305 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 564. State = [[-0.22212362 -0.07847672  0.3029695   1.        ]]. Action = [[ 0.5335133   0.20930588 -0.17966396  0.8956996 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 565. State = [[-0.20476909 -0.06383464  0.30365777  1.        ]]. Action = [[ 0.25794923  0.37230825 -0.01012373  0.799644  ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 566. State = [[-0.19875781 -0.06656805  0.29414988  1.        ]]. Action = [[-0.21836811 -0.6616124  -0.6990989   0.38925958]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 567. State = [[-0.18521908 -0.07305704  0.28181374  1.        ]]. Action = [[0.88134694 0.09182847 0.06685448 0.80783606]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 568. State = [[-0.17282502 -0.08628275  0.29191524  1.        ]]. Action = [[-0.8741166  -0.54994196  0.9547787   0.4658295 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 569. State = [[-0.17779776 -0.08895051  0.32988298  1.        ]]. Action = [[0.48020768 0.55644536 0.9050205  0.77584636]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 570. State = [[-0.17688127 -0.09015585  0.35118684  1.        ]]. Action = [[-0.3692472 -0.5219805 -0.3825202  0.8595654]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 571. State = [[-0.18608652 -0.11445659  0.36348864  1.        ]]. Action = [[-0.14169037 -0.6990882   0.7554848   0.13378918]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 572. State = [[-0.18289047 -0.1320934   0.39164203  1.        ]]. Action = [[ 0.77374697 -0.13845557  0.74309254  0.752162  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 572 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 572 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.16469942 -0.1386051   0.42376783  1.        ]]. Action = [[ 0.8793514  -0.32980466  0.62570226  0.86614275]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 573 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 573 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.16472428 -0.1385931   0.4237664   1.        ]]. Action = [[ 0.98501825 -0.62990075  0.9292786   0.6607156 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 574 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 574 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.16472428 -0.1385931   0.4237664   1.        ]]. Action = [[ 0.82464206 -0.60740817 -0.0486086   0.45307326]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 575 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 575 is tensor(0.0159, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of -1
Current timestep = 576. State = [[-0.16472428 -0.1385931   0.4237664   1.        ]]. Action = [[ 0.4263028 -0.4201473  0.7663872  0.8867456]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 576 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 576 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.1633238 -0.1247907  0.4121798  1.       ]]. Action = [[-0.03542161  0.81268394 -0.9185746   0.7164326 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 578. State = [[-0.15919267 -0.11037689  0.3924292   1.        ]]. Action = [[ 0.8401207  -0.26843357  0.3581239   0.28954268]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 579. State = [[-0.16154292 -0.10974338  0.39289442  1.        ]]. Action = [[-0.49791443 -0.12815076  0.04431951  0.7333337 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 580. State = [[-0.17450912 -0.10291691  0.3844004   1.        ]]. Action = [[-0.5978927   0.44818878 -0.50734603  0.96850514]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 581. State = [[-0.19446898 -0.09182394  0.36901733  1.        ]]. Action = [[0.5700381  0.29173195 0.7017554  0.2337321 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 582. State = [[-0.20988192 -0.08194355  0.35636073  1.        ]]. Action = [[-0.63138044  0.360062   -0.5383964   0.1581825 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 583. State = [[-0.21429698 -0.0672802   0.33814517  1.        ]]. Action = [[ 0.96432257  0.2866645  -0.15777314  0.7073604 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 584. State = [[-0.2129708  -0.04686801  0.31740892  1.        ]]. Action = [[-0.74519366  0.6869732  -0.7828131   0.35776675]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 585. State = [[-0.21022901 -0.03362951  0.31255394  1.        ]]. Action = [[ 0.96679854 -0.31190634  0.7340212   0.86223054]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 586. State = [[-0.19933315 -0.04821566  0.3199124   1.        ]]. Action = [[-0.09362948 -0.74625504 -0.16598964  0.8985429 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 587. State = [[-0.19997005 -0.06577522  0.3237972   1.        ]]. Action = [[-0.28023046  0.04903054  0.6006479   0.5974759 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 588. State = [[-0.25084504  0.00260082  0.23268087  1.        ]]. Action = [[ 0.19314432  0.37854075  0.7083924  -0.27835947]]. Reward = [-1.]
Curr episode timestep = 36
Current timestep = 589. State = [[-0.25319842  0.00204464  0.23446535  1.        ]]. Action = [[-0.2039777  -0.02468646  0.6778939   0.7578802 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 590. State = [[-0.24850388  0.00523167  0.2390814   1.        ]]. Action = [[ 0.76084185  0.2477014  -0.43908256  0.7652781 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 591. State = [[-0.23620681  0.01838436  0.23345217  1.        ]]. Action = [[0.15119779 0.4765606  0.03950286 0.78822017]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 592. State = [[-0.23077235  0.02998947  0.23222603  1.        ]]. Action = [[-0.62540597  0.79004955 -0.1142866   0.50046515]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 593. State = [[-0.22185986  0.03439309  0.2463523   1.        ]]. Action = [[0.3792919  0.15550256 0.9783169  0.83443785]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 594. State = [[-0.20945214  0.03042377  0.26153046  1.        ]]. Action = [[-0.03809547 -0.535465   -0.40240085  0.85224676]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 595. State = [[-0.19404761  0.03456119  0.2739885   1.        ]]. Action = [[0.99903274 0.7023356  0.75396955 0.6355339 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 596. State = [[-0.15553409  0.04489024  0.30900595  1.        ]]. Action = [[ 0.7693386  -0.12103307  0.83706415  0.87500525]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 597. State = [[-0.11563065  0.04096064  0.35246333  1.        ]]. Action = [[ 0.9871465  -0.25707018  0.9677645   0.7815803 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 598. State = [[-0.0936401   0.03032463  0.39714977  1.        ]]. Action = [[-0.9310198 -0.3619851  0.8361988  0.8674998]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 599. State = [[-0.11177904  0.00271743  0.4118765   1.        ]]. Action = [[-0.9501652  -0.8503775  -0.936942    0.61080337]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 600. State = [[-0.1450605  -0.00704656  0.3900407   1.        ]]. Action = [[-0.7409433   0.4815123  -0.5183186   0.50472355]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 601. State = [[-0.17354517 -0.01486489  0.37313905  1.        ]]. Action = [[-0.26937777 -0.6980668   0.02412975  0.09204328]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 602. State = [[-0.1892345  -0.04654147  0.37856945  1.        ]]. Action = [[-0.34110528 -0.7808619   0.22492051  0.7116505 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 603. State = [[-0.20472243 -0.06894604  0.38235468  1.        ]]. Action = [[-0.7852768  -0.39865613  0.95882034  0.8071078 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 604. State = [[-0.20622623 -0.07239754  0.38301766  1.        ]]. Action = [[-0.5635934  -0.6889443   0.82237494  0.42036855]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 605. State = [[-0.21286549 -0.07548126  0.37678862  1.        ]]. Action = [[-0.35791433 -0.11367619 -0.42692685  0.5762956 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 606. State = [[-0.21755227 -0.09500658  0.35462207  1.        ]]. Action = [[ 0.6084044  -0.8015181  -0.73678786  0.6661159 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 607. State = [[-0.2186096  -0.11797311  0.3232619   1.        ]]. Action = [[-0.79199284 -0.18247515 -0.7924259   0.80003583]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 608. State = [[-0.24295688 -0.11269783  0.2956318   1.        ]]. Action = [[-0.21782923  0.65295196  0.19338489  0.8341284 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 609. State = [[-0.24964736 -0.11717626  0.2973493   1.        ]]. Action = [[-0.13951218 -0.7019906   0.09916353  0.8223674 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 610. State = [[-0.2540226  -0.14075649  0.3036609   1.        ]]. Action = [[ 0.20091319 -0.5693777   0.10216308  0.8000126 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 610 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 610 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.25143105 -0.17637378  0.301834    1.        ]]. Action = [[ 0.40185857 -0.9519742  -0.91280377  0.79698086]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 611 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 611 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.24264568 -0.21653698  0.25662678  1.        ]]. Action = [[-0.06155461 -0.45765018 -0.62130237  0.7163689 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 612 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 612 is tensor(0.0243, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.24104902 -0.22924642  0.24778809  1.        ]]. Action = [[-0.26572645  0.28507662  0.9595916   0.6787442 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 613 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 613 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of 0
Current timestep = 614. State = [[-0.24614608 -0.2239509   0.2760683   1.        ]]. Action = [[0.06975877 0.00201106 0.66511476 0.5393077 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 614 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 614 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of 0
Current timestep = 615. State = [[-0.25424448 -0.21068391  0.30641556  1.        ]]. Action = [[-0.23751688  0.501629    0.9594147   0.89732146]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 615 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 615 is tensor(0.0170, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of 0
Current timestep = 616. State = [[-0.26021564 -0.18889183  0.32975432  1.        ]]. Action = [[ 0.77905583  0.20235372 -0.86525327  0.52708125]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 617. State = [[-0.23259483 -0.17340392  0.3222934   1.        ]]. Action = [[0.30683565 0.594543   0.53125334 0.7989831 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 618. State = [[-0.21530914 -0.16476417  0.33844417  1.        ]]. Action = [[ 0.4077283  -0.35957128  0.5972583   0.3306569 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 619. State = [[-0.19237882 -0.17166047  0.35180512  1.        ]]. Action = [[ 0.8682581  -0.24437535 -0.24291861  0.57344556]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 620. State = [[-0.1517646  -0.1633906   0.35158536  1.        ]]. Action = [[0.86003137 0.7583082  0.17867792 0.68235624]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 621. State = [[-0.11336745 -0.1515615   0.37444255  1.        ]]. Action = [[ 0.609975   -0.14282805  0.9600079   0.9122236 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 622. State = [[-0.10458886 -0.16118877  0.4104391   1.        ]]. Action = [[-0.5872243  -0.4888299   0.62250614  0.7879213 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 623. State = [[-0.11101338 -0.1728009   0.43012363  1.        ]]. Action = [[-0.5395565  -0.18918884  0.7192445   0.88525033]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 624. State = [[-0.11195973 -0.1746012   0.43192837  1.        ]]. Action = [[-0.09549618 -0.45941788  0.14119327  0.9406276 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 625. State = [[-0.11253406 -0.1750099   0.43228716  1.        ]]. Action = [[-0.31430244  0.7939935  -0.07878053  0.67044425]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 626. State = [[-0.11273058 -0.17529204  0.43245393  1.        ]]. Action = [[-0.88586736  0.0791868   0.8139169   0.33704185]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 627. State = [[-0.11273058 -0.17529204  0.43245393  1.        ]]. Action = [[ 0.8145046  -0.58686     0.40173995  0.81282353]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 628. State = [[-0.11333138 -0.16716382  0.4258038   1.        ]]. Action = [[-0.2551905   0.47257423 -0.6789066   0.67189634]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 629. State = [[-0.12279198 -0.17184076  0.40530363  1.        ]]. Action = [[-0.4663365  -0.7057991  -0.9525392   0.94098413]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 630. State = [[-0.13746078 -0.18273939  0.37856618  1.        ]]. Action = [[0.09598207 0.6867404  0.680606   0.6012285 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 631. State = [[-0.13888308 -0.18538463  0.37310374  1.        ]]. Action = [[ 0.12296224 -0.06301451 -0.14170271  0.7150439 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 632. State = [[-0.14351885 -0.20060332  0.37705696  1.        ]]. Action = [[-0.4361434  -0.6330906   0.4470868   0.47945476]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 633. State = [[-0.15290405 -0.21318832  0.38225082  1.        ]]. Action = [[-0.84462273 -0.8954279   0.95516825  0.8575454 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 634. State = [[-0.16841328 -0.22756442  0.38816404  1.        ]]. Action = [[-0.674012   -0.49233937  0.23949945  0.6253272 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 635. State = [[-0.18041623 -0.23988248  0.39539346  1.        ]]. Action = [[0.08431292 0.7841618  0.73180795 0.8402741 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 636. State = [[-0.1754714  -0.23496188  0.38390663  1.        ]]. Action = [[ 0.6017389   0.2889397  -0.92329276  0.6094953 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 637. State = [[-0.16311741 -0.23072629  0.36271164  1.        ]]. Action = [[-0.24866539  0.8277018   0.863672    0.4926628 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 638. State = [[-0.14960767 -0.21825524  0.3478284   1.        ]]. Action = [[ 0.7600628   0.5848484  -0.5442879   0.60642076]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 639. State = [[-0.12027282 -0.1986539   0.33708644  1.        ]]. Action = [[0.63947225 0.4833622  0.5520191  0.3319956 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 640. State = [[-0.11235014 -0.17441706  0.33403596  1.        ]]. Action = [[-0.69318026  0.608534   -0.65976065  0.67985153]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 641. State = [[-0.1305112  -0.16096453  0.31105137  1.        ]]. Action = [[-0.6277717  -0.03842604 -0.97967243  0.37461364]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 642. State = [[-0.15732616 -0.1666329   0.27924916  1.        ]]. Action = [[-0.63134354 -0.4282248  -0.09725118  0.63076663]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 643. State = [[-0.17161793 -0.18598911  0.28337476  1.        ]]. Action = [[ 0.1752634  -0.6674616   0.8441076   0.64256716]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 644. State = [[-0.17537785 -0.2034511   0.3112897   1.        ]]. Action = [[ 0.13817799 -0.19114643  0.8411523   0.7168801 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 645. State = [[-0.18427716 -0.21116084  0.34668896  1.        ]]. Action = [[-0.45762193  0.04773211  0.9413043   0.7429998 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 646. State = [[-0.20728047 -0.22868417  0.37171215  1.        ]]. Action = [[-0.6485379  -0.82976884 -0.42172283  0.11404502]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 647. State = [[-0.22193341 -0.23535772  0.379179    1.        ]]. Action = [[-0.05240959  0.610703    0.6432556   0.71618307]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 648. State = [[-0.21742119 -0.21266024  0.37956822  1.        ]]. Action = [[ 0.77020776  0.6603559  -0.82260853  0.5911958 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 649. State = [[-0.20389272 -0.19787602  0.36550164  1.        ]]. Action = [[-0.35626876  0.62874174  0.69655144  0.55407786]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 650. State = [[-0.20653066 -0.1805556   0.3539921   1.        ]]. Action = [[-0.5329361   0.82290626 -0.73027825  0.914516  ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 651. State = [[-0.22864893 -0.1774808   0.3334801   1.        ]]. Action = [[-0.7059613  -0.8617191  -0.58527887  0.86238503]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 652. State = [[-0.25743267 -0.17545635  0.30046317  1.        ]]. Action = [[-0.54341084  0.8443587  -0.9709926   0.51109576]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 653. State = [[-0.26369202 -0.15206045  0.26696733  1.        ]]. Action = [[2.7339411e-01 5.6954813e-01 7.4982643e-05 6.8270493e-01]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 654. State = [[-0.26207808 -0.13780619  0.25930715  1.        ]]. Action = [[-0.35832167  0.7340834   0.86119604  0.6684003 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 655. State = [[-0.25922588 -0.12121812  0.26576874  1.        ]]. Action = [[-0.03005785  0.5807817   0.9042729   0.49237537]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 656. State = [[-0.25537738 -0.09044361  0.28287652  1.        ]]. Action = [[0.5702646  0.596123   0.00264108 0.6647403 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 657. State = [[-0.2342951  -0.06031181  0.2877487   1.        ]]. Action = [[0.8761282  0.67886853 0.06752729 0.25238633]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 658. State = [[-0.2173763  -0.03485784  0.2969029   1.        ]]. Action = [[-0.46212924  0.254282    0.559556    0.88213325]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 659. State = [[-0.2509569   0.00258187  0.2326045   1.        ]]. Action = [[-0.34115434 -0.15271097  0.9868176  -0.00258666]]. Reward = [-1.]
Curr episode timestep = 70
Current timestep = 660. State = [[-0.23915073  0.00240194  0.21869873  1.        ]]. Action = [[ 0.8264711  -0.10364258 -0.20438373  0.66375995]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 661. State = [[-0.20378725 -0.00415238  0.20450705  1.        ]]. Action = [[ 0.92666686 -0.34581327 -0.15956295  0.27404618]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 662. State = [[-0.17791    -0.00992956  0.19638282  1.        ]]. Action = [[ 0.60058177  0.3915608  -0.6088587   0.8332052 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Current timestep = 663. State = [[-0.18272047 -0.002137    0.20017348  1.        ]]. Action = [[-0.78410876  0.62286425  0.5309994   0.55310965]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 664. State = [[-0.18204127  0.01976969  0.22286022  1.        ]]. Action = [[0.8443799  0.3770194  0.65477526 0.5096619 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 665. State = [[-0.16616029  0.01319755  0.25685507  1.        ]]. Action = [[-0.0795818  -0.8489194   0.8767495   0.72789526]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 666. State = [[-0.1535154  -0.01361797  0.29810366  1.        ]]. Action = [[ 0.6198001  -0.50818884  0.72030926  0.8954209 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 667. State = [[-0.14549112 -0.03157898  0.33436203  1.        ]]. Action = [[-0.5500285  -0.18372166  0.7264285   0.39831805]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 668. State = [[-0.15732795 -0.05423598  0.36073008  1.        ]]. Action = [[-0.17874753 -0.72771     0.14029467  0.43880343]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 669. State = [[-0.16362149 -0.07244026  0.3677927   1.        ]]. Action = [[-0.04927361 -0.24576706  0.9522042   0.7578869 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 670. State = [[-0.15496017 -0.07699233  0.36612     1.        ]]. Action = [[ 0.8597394  -0.0820232  -0.28180063  0.72542214]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 671. State = [[-0.13874276 -0.06565844  0.34972534  1.        ]]. Action = [[-0.2227425   0.644634   -0.8024849   0.21764505]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 672. State = [[-0.13910915 -0.07197714  0.31975698  1.        ]]. Action = [[-0.11775368 -0.8709199  -0.9175389   0.44330108]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 673. State = [[-0.12825726 -0.0784455   0.28355947  1.        ]]. Action = [[ 0.71984863  0.3428234  -0.53107774  0.71352243]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 674. State = [[-0.10710686 -0.08790084  0.25237578  1.        ]]. Action = [[ 0.8277714  -0.67800194 -0.6614136   0.90629697]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 675. State = [[-0.08920031 -0.09138755  0.23226191  1.        ]]. Action = [[-0.6841699   0.6956506   0.3265413   0.80674505]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 676. State = [[-0.09047308 -0.08018839  0.23376723  1.        ]]. Action = [[-0.49909824  0.9583721  -0.7584745   0.69429255]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 677. State = [[-0.0909162  -0.0790658   0.23365349  1.        ]]. Action = [[ 0.77140903 -0.1284424  -0.70997816  0.90642023]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 678. State = [[-0.09094441 -0.07900811  0.23366776  1.        ]]. Action = [[-0.84240454  0.03558099 -0.610315    0.5180576 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 679. State = [[-0.09094441 -0.07900811  0.23366776  1.        ]]. Action = [[-0.5420977  -0.3512857  -0.06993306  0.8993697 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 680. State = [[-0.08402428 -0.09360287  0.24588986  1.        ]]. Action = [[ 0.615505   -0.7820274   0.656626    0.43619227]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 681. State = [[-0.07672124 -0.10684847  0.25981575  1.        ]]. Action = [[ 0.3726964  -0.05179638 -0.51070267  0.7478838 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 682. State = [[-0.08149283 -0.11926709  0.27914292  1.        ]]. Action = [[-0.56143624 -0.31573564  0.9758713   0.6677693 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 683. State = [[-0.09126772 -0.1106132   0.29861495  1.        ]]. Action = [[-0.13426548  0.9011085  -0.57113683  0.73350644]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 684. State = [[-0.09303842 -0.0955027   0.2862512   1.        ]]. Action = [[ 0.5452819  -0.22000921 -0.54198474  0.8424089 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 685. State = [[-0.08822351 -0.11409403  0.2666618   1.        ]]. Action = [[-0.31610364 -0.8990218  -0.3247019   0.6443846 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 686. State = [[-0.08659366 -0.13344562  0.26023287  1.        ]]. Action = [[ 0.6282835   0.00453222 -0.8420133   0.8046007 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 687. State = [[-0.09731534 -0.13166866  0.25141412  1.        ]]. Action = [[-0.7600295   0.36477637 -0.3844179   0.43787575]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 688. State = [[-0.10356101 -0.12810917  0.24611486  1.        ]]. Action = [[ 0.75177634 -0.0386048   0.6508682   0.87170744]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 689. State = [[-0.08247278 -0.11221444  0.2659246   1.        ]]. Action = [[0.7745476 0.7307451 0.7464398 0.7174388]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 690. State = [[-0.0648167  -0.09524979  0.28823963  1.        ]]. Action = [[ 0.59063375  0.9770403  -0.6407365   0.64836454]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 691. State = [[-0.0535843  -0.10662452  0.3117623   1.        ]]. Action = [[ 0.44135094 -0.7694442   0.9576453   0.6079171 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 692. State = [[-0.05358184 -0.12274952  0.33955595  1.        ]]. Action = [[-0.9170367  -0.06804812  0.09497082  0.8339596 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 693. State = [[-0.07434832 -0.11457127  0.34169582  1.        ]]. Action = [[-0.8871646   0.92658365 -0.41484976  0.326437  ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 694. State = [[-0.09932544 -0.0735516   0.32374012  1.        ]]. Action = [[ 0.2869587   0.85834646 -0.8359089   0.05839646]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 695. State = [[-0.11000136 -0.03392286  0.29214147  1.        ]]. Action = [[-0.69848126  0.8218503  -0.6575161   0.387928  ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 696. State = [[-1.19489245e-01 -9.31971415e-04  2.63432294e-01  1.00000000e+00]]. Action = [[ 0.871006    0.3729527  -0.58242685  0.8729942 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 697. State = [[-0.09302811  0.00563085  0.24404517  1.        ]]. Action = [[ 0.94540286 -0.34640777  0.40756857  0.34602714]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 698. State = [[-0.06347687 -0.00802275  0.25463253  1.        ]]. Action = [[ 0.35541832 -0.5508891   0.43210208  0.43638945]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 699. State = [[-0.05290439 -0.01257287  0.27732286  1.        ]]. Action = [[-0.22382414  0.40476274  0.7270198   0.72946286]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 700. State = [[-0.04019027  0.00483916  0.29626736  1.        ]]. Action = [[0.7560446  0.7020346  0.00563967 0.6552341 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 701. State = [[-0.02703655  0.03953915  0.29036808  1.        ]]. Action = [[ 4.0864944e-04  9.9478626e-01 -7.5411189e-01  8.0745459e-01]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 702. State = [[-0.00531037  0.0642945   0.28107315  1.        ]]. Action = [[ 0.95556426 -0.15883768  0.6173035   0.7906021 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 703. State = [[0.02631807 0.05402242 0.30912945 1.        ]]. Action = [[ 0.77262545 -0.88805044  0.96256626  0.7187884 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 704. State = [[0.04658853 0.03273576 0.32996777 1.        ]]. Action = [[-0.5772872   0.02329111 -0.28264952  0.30359817]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 705. State = [[0.04649306 0.04268064 0.34691054 1.        ]]. Action = [[-0.04315263  0.6293597   0.9542601   0.913568  ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 706. State = [[0.04727944 0.06153548 0.35844105 1.        ]]. Action = [[ 0.84996545  0.4231286  -0.84400415  0.83346736]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 707. State = [[0.07158527 0.07555263 0.32340434 1.        ]]. Action = [[ 0.6920564   0.07627642 -0.7650727   0.49984908]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 708. State = [[0.098566   0.06356665 0.30525884 1.        ]]. Action = [[-0.33884275 -0.90925586  0.3362844   0.68991816]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 709. State = [[0.10086496 0.04657761 0.31232965 1.        ]]. Action = [[ 0.48537803  0.7931483  -0.26703644  0.7232493 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 710. State = [[0.09747154 0.03365436 0.30431223 1.        ]]. Action = [[-0.49892867 -0.53802913 -0.65768623  0.02326488]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 711. State = [[0.08337323 0.00288826 0.2945557  1.        ]]. Action = [[-0.9574306  -0.8676786  -0.05245912  0.7357385 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 712. State = [[ 0.06469389 -0.02245117  0.2888423   1.        ]]. Action = [[ 0.9498805  -0.45894837  0.26680136  0.47488558]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 713. State = [[ 0.05215511 -0.02029355  0.27784052  1.        ]]. Action = [[-0.9051033   0.29047537 -0.7247773   0.38698137]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 714. State = [[3.3042401e-02 3.4712264e-04 2.5286645e-01 1.0000000e+00]]. Action = [[ 0.7408905   0.7549689  -0.20444149  0.95308185]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 715. State = [[0.03426884 0.01601445 0.24544524 1.        ]]. Action = [[ 0.18271804 -0.6908661  -0.4080662   0.79301095]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 716. State = [[0.0381305  0.0175382  0.25682724 1.        ]]. Action = [[ 0.14425373 -0.04746097  0.8146932   0.8370168 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 717. State = [[0.04645859 0.00510001 0.28395292 1.        ]]. Action = [[ 0.6491301  -0.67128295  0.8630872   0.6916822 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 718. State = [[ 0.05297533 -0.00771918  0.29577315  1.        ]]. Action = [[ 0.05004716 -0.00464416 -0.88996965  0.7523391 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 719. State = [[ 0.05392052 -0.00795074  0.28580216  1.        ]]. Action = [[-0.5271863   0.1506176   0.36872387  0.5964607 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 720. State = [[ 0.04176774 -0.00889294  0.29218906  1.        ]]. Action = [[-0.96821976 -0.07236207  0.03132284  0.36466193]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 721. State = [[ 0.02421162 -0.00750817  0.28807682  1.        ]]. Action = [[ 0.9292855   0.07379174 -0.6420312   0.6696985 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 722. State = [[0.02822322 0.01254553 0.2611779  1.        ]]. Action = [[-0.4441554   0.95118856 -0.52335954  0.12828028]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 723. State = [[0.03025401 0.02063946 0.24295634 1.        ]]. Action = [[ 0.61670196 -0.82079595 -0.27399945  0.7855618 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 724. State = [[ 0.04810077 -0.00689046  0.22859584  1.        ]]. Action = [[ 0.8680804  -0.7184208   0.05722761  0.89900017]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 725. State = [[ 0.07620884 -0.02919612  0.22425418  1.        ]]. Action = [[ 0.936687   -0.13467312  0.6338973   0.7557727 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 726. State = [[ 0.07295801 -0.03469122  0.23623456  1.        ]]. Action = [[-0.9963812  -0.03409368  0.58858705  0.8027731 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 727. State = [[ 0.05981365 -0.03749654  0.24969736  1.        ]]. Action = [[-0.7346555  -0.37620556 -0.27683663  0.47964108]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 728. State = [[ 0.05772833 -0.04633038  0.26811743  1.        ]]. Action = [[-0.03460336 -0.39209968  0.77197623  0.6193986 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 729. State = [[ 0.05669623 -0.07019217  0.29875952  1.        ]]. Action = [[ 0.51437044 -0.89991146  0.529691    0.8459246 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 730. State = [[ 0.05586134 -0.07960492  0.3061484   1.        ]]. Action = [[-0.6564767  0.8219271 -0.4938143  0.7403414]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 731. State = [[ 0.0463542  -0.06462035  0.29995087  1.        ]]. Action = [[ 0.9647591  0.7901386 -0.9559893  0.8194988]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 732. State = [[ 0.04213883 -0.05429464  0.30667728  1.        ]]. Action = [[-0.4338174   0.43130362  0.4084351   0.80314803]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 733. State = [[ 0.0191741  -0.05913745  0.32604712  1.        ]]. Action = [[-0.8534661  -0.7577823   0.35281515  0.7200208 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 734. State = [[-0.01992673 -0.08821648  0.33655295  1.        ]]. Action = [[-0.5378863  -0.64317936 -0.04909968  0.6193073 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 735. State = [[-0.04786576 -0.12298662  0.3519958   1.        ]]. Action = [[-0.29369265 -0.8267973   0.6864991   0.7649951 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 736. State = [[-0.07491486 -0.15871754  0.38527307  1.        ]]. Action = [[-0.41917026 -0.8171771   0.94445336  0.7137952 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 737. State = [[-0.09049332 -0.18074894  0.39880413  1.        ]]. Action = [[-0.15524685  0.03693676 -0.73933834  0.68112195]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 738. State = [[-0.10941102 -0.194147    0.39091676  1.        ]]. Action = [[-0.65973186 -0.40879452 -0.00606215  0.6604235 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 739. State = [[-0.12462933 -0.21333475  0.37906316  1.        ]]. Action = [[ 0.2241584  -0.5351707  -0.47754467  0.64367783]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 740. State = [[-0.13981244 -0.24613774  0.37452218  1.        ]]. Action = [[-0.94467586 -0.84767264  0.01661313  0.8647431 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 741. State = [[-0.15970287 -0.2714083   0.36819738  1.        ]]. Action = [[ 0.5859711  -0.23793632  0.8502338   0.6434587 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 742. State = [[-0.15415798 -0.27160776  0.35541838  1.        ]]. Action = [[ 0.90458083  0.05385375 -0.90425545  0.8306768 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 743. State = [[-0.13070165 -0.26230624  0.3328863   1.        ]]. Action = [[0.21176267 0.586406   0.22455192 0.59697914]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 744. State = [[-0.12107487 -0.2437411   0.33323967  1.        ]]. Action = [[0.10968614 0.49869478 0.05055201 0.52577806]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 745. State = [[-0.1202904  -0.23797691  0.3221029   1.        ]]. Action = [[-0.04183316 -0.5626276  -0.8689402   0.68631434]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 746. State = [[-0.11324722 -0.25359493  0.31186968  1.        ]]. Action = [[ 0.24941778 -0.40299898  0.8285327   0.5760037 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 747. State = [[-0.11448357 -0.26862222  0.33598977  1.        ]]. Action = [[-0.57204914 -0.09076583  0.91441846  0.36451328]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 748. State = [[-0.1106016  -0.27603003  0.36537793  1.        ]]. Action = [[ 0.8593564  -0.3174199   0.37086117  0.6846578 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 749. State = [[-0.10039661 -0.27318114  0.3876737   1.        ]]. Action = [[-0.36802292  0.7164192   0.6209378   0.49019504]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 750. State = [[-0.10703044 -0.24523121  0.39620185  1.        ]]. Action = [[-0.26874226  0.83338356 -0.78777236  0.7070687 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 751. State = [[-0.11118875 -0.23080951  0.37574092  1.        ]]. Action = [[ 0.5453954 -0.5968186 -0.9958498  0.6274333]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 752. State = [[-0.10463695 -0.23493694  0.355456    1.        ]]. Action = [[-0.312914    0.25717616  0.6705692   0.4285698 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 753. State = [[-0.10085911 -0.22192988  0.3568023   1.        ]]. Action = [[ 0.23678005  0.61607254 -0.40232718  0.59227157]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 754. State = [[-0.09969781 -0.19482334  0.34187636  1.        ]]. Action = [[-0.112795   0.6219418 -0.858959   0.8687117]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 755. State = [[-0.10455678 -0.17103706  0.3228468   1.        ]]. Action = [[-0.51420194  0.3719201   0.03475559  0.46097684]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 756. State = [[-0.10831138 -0.15509406  0.32154703  1.        ]]. Action = [[0.73392797 0.06718791 0.2880559  0.7795253 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 757. State = [[-0.10152385 -0.15096545  0.32664803  1.        ]]. Action = [[-0.22743356  0.00535262  0.26407552  0.75917244]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 758. State = [[-0.10128149 -0.13687997  0.32763273  1.        ]]. Action = [[-0.18554366  0.78449714 -0.15460676  0.8034947 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 759. State = [[-0.11633021 -0.12595549  0.31904963  1.        ]]. Action = [[-0.6813783  -0.35647058 -0.7147019   0.52961636]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 760. State = [[-0.14830467 -0.14515659  0.30386844  1.        ]]. Action = [[-0.8626279  -0.7197742  -0.26564574  0.7792733 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 761. State = [[-0.18142664 -0.14655456  0.29932222  1.        ]]. Action = [[-0.84041256  0.9471451   0.4955343   0.6091713 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 762. State = [[-0.20238562 -0.1335133   0.31093788  1.        ]]. Action = [[ 0.71142685 -0.667031    0.41057754  0.932932  ]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 763. State = [[-0.1829954 -0.1433414  0.3195424  1.       ]]. Action = [[ 0.9065442  -0.10026908  0.1539123   0.8531294 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 764. State = [[-0.16325387 -0.15884145  0.31854138  1.        ]]. Action = [[ 0.27009356 -0.6160005  -0.49945378  0.8925414 ]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 765. State = [[-0.15702352 -0.18598627  0.3089586   1.        ]]. Action = [[-0.63524014 -0.36605477 -0.15991437  0.17742288]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 766. State = [[-0.17311044 -0.21021621  0.29268262  1.        ]]. Action = [[-0.03471804 -0.7805027  -0.96385086  0.8104303 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 767. State = [[-0.17758434 -0.2502628   0.26413524  1.        ]]. Action = [[-0.20987058 -0.9025869  -0.23385262  0.83165336]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 768. State = [[-0.17591053 -0.26540828  0.24076425  1.        ]]. Action = [[ 0.2266829   0.7544892  -0.8561103   0.75794125]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 769. State = [[-0.16116928 -0.24396147  0.20158805  1.        ]]. Action = [[ 0.37195516  0.63083494 -0.8451085   0.61529684]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 770. State = [[-0.16740447 -0.24697325  0.18340884  1.        ]]. Action = [[-0.9527467  -0.87048084  0.66187704  0.8884697 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 771. State = [[-0.1714179  -0.25634027  0.20208389  1.        ]]. Action = [[ 0.45090222 -0.3329751   0.97382903  0.80024207]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 772. State = [[-0.17875789 -0.2825728   0.21265851  1.        ]]. Action = [[ 0.00370967 -0.713532   -0.8210921   0.66343164]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 773. State = [[-0.18770555 -0.30722812  0.2116635   1.        ]]. Action = [[-0.8836639 -0.2482109  0.6579709  0.806254 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 774. State = [[-0.20134462 -0.3122824   0.22259468  1.        ]]. Action = [[-0.01730555 -0.80735815  0.05564177  0.6512263 ]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Current timestep = 775. State = [[-0.21010822 -0.31041458  0.2178335   1.        ]]. Action = [[-0.29567146  0.1294477  -0.6379759   0.53819454]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 776. State = [[-0.2254699  -0.3078513   0.22471662  1.        ]]. Action = [[-0.6856522   0.3261137   0.87110424  0.5533345 ]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 777. State = [[-0.23563832 -0.2855431   0.23853557  1.        ]]. Action = [[-0.8943916   0.7629278  -0.08155918  0.38233483]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Current timestep = 778. State = [[-0.24139006 -0.27841878  0.24368936  1.        ]]. Action = [[ 0.76309156 -0.9719048   0.73576236  0.6473906 ]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: Workspace boundary
Current timestep = 779. State = [[-0.24382353 -0.27798817  0.24659452  1.        ]]. Action = [[-0.6361843   0.27228832 -0.49399197  0.8472693 ]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: Workspace boundary
Current timestep = 780. State = [[-0.24499431 -0.27834505  0.2474803   1.        ]]. Action = [[-0.6104758   0.3429166  -0.54179364  0.86035705]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Current timestep = 781. State = [[-0.23722543 -0.28273338  0.24766022  1.        ]]. Action = [[ 0.90037835 -0.1899373  -0.17369157  0.8439456 ]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 782. State = [[-0.22741663 -0.2967348   0.24207394  1.        ]]. Action = [[-0.04258221 -0.7263418   0.98877287  0.3089366 ]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Current timestep = 783. State = [[-0.22712973 -0.30548203  0.23852253  1.        ]]. Action = [[-0.38576877 -0.9029539  -0.34224725  0.7056577 ]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Current timestep = 784. State = [[-0.21171106 -0.3014709   0.23405544  1.        ]]. Action = [[ 0.89002955  0.33943105 -0.2118445   0.9041494 ]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 785. State = [[-0.19022596 -0.2961987   0.22843675  1.        ]]. Action = [[-0.5498098  -0.40593445  0.64651656  0.85875154]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Current timestep = 786. State = [[-0.25064868  0.00260269  0.23269278  1.        ]]. Action = [[-0.23372889  0.46408224  0.69669795  0.7521001 ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 787. State = [[-0.25298753  0.00842177  0.22039345  1.        ]]. Action = [[0.00559461 0.3898369  0.09366167 0.40324736]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 788. State = [[-0.24151382  0.02809385  0.22502416  1.        ]]. Action = [[0.96890736 0.7374418  0.48393524 0.85070705]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 789. State = [[-0.21968034  0.03602751  0.24438958  1.        ]]. Action = [[-0.19752765 -0.57001126  0.83460116  0.6786412 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 790. State = [[-0.20312107  0.02358017  0.26803634  1.        ]]. Action = [[ 0.8837985  -0.27105623  0.11585307  0.82875395]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 791. State = [[-0.18486656  0.03471218  0.26844302  1.        ]]. Action = [[ 0.29059982  0.9455061  -0.77125496  0.6121659 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 792. State = [[-0.17817967  0.04133915  0.25195903  1.        ]]. Action = [[-0.897794   -0.58744395  0.1474402   0.45624137]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 793. State = [[-0.19131134  0.04519664  0.24532378  1.        ]]. Action = [[-0.05887836  0.54516745 -0.44839406  0.37657666]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 794. State = [[-0.193224    0.0537193   0.23785518  1.        ]]. Action = [[ 0.40425777 -0.08857924 -0.02206933  0.7208402 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 795. State = [[-0.19071926  0.06716125  0.24206275  1.        ]]. Action = [[-0.02642983  0.72603536  0.5554323   0.67994   ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 796. State = [[-0.18885499  0.08424861  0.2485796   1.        ]]. Action = [[ 0.7210643   0.40336823 -0.3560655   0.41592383]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 797. State = [[-0.1898866   0.08593407  0.25246894  1.        ]]. Action = [[-0.3065229  -0.08245742  0.16523182  0.73249006]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 798. State = [[-0.18333332  0.09021237  0.26487568  1.        ]]. Action = [[0.7933557  0.27684176 0.29429138 0.23376262]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 799. State = [[-0.15837318  0.08954746  0.28884828  1.        ]]. Action = [[ 0.6779177  -0.2981863   0.66768074  0.9212961 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 800. State = [[-0.1442807   0.10063193  0.3007523   1.        ]]. Action = [[-0.06705207  0.9638612  -0.5366017   0.7313901 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 801. State = [[-0.13879716  0.1423272   0.29915226  1.        ]]. Action = [[0.0733403  0.93320847 0.54091644 0.48186302]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 801 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 801 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of -1
Current timestep = 802. State = [[-0.11968163  0.1722239   0.3241419   1.        ]]. Action = [[ 0.98267865 -0.04640973  0.5336864   0.82851946]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 802 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 802 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of 0
Current timestep = 803. State = [[-0.07921697  0.15599194  0.33849338  1.        ]]. Action = [[ 0.5500902  -0.93963987 -0.26684797  0.7691531 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 803 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 803 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of 1
Current timestep = 804. State = [[-0.06322519  0.11782096  0.32355088  1.        ]]. Action = [[-0.30602503 -0.7492538  -0.52103424  0.7037988 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 805. State = [[-0.06065436  0.10810479  0.31022194  1.        ]]. Action = [[ 0.3345636   0.32203662 -0.15082169  0.4963386 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 806. State = [[-0.05265149  0.10624893  0.2948758   1.        ]]. Action = [[ 0.6593236  -0.10624051 -0.6513537   0.8509847 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 807. State = [[-0.02687417  0.10562503  0.2620173   1.        ]]. Action = [[ 0.99168134  0.14234519 -0.96712244  0.23065782]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 808. State = [[0.0177302  0.10964581 0.23586084 1.        ]]. Action = [[0.5675769  0.02113426 0.67237234 0.50633025]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 809. State = [[0.04341023 0.11345368 0.24364829 1.        ]]. Action = [[ 0.32706034 -0.21331155 -0.5491726   0.35012543]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 810. State = [[0.0510197  0.11544113 0.2468116  1.        ]]. Action = [[-0.2562505  -0.40669727 -0.27465004  0.6628171 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 811. State = [[0.05412416 0.12843356 0.25722045 1.        ]]. Action = [[0.23328006 0.73024464 0.5422814  0.7368367 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 812. State = [[0.06678776 0.13809523 0.28440517 1.        ]]. Action = [[ 0.3137324  -0.25892043  0.68474686  0.56964827]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 813. State = [[0.07423659 0.12574108 0.32373708 1.        ]]. Action = [[-0.8011896  -0.69350445  0.9233825   0.7456515 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 814. State = [[0.06151652 0.12398358 0.34375253 1.        ]]. Action = [[-0.6124971  0.4746127 -0.6499948  0.5667546]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 815. State = [[0.05016335 0.11747877 0.35308963 1.        ]]. Action = [[-0.43286932 -0.8802428   0.7557552   0.6277158 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 816. State = [[0.02340686 0.07778083 0.3666889  1.        ]]. Action = [[-0.6619837  -0.94970644 -0.4560992   0.7485409 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 817. State = [[-0.00228013  0.03082392  0.360558    1.        ]]. Action = [[-0.01416433 -0.9079261   0.1709876   0.66744506]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 818. State = [[-0.01169632 -0.01315967  0.3644095   1.        ]]. Action = [[ 0.1778903  -0.7855197   0.06058836  0.7708137 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 819. State = [[-0.01335532 -0.0261338   0.3659388   1.        ]]. Action = [[-0.00685561  0.53982973 -0.07016557  0.5876988 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 820. State = [[-0.02293441 -0.01810504  0.35227808  1.        ]]. Action = [[-0.7410543   0.17409086 -0.8219735   0.86387205]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 821. State = [[-0.04709407 -0.00211789  0.32448846  1.        ]]. Action = [[-0.51894444  0.52639747 -0.10823679  0.70752144]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 822. State = [[-0.05609132  0.00631948  0.32955906  1.        ]]. Action = [[ 0.62450063 -0.2966714   0.89902925  0.66775894]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 823. State = [[-0.05283915 -0.0163109   0.34793782  1.        ]]. Action = [[ 0.12977993 -0.9103296  -0.04224265  0.51326156]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 824. State = [[-0.04503883 -0.02919937  0.3449693   1.        ]]. Action = [[ 0.7354045   0.40525544 -0.39022768  0.8624866 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 825. State = [[-0.03288123 -0.02410731  0.3247343   1.        ]]. Action = [[ 0.02254915  0.0234648  -0.3630172   0.79643047]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 826. State = [[-0.03829984 -0.03450577  0.3053495   1.        ]]. Action = [[-0.97680837 -0.5905933  -0.8013793   0.6519022 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 827. State = [[-0.04470419 -0.0402359   0.2760647   1.        ]]. Action = [[ 0.9146199   0.35299838 -0.85235214  0.47185016]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 828. State = [[-0.03517325 -0.0355958   0.25376448  1.        ]]. Action = [[-0.68902975  0.14174926  0.69675446  0.834833  ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 829. State = [[-0.03211297 -0.02351458  0.27481025  1.        ]]. Action = [[0.55949044 0.42747557 0.791132   0.8732686 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 830. State = [[-0.01485688 -0.00849668  0.29772997  1.        ]]. Action = [[0.9557822  0.21663713 0.25484705 0.6591066 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 831. State = [[-0.00691428  0.00632213  0.2955448   1.        ]]. Action = [[-0.85784155  0.336097   -0.8850978   0.585449  ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 832. State = [[-0.0053168   0.00580662  0.29336196  1.        ]]. Action = [[ 0.6375375  -0.42581892  0.8310194   0.74898624]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 833. State = [[0.01319274 0.0161743  0.3173639  1.        ]]. Action = [[0.9487333  0.7268046  0.7273383  0.67738044]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 834. State = [[0.03775035 0.03513926 0.33914837 1.        ]]. Action = [[0.14482963 0.19782495 0.13894427 0.81370175]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 835. State = [[0.04703088 0.03825614 0.3352111  1.        ]]. Action = [[ 0.03641379 -0.14226753 -0.71479696  0.5958395 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 836. State = [[0.05483209 0.04750677 0.3158732  1.        ]]. Action = [[-0.05166876  0.533304   -0.10655046  0.57895374]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 837. State = [[0.0636887  0.04663199 0.31006858 1.        ]]. Action = [[ 0.60055995 -0.5310957   0.00573528  0.85006034]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 838. State = [[0.08233196 0.02266436 0.31751487 1.        ]]. Action = [[ 0.2617327 -0.7698227  0.533731   0.5893328]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 839. State = [[0.08569253 0.01365623 0.34751904 1.        ]]. Action = [[-0.85845643  0.53179574  0.7622616   0.80304337]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 840. State = [[0.07521386 0.02212611 0.3693097  1.        ]]. Action = [[0.5166724  0.7670374  0.20561826 0.82252645]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 841. State = [[0.07369307 0.02255227 0.37302458 1.        ]]. Action = [[ 0.8108103  -0.22068524 -0.95279974  0.257872  ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 842. State = [[0.06545149 0.01597406 0.38894388 1.        ]]. Action = [[-0.80647063 -0.3744552   0.48489988  0.601079  ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 843. State = [[0.04584476 0.00922908 0.40343755 1.        ]]. Action = [[-0.8205156   0.83838785  0.74859536  0.37448406]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 844. State = [[0.04327423 0.00725386 0.4051276  1.        ]]. Action = [[-0.90110344 -0.72933465  0.8875041   0.39621258]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 845. State = [[0.04283892 0.01864493 0.40737113 1.        ]]. Action = [[0.29943347 0.5955324  0.13252044 0.7101338 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 846. State = [[0.04184711 0.03157074 0.40438843 1.        ]]. Action = [[ 0.09885824  0.07037973 -0.2641586   0.60742605]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 847. State = [[0.04286619 0.03436173 0.39901072 1.        ]]. Action = [[ 0.9854697  -0.83577085 -0.58523965  0.29370713]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 848. State = [[0.04305929 0.03498351 0.39766836 1.        ]]. Action = [[-0.48917437 -0.36354578  0.71868086  0.60398936]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 849. State = [[0.05149853 0.03997948 0.3966203  1.        ]]. Action = [[0.9377562  0.19666874 0.06313634 0.6966083 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 850. State = [[0.07658864 0.06375173 0.39440033 1.        ]]. Action = [[0.2628168  0.77817345 0.16173542 0.42336178]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 851. State = [[0.08693597 0.08327457 0.39643195 1.        ]]. Action = [[-0.6204931  -0.09580457  0.9030242   0.74364066]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 852. State = [[0.08922566 0.10248233 0.40032265 1.        ]]. Action = [[-0.05678612  0.80642176  0.11515653  0.3315034 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 853. State = [[0.09011764 0.11953254 0.40595302 1.        ]]. Action = [[-0.23650742 -0.2824996   0.73811054  0.26924014]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 854. State = [[0.09112883 0.12226219 0.40386146 1.        ]]. Action = [[ 0.2754326  -0.25849593 -0.96225744  0.12338781]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 855. State = [[0.09143983 0.12287132 0.40233433 1.        ]]. Action = [[ 0.21097004 -0.8576243  -0.5987995   0.50133324]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 856. State = [[0.09170312 0.12350368 0.40218207 1.        ]]. Action = [[ 0.53267     0.7060194  -0.05926037  0.5679827 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 857. State = [[0.09405838 0.11527788 0.4052551  1.        ]]. Action = [[ 0.07187474 -0.4384601   0.22427344  0.66360486]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 858. State = [[0.09536299 0.10898316 0.41148216 1.        ]]. Action = [[-0.3763312   0.81883454  0.5579064   0.5254537 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 859. State = [[0.09549626 0.107874   0.41331813 1.        ]]. Action = [[ 0.11865616  0.5119984  -0.26052856  0.91444254]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 860. State = [[0.09498485 0.10694581 0.4148941  1.        ]]. Action = [[-0.96103483 -0.978669    0.07297027  0.777513  ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 861. State = [[0.0939481  0.10585365 0.40669677 1.        ]]. Action = [[-0.2830974  -0.04104835 -0.6701283   0.8430046 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 862. State = [[0.09421194 0.1050367  0.3972817  1.        ]]. Action = [[-0.47899234  0.39339292  0.52817047  0.58435345]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 863. State = [[0.09434905 0.10516932 0.39526024 1.        ]]. Action = [[-0.7795556  -0.39491987  0.44939375  0.8112352 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 864. State = [[0.09350112 0.11732306 0.39889547 1.        ]]. Action = [[0.07849598 0.6388781  0.3399917  0.6732415 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 865. State = [[0.09111976 0.14328374 0.40551132 1.        ]]. Action = [[0.05506575 0.7147975  0.16226935 0.701609  ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 866. State = [[0.08999699 0.16081573 0.4105232  1.        ]]. Action = [[ 0.76364374 -0.87784284 -0.8430632   0.7059083 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 867. State = [[0.09006285 0.16289261 0.40915155 1.        ]]. Action = [[-0.02566862  0.36482167  0.7428982   0.6545124 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 868. State = [[0.09019069 0.16320975 0.40779245 1.        ]]. Action = [[-0.7327763   0.63151217  0.97391343  0.84098625]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 869. State = [[0.0849995  0.17903952 0.3969661  1.        ]]. Action = [[-0.53370255  0.7631314  -0.97164184  0.4110092 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 870. State = [[0.07719256 0.19608039 0.36434177 1.        ]]. Action = [[ 0.0199852  -0.34073132 -0.65118325  0.62041855]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 871. State = [[0.07277048 0.18529074 0.34788337 1.        ]]. Action = [[-0.92289734 -0.5763993  -0.03703815  0.79658496]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 872. State = [[0.05504532 0.17500883 0.3565188  1.        ]]. Action = [[0.2902845  0.07477748 0.89050364 0.6386304 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 873. State = [[0.04658403 0.18360174 0.3849685  1.        ]]. Action = [[-0.69803244  0.4789114   0.39129663  0.7190865 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 873 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 873 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of -1
Current timestep = 874. State = [[0.02577909 0.18811765 0.3872719  1.        ]]. Action = [[ 0.5224786  -0.39752913 -0.7471138   0.5701368 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 875. State = [[0.03715064 0.18605703 0.36535606 1.        ]]. Action = [[ 0.8358295   0.46722126 -0.36734796  0.4593742 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 876. State = [[0.04923464 0.20035137 0.3276251  1.        ]]. Action = [[-0.45405126  0.43400455 -0.88755316  0.6974019 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 877. State = [[0.05643133 0.1978152  0.30491737 1.        ]]. Action = [[ 0.621835   -0.75549114  0.14873159  0.66323066]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 878. State = [[0.06895784 0.17699713 0.30797902 1.        ]]. Action = [[-0.01213616 -0.22020388  0.34479046  0.5174365 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 879. State = [[0.07498586 0.17790575 0.3060347  1.        ]]. Action = [[ 0.463037    0.3605976  -0.42728293  0.62843907]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 880. State = [[0.08895084 0.17177181 0.30311817 1.        ]]. Action = [[-0.01304078 -0.5814895   0.39925742  0.8618381 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 881. State = [[0.0924801  0.15978646 0.3218851  1.        ]]. Action = [[-0.27481794 -0.13584864  0.5826571   0.78182626]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 882. State = [[0.08343039 0.15684687 0.32483435 1.        ]]. Action = [[-0.71285945  0.00266778 -0.7539749   0.7033851 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 883. State = [[0.06168954 0.15400773 0.33023575 1.        ]]. Action = [[-0.9155371 -0.1886     0.8127587  0.8020377]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 884. State = [[0.03053747 0.14952312 0.35063967 1.        ]]. Action = [[0.66812754 0.7180923  0.58194137 0.52828884]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 885. State = [[0.0184868  0.12994064 0.36596248 1.        ]]. Action = [[-0.9445424  -0.9483576   0.44857383  0.72576904]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 886. State = [[-0.01780429  0.10281377  0.3919376   1.        ]]. Action = [[-0.12072194 -0.07317466  0.7947135   0.7932265 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 887. State = [[-0.02872038  0.09660818  0.41211656  1.        ]]. Action = [[ 0.09947252 -0.60994613  0.56055343  0.458673  ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 888. State = [[-0.03029486  0.09507845  0.41602302  1.        ]]. Action = [[0.82326496 0.32326996 0.44328678 0.79993725]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 889. State = [[-0.03041773  0.09505463  0.4160874   1.        ]]. Action = [[ 0.07734454 -0.5735796   0.48031497  0.7305207 ]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: Workspace boundary
Current timestep = 890. State = [[-0.03041773  0.09505463  0.4160874   1.        ]]. Action = [[-0.25348425  0.13193023  0.8759829   0.83346105]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Current timestep = 891. State = [[-0.03041773  0.09505463  0.4160874   1.        ]]. Action = [[ 0.6922269  -0.59471667  0.6927028   0.68956876]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Current timestep = 892. State = [[-0.02667545  0.08066542  0.40175518  1.        ]]. Action = [[ 0.9353683  -0.8978574  -0.89255595  0.6878445 ]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 893. State = [[-0.00904191  0.05478443  0.37929887  1.        ]]. Action = [[0.06216574 0.3300321  0.6184969  0.8307692 ]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Current timestep = 894. State = [[-0.01281275  0.0423651   0.36936265  1.        ]]. Action = [[-0.59053296 -0.37541652 -0.7000475   0.8145822 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 895. State = [[-0.01361298  0.03523115  0.34444675  1.        ]]. Action = [[ 0.5354445   0.08303082 -0.96520054  0.7499461 ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 896. State = [[0.00464356 0.02975202 0.31452033 1.        ]]. Action = [[ 0.8598349  -0.39618313  0.47202277  0.48775387]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 897. State = [[0.03272007 0.03497241 0.3056747  1.        ]]. Action = [[ 0.4022491   0.90812993 -0.45029402  0.8200512 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 898. State = [[0.04880365 0.0550408  0.281514   1.        ]]. Action = [[ 0.264202    0.01715279 -0.41537917  0.7758988 ]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 899. State = [[0.05130941 0.04383256 0.25789204 1.        ]]. Action = [[-0.92020214 -0.693687   -0.74317396  0.5534228 ]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 900. State = [[0.03986674 0.02606267 0.24565575 1.        ]]. Action = [[-0.47749615 -0.0192765  -0.4810195   0.66411555]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: No entry zone
Current timestep = 901. State = [[0.03893813 0.02464265 0.24493267 1.        ]]. Action = [[ 0.72950554 -0.5329515  -0.8966044   0.7991736 ]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: No entry zone
Current timestep = 902. State = [[0.038914   0.02462497 0.24490899 1.        ]]. Action = [[-0.25820243  0.66508985 -0.22906947  0.70303726]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: No entry zone
Current timestep = 903. State = [[0.038914   0.02462497 0.24490899 1.        ]]. Action = [[-0.92807364 -0.87171614 -0.41751468 -0.03240943]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: No entry zone
Current timestep = 904. State = [[0.04374637 0.00967015 0.24647166 1.        ]]. Action = [[ 0.8644068  -0.8957718   0.28806627  0.21105492]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 905. State = [[0.07025304 0.00413382 0.2577398  1.        ]]. Action = [[0.91984963 0.79507565 0.724102   0.6486447 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 906. State = [[0.09773828 0.01590182 0.27089083 1.        ]]. Action = [[ 0.31558537  0.81471586 -0.8065078   0.6771982 ]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: No entry zone
Current timestep = 907. State = [[0.10355847 0.03652544 0.2816575  1.        ]]. Action = [[-0.20256275  0.9139559   0.49873734  0.67636037]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 908. State = [[0.09629208 0.03623977 0.30092022 1.        ]]. Action = [[-0.7437722  -0.9136944   0.12846911  0.92862594]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 909. State = [[0.07288947 0.01028307 0.29669526 1.        ]]. Action = [[-0.97734463 -0.46731097 -0.63892454  0.52373767]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 910. State = [[ 0.04121974 -0.00136055  0.28384462  1.        ]]. Action = [[ 0.7078829  -0.30925393  0.8096843   0.552575  ]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Current timestep = 911. State = [[ 0.03164062 -0.01273384  0.27645972  1.        ]]. Action = [[-0.3633238  -0.37843382 -0.5467902   0.7629131 ]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 912. State = [[ 0.02167746 -0.035747    0.27068946  1.        ]]. Action = [[ 0.3620094  -0.86612976  0.35310125  0.66059124]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 913. State = [[-0.25033066  0.002761    0.23282306  1.        ]]. Action = [[-0.32259881  0.7149737  -0.38213658  0.7904707 ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 914. State = [[-0.24090059  0.01105018  0.21531215  1.        ]]. Action = [[ 0.77945566  0.5601616  -0.01330775  0.88961196]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 915. State = [[-0.21977954  0.01411585  0.21361125  1.        ]]. Action = [[ 0.16083872 -0.39366645  0.68920004  0.69004583]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 916. State = [[-0.21465714  0.00119265  0.2330342   1.        ]]. Action = [[-0.17181456 -0.39000565  0.5937195   0.8547479 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 917. State = [[-0.2182657  -0.00609711  0.24170457  1.        ]]. Action = [[ 0.03704     0.18003869 -0.74562764  0.7850206 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 918. State = [[-0.20972832  0.00231619  0.23182654  1.        ]]. Action = [[ 0.5064838   0.4113537  -0.04255813  0.79827404]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 919. State = [[-0.19802514  0.01101931  0.22735488  1.        ]]. Action = [[ 0.9445915  -0.86886084 -0.6171081   0.20453537]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 920. State = [[-0.1966276   0.01190693  0.2282608   1.        ]]. Action = [[ 0.65492713  0.44181347 -0.27838945  0.7318301 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 921. State = [[-0.18442294  0.00338846  0.24131562  1.        ]]. Action = [[ 0.55379796 -0.5016004   0.9061302   0.6171386 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 922. State = [[-0.16786106 -0.004576    0.25979578  1.        ]]. Action = [[ 0.58837974 -0.39957982 -0.42507148  0.7726941 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 923. State = [[-0.15238914 -0.01314778  0.27935484  1.        ]]. Action = [[ 0.705312   -0.3359698   0.8661883   0.84744215]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 924. State = [[-0.13868004 -0.00879726  0.29500738  1.        ]]. Action = [[-0.236579    0.6445923  -0.519929    0.59724927]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 925. State = [[-0.14402294  0.01765752  0.2887973   1.        ]]. Action = [[-0.37741774  0.6849935   0.01294398  0.56204534]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 926. State = [[-0.15804584  0.05099402  0.28043222  1.        ]]. Action = [[-0.43657702  0.6804011  -0.60997725  0.8612391 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 927. State = [[-0.15838988  0.0534161   0.26061112  1.        ]]. Action = [[ 0.8745369  -0.8157787  -0.5729254   0.56260824]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 928. State = [[-0.13355261  0.05540193  0.25007856  1.        ]]. Action = [[0.75585604 0.8520429  0.66159177 0.8410243 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 929. State = [[-0.1102989   0.06972568  0.26090607  1.        ]]. Action = [[ 0.23546815 -0.18558258 -0.8820287   0.7937796 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 930. State = [[-0.1061676   0.0718911   0.26440096  1.        ]]. Action = [[ 0.8676783 -0.8508208 -0.6451302  0.6284921]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 931. State = [[-0.09935287  0.05915435  0.27212155  1.        ]]. Action = [[ 0.2739401  -0.7121687   0.41572928  0.86556935]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 932. State = [[-0.09874673  0.03896138  0.28265333  1.        ]]. Action = [[-0.8391971  -0.38487625 -0.00414783  0.6441457 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 933. State = [[-0.10376364  0.03474947  0.2756362   1.        ]]. Action = [[ 0.6265348   0.39283276 -0.8189752   0.67426467]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 934. State = [[-0.10483583  0.03503633  0.26657823  1.        ]]. Action = [[-0.9713242  -0.28867126  0.7691293   0.64742136]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 935. State = [[-0.11164659  0.03315005  0.28439006  1.        ]]. Action = [[0.91950965 0.0728755  0.18072224 0.5826961 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 936. State = [[-0.11209265  0.01463016  0.30355826  1.        ]]. Action = [[-0.91391146 -0.9880108   0.73354256  0.13095975]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 937. State = [[-0.13591057 -0.01810965  0.34229904  1.        ]]. Action = [[-0.88146687 -0.37437844  0.95296633  0.8257381 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 938. State = [[-0.16397274 -0.03475117  0.37564912  1.        ]]. Action = [[-0.07814962 -0.1342299   0.07134318  0.9020405 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 939. State = [[-0.17030713 -0.04007537  0.37934592  1.        ]]. Action = [[0.89425087 0.31036723 0.9829662  0.7244277 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 940. State = [[-0.18181475 -0.03163646  0.3917666   1.        ]]. Action = [[-0.62315494  0.44818282  0.53815866  0.5685625 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 941. State = [[-0.19610582 -0.01381769  0.40084732  1.        ]]. Action = [[ 0.60526323  0.41267228 -0.48194832  0.6888373 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 942. State = [[-0.19641627  0.01159283  0.3923637   1.        ]]. Action = [[-0.54482776  0.761451    0.19968522  0.89693785]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 943. State = [[-0.20356096  0.0256293   0.39514086  1.        ]]. Action = [[ 0.2035222  -0.36665785 -0.09538019  0.435822  ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 944. State = [[-0.21156754  0.01237529  0.3956763   1.        ]]. Action = [[-0.6972454  -0.47511065  0.00477266  0.86750424]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 945. State = [[-0.21706115 -0.01642151  0.38409448  1.        ]]. Action = [[ 0.62825084 -0.8436453  -0.98532665  0.7893064 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 946. State = [[-0.19543935 -0.04820265  0.35097417  1.        ]]. Action = [[ 0.9219444  -0.54878813 -0.68011904  0.78905046]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 947. State = [[-0.1667258  -0.0505949   0.32882145  1.        ]]. Action = [[0.30411625 0.8587836  0.5669372  0.7899811 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 948. State = [[-0.16533956 -0.02572908  0.33991423  1.        ]]. Action = [[-0.7365816   0.4975176   0.33514452  0.5112355 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 949. State = [[-0.17231438 -0.00291     0.3620294   1.        ]]. Action = [[0.34400034 0.30875957 0.8139297  0.5045259 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 950. State = [[-0.16640612 -0.0031743   0.39426133  1.        ]]. Action = [[ 0.11426401 -0.48021805  0.7057123   0.61510754]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 951. State = [[-0.15088147 -0.01416312  0.4169626   1.        ]]. Action = [[ 0.9211402  -0.10631454  0.02473104  0.54349077]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 952. State = [[-0.13137381 -0.01565692  0.42368737  1.        ]]. Action = [[-0.28759253 -0.19506282  0.85371673  0.867535  ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 953. State = [[-0.13595314 -0.02833443  0.42123827  1.        ]]. Action = [[-0.5742833  -0.6026869  -0.56613106  0.49144924]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 954. State = [[-0.13670309 -0.0271981   0.4059321   1.        ]]. Action = [[ 0.9268625   0.7721119  -0.94459844  0.9187858 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 955. State = [[-0.12222409 -0.02802519  0.36312774  1.        ]]. Action = [[-0.7942934  -0.7697822  -0.4405223   0.89405763]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 956. State = [[-0.12383709 -0.03310031  0.33672357  1.        ]]. Action = [[ 0.9282334  0.4458096 -0.9068159  0.8545308]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 957. State = [[-0.09358094 -0.03746533  0.2956308   1.        ]]. Action = [[ 0.9442233  -0.5282921  -0.28879398  0.4437605 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 958. State = [[-0.06957022 -0.05978873  0.29486194  1.        ]]. Action = [[-0.70454675 -0.46454173  0.89732504  0.76268935]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 959. State = [[-0.07914416 -0.09051227  0.31242073  1.        ]]. Action = [[-0.04940796 -0.90443295 -0.23646736  0.6070392 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 960. State = [[-0.079063   -0.1072687   0.31095675  1.        ]]. Action = [[ 0.13731849  0.3250425  -0.08634895  0.70483375]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 961. State = [[-0.07994096 -0.08753516  0.29692733  1.        ]]. Action = [[ 0.03583503  0.9485284  -0.7607937   0.38481474]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 962. State = [[-0.08281966 -0.0485771   0.28765577  1.        ]]. Action = [[-0.7800321  0.9561324  0.6208389  0.8678055]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 963. State = [[-0.11166877 -0.01353303  0.28390023  1.        ]]. Action = [[-0.73491555  0.2816311  -0.824404    0.6085472 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 964. State = [[-0.14352258  0.01328662  0.27464426  1.        ]]. Action = [[-0.8455818   0.76086414  0.47309327  0.2973802 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 965. State = [[-0.16422877  0.04435356  0.29901016  1.        ]]. Action = [[0.5747708  0.40399468 0.9823296  0.66897166]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 966. State = [[-0.16068605  0.04148266  0.3063004   1.        ]]. Action = [[ 0.50917065 -0.75838596 -0.9041592   0.81293154]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 967. State = [[-0.13882183  0.0123936   0.28346118  1.        ]]. Action = [[ 0.85125864 -0.8135837  -0.4473834   0.54137874]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 968. State = [[-0.12405972 -0.00455377  0.25919974  1.        ]]. Action = [[-0.8951987   0.36795163 -0.53045243  0.71717644]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 969. State = [[-0.14498365  0.01015123  0.25399154  1.        ]]. Action = [[-0.99048513  0.6035583   0.3333645   0.8673897 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 970. State = [[-0.17156525  0.02700782  0.2561615   1.        ]]. Action = [[ 0.93280625 -0.8380048  -0.90279204  0.84633684]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 971. State = [[-0.172097    0.01256259  0.27110633  1.        ]]. Action = [[ 0.13399172 -0.851477    0.82085824  0.86707187]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 972. State = [[-0.1750635  -0.00451646  0.28872192  1.        ]]. Action = [[ 0.8139436  -0.6302413  -0.9234416   0.66312885]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 973. State = [[-0.17190976  0.00103159  0.28870243  1.        ]]. Action = [[ 0.5121856   0.5409539  -0.21211517  0.7937238 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 974. State = [[-0.16877927  0.01758869  0.2850326   1.        ]]. Action = [[-0.17837036  0.41363     0.09193051  0.50254583]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 975. State = [[-0.16145729  0.02236048  0.2815979   1.        ]]. Action = [[ 0.85527503 -0.36962545 -0.3466264   0.7973232 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 976. State = [[-0.14641097  0.0112284   0.2851831   1.        ]]. Action = [[-0.5084971  -0.37616736  0.69613993  0.6254399 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 977. State = [[-0.15429308  0.00283988  0.2877776   1.        ]]. Action = [[-0.30798024  0.06826925 -0.6881204   0.6995623 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 978. State = [[-0.16959248 -0.00794063  0.2744009   1.        ]]. Action = [[-0.73507446 -0.5147868  -0.45487565  0.6176783 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 979. State = [[-0.18322262 -0.01104297  0.2647806   1.        ]]. Action = [[0.46572304 0.45659137 0.2438736  0.64360666]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 980. State = [[-1.8311572e-01  6.9980393e-04  2.5410950e-01  1.0000000e+00]]. Action = [[-0.10660565  0.34481323 -0.80561644  0.6970382 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 981. State = [[-0.18114012  0.01074216  0.23898692  1.        ]]. Action = [[ 0.41628397 -0.71769404 -0.8590109   0.6457397 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 982. State = [[-0.18105537  0.01251943  0.23765981  1.        ]]. Action = [[ 0.32033467 -0.5464531  -0.07741296  0.54375505]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 983. State = [[-0.19219989 -0.00364974  0.2419792   1.        ]]. Action = [[-0.8155296  -0.8391687   0.37339282  0.20815253]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 984. State = [[-0.21762095 -0.02632136  0.23542887  1.        ]]. Action = [[-0.41895568 -0.21702182 -0.6970452   0.5343032 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 985. State = [[-0.21884818 -0.03273932  0.23094602  1.        ]]. Action = [[0.8758409  0.15330124 0.53273106 0.5240185 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 986. State = [[-0.21594098 -0.01783811  0.22234808  1.        ]]. Action = [[-0.21221215  0.7354884  -0.84924054  0.43605292]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 987. State = [[-0.22363192 -0.01932622  0.21006382  1.        ]]. Action = [[-0.7246574  -0.884676    0.35696054  0.66583204]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 988. State = [[-0.22858478 -0.02894877  0.21825516  1.        ]]. Action = [[0.5547497  0.22930646 0.52416134 0.5976262 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 989. State = [[-0.22600529 -0.02660849  0.22403166  1.        ]]. Action = [[-0.6988093  -0.6956141   0.73632884  0.4615562 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 990. State = [[-0.21692584 -0.04318335  0.2351694   1.        ]]. Action = [[ 0.59605265 -0.89402467  0.3824482   0.698266  ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 991. State = [[-0.21340388 -0.07870898  0.2584904   1.        ]]. Action = [[-0.6569225  -0.7163354   0.76513743  0.5789913 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 992. State = [[-0.22682771 -0.09444479  0.2748393   1.        ]]. Action = [[-0.2814244   0.2871101  -0.37878513  0.7283634 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 993. State = [[-0.22321181 -0.10060658  0.27076858  1.        ]]. Action = [[ 0.86527324 -0.39777625 -0.1618796   0.67990136]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 994. State = [[-0.20099741 -0.1167724   0.25781772  1.        ]]. Action = [[ 0.8775282  -0.58025086 -0.85651726  0.6469526 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 995. State = [[-0.16747358 -0.12195284  0.23290144  1.        ]]. Action = [[-0.04969674  0.76366687  0.05709565  0.76974154]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 996. State = [[-0.15303144 -0.12221224  0.2315131   1.        ]]. Action = [[ 0.930176   -0.94902223  0.11165297  0.77547383]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 997. State = [[-0.11215096 -0.14767599  0.24588066  1.        ]]. Action = [[ 0.9352683  -0.47884214  0.9839256   0.4610449 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 998. State = [[-0.07192017 -0.17346877  0.2729551   1.        ]]. Action = [[ 0.69950473 -0.5941958   0.34293985  0.43243778]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 999. State = [[-0.05889795 -0.18961395  0.2796293   1.        ]]. Action = [[-0.76816976  0.2247734  -0.3832879   0.7361307 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1000. State = [[-0.05218729 -0.19921213  0.28791907  1.        ]]. Action = [[ 0.9313636 -0.6108169  0.7275703  0.3731556]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1001. State = [[-0.02492646 -0.20314315  0.29945976  1.        ]]. Action = [[ 0.7130288   0.38421035 -0.2712912   0.6638706 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1001 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1001 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of -1
Current timestep = 1002. State = [[ 0.01572017 -0.19286826  0.29319933  1.        ]]. Action = [[ 0.982695    0.06212139 -0.20313233  0.79268646]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1003. State = [[-0.25079587  0.00240284  0.23265933  1.        ]]. Action = [[ 0.9450526  -0.05607408 -0.54010946 -0.05447388]]. Reward = [-1.]
Curr episode timestep = 89
Current timestep = 1004. State = [[-0.2529045   0.00151335  0.21839182  1.        ]]. Action = [[-0.41959804  0.20075059  0.49496555  0.8068317 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1005. State = [[-0.24003717 -0.014423    0.22322688  1.        ]]. Action = [[ 0.7291963 -0.8294692  0.8648524  0.7114916]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1006. State = [[-0.22692211 -0.03337673  0.23676331  1.        ]]. Action = [[-0.7311469  -0.16847855  0.9058206   0.69362295]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1007. State = [[-0.22105502 -0.02904305  0.22842172  1.        ]]. Action = [[ 0.55517316  0.40170836 -0.94368166  0.75259423]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1008. State = [[-0.19702606 -0.03922235  0.20939074  1.        ]]. Action = [[ 0.3620993  -0.7910043  -0.01906174  0.6767701 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1009. State = [[-0.19494712 -0.05934897  0.21492687  1.        ]]. Action = [[-0.91539574 -0.30270004  0.7159376   0.6353414 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1010. State = [[-0.19844319 -0.06283946  0.24207518  1.        ]]. Action = [[0.9018619  0.40145183 0.80665994 0.54038465]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1011. State = [[-0.18904932 -0.04655078  0.27164575  1.        ]]. Action = [[-0.41904664  0.49640596  0.44353914  0.6598438 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1012. State = [[-0.2025344  -0.03894891  0.30044088  1.        ]]. Action = [[-0.88309115 -0.29277843  0.81036246  0.73558307]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1013. State = [[-0.22313072 -0.04491588  0.3313973   1.        ]]. Action = [[ 0.10793781 -0.09099251  0.01694798  0.4864639 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1014. State = [[-0.23529793 -0.03333214  0.34417868  1.        ]]. Action = [[-0.75870794  0.70540166  0.6874535   0.66876817]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1015. State = [[-0.26131582 -0.03565456  0.37634957  1.        ]]. Action = [[-0.45171523 -0.9466619   0.3252877   0.4262079 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1016. State = [[-0.2680298  -0.03828907  0.395772    1.        ]]. Action = [[0.46071112 0.750453   0.5587001  0.4528376 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1017. State = [[-0.26465508 -0.0258126   0.4113336   1.        ]]. Action = [[-0.1840778 -0.9229725  0.5544418  0.6792095]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 1018. State = [[-0.26705813 -0.01924877  0.4039537   1.        ]]. Action = [[-0.07861215  0.26746964 -0.96817523  0.5811025 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1019. State = [[-0.26622286 -0.01325168  0.39107174  1.        ]]. Action = [[ 0.05879879 -0.3728205   0.31629634  0.79484916]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1020. State = [[-0.2661276  -0.01225628  0.38905263  1.        ]]. Action = [[-0.29248953 -0.917965   -0.22739625  0.9009541 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1021. State = [[-0.26618114 -0.01217873  0.38905722  1.        ]]. Action = [[0.10046113 0.00082994 0.04087257 0.815799  ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1022. State = [[-0.26607066 -0.01218771  0.38905653  1.        ]]. Action = [[-0.22797734  0.5652312   0.4047476   0.6230202 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1023. State = [[-0.26603353 -0.01213353  0.38905177  1.        ]]. Action = [[ 0.5227747  -0.3441924   0.53419566  0.65057635]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1024. State = [[-0.26603353 -0.01213353  0.38905177  1.        ]]. Action = [[ 0.7091131  -0.56948113  0.62912655  0.8480985 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 1025. State = [[-0.261935   -0.01079009  0.3813021   1.        ]]. Action = [[ 0.2939198   0.11734629 -0.4356599   0.6291251 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1026. State = [[-0.25403604 -0.00915475  0.36475953  1.        ]]. Action = [[-0.6270937   0.43157697 -0.52013296  0.5343584 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1027. State = [[-0.25245965 -0.0085245   0.36254644  1.        ]]. Action = [[ 0.87278914 -0.49237913  0.936784    0.69294345]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1028. State = [[-0.25239488 -0.00832788  0.36262414  1.        ]]. Action = [[-0.84327596 -0.49823117  0.7508707   0.66864467]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 1029. State = [[-0.25239488 -0.00832788  0.36262414  1.        ]]. Action = [[-0.6801451   0.8976946   0.39515948  0.78385687]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1030. State = [[-0.24805926  0.01142282  0.3676703   1.        ]]. Action = [[0.3165635  0.967288   0.4474666  0.80627966]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1031. State = [[-0.24730824  0.03317807  0.3742343   1.        ]]. Action = [[ 0.68791366  0.658247    0.9473951  -0.06567383]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1032. State = [[-0.23451637  0.02232511  0.3784054   1.        ]]. Action = [[ 0.75191903 -0.82902277  0.04000199  0.7857554 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1033. State = [[-0.20895498 -0.00494377  0.3720597   1.        ]]. Action = [[ 0.38048398 -0.65481865 -0.3875805   0.52845097]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1034. State = [[-0.18386786 -0.04006931  0.36472753  1.        ]]. Action = [[ 0.6338475  -0.85252213 -0.00852913  0.83347523]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1035. State = [[-0.15392692 -0.05787969  0.34954676  1.        ]]. Action = [[ 0.8658471   0.28352666 -0.90450853  0.5920849 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1036. State = [[-0.11734612 -0.05951386  0.3108074   1.        ]]. Action = [[ 0.6728859  -0.12701279 -0.8616492   0.5182891 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1037. State = [[-0.07782491 -0.07897289  0.2990561   1.        ]]. Action = [[ 0.81982267 -0.87500465  0.90070105  0.8723024 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1038. State = [[-0.06339743 -0.10113403  0.3260584   1.        ]]. Action = [[-0.8809601   0.00105703  0.70681     0.50150704]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1039. State = [[-0.06790838 -0.10529257  0.33716252  1.        ]]. Action = [[ 0.62814915  0.03346705 -0.55199677  0.5179224 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1040. State = [[-0.06634638 -0.10074469  0.34393838  1.        ]]. Action = [[-0.9827247   0.24625838  0.8519647   0.924958  ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1041. State = [[-0.09460045 -0.11693383  0.36596653  1.        ]]. Action = [[-0.9623674  -0.95177025 -0.21678191  0.5647733 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1042. State = [[-0.12378755 -0.13739349  0.36480364  1.        ]]. Action = [[-0.7949123   0.6651192   0.8500004   0.59053636]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 1043. State = [[-0.14022614 -0.13339762  0.35899037  1.        ]]. Action = [[-0.66995966  0.40324116 -0.3431968   0.5756639 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1044. State = [[-0.14791809 -0.12620385  0.35519916  1.        ]]. Action = [[ 0.874913   -0.12657297  0.24786997  0.59741783]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1045. State = [[-0.1374032  -0.11950382  0.34342057  1.        ]]. Action = [[ 0.2923528   0.28862524 -0.9597022   0.3732965 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1046. State = [[-0.11891742 -0.10253121  0.30978036  1.        ]]. Action = [[ 0.5967628   0.54979813 -0.42523825  0.52808857]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1047. State = [[-0.10268852 -0.07717573  0.2798417   1.        ]]. Action = [[ 0.00670326  0.69728565 -0.7200753   0.88327646]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1048. State = [[-0.08648925 -0.05265163  0.27000207  1.        ]]. Action = [[0.5252948  0.31503034 0.627655   0.71878076]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1049. State = [[-0.0669768  -0.04542545  0.29404905  1.        ]]. Action = [[ 0.47364724 -0.2455002   0.9229667   0.648584  ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1050. State = [[-0.05636593 -0.03218699  0.30937684  1.        ]]. Action = [[-0.16676867  0.7486305  -0.42481768  0.751374  ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1051. State = [[-0.06001913 -0.03454888  0.29670215  1.        ]]. Action = [[ 0.02903986 -0.8652645  -0.8149133   0.5745485 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1052. State = [[-0.04880926 -0.06126884  0.2660799   1.        ]]. Action = [[ 0.7035396  -0.7162666  -0.8658997   0.71452665]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1053. State = [[-0.02555699 -0.08668507  0.24028033  1.        ]]. Action = [[ 0.08114505 -0.17734116  0.46193635  0.65586495]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1054. State = [[-0.01247038 -0.08166822  0.2466265   1.        ]]. Action = [[0.48786402 0.6334883  0.05215156 0.6766032 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1055. State = [[-0.00558244 -0.06339839  0.26197615  1.        ]]. Action = [[-0.91905004  0.41307998  0.8914585   0.76189613]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1056. State = [[-0.02525348 -0.0691724   0.29456383  1.        ]]. Action = [[-0.546375   -0.8657083   0.31645298  0.7630875 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1057. State = [[-0.05183728 -0.07699157  0.3120325   1.        ]]. Action = [[-0.7861767   0.41080475  0.2542113   0.74767613]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1058. State = [[-0.07034787 -0.06634562  0.32213625  1.        ]]. Action = [[0.6762527  0.23035145 0.09825039 0.43437958]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1059. State = [[-0.05742079 -0.05460674  0.32990497  1.        ]]. Action = [[0.68362045 0.29953766 0.29452193 0.67504644]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1060. State = [[-0.0375927  -0.03790718  0.34895745  1.        ]]. Action = [[0.2033242  0.33475602 0.7078972  0.66753423]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1061. State = [[-0.03632412 -0.02724816  0.3778369   1.        ]]. Action = [[-0.6705476   0.07675195  0.5433605   0.45373452]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1062. State = [[-0.04986298 -0.01537499  0.3914478   1.        ]]. Action = [[-0.41289467  0.35389936 -0.29988807  0.6269083 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1063. State = [[-0.05953911 -0.00589773  0.39066133  1.        ]]. Action = [[0.90443873 0.8220943  0.449566   0.8340242 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 1064. State = [[-0.06055908 -0.00509217  0.39026678  1.        ]]. Action = [[ 0.5467024  -0.08168453  0.9107063   0.77092576]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 1065. State = [[-0.06809556 -0.00211137  0.3869018   1.        ]]. Action = [[-0.5266045   0.11020112 -0.20494878  0.46785593]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1066. State = [[-0.07518619  0.01664159  0.37616348  1.        ]]. Action = [[ 0.9683161   0.79135776 -0.45625645  0.8066039 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1067. State = [[-0.05457161  0.02478704  0.35452512  1.        ]]. Action = [[ 0.6069131  -0.41707337 -0.39556015  0.5253463 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1068. State = [[-0.03502129  0.02668259  0.3506235   1.        ]]. Action = [[-0.06622779  0.27761018  0.62069404  0.6635866 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1069. State = [[-0.0242422   0.01706764  0.35426652  1.        ]]. Action = [[ 0.9117117 -0.8663693 -0.3714912  0.8254373]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1070. State = [[ 0.01459245 -0.0010989   0.35406488  1.        ]]. Action = [[0.7678175  0.08294964 0.4806075  0.70901287]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1071. State = [[3.3861227e-02 1.5819145e-04 3.5999528e-01 1.0000000e+00]]. Action = [[-0.50416744  0.19443464 -0.22721541  0.8628659 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1072. State = [[0.02938592 0.01975329 0.34983537 1.        ]]. Action = [[-0.15444624  0.9374058  -0.7267677   0.7180315 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1073. State = [[0.02223365 0.05079347 0.3372452  1.        ]]. Action = [[-0.24267554  0.3033669   0.45738292  0.7189722 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1074. State = [[0.01590651 0.07586113 0.3523455  1.        ]]. Action = [[-0.23133826  0.58432543  0.45319796  0.17978156]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1075. State = [[-3.81380705e-05  1.06476925e-01  3.53225440e-01  1.00000000e+00]]. Action = [[-0.7715953   0.716537   -0.81440336  0.53345907]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1076. State = [[-0.01062566  0.14271899  0.3514633   1.        ]]. Action = [[0.9666743  0.73617435 0.8105619  0.76862884]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1077. State = [[0.0081137  0.14849064 0.37462878 1.        ]]. Action = [[ 0.8694985  -0.5926791   0.75806904  0.589368  ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1078. State = [[0.0383483  0.12094751 0.378369   1.        ]]. Action = [[ 0.85889864 -0.76726586 -0.76130813  0.47975826]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1079. State = [[0.0662617  0.10286789 0.36718097 1.        ]]. Action = [[-0.834011   -0.06301671  0.14451051  0.8980495 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1079 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1079 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of 0
Current timestep = 1080. State = [[0.0626286  0.09856094 0.373      1.        ]]. Action = [[-0.35567456  0.90405476  0.9573133   0.64916897]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 1081. State = [[0.05256419 0.09354243 0.3699793  1.        ]]. Action = [[-0.9364006  -0.30167544 -0.32294655  0.31809282]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1082. State = [[0.02531955 0.10623278 0.35967532 1.        ]]. Action = [[-0.5810676   0.87694955 -0.4185704   0.71259165]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1083. State = [[-0.00684081  0.13884805  0.35911414  1.        ]]. Action = [[-0.6245387   0.5336566   0.60976136  0.41485727]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1084. State = [[-0.03725884  0.15777892  0.35750574  1.        ]]. Action = [[-0.28856117  0.14482057 -0.9040749   0.5034244 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1085. State = [[-0.04493103  0.17066224  0.34993887  1.        ]]. Action = [[0.5734576 0.4437381 0.6357131 0.7904575]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1086. State = [[-0.04734206  0.16072161  0.3629301   1.        ]]. Action = [[-0.68900484 -0.9342743   0.26035762  0.5135157 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1087. State = [[-0.06400124  0.13640542  0.384785    1.        ]]. Action = [[-0.66841644 -0.37743205  0.6270982   0.58723164]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1087 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 1087 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of -1
Current timestep = 1088. State = [[-0.09184218  0.11907165  0.40644762  1.        ]]. Action = [[0.8484945 0.0752995 0.5621569 0.71148  ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 1089. State = [[-0.09549548  0.1317509   0.40234935  1.        ]]. Action = [[-0.01999462  0.70188737 -0.2836544   0.65608525]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1089 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 1089 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of -1
Current timestep = 1090. State = [[-0.10957067  0.1376489   0.38856518  1.        ]]. Action = [[-0.561504   -0.71160656 -0.44116032  0.77829015]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1090 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 1090 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1090 of 1
Current timestep = 1091. State = [[-0.12030251  0.12119643  0.37615594  1.        ]]. Action = [[ 0.59164953 -0.11708903  0.79102516  0.26083267]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 1092. State = [[-0.12030251  0.12119643  0.37615594  1.        ]]. Action = [[ 0.72661924 -0.8150126   0.9740269   0.55500746]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 1093. State = [[-0.12859131  0.12439576  0.38574246  1.        ]]. Action = [[-0.70093465  0.10626948  0.49610174  0.72199845]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1094. State = [[-0.15043186  0.10829841  0.40353593  1.        ]]. Action = [[ 0.3475896  -0.96185887  0.41857338  0.6971092 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1095. State = [[-0.14802681  0.08775681  0.41152325  1.        ]]. Action = [[-0.89019674 -0.3055967   0.9619913   0.643458  ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 1096. State = [[-0.15358716  0.0931247   0.41103777  1.        ]]. Action = [[-0.32624894  0.5578549  -0.25555313  0.6761441 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1097. State = [[-0.1719545   0.09925359  0.40493068  1.        ]]. Action = [[-0.83453566 -0.18705487 -0.2606963   0.8789966 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1098. State = [[-0.19391696  0.09729226  0.39738882  1.        ]]. Action = [[0.69752336 0.6913054  0.5854535  0.5777075 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 1099. State = [[-0.19886374  0.09200094  0.39258713  1.        ]]. Action = [[-0.24305719 -0.20395297 -0.3623247   0.8134192 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1100. State = [[-0.20178981  0.07654142  0.38528606  1.        ]]. Action = [[ 0.7188239  -0.47914642  0.04665649  0.47783315]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1101. State = [[-0.19520816  0.06385086  0.3846469   1.        ]]. Action = [[-0.4963355  -0.43965554  0.96692514  0.5823195 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 1102. State = [[-0.18877903  0.06464884  0.38589662  1.        ]]. Action = [[0.38208067 0.11707151 0.10419142 0.5189142 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1103. State = [[-0.18293008  0.06591015  0.3825404   1.        ]]. Action = [[-0.20068699 -0.42105567  0.6292796   0.72268486]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 1104. State = [[-0.17866689  0.07260908  0.37962535  1.        ]]. Action = [[ 0.12288618  0.37794638 -0.2592293   0.40268874]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1105. State = [[-0.17465548  0.08050582  0.3750022   1.        ]]. Action = [[ 0.45271063 -0.6965895   0.73998547  0.5214727 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 1106. State = [[-0.16076389  0.08918677  0.37296376  1.        ]]. Action = [[ 0.8205509   0.32826984 -0.04444903  0.39087558]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1107. State = [[-0.12615733  0.08859394  0.36601794  1.        ]]. Action = [[ 0.97705436 -0.5005551   0.01814222  0.6985743 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1108. State = [[-0.08584482  0.09270465  0.37587053  1.        ]]. Action = [[0.43215978 0.6948662  0.79360664 0.4942751 ]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1109. State = [[-0.06783967  0.10606021  0.39206332  1.        ]]. Action = [[ 0.33757222 -0.21900654  0.7858553   0.56974113]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: Workspace boundary
Current timestep = 1110. State = [[-0.06219798  0.09926252  0.38445994  1.        ]]. Action = [[ 0.8600317  -0.711349   -0.72460485  0.77850974]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1111. State = [[-0.02935818  0.0712347   0.35445207  1.        ]]. Action = [[-0.00225788 -0.29219204 -0.6982582   0.7665831 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1112. State = [[-0.01638369  0.07126436  0.32460004  1.        ]]. Action = [[ 0.21415353  0.39853    -0.62378466  0.77321506]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1113. State = [[-3.4960586e-04  8.2680739e-02  2.9304585e-01  1.0000000e+00]]. Action = [[ 0.7361758   0.15473473 -0.9637016   0.7667979 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1114. State = [[0.01594195 0.06905942 0.24340348 1.        ]]. Action = [[-0.8963465  -0.8807747  -0.93384683  0.5697534 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1115. State = [[0.01192203 0.05970448 0.22942564 1.        ]]. Action = [[0.30478215 0.46533477 0.7669947  0.70466566]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1116. State = [[0.00878552 0.04511179 0.24840514 1.        ]]. Action = [[-0.44171214 -0.9282663   0.51532495  0.752427  ]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1117. State = [[-0.00411231  0.00843426  0.2778234   1.        ]]. Action = [[-0.11064166 -0.9512297   0.7708223   0.52721393]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1118. State = [[-0.01194192 -0.01415224  0.2868042   1.        ]]. Action = [[ 0.00681639  0.06146061 -0.77294946  0.72173095]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1119. State = [[-0.01392029 -0.01474289  0.28513983  1.        ]]. Action = [[-0.06820518  0.21167803  0.5888258   0.7978616 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1120. State = [[-0.01081878 -0.02359329  0.2802513  -1.        ]]. Action = [[ 0.6792958  -0.6165634  -0.80921656 -0.03667569]]. Reward = [1000.]
Curr episode timestep = 116
Scene graph at timestep 1120 is [False, True, False, False, True, False, False, True, False, True]
State prediction error at timestep 1120 is tensor(0.9615, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1120 of 1
Current timestep = 1121. State = [[ 0.00656913 -0.0367985   0.26310414  1.        ]]. Action = [[ 0.01238978  0.15178668 -0.00607377  0.85800505]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1122. State = [[ 0.00666281 -0.02720986  0.2595632   1.        ]]. Action = [[-0.20012927  0.38682234 -0.24159181  0.70809984]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1123. State = [[-0.00526149 -0.0386992   0.2556822   1.        ]]. Action = [[-0.8332661  -0.9248966  -0.07962316  0.5079366 ]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1124. State = [[-0.01795526 -0.04047778  0.25537148  1.        ]]. Action = [[0.15256345 0.96123624 0.38658404 0.58075   ]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1125. State = [[-0.02168103 -0.00356312  0.26487488  1.        ]]. Action = [[-0.14814526  0.8562838   0.3755399   0.7668047 ]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1126. State = [[-0.02926365  0.02148701  0.27257335  1.        ]]. Action = [[ 0.54317164 -0.39727503 -0.66893655  0.68317294]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: No entry zone
Current timestep = 1127. State = [[-0.02380544  0.04117314  0.27114648  1.        ]]. Action = [[ 0.53491795  0.7727244  -0.2429393   0.47986066]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1128. State = [[-0.01544662  0.05778937  0.2690603   1.        ]]. Action = [[ 0.649868   -0.38690293 -0.7856286   0.69184804]]. Reward = [0.]
Curr episode timestep = 124
Action ignored: No entry zone
Current timestep = 1129. State = [[-0.00592601  0.07427677  0.26706287  1.        ]]. Action = [[ 0.61539614  0.52113855 -0.10412121  0.59657145]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1130. State = [[-0.2506754   0.00260902  0.23271658  1.        ]]. Action = [[-0.04429853  0.67976356  0.24059081  0.61144984]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1131. State = [[-0.24495424 -0.01018568  0.20603551  1.        ]]. Action = [[ 0.6267977 -0.6418018 -0.7097947  0.6596606]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1132. State = [[-0.22376548 -0.02019694  0.16501795  1.        ]]. Action = [[ 0.731426    0.17096758 -0.88439655  0.51234865]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1133. State = [[-0.20947254 -0.01968898  0.12491971  1.        ]]. Action = [[-0.48169756  0.0999707  -0.5516369   0.1299845 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1134. State = [[-0.21358635 -0.01862182  0.1041621   1.        ]]. Action = [[ 0.75689244 -0.13112777 -0.67282933  0.7806426 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: No entry zone
Current timestep = 1135. State = [[-0.21413378 -0.01859767  0.1021475   1.        ]]. Action = [[ 0.94119906 -0.596462   -0.83212364  0.8097608 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 1136. State = [[-0.2140176  -0.01853013  0.10215071  1.        ]]. Action = [[ 0.92066264 -0.90256155  0.37377918  0.12395644]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 1137. State = [[-0.2140176  -0.01853013  0.10215071  1.        ]]. Action = [[ 0.8903456  -0.8022225  -0.02006453  0.6681788 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 1138. State = [[-0.21419543 -0.0269748   0.09343974  1.        ]]. Action = [[ 0.14975047 -0.49773782 -0.69411224  0.4807942 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1139. State = [[-0.22339201 -0.02056185  0.05954963  1.        ]]. Action = [[-0.44094884  0.79788697 -0.8918088   0.56745684]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1140. State = [[-0.23588054 -0.02421906  0.02857946  1.        ]]. Action = [[-0.1923728  -0.884132   -0.04730296  0.29156995]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1141. State = [[-0.23921178 -0.03731887  0.02287102  1.        ]]. Action = [[ 0.9347086  -0.40910685 -0.94618165  0.30071437]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1142. State = [[-0.23948865 -0.04045611  0.02221153  1.        ]]. Action = [[ 0.9003229  -0.85907483 -0.68322253 -0.13222623]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 1143. State = [[-0.23938218 -0.04086581  0.02218357  1.        ]]. Action = [[ 0.7927251  -0.9841835  -0.80295885  0.6396792 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 1144. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.96947026 -0.80625474 -0.86908644  0.16453183]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 1145. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.97682977 -0.92889553 -0.9122149   0.00556874]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 1146. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.98576343 -0.899519   -0.90571314  0.17192483]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1147. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.95550776 -0.8511692  -0.945449    0.00506306]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1148. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9645872 -0.9443387 -0.9278502 -0.2192415]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 1149. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9240637  -0.9120179  -0.9865926   0.01888001]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1150. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.981488   -0.7271538  -0.97128916 -0.06301308]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1151. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8812177  -0.8738981  -0.94212705 -0.00845605]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 1152. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7443303 -0.9861858 -0.8640934 -0.0474785]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 1153. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.93523264 -0.7219432  -0.9838447  -0.07707965]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1154. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.89956    -0.39949536 -0.9760328   0.12806642]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1155. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.97752166 -0.886667   -0.8281791   0.00229251]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 1156. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.78203773 -0.9318834  -0.91519344  0.00342155]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1157. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.93412244 -0.24152344 -0.97149336  0.03692496]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1158. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.73273087 -0.544375   -0.98751146 -0.02793598]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1159. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8072219  -0.8278872  -0.95612216  0.04687119]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 1160. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.96353185 -0.89827245 -0.9018194  -0.08954769]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 1161. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.97578335 -0.9463358  -0.9831799   0.0016427 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 1162. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8471353  -0.9602097  -0.83588403 -0.13690645]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 1163. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9480057  -0.9633749  -0.9471974  -0.06243503]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 1164. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9820719  -0.7831558  -0.89483136 -0.06942809]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 1165. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.6560173  -0.9882223  -0.97843575 -0.08900571]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 1166. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8429184  -0.90910953 -0.9477508  -0.05774206]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 1167. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.85480833 -0.8055906  -0.89113915 -0.07041001]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Current timestep = 1168. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.49431062 -0.8875031  -0.63230705  0.01786375]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 1169. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8971522  -0.9192605  -0.8457926  -0.02723801]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 1170. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.92315817 -0.87069577 -0.98269945 -0.04007089]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 1171. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.70024943 -0.8633815  -0.9108763  -0.04167849]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 1172. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9195199  -0.8006816  -0.8982885   0.01298976]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 1173. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.53945434 -0.9659961  -0.9617101  -0.01630622]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 1174. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7064159  -0.93992    -0.89840806 -0.06275475]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 1175. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.88402796 -0.95022315 -0.85798764 -0.01700079]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 1176. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.90910006 -0.8563312  -0.9738127  -0.10699391]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 1177. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.961946   -0.9514618  -0.881848   -0.02182043]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 1178. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 9.7454107e-01 -8.9142925e-01 -9.8083675e-01  5.9151649e-04]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 1179. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.884145   -0.81666476 -0.95333576 -0.04061371]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 1180. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9723568   0.15176344 -0.7912197  -0.02988344]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 1181. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9670558  -0.66345197 -0.45738548 -0.11041868]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 1182. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9102099  -0.82197917 -0.8465115   0.00159347]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 1183. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.90155673 -0.8950348  -0.94703203 -0.0288893 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 1184. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.837255   -0.9448146  -0.93210167  0.06058574]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 1185. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.98586845 -0.93074435 -0.7516161  -0.11982179]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 1186. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.76068115 -0.92438596 -0.9637875  -0.06050324]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 1187. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.83967495 -0.97254324 -0.94963866 -0.12637818]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 1188. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.92078686 -0.83732945 -0.9798587  -0.09346747]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 1189. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 8.3603501e-01 -7.6244080e-01 -9.2744648e-01  5.6385994e-04]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 1190. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.86550426 -0.7128329  -0.9906262  -0.04923224]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 1191. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9203172  -0.8994548  -0.977267   -0.09868211]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 1192. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.82839143 -0.8833312  -0.9740041  -0.13150859]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 1193. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.91033864 -0.81074554 -0.9125575  -0.01443934]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 1194. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9887706  -0.8693947  -0.9486247  -0.04960167]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 1195. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9476501  -0.92626995 -0.956485   -0.03835684]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 1196. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8561597 -0.6457157 -0.9446493 -0.0299148]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 1197. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.95224    -0.7930407  -0.97212505  0.05001569]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 1198. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9623728  -0.5153598  -0.9724025  -0.07419795]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 1199. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.85322356 -0.7821138  -0.9311573   0.01369083]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 1200. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.93165016 -0.88907325 -0.9390858   0.0036335 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 1201. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.97240496 -0.7389865  -0.9830172   0.01716065]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 1202. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.95336604 -0.046152   -0.86850625  0.05307639]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 1203. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.884902   -0.8058183  -0.9354141   0.07299531]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 1204. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.50661266 -0.94544685 -0.97582585  0.08911812]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 1205. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8041134  -0.82825446 -0.98906624  0.04810953]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 1206. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.73381567 -0.7950737  -0.9196435   0.06544375]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 1207. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9689752  -0.74438643 -0.9604686   0.04360998]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 1208. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.74941564 -0.8867045  -0.7916547   0.02997887]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 1209. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.68758464 -0.786492   -0.9594084   0.10811234]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 1210. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.934579   -0.73204553 -0.93444073  0.10716426]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 1211. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.91853356 -0.7805274  -0.9860822   0.03199911]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 1212. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.50703466 -0.7535475  -0.97329193  0.05499661]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 1213. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7817211  -0.78847295 -0.6867429   0.13047087]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 1214. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7618575  -0.85569793 -0.9670884   0.03133655]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 1215. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.95856977 -0.9168457  -0.92456895  0.04892194]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 1216. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.85371304 -0.92990696 -0.7344128   0.06168783]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 1217. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.91208434 -0.8506556  -0.90317196  0.07360649]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 1218. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8027954  -0.7204305  -0.9825847   0.04670358]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 1219. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.9094622  -0.9590951  -0.9544901   0.08356488]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 1220. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.84853816 -0.91012204 -0.91170913  0.06832325]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 1221. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.68471813 -0.76982176 -0.9416541   0.09344232]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 1222. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8368077  -0.9040877  -0.9691917   0.01326597]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 1223. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7773361  -0.924721   -0.9606347   0.05409765]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 1224. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7436037  -0.8943194  -0.9391876   0.01163661]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 1225. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.63702214 -0.51925206 -0.9641824   0.05134439]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 1226. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.87423444 -0.7655378  -0.98106194  0.04368639]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 1227. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8699372  -0.91600096 -0.87574506 -0.05155438]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 1228. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.6827986  -0.80095536 -0.9832077   0.07716131]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 1229. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.86322546 -0.9382687  -0.92858607 -0.04113758]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 1230. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8517411  -0.9418148  -0.9143022   0.05618012]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 1231. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.95158505 -0.98636407 -0.90103745  0.0525912 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 1232. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.73765874 -0.80197537 -0.97230685  0.03053463]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 1233. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.890452   -0.54639816 -0.9880328   0.04912448]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: Workspace boundary
Current timestep = 1234. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8699777  -0.84098375 -0.9167846   0.08588922]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Current timestep = 1235. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8388481  -0.55274045 -0.9144636  -0.00162631]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Current timestep = 1236. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.92553425 -0.77789086 -0.9134533   0.08925605]]. Reward = [0.]
Curr episode timestep = 105
Action ignored: Workspace boundary
Current timestep = 1237. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.67367315 -0.6494467  -0.99544066  0.06555951]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Current timestep = 1238. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.89999735 -0.56463206 -0.98329955  0.08560574]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Current timestep = 1239. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7137306  -0.89817816 -0.9675455   0.12936461]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: Workspace boundary
Current timestep = 1240. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.5276942  -0.806511   -0.9552804   0.10611463]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: Workspace boundary
Current timestep = 1241. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8703151  -0.90705204 -0.9408      0.07848167]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: Workspace boundary
Current timestep = 1242. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[-0.4284153  -0.86260504 -0.9153916   0.13701642]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: Workspace boundary
Current timestep = 1243. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7684746  -0.8784573  -0.95286274  0.16531265]]. Reward = [0.]
Curr episode timestep = 112
Action ignored: Workspace boundary
Current timestep = 1244. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.8298154  -0.9424781  -0.9474171   0.13463283]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Current timestep = 1245. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7582475  -0.990844   -0.974144    0.12673533]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Current timestep = 1246. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.84186804 -0.9607304  -0.97659516  0.10549593]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Current timestep = 1247. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.84441507 -0.43755156 -0.97779995  0.140419  ]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: Workspace boundary
Current timestep = 1248. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.7156615 -0.8129361 -0.9713804  0.185691 ]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Current timestep = 1249. State = [[-0.2393546  -0.04086771  0.0221834   1.        ]]. Action = [[ 0.91542864 -0.7898252  -0.9168923   0.2059815 ]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: Workspace boundary
