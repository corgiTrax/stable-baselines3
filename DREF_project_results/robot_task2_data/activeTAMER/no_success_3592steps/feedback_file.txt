Current timestep = 0. State = [[-0.24292658 -0.04958475  0.11582063  1.        ]]. Action = [[ 0.6785432  -0.29487896  0.85969734  0.00305367]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3814, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.23071094 -0.0570488   0.12458712  1.        ]]. Action = [[-0.59439504  0.05494308 -0.17432624  0.4950707 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3627, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.23177728 -0.05712238  0.12431888  1.        ]]. Action = [[-0.9715459   0.6843103   0.86302567  0.14900196]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3283, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.2552647   0.00450156  0.12350445  1.        ]]. Action = [[-0.6735448   0.45129037 -0.67825764 -0.74707097]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4. State = [[-0.25629106  0.00475336  0.10988861  1.        ]]. Action = [[-0.51753527 -0.16708928 -0.29554212 -0.58133245]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2771, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.2562046   0.00481402  0.10988511  1.        ]]. Action = [[-0.7911224   0.23747194 -0.0070219   0.23044944]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2614, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.25619775  0.00493555  0.10987853  1.        ]]. Action = [[-0.5631338  -0.9290398   0.07216942 -0.71884084]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.25619775  0.00493555  0.10987853  1.        ]]. Action = [[-0.7798003  -0.97615993 -0.7634469  -0.34011865]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.1916, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25619775  0.00493555  0.10987853  1.        ]]. Action = [[-0.8030978  -0.44650114 -0.07180071 -0.45551294]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1806, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.2610065   0.04959501  0.12251875  1.        ]]. Action = [[ 0.18606842 -0.6249492  -0.59180707 -0.7886304 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 10. State = [[-0.24723946  0.05737338  0.10836144  1.        ]]. Action = [[ 0.9088843   0.12602592 -0.19889057  0.30505252]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1326, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.21783027  0.06149315  0.10220152  1.        ]]. Action = [[ 0.6844121  -0.03073871  0.07615745  0.64965177]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1189, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 0
Current timestep = 12. State = [[-0.25888792 -0.04775685  0.12165073  1.        ]]. Action = [[ 0.6757295   0.97705686  0.5306903  -0.8560925 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 13. State = [[-0.25958893 -0.05291932  0.10841898  1.        ]]. Action = [[-0.62077576 -0.49489462  0.188707    0.8257376 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.0819, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.25748295 -0.05772202  0.11516508  1.        ]]. Action = [[-0.14251381 -0.18217385  0.91567636  0.7352927 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0576, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.24595965 -0.10316768  0.1233511   1.        ]]. Action = [[ 0.65038764  0.92155814 -0.04648447 -0.11841679]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 16. State = [[-0.24092959 -0.12387136  0.11719465  1.        ]]. Action = [[ 0.35898495 -0.61623216  0.87208104  0.98840404]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0295, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.22478034 -0.12959038  0.11945283  1.        ]]. Action = [[ 0.9279363   0.6094016  -0.91503245  0.10875809]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0394, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of 0
Current timestep = 18. State = [[-0.2617608   0.03886208  0.12142094  1.        ]]. Action = [[-0.48802066  0.2846893  -0.18870038 -0.6614008 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 19. State = [[-0.25970733  0.04425833  0.10876936  1.        ]]. Action = [[-0.8330219  -0.8560046   0.25748277  0.04713392]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.25970733  0.04425833  0.10876936  1.        ]]. Action = [[-0.21383625 -0.8469914   0.42958856 -0.39332515]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 20 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 20 is tensor(0.0230, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.25401694  0.04185404  0.1137701   1.        ]]. Action = [[ 0.3197286  -0.22474611  0.7414191   0.9611224 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.25045508  0.05136123  0.1211516   1.        ]]. Action = [[-0.12557805  0.81528044  0.12768471  0.29789877]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0422, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.2620354   0.15752876  0.120873    1.        ]]. Action = [[ 0.5650692 -0.5003895 -0.751789  -0.3190199]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 24. State = [[-0.2611951   0.1743567   0.10749951  1.        ]]. Action = [[-0.8324397   0.87414694  0.01562715 -0.13856173]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0473, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.25524458  0.1679727   0.11584066  1.        ]]. Action = [[ 0.08265269 -0.49578142  0.979388    0.57828116]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 25 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0338, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.24563284  0.16137451  0.12495769  1.        ]]. Action = [[ 0.6096425   0.07258534 -0.5265394   0.28502762]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 26 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0528, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of 0
Current timestep = 27. State = [[-0.25590497 -0.09951023  0.11610056  1.        ]]. Action = [[-0.523327    0.41353428  0.7693958  -0.397992  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 28. State = [[-0.25706273 -0.10942465  0.10332645  1.        ]]. Action = [[-0.40953827 -0.31664956  0.13683605  0.9402778 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.25706273 -0.10942465  0.10332645  1.        ]]. Action = [[-0.30289435 -0.75893337 -0.41398412  0.8241966 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0395, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.24737051 -0.10897163  0.10286934  1.        ]]. Action = [[ 0.7873845   0.05722857 -0.10952288  0.37371206]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.22070286 -0.11039565  0.10550176  1.        ]]. Action = [[0.9270259  0.09516692 0.8652458  0.29902434]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0466, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.195156   -0.10305747  0.11873589  1.        ]]. Action = [[0.3109336  0.54008067 0.17574167 0.77857804]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0609, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.26090854  0.13516437  0.12194893  1.        ]]. Action = [[-0.46632618  0.4447254   0.9377489  -0.67673457]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 34. State = [[-0.2584625   0.15078573  0.10848396  1.        ]]. Action = [[-0.4743485  -0.69011116 -0.829823   -0.7938165 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0466, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.25164235  0.14025404  0.11021887  1.        ]]. Action = [[ 0.3792045 -0.7411334  0.350114   0.8307313]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0605, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.24243763  0.12685739  0.11319166  1.        ]]. Action = [[-0.5512807   0.37674546  0.8523247   0.74834037]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 36 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0521, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.23934993  0.1115242   0.10954847  1.        ]]. Action = [[ 0.11061728 -0.9976256  -0.44234157  0.36470664]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[-0.25821307  0.0122256   0.11867835  1.        ]]. Action = [[-0.06866038 -0.7892889  -0.01614338 -0.31161368]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 39. State = [[-0.25890452  0.15250365  0.12148131  1.        ]]. Action = [[ 0.45324826 -0.27372175  0.6998713  -0.77595854]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 40. State = [[-0.24454077  0.16205749  0.10792278  1.        ]]. Action = [[ 0.9070685  -0.47475857 -0.03200018  0.1375593 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0728, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 0
Current timestep = 41. State = [[-0.23138185  0.16123694  0.10068797  1.        ]]. Action = [[-0.4986074   0.5327368  -0.29801667  0.9818866 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0700, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.26315197  0.18096758  0.11955288  1.        ]]. Action = [[-0.10765648 -0.3400731  -0.29324758 -0.4307341 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 43. State = [[-0.24973622  0.20452788  0.0987535   1.        ]]. Action = [[ 0.9505801   0.18715239 -0.72861123  0.46298587]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 43 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0747, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.2588103  -0.00471178  0.11382413  1.        ]]. Action = [[ 0.6131487  -0.66841716 -0.5064429  -0.30137706]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 45. State = [[-0.26037467  0.03926116  0.12032945  1.        ]]. Action = [[ 0.7607621   0.9154135   0.66881406 -0.69219655]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 46. State = [[-0.26133886  0.04496051  0.10034735  1.        ]]. Action = [[-0.02774441  0.0939306  -0.75471807  0.43616557]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0852, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.25837788  0.01938247  0.12438394  1.        ]]. Action = [[ 0.0272007  -0.31572622  0.7461873  -0.18684953]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 48. State = [[-0.2588996   0.06603318  0.12281237  1.        ]]. Action = [[ 0.7748933   0.7570354   0.61126566 -0.6591145 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 49. State = [[-0.2561458   0.06732579  0.11369199  1.        ]]. Action = [[ 0.07918358 -0.43199235  0.5555259   0.6775553 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0815, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 0
Current timestep = 50. State = [[-0.25618026 -0.00851218  0.11942051  1.        ]]. Action = [[ 0.7977514  -0.57309604 -0.10131872 -0.44729865]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 51. State = [[-0.25729445 -0.00931145  0.1067978   1.        ]]. Action = [[-0.26353705 -0.760652   -0.22423899 -0.31678504]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 51 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 51 is tensor(0.0854, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.2498375  -0.01956035  0.1106003   1.        ]]. Action = [[ 0.51050806 -0.5972067   0.6642878   0.23578024]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0867, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.22582513 -0.03584155  0.12593038  1.        ]]. Action = [[ 0.9559233  -0.06888258  0.7633848   0.17592132]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0849, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.26447994  0.09677593  0.12163112  1.        ]]. Action = [[-0.2722034   0.79343796 -0.63022083 -0.12490147]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 55. State = [[-0.25257528  0.10595196  0.11621281  1.        ]]. Action = [[ 0.61949444 -0.11459488  0.9565759   0.8130896 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0768, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.23888953  0.1052114   0.12602058  1.        ]]. Action = [[-0.7998406   0.48217487  0.77681804  0.84075403]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.0740, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.23684335  0.0996571   0.13860427  1.        ]]. Action = [[-0.22747827 -0.37381732  0.93103576  0.80337405]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.0818, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.25599074 -0.04911825  0.11588557  1.        ]]. Action = [[ 0.20519924 -0.82577544 -0.27118075 -0.66815937]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 59. State = [[-0.26114357  0.03330924  0.12457445  1.        ]]. Action = [[ 0.90451145  0.40498674  0.9203136  -0.21063554]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 60. State = [[-0.2598742   0.03799913  0.11108962  1.        ]]. Action = [[-0.7291807  -0.56558144  0.1326369   0.89084744]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.0901, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.25265443 -0.13080785  0.11866372  1.        ]]. Action = [[ 0.59271145 -0.85827637  0.49880195 -0.1741308 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 62. State = [[-0.24290523 -0.14960608  0.10280543  1.        ]]. Action = [[ 0.842144   -0.33288372 -0.25015754  0.583411  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 62 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1154, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.25146034 -0.08039724  0.12479009  1.        ]]. Action = [[-0.5817273   0.757679   -0.03865254 -0.3449154 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 64. State = [[-0.24097542 -0.08575794  0.11418513  1.        ]]. Action = [[0.84029746 0.38163483 0.5175781  0.8298608 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.1015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.22728115 -0.0795403   0.11397935  1.        ]]. Action = [[-0.39774382  0.22677362 -0.4802972   0.68388677]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.2612881   0.02871098  0.1219656   1.        ]]. Action = [[ 0.03412306  0.9278517  -0.09987587 -0.07432824]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 67. State = [[-0.25350174  0.04316295  0.10714193  1.        ]]. Action = [[ 0.5945351   0.6579088  -0.20032161  0.9529431 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.1063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 0
Current timestep = 68. State = [[-0.22993542  0.06130777  0.10154743  1.        ]]. Action = [[0.78147554 0.12864697 0.12678242 0.28816462]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1199, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 0
Current timestep = 69. State = [[-0.2068595   0.06311565  0.10641985  1.        ]]. Action = [[ 0.6061094  -0.23910755  0.25386465  0.7654593 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.19870421  0.04661944  0.1138815   1.        ]]. Action = [[-0.96955067 -0.88354206  0.50923586  0.4417715 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.0760, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-0.2104631   0.02605715  0.11841394  1.        ]]. Action = [[ 0.98279977 -0.05778921 -0.83649606  0.72166276]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.1081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.21559034  0.0164561   0.12745374  1.        ]]. Action = [[-0.6477605  -0.62565714  0.8567064   0.84320545]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.0791, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.23287833 -0.00370048  0.1433147   1.        ]]. Action = [[ 0.4318874  -0.36807525 -0.8074712   0.5127368 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.1168, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-2.3557957e-01  8.0330513e-04  1.3725178e-01  1.0000000e+00]]. Action = [[-0.62527007  0.56421065  0.21960425  0.22074628]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.1107, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.23671567  0.0112996   0.13705495  1.        ]]. Action = [[ 0.5125271   0.3161223  -0.16902542  0.46172643]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1201, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 0
Current timestep = 76. State = [[-0.22211526  0.00870278  0.13817605  1.        ]]. Action = [[ 0.853513   -0.8373043   0.00563931  0.58971965]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.0964, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 0
Current timestep = 77. State = [[-0.20841862 -0.02195128  0.13748437  1.        ]]. Action = [[-0.4543599  -0.976849    0.10798669  0.12992418]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.0953, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 0
Current timestep = 78. State = [[-0.21343142 -0.05842808  0.14106919  1.        ]]. Action = [[-0.22028005 -0.9616201   0.10819817  0.67703164]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.0960, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.25864074 -0.03150964  0.11929815  1.        ]]. Action = [[ 0.46801102 -0.34397948 -0.46553552 -0.23749441]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 80. State = [[-0.2594913   0.07740317  0.12325355  1.        ]]. Action = [[ 0.9914595  -0.53356385  0.78139377 -0.59022874]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 81. State = [[-0.25186494  0.076125    0.11621519  1.        ]]. Action = [[ 0.4517951  -0.7214345   0.82941365  0.77754164]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.0783, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.2327571   0.05066245  0.1264768   1.        ]]. Action = [[ 0.6032629  -0.74695426 -0.1087141   0.8512509 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.0828, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.26359084  0.12567843  0.12258507  1.        ]]. Action = [[ 0.33146513  0.41246045  0.59084284 -0.03786707]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 84. State = [[-0.26155815  0.13882388  0.10872844  1.        ]]. Action = [[-0.82455605 -0.50725967  0.32633555  0.80313396]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0717, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.2611413   0.02995392  0.12011813  1.        ]]. Action = [[ 0.352659    0.6953001   0.52721405 -0.5374604 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 86. State = [[-0.2614125   0.11098881  0.12256049  1.        ]]. Action = [[ 0.01389062 -0.74507046  0.4045956  -0.59277356]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 87. State = [[-0.25833663  0.12297595  0.1102092   1.        ]]. Action = [[-0.41176617 -0.6073187   0.7517047  -0.66238064]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.24587989  0.1244611   0.11685384  1.        ]]. Action = [[0.7873173  0.07108366 0.82581735 0.77879953]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0723, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25744662  0.0060636   0.11665752  1.        ]]. Action = [[ 0.10258019 -0.9100196  -0.23058182 -0.7204779 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 90. State = [[-0.245162   -0.00393334  0.11103329  1.        ]]. Action = [[ 0.9098458  -0.7362626   0.88867426  0.48765326]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0697, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.21417747 -0.02654402  0.13512515  1.        ]]. Action = [[ 0.77747655 -0.51271343  0.98812103  0.9022074 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.19472092 -0.03630535  0.15749499  1.        ]]. Action = [[ 0.6803137  -0.07216167  0.05852306 -0.08194804]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.1058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.18889794 -0.04299333  0.17099807  1.        ]]. Action = [[ 0.17592347 -0.36806673  0.9680326   0.87484396]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0828, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.17273492 -0.05866556  0.2097696   1.        ]]. Action = [[ 0.5530498  -0.46427035  0.9185095   0.89893985]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0881, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.16086748 -0.08335523  0.24551578  1.        ]]. Action = [[-0.35488224 -0.7546081   0.85700643  0.9329164 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0886, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.16019979 -0.11368409  0.28071243  1.        ]]. Action = [[ 0.58243406 -0.82723373  0.8526808   0.7640023 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.1020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 0
Current timestep = 97. State = [[-0.14802216 -0.14220342  0.29723585  1.        ]]. Action = [[ 0.19968534 -0.60385036 -0.94155073  0.47790956]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 97 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.1166, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 0
Current timestep = 98. State = [[-0.25639796 -0.0822955   0.10621482  1.        ]]. Action = [[-0.8376984   0.5098102   0.95901394 -0.03797734]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 99. State = [[-0.24888559 -0.09193446  0.08728903  1.        ]]. Action = [[ 0.716032   -0.11899334 -0.52964103  0.8327334 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.1045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.2551444  -0.11514536  0.12086488  1.        ]]. Action = [[ 0.45352256 -0.9843248   0.6128359  -0.60156745]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 101. State = [[-0.2462618  -0.12870274  0.10067441  1.        ]]. Action = [[ 0.81007576 -0.17579925 -0.61164874  0.906163  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 101 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.1010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.2576225   0.17348441  0.1262903   1.        ]]. Action = [[-0.6256924   0.3521359  -0.79887176 -0.7709609 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 103. State = [[-0.25483558  0.19288485  0.11229279  1.        ]]. Action = [[-0.5292349   0.33556414  0.1494323   0.6319697 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 103 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 103 is tensor(0.1159, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.2547212   0.19291596  0.11232848  1.        ]]. Action = [[-0.8763554  -0.93541807 -0.00335938  0.84972954]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0908, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.2546328   0.19295917  0.11236409  1.        ]]. Action = [[-0.43914282 -0.7401898   0.51541495  0.7197721 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 105 is tensor(0.1049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.2631242   0.13547547  0.11979239  1.        ]]. Action = [[ 0.3820821  -0.09772009 -0.36509413 -0.83752   ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 107. State = [[-0.26062372  0.14994457  0.10681502  1.        ]]. Action = [[-0.50346655 -0.9180207   0.94312453  0.9397416 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0854, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.2508429   0.16317481  0.10505633  1.        ]]. Action = [[ 0.7654276   0.83253574 -0.2348237   0.79407716]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 108 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 108 is tensor(0.1026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.22615114  0.18714648  0.0899914   1.        ]]. Action = [[ 0.78556263  0.45515752 -0.6794923   0.56701684]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 109 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 109 is tensor(0.1033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.20971626  0.19308992  0.06234185  1.        ]]. Action = [[-0.45747823 -0.50745475 -0.77649224  0.78540266]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 110 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 110 is tensor(0.1032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.20211439  0.18002717  0.05034903  1.        ]]. Action = [[ 0.8809726  -0.18006802  0.890193    0.958122  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 111 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0978, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.18407226  0.1752357   0.06008921  1.        ]]. Action = [[ 0.3272642  -0.714461    0.98335314  0.78071415]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 112 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0968, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.17551495  0.17424078  0.06464361  1.        ]]. Action = [[ 0.56342363 -0.11467791  0.27353263  0.96418095]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 113 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 113 is tensor(0.1048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.1623132   0.17154065  0.07230844  1.        ]]. Action = [[ 0.4543718  -0.5221013   0.01340985  0.51398385]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 114 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 114 is tensor(0.1074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.2612117   0.00150226  0.10806911  1.        ]]. Action = [[ 0.95576143  0.2787441  -0.77008307 -0.02581596]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 116. State = [[-0.2623992   0.00115345  0.096119    1.        ]]. Action = [[-0.580043    0.06691194 -0.6549907   0.2437017 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.0924, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.2623992   0.00115345  0.096119    1.        ]]. Action = [[-0.16400611  0.15295386  0.49033952  0.41950798]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.0994, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.25157934  0.01027082  0.09658785  1.        ]]. Action = [[0.8712821  0.7383244  0.02201271 0.29032683]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.0935, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.24126434  0.03019076  0.09364446  1.        ]]. Action = [[-0.46313453  0.59106874  0.08190775  0.17894149]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.0930, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.24831998  0.03928467  0.09526644  1.        ]]. Action = [[-0.2506398  -0.37097418  0.4080695   0.8547604 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0860, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 0
Current timestep = 121. State = [[-0.2508559  -0.07849833  0.12024924  1.        ]]. Action = [[ 0.31466866  0.805341    0.4504249  -0.31667644]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 122. State = [[-0.25503603 -0.15293764  0.12318341  1.        ]]. Action = [[ 0.9346708   0.8893378   0.33685708 -0.15913022]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 123. State = [[-0.25689468 -0.16886367  0.10925419  1.        ]]. Action = [[-0.372321   -0.8994807   0.9474323   0.91014266]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 123 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.24526097 -0.18170221  0.11035696  1.        ]]. Action = [[ 0.84696925 -0.9409786   0.01720357  0.6337588 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 124 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0821, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of -1
Current timestep = 125. State = [[-0.22737272 -0.20409101  0.11282148  1.        ]]. Action = [[0.24811697 0.20913374 0.7882887  0.939136  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 125 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0803, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.21039483 -0.19194199  0.11866298  1.        ]]. Action = [[ 0.85939527  0.6428019  -0.6972125   0.8812597 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 126 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0849, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.2508094  -0.14235051  0.12333531  1.        ]]. Action = [[ 0.2982849  -0.71450627  0.7500994  -0.73339415]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 128. State = [[-0.24939625 -0.15326291  0.10066871  1.        ]]. Action = [[ 0.33740318  0.13908923 -0.8823197   0.43972945]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0880, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.24212089 -0.1473786   0.07062893  1.        ]]. Action = [[-0.10031486  0.3417076  -0.35905665  0.8711978 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 129 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0848, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.23108405 -0.14762013  0.06893325  1.        ]]. Action = [[ 0.95810235 -0.321447    0.8561808   0.7302996 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0817, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.20104659 -0.16538195  0.08349311  1.        ]]. Action = [[ 0.809999   -0.8822086   0.34567368  0.49800825]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 131 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0893, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.17831083 -0.18256196  0.09309547  1.        ]]. Action = [[ 0.29684353  0.56711686 -0.9723788   0.73390305]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 132 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0814, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.17809993 -0.18262844  0.09315569  1.        ]]. Action = [[0.18207312 0.6873112  0.9575145  0.6867447 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 133 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0768, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.17139815 -0.19631606  0.09779686  1.        ]]. Action = [[ 0.49920034 -0.8833304   0.20020056  0.54573584]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 134 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0995, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 0
Current timestep = 135. State = [[-0.1494679  -0.23020032  0.10252339  1.        ]]. Action = [[ 0.67334414 -0.9092147  -0.47890294  0.50487494]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 135 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0950, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.12326501 -0.26643664  0.10251896  1.        ]]. Action = [[ 0.8808836  -0.99807125  0.43240738  0.00263035]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 136 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.1032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 0
Current timestep = 137. State = [[-0.10259037 -0.2908747   0.11331716  1.        ]]. Action = [[ 0.7515583  -0.40633142  0.8827957   0.9820998 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 137 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0947, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.10250943 -0.29137668  0.11328441  1.        ]]. Action = [[ 0.5643904  -0.87593585  0.9472573  -0.2751088 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 138 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.1088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of -1
Current timestep = 139. State = [[-0.10240671 -0.28797486  0.11741418  1.        ]]. Action = [[-0.3814392   0.5174148   0.57797134  0.805444  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 139 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0897, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 0
Current timestep = 140. State = [[-0.10422546 -0.290286    0.12107389  1.        ]]. Action = [[-0.09286559 -0.396752   -0.3679104   0.09301007]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 140 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.1178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.10660022 -0.29293633  0.12024975  1.        ]]. Action = [[ 0.80372    -0.53552675  0.84814024  0.846143  ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 141 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0915, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.10302217 -0.28287345  0.11542941  1.        ]]. Action = [[ 0.5663481   0.6377604  -0.31426287  0.61198485]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 142 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0974, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.09212466 -0.27759418  0.11746305  1.        ]]. Action = [[ 0.16215336 -0.2561401   0.81916046  0.38915992]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 143 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.1000, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.07313511 -0.2717881   0.13263735  1.        ]]. Action = [[0.93801236 0.58821917 0.32934535 0.81662226]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 144 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0823, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.04143631 -0.2664229   0.15712765  1.        ]]. Action = [[ 0.78124285 -0.45811522  0.826895    0.96025014]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 145 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0806, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.02029871 -0.2742384   0.17890699  1.        ]]. Action = [[-0.07674825 -0.75564176  0.500263    0.28607488]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 146 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.1020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.25916985 -0.1797259   0.10925249  1.        ]]. Action = [[-0.5813874  -0.507075   -0.12652582 -0.60656565]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 148. State = [[-0.25337186  0.00453391  0.12565665  1.        ]]. Action = [[ 0.6443399  -0.43982238  0.68763256 -0.2710929 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 149. State = [[-0.25438485  0.00635315  0.11365526  1.        ]]. Action = [[-0.47738022 -0.09101444  0.16412807  0.753273  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 150. State = [[-0.25230876  0.010594    0.10599982  1.        ]]. Action = [[ 0.23856163  0.2989136  -0.63142633  0.93223333]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 151. State = [[-0.24555641  0.01393003  0.08967263  1.        ]]. Action = [[-0.43358648  0.6329802   0.24307811  0.46787715]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 152. State = [[-0.2407669   0.02153876  0.08115587  1.        ]]. Action = [[ 0.514568   0.4568653 -0.8780715  0.2586565]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 153. State = [[-0.26588473  0.13600908  0.12228435  1.        ]]. Action = [[ 0.9617106   0.97143006  0.2923658  -0.44636935]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 154. State = [[-0.26211914  0.09746154  0.11977775  1.        ]]. Action = [[ 0.6796484 -0.9306195  0.8010366 -0.4149183]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 155. State = [[-0.2460074   0.11896432  0.11580793  1.        ]]. Action = [[0.9067149  0.62682176 0.953534   0.92667437]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 156. State = [[-0.22450697  0.12512459  0.13061011  1.        ]]. Action = [[ 0.15491557 -0.47260368  0.34569097  0.9326339 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 156 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 0
Current timestep = 157. State = [[-0.22622734  0.12327092  0.14921363  1.        ]]. Action = [[-0.90599114  0.19108558  0.7889726   0.33543754]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 158. State = [[-0.23374268  0.11824264  0.1617579   1.        ]]. Action = [[ 0.522277   -0.5868097  -0.75623786  0.6072798 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 159. State = [[-0.23242256  0.10898498  0.15614192  1.        ]]. Action = [[-0.89016    -0.15371192  0.4209293   0.35656393]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 160. State = [[-0.22175877  0.09535818  0.1508352   1.        ]]. Action = [[ 0.7490355 -0.7666851 -0.3908354  0.6937988]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 161. State = [[-0.2167328   0.09117757  0.13862266  1.        ]]. Action = [[-0.9356109   0.56691134 -0.30552566  0.96695113]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 162. State = [[-0.22412987  0.09977246  0.12704915  1.        ]]. Action = [[ 0.22486997  0.27437127 -0.30670714  0.64818573]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 163. State = [[-0.22571784  0.10158234  0.11196089  1.        ]]. Action = [[ 0.09155381 -0.14560562 -0.5940364   0.8984375 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 164. State = [[-0.22394377  0.09907752  0.10114918  1.        ]]. Action = [[ 0.12475538 -0.19074112  0.35542166  0.45718157]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 165. State = [[-0.21044882  0.09183851  0.11214415  1.        ]]. Action = [[ 0.7119523  -0.3596077   0.8637476   0.84910583]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 166. State = [[-0.18671064  0.07677697  0.1290948   1.        ]]. Action = [[ 0.6969472 -0.4793231  0.1534077  0.880769 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 167. State = [[-0.17374988  0.06090714  0.14178048  1.        ]]. Action = [[-0.2821201  -0.4810691   0.3758204   0.94656026]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 168. State = [[-0.1740166   0.05063736  0.1475664   1.        ]]. Action = [[0.5192051  0.71792245 0.84975934 0.93558824]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 169. State = [[-0.17421113  0.0477388   0.1482546   1.        ]]. Action = [[ 0.5964434  -0.5788293   0.41847444  0.9838532 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 170. State = [[-0.18071139  0.05839841  0.16029452  1.        ]]. Action = [[-0.8371628   0.71024466  0.8279402   0.4920467 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 171. State = [[-0.19513737  0.06383435  0.17397463  1.        ]]. Action = [[-0.1443302  -0.48334146 -0.51333064  0.64813805]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 172. State = [[-0.19993237  0.05992158  0.17195025  1.        ]]. Action = [[ 9.0002465e-01 -9.1671944e-05  4.2648125e-01  8.7141013e-01]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 173. State = [[-0.20011494  0.05920493  0.17201678  1.        ]]. Action = [[ 0.7427783  -0.33391613 -0.83886313 -0.21143645]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 174. State = [[-0.20262736  0.05603287  0.17385396  1.        ]]. Action = [[-0.35540128 -0.08949101  0.30012965  0.4717841 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 175. State = [[-0.20606382  0.05299829  0.1748077   1.        ]]. Action = [[0.9063313  0.8416246  0.83377457 0.8515394 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 176. State = [[-0.2630043   0.09395227  0.12011929  1.        ]]. Action = [[ 0.21870351  0.58550656  0.47573435 -0.40206504]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 177. State = [[-0.2536073   0.09560746  0.10156346  1.        ]]. Action = [[ 0.653234   -0.5728421  -0.51068515  0.7791996 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 178. State = [[-0.23612101  0.09500846  0.08505278  1.        ]]. Action = [[ 0.33747792  0.45787537 -0.21331954  0.65113497]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 179. State = [[-0.2641743   0.11420581  0.11940764  1.        ]]. Action = [[-0.37239754 -0.6551186   0.9866601  -0.30602545]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 180. State = [[-0.26112285  0.12654017  0.10537088  1.        ]]. Action = [[-0.57868224 -0.8550204  -0.7312981   0.91227734]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 180 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0390, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.24794935  0.12862842  0.11030884  1.        ]]. Action = [[0.89314497 0.10674512 0.61230564 0.7308016 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 181 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0494, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.22898379  0.13243043  0.11731362  1.        ]]. Action = [[-0.8210518   0.6023245   0.9577613  -0.24215788]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 182 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 182 is tensor(0.0416, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.2287517   0.13091986  0.11871655  1.        ]]. Action = [[-0.12003791 -0.11854565  0.20795488  0.98461044]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 183 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0531, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.23254208  0.11696742  0.12135883  1.        ]]. Action = [[-0.5440882  -0.8352592  -0.04598725  0.96957195]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 185. State = [[-0.22970349  0.10115456  0.1347259   1.        ]]. Action = [[ 0.5192629  -0.09536242  0.9162061   0.9282495 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 186. State = [[-0.22765554  0.10895204  0.16483788  1.        ]]. Action = [[-0.07723254  0.79261315  0.8782674   0.3500023 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 187. State = [[-0.2299404   0.11814258  0.18190144  1.        ]]. Action = [[ 0.00898015 -0.29851103 -0.38801837  0.7720063 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 188. State = [[-0.2661528   0.17224763  0.12380601  1.        ]]. Action = [[-0.3419826  -0.07862377  0.8130841  -0.01806045]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 189. State = [[-0.26323897  0.19111937  0.11126813  1.        ]]. Action = [[-0.83636516 -0.5328116   0.567332    0.93935394]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 189 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0448, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.25606102  0.1849375   0.10606766  1.        ]]. Action = [[ 0.5713475  -0.42704833 -0.3476647   0.57662547]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 190 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0609, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.24690719  0.1773121   0.089981    1.        ]]. Action = [[-0.22839117  0.11554337 -0.5392034   0.6096163 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 191 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0611, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.24008578  0.17113072  0.07441804  1.        ]]. Action = [[ 0.6929307  -0.5059327  -0.20370203  0.9180169 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 192 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.21308517  0.17521717  0.07274949  1.        ]]. Action = [[0.8915353 0.8262615 0.3092389 0.9717815]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 193 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0468, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-0.18875921  0.19273116  0.08081231  1.        ]]. Action = [[0.2751994  0.16610348 0.4348998  0.83744156]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 194 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0592, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.17685063  0.19097786  0.09365668  1.        ]]. Action = [[ 0.10750616 -0.4764862   0.6699884   0.8537632 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 195 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0587, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.16941032  0.18478906  0.10494835  1.        ]]. Action = [[ 0.961648   -0.8310624   0.5757537   0.54765487]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 196 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.16560026  0.18633106  0.11570659  1.        ]]. Action = [[0.18002653 0.10218096 0.65731704 0.9493444 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 197 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0554, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.16163342  0.19519965  0.1445084   1.        ]]. Action = [[-0.4700318   0.22915173  0.92507124  0.8645357 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 198 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0525, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.16849236  0.1903166   0.17650287  1.        ]]. Action = [[ 0.24130535 -0.5790268   0.42319417  0.09181023]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 199 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0618, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.16807689  0.18979995  0.18276462  1.        ]]. Action = [[-0.09289658  0.9116652  -0.20882154  0.6839819 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 200 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0566, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.17073902  0.20687918  0.17786065  1.        ]]. Action = [[ 0.7464373   0.03093469 -0.8160249   0.82087016]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 201 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0599, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.15037645  0.22173506  0.16072525  1.        ]]. Action = [[ 0.18232441  0.69645715 -0.02094537  0.47578597]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 202 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[-0.13692792  0.23632821  0.16004047  1.        ]]. Action = [[0.38336158 0.12655115 0.47508502 0.20197392]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 203 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0595, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.1273135   0.23204629  0.17445214  1.        ]]. Action = [[-0.64818084 -0.9049223   0.38486528  0.6148095 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 204 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0446, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.13164729  0.20374182  0.17617829  1.        ]]. Action = [[-0.18177968 -0.8644899  -0.78868324  0.8815812 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 205 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0501, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.15036307  0.17737249  0.15712264  1.        ]]. Action = [[-0.6658258  -0.55519426 -0.53769326  0.9224161 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 206 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0468, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.17201152  0.1677067   0.14582448  1.        ]]. Action = [[-0.5628405   0.12797415  0.5207386   0.44308174]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 207 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0529, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 0
Current timestep = 208. State = [[-0.18137534  0.17161986  0.15971667  1.        ]]. Action = [[-0.40807766  0.14072907  0.4906814   0.9318658 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 208 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0468, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 0
Current timestep = 209. State = [[-0.20238411  0.16661668  0.18483044  1.        ]]. Action = [[-0.84139425 -0.48404187  0.9029782   0.83752036]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 209 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of 0
Current timestep = 210. State = [[-0.23835708  0.14378063  0.1974567   1.        ]]. Action = [[-0.38622016 -0.6590199  -0.8594169   0.60275126]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 210 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0461, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.24962479  0.1260534   0.18012469  1.        ]]. Action = [[ 0.20922053 -0.2510054  -0.42124474  0.56735384]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 211 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0539, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.24747425  0.12189237  0.17418252  1.        ]]. Action = [[-0.57148653 -0.46892333 -0.8916755   0.51861763]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 213. State = [[-0.24747425  0.12189237  0.17418252  1.        ]]. Action = [[-0.8826422   0.9158269   0.49112546  0.44550848]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 214. State = [[-0.2474884   0.12188448  0.17413636  1.        ]]. Action = [[-0.44696188  0.6527395   0.15873396  0.9555323 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 215. State = [[-0.237064    0.10986631  0.18308334  1.        ]]. Action = [[ 0.78406286 -0.6183381   0.8366697   0.92000437]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 216. State = [[-0.2312621   0.1020989   0.18317094  1.        ]]. Action = [[-0.42868924  0.5024164  -0.9963936   0.78131735]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 217. State = [[-0.23604052  0.1052448   0.16727652  1.        ]]. Action = [[-0.86303365  0.8886417   0.39917314  0.11793506]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 218. State = [[-0.2359131   0.10679214  0.16607566  1.        ]]. Action = [[-0.8022142  -0.52733487 -0.11861533  0.6807643 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 219. State = [[-0.22616963  0.10545765  0.16922748  1.        ]]. Action = [[ 0.78023577 -0.17716265  0.28232086  0.8088982 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 220. State = [[-0.20715636  0.09867549  0.17085525  1.        ]]. Action = [[ 0.848603   -0.29915702 -0.18187654  0.40892804]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 221. State = [[-0.19015765  0.09212756  0.16433631  1.        ]]. Action = [[ 0.80529344 -0.31236982 -0.8013818   0.56257224]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 222. State = [[-0.18736444  0.09194221  0.1648719   1.        ]]. Action = [[0.90409374 0.36758852 0.9161272  0.7353625 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 223. State = [[-0.19018202  0.10042783  0.16544008  1.        ]]. Action = [[-0.34634018  0.66837525  0.18663919  0.9397503 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 224. State = [[-0.1923575   0.10807295  0.16618739  1.        ]]. Action = [[ 0.7361373  -0.29202706 -0.14377701  0.46553135]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 225. State = [[-0.19465408  0.09838405  0.17237884  1.        ]]. Action = [[-0.53943384 -0.80694395  0.52731085  0.6088245 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 226. State = [[-0.21174246  0.08439772  0.1720547   1.        ]]. Action = [[-0.9209906  -0.16840148 -0.85962886  0.33728027]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 227. State = [[-0.23248415  0.07324223  0.16051617  1.        ]]. Action = [[-0.19544262 -0.31003332  0.06982017  0.85266936]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 228. State = [[-0.2355721   0.06924681  0.15952832  1.        ]]. Action = [[-0.97234833  0.8346404  -0.19598031  0.98139143]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 229. State = [[-0.23186909  0.05980935  0.15603404  1.        ]]. Action = [[ 0.44848442 -0.48798907 -0.43165213  0.70586085]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 230. State = [[-0.21922828  0.0626171   0.1453088   1.        ]]. Action = [[ 0.9739609   0.9859288  -0.494439    0.65065384]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 231. State = [[-0.20594968  0.0657048   0.13824855  1.        ]]. Action = [[-0.14977372 -0.7726057   0.6246443   0.4939084 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 232. State = [[-0.25900218 -0.03778843  0.1129328   1.        ]]. Action = [[ 0.0355444   0.7535877   0.8050482  -0.15916562]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 233. State = [[-0.25648606 -0.04338198  0.1064035   1.        ]]. Action = [[-0.03486186  0.09984148  0.8860166   0.8820282 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 234. State = [[-0.25355348 -0.04444432  0.11440194  1.        ]]. Action = [[-0.64499134 -0.21463591  0.56479156  0.8392048 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 235. State = [[-0.25230545 -0.05429228  0.11839771  1.        ]]. Action = [[ 0.20704341 -0.6120004   0.17123199  0.6803663 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 236. State = [[-0.24478714 -0.06336551  0.13281602  1.        ]]. Action = [[0.5721874  0.12226939 0.97772646 0.9807501 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 237. State = [[-0.22653854 -0.05250282  0.16363329  1.        ]]. Action = [[0.6765449  0.7893456  0.48112762 0.8866861 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 238. State = [[-0.19976243 -0.03322529  0.18370028  1.        ]]. Action = [[0.7074727  0.38997173 0.27777147 0.16502833]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 239. State = [[-0.19237931 -0.02243139  0.1875403   1.        ]]. Action = [[-0.82655597  0.06460774 -0.6697471   0.49658442]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 240. State = [[-0.19892587 -0.02033828  0.18161376  1.        ]]. Action = [[0.68981206 0.23067153 0.09345913 0.24442184]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 241. State = [[-0.1982512  -0.02752314  0.18163747  1.        ]]. Action = [[ 0.22352862 -0.5729709   0.05537581  0.13403189]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 242. State = [[-0.19820847 -0.03271221  0.18204883  1.        ]]. Action = [[ 0.84906447 -0.15653253 -0.06491572  0.5716189 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 243. State = [[-0.19675091 -0.0488984   0.18885459  1.        ]]. Action = [[-0.09122741 -0.8871323   0.82241035  0.15820909]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 244. State = [[-0.20095533 -0.05632941  0.21178412  1.        ]]. Action = [[-0.55406165  0.6367878   0.97730374  0.9078599 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 245. State = [[-0.20265225 -0.06220615  0.23802629  1.        ]]. Action = [[ 0.6451558  -0.84919536  0.11039174  0.8196535 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 246. State = [[-0.19510424 -0.08293089  0.24819705  1.        ]]. Action = [[ 0.22772896 -0.44037473  0.24660397  0.18438089]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 247. State = [[-0.18949646 -0.08379485  0.26448655  1.        ]]. Action = [[-0.09736514  0.61057496  0.7231326   0.49733102]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 248. State = [[-0.2600818   0.00272855  0.11545143  1.        ]]. Action = [[ 0.8764422   0.6835165  -0.5333614  -0.04632646]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 249. State = [[-0.25707862  0.01714489  0.10325292  1.        ]]. Action = [[ 0.40212333  0.9728658  -0.01081574  0.6998787 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 250. State = [[-0.25246227  0.03255348  0.10157181  1.        ]]. Action = [[-0.81606984 -0.9901935  -0.4001714   0.8234265 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 251. State = [[-0.25146794  0.02299639  0.0966269   1.        ]]. Action = [[ 0.06869948 -0.8742897  -0.39573824  0.6508912 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 252. State = [[-0.23706734  0.01179712  0.08593854  1.        ]]. Action = [[ 0.8270489  -0.07703251 -0.1415441   0.8164257 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 253. State = [[-0.22003679  0.00811524  0.07994072  1.        ]]. Action = [[-0.8799761  -0.5054525  -0.70277774  0.73643315]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 254. State = [[-0.22824354  0.01075239  0.07290908  1.        ]]. Action = [[-0.8605014   0.31445956 -0.59782666  0.05511546]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 255. State = [[-0.23635049  0.0169492   0.06167429  1.        ]]. Action = [[-0.14349377  0.22990406  0.32871127  0.90943944]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 256. State = [[-0.23827615  0.02228254  0.06176534  1.        ]]. Action = [[ 0.407431    0.62569237 -0.83456314  0.8900969 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 257. State = [[-0.23836495  0.02271428  0.06177719  1.        ]]. Action = [[-0.58434653 -0.4476081  -0.846266    0.6661452 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 258. State = [[-0.24035716  0.01603874  0.07031525  1.        ]]. Action = [[-0.3109427  -0.44368434  0.88543785  0.64375544]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 259. State = [[-0.24155426 -0.00452865  0.08491002  1.        ]]. Action = [[ 0.90189457 -0.8474228  -0.06027901  0.267267  ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 260. State = [[-0.22826797 -0.02197387  0.08047011  1.        ]]. Action = [[ 0.46664357 -0.1763199  -0.8331854   0.4980476 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 261. State = [[-0.2029911  -0.03833504  0.0633829   1.        ]]. Action = [[ 0.9428537  -0.7258237  -0.49429226  0.9061407 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 262. State = [[-0.25738674 -0.0559678   0.11779366  1.        ]]. Action = [[ 0.26155448 -0.6759728   0.17533422 -0.03579605]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 263. State = [[-0.25740996 -0.07077252  0.10176319  1.        ]]. Action = [[ 0.01247156 -0.5125747  -0.23286712  0.90349054]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 264. State = [[-0.25613108 -0.09710717  0.10192896  1.        ]]. Action = [[-0.06773877 -0.95337737  0.8077061   0.30043662]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 265. State = [[-0.25408602 -0.11912145  0.10729711  1.        ]]. Action = [[ 0.2819891  -0.16286957 -0.36755133  0.84585845]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 266. State = [[-0.24032573 -0.11470778  0.10716478  1.        ]]. Action = [[ 0.8887465   0.75479543 -0.01735449  0.9462087 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 267. State = [[-0.21434633 -0.11279763  0.11429621  1.        ]]. Action = [[ 0.6708257  -0.48321843  0.8102493   0.44128132]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 268. State = [[-0.19732715 -0.13098519  0.13821223  1.        ]]. Action = [[-0.27548432 -0.7941088   0.974504    0.6805371 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 268 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.0364, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.19486329 -0.14096648  0.17415954  1.        ]]. Action = [[0.03533912 0.61426926 0.68590474 0.6387311 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 269 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.0392, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of 1
Current timestep = 270. State = [[-0.19834478 -0.14136094  0.2030958   1.        ]]. Action = [[-0.64284486 -0.5138297   0.87389064  0.9617946 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 270 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.0414, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.21470198 -0.16636492  0.22977269  1.        ]]. Action = [[-0.49565208 -0.8415846  -0.1800304   0.65833294]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 271 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of 0
Current timestep = 272. State = [[-0.2345168  -0.17018297  0.23842372  1.        ]]. Action = [[-0.5719124   0.8126602   0.785702    0.79605937]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 272 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 272 is tensor(0.0405, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.24514325 -0.15688993  0.25420824  1.        ]]. Action = [[-0.6473752  -0.93612707  0.64161515  0.92079854]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 273 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.0387, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.24567851 -0.16134036  0.2567433   1.        ]]. Action = [[ 0.13865745 -0.40965533  0.2726457   0.39284873]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 274 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 274 is tensor(0.0496, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 0
Current timestep = 275. State = [[-0.24787997 -0.16352025  0.25962055  1.        ]]. Action = [[ 0.01880133  0.41548085 -0.1995042   0.9023323 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 275 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 275 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.24041018 -0.15858413  0.26726672  1.        ]]. Action = [[ 0.56031275 -0.24141222  0.7185304   0.90573907]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 276 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.0414, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.22302637 -0.1704888   0.29302445  1.        ]]. Action = [[ 0.33599663 -0.6337499   0.7724204   0.41640437]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 277 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.2581882   0.01060309  0.10599621  1.        ]]. Action = [[-0.96403414 -0.6078447   0.00669634 -0.00919098]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 279. State = [[-0.259677    0.01396438  0.08438516  1.        ]]. Action = [[ 0.03178656  0.08618975 -0.98095965  0.4388522 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 280. State = [[-0.25210643  0.00161876  0.06723611  1.        ]]. Action = [[ 0.35546494 -0.9556315   0.8753611   0.22752512]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 281. State = [[-0.24404891 -0.02052917  0.08224752  1.        ]]. Action = [[-0.13669145 -0.48995817  0.92294645  0.9827907 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 282. State = [[-0.24181865 -0.0456869   0.10751264  1.        ]]. Action = [[-0.27849942 -0.9196735   0.4563738   0.9365628 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 283. State = [[-0.24664943 -0.06411389  0.11963788  1.        ]]. Action = [[-0.8927342  -0.2858398   0.44863284  0.65487933]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 284. State = [[-0.242883   -0.06544409  0.13049793  1.        ]]. Action = [[0.55608654 0.15352261 0.6834223  0.696368  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 285. State = [[-0.2323464  -0.0623362   0.13942188  1.        ]]. Action = [[ 0.4984095   0.03107512 -0.99010015  0.74847007]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 286. State = [[-0.21588926 -0.06579296  0.1232238   1.        ]]. Action = [[ 0.4764974  -0.28921306 -0.32676256  0.8628528 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 287. State = [[-0.2098762  -0.06415664  0.11814784  1.        ]]. Action = [[-0.75790024  0.50494254  0.4653424   0.8346052 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 288. State = [[-0.20474859 -0.04702884  0.12941907  1.        ]]. Action = [[0.8027408  0.75660336 0.6783304  0.66608477]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 289. State = [[-0.20556112 -0.02203411  0.13652544  1.        ]]. Action = [[-0.43626523  0.56307995 -0.7196633   0.56641006]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 290. State = [[-0.20850813 -0.01884049  0.1352753   1.        ]]. Action = [[-0.00642556 -0.6067119   0.60295177  0.18199563]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 291. State = [[-0.20855655 -0.02465334  0.14993046  1.        ]]. Action = [[-0.3093537   0.01389813  0.8134692   0.69710755]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 292. State = [[-0.20702784 -0.01616791  0.17772454  1.        ]]. Action = [[0.52354884 0.68405044 0.6355226  0.53942966]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 293. State = [[-0.20475738 -0.00828718  0.19055776  1.        ]]. Action = [[-0.07050413 -0.31163895 -0.31008983  0.8577554 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 294. State = [[-0.2062609  -0.02587764  0.19891796  1.        ]]. Action = [[-0.3210243  -0.98046625  0.7826252   0.567783  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 295. State = [[-0.21583824 -0.04765015  0.21958336  1.        ]]. Action = [[-0.60031205 -0.12656331  0.427706    0.7689228 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 296. State = [[-0.23535526 -0.06578125  0.22800045  1.        ]]. Action = [[-0.7333825  -0.68976563 -0.39678216  0.11776161]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 297. State = [[-0.25186467 -0.06611297  0.21880618  1.        ]]. Action = [[-0.05702221  0.86978817 -0.7495991   0.62326634]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 298. State = [[-0.25105467 -0.06772181  0.21938989  1.        ]]. Action = [[ 0.343691   -0.78633255  0.9840801   0.8261585 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 299. State = [[-0.24201387 -0.08873364  0.23241621  1.        ]]. Action = [[ 0.56822824 -0.7746519   0.11508942  0.966532  ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 300. State = [[-0.22611435 -0.10689503  0.23453434  1.        ]]. Action = [[ 0.9913659  -0.21488917 -0.61252093  0.5739244 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 301. State = [[-0.21268953 -0.1294877   0.22558624  1.        ]]. Action = [[-0.705502   -0.7827022   0.17015028  0.7432717 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 301 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 301 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 0
Current timestep = 302. State = [[-0.22734663 -0.16331868  0.23889074  1.        ]]. Action = [[-0.68904793 -0.8592524   0.8728895   0.7108071 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 302 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 302 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.23157978 -0.1753016   0.26404136  1.        ]]. Action = [[0.79783845 0.6223358  0.822376   0.52026796]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 303 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 303 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.2067721  -0.15731306  0.27644056  1.        ]]. Action = [[ 0.88474417  0.33375263 -0.6155368   0.35426962]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 304 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 304 is tensor(0.0297, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 1
Current timestep = 305. State = [[-0.19188726 -0.14928387  0.27770424  1.        ]]. Action = [[-0.30443978  0.02717578  0.8606005   0.03438485]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 305 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 305 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.19342299 -0.14724134  0.28560534  1.        ]]. Action = [[-0.0448699   0.14477718 -0.8176307   0.08428693]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 306 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 306 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.18978794 -0.14783831  0.2821964   1.        ]]. Action = [[ 0.35121787 -0.2442025   0.5989474   0.8878635 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 307 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 307 is tensor(0.0339, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 0
Current timestep = 308. State = [[-0.257515    0.00291545  0.10614797  1.        ]]. Action = [[-0.7710668   0.5073035   0.9692576  -0.38011956]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 309. State = [[-2.5556856e-01  6.0917781e-04  9.4268680e-02  1.0000000e+00]]. Action = [[ 0.10395479 -0.18661809  0.5056751   0.7597437 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 310. State = [[-0.25408643 -0.00319397  0.09629159  1.        ]]. Action = [[-0.9099384  -0.5446977   0.26936388  0.04005897]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 311. State = [[-0.25391683 -0.00461392  0.09680264  1.        ]]. Action = [[-0.8897584  -0.38640457  0.38341546  0.51502323]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 312. State = [[-0.24371581 -0.01405371  0.09167468  1.        ]]. Action = [[ 0.7623577  -0.65945035 -0.75671774  0.84332776]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 313. State = [[-0.22976917 -0.02319851  0.08082173  1.        ]]. Action = [[-0.5499244 -0.5993166  0.8674917  0.7511661]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 314. State = [[-0.21977174 -0.02367495  0.07417781  1.        ]]. Action = [[ 0.64741766  0.12402678 -0.565725    0.8554058 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 315. State = [[-0.20057528 -0.02546673  0.06310534  1.        ]]. Action = [[ 0.3794502  -0.16964829  0.14503133  0.48838782]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 316. State = [[-0.1837597  -0.0217324   0.06711774  1.        ]]. Action = [[0.60532904 0.49356794 0.43576407 0.822489  ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 317. State = [[-0.16793825 -0.01823085  0.0726223   1.        ]]. Action = [[ 0.44494557 -0.41687799  0.20541656  0.83191955]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 318. State = [[-0.16640691 -0.01760856  0.07321519  1.        ]]. Action = [[ 0.27278638  0.5775417  -0.8379152   0.44845653]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 319. State = [[-0.16619419 -0.01748961  0.0732705   1.        ]]. Action = [[0.20451045 0.8339405  0.7100693  0.8110236 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 320. State = [[-0.17164822 -0.00738817  0.08052449  1.        ]]. Action = [[-0.7627538   0.6119658   0.700443    0.96291757]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 321. State = [[-0.17697984  0.00729944  0.09184344  1.        ]]. Action = [[ 0.38652134  0.28736567 -0.07423705  0.77040803]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 322. State = [[-0.17756186  0.00914418  0.09247196  1.        ]]. Action = [[ 0.4279635  -0.13817191 -0.9493841  -0.05271226]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 323. State = [[-0.17762777  0.00942881  0.09246675  1.        ]]. Action = [[ 0.7511612  -0.6228352  -0.85225934  0.8887446 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 324. State = [[-0.17762777  0.00942881  0.09246675  1.        ]]. Action = [[ 0.24122107 -0.94298613  0.8393376   0.9370527 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 325. State = [[-0.17762777  0.00942881  0.09246675  1.        ]]. Action = [[ 0.5175724  -0.45984244  0.02905309  0.61612594]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 326. State = [[-0.18190084 -0.00226973  0.08514372  1.        ]]. Action = [[-0.2777443  -0.80268794 -0.91526985  0.6601541 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 327. State = [[-0.18565024 -0.00666339  0.08216852  1.        ]]. Action = [[0.00689268 0.47748172 0.7971208  0.8616538 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 328. State = [[-0.19698413 -0.00255206  0.08310263  1.        ]]. Action = [[-0.6570494   0.07766914 -0.62754273  0.60323775]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 329. State = [[-0.20682783 -0.01482515  0.08100321  1.        ]]. Action = [[ 0.13242567 -0.9058898   0.28262305  0.35129   ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 330. State = [[-0.26606795  0.15761039  0.12136271  1.        ]]. Action = [[ 0.6242845  -0.41277772 -0.456671   -0.26244748]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 331. State = [[-0.26424807  0.17520143  0.10784786  1.        ]]. Action = [[-0.9472122   0.4650395   0.19428194  0.88231385]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 331 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 331 is tensor(0.0161, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.25792423  0.16933022  0.10341016  1.        ]]. Action = [[ 0.5189285  -0.45144546 -0.3109687   0.50262094]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 332 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 332 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.24485882  0.16322204  0.09616604  1.        ]]. Action = [[-0.6134696   0.6259695   0.08754349  0.7445518 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 333 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 333 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.23515825  0.15661867  0.09770031  1.        ]]. Action = [[ 0.573071   -0.38595736  0.08265185  0.86377525]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 334 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 334 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 334 of 1
Current timestep = 335. State = [[-0.22543241  0.14465475  0.09092128  1.        ]]. Action = [[ 0.09081912 -0.3142997  -0.9051178   0.81823146]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 335 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 335 is tensor(0.0159, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.22570145  0.14881822  0.06670488  1.        ]]. Action = [[-0.5328159   0.716118    0.4447528   0.96954966]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 336 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 336 is tensor(0.0168, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.22677541  0.1539066   0.0692329   1.        ]]. Action = [[ 0.5896907  -0.5294022   0.00213349  0.5760076 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 337 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 337 is tensor(0.0212, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 0
Current timestep = 338. State = [[-0.25980067 -0.05049745  0.11557642  1.        ]]. Action = [[ 0.5012467   0.81849575  0.8104856  -0.09235942]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 339. State = [[-0.26041517 -0.05591104  0.10270462  1.        ]]. Action = [[-0.40208483 -0.6563121  -0.3860054   0.6209165 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 340. State = [[-0.26041517 -0.05591104  0.10270462  1.        ]]. Action = [[-0.7073165  -0.81556684  0.6975539   0.6397729 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 341. State = [[-0.25408757 -0.04848498  0.10739217  1.        ]]. Action = [[0.4881704  0.68714356 0.7199824  0.29593682]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 342. State = [[-0.24974829 -0.04228606  0.1115218   1.        ]]. Action = [[-0.71002144 -0.63225937  0.7822838  -0.4906788 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 343. State = [[-0.23564135 -0.02751783  0.11767787  1.        ]]. Action = [[0.944976   0.87661326 0.3728156  0.8969207 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 344. State = [[-0.21231548  0.00176913  0.13383044  1.        ]]. Action = [[0.20462143 0.6846218  0.40641844 0.641623  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 345. State = [[-0.19261813  0.00762502  0.1487541   1.        ]]. Action = [[ 0.5992005  -0.7402449   0.42542195  0.634341  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 346. State = [[-0.17649958 -0.00386235  0.16042705  1.        ]]. Action = [[ 0.2004416  -0.25100887 -0.05589765  0.04401362]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 347. State = [[-0.17142618 -0.00874809  0.16368975  1.        ]]. Action = [[ 0.31124842 -0.88692594 -0.56723034  0.67800653]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 348. State = [[-0.17717266 -0.01297312  0.17263344  1.        ]]. Action = [[-0.9308768  -0.094697    0.69861233  0.1253351 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 349. State = [[-0.18667594 -0.0157068   0.18566522  1.        ]]. Action = [[0.60843515 0.28409815 0.37284493 0.85656595]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 350. State = [[-0.19071588 -0.00630666  0.20034164  1.        ]]. Action = [[-0.17655957  0.61177766  0.8733572   0.19999957]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 351. State = [[-0.20241787  0.01275536  0.21788734  1.        ]]. Action = [[-0.54102254  0.47082412 -0.16066432  0.5969944 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 352. State = [[-0.22249995  0.00869583  0.21126185  1.        ]]. Action = [[-0.6948664  -0.91255367 -0.734564    0.7008357 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 353. State = [[-0.23753351  0.00697099  0.20842479  1.        ]]. Action = [[0.01038527 0.7286463  0.6656709  0.7478888 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 354. State = [[-0.24033657  0.00727639  0.21758917  1.        ]]. Action = [[-0.1291163 -0.5982713  0.1928953  0.5202277]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 355. State = [[-0.23531757 -0.01320714  0.22637376  1.        ]]. Action = [[ 0.86385775 -0.72201335  0.36068237  0.9330118 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 356. State = [[-0.222538   -0.03971139  0.23577817  1.        ]]. Action = [[ 0.30780554 -0.76941794 -0.24500465  0.9002392 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 357. State = [[-0.25803804  0.03299234  0.11707707  1.        ]]. Action = [[-0.64005494 -0.61440593  0.14940047 -0.08162212]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 358. State = [[-0.2456198   0.05173549  0.10314376  1.        ]]. Action = [[ 0.89563596  0.8776549  -0.15481985  0.1770351 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 359. State = [[-0.22939067  0.0645413   0.09289043  1.        ]]. Action = [[-0.02470642 -0.46395314 -0.17545581  0.23758268]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 360. State = [[-0.21945895  0.06184774  0.08454317  1.        ]]. Action = [[ 0.7461599  -0.02815413 -0.5547219   0.8085325 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 361. State = [[-0.19620207  0.04688324  0.07853216  1.        ]]. Action = [[ 0.3096     -0.83734065  0.95651126  0.82684183]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 362. State = [[-0.18679164  0.03081962  0.08899385  1.        ]]. Action = [[ 0.82369626 -0.12764883  0.4628079   0.5396173 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 363. State = [[-0.26511818  0.17396553  0.12103665  1.        ]]. Action = [[-0.54667026  0.6832119   0.878386   -0.01093245]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 364. State = [[-0.2514147   0.18104087  0.10176607  1.        ]]. Action = [[ 0.7536138  -0.78054565 -0.58222276  0.7640765 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 364 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 364 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.23056965  0.17075667  0.09278502  1.        ]]. Action = [[0.06021857 0.17483413 0.8936641  0.95313466]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 365 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 365 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.21874636  0.17566751  0.104633    1.        ]]. Action = [[0.62220144 0.17043912 0.13845134 0.65987146]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 366 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 366 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of 1
Current timestep = 367. State = [[-0.20497341  0.1748607   0.10919953  1.        ]]. Action = [[ 0.25984156 -0.27596015 -0.26717603  0.774135  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 367 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 367 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.20114806  0.16988175  0.109306    1.        ]]. Action = [[ 0.8444407  -0.48772454  0.97890294  0.8644937 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 368 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 368 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.20057166  0.16962333  0.10951744  1.        ]]. Action = [[ 0.6777556  -0.61815083 -0.09048808  0.81653106]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 369 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 369 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 369 of -1
Current timestep = 370. State = [[-0.20041952  0.16907126  0.10956135  1.        ]]. Action = [[ 0.7162149  -0.86706835 -0.4115697   0.7692189 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 370 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 370 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.2080508   0.16950089  0.10818725  1.        ]]. Action = [[-0.91408914 -0.09248537  0.12130141  0.7791202 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 371 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 371 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.2223719   0.16318995  0.11289604  1.        ]]. Action = [[-0.6148893  -0.51874274  0.6230799   0.58639956]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 372 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 372 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 0
Current timestep = 373. State = [[-0.23233268  0.15652277  0.13432182  1.        ]]. Action = [[0.7460159 0.5234318 0.5300789 0.667223 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 373 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 373 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of 1
Current timestep = 374. State = [[-0.22950728  0.1738838   0.15715902  1.        ]]. Action = [[-0.6301588   0.4885255   0.76716924  0.72947764]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 374 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 374 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of 0
Current timestep = 375. State = [[-0.23360316  0.17915307  0.1811696   1.        ]]. Action = [[ 0.6388409  -0.4782344   0.2627493   0.92795277]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 375 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 375 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.21842787  0.1815318   0.20227152  1.        ]]. Action = [[0.27140713 0.69531274 0.98511386 0.4317751 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 376 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 376 is tensor(0.0158, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.21134612  0.20002298  0.22724268  1.        ]]. Action = [[-0.16512972  0.28703356  0.2584666   0.336807  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 377 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 377 is tensor(0.0191, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 0
Current timestep = 378. State = [[-0.21317221  0.19912434  0.24869731  1.        ]]. Action = [[-0.7133989 -0.73905    0.9639807  0.9186652]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 378 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 378 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of 0
Current timestep = 379. State = [[-0.23985842  0.1868576   0.27215213  1.        ]]. Action = [[-0.6566917   0.1954068  -0.84836626  0.8719554 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 379 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 379 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.25520805  0.19954936  0.2686562   1.        ]]. Action = [[-0.01910168  0.6108036   0.7509062   0.9442754 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 380 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 380 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.2621068   0.21167482  0.27487937  1.        ]]. Action = [[-0.7959343  -0.5012944  -0.5944672   0.23963726]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 381 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 381 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.26189125  0.2117915   0.2751506   1.        ]]. Action = [[-0.24646646 -0.17058587 -0.75023913  0.46018183]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 382 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 382 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.26116776  0.21928045  0.28613335  1.        ]]. Action = [[0.08468819 0.42210138 0.87072253 0.8772571 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 384. State = [[-0.2600235   0.22699113  0.30160272  1.        ]]. Action = [[-0.6372075   0.81067467 -0.7610741   0.8288379 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 385. State = [[-0.24898821  0.23751907  0.31572574  1.        ]]. Action = [[0.9505639  0.74237585 0.6486434  0.8953203 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 386. State = [[-0.2345505   0.24983771  0.33674312  1.        ]]. Action = [[-0.8687997   0.0086118   0.30933022  0.69350314]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 387. State = [[-0.23997737  0.25928137  0.342703    1.        ]]. Action = [[-0.676171    0.34672344  0.31621742  0.6233077 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 388. State = [[-0.25068364  0.26684374  0.36027563  1.        ]]. Action = [[-0.2698655  -0.29659915  0.762797    0.8114033 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 389. State = [[-0.24919192  0.25687987  0.3751862   1.        ]]. Action = [[ 0.7681967  -0.23900163 -0.0079093   0.67409146]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 390. State = [[-0.24172883  0.25122723  0.38279125  1.        ]]. Action = [[-0.8053686  -0.54637104  0.9694934   0.7797078 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 391. State = [[-0.2592978   0.16127609  0.10335294  1.        ]]. Action = [[-0.49420285  0.59046364  0.36679304 -0.04722035]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 392. State = [[-0.2459327   0.18440661  0.08371283  1.        ]]. Action = [[ 0.91715837  0.4534489  -0.93311274  0.5267217 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 393. State = [[-0.22625281  0.19340125  0.05751575  1.        ]]. Action = [[-0.88687     0.73269916 -0.70602506  0.6982428 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 394. State = [[-0.23327942  0.20774594  0.0464138   1.        ]]. Action = [[-0.6803584   0.77638257 -0.47331703  0.72949827]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 395. State = [[-0.24084352  0.21343799  0.03844157  1.        ]]. Action = [[ 0.28032565 -0.688183    0.09398615  0.5311315 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 396. State = [[-0.23786531  0.20509687  0.03839974  1.        ]]. Action = [[ 0.5078088  -0.6300399  -0.6434169  -0.03024578]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 397. State = [[-0.23701955  0.20265801  0.03849482  1.        ]]. Action = [[-0.85015744  0.88266766  0.9597491   0.9354644 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 398. State = [[-0.2368745   0.20213413  0.03852035  1.        ]]. Action = [[-0.5782312   0.39249468 -0.7227146   0.9241891 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 399. State = [[-0.23020849  0.19582275  0.04514785  1.        ]]. Action = [[ 0.24352503 -0.43550313  0.827072    0.7076969 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 400. State = [[-0.22388422  0.17548704  0.05263414  1.        ]]. Action = [[-0.25309914 -0.92436635 -0.04380822  0.7985077 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 401. State = [[-0.22930096  0.17186816  0.05405628  1.        ]]. Action = [[-0.61830646  0.65747726  0.1674583   0.89137363]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 402. State = [[-0.24714796  0.18667267  0.06720359  1.        ]]. Action = [[-0.6153434   0.1913774   0.91055655  0.8686991 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 403. State = [[-0.2598704   0.20207727  0.09287313  1.        ]]. Action = [[0.68984985 0.9017296  0.12039995 0.707585  ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 404. State = [[-0.2574534   0.2137568   0.09399711  1.        ]]. Action = [[-0.12127697 -0.1684866  -0.49640965  0.32367313]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 405. State = [[-0.24995078  0.21809971  0.08888844  1.        ]]. Action = [[ 0.83977866  0.33612418 -0.3029018   0.6565567 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 406. State = [[-0.23679613  0.22117762  0.08639898  1.        ]]. Action = [[-0.3466997  -0.35217565  0.57812214  0.72998   ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 407. State = [[-0.23992331  0.20951752  0.08713017  1.        ]]. Action = [[-0.5322238  -0.53461754 -0.16950619  0.81682706]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 408. State = [[-0.24865144  0.18592691  0.08824924  1.        ]]. Action = [[-0.20744514 -0.95160264  0.22538674  0.5646975 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 409. State = [[-0.24765252  0.1610227   0.08788636  1.        ]]. Action = [[ 0.78382075 -0.38533306 -0.77817655  0.13396907]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 410. State = [[-0.2412758   0.14870349  0.07904547  1.        ]]. Action = [[-0.8992042   0.37089753 -0.45286858  0.9446069 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 411. State = [[-0.24283214  0.15623651  0.0842838   1.        ]]. Action = [[-0.37023413  0.62070525  0.68214726  0.882216  ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 412. State = [[-0.24021327  0.15752552  0.08882901  1.        ]]. Action = [[ 0.48990655 -0.55365676 -0.42235267  0.7317965 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 413. State = [[-0.22839527  0.13730925  0.09261868  1.        ]]. Action = [[ 0.2891078  -0.8313399   0.47481287  0.44269454]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 414. State = [[-0.21462853  0.11044592  0.10536418  1.        ]]. Action = [[ 0.28345346 -0.73104036  0.7973596   0.96177506]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 414 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 414 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.19933888  0.08729111  0.1189642   1.        ]]. Action = [[ 0.72228944 -0.03176844 -0.94111264  0.9080558 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 415 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 415 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.1855567   0.0867517   0.10392669  1.        ]]. Action = [[-0.45744783  0.11482441  0.0489397   0.6921487 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 417. State = [[-0.18763429  0.08767298  0.10189628  1.        ]]. Action = [[ 0.08722579  0.07473791 -0.24064761  0.30013537]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 418. State = [[-0.257756   -0.08454784  0.12035645  1.        ]]. Action = [[-0.5215173  -0.8197465  -0.06471914 -0.2501884 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 419. State = [[-0.2590249  -0.09367488  0.10593095  1.        ]]. Action = [[-0.52084446 -0.1925658   0.15423214  0.23981428]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 420. State = [[-0.25237316 -0.0904636   0.10609837  1.        ]]. Action = [[0.5672474  0.32676947 0.02123487 0.5688306 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 421. State = [[-0.24902052 -0.08786672  0.10423753  1.        ]]. Action = [[-0.17331892  0.05708647 -0.17061573  0.61176836]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 422. State = [[-0.23997426 -0.0921965   0.10408723  1.        ]]. Action = [[ 0.6512263  -0.49794042  0.31135416  0.8684634 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 423. State = [[-0.2280147  -0.10095552  0.10788783  1.        ]]. Action = [[ 0.08604646 -0.17409426  0.3562076   0.6156776 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 424. State = [[-0.2254453  -0.0938758   0.12043615  1.        ]]. Action = [[-0.38770723  0.8769361   0.908983    0.56127167]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 425. State = [[-0.25649226  0.17899404  0.12606901  1.        ]]. Action = [[-0.6209385   0.42024064 -0.03071827 -0.29417217]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 426. State = [[-0.2434056   0.20995718  0.12081671  1.        ]]. Action = [[0.6968689  0.67757034 0.88204765 0.9117584 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 426 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 426 is tensor(0.0106, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.23244219  0.23756033  0.13953272  1.        ]]. Action = [[-0.5215553   0.66832805  0.33961332  0.02290213]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 427 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 427 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.24606344  0.2535401   0.14601819  1.        ]]. Action = [[-0.80768865  0.24721825 -0.57302195  0.950137  ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 429. State = [[-0.2346215   0.26046345  0.15355292  1.        ]]. Action = [[0.84462976 0.47595918 0.41734552 0.40435815]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 430. State = [[-0.22302598  0.26497003  0.15866315  1.        ]]. Action = [[-0.5563814  -0.47294116 -0.70266277  0.80441284]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 431. State = [[-0.21461657  0.24472053  0.15001234  1.        ]]. Action = [[ 0.8609414  -0.9299951   0.05569804  0.795079  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 432. State = [[-0.20258214  0.21563555  0.14154641  1.        ]]. Action = [[ 0.09953249 -0.7037156  -0.8685593   0.7238947 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 433. State = [[-0.1968568   0.19912916  0.12801494  1.        ]]. Action = [[-0.31317043 -0.18094063  0.6884258   0.589906  ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 434. State = [[-0.19210212  0.18647392  0.13324583  1.        ]]. Action = [[ 0.04478419 -0.5428381   0.16004038  0.86323166]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 435. State = [[-0.18665661  0.18396786  0.1455906   1.        ]]. Action = [[0.46550453 0.56704926 0.5414088  0.2946905 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 436. State = [[-0.17675361  0.18369429  0.16327307  1.        ]]. Action = [[ 0.44225264 -0.28731441  0.54525316  0.7919433 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 437. State = [[-0.17353125  0.1813899   0.17132014  1.        ]]. Action = [[-0.32185888 -0.06239843 -0.582219    0.90552115]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 438. State = [[-0.18495883  0.18138492  0.17290649  1.        ]]. Action = [[-0.95070577 -0.05403692  0.5021665   0.171862  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 439. State = [[-0.20965326  0.17986687  0.17223825  1.        ]]. Action = [[-0.8033584  -0.06709528 -0.32230592  0.79992723]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 440. State = [[-0.23801912  0.1837629   0.16212393  1.        ]]. Action = [[-0.6417238   0.40578616 -0.6863979   0.730896  ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 441. State = [[-0.26090267  0.19875342  0.15564215  1.        ]]. Action = [[-0.02332813  0.674667    0.44357347  0.6870347 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 442. State = [[-0.26552445  0.22261639  0.16407597  1.        ]]. Action = [[0.25097752 0.7257805  0.55230784 0.9433093 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 443. State = [[-0.26271045  0.24540426  0.16475208  1.        ]]. Action = [[ 0.41218114  0.65660286 -0.7712447   0.9585836 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 444. State = [[-0.25110045  0.27190122  0.16556145  1.        ]]. Action = [[0.32389474 0.928264   0.7159718  0.74380374]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 445. State = [[-0.2501897   0.2936811   0.17101379  1.        ]]. Action = [[-0.22753245  0.22105122 -0.7679186   0.2819085 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 446. State = [[-0.24952814  0.3029819   0.15640849  1.        ]]. Action = [[ 0.6967722   0.9300673  -0.45764875  0.719774  ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 447. State = [[-0.25019333  0.3034657   0.15489532  1.        ]]. Action = [[-0.6908014  -0.7889449  -0.2148773   0.79434896]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 448. State = [[-0.25058877  0.30363536  0.15382342  1.        ]]. Action = [[ 0.59843457  0.6105788  -0.03492779  0.88663316]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 449. State = [[-0.25062463  0.30369443  0.1533225   1.        ]]. Action = [[ 0.3941276   0.2579205  -0.12922847  0.88924336]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 450. State = [[-0.25050393  0.3037771   0.15312758  1.        ]]. Action = [[0.90348744 0.0316416  0.43218148 0.5960374 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 451. State = [[-0.25054717  0.3037444   0.15295945  1.        ]]. Action = [[0.8839104  0.6334952  0.85299754 0.837399  ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 452. State = [[-0.2505913   0.30371106  0.15278807  1.        ]]. Action = [[-0.18745565  0.62376297  0.6080272   0.33591056]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 453. State = [[-0.2505913   0.30371106  0.15278807  1.        ]]. Action = [[ 0.7865323   0.24295926 -0.09823561  0.3239044 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 454. State = [[-0.24364774  0.29913875  0.15485758  1.        ]]. Action = [[ 0.49511135 -0.3733405   0.10838974  0.8436675 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 455. State = [[-0.22739862  0.29260013  0.1646317   1.        ]]. Action = [[ 0.5778532  -0.24109477  0.67762196  0.50990176]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 456. State = [[-0.21213457  0.28875378  0.17766303  1.        ]]. Action = [[-0.3643322   0.83055913  0.38498342  0.735288  ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 457. State = [[-0.20323692  0.28505057  0.17443949  1.        ]]. Action = [[ 0.82293034  0.00580513 -0.60796165  0.874681  ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 458. State = [[-0.18505637  0.28854445  0.16010205  1.        ]]. Action = [[-0.3471979   0.86395335 -0.7302839   0.81094956]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 459. State = [[-0.16894375  0.2848563   0.16572526  1.        ]]. Action = [[ 0.88019514 -0.21722561  0.3506515   0.88799036]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 460. State = [[-0.15123107  0.28209177  0.17123896  1.        ]]. Action = [[0.41951883 0.8531046  0.72599304 0.699867  ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 461. State = [[-0.13434167  0.27277046  0.18434238  1.        ]]. Action = [[ 0.8085623  -0.5085134   0.73033607  0.7497313 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 462. State = [[-0.12749255  0.26846352  0.192072    1.        ]]. Action = [[-0.95336217  0.17256248 -0.65664375  0.8154001 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 463. State = [[-0.13478766  0.2726764   0.19150752  1.        ]]. Action = [[-0.4163618  -0.00791401  0.897321    0.8161123 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 464. State = [[-0.1416166   0.27468607  0.20147349  1.        ]]. Action = [[0.11744368 0.66320705 0.88176286 0.94298697]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 465. State = [[-0.14214402  0.27480337  0.20214859  1.        ]]. Action = [[-0.8944514   0.7624643   0.84732723  0.8355807 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 466. State = [[-0.14076515  0.26348105  0.21623772  1.        ]]. Action = [[-0.2988323  -0.82121223  0.75381255  0.9702902 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 467. State = [[-0.14511682  0.23667455  0.24309687  1.        ]]. Action = [[ 0.10484993 -0.73821115  0.43620753  0.7189865 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 468. State = [[-0.14545783  0.22262655  0.26519886  1.        ]]. Action = [[0.13587308 0.19311345 0.6194248  0.4861226 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 469. State = [[-0.15066911  0.22486523  0.2922923   1.        ]]. Action = [[-0.3502118   0.1343118   0.969771    0.53109217]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 470. State = [[-0.16622195  0.23781723  0.32544175  1.        ]]. Action = [[-0.50715274  0.7626803   0.67957115  0.6646565 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 471. State = [[-0.18168762  0.25261152  0.33246553  1.        ]]. Action = [[-6.3061714e-05  1.5504503e-01 -8.9429188e-01  2.0645142e-01]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 472. State = [[-0.18785872  0.25732     0.32279155  1.        ]]. Action = [[-0.43180478 -0.09308481 -0.09497273  0.8813753 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 473. State = [[-0.19410388  0.25476015  0.31344405  1.        ]]. Action = [[ 0.08704495 -0.23309767 -0.57105774  0.8358785 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 474. State = [[-0.19723012  0.24961212  0.3046102   1.        ]]. Action = [[0.3951416  0.93647254 0.23178148 0.93731236]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 475. State = [[-0.19469094  0.2558782   0.2962152   1.        ]]. Action = [[ 0.8739526   0.6880424  -0.80923235  0.597986  ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 476. State = [[-0.1785415   0.25366655  0.28030068  1.        ]]. Action = [[-0.02812475 -0.88645864  0.91686034  0.7791914 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 477. State = [[-0.16454539  0.23657082  0.2821704   1.        ]]. Action = [[ 0.9865507  -0.13743597 -0.7964465   0.727466  ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 478. State = [[-0.1447391   0.21777438  0.2646839   1.        ]]. Action = [[-0.20223022 -0.96437407 -0.297356    0.40115976]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 479. State = [[-0.13566451  0.20299242  0.25579327  1.        ]]. Action = [[ 0.43058836  0.19693518 -0.6044286   0.5644804 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 480. State = [[-0.12961404  0.1884857   0.24712153  1.        ]]. Action = [[-0.47674906 -0.94751716  0.73118854  0.70059276]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 481. State = [[-0.13194855  0.1643763   0.24663563  1.        ]]. Action = [[-0.12054771 -0.5119237  -0.73469335  0.8625467 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 482. State = [[-0.1291076   0.145177    0.23634797  1.        ]]. Action = [[ 0.82825637 -0.43988526 -0.13908482  0.86970794]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 483. State = [[-0.11880735  0.13027638  0.2312557   1.        ]]. Action = [[ 0.6652994  -0.16728354 -0.8784319   0.7757366 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 484. State = [[-0.11821453  0.12891018  0.23075922  1.        ]]. Action = [[ 0.9421618  -0.65709925 -0.53698266  0.6846603 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 485. State = [[-0.10885076  0.1234814   0.2372511   1.        ]]. Action = [[ 0.7695432  -0.2965123   0.5743365   0.37267828]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 485 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 485 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of 1
Current timestep = 486. State = [[-0.09798366  0.11909277  0.24291919  1.        ]]. Action = [[-0.25210822  0.3616525  -0.13776153  0.909945  ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 487. State = [[-0.09689321  0.13614902  0.239682    1.        ]]. Action = [[ 0.3258742   0.88596773 -0.35423982  0.79435897]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 487 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 487 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[-0.08924536  0.15892532  0.22727658  1.        ]]. Action = [[-0.2647276   0.17621195  0.03388262  0.24115407]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 488 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 488 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.09851252  0.16931029  0.23762292  1.        ]]. Action = [[-0.9228881   0.11112106  0.8181187   0.35039544]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 490. State = [[-0.10379207  0.1834698   0.26370764  1.        ]]. Action = [[0.87789345 0.5325515  0.8738265  0.80886877]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 491. State = [[-0.09499805  0.19882862  0.27465343  1.        ]]. Action = [[ 0.5329832   0.59358406 -0.7588966   0.79651237]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 492. State = [[-0.07662277  0.21518937  0.27076712  1.        ]]. Action = [[0.61290765 0.15167618 0.5574026  0.7482927 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 493. State = [[-0.05704285  0.2159675   0.2793586   1.        ]]. Action = [[ 0.7449553  -0.2618447   0.01001465  0.93703187]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 494. State = [[-0.03562743  0.19911556  0.2905646   1.        ]]. Action = [[-0.30836594 -0.96126795  0.8282627   0.50216126]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 494 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 494 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of 1
Current timestep = 495. State = [[-0.03147608  0.1678536   0.3049907   1.        ]]. Action = [[ 0.7335062  -0.62312704 -0.95981103  0.69830537]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 495 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 495 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.01070958  0.15100138  0.2877974   1.        ]]. Action = [[-0.55434513 -0.2535479   0.44856632  0.81106794]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 496 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 496 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.01774345  0.15829024  0.30976307  1.        ]]. Action = [[-0.6560086   0.7718048   0.94034123  0.56104374]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 497 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 497 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.02792537  0.16959736  0.34382316  1.        ]]. Action = [[ 0.89658904 -0.31800586  0.8747964   0.625561  ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 498 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 498 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of 0
Current timestep = 499. State = [[-0.02040726  0.1588654   0.3602522   1.        ]]. Action = [[ 0.33150363 -0.12456506 -0.13191962  0.8102503 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 499 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 499 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.01158201  0.15890813  0.36517116  1.        ]]. Action = [[0.23537755 0.12640357 0.38614333 0.49835074]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 500 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 500 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.00373941  0.16281533  0.37567818  1.        ]]. Action = [[ 0.40436673 -0.78282684  0.9509785   0.6821356 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 501 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 501 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.00373941  0.16281533  0.37567818  1.        ]]. Action = [[ 0.04751515 -0.22151357  0.8908454   0.9164486 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Scene graph at timestep 502 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 502 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.00373941  0.16281533  0.37567818  1.        ]]. Action = [[-0.7883238 -0.1809175  0.8031492  0.6092942]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 503 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 503 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.00639469  0.16788489  0.37785944  1.        ]]. Action = [[-0.54601943  0.3351053   0.22049916  0.5714674 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 504 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 504 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.26081628 -0.05975357  0.08873482  1.        ]]. Action = [[ 0.54695916  0.33190513 -0.54289114 -0.03702492]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 506. State = [[-0.24771838 -0.07978153  0.07658449  1.        ]]. Action = [[ 0.95872235 -0.8371761   0.06060338  0.94319534]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 507. State = [[-0.2346118  -0.09489025  0.07987453  1.        ]]. Action = [[-0.25277627  0.22038901  0.60900235  0.68691266]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 508. State = [[-0.23418161 -0.09575421  0.07815613  1.        ]]. Action = [[ 0.11738694 -0.15302587 -0.5793687   0.6850058 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 509. State = [[-0.23684181 -0.09682276  0.07748653  1.        ]]. Action = [[-0.5704609   0.07256174  0.4319433   0.7402648 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 510. State = [[-0.23335835 -0.08942271  0.08139634  1.        ]]. Action = [[0.78085375 0.5711236  0.2527485  0.50165653]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 511. State = [[-0.22674939 -0.08235855  0.09631537  1.        ]]. Action = [[-0.02248424  0.04556823  0.5633677   0.82448494]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 512. State = [[-0.22050308 -0.07227488  0.11860383  1.        ]]. Action = [[0.30199003 0.49940205 0.78975844 0.6897707 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 513. State = [[-0.21751176 -0.06822612  0.14437015  1.        ]]. Action = [[-0.50540394 -0.51304775  0.6461835   0.89434505]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 514. State = [[-0.2191843  -0.07111003  0.17120299  1.        ]]. Action = [[-0.00515008  0.21317208  0.7087724   0.8185561 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 515. State = [[-0.21329275 -0.05940073  0.1955791   1.        ]]. Action = [[0.6458657  0.61415696 0.39741802 0.6787945 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 516. State = [[-0.19454055 -0.05104198  0.2163768   1.        ]]. Action = [[ 0.6407187  -0.23294067  0.6850909   0.92429876]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 517. State = [[-0.17718227 -0.05196083  0.23338133  1.        ]]. Action = [[ 0.92675817 -0.24260986 -0.9721581   0.44465792]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 518. State = [[-0.16362478 -0.04838596  0.24301001  1.        ]]. Action = [[0.9098271  0.298254   0.32340455 0.7818847 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 519. State = [[-0.14829287 -0.04511249  0.25558192  1.        ]]. Action = [[ 0.4664036  0.4112357 -0.5306601  0.9140055]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 520. State = [[-0.13157356 -0.05029206  0.269732    1.        ]]. Action = [[ 0.86179423 -0.48097783  0.77801335  0.81521225]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 521. State = [[-0.11843848 -0.04875181  0.28769216  1.        ]]. Action = [[-0.59669715  0.43533814  0.2241342   0.645489  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 522. State = [[-0.12019864 -0.05940397  0.30473354  1.        ]]. Action = [[ 0.04984438 -0.89143324  0.9536145   0.66815543]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 523. State = [[-0.12479208 -0.06158953  0.33378968  1.        ]]. Action = [[-0.41872203  0.77139306  0.48054528  0.4249636 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 524. State = [[-0.2598155   0.01110133  0.11056045  1.        ]]. Action = [[-0.02539194  0.03158498  0.9109688  -0.02005988]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 525. State = [[-0.25382     0.02136382  0.10442793  1.        ]]. Action = [[0.52684855 0.59522986 0.86712027 0.76824045]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 526. State = [[-0.23825276  0.02379156  0.10946448  1.        ]]. Action = [[ 0.50903296 -0.7434211  -0.42988098  0.817708  ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 527. State = [[-0.23189047  0.01698571  0.10746409  1.        ]]. Action = [[-0.28760326  0.03493345 -0.01993436  0.44135976]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 528. State = [[-0.23572566  0.01275789  0.10665314  1.        ]]. Action = [[-0.32570797 -0.17820805  0.19040596  0.522593  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 529. State = [[-0.23783326  0.00870041  0.10653572  1.        ]]. Action = [[-0.845156   0.6498729  0.5160222  0.6841693]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 530. State = [[-0.22743435 -0.00624988  0.115669    1.        ]]. Action = [[ 0.9500003  -0.8398018   0.75991774  0.5287955 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 531. State = [[-0.20232287 -0.01733639  0.14253095  1.        ]]. Action = [[0.6353831  0.22456944 0.93328273 0.9141458 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 532. State = [[-0.18554562 -0.01629613  0.16567469  1.        ]]. Action = [[ 0.88035893 -0.6863892   0.71566105  0.8579564 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 533. State = [[-0.18152252 -0.01622002  0.16799597  1.        ]]. Action = [[0.83095694 0.5199686  0.7423873  0.65286076]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 534. State = [[-0.18115717 -0.01622298  0.16818663  1.        ]]. Action = [[ 0.5807829   0.42340374 -0.17257005  0.9569037 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 535. State = [[-0.18043621 -0.012362    0.16884688  1.        ]]. Action = [[ 0.12163484  0.3125     -0.03444546  0.10866117]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 536. State = [[-0.18101868 -0.02114505  0.16868684  1.        ]]. Action = [[-0.34942007 -0.8188505  -0.22103888  0.69294167]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 537. State = [[-0.18499978 -0.03268712  0.15946853  1.        ]]. Action = [[ 0.0846802  -0.04700613 -0.92658997  0.931427  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 538. State = [[-0.18352805 -0.04917286  0.14097011  1.        ]]. Action = [[ 0.27408767 -0.91918194 -0.14903724  0.71207595]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 539. State = [[-0.18020177 -0.08140253  0.14310698  1.        ]]. Action = [[-0.4686265  -0.71092457  0.6535753   0.82881474]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 540. State = [[-0.18317184 -0.09732052  0.14625902  1.        ]]. Action = [[ 0.6851201  -0.7471398  -0.12314034  0.7640078 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 541. State = [[-0.1837819  -0.0999518   0.14672707  1.        ]]. Action = [[ 0.7794665   0.3237877  -0.14052606  0.85471225]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 542. State = [[-0.1887059  -0.106966    0.14607155  1.        ]]. Action = [[-0.38308644 -0.3560977  -0.16437668  0.82378817]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 543. State = [[-0.20110059 -0.10626214  0.14837918  1.        ]]. Action = [[-0.82870007  0.5204332   0.361094    0.771214  ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 544. State = [[-0.22680427 -0.11066915  0.16190624  1.        ]]. Action = [[-0.55756044 -0.57083887  0.46732104  0.6490202 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 545. State = [[-0.23751923 -0.11311489  0.17014194  1.        ]]. Action = [[ 0.5486511   0.30559504 -0.38449484  0.831491  ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 546. State = [[-0.23039761 -0.10026828  0.1776058   1.        ]]. Action = [[0.35223722 0.64788115 0.88875115 0.7484982 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 547. State = [[-0.21314691 -0.09738521  0.190379    1.        ]]. Action = [[ 0.93693185 -0.66387063 -0.00706196  0.48960125]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 548. State = [[-0.2005987  -0.10139699  0.19109255  1.        ]]. Action = [[-0.12147534  0.19797826 -0.4812854   0.93075526]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 549. State = [[-0.20426115 -0.11538462  0.18657263  1.        ]]. Action = [[-0.7933135  -0.82551825  0.24532127  0.87829685]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 550. State = [[-0.20336848 -0.1310341   0.20040476  1.        ]]. Action = [[ 0.72274756 -0.1656304   0.98833656  0.73285437]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 550 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 550 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.2569847  -0.0466738   0.11906967  1.        ]]. Action = [[ 0.8115511   0.6936741   0.9018588  -0.05236459]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 552. State = [[-0.24819064 -0.04650608  0.1104726   1.        ]]. Action = [[0.72418   0.5074775 0.6070638 0.3433075]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 553. State = [[-0.23738225 -0.04257598  0.11285177  1.        ]]. Action = [[-0.8544334  -0.20290542  0.9399512   0.73894954]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 554. State = [[-0.23745534 -0.03669962  0.12302724  1.        ]]. Action = [[-0.58892596  0.25023437  0.84225106  0.5551634 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 555. State = [[-0.2429568  -0.03106348  0.14151946  1.        ]]. Action = [[-0.952337   -0.8442326   0.3185904   0.94597316]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 556. State = [[-0.24674231 -0.03995115  0.14495261  1.        ]]. Action = [[-0.35701776 -0.66331697  0.11646044  0.90611565]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 557. State = [[-0.25604874 -0.061837    0.15813452  1.        ]]. Action = [[-0.3218603  -0.69186455  0.854275    0.58870363]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 558. State = [[-0.2616995  -0.07626644  0.18044356  1.        ]]. Action = [[-0.39480746  0.20111203 -0.45954657  0.6898103 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 559. State = [[-0.2631824 -0.0780985  0.1782207  1.       ]]. Action = [[ 0.25798416  0.06569564 -0.5921038   0.61364055]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 560. State = [[-0.2527216  -0.09184408  0.18382193  1.        ]]. Action = [[ 0.65926623 -0.92718273  0.9555303   0.948488  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 561. State = [[-0.24393784 -0.12037442  0.20031728  1.        ]]. Action = [[-0.04424316 -0.7561649   0.39377832  0.6655785 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 562. State = [[-0.24254121 -0.13683785  0.21255691  1.        ]]. Action = [[-0.7052637  -0.5939687   0.05977082  0.51571727]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 562 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 562 is tensor(0.0276, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 562 of 0
Current timestep = 563. State = [[-0.2353572  -0.13179101  0.22163378  1.        ]]. Action = [[0.5014951  0.75854254 0.55535126 0.623719  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 563 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 563 is tensor(0.0249, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of 1
Current timestep = 564. State = [[-0.21640493 -0.11648325  0.23814194  1.        ]]. Action = [[0.5535461  0.14169002 0.00120783 0.8865396 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 565. State = [[-0.20281492 -0.10911997  0.24518628  1.        ]]. Action = [[0.30523872 0.29250753 0.15710211 0.7841823 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 566. State = [[-0.19770947 -0.11255545  0.24266092  1.        ]]. Action = [[-0.12861568 -0.65329206 -0.5771631   0.36249232]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 567. State = [[-0.18549065 -0.11910262  0.2375652   1.        ]]. Action = [[0.8700497  0.01253676 0.22542596 0.7975254 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 568. State = [[-0.1762093  -0.13129346  0.24816792  1.        ]]. Action = [[-0.81357723 -0.5662619   0.96202874  0.45534742]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 568 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 568 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 568 of 1
Current timestep = 569. State = [[-0.17876706 -0.13803308  0.27963576  1.        ]]. Action = [[0.5270891  0.42522967 0.92324615 0.7713127 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 569 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 569 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 569 of 0
Current timestep = 570. State = [[-0.16758518 -0.14081313  0.30493128  1.        ]]. Action = [[-0.2198857 -0.5634285 -0.1956408  0.8549191]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 570 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 570 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of -1
Current timestep = 571. State = [[-0.17349312 -0.16282016  0.31526634  1.        ]]. Action = [[ 0.0066905  -0.7160572   0.8093195   0.15406144]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 571 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 571 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.16444625 -0.18088217  0.34178764  1.        ]]. Action = [[ 0.793831   -0.26487333  0.80194426  0.74869347]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 572 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 572 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.13776948 -0.19230412  0.380421    1.        ]]. Action = [[ 0.8893776  -0.43782568  0.7177887   0.86083233]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 573 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 573 is tensor(0.0154, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of 0
Current timestep = 574. State = [[-0.11661797 -0.20121568  0.398507    1.        ]]. Action = [[ 0.98407245 -0.68301404  0.94341516  0.66924644]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 574 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 574 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.10574545 -0.21214795  0.40089697  1.        ]]. Action = [[ 0.85766816 -0.6657876   0.18606615  0.48298955]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 575 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 575 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of 0
Current timestep = 576. State = [[-0.08278365 -0.22681035  0.4078057   1.        ]]. Action = [[ 0.5755842  -0.52386266  0.82648313  0.88357353]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 576 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 576 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.079612   -0.2185717   0.39959803  1.        ]]. Action = [[ 0.22233653  0.6570811  -0.839166    0.72413945]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 577 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 577 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of 0
Current timestep = 578. State = [[-0.06128627 -0.21386182  0.3883123   1.        ]]. Action = [[ 0.8750031  -0.40212345  0.5267403   0.3381015 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 578 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 578 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 578 of 0
Current timestep = 579. State = [[-0.04247037 -0.22341774  0.40086555  1.        ]]. Action = [[-0.24951077 -0.2817608   0.26570845  0.74035144]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 579 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 579 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 579 of -1
Current timestep = 580. State = [[-0.04523621 -0.22962219  0.40519533  1.        ]]. Action = [[-0.36383343  0.23415494 -0.28001493  0.96502924]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 580 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 580 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of -1
Current timestep = 581. State = [[-0.04539715 -0.22929259  0.40515932  1.        ]]. Action = [[0.67816234 0.08345544 0.77715397 0.28057528]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 581 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 581 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 581 of -1
Current timestep = 582. State = [[-0.04940176 -0.22727081  0.40021548  1.        ]]. Action = [[-0.42834413  0.16025615 -0.33243245  0.20065594]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 582 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 582 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 582 of 0
Current timestep = 583. State = [[-0.05011201 -0.22222267  0.3916084   1.        ]]. Action = [[0.96537256 0.09757042 0.05543458 0.7055727 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 583 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 583 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of 0
Current timestep = 584. State = [[-0.04422902 -0.21001485  0.3845818   1.        ]]. Action = [[-0.6008296   0.52716684 -0.66367847  0.37384915]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 584 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 584 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of 1
Current timestep = 585. State = [[-0.04758761 -0.201426    0.37790626  1.        ]]. Action = [[ 0.9648019  -0.40348363  0.7858397   0.8544586 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 585 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 585 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[-0.04912192 -0.21045567  0.37743846  1.        ]]. Action = [[ 0.05373824 -0.7583884  -0.00111562  0.8913454 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 586 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 586 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.05198503 -0.22498614  0.38146877  1.        ]]. Action = [[-0.10778779 -0.11430436  0.6748595   0.597558  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 587 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 587 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 587 of 0
Current timestep = 588. State = [[-0.05435402 -0.22956878  0.3857919   1.        ]]. Action = [[ 0.32653487  0.1763326   0.7623699  -0.23434913]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 588 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 588 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.05456033 -0.22996213  0.3860066   1.        ]]. Action = [[-0.11779147 -0.12406152  0.7276106   0.7369274 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 589 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 589 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.0483618  -0.22691853  0.38283363  1.        ]]. Action = [[ 0.7480352   0.12753022 -0.32333636  0.7400421 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 590 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 590 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of 0
Current timestep = 591. State = [[-0.03606703 -0.21882953  0.38088942  1.        ]]. Action = [[0.18496919 0.3587818  0.15605748 0.7601516 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 591 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 591 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.03012668 -0.20217754  0.38525873  1.        ]]. Action = [[-0.5631459   0.71125484  0.00154126  0.44746375]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 592 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 592 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of 1
Current timestep = 593. State = [[-0.03084497 -0.18784921  0.3869276   1.        ]]. Action = [[0.36833835 0.09204173 0.9809375  0.8082609 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 593 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 593 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.03300323 -0.1943192   0.3846368   1.        ]]. Action = [[-0.02326965 -0.5222579  -0.3070475   0.82757735]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 594 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 594 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 594 of 0
Current timestep = 595. State = [[-0.03796975 -0.1997485   0.37967622  1.        ]]. Action = [[0.9983678  0.641145   0.79063797 0.5824504 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 595 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 595 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.03796975 -0.1997485   0.37967622  1.        ]]. Action = [[ 0.75429714 -0.14130759  0.86226976  0.8539934 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 596 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 596 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.03801506 -0.19975053  0.3796215   1.        ]]. Action = [[ 0.9839885  -0.27119923  0.9722314   0.75582147]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 597 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 597 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.03801506 -0.19975053  0.3796215   1.        ]]. Action = [[-0.8911941  -0.37827897  0.86544824  0.85545814]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 598 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 598 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.04980965 -0.21507882  0.37029174  1.        ]]. Action = [[-0.9128576 -0.8333763 -0.9040988  0.6120634]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 599 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 599 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.07890257 -0.2303831   0.3392116   1.        ]]. Action = [[-0.6130311   0.36964893 -0.36803973  0.5213444 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 600 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 600 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.09153376 -0.23837131  0.33142722  1.        ]]. Action = [[-0.10422426 -0.682722    0.18503833  0.15210009]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 601 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 601 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.1009492  -0.26104867  0.34259975  1.        ]]. Action = [[-0.19078326 -0.7565469   0.36586797  0.70790684]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 602 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 602 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.11530191 -0.28755456  0.36444378  1.        ]]. Action = [[-0.6718068  -0.42382407  0.9665469   0.795311  ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 603 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 603 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.12940307 -0.30048484  0.38478625  1.        ]]. Action = [[-0.3895713  -0.68661976  0.86425745  0.44191575]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 604 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 604 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.12963708 -0.3004818   0.38486922  1.        ]]. Action = [[-0.15413636 -0.2075417  -0.22322452  0.5769248 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 605 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 605 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of -1
Current timestep = 606. State = [[-0.12963708 -0.3004818   0.38486922  1.        ]]. Action = [[ 0.6837256  -0.7964616  -0.5949667   0.65183187]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 606 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 606 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.12964253 -0.30048162  0.38492036  1.        ]]. Action = [[-0.6591802  -0.28148788 -0.6676294   0.7761233 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 607 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 607 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.12940437 -0.29280487  0.38928428  1.        ]]. Action = [[-0.00126219  0.5059626   0.38570952  0.8086369 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 608 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 608 is tensor(0.0131, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of 0
Current timestep = 609. State = [[-0.13062212 -0.28353968  0.39684558  1.        ]]. Action = [[ 0.05033398 -0.7194253   0.2978431   0.7919736 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 609 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 609 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of -1
Current timestep = 610. State = [[-0.13062875 -0.28353968  0.3968957   1.        ]]. Action = [[ 0.34925163 -0.6110261   0.29785872  0.7667657 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 610 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 610 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.13062875 -0.28353968  0.3968957   1.        ]]. Action = [[ 0.52087307 -0.94817245 -0.84819263  0.76396465]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 611 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 611 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.13062875 -0.28353968  0.3968957   1.        ]]. Action = [[ 0.14024842 -0.5427006  -0.43576354  0.6834848 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Scene graph at timestep 612 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 612 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.13062875 -0.28353968  0.3968957   1.        ]]. Action = [[-0.0439229   0.09006488  0.9675281   0.6505382 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 613 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 613 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of -1
Current timestep = 614. State = [[-0.13062875 -0.28353968  0.3968957   1.        ]]. Action = [[ 0.2600106 -0.1713103  0.7517822  0.5236385]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 614 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 614 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.13062875 -0.28353968  0.3968957   1.        ]]. Action = [[-0.02311265  0.31496787  0.96698093  0.8792474 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Scene graph at timestep 615 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 615 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.12546372 -0.27952453  0.38751453  1.        ]]. Action = [[ 0.8072059   0.02306128 -0.7689184   0.5360987 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 616 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 616 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of 0
Current timestep = 617. State = [[-0.1096571  -0.26949334  0.38133392  1.        ]]. Action = [[0.42018116 0.43586755 0.6472081  0.7871897 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 617 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 617 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 0
Current timestep = 618. State = [[-0.10111903 -0.26139617  0.38823858  1.        ]]. Action = [[ 0.49452794 -0.4426629   0.6962333   0.38626206]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Scene graph at timestep 618 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 618 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 618 of -1
Current timestep = 619. State = [[-0.09156813 -0.26327458  0.38915768  1.        ]]. Action = [[ 0.86831    -0.34658027 -0.02764297  0.5940143 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 619 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 619 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.06145246 -0.25539172  0.39365333  1.        ]]. Action = [[0.8643732  0.6293011  0.36819816 0.68850756]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 620 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 620 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of 1
Current timestep = 621. State = [[-0.03759807 -0.2454121   0.40317866  1.        ]]. Action = [[ 0.6712655  -0.27993405  0.96819806  0.9024743 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Scene graph at timestep 621 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 621 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.03769546 -0.24526985  0.40321657  1.        ]]. Action = [[-0.36656868 -0.57348776  0.72410226  0.78658605]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 622 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 622 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.03769546 -0.24526985  0.40321657  1.        ]]. Action = [[-0.29131746 -0.3445152   0.79699695  0.8782363 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 623 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 623 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.03769546 -0.24526985  0.40321657  1.        ]]. Action = [[ 0.15935981 -0.5680151   0.3575716   0.9331205 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 624 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 624 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of -1
Current timestep = 625. State = [[-0.03548676 -0.2359003   0.40505353  1.        ]]. Action = [[-0.03212816  0.65548897  0.16391301  0.68122077]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 625 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 625 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.03485127 -0.22482464  0.40527186  1.        ]]. Action = [[-0.75288194 -0.12383366  0.8626821   0.39302647]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Scene graph at timestep 626 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 626 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.03429195 -0.22431037  0.40601483  1.        ]]. Action = [[ 0.8566685  -0.66930705  0.55664206  0.8140831 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 627 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 627 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.03306041 -0.22013183  0.4018632   1.        ]]. Action = [[ 0.05389249  0.26728725 -0.5031494   0.6906667 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 628 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 628 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.03576697 -0.2252706   0.38703245  1.        ]]. Action = [[-0.15840954 -0.75874394 -0.91152346  0.93605816]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 629 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 629 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of 0
Current timestep = 630. State = [[-0.02887698 -0.2292339   0.37360725  1.        ]]. Action = [[0.360386   0.53367376 0.75847566 0.6251688 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 630 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 630 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 0
Current timestep = 631. State = [[-0.01881981 -0.22396514  0.383827    1.        ]]. Action = [[ 0.37593937 -0.22460377  0.06631351  0.72245693]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 631 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 631 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of -1
Current timestep = 632. State = [[-0.01890471 -0.23526411  0.3924217   1.        ]]. Action = [[-0.13908798 -0.69342667  0.5719408   0.50663877]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 632 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 632 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.01857528 -0.24771667  0.40422463  1.        ]]. Action = [[-0.6790345  -0.9063941   0.96297467  0.85375905]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Scene graph at timestep 633 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 633 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.01859518 -0.24769735  0.40422294  1.        ]]. Action = [[-0.42346275 -0.5803786   0.40661907  0.6328174 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Scene graph at timestep 634 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 634 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.01846923 -0.24766175  0.40436974  1.        ]]. Action = [[0.3340708  0.6757597  0.79522777 0.83563995]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 635 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 635 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.01472776 -0.24358578  0.39568552  1.        ]]. Action = [[ 0.7109914   0.11584461 -0.8680379   0.61607623]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 636 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 636 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 0
Current timestep = 637. State = [[ 0.00406232 -0.24378715  0.37531155  1.        ]]. Action = [[0.0223248  0.7356833  0.89468694 0.51120496]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Scene graph at timestep 637 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 637 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[ 0.01214803 -0.23586668  0.37500408  1.        ]]. Action = [[ 0.815459    0.4342252  -0.35628128  0.62053967]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 638 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 638 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 638 of 0
Current timestep = 639. State = [[ 0.04982091 -0.22259909  0.36651796  1.        ]]. Action = [[0.7251847  0.32101798 0.66054535 0.3751757 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 639 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 639 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[ 0.06607533 -0.21014278  0.3781601   1.        ]]. Action = [[-0.49598432  0.46029234 -0.49817598  0.69903076]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 640 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 640 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of 1
Current timestep = 641. State = [[ 0.05878458 -0.21072398  0.36367175  1.        ]]. Action = [[-0.4258455  -0.19102424 -0.9615435   0.42435575]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 641 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 641 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of 0
Current timestep = 642. State = [[ 0.04944002 -0.22195649  0.34327093  1.        ]]. Action = [[-0.4496429  -0.51858085  0.09825087  0.6539488 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 642 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 642 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[ 0.03936664 -0.24419823  0.3535781   1.        ]]. Action = [[ 0.3203125  -0.7145673   0.8766153   0.66052103]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 643 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 643 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of -1
Current timestep = 644. State = [[ 0.04110343 -0.26019463  0.37915882  1.        ]]. Action = [[ 0.3008144  -0.33190805  0.87500274  0.7269815 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 644 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 644 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[ 0.04365214 -0.26513883  0.39534757  1.        ]]. Action = [[-0.23611987 -0.14426875  0.95192575  0.7516358 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Scene graph at timestep 645 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 645 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[ 0.0374064  -0.27976865  0.39585063  1.        ]]. Action = [[-0.45095956 -0.8649808  -0.22535795  0.16655707]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 646 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 646 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[ 0.0333592  -0.30058292  0.3962965   1.        ]]. Action = [[0.16969097 0.41974187 0.72434163 0.7230563 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Scene graph at timestep 647 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 647 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[ 0.03745041 -0.29187152  0.39021403  1.        ]]. Action = [[ 0.81825304  0.47878432 -0.7209909   0.5970378 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 648 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 648 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of 0
Current timestep = 649. State = [[ 0.05864442 -0.27871114  0.37436673  1.        ]]. Action = [[-0.12977326  0.4545759   0.75779605  0.5519481 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 649 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 649 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of 0
Current timestep = 650. State = [[ 0.06134006 -0.2605981   0.38476625  1.        ]]. Action = [[-0.3376242   0.7234535  -0.62427276  0.9110315 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 650 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 650 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of 1
Current timestep = 651. State = [[ 0.05001371 -0.25871524  0.37296095  1.        ]]. Action = [[-0.56876445 -0.88805616 -0.47140908  0.8541491 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 651 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 651 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[ 0.03127411 -0.26484329  0.34787732  1.        ]]. Action = [[-0.3997953   0.78064823 -0.9591607   0.46445334]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 652 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 652 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 652 of 1
Current timestep = 653. State = [[-0.25775898 -0.10614299  0.09893817  1.        ]]. Action = [[0.3653221  0.4704399  0.0909301  0.64092743]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 654. State = [[-0.258778   -0.1162743   0.08579951  1.        ]]. Action = [[-0.29500973  0.71742654  0.864033    0.6305622 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 655. State = [[-0.25571075 -0.11084964  0.09240068  1.        ]]. Action = [[0.01521897 0.57210827 0.90504885 0.42112434]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 656. State = [[-0.24918243 -0.09519727  0.10247486  1.        ]]. Action = [[ 0.56733656  0.6003151  -0.02753407  0.6185825 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 657. State = [[-0.2325074  -0.07039639  0.10501806  1.        ]]. Action = [[0.86079    0.697489   0.02215469 0.14897537]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 658. State = [[-0.21887735 -0.05004306  0.11411918  1.        ]]. Action = [[-0.44452417  0.30868554  0.5238452   0.8725147 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 659. State = [[-0.2631689   0.17030172  0.12220668  1.        ]]. Action = [[-0.33923197 -0.0763886   0.9865359  -0.07086885]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 660. State = [[-0.2504139   0.19212523  0.10445155  1.        ]]. Action = [[ 0.74769163  0.06374204 -0.35656393  0.6465707 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 661. State = [[-0.22414476  0.19012502  0.09047832  1.        ]]. Action = [[ 0.8850322  -0.18725556 -0.31830144  0.2547778 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 662. State = [[-0.20146112  0.19464852  0.07477696  1.        ]]. Action = [[ 0.47919226  0.5268158  -0.7137204   0.83389413]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 663. State = [[-0.19712098  0.2141236   0.05874469  1.        ]]. Action = [[-0.81532526  0.71223736  0.44662213  0.5655619 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 664. State = [[-0.19779117  0.23940945  0.06935031  1.        ]]. Action = [[0.779336   0.50962424 0.59418344 0.5275862 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 665. State = [[-0.18199576  0.24133654  0.09018756  1.        ]]. Action = [[-0.22791481 -0.7965763   0.86004734  0.7352483 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 666. State = [[-0.17075886  0.22412258  0.1201107   1.        ]]. Action = [[ 0.51131463 -0.39299047  0.68096924  0.897213  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 667. State = [[-0.16591407  0.21683344  0.14791706  1.        ]]. Action = [[-0.60542    -0.076469    0.6984408   0.41400564]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 668. State = [[-0.17019396  0.20624952  0.16797611  1.        ]]. Action = [[-0.24785209 -0.69197816  0.07264996  0.4489889 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 669. State = [[-0.17615895  0.19101113  0.18587944  1.        ]]. Action = [[-0.11416483 -0.19507813  0.95264363  0.75942373]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 670. State = [[-0.17609005  0.18216668  0.20498961  1.        ]]. Action = [[ 0.83331203 -0.04684567 -0.3279922   0.7273865 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 671. State = [[-0.17715712  0.18902682  0.19679125  1.        ]]. Action = [[-0.25131583  0.6584432  -0.82277787  0.23452091]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 672. State = [[-0.17873919  0.18326718  0.16985837  1.        ]]. Action = [[-0.13757592 -0.86904496 -0.92726827  0.46178317]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 673. State = [[-0.17309588  0.17797889  0.14094687  1.        ]]. Action = [[ 0.69195414  0.35053623 -0.5539493   0.7248826 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 674. State = [[-0.16470915  0.18127783  0.12306022  1.        ]]. Action = [[ 0.80768466 -0.6824955  -0.6797116   0.91010666]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 675. State = [[-0.16986817  0.19254111  0.12214255  1.        ]]. Action = [[-0.651093   0.6878121  0.3346908  0.8196714]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 676. State = [[-0.18669748  0.21780336  0.11597124  1.        ]]. Action = [[-0.44887483  0.9569328  -0.76910555  0.72122955]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 677. State = [[-0.19199955  0.23221926  0.09830685  1.        ]]. Action = [[ 0.75835013 -0.16124171 -0.7214868   0.9151802 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 678. State = [[-0.19028208  0.23684269  0.06803117  1.        ]]. Action = [[-8.0618846e-01 -4.3082237e-04 -6.2076789e-01  5.6254065e-01]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 679. State = [[-0.20051427  0.23306994  0.05019679  1.        ]]. Action = [[-0.48639035 -0.39146483 -0.06555295  0.91056633]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 680. State = [[-0.19613083  0.21545658  0.05291881  1.        ]]. Action = [[ 0.60373425 -0.8066364   0.67536914  0.46709633]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 681. State = [[-0.18661244  0.19878629  0.05592815  1.        ]]. Action = [[ 0.3880769  -0.11110759 -0.5189121   0.76563   ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 682. State = [[-0.18320726  0.18819748  0.0628555   1.        ]]. Action = [[-0.48351032 -0.38743687  0.97969747  0.6829877 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 683. State = [[-0.19328049  0.19277382  0.07029093  1.        ]]. Action = [[-0.06568789  0.8934082  -0.5827464   0.74652815]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 684. State = [[-0.19432767  0.19698463  0.06188301  1.        ]]. Action = [[ 0.5425272  -0.29310763 -0.56399626  0.8532922 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 685. State = [[-0.19133988  0.1838883   0.04860456  1.        ]]. Action = [[-0.26701117 -0.916211   -0.3628226   0.6465857 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 686. State = [[-0.18730617  0.16841821  0.03602169  1.        ]]. Action = [[ 0.6101346  -0.04969734 -0.8665487   0.8152497 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 687. State = [[-0.1875362   0.16715157  0.03393303  1.        ]]. Action = [[-0.70435673  0.32033944 -0.43828177  0.4434626 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 688. State = [[-0.17719093  0.1644531   0.0393488   1.        ]]. Action = [[ 0.7238201  -0.0860377   0.63661087  0.8836596 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 689. State = [[-0.1579376   0.17209738  0.05500077  1.        ]]. Action = [[0.74155235 0.72035766 0.73641634 0.73853934]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 690. State = [[-0.14195335  0.18215095  0.06890087  1.        ]]. Action = [[ 0.5647011   0.97659874 -0.70048356  0.6788311 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 691. State = [[-0.13862757  0.18377863  0.07117559  1.        ]]. Action = [[ 0.42880595 -0.78555685  0.9598017   0.64502144]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 692. State = [[-0.14429556  0.18410495  0.07031964  1.        ]]. Action = [[-0.89361995 -0.11677694  0.04141545  0.8533689 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 693. State = [[-0.16641237  0.19743888  0.06426159  1.        ]]. Action = [[-0.85844016  0.91714835 -0.47119248  0.39329028]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 694. State = [[-0.1843356  0.2161353  0.0624244  1.       ]]. Action = [[ 0.29162574  0.8448372  -0.86756593  0.12104082]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 695. State = [[-0.18667091  0.21851498  0.06201708  1.        ]]. Action = [[-0.69734985  0.8216028  -0.7152436   0.41195965]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 696. State = [[-0.1833506   0.22205155  0.05603075  1.        ]]. Action = [[ 0.8417008   0.40357506 -0.6519406   0.8778248 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 697. State = [[-0.16257206  0.22372395  0.05119488  1.        ]]. Action = [[ 0.92807746 -0.28805482  0.37687182  0.32798684]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 698. State = [[-0.14324759  0.21251486  0.05907931  1.        ]]. Action = [[ 0.27048826 -0.50849676  0.41465008  0.41271758]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 699. State = [[-0.13627492  0.2106561   0.07215361  1.        ]]. Action = [[-0.27086395  0.4396236   0.74246264  0.7232549 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 700. State = [[-0.1256977   0.22752018  0.08852937  1.        ]]. Action = [[ 0.7417458   0.718737   -0.01492888  0.64536345]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 701. State = [[-0.11943052  0.25354356  0.08835815  1.        ]]. Action = [[-0.02396768  0.9953531  -0.78840286  0.8065436 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 702. State = [[-0.10211363  0.2723805   0.07937346  1.        ]]. Action = [[ 0.9524139  -0.1128552   0.6372137   0.79171467]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 703. State = [[-0.0697967   0.26363954  0.10448619  1.        ]]. Action = [[ 0.75773644 -0.8790513   0.97043943  0.7228359 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 704. State = [[-0.05670998  0.2530424   0.12226222  1.        ]]. Action = [[-0.5725594   0.05291104 -0.30511647  0.32371235]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 705. State = [[-0.05756265  0.26304397  0.13259405  1.        ]]. Action = [[-0.01129693  0.63292885  0.96449995  0.91966915]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 706. State = [[-0.05419269  0.27453393  0.13933888  1.        ]]. Action = [[ 0.8522713   0.42744815 -0.8654316   0.84760165]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 707. State = [[-0.03634986  0.2854463   0.11157374  1.        ]]. Action = [[ 0.690758    0.08781052 -0.79423094  0.5473752 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 707 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 707 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 707 of -1
Current timestep = 708. State = [[-0.00105022  0.2848118   0.09245119  1.        ]]. Action = [[-0.3236357  -0.9104855   0.33619153  0.7213142 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 708 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 708 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of 1
Current timestep = 709. State = [[0.00287531 0.2683384  0.10068718 1.        ]]. Action = [[ 0.49614406  0.7990949  -0.30572313  0.75222015]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 709 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 709 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[-0.00157398  0.2601306   0.09199604  1.        ]]. Action = [[-0.4547946  -0.559535   -0.7013685   0.09809279]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 710 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 710 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of -1
Current timestep = 711. State = [[-0.01557656  0.2347994   0.07632219  1.        ]]. Action = [[-0.94729024 -0.8807773  -0.09784216  0.7594259 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 711 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 711 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 711 of -1
Current timestep = 712. State = [[-0.02606288  0.2030617   0.07604752  1.        ]]. Action = [[ 0.9476695  -0.49588454  0.23419905  0.5140734 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 712 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 712 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of 1
Current timestep = 713. State = [[-0.02580403  0.19509344  0.074537    1.        ]]. Action = [[-0.8796316   0.261055   -0.77911264  0.42293262]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 713 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 713 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of -1
Current timestep = 714. State = [[-0.03496022  0.20826125  0.0555091   1.        ]]. Action = [[ 0.74537814  0.75828433 -0.2952003   0.95691323]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 714 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 714 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of -1
Current timestep = 715. State = [[-0.0308367   0.22097075  0.04534119  1.        ]]. Action = [[ 0.20342481 -0.7045051  -0.5130169   0.80135846]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 715 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 715 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of -1
Current timestep = 716. State = [[-0.02503127  0.22272271  0.05457885  1.        ]]. Action = [[ 0.16720867 -0.02662677  0.8007754   0.8443489 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 716 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 716 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.00724478  0.21380761  0.07872409  1.        ]]. Action = [[ 0.65170145 -0.6751412   0.85394156  0.6999204 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 717 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 717 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[8.1495295e-05 2.0069347e-01 9.0819694e-02 1.0000000e+00]]. Action = [[ 0.11930537  0.00618958 -0.9239082   0.7639502 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 718 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 718 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of -1
Current timestep = 719. State = [[-0.00309931  0.20168899  0.07276589  1.        ]]. Action = [[-0.42655742  0.15582347  0.29992664  0.61553013]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 719 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 719 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.0100646   0.20472404  0.07409748  1.        ]]. Action = [[-0.9498981  -0.06970286 -0.06077868  0.39599276]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 720 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 720 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of 0
Current timestep = 721. State = [[-0.0228454   0.20494665  0.06619123  1.        ]]. Action = [[ 0.92820513  0.08416545 -0.71051824  0.69674027]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 721 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 721 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 721 of -1
Current timestep = 722. State = [[-0.01985181  0.20611268  0.05438916  1.        ]]. Action = [[-0.3374592   0.9553325  -0.598564    0.18890166]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 722 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 722 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of -1
Current timestep = 723. State = [[-0.01479914  0.19626148  0.04997382  1.        ]]. Action = [[ 0.6376785  -0.81276745 -0.35283893  0.8076384 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 723 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 723 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of -1
Current timestep = 724. State = [[0.00507975 0.17472163 0.03844098 1.        ]]. Action = [[ 0.8653743  -0.6993349  -0.00170332  0.90919876]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Scene graph at timestep 724 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 724 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of -1
Current timestep = 725. State = [[0.01581452 0.17206116 0.04720645 1.        ]]. Action = [[ 0.93517566 -0.1163125   0.6335393   0.7845689 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 725 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 725 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of 0
Current timestep = 726. State = [[0.03353263 0.16816626 0.066792   1.        ]]. Action = [[-0.9928378  -0.04076052  0.60061467  0.8262193 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 726 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 726 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 726 of 1
Current timestep = 727. State = [[0.02822148 0.16649158 0.07640775 1.        ]]. Action = [[-0.6355657  -0.38491523 -0.28485596  0.5324923 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Scene graph at timestep 727 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 727 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of -1
Current timestep = 728. State = [[0.02740478 0.16607225 0.07719147 1.        ]]. Action = [[ 0.0851115  -0.40081704  0.79244924  0.65194905]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Scene graph at timestep 728 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 728 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of -1
Current timestep = 729. State = [[0.02740478 0.16607225 0.07719147 1.        ]]. Action = [[ 0.57596064 -0.9037244   0.5652175   0.8591316 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Scene graph at timestep 729 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 729 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of -1
Current timestep = 730. State = [[0.02099188 0.1773497  0.07320283 1.        ]]. Action = [[-0.54681677  0.8050842  -0.46830297  0.7599629 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 730 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 730 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of -1
Current timestep = 731. State = [[0.00634085 0.19856891 0.07041734 1.        ]]. Action = [[ 0.9673202   0.7649263  -0.95545655  0.83338284]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 731 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 731 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of -1
Current timestep = 732. State = [[0.00352376 0.20666379 0.07449462 1.        ]]. Action = [[-0.31532806  0.38589048  0.47424805  0.8168318 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 732 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 732 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of -1
Current timestep = 733. State = [[-0.00908326  0.20280746  0.08444039  1.        ]]. Action = [[-0.79946065 -0.77737284  0.42753577  0.73929715]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 733 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 733 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.03492969  0.18472664  0.09939923  1.        ]]. Action = [[-0.43274915 -0.67472845  0.0365622   0.6461654 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Scene graph at timestep 734 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 734 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of -1
Current timestep = 735. State = [[-0.03516486  0.18408105  0.09930643  1.        ]]. Action = [[-0.18239224 -0.84625167  0.7435734   0.78446066]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Scene graph at timestep 735 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 735 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 735 of -1
Current timestep = 736. State = [[-0.03538325  0.18378668  0.09916526  1.        ]]. Action = [[-0.3088715  -0.84459263  0.9584174   0.7419956 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: No entry zone
Scene graph at timestep 736 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 736 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of -1
Current timestep = 737. State = [[-0.03769807  0.18181105  0.0921696   1.        ]]. Action = [[-0.03576159 -0.11234117 -0.681359    0.720183  ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 737 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 737 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of -1
Current timestep = 738. State = [[-0.03986062  0.17989497  0.08504866  1.        ]]. Action = [[-0.55972904 -0.52787334  0.13812184  0.7078526 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: No entry zone
Scene graph at timestep 738 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 738 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of -1
Current timestep = 739. State = [[-0.03986062  0.17989497  0.08504866  1.        ]]. Action = [[ 0.31077504 -0.63448375 -0.35613656  0.6988375 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Scene graph at timestep 739 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 739 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 739 of -1
Current timestep = 740. State = [[-0.03991999  0.17984712  0.08484993  1.        ]]. Action = [[-0.9184848  -0.8799389   0.17139983  0.88219404]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Scene graph at timestep 740 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 740 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of -1
Current timestep = 741. State = [[-0.03157132  0.1741584   0.09477612  1.        ]]. Action = [[ 0.61814904 -0.40755832  0.8898008   0.7065313 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 741 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 741 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.01862193  0.16455589  0.09814958  1.        ]]. Action = [[ 0.90148115 -0.15586585 -0.868717    0.85533404]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 742 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 742 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of -1
Current timestep = 743. State = [[-0.00103492  0.16349304  0.08473405  1.        ]]. Action = [[0.26670766 0.41246355 0.3697846  0.668144  ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 743 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 743 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 743 of 0
Current timestep = 744. State = [[0.00716824 0.17696267 0.09173024 1.        ]]. Action = [[0.17577302 0.3243301  0.20244622 0.60892177]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 744 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 744 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of 0
Current timestep = 745. State = [[0.01036634 0.1838726  0.09819171 1.        ]]. Action = [[ 0.02444088 -0.65496635 -0.8290906   0.73397934]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Scene graph at timestep 745 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 745 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 745 of -1
Current timestep = 746. State = [[0.01565088 0.17647836 0.11005094 1.        ]]. Action = [[ 0.27765048 -0.51172334  0.86732864  0.63252497]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 746 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 746 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 746 of 1
Current timestep = 747. State = [[0.01950336 0.16580686 0.12917878 1.        ]]. Action = [[-0.5103018  -0.23070818  0.9345629   0.4322275 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Scene graph at timestep 747 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 747 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of -1
Current timestep = 748. State = [[0.01982624 0.16535427 0.13085237 1.        ]]. Action = [[ 0.8532702  -0.43492222  0.48025692  0.7110723 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Scene graph at timestep 748 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 748 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of -1
Current timestep = 749. State = [[0.02015027 0.17530419 0.14073372 1.        ]]. Action = [[-0.30008614  0.6321099   0.69812787  0.5234144 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 749 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 749 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[0.01779432 0.19941069 0.14751665 1.        ]]. Action = [[-0.1953252   0.78101206 -0.7342229   0.72578406]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 750 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 750 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 750 of -1
Current timestep = 751. State = [[0.01409242 0.20613714 0.12692237 1.        ]]. Action = [[ 0.55227697 -0.64919573 -0.99489254  0.64750767]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 751 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 751 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of -1
Current timestep = 752. State = [[0.02810434 0.1991513  0.10547161 1.        ]]. Action = [[-0.27251464  0.18515027  0.73196125  0.4562472 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 752 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 752 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[0.03045536 0.20975019 0.11108719 1.        ]]. Action = [[ 0.25957274  0.5636363  -0.30820864  0.6201756 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 753 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 753 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 753 of -1
Current timestep = 754. State = [[0.02830062 0.23064844 0.10324625 1.        ]]. Action = [[-0.06843358  0.5767319  -0.8335419   0.8787296 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 754 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 754 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of -1
Current timestep = 755. State = [[0.02288528 0.25096858 0.08047886 1.        ]]. Action = [[-0.46936417  0.3396374   0.12855875  0.5224266 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 755 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 755 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of -1
Current timestep = 756. State = [[0.0236349  0.25885493 0.0851479  1.        ]]. Action = [[0.71698713 0.06366742 0.3674202  0.80324733]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 756 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 756 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 756 of -1
Current timestep = 757. State = [[0.03084153 0.26288176 0.09577587 1.        ]]. Action = [[-0.21247143  0.01658726  0.338238    0.7868556 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 757 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 757 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of -1
Current timestep = 758. State = [[0.03155911 0.26309854 0.09759583 1.        ]]. Action = [[-0.1641007   0.771518   -0.08389795  0.82634294]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Scene graph at timestep 758 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 758 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of -1
Current timestep = 759. State = [[0.02709928 0.2583087  0.09168458 1.        ]]. Action = [[-0.65544856 -0.33683962 -0.688404    0.6009877 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 759 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 759 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of -1
Current timestep = 760. State = [[0.01510673 0.23977861 0.08328053 1.        ]]. Action = [[-0.85004205 -0.69387424 -0.21070415  0.8033885 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 760 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 760 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 760 of -1
Current timestep = 761. State = [[-0.24603881 -0.11152786  0.0921999   1.        ]]. Action = [[-0.8341087   0.9393878   0.5492449   0.65166044]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 762. State = [[-0.23630357 -0.13399707  0.08013675  1.        ]]. Action = [[ 0.71367025 -0.63085514  0.45833158  0.9319358 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 763. State = [[-0.21151718 -0.14746694  0.08582629  1.        ]]. Action = [[ 0.8965198  -0.05191517  0.19767642  0.851058  ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 764. State = [[-0.18656322 -0.1617063   0.0873858   1.        ]]. Action = [[ 0.31315994 -0.5896564  -0.47515893  0.8908706 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 765. State = [[-0.18160051 -0.17905474  0.08030751  1.        ]]. Action = [[-0.54850847 -0.3604977  -0.11401725  0.1825031 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 766. State = [[-0.18816712 -0.19916566  0.0680087   1.        ]]. Action = [[ 0.07506144 -0.78244734 -0.96304286  0.80875516]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 767. State = [[-0.1903974  -0.23099242  0.044941    1.        ]]. Action = [[-0.09320164 -0.9046183  -0.18322402  0.82842255]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 768. State = [[-0.19028108 -0.25271833  0.03837999  1.        ]]. Action = [[ 0.33335114  0.7087705  -0.84698987  0.75362873]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 769. State = [[-0.19001704 -0.25460777  0.03837242  1.        ]]. Action = [[ 0.4691702  0.5642803 -0.8359617  0.6129048]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 770. State = [[-0.19675207 -0.27063662  0.04256121  1.        ]]. Action = [[-0.9235531  -0.87914824  0.7030548   0.8874084 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 771. State = [[-0.20145376 -0.29863924  0.05968636  1.        ]]. Action = [[ 0.5318959  -0.385471    0.97909546  0.801852  ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 772. State = [[-0.19684304 -0.30792415  0.08265216  1.        ]]. Action = [[ 0.16219342 -0.74218255 -0.819658    0.67421997]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 773. State = [[-0.19602683 -0.30920085  0.08509356  1.        ]]. Action = [[-0.8055007  -0.34044957  0.6908635   0.8116348 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 774. State = [[-0.19591874 -0.3094563   0.08535469  1.        ]]. Action = [[ 0.184412   -0.83894265  0.09889448  0.67158365]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 775. State = [[-0.19838361 -0.30893314  0.08038232  1.        ]]. Action = [[-0.07480383 -0.01149487 -0.6249913   0.56834567]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 776. State = [[-0.20312119 -0.30938348  0.08324455  1.        ]]. Action = [[-0.5147856   0.18533742  0.89161706  0.575866  ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 777. State = [[-0.22458845 -0.29644012  0.09396437  1.        ]]. Action = [[-0.8107014  0.6810477 -0.0320462  0.4080994]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 778. State = [[-0.24187647 -0.28531486  0.09770158  1.        ]]. Action = [[ 0.8103688 -0.9762038  0.7723093  0.6454233]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 779. State = [[-0.2524811  -0.27957115  0.09398384  1.        ]]. Action = [[-0.4603864   0.13635087 -0.46606964  0.83680344]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 780. State = [[-0.2611989  -0.27482635  0.09105623  1.        ]]. Action = [[-0.428392    0.2070955  -0.51223004  0.84734356]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 781. State = [[-0.25393844 -0.2756744   0.09042186  1.        ]]. Action = [[ 0.9163344  -0.31941396 -0.11067492  0.8270898 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 782. State = [[-0.246263   -0.27782857  0.08610452  1.        ]]. Action = [[ 0.16643643 -0.781728    0.9922149   0.26897168]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 783. State = [[-0.24518432 -0.27782732  0.0861083   1.        ]]. Action = [[-0.15568465 -0.92303604 -0.27632236  0.67736936]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 784. State = [[-0.23135857 -0.27542752  0.08534705  1.        ]]. Action = [[ 0.9111316   0.17768288 -0.1310159   0.89035046]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 785. State = [[-0.21964076 -0.2828078   0.08937927  1.        ]]. Action = [[-0.3325547  -0.52050984  0.7152896   0.84155667]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 786. State = [[-0.21780846 -0.28751415  0.10440487  1.        ]]. Action = [[0.01563954 0.31189072 0.7575904  0.73294044]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 787. State = [[-0.21425825 -0.28169832  0.12197063  1.        ]]. Action = [[0.00691903 0.4684372  0.08157516 0.35105968]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 788. State = [[-0.20193848 -0.2620865   0.13208063  1.        ]]. Action = [[0.95413756 0.77611613 0.4824363  0.82808805]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 789. State = [[-0.18964443 -0.25093496  0.15504593  1.        ]]. Action = [[-0.20156443 -0.46521282  0.84153676  0.6454334 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 790. State = [[-0.17693242 -0.25582975  0.17725892  1.        ]]. Action = [[ 0.8501003  -0.14158571  0.08107781  0.8091471 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 791. State = [[-0.15975758 -0.24449106  0.176825    1.        ]]. Action = [[ 0.2757225   0.95160496 -0.79870063  0.58963084]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 792. State = [[-0.1582698  -0.2386024   0.16700958  1.        ]]. Action = [[-0.8683156  -0.48996294  0.11211872  0.43487   ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 793. State = [[-0.16717604 -0.23611574  0.16162556  1.        ]]. Action = [[-0.05462152  0.61683166 -0.4913432   0.3519913 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 794. State = [[-0.16907802 -0.22573383  0.15457101  1.        ]]. Action = [[ 0.35122085  0.05770791 -0.07074416  0.7022035 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 795. State = [[-0.16467416 -0.21024475  0.15656042  1.        ]]. Action = [[-0.06914461  0.7818303   0.53231144  0.66075134]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 796. State = [[-0.15806243 -0.18400271  0.15524633  1.        ]]. Action = [[ 0.64709353  0.52719367 -0.41253543  0.39689052]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 797. State = [[-0.15205735 -0.17031261  0.1546575   1.        ]]. Action = [[-0.34109366  0.10434258  0.11782849  0.7216697 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 798. State = [[-0.15209799 -0.16719288  0.15462068  1.        ]]. Action = [[0.7173407  0.4328432  0.25445914 0.22044706]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 799. State = [[-0.15209076 -0.16685387  0.15462054  1.        ]]. Action = [[ 0.5888803  -0.10245979  0.6530653   0.917691  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 800. State = [[-0.15209076 -0.16685387  0.15462054  1.        ]]. Action = [[-0.0958342   0.97177184 -0.58359665  0.72497153]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 801. State = [[-0.15209076 -0.16685387  0.15462054  1.        ]]. Action = [[0.04753458 0.94982636 0.5157231  0.47084308]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 802. State = [[-0.15209076 -0.16685387  0.15462054  1.        ]]. Action = [[0.97001815 0.1735084  0.5004953  0.8243375 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 803. State = [[-0.14849819 -0.17835166  0.15161107  1.        ]]. Action = [[ 0.5048373  -0.8996799  -0.33550054  0.7634444 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 804. State = [[-0.14483714 -0.19992511  0.13921943  1.        ]]. Action = [[-0.23009145 -0.6248313  -0.57467866  0.69852066]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 805. State = [[-0.14857082 -0.2131482   0.12657392  1.        ]]. Action = [[ 0.36985898  0.47147632 -0.20294887  0.48589957]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 806. State = [[-0.14276814 -0.21323074  0.11980745  1.        ]]. Action = [[ 0.6579925   0.07064307 -0.6858764   0.8491354 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 807. State = [[-0.11804052 -0.20977823  0.09319365  1.        ]]. Action = [[ 0.9873344   0.30113268 -0.97251904  0.19797492]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 808. State = [[-0.08468747 -0.20482843  0.07383502  1.        ]]. Action = [[0.583043   0.17736006 0.6799264  0.48295343]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 809. State = [[-0.06776986 -0.20176725  0.07458583  1.        ]]. Action = [[ 0.39462256 -0.06679821 -0.5569177   0.31810963]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 810. State = [[-0.05815719 -0.20629893  0.05850128  1.        ]]. Action = [[-0.10334504 -0.27728462 -0.25942308  0.6533259 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 811. State = [[-0.05479354 -0.20969291  0.05288721  1.        ]]. Action = [[0.33720732 0.76709604 0.5859579  0.73160386]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 812. State = [[-0.04831406 -0.21280196  0.0594401   1.        ]]. Action = [[ 0.4037186  -0.12492442  0.7260208   0.5591717 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 812 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 812 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[-0.03763092 -0.2316266   0.08140044  1.        ]]. Action = [[-0.6912588  -0.5991657   0.9388505   0.74658656]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 813 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 813 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of 1
Current timestep = 814. State = [[-0.04910958 -0.23941986  0.09482049  1.        ]]. Action = [[-0.44197726  0.52571    -0.6261822   0.5700829 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 814 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 814 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of -1
Current timestep = 815. State = [[-0.06144514 -0.24267814  0.09725663  1.        ]]. Action = [[-0.24621314 -0.83357877  0.7934065   0.63579905]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 816. State = [[-0.07560334 -0.27010047  0.10698021  1.        ]]. Action = [[-0.5061268  -0.9247737  -0.41602743  0.7594863 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 817. State = [[-0.0853444  -0.28847402  0.10546055  1.        ]]. Action = [[ 0.16187549 -0.8721709   0.23984587  0.68294525]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 818. State = [[-0.08613084 -0.29093084  0.10539551  1.        ]]. Action = [[ 0.330701   -0.7335312   0.13242674  0.7853093 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 819. State = [[-0.08460008 -0.2847552   0.10504562  1.        ]]. Action = [[0.18171763 0.5279641  0.01060081 0.6120622 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 820. State = [[-0.09247017 -0.27604213  0.09586465  1.        ]]. Action = [[-0.5961573   0.18071377 -0.80518717  0.8775003 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 821. State = [[-0.11021894 -0.26251164  0.08063076  1.        ]]. Action = [[-0.3478353   0.5251602  -0.02111977  0.7323005 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 822. State = [[-0.11223738 -0.2554011   0.08540826  1.        ]]. Action = [[ 0.66401315 -0.20986181  0.92428136  0.6897116 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 823. State = [[-0.10352139 -0.2664164   0.09974816  1.        ]]. Action = [[ 0.22460961 -0.8724997   0.04696059  0.52874756]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 824. State = [[-0.09195652 -0.26968157  0.10090473  1.        ]]. Action = [[ 0.76379895  0.4461521  -0.31519663  0.8738811 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 825. State = [[-0.07817017 -0.26541188  0.09698112  1.        ]]. Action = [[ 0.16021395  0.09077907 -0.27667356  0.80956876]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 826. State = [[-0.08624715 -0.27136     0.08550486  1.        ]]. Action = [[-0.95954126 -0.51238984 -0.7734921   0.6635109 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 827. State = [[-0.0879923  -0.27049404  0.05902482  1.        ]]. Action = [[ 0.92003715  0.4031155  -0.8328067   0.47574198]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 828. State = [[-0.07606611 -0.26662064  0.04165895  1.        ]]. Action = [[-0.5877776   0.20619118  0.76492214  0.8474772 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 829. State = [[-0.06781484 -0.25804904  0.05531335  1.        ]]. Action = [[0.60736346 0.46377742 0.8415444  0.8852596 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 830. State = [[-0.04931553 -0.24025716  0.07996231  1.        ]]. Action = [[0.95485723 0.26698947 0.34492993 0.67618084]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 830 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 830 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of 1
Current timestep = 831. State = [[-0.04037048 -0.22783978  0.08417112  1.        ]]. Action = [[-0.7885581   0.37128544 -0.8781527   0.6100577 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 831 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 831 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[-0.03909545 -0.22698694  0.08093034  1.        ]]. Action = [[ 0.67630315 -0.36519444  0.8532841   0.77339315]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 832 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 832 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of 1
Current timestep = 833. State = [[-0.01584922 -0.22020543  0.10032083  1.        ]]. Action = [[0.947618   0.73095345 0.7507496  0.70734525]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 833 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 833 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 833 of 1
Current timestep = 834. State = [[ 0.00891599 -0.2002503   0.12431523  1.        ]]. Action = [[0.26291192 0.22613549 0.14886463 0.8389993 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 834 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 834 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 834 of 1
Current timestep = 835. State = [[ 0.01425168 -0.19557242  0.12353644  1.        ]]. Action = [[ 0.17336631 -0.10536373 -0.7250522   0.6416197 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 835 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 835 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of -1
Current timestep = 836. State = [[ 0.02011024 -0.19644827  0.11005511  1.        ]]. Action = [[ 0.09277892  0.54120183 -0.13510323  0.62568736]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Scene graph at timestep 836 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 836 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of -1
Current timestep = 837. State = [[ 0.02629018 -0.20278598  0.11100833  1.        ]]. Action = [[ 0.6500673  -0.49465078 -0.03577334  0.8734492 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 837 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 837 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[ 0.05116725 -0.22694646  0.11282479  1.        ]]. Action = [[ 0.36794353 -0.7456546   0.49730253  0.6322895 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 838 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 838 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 1
Current timestep = 839. State = [[ 0.05907803 -0.23676358  0.13324846  1.        ]]. Action = [[-0.7729425  0.5284108  0.7391646  0.8341656]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 839 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 839 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[ 0.05924734 -0.22022747  0.14577678  1.        ]]. Action = [[0.6192882  0.76060474 0.14272118 0.8527765 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 840 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 840 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of 1
Current timestep = 841. State = [[ 0.06537817 -0.1994369   0.15211582  1.        ]]. Action = [[ 0.8440716  -0.20310014 -0.9592752   0.30295324]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 841 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 841 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of -1
Current timestep = 842. State = [[ 0.06287322 -0.207032    0.1593299   1.        ]]. Action = [[-0.7049492  -0.35188186  0.43783236  0.6437597 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 842 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 842 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 1
Current timestep = 843. State = [[ 0.05755978 -0.216479    0.17050503  1.        ]]. Action = [[-0.7229443   0.8446336   0.7244034   0.41978526]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Scene graph at timestep 843 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 843 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 843 of -1
Current timestep = 844. State = [[ 0.05098993 -0.23117615  0.18489937  1.        ]]. Action = [[-0.8405134  -0.7067213   0.8771477   0.43891716]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 844 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 844 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of 1
Current timestep = 845. State = [[ 0.02944715 -0.23329906  0.21068726  1.        ]]. Action = [[0.43226612 0.6354749  0.06553102 0.74657655]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 845 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 845 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of 1
Current timestep = 846. State = [[ 0.0341004  -0.21526848  0.20975591  1.        ]]. Action = [[ 0.2580644   0.15584779 -0.34002924  0.6479119 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 846 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 846 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 0
Current timestep = 847. State = [[ 0.03729631 -0.2199404   0.20315391  1.        ]]. Action = [[ 0.98576546 -0.80670106 -0.63865167  0.33821034]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 847 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 847 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of -1
Current timestep = 848. State = [[ 0.06249583 -0.23936766  0.18517669  1.        ]]. Action = [[-0.31608367 -0.27286935  0.69931066  0.64924645]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 848 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 848 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of 0
Current timestep = 849. State = [[ 0.06146919 -0.24891615  0.19544119  1.        ]]. Action = [[0.9455793  0.29349613 0.00856757 0.73939586]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Scene graph at timestep 849 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 849 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 849 of -1
Current timestep = 850. State = [[ 0.0681913  -0.23629169  0.19995487  1.        ]]. Action = [[0.42544627 0.81721497 0.10900748 0.49028182]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 850 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 850 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of 1
Current timestep = 851. State = [[ 0.07090104 -0.22111247  0.21695049  1.        ]]. Action = [[-0.42500615  0.02219284  0.8998835   0.7855644 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 851 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 851 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 1
Current timestep = 852. State = [[ 0.07045265 -0.20692797  0.23619968  1.        ]]. Action = [[0.16766918 0.84612966 0.05683112 0.41021252]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 852 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 852 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of 1
Current timestep = 853. State = [[ 0.07177845 -0.18926194  0.25043234  1.        ]]. Action = [[-0.00366825 -0.15057898  0.7104467   0.34974205]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 853 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 853 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 853 of 1
Current timestep = 854. State = [[ 0.07128764 -0.19125728  0.2597512   1.        ]]. Action = [[ 0.4423499  -0.11820209 -0.9684592   0.20642424]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 854 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 854 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 854 of -1
Current timestep = 855. State = [[ 0.07864557 -0.20153157  0.23211977  1.        ]]. Action = [[ 0.39246595 -0.8167771  -0.65080583  0.5666685 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 855 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 855 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 855 of -1
Current timestep = 856. State = [[ 0.09323657 -0.21933518  0.21276799  1.        ]]. Action = [[ 0.64759135  0.7708752  -0.13646865  0.6237483 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Scene graph at timestep 856 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 856 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of -1
Current timestep = 857. State = [[ 0.09323657 -0.21933518  0.21276799  1.        ]]. Action = [[ 0.27818274 -0.3249383   0.14713144  0.70796216]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Scene graph at timestep 857 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 857 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of -1
Current timestep = 858. State = [[ 0.09627949 -0.20855682  0.21847758  1.        ]]. Action = [[-0.14786124  0.8551792   0.49413252  0.57975185]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 858 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 858 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of 1
Current timestep = 859. State = [[ 0.09733836 -0.19463171  0.22251192  1.        ]]. Action = [[ 0.31493068  0.5860262  -0.31409144  0.9291167 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Action ignored: No entry zone
Scene graph at timestep 859 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 859 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of -1
Current timestep = 860. State = [[ 0.08900336 -0.21141227  0.2258766   1.        ]]. Action = [[-0.9281262  -0.97338057  0.00905156  0.8084918 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 860 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 860 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 860 of -1
Current timestep = 861. State = [[ 0.07251792 -0.23127681  0.22320648  1.        ]]. Action = [[-0.08393925  0.06725287 -0.68457484  0.866423  ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 861 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 861 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of -1
Current timestep = 862. State = [[ 0.06927048 -0.22409926  0.22000508  1.        ]]. Action = [[-0.3026685  0.4879378  0.4527892  0.6324533]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 862 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 862 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 862 of 1
Current timestep = 863. State = [[-0.26231694 -0.01217736  0.10435413  1.        ]]. Action = [[-0.676113   -0.28009135  0.37568295  0.83636737]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 864. State = [[-0.2612881  -0.00775417  0.09262083  1.        ]]. Action = [[0.2611121  0.62372327 0.43558645 0.6829021 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 865. State = [[-0.25548232  0.00987328  0.09718773  1.        ]]. Action = [[0.22897208 0.6983893  0.28036094 0.7068012 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 866. State = [[-0.24032664  0.01452486  0.09725387  1.        ]]. Action = [[ 0.79623294 -0.82918906 -0.80207497  0.7076771 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 867. State = [[-0.22175583  0.00944041  0.09306844  1.        ]]. Action = [[0.12738407 0.3996141  0.79514575 0.6505058 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 868. State = [[-0.21828517  0.02217647  0.11379161  1.        ]]. Action = [[-0.61067945  0.63729906  0.9796691   0.8376912 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 869. State = [[-0.23098785  0.04873868  0.12821823  1.        ]]. Action = [[-0.36923385  0.7592623  -0.9613489   0.38620627]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 870. State = [[-0.24067406  0.0606863   0.1124537   1.        ]]. Action = [[ 0.17714179 -0.2317236  -0.56043446  0.5979916 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 871. State = [[-0.24130264  0.06002429  0.10037135  1.        ]]. Action = [[-0.87363684 -0.45734072  0.10237718  0.7824402 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 872. State = [[-0.23441067  0.06198449  0.10802993  1.        ]]. Action = [[0.42014182 0.16659915 0.9182863  0.61195636]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 873. State = [[-0.23331852  0.07265591  0.12358187  1.        ]]. Action = [[-0.55148727  0.5162983   0.51704884  0.7018032 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 874. State = [[-0.23399928  0.08027992  0.13058314  1.        ]]. Action = [[ 0.6201515  -0.27097225 -0.67148036  0.5463071 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 875. State = [[-0.21633802  0.08641868  0.1232008   1.        ]]. Action = [[ 0.85924625  0.5209689  -0.23353493  0.43346953]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 876. State = [[-0.20601521  0.1003429   0.10742452  1.        ]]. Action = [[-0.25951195  0.48747528 -0.85345864  0.68728423]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 877. State = [[-0.1926156   0.10052283  0.09172606  1.        ]]. Action = [[ 0.68739617 -0.6391658   0.30910695  0.6563599 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 878. State = [[-0.17804909  0.09384229  0.09723674  1.        ]]. Action = [[ 0.1603229  -0.07313281  0.49416506  0.52089953]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 879. State = [[-0.1700829   0.08987584  0.10282752  1.        ]]. Action = [[ 0.5695822   0.41527402 -0.2806363   0.6392276 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 880. State = [[-0.16975361  0.08981238  0.1032012   1.        ]]. Action = [[ 0.16576767 -0.44305128  0.54619026  0.859684  ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 881. State = [[-0.16766372  0.08963325  0.1134476   1.        ]]. Action = [[-0.07815427 -0.01379967  0.70037794  0.78514075]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 882. State = [[-0.17341453  0.09067753  0.11935861  1.        ]]. Action = [[-0.5626016   0.10239828 -0.6822435   0.7148262 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 883. State = [[-0.18533641  0.09273928  0.12403005  1.        ]]. Action = [[-0.854305  -0.059937   0.8737024  0.8026304]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 884. State = [[-0.20518649  0.0923324   0.13627641  1.        ]]. Action = [[0.6882932  0.7365217  0.69479716 0.54496276]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 885. State = [[-0.21800548  0.07706572  0.14570332  1.        ]]. Action = [[-0.913327   -0.90762115  0.5769223   0.72078717]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 886. State = [[-0.23718184  0.0629681   0.16961722  1.        ]]. Action = [[-0.04398412  0.07886684  0.8587122   0.78284574]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 887. State = [[-0.24102117  0.05287318  0.19896692  1.        ]]. Action = [[ 0.1367135  -0.47121918  0.665359    0.4495269 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 888. State = [[-0.23452467  0.04970954  0.22265168  1.        ]]. Action = [[0.80230296 0.45231295 0.5490445  0.78508234]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 889. State = [[-0.2178335   0.04976857  0.24409299  1.        ]]. Action = [[ 0.10151792 -0.42084527  0.5723741   0.7117727 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 890. State = [[-0.21608861  0.04968544  0.27017274  1.        ]]. Action = [[-0.20627987  0.3097      0.9084995   0.8180444 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 891. State = [[-0.20847954  0.04651668  0.3050907   1.        ]]. Action = [[ 0.6735425 -0.4445691  0.7467469  0.6734791]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 892. State = [[-0.18897572  0.0252427   0.31854385  1.        ]]. Action = [[ 0.9223714  -0.84847736 -0.89946294  0.67538977]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 893. State = [[-0.16580254  0.01697818  0.31548473  1.        ]]. Action = [[0.11669862 0.4987769  0.64935875 0.82003856]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 894. State = [[-0.1633596   0.01837187  0.31666306  1.        ]]. Action = [[-0.48044276 -0.22748238 -0.7053756   0.807158  ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 895. State = [[-0.16311534  0.01842858  0.29985687  1.        ]]. Action = [[ 0.58845496  0.21981514 -0.968619    0.7462528 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 896. State = [[-0.14456189  0.01683553  0.28073812  1.        ]]. Action = [[ 0.8702414  -0.29704857  0.51037455  0.5105592 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 897. State = [[-0.12166424  0.02754198  0.27773267  1.        ]]. Action = [[ 0.51035285  0.9179684  -0.4321574   0.81181324]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 898. State = [[-0.10076336  0.04099441  0.26336178  1.        ]]. Action = [[ 0.4278766   0.06107593 -0.38754314  0.77042973]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 899. State = [[-0.08955634  0.04487704  0.25456265  1.        ]]. Action = [[-0.85206926 -0.66607916 -0.7334354   0.56673074]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Above hoop
Current timestep = 900. State = [[-0.08915027  0.04600536  0.25390682  1.        ]]. Action = [[-0.25824022 -0.01835501 -0.45680898  0.6660466 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Above hoop
Current timestep = 901. State = [[-0.08914193  0.04649608  0.25377783  1.        ]]. Action = [[ 0.7915604 -0.5389495 -0.8957429  0.7885492]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Above hoop
Current timestep = 902. State = [[-0.08994362  0.05684705  0.25041345  1.        ]]. Action = [[-0.0348084   0.62460923 -0.21050501  0.69971275]]. Reward = [0.]
Curr episode timestep = 38
Above hoop
Current timestep = 903. State = [[-0.08705335  0.06647087  0.24731366  1.        ]]. Action = [[-0.8670406  -0.8723508  -0.40530193  0.05759585]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Above hoop
Current timestep = 904. State = [[-0.07572611  0.05536595  0.2511613   1.        ]]. Action = [[ 0.8865477  -0.89557767  0.31265426  0.271474  ]]. Reward = [0.]
Curr episode timestep = 40
Above hoop
Current timestep = 905. State = [[-0.04858559  0.05394789  0.26710677  1.        ]]. Action = [[0.93034554 0.7435801  0.73640585 0.6502323 ]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Scene graph at timestep 905 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 905 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 905 of 1
Current timestep = 906. State = [[-0.01910685  0.07900444  0.28043348  1.        ]]. Action = [[ 0.46768844  0.7631283  -0.80773187  0.67901206]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 906 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 906 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 906 of -1
Current timestep = 907. State = [[0.00613456 0.10823713 0.26006603 1.        ]]. Action = [[0.01760411 0.8805051  0.5041865  0.68205047]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 907 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 907 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 907 of -1
Current timestep = 908. State = [[0.00512398 0.11564235 0.27589828 1.        ]]. Action = [[-0.59829026 -0.9065044   0.15920508  0.91644216]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 908 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 908 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 908 of 1
Current timestep = 909. State = [[-0.00528396  0.09373988  0.27342921  1.        ]]. Action = [[-0.9537365  -0.4868827  -0.62095124  0.54777694]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 909 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 909 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 909 of 1
Current timestep = 910. State = [[-0.0149117   0.07614019  0.27275026  1.        ]]. Action = [[ 0.74679995 -0.3388182   0.82395625  0.56952405]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 910 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 910 is tensor(3.0105e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of 1
Current timestep = 911. State = [[-0.01094345  0.0624522   0.282667    1.        ]]. Action = [[-0.22731525 -0.38120455 -0.52942926  0.75510013]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Scene graph at timestep 911 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 911 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 911 of 0
Current timestep = 912. State = [[-0.01209101  0.04217295  0.28326458  1.        ]]. Action = [[ 0.41786122 -0.8427553   0.3851304   0.66115606]]. Reward = [0.]
Curr episode timestep = 48
Above hoop
Scene graph at timestep 912 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 912 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of 0
Current timestep = 913. State = [[-0.0091333   0.03206593  0.28646863  1.        ]]. Action = [[-0.21233797  0.679199   -0.35098195  0.7837553 ]]. Reward = [0.]
Curr episode timestep = 49
Above hoop
Scene graph at timestep 913 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 913 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 913 of 0
Current timestep = 914. State = [[-0.00858929  0.05205769  0.27592915  1.        ]]. Action = [[ 0.6005876   0.65166736 -0.06880456  0.8669491 ]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Scene graph at timestep 914 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 914 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 914 of -1
Current timestep = 915. State = [[5.926885e-04 6.187591e-02 2.802354e-01 1.000000e+00]]. Action = [[-0.12887543 -0.23299205  0.6983514   0.654235  ]]. Reward = [0.]
Curr episode timestep = 51
Above hoop
Scene graph at timestep 915 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 915 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 915 of 0
Current timestep = 916. State = [[0.0018122  0.05591881 0.29985863 1.        ]]. Action = [[-0.41394234 -0.21565425  0.6034211   0.8335762 ]]. Reward = [0.]
Curr episode timestep = 52
Above hoop
Scene graph at timestep 916 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 916 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 916 of 0
Current timestep = 917. State = [[-0.0085009   0.05750453  0.31037697  1.        ]]. Action = [[-0.23337674  0.34909928 -0.7726238   0.7691307 ]]. Reward = [0.]
Curr episode timestep = 53
Above hoop
Scene graph at timestep 917 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 917 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 917 of -1
Current timestep = 918. State = [[-0.01596037  0.07293277  0.2965546   1.        ]]. Action = [[ 0.24592364  0.55108976 -0.06233257  0.7865865 ]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Scene graph at timestep 918 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 918 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 918 of -1
Current timestep = 919. State = [[-0.01231976  0.07089302  0.2898641   1.        ]]. Action = [[ 0.8869643  -0.7963738  -0.63866144  0.23310435]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Scene graph at timestep 919 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 919 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of -1
Current timestep = 920. State = [[0.01374248 0.06643084 0.2604276  1.        ]]. Action = [[ 0.44528604  0.5813694  -0.2903573   0.72473454]]. Reward = [0.]
Curr episode timestep = 56
Above hoop
Scene graph at timestep 920 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 920 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 920 of 0
Current timestep = 921. State = [[0.03577709 0.07378449 0.26140758 1.        ]]. Action = [[ 0.3198924 -0.3204776  0.9208727  0.617275 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 921 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 921 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 921 of 0
Current timestep = 922. State = [[0.04609424 0.06515846 0.27711397 1.        ]]. Action = [[ 0.3850968  -0.21885717 -0.42391956  0.7684939 ]]. Reward = [0.]
Curr episode timestep = 58
Above hoop
Scene graph at timestep 922 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 922 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of -1
Current timestep = 923. State = [[0.06022667 0.05771626 0.28149888 1.        ]]. Action = [[ 0.56153893 -0.15762615  0.88196015  0.84364796]]. Reward = [0.]
Curr episode timestep = 59
Above hoop
Scene graph at timestep 923 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 923 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 923 of -1
Current timestep = 924. State = [[0.07378362 0.06591859 0.29352838 1.        ]]. Action = [[-0.37919164  0.7202263  -0.52168834  0.6214304 ]]. Reward = [0.]
Curr episode timestep = 60
Above hoop
Scene graph at timestep 924 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 924 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 924 of -1
Current timestep = 925. State = [[0.06676318 0.09009121 0.2849417  1.        ]]. Action = [[-0.47966552  0.7486763   0.03153491  0.5962533 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 925 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 925 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 925 of -1
Current timestep = 926. State = [[0.05472541 0.12127539 0.27940527 1.        ]]. Action = [[-0.53713626  0.74471426 -0.6121641   0.8596401 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 926 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 926 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of -1
Current timestep = 927. State = [[0.04729313 0.12812412 0.2641454  1.        ]]. Action = [[ 0.8007723 -0.734939  -0.5784188  0.5767627]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 928. State = [[0.06358408 0.12958612 0.25355464 1.        ]]. Action = [[0.63828397 0.8779954  0.70331454 0.83336544]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 928 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 928 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 928 of -1
Current timestep = 929. State = [[0.07613801 0.14610901 0.26028988 1.        ]]. Action = [[ 0.07862651 -0.04789251 -0.889834    0.7886814 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Scene graph at timestep 929 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 929 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of -1
Current timestep = 930. State = [[0.07763577 0.14759168 0.2604845  1.        ]]. Action = [[ 0.80995536 -0.7963499  -0.6432059   0.63423586]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Action ignored: No entry zone
Scene graph at timestep 930 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 930 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 930 of -1
Current timestep = 931. State = [[0.08117954 0.13724136 0.26714593 1.        ]]. Action = [[ 0.14841712 -0.6349857   0.46477056  0.8581636 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 931 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 931 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 931 of -1
Current timestep = 932. State = [[0.08453556 0.11872083 0.28038147 1.        ]]. Action = [[-0.8486997  -0.29214442  0.03232181  0.64190197]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 932 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 932 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[0.08029655 0.11280713 0.28732193 1.        ]]. Action = [[ 0.543921    0.45385802 -0.8241913   0.6728697 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Scene graph at timestep 933 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 933 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of -1
Current timestep = 934. State = [[0.07335228 0.11031431 0.30132958 1.        ]]. Action = [[-0.97111225 -0.208453    0.78998375  0.6452329 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 934 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 934 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[0.04646257 0.10680486 0.3217952  1.        ]]. Action = [[0.8834684  0.1669383  0.18329895 0.58433235]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 936. State = [[0.04501336 0.09300851 0.33653864 1.        ]]. Action = [[-0.9200023  -0.9862577   0.74189913  0.1461848 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 937. State = [[0.03124715 0.07183445 0.36927307 1.        ]]. Action = [[-0.8902877 -0.2899878  0.9559808  0.8298137]]. Reward = [0.]
Curr episode timestep = 73
Above hoop
Current timestep = 938. State = [[0.00674939 0.06294432 0.39313397 1.        ]]. Action = [[-0.19498992 -0.0291416   0.01206875  0.9065323 ]]. Reward = [0.]
Curr episode timestep = 74
Above hoop
Current timestep = 939. State = [[-0.00268029  0.05978745  0.39545998  1.        ]]. Action = [[0.8432789  0.43698978 0.9822352  0.7415149 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Above hoop
Current timestep = 940. State = [[-0.0028634   0.05992724  0.39582145  1.        ]]. Action = [[-0.693133    0.577459    0.4727353   0.59851587]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Above hoop
Current timestep = 941. State = [[-0.00324063  0.06704862  0.38818043  1.        ]]. Action = [[ 0.46087825  0.5674379  -0.5807493   0.7127199 ]]. Reward = [0.]
Curr episode timestep = 77
Above hoop
Current timestep = 942. State = [[-0.00537694  0.08841848  0.3805994   1.        ]]. Action = [[-0.6589546   0.85574436  0.0699904   0.90466213]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 943. State = [[-0.0153205   0.10759069  0.37886268  1.        ]]. Action = [[-0.01811445 -0.18594956 -0.23594409  0.47179127]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 944. State = [[-0.02376363  0.10350336  0.37461093  1.        ]]. Action = [[-0.78517365 -0.3080355  -0.13382435  0.87461495]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 945. State = [[-0.03799958  0.08485413  0.3560024   1.        ]]. Action = [[ 0.4607495  -0.79609877 -0.9904936   0.7995955 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 946. State = [[-0.03374044  0.06213799  0.3291034   1.        ]]. Action = [[ 0.87474465 -0.42282116 -0.75149137  0.7974696 ]]. Reward = [0.]
Curr episode timestep = 82
Above hoop
Current timestep = 947. State = [[-0.01519843  0.06304089  0.30936512  1.        ]]. Action = [[0.15585244 0.9053551  0.52498937 0.7989719 ]]. Reward = [0.]
Curr episode timestep = 83
Above hoop
Current timestep = 948. State = [[-0.01466515  0.08458302  0.31746286  1.        ]]. Action = [[-0.76186115  0.5953684   0.30182946  0.53151095]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 949. State = [[-0.02296851  0.10724398  0.33191147  1.        ]]. Action = [[0.2533319  0.41731048 0.8174691  0.51736283]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 950. State = [[-0.02410541  0.10977358  0.3553305   1.        ]]. Action = [[ 0.03291667 -0.38736314  0.7151705   0.621017  ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 951. State = [[-0.01695634  0.10549279  0.37222055  1.        ]]. Action = [[ 0.9027486  -0.01189929  0.02877033  0.55471396]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 952. State = [[-0.00286566  0.10393649  0.37936172  1.        ]]. Action = [[-0.26455653 -0.13112766  0.86621857  0.8730928 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 953. State = [[-0.00480378  0.09613451  0.373893    1.        ]]. Action = [[-0.51376134 -0.5657554  -0.53941935  0.5273464 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 954. State = [[-0.00454182  0.09824879  0.36107683  1.        ]]. Action = [[ 0.92267406  0.7561829  -0.9389728   0.9234896 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 955. State = [[0.00752346 0.09877128 0.32879856 1.        ]]. Action = [[-0.7373397  -0.75254226 -0.37308133  0.9008491 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 956. State = [[0.00757297 0.09377049 0.31301776 1.        ]]. Action = [[ 0.9213582   0.38246918 -0.8887831   0.86501336]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 957. State = [[0.02996171 0.08970341 0.2765597  1.        ]]. Action = [[ 0.93530273 -0.53101224 -0.18376303  0.49977374]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 958. State = [[0.05428525 0.07082547 0.27995113 1.        ]]. Action = [[-0.6283928  -0.48304868  0.9240093   0.7826946 ]]. Reward = [0.]
Curr episode timestep = 94
Above hoop
Scene graph at timestep 958 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 958 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 958 of 1
Current timestep = 959. State = [[0.05199313 0.0485945  0.2976329  1.        ]]. Action = [[ 0.04270446 -0.8940108  -0.11200827  0.6451595 ]]. Reward = [0.]
Curr episode timestep = 95
Above hoop
Scene graph at timestep 959 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 959 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of 0
Current timestep = 960. State = [[0.05431006 0.02912428 0.29795128 1.        ]]. Action = [[0.20378017 0.24156213 0.01829493 0.7367308 ]]. Reward = [0.]
Curr episode timestep = 96
Above hoop
Scene graph at timestep 960 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 960 is tensor(8.6209e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of -1
Current timestep = 961. State = [[0.05610628 0.04492142 0.29021993 1.        ]]. Action = [[ 0.10111642  0.92840123 -0.7173378   0.44695163]]. Reward = [0.]
Curr episode timestep = 97
Above hoop
Scene graph at timestep 961 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 961 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 961 of -1
Current timestep = 962. State = [[0.05823284 0.08035439 0.2823687  1.        ]]. Action = [[-0.7312457   0.94092906  0.6612433   0.8840858 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 962 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 962 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 962 of -1
Current timestep = 963. State = [[0.03597518 0.11054142 0.28675535 1.        ]]. Action = [[-0.7043604   0.24776769 -0.7974192   0.6475148 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 964. State = [[0.01287422 0.12886906 0.28293008 1.        ]]. Action = [[-0.8390681   0.74250305  0.5154784   0.3402822 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 965. State = [[-0.25960007  0.11400995  0.08964578  1.        ]]. Action = [[0.43460453 0.4221766  0.9845414  0.6840222 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 966. State = [[-0.25363347  0.11602664  0.0697055   1.        ]]. Action = [[ 0.38470936 -0.6939774  -0.8975279   0.8195703 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 967. State = [[-0.23531927  0.09444647  0.04009283  1.        ]]. Action = [[ 0.774649   -0.7591489  -0.376696    0.54890084]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 968. State = [[-0.21831919  0.08063037  0.028494    1.        ]]. Action = [[-0.8688851   0.3666017  -0.45477897  0.72438526]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 969. State = [[-0.22622052  0.08703016  0.02975186  1.        ]]. Action = [[-0.98319876  0.57650733  0.46751916  0.87313485]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 970. State = [[-0.233216    0.09737039  0.03041204  1.        ]]. Action = [[ 0.8880241  -0.79904443 -0.8946554   0.8498812 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 971. State = [[-0.23041332  0.08500431  0.04055622  1.        ]]. Action = [[ 0.00446594 -0.8118359   0.8788235   0.8683214 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 972. State = [[-0.23802252  0.07107697  0.05870488  1.        ]]. Action = [[ 0.71844673 -0.58400893 -0.9196877   0.66072226]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 973. State = [[-0.2359684   0.07405165  0.06056447  1.        ]]. Action = [[ 0.39416182  0.5064006  -0.06480736  0.79507494]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 974. State = [[-0.2374283   0.08548553  0.06420122  1.        ]]. Action = [[-0.23563868  0.3742689   0.27252293  0.5003246 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 975. State = [[-0.23039745  0.09004146  0.07029428  1.        ]]. Action = [[ 0.7770424  -0.3581478  -0.21011364  0.7985947 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 976. State = [[-0.22452623  0.08294431  0.07923506  1.        ]]. Action = [[-0.5254373  -0.36979413  0.812292    0.62834334]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 977. State = [[-0.2292954   0.07720989  0.08673745  1.        ]]. Action = [[-0.3292967   0.02193189 -0.6232557   0.70534587]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 978. State = [[-0.24337874  0.06895758  0.07642495  1.        ]]. Action = [[-0.7147045  -0.51785165 -0.33335757  0.6299199 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 979. State = [[-0.25443587  0.06537     0.07503802  1.        ]]. Action = [[0.3574983  0.39221108 0.43725598 0.6561308 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 980. State = [[-0.2568826   0.07132325  0.06909695  1.        ]]. Action = [[-0.18351877  0.29371786 -0.79383755  0.7064295 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 981. State = [[-0.25992504  0.07693493  0.06070666  1.        ]]. Action = [[ 0.28849196 -0.6964789  -0.86228806  0.65917253]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 982. State = [[-0.2570849   0.07236146  0.06053129  1.        ]]. Action = [[ 0.18686211 -0.52024     0.05121946  0.56489754]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 983. State = [[-0.2535651   0.06571903  0.06055017  1.        ]]. Action = [[-0.81569403 -0.8139976   0.51854944  0.2540059 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 984. State = [[-0.25357643  0.06531011  0.06051387  1.        ]]. Action = [[-0.478531   -0.20959818 -0.7006424   0.550241  ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 985. State = [[-0.2444975   0.06634945  0.06595656  1.        ]]. Action = [[0.79733014 0.13432705 0.6556034  0.5376415 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 986. State = [[-0.23434746  0.06773802  0.06994522  1.        ]]. Action = [[-0.33146924  0.7014389  -0.8732395   0.45486617]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 987. State = [[-0.2328951   0.06807176  0.07033142  1.        ]]. Action = [[-0.7650471  -0.86628705  0.4618193   0.66100764]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 988. State = [[-0.22629128  0.07033712  0.07931925  1.        ]]. Action = [[0.3635559  0.22276652 0.61842644 0.59450483]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 989. State = [[-0.22145434  0.0638159   0.1030031   1.        ]]. Action = [[-0.75890803 -0.6729282   0.8089657   0.4686576 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 990. State = [[-0.22352235  0.03827566  0.12878303  1.        ]]. Action = [[ 0.3983345  -0.8797798   0.45280194  0.6864102 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 991. State = [[-0.22793962  0.00801502  0.15315259  1.        ]]. Action = [[-0.7328031  -0.7034455   0.8170438   0.56990707]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 992. State = [[-0.24497141 -0.00468044  0.16857035  1.        ]]. Action = [[-0.4196198   0.26393294 -0.4071738   0.7137991 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 993. State = [[-0.2465461  -0.01078284  0.16673109  1.        ]]. Action = [[ 0.7852063  -0.40592074 -0.18478125  0.66166306]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 994. State = [[-0.23195356 -0.02760632  0.1606533   1.        ]]. Action = [[ 0.8052056  -0.5877769  -0.88659036  0.62354565]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 995. State = [[-0.21657111 -0.03114075  0.13842449  1.        ]]. Action = [[-0.19420052  0.7566569   0.05253232  0.7477944 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 996. State = [[-0.20290036 -0.03304951  0.13921972  1.        ]]. Action = [[ 0.8976295  -0.9504354   0.13382816  0.7556608 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 997. State = [[-0.19069217 -0.04768074  0.140851    1.        ]]. Action = [[ 0.9076729 -0.4986812  0.9886477  0.4477284]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 998. State = [[-0.18912129 -0.04858723  0.1413533   1.        ]]. Action = [[ 0.6646215  -0.6211271   0.39068413  0.4432577 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 999. State = [[-0.19374572 -0.04892834  0.13584673  1.        ]]. Action = [[-0.6999459   0.11562228 -0.3316167   0.73989964]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1000. State = [[-0.19944367 -0.04893341  0.13159414  1.        ]]. Action = [[ 0.9312198  -0.6654211   0.786752    0.41918993]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 1001. State = [[-0.20102124 -0.04893672  0.13095462  1.        ]]. Action = [[ 0.7520175   0.2262137  -0.15480292  0.6773882 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 1002. State = [[-0.20155036 -0.0489376   0.13074185  1.        ]]. Action = [[ 0.98311377 -0.11599553 -0.05235356  0.79578567]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 1003. State = [[-0.20155036 -0.0489376   0.13074185  1.        ]]. Action = [[ 0.9552915  -0.23543036 -0.41239113  0.05285907]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 1004. State = [[-0.20698704 -0.04475297  0.13351932  1.        ]]. Action = [[-0.5779756   0.28704846  0.5219641   0.776067  ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1005. State = [[-0.20720416 -0.05259712  0.149868    1.        ]]. Action = [[ 0.5647731  -0.80267596  0.8881688   0.6714933 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1006. State = [[-0.21155013 -0.06636083  0.17894699  1.        ]]. Action = [[-0.7929589  -0.09904248  0.92312884  0.6543374 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1007. State = [[-0.22172272 -0.06482414  0.19238044  1.        ]]. Action = [[ 0.38924718  0.457708   -0.9528459   0.72098875]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1008. State = [[-0.22034228 -0.07041979  0.18434663  1.        ]]. Action = [[ 0.18002856 -0.77036506 -0.01773316  0.6369755 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1009. State = [[-0.22566734 -0.0866566   0.1903516   1.        ]]. Action = [[-0.92147464 -0.26470304  0.74719334  0.59496117]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1010. State = [[-0.22509187 -0.0865897   0.20857821  1.        ]]. Action = [[0.84880733 0.42840695 0.8315731  0.50092554]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1011. State = [[-0.22381105 -0.0712932   0.23091136  1.        ]]. Action = [[-0.49480343  0.5227939   0.47341955  0.6294129 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1012. State = [[-0.23077261 -0.06115931  0.24321923  1.        ]]. Action = [[-0.88721824 -0.25613403  0.8281164   0.7129319 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 1013. State = [[-0.23250595 -0.05952448  0.24423005  1.        ]]. Action = [[-0.02924955 -0.05079734  0.03367424  0.47300994]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1014. State = [[-0.23272581 -0.05996996  0.24443406  1.        ]]. Action = [[-0.78524023  0.7169678   0.7102885   0.65758085]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 1015. State = [[-0.23844922 -0.07539082  0.2515349   1.        ]]. Action = [[-0.54373455 -0.94003004  0.3442483   0.43872452]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1016. State = [[-0.24553691 -0.08057223  0.26864007  1.        ]]. Action = [[0.27486694 0.77807343 0.5638528  0.47157323]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1017. State = [[-0.25170648 -0.08317489  0.28767806  1.        ]]. Action = [[-0.33828282 -0.91383016  0.5428411   0.6852186 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1018. State = [[-0.26097456 -0.09422427  0.29301998  1.        ]]. Action = [[-0.24801314  0.36435294 -0.97221524  0.59914327]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1019. State = [[-0.2651413  -0.09610645  0.28791204  1.        ]]. Action = [[-0.1331892  -0.27353764  0.2469747   0.79459   ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1020. State = [[-0.26648802 -0.10017788  0.28900146  1.        ]]. Action = [[-0.4364438  -0.90668887 -0.30997902  0.89585674]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 1021. State = [[-0.2671013  -0.100957    0.28902617  1.        ]]. Action = [[-0.09069765  0.13940215 -0.06214404  0.81267726]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 1022. State = [[-0.2670127  -0.10096232  0.28908277  1.        ]]. Action = [[-0.37543058  0.6751139   0.3065499   0.61961484]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Current timestep = 1023. State = [[-0.26477873 -0.10348773  0.29190567  1.        ]]. Action = [[ 0.35372138 -0.22974187  0.43863058  0.6363869 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1024. State = [[-0.2540358  -0.11242882  0.302974    1.        ]]. Action = [[ 0.5798255  -0.48921084  0.53297865  0.8391657 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1025. State = [[-0.24511826 -0.11800499  0.3105054   1.        ]]. Action = [[ 0.11662924  0.3156482  -0.5693227   0.60071206]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1026. State = [[-0.24283783 -0.11677749  0.30582654  1.        ]]. Action = [[-0.6977305  0.6334789 -0.6554129  0.4957937]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 1027. State = [[-0.2302544  -0.11911644  0.31638694  1.        ]]. Action = [[ 0.8024337  -0.33574033  0.90189815  0.67346644]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1028. State = [[-0.21252939 -0.12175576  0.33427423  1.        ]]. Action = [[-0.87028784 -0.31646842  0.61635137  0.65386295]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 1029. State = [[-0.21523108 -0.10989641  0.33700603  1.        ]]. Action = [[-0.74827135  0.9640677   0.12815297  0.7864206 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1030. State = [[-0.22204117 -0.07989588  0.3405124   1.        ]]. Action = [[0.12686646 0.99061346 0.18593025 0.81392384]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1031. State = [[-0.25666457 -0.01056382  0.10297287  1.        ]]. Action = [[ 0.54847777  0.84984684  0.90719974 -0.09284419]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1032. State = [[-0.2461092  -0.02566036  0.09111516  1.        ]]. Action = [[ 0.6675513  -0.78374183  0.03815985  0.7717763 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1033. State = [[-0.23074354 -0.05073176  0.0838443   1.        ]]. Action = [[ 0.27339876 -0.56919575 -0.40569824  0.50591385]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1034. State = [[-0.21487491 -0.07673581  0.07498337  1.        ]]. Action = [[ 0.5514965  -0.8021661  -0.02039027  0.83457637]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1035. State = [[-0.19480696 -0.08832207  0.06436515  1.        ]]. Action = [[ 0.8257928   0.38971627 -0.91487134  0.59763527]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1036. State = [[-0.17532194 -0.08884092  0.0412544   1.        ]]. Action = [[ 0.6268923  -0.00567728 -0.8718781   0.5325475 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 1037. State = [[-0.1716035  -0.08920431  0.03825274  1.        ]]. Action = [[ 0.791958   -0.82997185  0.9162152   0.88439107]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 1038. State = [[-0.17596738 -0.08927069  0.04263966  1.        ]]. Action = [[-0.8458889   0.06330252  0.75017905  0.5284307 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1039. State = [[-0.18465012 -0.08801576  0.0502842   1.        ]]. Action = [[ 0.64868665  0.05443168 -0.5074616   0.55208826]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 1040. State = [[-0.19864254 -0.08511684  0.06221993  1.        ]]. Action = [[-0.97188234  0.23647738  0.8861755   0.9368446 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1041. State = [[-0.23267224 -0.09542121  0.07997355  1.        ]]. Action = [[-0.9417755  -0.93740416 -0.13218868  0.60392356]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1042. State = [[-0.24999498 -0.10969103  0.0813662   1.        ]]. Action = [[-0.7342178   0.62874925  0.8838613   0.6328144 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1043. State = [[-0.25202006 -0.11107145  0.08126608  1.        ]]. Action = [[-0.6095954   0.36107516 -0.27472866  0.6179874 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 1044. State = [[-0.24489936 -0.11290137  0.08565219  1.        ]]. Action = [[ 0.85663533 -0.15481728  0.35094607  0.638016  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1045. State = [[-0.23733558 -0.11039046  0.08446127  1.        ]]. Action = [[ 0.2825215   0.22883976 -0.958173    0.42618084]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1046. State = [[-0.22389379 -0.10196347  0.06942986  1.        ]]. Action = [[ 0.578804    0.47067237 -0.32948673  0.5739838 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1047. State = [[-0.21304566 -0.0830338   0.05371546  1.        ]]. Action = [[ 0.03220952  0.6141403  -0.65928555  0.8973999 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1048. State = [[-0.19977508 -0.0705939   0.04601568  1.        ]]. Action = [[0.5085038  0.21668637 0.7393534  0.74638164]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1049. State = [[-0.18409804 -0.06962492  0.0614937   1.        ]]. Action = [[ 0.45893776 -0.3001544   0.95336485  0.6805644 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1050. State = [[-0.17594798 -0.06105744  0.08053499  1.        ]]. Action = [[-0.11182857  0.6715609  -0.2742977   0.7782123 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1051. State = [[-0.17623517 -0.0623412   0.07351281  1.        ]]. Action = [[ 0.08433032 -0.8654869  -0.76059085  0.6134367 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1052. State = [[-0.17076157 -0.07190664  0.06105794  1.        ]]. Action = [[ 0.7003448 -0.7321854 -0.8268979  0.7428895]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 1053. State = [[-0.1696378  -0.07253633  0.06013161  1.        ]]. Action = [[ 0.13742721 -0.27014136  0.6197059   0.6901934 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 1054. State = [[-0.16944687 -0.07291076  0.05929665  1.        ]]. Action = [[0.5336335 0.5143995 0.2636485 0.7121868]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 1055. State = [[-0.17444399 -0.072426    0.06655285  1.        ]]. Action = [[-0.87883765  0.27107787  0.93495035  0.7907785 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1056. State = [[-0.19154857 -0.08573277  0.08558103  1.        ]]. Action = [[-0.43090755 -0.87891734  0.495497    0.79056597]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1057. State = [[-0.21110302 -0.0962109   0.10233663  1.        ]]. Action = [[-0.7109165   0.28354132  0.42667103  0.77537394]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1058. State = [[-0.22132924 -0.09318849  0.11739787  1.        ]]. Action = [[0.6926651  0.11955547 0.25073647 0.4698211 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1059. State = [[-0.20976861 -0.08845407  0.12986292  1.        ]]. Action = [[0.68622065 0.21378815 0.41002178 0.70574033]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1060. State = [[-0.1920582  -0.08112587  0.15003346  1.        ]]. Action = [[0.2546618  0.2621827  0.7639425  0.70031404]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1061. State = [[-0.18756758 -0.07371061  0.17679618  1.        ]]. Action = [[-0.5867757   0.01455891  0.60237396  0.49452472]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1062. State = [[-0.19596519 -0.06656376  0.19020192  1.        ]]. Action = [[-0.31220174  0.31711447 -0.2564478   0.66964793]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1063. State = [[-0.20073448 -0.06118857  0.18889116  1.        ]]. Action = [[0.89877284 0.8176364  0.46822143 0.8616755 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 1064. State = [[-0.19568613 -0.06108934  0.20022781  1.        ]]. Action = [[ 0.55639076 -0.09554821  0.9131433   0.8024082 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1065. State = [[-0.19554462 -0.06036574  0.21444918  1.        ]]. Action = [[-0.46131766  0.11609066 -0.22273141  0.5134196 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1066. State = [[-0.1973467  -0.05981451  0.21400827  1.        ]]. Action = [[ 0.96130276  0.8112352  -0.4969728   0.83652544]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 1067. State = [[-0.1948957  -0.06279375  0.21082844  1.        ]]. Action = [[ 0.58572793 -0.38157105 -0.46018898  0.5693412 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1068. State = [[-0.18944168 -0.06155292  0.21120487  1.        ]]. Action = [[-0.03351843  0.34751666  0.5650741   0.69952047]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1069. State = [[-0.18823569 -0.06064307  0.21558826  1.        ]]. Action = [[ 0.90598774 -0.85692984 -0.45534575  0.84875226]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 1070. State = [[-0.18807812 -0.06053567  0.21564461  1.        ]]. Action = [[0.7787225  0.1466105  0.40426338 0.7395439 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 1071. State = [[-0.19019859 -0.05639471  0.21491925  1.        ]]. Action = [[-0.35525328  0.23450649 -0.31105423  0.8831794 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1072. State = [[-0.19558044 -0.0370672   0.20579357  1.        ]]. Action = [[ 0.03726196  0.9416957  -0.76827717  0.74754953]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1073. State = [[-0.20076409 -0.01304709  0.19498113  1.        ]]. Action = [[-0.05908912  0.33137655  0.4121666   0.74145603]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1074. State = [[-0.20256221  0.00738857  0.20083423  1.        ]]. Action = [[-0.07493699  0.6005149   0.41945267  0.18486762]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1075. State = [[-0.21342143  0.03316041  0.19958441  1.        ]]. Action = [[-0.69763416  0.73115706 -0.8438695   0.53541243]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1076. State = [[-0.22419097  0.04957285  0.1917268   1.        ]]. Action = [[0.9679334  0.76036596 0.8005123  0.7667903 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 1077. State = [[-0.21343943  0.04419711  0.19901235  1.        ]]. Action = [[ 0.8683927  -0.49621463  0.7391491   0.5758605 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1078. State = [[-0.19208728  0.02634062  0.1986646   1.        ]]. Action = [[ 0.863482   -0.69977313 -0.8037549   0.46753168]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1079. State = [[-0.18126243  0.01400072  0.18717337  1.        ]]. Action = [[-0.7920033   0.02339911  0.09173238  0.8968104 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1080. State = [[-0.18751737  0.02425888  0.1972218   1.        ]]. Action = [[-0.22828472  0.8999133   0.9593984   0.6530509 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1081. State = [[-0.2047065   0.03771687  0.20853283  1.        ]]. Action = [[-0.9113319  -0.24158108 -0.36399615  0.33557093]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1082. State = [[-0.23083109  0.05011176  0.20221043  1.        ]]. Action = [[-0.48044515  0.8654702  -0.45768082  0.71702385]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1083. State = [[-0.251686    0.07483791  0.20237055  1.        ]]. Action = [[-0.55627424  0.53816116  0.59594846  0.43196428]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1084. State = [[-0.26687732  0.092513    0.20062311  1.        ]]. Action = [[-0.25513577  0.19490051 -0.9189069   0.5056672 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1085. State = [[-0.2683288   0.10494545  0.19481832  1.        ]]. Action = [[0.5501611 0.4721558 0.6183225 0.7785084]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1086. State = [[-0.2658433   0.11283241  0.1982798   1.        ]]. Action = [[-0.6978755  -0.9112394   0.23154545  0.49670243]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 1087. State = [[-0.26558515  0.11346541  0.19858058  1.        ]]. Action = [[-0.6707576 -0.3355863  0.6249697  0.5679122]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Current timestep = 1088. State = [[-0.2540375   0.11619384  0.20666975  1.        ]]. Action = [[0.8385186  0.05245543 0.5730462  0.6896987 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1089. State = [[-0.24328193  0.1280901   0.21401049  1.        ]]. Action = [[-0.04150659  0.63799524 -0.2590629   0.6269126 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1090. State = [[-0.24308419  0.13674356  0.21446592  1.        ]]. Action = [[-0.5781503  -0.7101944  -0.41488683  0.7491286 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 1091. State = [[-0.23487525  0.1370797   0.22360028  1.        ]]. Action = [[ 0.5426786  -0.1751315   0.8024484   0.21342266]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1092. State = [[-0.2104123   0.12365537  0.25085118  1.        ]]. Action = [[ 0.6847501  -0.8190352   0.9755155   0.50548005]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1093. State = [[-0.20156431  0.11081369  0.28198236  1.        ]]. Action = [[-0.7298211   0.01291513  0.509454    0.6783414 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1094. State = [[-0.20547105  0.0935497   0.30183598  1.        ]]. Action = [[ 0.27113616 -0.96203905  0.4140184   0.6481719 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1095. State = [[-0.2116063   0.07009467  0.32528642  1.        ]]. Action = [[-0.9029802 -0.3698401  0.9604163  0.5904684]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1096. State = [[-0.23192433  0.06888074  0.34867728  1.        ]]. Action = [[-0.37798333  0.49443793 -0.28684515  0.62932384]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1097. State = [[-0.24108237  0.07579754  0.35079473  1.        ]]. Action = [[-0.8496711  -0.2217583  -0.32785022  0.8514273 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 1098. State = [[-0.23857228  0.08584991  0.35725847  1.        ]]. Action = [[0.61888707 0.68988967 0.5233172  0.5347526 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1099. State = [[-0.23510087  0.09352166  0.36355036  1.        ]]. Action = [[-0.3707105  -0.19790053 -0.45234203  0.7797742 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1100. State = [[-0.23122321  0.08591542  0.3594632   1.        ]]. Action = [[ 0.61323047 -0.4662137  -0.06919301  0.4671712 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1101. State = [[-0.23086476  0.07239614  0.36678958  1.        ]]. Action = [[-0.58505934 -0.41567212  0.95900106  0.578531  ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1102. State = [[-0.23465972  0.06473752  0.3801509   1.        ]]. Action = [[ 0.2383486   0.17669177 -0.0386157   0.5441723 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1103. State = [[-0.23721048  0.06012487  0.3879338   1.        ]]. Action = [[-0.3097487  -0.33973372  0.51800513  0.7270882 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1104. State = [[-0.24254693  0.06174517  0.3946911   1.        ]]. Action = [[-0.00993335  0.49338222 -0.42635727  0.4757359 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1105. State = [[-0.2451075   0.06757546  0.39380074  1.        ]]. Action = [[ 0.31721246 -0.61667484  0.63939893  0.57024324]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 1106. State = [[-0.23943867  0.07564528  0.38934988  1.        ]]. Action = [[ 0.7360333   0.48484623 -0.23623341  0.45665848]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1107. State = [[-0.21266626  0.08018845  0.38279068  1.        ]]. Action = [[ 0.95798516 -0.36036146 -0.16226983  0.7068422 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1108. State = [[-0.19073705  0.08019608  0.37841222  1.        ]]. Action = [[0.34547734 0.781309   0.73183537 0.5286411 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 1109. State = [[-0.18802226  0.08051665  0.37656567  1.        ]]. Action = [[ 0.30527997 -0.0509342   0.7311517   0.5917592 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 1110. State = [[-0.18016617  0.0691314   0.36896124  1.        ]]. Action = [[ 0.8269143  -0.6115462  -0.7766175   0.77769685]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1111. State = [[-0.15926734  0.05512696  0.34219155  1.        ]]. Action = [[ 0.02643692 -0.11058927 -0.7531425   0.76707685]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1112. State = [[-0.15066044  0.0594953   0.31604654  1.        ]]. Action = [[ 0.26033783  0.5173485  -0.6702418   0.77468705]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1113. State = [[-0.13773118  0.06984549  0.28728202  1.        ]]. Action = [[ 0.72319484  0.26845002 -0.96787757  0.7626345 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1114. State = [[-0.12509237  0.06608326  0.25147074  1.        ]]. Action = [[-0.8372915  -0.84391636 -0.9380677   0.55342317]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1115. State = [[-0.13027684  0.06078359  0.23966086  1.        ]]. Action = [[0.34267378 0.48855412 0.7890371  0.6857972 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1116. State = [[-0.12771939  0.04967332  0.2531198   1.        ]]. Action = [[-0.34433073 -0.90798086  0.55083513  0.7331158 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1117. State = [[-0.13070223  0.02074588  0.27818683  1.        ]]. Action = [[-0.03194261 -0.9367826   0.7865124   0.50077343]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1118. State = [[-0.13383205  0.00230114  0.28874037  1.        ]]. Action = [[ 0.0757941   0.1226747  -0.79522187  0.72088766]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1119. State = [[-0.133848    0.00471517  0.28481746  1.        ]]. Action = [[-2.2590160e-05  2.7671087e-01  5.5455387e-01  8.0537486e-01]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1120. State = [[-0.12869881 -0.00262598  0.2789399   1.        ]]. Action = [[ 0.65677524 -0.56555426 -0.83881426  0.00853229]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1121. State = [[-0.11654476 -0.01013805  0.26861197  1.        ]]. Action = [[0.37062037 0.02185428 0.1161865  0.89079165]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1122. State = [[-0.11282584 -0.00689583  0.2617295   1.        ]]. Action = [[-0.1199376   0.3895346  -0.30620652  0.7342379 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1123. State = [[-0.11877649 -0.01701576  0.25721008  1.        ]]. Action = [[-0.7756653  -0.9134247  -0.14456367  0.55457807]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1124. State = [[-0.12416717 -0.01438798  0.25659406  1.        ]]. Action = [[0.17436266 0.95511913 0.3348782  0.62291527]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1125. State = [[-0.1254689   0.01098112  0.26167354  1.        ]]. Action = [[-0.11860746  0.8426318   0.32519913  0.78449845]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1126. State = [[-0.12947741  0.03038721  0.26523507  1.        ]]. Action = [[ 0.5016525  -0.3888762  -0.7057299   0.70024896]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 1127. State = [[-0.12594903  0.04374999  0.26401085  1.        ]]. Action = [[ 0.47947526  0.75558424 -0.28001416  0.5085125 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1128. State = [[-0.12154795  0.05586672  0.2637849   1.        ]]. Action = [[ 0.59297574 -0.37853968 -0.7940387   0.70649815]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 1129. State = [[-0.11397512  0.06643242  0.26196238  1.        ]]. Action = [[ 0.5531008   0.5109459  -0.09522194  0.62973833]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1130. State = [[-0.10262193  0.08801769  0.26295224  1.        ]]. Action = [[-0.1032052   0.6739137   0.2760501   0.65799487]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1131. State = [[-0.10236152  0.1026727   0.26804015  1.        ]]. Action = [[ 0.3179363 -0.6201109 -0.6820107  0.6918051]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 1132. State = [[-0.10277469  0.10409082  0.2682717   1.        ]]. Action = [[ 0.4714594   0.21863699 -0.8573164   0.5944369 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 1133. State = [[-0.26006222 -0.05381482  0.10661822  1.        ]]. Action = [[-0.7152113   0.16918457 -0.43449712  0.32530463]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1134. State = [[-0.25616285 -0.06105892  0.08607364  1.        ]]. Action = [[ 0.52220607  0.01289833 -0.541849    0.8273046 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1135. State = [[-0.23525494 -0.06998149  0.06521133  1.        ]]. Action = [[ 0.84552383 -0.4702915  -0.7339209   0.85302186]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1136. State = [[-0.20480688 -0.09284127  0.05304614  1.        ]]. Action = [[ 0.7749201  -0.858536    0.64702165  0.32159424]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1137. State = [[-0.17887986 -0.12147709  0.06192954  1.        ]]. Action = [[ 0.6681216  -0.69297767  0.40991902  0.76771426]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1138. State = [[-0.16565081 -0.14220156  0.06804066  1.        ]]. Action = [[-0.42041934 -0.20214194 -0.34805876  0.647329  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1139. State = [[-0.17819108 -0.13474263  0.05918066  1.        ]]. Action = [[-0.8089203   0.93326354 -0.71778375  0.74174356]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1140. State = [[-0.19864248 -0.13680425  0.04980703  1.        ]]. Action = [[-0.75059897 -0.7585496   0.64869535  0.561913  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1141. State = [[-0.20991431 -0.14505912  0.05359856  1.        ]]. Action = [[ 0.6493306   0.21145952 -0.8065669   0.6227739 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 1142. State = [[-0.20945548 -0.1552297   0.0564515   1.        ]]. Action = [[ 0.4356265  -0.6132916   0.19459093  0.26196492]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1143. State = [[-0.20825458 -0.1800938   0.06347594  1.        ]]. Action = [[-0.06874555 -0.97011465  0.04048598  0.9271089 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1144. State = [[-0.20105878 -0.20554079  0.06593198  1.        ]]. Action = [[ 0.7273443  -0.18193114 -0.05932552  0.72709167]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1145. State = [[-0.18340123 -0.22098286  0.06823901  1.        ]]. Action = [[ 0.7424381  -0.65086865 -0.13844919  0.5766264 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1146. State = [[-0.15724196 -0.23905939  0.070338    1.        ]]. Action = [[ 0.8160894  -0.32122195  0.22107935  0.76683295]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1147. State = [[-0.13506465 -0.24564238  0.07659334  1.        ]]. Action = [[ 0.16469479  0.1280626  -0.01758349  0.57191813]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1148. State = [[-0.1232093  -0.25382885  0.0822712   1.        ]]. Action = [[ 0.2440362  -0.5482506   0.46690297  0.04614067]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1149. State = [[-0.1218356  -0.26735032  0.08043683  1.        ]]. Action = [[-0.42439663 -0.20589471 -0.8161386   0.7547002 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1150. State = [[-0.11976906 -0.26425564  0.06310382  1.        ]]. Action = [[ 0.6625639   0.5917425  -0.30529547  0.50887465]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1151. State = [[-0.11176981 -0.26088682  0.0572397   1.        ]]. Action = [[-0.6270357  -0.00230694  0.3918978   0.65625286]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1152. State = [[-0.12195764 -0.27934775  0.0667055   1.        ]]. Action = [[-0.85622966 -0.90606207  0.8359034   0.6380558 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1153. State = [[-0.14224832 -0.28436998  0.07512134  1.        ]]. Action = [[ 0.0559938   0.45133865 -0.7478236   0.54917693]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1154. State = [[-0.14574453 -0.26822802  0.06536711  1.        ]]. Action = [[-0.18711972  0.8129195  -0.5912953   0.9385731 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1155. State = [[-0.1438597  -0.25503334  0.05836745  1.        ]]. Action = [[ 0.7947024  -0.16572231  0.7737427   0.6955576 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1156. State = [[-0.14100617 -0.26191127  0.06960075  1.        ]]. Action = [[-0.5543712  -0.4795494   0.34601212  0.8025601 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1157. State = [[-0.1400905  -0.2533999   0.07193775  1.        ]]. Action = [[ 0.33297062  0.9285425  -0.53695005  0.8471484 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1158. State = [[-0.14839555 -0.22616729  0.059472    1.        ]]. Action = [[-0.6958085   0.8525231  -0.8588645   0.56050754]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1159. State = [[-0.16484691 -0.20408806  0.03730877  1.        ]]. Action = [[-0.5331689   0.3862791  -0.18295157  0.91650605]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1160. State = [[-0.16875264 -0.19379267  0.03714691  1.        ]]. Action = [[ 0.6944214  -0.03149307  0.55814815  0.57737446]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1161. State = [[-0.16560073 -0.19030197  0.03995609  1.        ]]. Action = [[ 0.83339834 -0.5094466  -0.75267065  0.812135  ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1162. State = [[-0.26037252  0.06469759  0.10835356  1.        ]]. Action = [[-0.55839455 -0.6800138   0.8523929  -0.00272679]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1163. State = [[-0.25324062  0.06140221  0.09650324  1.        ]]. Action = [[ 0.45499754 -0.74576074  0.13351417  0.80186725]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1164. State = [[-0.23212351  0.05904758  0.10412495  1.        ]]. Action = [[0.9152912 0.4783486 0.6903107 0.6807004]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1165. State = [[-0.21277617  0.06348667  0.11195387  1.        ]]. Action = [[-0.9133169  -0.9597549  -0.64506054  0.47346842]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1166. State = [[-0.21110821  0.05904093  0.1137058   1.        ]]. Action = [[-0.53774554 -0.3718922   0.12931323  0.6588602 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1167. State = [[-0.2197224   0.05707496  0.12449542  1.        ]]. Action = [[-0.42231452  0.14008534  0.7180722   0.6263926 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1168. State = [[-0.23937792  0.05353056  0.15171418  1.        ]]. Action = [[-0.9477033  -0.31175888  0.9800668   0.875185  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1169. State = [[-0.25795904  0.03974732  0.18820986  1.        ]]. Action = [[-0.0066371  -0.51811826  0.85411     0.67521274]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1170. State = [[-0.25933316  0.02767855  0.20480163  1.        ]]. Action = [[ 0.29796576 -0.19260919 -0.79237807  0.5525    ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1171. State = [[-0.259663    0.02480517  0.1994487   1.        ]]. Action = [[-0.7765822  -0.10452318  0.5755365   0.5685947 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 1172. State = [[-0.25539994  0.02562752  0.20473576  1.        ]]. Action = [[0.288265   0.25483096 0.67149067 0.8098413 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1173. State = [[-0.2521202   0.02654344  0.20732918  1.        ]]. Action = [[-0.8940423  -0.7991193  -0.20415342  0.73578405]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1174. State = [[-0.25196132  0.02654715  0.20740703  1.        ]]. Action = [[-0.74134797 -0.5320162   0.6790513   0.4936502 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 1175. State = [[-0.25076032  0.01838327  0.21903698  1.        ]]. Action = [[-0.0714314  -0.6027074   0.82697606  0.7013655 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1176. State = [[-0.2526721   0.01215243  0.23374495  1.        ]]. Action = [[ 0.14468765  0.24485338 -0.53298795  0.3455161 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1177. State = [[-0.24079318  0.00538569  0.24102063  1.        ]]. Action = [[ 0.71706367 -0.5654796   0.708966    0.7658752 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1178. State = [[-0.21800408 -0.00144887  0.24592692  1.        ]]. Action = [[ 0.83939505  0.08275676 -0.73501307  0.78123283]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1179. State = [[-0.19974452  0.00526738  0.231382    1.        ]]. Action = [[-0.11645073  0.50299275 -0.13311017  0.5922549 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1180. State = [[-0.18861252  0.02566728  0.23703782  1.        ]]. Action = [[0.8086444  0.9874518  0.85838485 0.66474915]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1181. State = [[-0.16029358  0.05853327  0.26692384  1.        ]]. Action = [[0.7523751  0.7526717  0.9801364  0.26608908]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1182. State = [[-0.13723549  0.08401413  0.30203143  1.        ]]. Action = [[0.11798918 0.3672198  0.75125957 0.7420347 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1183. State = [[-0.12789476  0.09350577  0.3213894   1.        ]]. Action = [[ 0.02822626 -0.04755235  0.01264381  0.56081057]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1184. State = [[-0.12703277  0.08753081  0.3299093   1.        ]]. Action = [[-0.3908677  -0.491677    0.262671    0.87211263]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1185. State = [[-0.12012403  0.07409508  0.34833524  1.        ]]. Action = [[ 0.93045473 -0.3381182   0.91574574  0.2695806 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1186. State = [[-0.1123697   0.06402053  0.36813352  1.        ]]. Action = [[-0.66004217 -0.23566562 -0.29254586  0.6549722 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1187. State = [[-0.11721633  0.04623735  0.3704721   1.        ]]. Action = [[-0.4526242  -0.75681347  0.01265919  0.20775056]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1188. State = [[-0.12463158  0.03922655  0.36540997  1.        ]]. Action = [[ 0.0819037   0.43478417 -0.697797    0.37043357]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1189. State = [[-0.13299127  0.05510526  0.3581083   1.        ]]. Action = [[-0.5532894   0.6599766   0.38412905  0.85824656]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1190. State = [[-0.15051724  0.08140446  0.34974006  1.        ]]. Action = [[-0.42497241  0.71584177 -0.91628855  0.79491615]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1191. State = [[-0.16594781  0.09324975  0.3267561   1.        ]]. Action = [[ 0.03304136 -0.0566076  -0.5855219   0.63805246]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1192. State = [[-0.17606212  0.09482519  0.3056413   1.        ]]. Action = [[-0.6142665   0.00403583 -0.4671296   0.3780799 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1193. State = [[-0.18995531  0.10252776  0.29770634  1.        ]]. Action = [[-0.0975194   0.35282016  0.6039753   0.8583429 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1194. State = [[-0.184737    0.11103513  0.30488437  1.        ]]. Action = [[0.94792414 0.05171323 0.15852356 0.76318073]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1195. State = [[-0.1731037   0.10581736  0.30913755  1.        ]]. Action = [[ 0.41433334 -0.3946718  -0.01979285  0.8164973 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1196. State = [[-0.1670724   0.11009721  0.31087878  1.        ]]. Action = [[-0.43765098  0.6924162   0.21695471  0.7697909 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1197. State = [[-0.16561273  0.12277991  0.308071    1.        ]]. Action = [[ 0.51238585  0.3464011  -0.4047916   0.8838146 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1198. State = [[-0.15096873  0.14284696  0.29681498  1.        ]]. Action = [[ 0.66754496  0.80131376 -0.38760746  0.16954386]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1199. State = [[-0.13730916  0.16626814  0.2931587   1.        ]]. Action = [[-0.32863688  0.32912934  0.51231956  0.69963205]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1200. State = [[-0.13500224  0.17285593  0.30439588  1.        ]]. Action = [[ 0.36010242 -0.23896986  0.44572937  0.65790915]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1201. State = [[-0.12509036  0.17470987  0.31028187  1.        ]]. Action = [[ 0.8245952   0.42816424 -0.6834321   0.61409235]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1202. State = [[-0.09901004  0.19886439  0.30910805  1.        ]]. Action = [[0.643826   0.95323944 0.8728814  0.6464319 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1203. State = [[-0.07886827  0.22141449  0.33278325  1.        ]]. Action = [[0.03094125 0.1910665  0.52512264 0.6394435 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1204. State = [[-0.07802711  0.22015907  0.342879    1.        ]]. Action = [[-0.8593356  -0.7117661  -0.45208097  0.6905385 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1205. State = [[-0.09012978  0.21383217  0.32938093  1.        ]]. Action = [[-0.32749885  0.09049499 -0.87477857  0.5439365 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1206. State = [[-0.10115096  0.21989456  0.32179618  1.        ]]. Action = [[-0.50957936  0.2402314   0.5520041   0.7058525 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1207. State = [[-0.10804254  0.22755192  0.3226497   1.        ]]. Action = [[ 0.87630165  0.40772092 -0.17575836  0.6715667 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1208. State = [[-0.10326338  0.22890541  0.33034134  1.        ]]. Action = [[-0.3988887  -0.3266288   0.88971317  0.6186948 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1209. State = [[-0.11061252  0.23167603  0.34200886  1.        ]]. Action = [[-0.56223863  0.21192622 -0.2673449   0.82485604]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1210. State = [[-0.11430775  0.23907323  0.3448684   1.        ]]. Action = [[0.69162536 0.38725984 0.13034236 0.79136217]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1211. State = [[-0.11166497  0.24249195  0.33802062  1.        ]]. Action = [[ 0.58895123  0.18331003 -0.8753133   0.47007585]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1212. State = [[-0.10645869  0.25528482  0.30603427  1.        ]]. Action = [[-0.8314677   0.24704313 -0.647064    0.54597414]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1213. State = [[-0.11238305  0.26482028  0.30514312  1.        ]]. Action = [[-0.26030928  0.05456209  0.91963315  0.80189323]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1214. State = [[-0.11887211  0.2658449   0.31150305  1.        ]]. Action = [[-0.32998455 -0.3029858  -0.5408913   0.5057037 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1215. State = [[-0.11445503  0.24963161  0.30980027  1.        ]]. Action = [[ 0.90739477 -0.6581796   0.12253523  0.6224849 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1216. State = [[-0.10015247  0.22466527  0.32481578  1.        ]]. Action = [[ 0.19281292 -0.72976846  0.8719604   0.66796875]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1217. State = [[-0.08921524  0.20286441  0.34316587  1.        ]]. Action = [[ 0.62338746 -0.2660988   0.3523023   0.6892388 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1218. State = [[-0.08475316  0.19777827  0.3388442   1.        ]]. Action = [[-0.23902059  0.3253647  -0.8443982   0.590744  ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1219. State = [[-0.07871836  0.18800977  0.3225338   1.        ]]. Action = [[ 0.56386065 -0.8657691  -0.29928547  0.7217629 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1220. State = [[-0.06518578  0.1633476   0.3193321   1.        ]]. Action = [[-0.02312255 -0.53942126  0.36858368  0.7055242 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1221. State = [[-0.06681974  0.16128609  0.32359356  1.        ]]. Action = [[-0.77603513  0.30167258  0.01858747  0.7998816 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1222. State = [[-0.07635603  0.15728132  0.3196434   1.        ]]. Action = [[-0.20711553 -0.4205631  -0.52924     0.5668187 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1223. State = [[-0.08840725  0.14156964  0.30794698  1.        ]]. Action = [[-0.57139593 -0.55297095 -0.302212    0.7219312 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1224. State = [[-0.10725444  0.12515792  0.30164337  1.        ]]. Action = [[-0.68902117 -0.2693472   0.16083884  0.57541037]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1225. State = [[-0.13595976  0.13053767  0.30209455  1.        ]]. Action = [[-0.8626279   0.850245   -0.36337203  0.73453486]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1226. State = [[-0.1607939   0.15184036  0.28489447  1.        ]]. Action = [[ 0.08468127  0.48222136 -0.7776833   0.72126913]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1227. State = [[-0.16110958  0.15767534  0.28219593  1.        ]]. Action = [[ 0.06218684 -0.3919978   0.7353163   0.2854755 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1228. State = [[-0.17085047  0.1599132   0.27857867  1.        ]]. Action = [[-0.7559141   0.3354802  -0.81082433  0.8144207 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1229. State = [[-0.1870584   0.15456957  0.26964325  1.        ]]. Action = [[ 0.05557323 -0.5788144   0.3247702   0.30717647]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1230. State = [[-0.19064958  0.13443032  0.2773263   1.        ]]. Action = [[ 0.00098658 -0.60374653  0.47732008  0.72326016]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1231. State = [[-0.18367262  0.10447768  0.29539508  1.        ]]. Action = [[ 0.83078575 -0.9519863   0.573123    0.67614627]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1232. State = [[-0.18000384  0.08825798  0.2981652   1.        ]]. Action = [[-0.51632303  0.33107066 -0.5593618   0.5705148 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1233. State = [[-0.1852908   0.09856709  0.28343758  1.        ]]. Action = [[ 0.36813295  0.8038502  -0.8911867   0.65349436]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1234. State = [[-0.17891395  0.11427411  0.26803306  1.        ]]. Action = [[0.21941984 0.13160658 0.4385898  0.75996375]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1235. State = [[-0.17229258  0.13276924  0.27642956  1.        ]]. Action = [[0.01025057 0.78197503 0.49006677 0.2294401 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1236. State = [[-0.16418102  0.1547446   0.28897282  1.        ]]. Action = [[0.65948856 0.37323165 0.51437426 0.66061175]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1237. State = [[-0.16434693  0.17307608  0.2888723   1.        ]]. Action = [[-0.61222863  0.65893507 -0.9728341   0.4947474 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1238. State = [[-0.1680824   0.19725834  0.266751    1.        ]]. Action = [[ 0.49258375  0.75058174 -0.7207153   0.6017126 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1239. State = [[-0.16108334  0.21225294  0.24400693  1.        ]]. Action = [[-0.47460926 -0.3192569  -0.24142581  0.7958212 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1240. State = [[-0.1712645   0.21674232  0.23715113  1.        ]]. Action = [[-0.7957295   0.16824722  0.1057663   0.70405924]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1241. State = [[-0.1790953   0.21487574  0.2419094   1.        ]]. Action = [[ 0.31012976 -0.4160396   0.3926636   0.4705565 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1242. State = [[-0.18786289  0.20730379  0.25769472  1.        ]]. Action = [[-0.99608105 -0.1318931   0.6620352   0.68816817]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1243. State = [[-0.2146106   0.19780953  0.27515715  1.        ]]. Action = [[-0.2706834  -0.2423988   0.23567533  0.7853277 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1244. State = [[-0.22135818  0.18191072  0.28755477  1.        ]]. Action = [[ 0.09661186 -0.7063504   0.31046104  0.7165966 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1245. State = [[-0.22769295  0.1528513   0.29169807  1.        ]]. Action = [[-0.23557335 -0.98282033 -0.35832798  0.72690916]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1246. State = [[-0.23558459  0.11817265  0.28435385  1.        ]]. Action = [[ 0.26061785 -0.84433657 -0.45209527  0.60695076]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1247. State = [[-0.22988947  0.1063562   0.27284777  1.        ]]. Action = [[ 0.31545115  0.8117304  -0.50374264  0.6873574 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1248. State = [[-0.22733754  0.11441072  0.25662693  1.        ]]. Action = [[-0.28511083  0.03323555 -0.3145271   0.7920642 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1249. State = [[-0.21978177  0.12020505  0.25569862  1.        ]]. Action = [[0.78977835 0.09345973 0.5748775  0.82414556]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1250. State = [[-0.20400065  0.13239028  0.25195128  1.        ]]. Action = [[ 0.6504582   0.615911   -0.7768695   0.82470274]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1251. State = [[-0.18166433  0.14022447  0.24309605  1.        ]]. Action = [[ 0.3717208  -0.3009892   0.58805585  0.5755317 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1252. State = [[-0.1726487   0.13798797  0.24690013  1.        ]]. Action = [[ 0.48479176 -0.39040172 -0.92384195  0.65558136]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 1253. State = [[-0.16488312  0.1250588   0.25844562  1.        ]]. Action = [[ 0.4187081 -0.8616106  0.6735923  0.6900084]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1254. State = [[-0.15490712  0.10930188  0.26555237  1.        ]]. Action = [[ 0.9728875   0.61745286 -0.55454725  0.849507  ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: No entry zone
Current timestep = 1255. State = [[-0.1546558   0.1087655   0.26792186  1.        ]]. Action = [[-0.17188036  0.19163561  0.04344809  0.74862814]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1256. State = [[-0.15302013  0.11053681  0.2759048   1.        ]]. Action = [[0.0784831  0.01912034 0.50461054 0.7860538 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1257. State = [[-0.14484589  0.09987964  0.27942082  1.        ]]. Action = [[ 0.749634  -0.7809987 -0.6820845  0.7428527]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1258. State = [[-0.12728743  0.07206271  0.27488136  1.        ]]. Action = [[ 0.2724414  -0.7714481   0.23713279  0.7370379 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1259. State = [[-0.11696886  0.04497537  0.27432975  1.        ]]. Action = [[ 0.23506081 -0.65307105 -0.22386742  0.7352822 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1260. State = [[-0.1057281   0.03613147  0.27726173  1.        ]]. Action = [[0.31211424 0.35138297 0.41015768 0.61567605]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1261. State = [[-0.0971981   0.05127779  0.27582002  1.        ]]. Action = [[ 0.03886187  0.7794515  -0.6648407   0.94592726]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1262. State = [[-0.08980396  0.05378722  0.26337862  1.        ]]. Action = [[ 0.187083   -0.61935556 -0.07188511  0.85207343]]. Reward = [0.]
Curr episode timestep = 99
Above hoop
Current timestep = 1263. State = [[-0.08603182  0.0433426   0.26869428  1.        ]]. Action = [[ 0.06512356 -0.27072263  0.67150307  0.46685386]]. Reward = [0.]
Curr episode timestep = 100
Above hoop
Current timestep = 1264. State = [[-0.25680032 -0.06323639  0.08977084  1.        ]]. Action = [[-0.7524765   0.31615067 -0.39574176  0.40660548]]. Reward = [0.]
Curr episode timestep = 101
Above hoop
Current timestep = 1265. State = [[-0.25466183 -0.06267891  0.08275524  1.        ]]. Action = [[0.11720264 0.6758951  0.9114549  0.80181694]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1266. State = [[-0.25106132 -0.054727    0.09257685  1.        ]]. Action = [[-0.7873377   0.63182425 -0.25674427  0.7733748 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 1267. State = [[-0.24505822 -0.04728504  0.08750996  1.        ]]. Action = [[ 0.63138676  0.33742762 -0.7541424   0.769459  ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1268. State = [[-0.2364233  -0.04449891  0.08549365  1.        ]]. Action = [[-0.1460867  -0.201608    0.6932039   0.81891036]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1269. State = [[-0.22557068 -0.05581557  0.09083188  1.        ]]. Action = [[ 0.7934253 -0.765827  -0.3123024  0.9440012]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1270. State = [[-0.21296306 -0.06799068  0.09122559  1.        ]]. Action = [[-0.96020544  0.8357837   0.09497261  0.77602506]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 1271. State = [[-0.1978884  -0.08190886  0.09429333  1.        ]]. Action = [[ 0.84360766 -0.8295854   0.22477901  0.2670287 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1272. State = [[-0.18178777 -0.10014804  0.09918994  1.        ]]. Action = [[-0.14309776 -0.04548299  0.12312758  0.5620837 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1273. State = [[-0.18708563 -0.1008677   0.0984786   1.        ]]. Action = [[-0.76317805  0.22713757 -0.02438313  0.750473  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1274. State = [[-0.198569   -0.09925591  0.09130969  1.        ]]. Action = [[-0.26288337  0.01874804 -0.67484534  0.6929387 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1275. State = [[-0.21406272 -0.09908184  0.08755202  1.        ]]. Action = [[-0.79023457  0.03237891  0.5846822   0.6117637 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1276. State = [[-0.22031327 -0.09469529  0.09169253  1.        ]]. Action = [[ 0.880332    0.30570102 -0.3299439   0.8516364 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1277. State = [[-0.20788217 -0.0922642   0.08620454  1.        ]]. Action = [[ 0.46918654 -0.20835477 -0.4582702   0.5725827 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1278. State = [[-0.19922721 -0.09737872  0.08072278  1.        ]]. Action = [[-0.2168141  -0.30387157  0.500196    0.56949186]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1279. State = [[-0.20027912 -0.10875195  0.09314527  1.        ]]. Action = [[-0.38306212 -0.32311082  0.91995573  0.43917835]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1280. State = [[-0.21028504 -0.1066842   0.10132466  1.        ]]. Action = [[-0.10653335  0.57993793 -0.9415056   0.16578889]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1281. State = [[-0.20548733 -0.11148386  0.08453884  1.        ]]. Action = [[ 0.86632156 -0.84496224 -0.9261655   0.44756603]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1282. State = [[-0.19460706 -0.141127    0.07281964  1.        ]]. Action = [[-0.42116582 -0.9237658   0.9820788   0.7907238 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1283. State = [[-0.19419707 -0.15870097  0.07981205  1.        ]]. Action = [[0.83614016 0.7137203  0.75566053 0.7542254 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 1284. State = [[-0.19316204 -0.16202381  0.08166384  1.        ]]. Action = [[ 0.5190222  0.4592402 -0.0609777  0.7242379]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 1285. State = [[-0.20057067 -0.15914692  0.09180705  1.        ]]. Action = [[-0.7450133   0.38388383  0.87153614  0.4030249 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1286. State = [[-0.22859213 -0.15092815  0.10564485  1.        ]]. Action = [[-0.9350317   0.21920812 -0.26554     0.43968427]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1287. State = [[-0.24475983 -0.14619178  0.10637238  1.        ]]. Action = [[ 0.13635218  0.03147233 -0.15553105  0.808784  ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1288. State = [[-0.24513866 -0.14119749  0.10294878  1.        ]]. Action = [[ 0.17153168  0.263669   -0.37421536  0.7126429 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1289. State = [[-0.23747574 -0.12129138  0.08791246  1.        ]]. Action = [[ 0.93895817  0.87078094 -0.9497239   0.9120078 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1290. State = [[-0.23384228 -0.10217276  0.05606041  1.        ]]. Action = [[-0.54414487  0.04309022 -0.7532685   0.9122803 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1291. State = [[-0.2407369  -0.09970828  0.03586117  1.        ]]. Action = [[ 0.01496363  0.40805173 -0.9699139   0.49563766]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1292. State = [[-0.2408515  -0.09769277  0.03454621  1.        ]]. Action = [[ 0.4068303   0.47948635 -0.8348851   0.63166845]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 1293. State = [[-0.23366901 -0.0831159   0.03939427  1.        ]]. Action = [[0.61123323 0.8880689  0.6613648  0.7739167 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1294. State = [[-0.22741778 -0.06682362  0.04405732  1.        ]]. Action = [[-0.742868   -0.3409499   0.01025546  0.46772707]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 1295. State = [[-0.23030627 -0.05315983  0.05137379  1.        ]]. Action = [[-0.6486849   0.7501849   0.61791945  0.7741432 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1296. State = [[-0.23489194 -0.03693138  0.06241478  1.        ]]. Action = [[ 0.01344478  0.48097372 -0.67870307  0.4720049 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 1297. State = [[-0.23237328 -0.04407708  0.07096676  1.        ]]. Action = [[ 0.4017297  -0.6171324   0.4590925   0.76624346]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1298. State = [[-0.21739572 -0.05423727  0.0939052   1.        ]]. Action = [[ 0.8917271  -0.07751048  0.9221747   0.8693571 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1299. State = [[-0.19135964 -0.04286536  0.12244761  1.        ]]. Action = [[0.61634684 0.9119574  0.34191442 0.6283548 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1300. State = [[-0.17480512 -0.02745005  0.13347971  1.        ]]. Action = [[ 0.52702427  0.29102528 -0.20780367  0.7977781 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: No entry zone
Current timestep = 1301. State = [[-0.17576614 -0.03928042  0.14223656  1.        ]]. Action = [[-0.70703834 -0.8318552   0.6108742   0.5197302 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1302. State = [[-0.181      -0.05417536  0.15960751  1.        ]]. Action = [[-0.09960866 -0.01326519  0.22302055  0.6394882 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1303. State = [[-0.18631527 -0.05272542  0.15893157  1.        ]]. Action = [[ 0.12458575  0.19776893 -0.93579894  0.91294   ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1304. State = [[-0.19745487 -0.05882721  0.15111218  1.        ]]. Action = [[-0.8330769  -0.5481441   0.3596251   0.36041045]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1305. State = [[-0.20704533 -0.0658826   0.14373544  1.        ]]. Action = [[ 0.3445443   0.08527124 -0.72545725  0.93329465]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1306. State = [[-0.20940991 -0.05988638  0.12988606  1.        ]]. Action = [[-0.22299802  0.4709816  -0.25226063  0.7616253 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1307. State = [[-0.20831214 -0.04954286  0.12824318  1.        ]]. Action = [[0.4166155  0.24636447 0.45925748 0.8000541 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1308. State = [[-0.19756316 -0.05231082  0.12619147  1.        ]]. Action = [[ 0.72339904 -0.7048339  -0.54110587  0.64305985]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1309. State = [[-0.18288608 -0.06058171  0.11841936  1.        ]]. Action = [[ 0.66242385 -0.53957194  0.69765306  0.8659167 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 1310. State = [[-0.17721894 -0.07358558  0.11937726  1.        ]]. Action = [[ 0.262609   -0.73616713 -0.0416109   0.51941776]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1311. State = [[-0.17434195 -0.09026436  0.12480556  1.        ]]. Action = [[-0.38271046 -0.11178845  0.7257184   0.6517525 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1312. State = [[-0.17374696 -0.09446274  0.13066505  1.        ]]. Action = [[ 0.5802244  -0.3220917  -0.79587597  0.33132243]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 1313. State = [[-0.17401485 -0.09506778  0.13215542  1.        ]]. Action = [[ 0.23615515  0.01054573 -0.82073295  0.82949877]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 1314. State = [[-0.17421165 -0.09582397  0.13245599  1.        ]]. Action = [[0.9484867  0.55370295 0.56246185 0.48786736]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 1315. State = [[-0.17454219 -0.09628165  0.13259639  1.        ]]. Action = [[ 0.09951615 -0.1655407   0.41829157  0.81666064]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 1316. State = [[-0.17448609 -0.0963565   0.13263187  1.        ]]. Action = [[0.34137    0.5600643  0.79687357 0.8102176 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 1317. State = [[-0.18315488 -0.11155764  0.14356004  1.        ]]. Action = [[-0.9129781  -0.82417     0.87663877  0.64649963]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1318. State = [[-0.21087758 -0.11775005  0.15575822  1.        ]]. Action = [[-0.64856875  0.63060844 -0.53324     0.7176496 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1319. State = [[-0.22049221 -0.10252113  0.14809105  1.        ]]. Action = [[ 0.5948565   0.53513765 -0.4121796   0.24088264]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1320. State = [[-0.20565926 -0.09269125  0.14044234  1.        ]]. Action = [[ 0.9359633  -0.26408738 -0.3603691   0.85585666]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1321. State = [[-0.18933749 -0.094901    0.1295789   1.        ]]. Action = [[ 0.7997942  -0.584741    0.8486155   0.76383245]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 1322. State = [[-0.18909416 -0.10533765  0.12842405  1.        ]]. Action = [[-0.36841846 -0.65177166 -0.13127458  0.39785552]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1323. State = [[-0.19123948 -0.11574577  0.1203877   1.        ]]. Action = [[ 0.10977066  0.06357026 -0.43595016  0.7259592 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1324. State = [[-0.18776332 -0.11251435  0.11763572  1.        ]]. Action = [[0.327729   0.32865846 0.7314751  0.8911712 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1325. State = [[-0.18393093 -0.11023899  0.12141841  1.        ]]. Action = [[ 0.86381364 -0.17872238 -0.592411    0.72464204]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 1326. State = [[-0.1830643  -0.10953403  0.12163894  1.        ]]. Action = [[0.53808403 0.54723096 0.8308022  0.81305945]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 1327. State = [[-0.1828772  -0.10956879  0.12169874  1.        ]]. Action = [[0.7002351  0.82711935 0.37280655 0.6855974 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 1328. State = [[-0.184321   -0.11293283  0.11782599  1.        ]]. Action = [[-0.09121096 -0.32300222 -0.5349029   0.63101435]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1329. State = [[-0.18541147 -0.11485782  0.11525249  1.        ]]. Action = [[ 0.5519588  -0.90743387  0.20678258  0.6929053 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 1330. State = [[-0.19428574 -0.11103933  0.11298082  1.        ]]. Action = [[-0.92264277  0.39862514  0.08790016  0.627548  ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1331. State = [[-0.20541418 -0.10848219  0.10969678  1.        ]]. Action = [[ 0.87381864 -0.68268937 -0.20615059  0.6454506 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 1332. State = [[-0.20612757 -0.10820153  0.10948222  1.        ]]. Action = [[ 0.84505296  0.21872175 -0.9932014   0.6793604 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 1333. State = [[-0.20975648 -0.11243089  0.1026511   1.        ]]. Action = [[-0.30395734 -0.35400027 -0.6512551   0.27211988]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1334. State = [[-0.21260938 -0.12043311  0.09817358  1.        ]]. Action = [[-0.04382598 -0.28988814  0.62646854  0.39902568]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1335. State = [[-0.2066429  -0.11711271  0.10309561  1.        ]]. Action = [[ 0.90278625  0.5265522  -0.17652607  0.7978189 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1336. State = [[-0.20174922 -0.11170445  0.10289828  1.        ]]. Action = [[ 0.9299145  -0.6607515   0.9103602   0.76759577]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 1337. State = [[-0.20111603 -0.11046851  0.10276243  1.        ]]. Action = [[ 0.8397943  -0.12616622 -0.8760102   0.6789894 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 1338. State = [[-0.20066345 -0.1092778   0.10259405  1.        ]]. Action = [[ 0.6761881  -0.33858526  0.68093514  0.8778374 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 1339. State = [[-0.20593737 -0.11738204  0.09822977  1.        ]]. Action = [[-0.67949706 -0.58234    -0.45976377  0.8135333 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1340. State = [[-0.20262168 -0.12166381  0.10193485  1.        ]]. Action = [[0.66915035 0.37788475 0.98127556 0.7896067 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1341. State = [[-0.19199388 -0.11681827  0.11196647  1.        ]]. Action = [[ 0.5316802   0.07173193 -0.33461714  0.5933118 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1342. State = [[-0.19253571 -0.11394344  0.10779694  1.        ]]. Action = [[-0.60102385 -0.09637225 -0.31546986  0.85090923]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1343. State = [[-0.1979887  -0.12587984  0.09977186  1.        ]]. Action = [[-0.14000666 -0.74394923 -0.29762232  0.8172617 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1344. State = [[-0.19859713 -0.14290272  0.0996104   1.        ]]. Action = [[ 0.02429509 -0.15172124  0.6982055   0.59488535]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1345. State = [[-0.19601248 -0.13493694  0.10029323  1.        ]]. Action = [[ 0.39837003  0.82349265 -0.6991889   0.51030445]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1346. State = [[-0.19009218 -0.1338029   0.09712381  1.        ]]. Action = [[ 0.2116642  -0.7388609   0.49838638  0.90063715]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1347. State = [[-0.18851472 -0.149464    0.09124477  1.        ]]. Action = [[-0.13765073 -0.47335553 -0.99563414  0.6316669 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1348. State = [[-0.19324622 -0.1726371   0.07026701  1.        ]]. Action = [[-0.2681179  -0.67690086 -0.4763137   0.19899678]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1349. State = [[-0.20840256 -0.19110692  0.04799671  1.        ]]. Action = [[-0.8981796  -0.07434565 -0.8014815   0.61354065]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1350. State = [[-0.21939936 -0.18695812  0.02254828  1.        ]]. Action = [[ 0.21053195  0.50840616 -0.33369172  0.6325247 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1351. State = [[-0.21234766 -0.17744565  0.01870754  1.        ]]. Action = [[0.8000535  0.22932506 0.6527965  0.789052  ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1352. State = [[-0.20322005 -0.16908099  0.02634897  1.        ]]. Action = [[-0.25521684  0.2840073   0.2225684   0.7627566 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1353. State = [[-0.20232882 -0.16347222  0.0271242   1.        ]]. Action = [[ 0.07952297  0.38665533 -0.80368394  0.8189657 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 1354. State = [[-0.2039944  -0.15269156  0.03820912  1.        ]]. Action = [[-0.33676767  0.65916014  0.9334693   0.7967446 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1355. State = [[-0.20545901 -0.13894549  0.0603987   1.        ]]. Action = [[ 0.849267   -0.22637635  0.8790457   0.18906367]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 1356. State = [[-0.21779759 -0.12459252  0.07250573  1.        ]]. Action = [[-0.68179905  0.815356    0.81731427  0.2560234 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1357. State = [[-0.23174942 -0.10655365  0.09149933  1.        ]]. Action = [[-0.6272692   0.5063484  -0.95027876  0.79825425]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 1358. State = [[-0.24502233 -0.09074287  0.09011222  1.        ]]. Action = [[-0.6984033  0.8597889 -0.6359058  0.6057205]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1359. State = [[-0.25834084 -0.07463395  0.08552018  1.        ]]. Action = [[-0.63100976  0.44159806 -0.48174405  0.739063  ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 1360. State = [[-0.25311163 -0.07107297  0.08431819  1.        ]]. Action = [[ 0.83648515 -0.00307781 -0.14150977  0.7604252 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1361. State = [[-0.24105892 -0.0803735   0.08550613  1.        ]]. Action = [[ 0.14891148 -0.8109996   0.3286754   0.71975994]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1362. State = [[-0.23417908 -0.09293391  0.09604314  1.        ]]. Action = [[ 0.17436588 -0.08378625  0.894799    0.50179887]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1363. State = [[-0.22664304 -0.10193118  0.10872979  1.        ]]. Action = [[ 0.29476488 -0.33911103 -0.3733716   0.6833092 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1364. State = [[-0.20858727 -0.12186365  0.11472102  1.        ]]. Action = [[ 0.86906576 -0.8662365   0.43983543  0.66394556]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1365. State = [[-0.1880748  -0.13076228  0.13187402  1.        ]]. Action = [[-0.00996351  0.5396364   0.8872889   0.7402017 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1366. State = [[-0.25308383 -0.0315396   0.12188575  1.        ]]. Action = [[ 0.66459465 -0.40986848 -0.15453833  0.7638042 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 1367. State = [[-0.24621974 -0.0305472   0.10939889  1.        ]]. Action = [[0.4560536  0.4793172  0.17640257 0.46739173]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1368. State = [[-0.2342979  -0.03604414  0.10476825  1.        ]]. Action = [[ 0.48574305 -0.6890423  -0.617789    0.69693255]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1369. State = [[-0.21135485 -0.05582438  0.09232691  1.        ]]. Action = [[ 0.7639997  -0.7769263   0.05185068  0.08209467]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1370. State = [[-0.18687537 -0.08254229  0.09762629  1.        ]]. Action = [[ 0.53983855 -0.59031814  0.869045    0.10868609]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1371. State = [[-0.1709564  -0.09854004  0.12172089  1.        ]]. Action = [[-0.15357435 -0.07730573  0.9436374   0.55198765]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1372. State = [[-0.16909602 -0.1023048   0.14487067  1.        ]]. Action = [[ 0.26056027 -0.00271225  0.60797334  0.7975869 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Current timestep = 1373. State = [[-0.16882847 -0.10248289  0.14669302  1.        ]]. Action = [[ 0.84335387 -0.2722062  -0.7250709   0.7670226 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 1374. State = [[-0.16902868 -0.10272206  0.14670375  1.        ]]. Action = [[ 0.3633921   0.75898635 -0.37874317  0.43722975]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 1375. State = [[-0.17792138 -0.09232342  0.15191434  1.        ]]. Action = [[-0.980008    0.76647186  0.39573574  0.69141626]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1376. State = [[-0.19399127 -0.08113163  0.1624552   1.        ]]. Action = [[ 0.6104524  -0.45808506  0.30135036  0.683514  ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 1377. State = [[-0.20600449 -0.08868255  0.16647807  1.        ]]. Action = [[-0.9063629  -0.5310715   0.30136085  0.8478718 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1378. State = [[-0.22291987 -0.07967167  0.16957466  1.        ]]. Action = [[ 0.39148748  0.9817238  -0.6192086   0.22674417]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1379. State = [[-0.21127865 -0.05007831  0.17008051  1.        ]]. Action = [[0.9301448  0.9652102  0.58700943 0.87614226]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1380. State = [[-0.1914564  -0.01916544  0.18436658  1.        ]]. Action = [[0.47920632 0.69217837 0.71261346 0.8229368 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1381. State = [[-0.17331326  0.0080425   0.20339705  1.        ]]. Action = [[0.16870248 0.41549003 0.21148396 0.7511182 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1382. State = [[-0.17157787  0.01881474  0.21573733  1.        ]]. Action = [[-0.7602465  -0.02917331  0.3702476   0.8759297 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1383. State = [[-0.18033642  0.01210563  0.23569462  1.        ]]. Action = [[-0.16956097 -0.51093096  0.6475854   0.80113494]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1384. State = [[-0.19029304  0.01583116  0.26228213  1.        ]]. Action = [[-0.33561194  0.7342708   0.763119    0.8168613 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1385. State = [[-0.19560881  0.04328039  0.28559     1.        ]]. Action = [[0.52652097 0.92789125 0.2877314  0.7710428 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1386. State = [[-0.19653821  0.05376317  0.28618935  1.        ]]. Action = [[-0.45527494 -0.47579014 -0.7960611   0.7886864 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1387. State = [[-0.19625193  0.05219802  0.27114555  1.        ]]. Action = [[ 0.54276574  0.24852562 -0.5651991   0.5996182 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1388. State = [[-0.1806987   0.05407775  0.2665867   1.        ]]. Action = [[ 0.5561249  -0.09013343  0.7208035   0.5896864 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1389. State = [[-0.17809781  0.05158641  0.26332596  1.        ]]. Action = [[-0.69051445 -0.20737767 -0.87031066  0.60483825]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1390. State = [[-0.19062658  0.06049612  0.2581857   1.        ]]. Action = [[-0.8458198   0.72580266  0.42147493  0.729537  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1391. State = [[-0.2092754   0.06223637  0.25313425  1.        ]]. Action = [[-0.22679281 -0.7265048  -0.652753    0.6597693 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1392. State = [[-0.21159127  0.03746164  0.2513155   1.        ]]. Action = [[ 0.71246886 -0.948241    0.5904188   0.7037885 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1393. State = [[-0.20917843  0.03002165  0.2655317   1.        ]]. Action = [[-0.3496176  0.7238717  0.6731378  0.6009381]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1394. State = [[-0.206066    0.04632314  0.2774575   1.        ]]. Action = [[0.71106243 0.67600596 0.0417974  0.83384204]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1395. State = [[-0.20248239  0.06058509  0.27326083  1.        ]]. Action = [[-0.4122299   0.15036786 -0.8010076   0.40942   ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1396. State = [[-0.20698716  0.07716201  0.26347262  1.        ]]. Action = [[0.09044302 0.66465163 0.10172641 0.83019996]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1397. State = [[-0.2046323   0.07530642  0.26628673  1.        ]]. Action = [[ 0.09896386 -0.95550203  0.36522627  0.57061005]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1398. State = [[-0.19276889  0.06886501  0.27916738  1.        ]]. Action = [[0.60111594 0.32351768 0.7079308  0.40138328]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1399. State = [[-0.18704358  0.05969453  0.289991    1.        ]]. Action = [[-0.7328005  -0.7302667  -0.25327718  0.71909857]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1400. State = [[-0.19712394  0.03663784  0.28235552  1.        ]]. Action = [[-0.21951354 -0.7140716  -0.787838    0.76408315]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1401. State = [[-0.1965325   0.02861259  0.27961835  1.        ]]. Action = [[0.8365681  0.42146325 0.40436578 0.78865814]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1402. State = [[-0.18638045  0.0329681   0.2826988   1.        ]]. Action = [[0.19496536 0.13674974 0.29144454 0.6455654 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1403. State = [[-0.17912272  0.04007632  0.289475    1.        ]]. Action = [[0.03529048 0.30780125 0.25444615 0.70109415]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1404. State = [[-0.16409066  0.03649133  0.30115557  1.        ]]. Action = [[ 0.9318962  -0.63840455  0.35409307  0.5898243 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1405. State = [[-0.15212873  0.03609     0.30860117  1.        ]]. Action = [[-0.23677886  0.5621543  -0.26030838  0.7413666 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1406. State = [[-0.14851911  0.03849581  0.2974104   1.        ]]. Action = [[ 0.18233943 -0.25408947 -0.4542979   0.66737294]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1407. State = [[-0.13848016  0.02847155  0.3011049   1.        ]]. Action = [[ 0.45027232 -0.59027916  0.9614742   0.7499068 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1408. State = [[-0.1299841   0.02486704  0.3201272   1.        ]]. Action = [[-0.2906043   0.36761093  0.23819447  0.80083215]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1409. State = [[-0.13873054  0.01863513  0.31559193  1.        ]]. Action = [[-0.75917304 -0.54473084 -0.783404    0.6437403 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1410. State = [[-0.1583029   0.00876692  0.30031705  1.        ]]. Action = [[-0.9414822  -0.0816412  -0.8439258   0.57723737]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1411. State = [[-1.8742749e-01  4.3920070e-04  2.7124488e-01  1.0000000e+00]]. Action = [[-0.7047025  -0.22752118 -0.68744063  0.77808213]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1412. State = [[-0.20571303  0.00217435  0.26323548  1.        ]]. Action = [[-0.05052459  0.3668964   0.87187123  0.5550382 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1413. State = [[-0.20912476  0.01742884  0.27370045  1.        ]]. Action = [[0.43978715 0.5770284  0.07336879 0.74590623]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1414. State = [[-0.21071574  0.02731882  0.27480492  1.        ]]. Action = [[-3.2563376e-01  1.3494492e-04 -1.9324881e-01  6.7892671e-01]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1415. State = [[-0.20538673  0.02122515  0.28054446  1.        ]]. Action = [[ 0.70603704 -0.5818833   0.6075909   0.9117949 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1416. State = [[-0.20545971  0.01694896  0.28232852  1.        ]]. Action = [[-0.6196082   0.32559574 -0.64356035  0.76691556]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1417. State = [[-0.21047252  0.00760962  0.27752233  1.        ]]. Action = [[ 0.01916242 -0.77241755 -0.06226003  0.75705814]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1418. State = [[-0.21058875  0.00747908  0.2664646   1.        ]]. Action = [[ 0.28019834  0.7915541  -0.91077226  0.8856467 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1419. State = [[-0.2132212   0.03116209  0.23811132  1.        ]]. Action = [[-0.53742856  0.8985038  -0.47350323  0.41149437]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1420. State = [[-0.22482021  0.04116788  0.23769426  1.        ]]. Action = [[-0.79667485 -0.610788    0.94426847  0.7258527 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1421. State = [[-0.24166907  0.03535134  0.24796636  1.        ]]. Action = [[-0.9167551 -0.8663672  0.5145886  0.7672174]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 1422. State = [[-0.24121982  0.02291405  0.25980884  1.        ]]. Action = [[ 0.3911929  -0.6874022   0.7547281   0.52706504]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1423. State = [[-0.24554333  0.0226443   0.27232715  1.        ]]. Action = [[-0.4755162   0.72333574  0.1094178   0.65053   ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1424. State = [[-0.24355182  0.0232257   0.29143313  1.        ]]. Action = [[ 0.82102084 -0.58335054  0.95196605  0.7543845 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1425. State = [[-0.23204006  0.00499407  0.31884858  1.        ]]. Action = [[ 0.11023915 -0.57031804  0.55065703  0.65871   ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1426. State = [[-0.22910857 -0.00667638  0.3341844   1.        ]]. Action = [[-0.8611331  -0.00588119 -0.16136867  0.7095113 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 1427. State = [[-0.22882412 -0.00904693  0.33262426  1.        ]]. Action = [[-0.01837885 -0.07392216 -0.5315538   0.7171223 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1428. State = [[-2.2091278e-01  2.0890363e-04  3.2200104e-01  1.0000000e+00]]. Action = [[ 0.9156337   0.68948126 -0.57656366  0.891024  ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1429. State = [[-0.20264521  0.01650786  0.29691187  1.        ]]. Action = [[ 0.02912807  0.4510231  -0.81751114  0.66764605]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1430. State = [[-0.187725    0.01340766  0.27063042  1.        ]]. Action = [[ 0.7482691  -0.8044027  -0.69333017  0.65526056]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1431. State = [[-0.17052293  0.00511554  0.25184348  1.        ]]. Action = [[0.09537983 0.10752523 0.03338885 0.79290605]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1432. State = [[-0.16472751  0.00172482  0.25885788  1.        ]]. Action = [[-0.09150118 -0.17617649  0.8559725   0.766201  ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1433. State = [[-0.1627084  -0.00280067  0.26780775  1.        ]]. Action = [[ 0.05112743 -0.48615634 -0.54532653  0.5725763 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 1434. State = [[-0.16915032 -0.01453572  0.26262355  1.        ]]. Action = [[-0.48556685 -0.7239865  -0.8153604   0.7705612 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1435. State = [[-0.17062418 -0.02733444  0.26197046  1.        ]]. Action = [[ 0.5288842  -0.05005461  0.63988435  0.47409463]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1436. State = [[-0.17425635 -0.01981756  0.27735436  1.        ]]. Action = [[-0.9255565  0.6986587  0.8012371  0.8487406]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1437. State = [[-1.8755990e-01  8.0222555e-04  2.9300147e-01  1.0000000e+00]]. Action = [[-0.05838156  0.59095573 -0.11808604  0.8348248 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1438. State = [[-0.19006036  0.0194908   0.28371787  1.        ]]. Action = [[ 0.6353835   0.5746821  -0.9732217   0.80047226]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1439. State = [[-0.18025777  0.03086166  0.25795916  1.        ]]. Action = [[ 0.11695218 -0.06376427 -0.87031883  0.62872744]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1440. State = [[-0.18169609  0.03382001  0.23689781  1.        ]]. Action = [[-0.78589785  0.02881813 -0.09155834  0.83705664]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1441. State = [[-0.19199638  0.04289807  0.2308318   1.        ]]. Action = [[-0.22810918  0.5680177  -0.09074295  0.5989721 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1442. State = [[-0.20141499  0.0600372   0.22771737  1.        ]]. Action = [[-0.19284588  0.37806404 -0.13366175  0.7473906 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1443. State = [[-0.2070965  0.0596463  0.2324478  1.       ]]. Action = [[-0.2610476 -0.627175   0.590477   0.669739 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1444. State = [[-0.21239468  0.05273792  0.24124382  1.        ]]. Action = [[0.04145348 0.04676867 0.3602376  0.91298974]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1445. State = [[-0.21826299  0.0502242   0.25867897  1.        ]]. Action = [[-0.43441927 -0.11752665  0.76007295  0.8659102 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1446. State = [[-0.23780613  0.05011178  0.2641835   1.        ]]. Action = [[-0.48830462  0.1910162  -0.9379511   0.5504888 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1447. State = [[-0.2421966   0.03744038  0.24715833  1.        ]]. Action = [[ 0.7567334  -0.79594815 -0.8459727   0.84660923]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1448. State = [[-0.22189781  0.01713545  0.2335244   1.        ]]. Action = [[ 0.8472955  -0.57505596  0.43323505  0.5817368 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1449. State = [[-0.20225184  0.0078857   0.24147595  1.        ]]. Action = [[0.4192376  0.16482306 0.83480096 0.82770896]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1450. State = [[-2.0088135e-01 -8.5300839e-05  2.4499303e-01  1.0000000e+00]]. Action = [[-0.88108975 -0.49714506 -0.77530056  0.74926496]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1451. State = [[-0.2068924   0.00196812  0.23212408  1.        ]]. Action = [[ 0.63712     0.86294174 -0.5697504   0.8337556 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1452. State = [[-2.0005819e-01  3.2429256e-05  2.2923821e-01  1.0000000e+00]]. Action = [[-0.5070105  -0.9629824   0.80295324  0.86506224]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1453. State = [[-0.21039265 -0.00658161  0.23431422  1.        ]]. Action = [[-0.47582138  0.4231217  -0.23070347  0.76140213]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1454. State = [[-0.21348462 -0.01307561  0.23158927  1.        ]]. Action = [[ 0.4656787  -0.6791592  -0.32079536  0.7599852 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1455. State = [[-0.21423016 -0.02237743  0.23136644  1.        ]]. Action = [[-0.57153577  0.1613431   0.2928753   0.63612556]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1456. State = [[-0.20824066 -0.00777359  0.23340963  1.        ]]. Action = [[0.939142   0.98567533 0.00751352 0.9011986 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1457. State = [[-0.19070059  0.02197217  0.24376267  1.        ]]. Action = [[0.82469225 0.6171496  0.8705107  0.83270454]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1458. State = [[-0.17647564  0.03301655  0.2681182   1.        ]]. Action = [[-0.2694134  -0.20538092  0.9291636   0.5589814 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1459. State = [[-0.17311803  0.02560873  0.2914949   1.        ]]. Action = [[ 0.5977982  -0.4926899   0.24172509  0.7285937 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1460. State = [[-0.1590608   0.030422    0.29402333  1.        ]]. Action = [[ 0.6568172   0.8460779  -0.84009117  0.7003666 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1461. State = [[-0.26072046  0.00176372  0.09770183  1.        ]]. Action = [[-0.773381   0.3859191  0.8868983 -0.501035 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1462. State = [[-0.25857916  0.00701908  0.08469394  1.        ]]. Action = [[0.19649196 0.59427285 0.33111453 0.25770748]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1463. State = [[-0.2441773   0.01208707  0.09043358  1.        ]]. Action = [[ 0.7448945  -0.21685427  0.4316877   0.66297305]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1464. State = [[-0.21806043  0.01356303  0.09734356  1.        ]]. Action = [[ 0.9821111   0.27989125 -0.21572876  0.58916616]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1465. State = [[-0.19987273  0.00271291  0.10687985  1.        ]]. Action = [[-0.5816648  -0.95566016  0.8362911   0.8746091 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1466. State = [[-0.20052563 -0.0112026   0.11941613  1.        ]]. Action = [[ 0.76537144 -0.5710703   0.6618246   0.7782737 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 1467. State = [[-0.20099778 -0.00484401  0.12720157  1.        ]]. Action = [[-0.02621716  0.5799812   0.5461321   0.8765347 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1468. State = [[-0.20556313 -0.00271748  0.14571801  1.        ]]. Action = [[-0.49648142 -0.42877316  0.36778343  0.63558674]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1469. State = [[-0.22050248 -0.02010655  0.15121454  1.        ]]. Action = [[-0.60359406 -0.80655515 -0.45826322  0.5110278 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1470. State = [[-0.24703622 -0.02560965  0.14309871  1.        ]]. Action = [[-0.9234344  0.6201494 -0.5050233  0.8099768]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1471. State = [[-0.26755443 -0.02178219  0.12668253  1.        ]]. Action = [[ 0.27465177 -0.15530294 -0.8513078   0.74883866]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1472. State = [[-0.2714784  -0.02232142  0.10473622  1.        ]]. Action = [[-0.24833083  0.77505076  0.277254    0.47859526]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1473. State = [[-0.2710604  -0.01459577  0.107499    1.        ]]. Action = [[0.07083142 0.57648396 0.74393654 0.76325274]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1474. State = [[-0.26970202  0.0078615   0.12343266  1.        ]]. Action = [[0.06699657 0.9092871  0.7346139  0.63817835]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1475. State = [[-0.26943788  0.02825203  0.14009893  1.        ]]. Action = [[-0.6897321  -0.23490286  0.83951795  0.7569735 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 1476. State = [[-0.26997653  0.0304357   0.14042902  1.        ]]. Action = [[-0.8079309   0.18872619 -0.09740102  0.54721403]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 1477. State = [[-0.2701487   0.0308342   0.14042908  1.        ]]. Action = [[-0.20349503 -0.7389418  -0.89491034  0.44577587]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1478. State = [[-0.27016798  0.03087875  0.1404291   1.        ]]. Action = [[-0.7914553  -0.1545133  -0.7621063   0.64168835]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1479. State = [[-0.25965506  0.03276976  0.1349344   1.        ]]. Action = [[ 0.91142106  0.1126622  -0.8821377   0.6618085 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1480. State = [[-0.24436474  0.03026499  0.1255679   1.        ]]. Action = [[-0.13528985 -0.41439742  0.28405333  0.62119174]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1481. State = [[-0.24303389  0.03280104  0.13292925  1.        ]]. Action = [[0.08775783 0.48817372 0.595541   0.26593947]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1482. State = [[-0.24562189  0.04122007  0.14032829  1.        ]]. Action = [[-0.48506582  0.21575415 -0.23481983  0.7961509 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1483. State = [[-0.24994174  0.04674137  0.13969858  1.        ]]. Action = [[-0.8378327   0.26794314  0.6379142   0.4709053 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 1484. State = [[-0.25020903  0.0581893   0.13844079  1.        ]]. Action = [[ 0.32338607  0.72082496 -0.24187076  0.7058084 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1485. State = [[-0.24964696  0.07047681  0.13875772  1.        ]]. Action = [[-0.936909    0.68557334 -0.901189    0.668545  ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 1486. State = [[-0.24990098  0.07379069  0.13845749  1.        ]]. Action = [[-0.5617449   0.53118694  0.73623466  0.7532222 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 1487. State = [[-0.24975462  0.07496511  0.13844505  1.        ]]. Action = [[-0.500817   -0.09914005 -0.60679376  0.80124736]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1488. State = [[-0.24979559  0.0753499   0.13840732  1.        ]]. Action = [[-0.41017187  0.5224471   0.45480478  0.5050179 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1489. State = [[-0.24871936  0.07612737  0.13864161  1.        ]]. Action = [[ 0.11112761  0.04912448 -0.04404473  0.78529453]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1490. State = [[-0.24544343  0.06890824  0.14403361  1.        ]]. Action = [[-0.3282873  -0.6424494   0.74879384  0.84046507]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1491. State = [[-0.24099165  0.0478161   0.15095915  1.        ]]. Action = [[ 0.4592837  -0.6624777  -0.19943029  0.83388186]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1492. State = [[-0.23925786  0.03847058  0.15178609  1.        ]]. Action = [[-0.39060974  0.13349235  0.08831728  0.649372  ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1493. State = [[-0.24165075  0.03110057  0.16231807  1.        ]]. Action = [[-0.19560188 -0.45408797  0.8681934   0.27472115]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1494. State = [[-0.24606106  0.02554804  0.17388076  1.        ]]. Action = [[ 0.11514056  0.30157435 -0.6262087   0.84015226]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1495. State = [[-0.24705526  0.02790773  0.1717928   1.        ]]. Action = [[-0.59282964 -0.55188465  0.6843103   0.4474703 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 1496. State = [[-0.24711452  0.02807737  0.17167306  1.        ]]. Action = [[-0.76469314 -0.06912661 -0.6324997   0.83156085]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 1497. State = [[-0.24181806  0.03231295  0.16488872  1.        ]]. Action = [[ 0.7606313   0.26299453 -0.6864489   0.7471826 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1498. State = [[-0.22823852  0.04463165  0.15793487  1.        ]]. Action = [[-0.23178339  0.4525447   0.88328445  0.7821858 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1499. State = [[-0.22903933  0.05351294  0.16526483  1.        ]]. Action = [[-0.8183813   0.10221756 -0.17090142  0.6453215 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 1500. State = [[-0.22083789  0.06740201  0.17158484  1.        ]]. Action = [[0.726341  0.8386041 0.3992772 0.8770629]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1501. State = [[-0.20230661  0.07006393  0.18661301  1.        ]]. Action = [[ 0.28608513 -0.90475243  0.24691474  0.70656466]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1502. State = [[-0.19275752  0.04876479  0.19370806  1.        ]]. Action = [[ 0.06449437 -0.67402786 -0.13434762  0.4914285 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1503. State = [[-0.1901504   0.03663079  0.19536304  1.        ]]. Action = [[0.9060931  0.74182284 0.31898916 0.55988026]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 1504. State = [[-0.18911216  0.03484995  0.19566047  1.        ]]. Action = [[ 0.8730384  -0.21880037  0.858984    0.84268737]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 1505. State = [[-0.18814786  0.01973967  0.20041843  1.        ]]. Action = [[-0.13072097 -0.82134753  0.4462366   0.7371361 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1506. State = [[-0.19756258  0.0162736   0.20917398  1.        ]]. Action = [[-0.8692573   0.7731377   0.26229954  0.51241684]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1507. State = [[-0.21835023  0.01768343  0.21396217  1.        ]]. Action = [[-0.6058493  -0.53120744 -0.22355014  0.8557832 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1508. State = [[-0.22588766  0.00344109  0.22301528  1.        ]]. Action = [[ 0.4753487  -0.40235984  0.8332927   0.70071614]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1509. State = [[-0.21385075 -0.01998059  0.24294451  1.        ]]. Action = [[ 0.9345846  -0.9182621   0.13441646  0.7389804 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1510. State = [[-0.19986884 -0.02927789  0.2431679   1.        ]]. Action = [[ 0.12563205  0.613448   -0.6115254   0.8108711 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1511. State = [[-0.18727024 -0.02264627  0.23758908  1.        ]]. Action = [[0.55413187 0.14850843 0.16343582 0.8886187 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1512. State = [[-0.17522518 -0.0192952   0.23703104  1.        ]]. Action = [[ 0.7135453   0.7686639  -0.43457752  0.46729612]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 1513. State = [[-0.174877   -0.02715253  0.24043673  1.        ]]. Action = [[-0.20095176 -0.6203221   0.5079      0.7935804 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1514. State = [[-0.17447852 -0.05105694  0.2558449   1.        ]]. Action = [[ 0.02534127 -0.89485765  0.5533091   0.1329962 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1515. State = [[-0.17855182 -0.06138955  0.26105785  1.        ]]. Action = [[-0.5331464   0.6123477  -0.8559786   0.79693794]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1516. State = [[-0.1885812  -0.05964216  0.26318213  1.        ]]. Action = [[-0.49466747 -0.19000363  0.81078863  0.6533685 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1517. State = [[-0.19316284 -0.05056467  0.27669206  1.        ]]. Action = [[0.3552326  0.55476284 0.3454666  0.7312244 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1518. State = [[-0.1943857  -0.02759767  0.2893497   1.        ]]. Action = [[-0.0167383  0.8311436  0.6390033  0.6603178]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1519. State = [[-0.20287867 -0.01773949  0.30462787  1.        ]]. Action = [[-0.8704971  -0.4462213  -0.05551952  0.8913858 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1520. State = [[-0.22847359 -0.02544148  0.30517104  1.        ]]. Action = [[-0.9275786  -0.17853415 -0.46992552  0.70795727]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1521. State = [[-0.25403777 -0.04069078  0.2915065   1.        ]]. Action = [[-0.19532204 -0.6994672  -0.95517725  0.6127753 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1522. State = [[-0.2665194  -0.06789117  0.267205    1.        ]]. Action = [[ 0.21597266 -0.89865506 -0.893442    0.6271063 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1523. State = [[-0.26308164 -0.0901075   0.24469393  1.        ]]. Action = [[-0.927521  -0.9248828 -0.9742104  0.7792523]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 1524. State = [[-0.2613711  -0.09360794  0.24293771  1.        ]]. Action = [[-0.52381104  0.9695623  -0.57407534  0.46850812]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 1525. State = [[-0.2593207  -0.08555482  0.2350267   1.        ]]. Action = [[ 0.1399188   0.65408623 -0.4808222   0.7899593 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1526. State = [[-0.2556231  -0.07636335  0.22584285  1.        ]]. Action = [[-0.58648163  0.0528717  -0.64465195  0.8802403 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 1527. State = [[-0.2540186  -0.07271087  0.2238029   1.        ]]. Action = [[-0.8508632  -0.93277246  0.8705127   0.3558185 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 1528. State = [[-0.25383228 -0.07205533  0.22351037  1.        ]]. Action = [[-0.61808014  0.93143845  0.93969834  0.9559151 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 1529. State = [[-0.25379857 -0.07187921  0.22344294  1.        ]]. Action = [[-0.6316475   0.55504036 -0.7838931   0.58378196]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 1530. State = [[-0.2537761  -0.0717615   0.22339791  1.        ]]. Action = [[-0.6643633  -0.17700309  0.7853304   0.8045435 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 1531. State = [[-0.24775754 -0.075139    0.21476619  1.        ]]. Action = [[ 0.5473664  -0.1597842  -0.7006281   0.40493703]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1532. State = [[-0.22960198 -0.06307581  0.19767022  1.        ]]. Action = [[0.7724315  0.8611717  0.26618016 0.73773694]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1533. State = [[-0.21730919 -0.03534314  0.18508     1.        ]]. Action = [[ 0.33693933  0.8223436  -0.9189837   0.817919  ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1534. State = [[-0.21060322 -0.00154616  0.16330293  1.        ]]. Action = [[-0.4376619   0.9508414   0.23136687  0.4313259 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1535. State = [[-0.22023456  0.00768324  0.17104565  1.        ]]. Action = [[-0.7674832  -0.75386274  0.7774147   0.82550883]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1536. State = [[-0.22579248 -0.01340261  0.18193167  1.        ]]. Action = [[ 0.76346016 -0.85575426 -0.3831538   0.77047825]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1537. State = [[-0.2137051  -0.03512457  0.18465732  1.        ]]. Action = [[ 0.46827185 -0.41545832  0.42118454  0.82589555]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1538. State = [[-0.21057959 -0.036062    0.19177817  1.        ]]. Action = [[-0.6244434   0.55300903  0.43042958  0.61638486]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1539. State = [[-0.21457507 -0.03163345  0.20868886  1.        ]]. Action = [[ 0.1749711  -0.1138947   0.66995645  0.674361  ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1540. State = [[-0.22436512 -0.04770181  0.21827295  1.        ]]. Action = [[-0.5761053 -0.9185185 -0.54109    0.8297204]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1541. State = [[-0.24142013 -0.06293064  0.21142241  1.        ]]. Action = [[-0.7293011   0.11458194 -0.7364483   0.8932886 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1542. State = [[-0.25047323 -0.06284382  0.21018083  1.        ]]. Action = [[ 0.4735911  -0.19244194  0.72740126  0.60401237]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1543. State = [[-0.25106594 -0.07104561  0.21638024  1.        ]]. Action = [[-0.36442333 -0.6308644   0.08993161  0.549916  ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1544. State = [[-0.25532922 -0.0729054   0.21498764  1.        ]]. Action = [[-0.02986026  0.7122903  -0.66744274  0.58703125]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1545. State = [[-0.25594565 -0.06011375  0.21818131  1.        ]]. Action = [[0.3199352  0.06711936 0.87609196 0.80123925]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1546. State = [[-0.25249872 -0.06207782  0.22167656  1.        ]]. Action = [[-0.4586534  -0.20660603  0.19179845  0.792508  ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 1547. State = [[-0.24786109 -0.05961352  0.23029935  1.        ]]. Action = [[0.21648788 0.16774249 0.79614353 0.763185  ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1548. State = [[-0.23612204 -0.05070015  0.2407203   1.        ]]. Action = [[ 0.7180338   0.71267414 -0.62240833  0.73585844]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1549. State = [[-0.22876541 -0.03317931  0.23252149  1.        ]]. Action = [[-0.5730724   0.38564503 -0.30484116  0.64178586]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1550. State = [[-0.2253242  -0.03730108  0.23610687  1.        ]]. Action = [[ 0.732131   -0.8867259   0.59931993  0.88667035]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1551. State = [[-0.22674929 -0.06114726  0.24170993  1.        ]]. Action = [[-0.8641779  -0.92165583 -0.26034486  0.708099  ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1552. State = [[-0.2427715 -0.0858169  0.2422147  1.       ]]. Action = [[-0.3974377  -0.27596933  0.22138548  0.91059804]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1553. State = [[-0.24637142 -0.08588186  0.23909454  1.        ]]. Action = [[ 0.36491752  0.7384691  -0.61219186  0.7197614 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1554. State = [[-0.2354159  -0.06566864  0.23731756  1.        ]]. Action = [[0.5751066  0.6109625  0.64321065 0.8349607 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1555. State = [[-0.22113483 -0.0422965   0.2358165   1.        ]]. Action = [[ 0.5772009   0.72388065 -0.6776052   0.9097192 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1556. State = [[-0.21298659 -0.01751884  0.21490295  1.        ]]. Action = [[-0.5205849   0.5812577  -0.85656655  0.6461729 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1557. State = [[-0.22356984  0.00234307  0.18924932  1.        ]]. Action = [[-0.76668966  0.51329947 -0.65001625  0.871145  ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1558. State = [[-0.23481178  0.01177664  0.17315243  1.        ]]. Action = [[ 0.2566862  -0.37440127  0.01398742  0.70536613]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1559. State = [[-0.24009414  0.02848608  0.167691    1.        ]]. Action = [[-0.34679592  0.4112662  -0.57459044  0.48959172]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1560. State = [[-0.24386218  0.03810281  0.16731706  1.        ]]. Action = [[ 0.46622932 -0.5003637   0.4886148   0.3309393 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1561. State = [[-0.24101165  0.03363283  0.16415139  1.        ]]. Action = [[ 0.05754495  0.03744519 -0.75772053  0.8117957 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1562. State = [[-0.24295479  0.03096424  0.14526647  1.        ]]. Action = [[-0.42679417 -0.9120519  -0.92283535  0.6981292 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1563. State = [[-0.2588984   0.06582741  0.11422825  1.        ]]. Action = [[-0.23066366  0.5260353  -0.11887807  0.6378503 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1564. State = [[-0.25634864  0.06941012  0.10680474  1.        ]]. Action = [[ 0.11704195 -0.24508995  0.6749089   0.5202799 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1565. State = [[-0.25268573  0.07237183  0.10474686  1.        ]]. Action = [[ 0.23995054  0.45432055 -0.8387161   0.8620372 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1566. State = [[-0.24619791  0.07851661  0.09428883  1.        ]]. Action = [[-7.7986252e-01  8.8523364e-01 -8.6343288e-04  6.6913939e-01]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1567. State = [[-0.24330793  0.0867513   0.08723202  1.        ]]. Action = [[ 0.37882364  0.48119366 -0.81448686  0.701561  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1568. State = [[-0.23081134  0.10603164  0.07396347  1.        ]]. Action = [[0.1131779  0.42895293 0.7857075  0.7333605 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1569. State = [[-0.2287748   0.12170131  0.09066278  1.        ]]. Action = [[-0.16890371  0.21964335  0.92954564  0.88923323]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1570. State = [[-0.23303027  0.13176404  0.11136865  1.        ]]. Action = [[-0.24595547  0.17317462  0.06311035  0.80525875]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1571. State = [[-0.2267218   0.13909586  0.12379758  1.        ]]. Action = [[0.756446   0.2388699  0.6154132  0.75997114]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1572. State = [[-0.21515709  0.1372122   0.14568667  1.        ]]. Action = [[-0.44244862 -0.48796773  0.55479574  0.8382468 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1573. State = [[-0.21355397  0.11738445  0.16419664  1.        ]]. Action = [[ 0.03094339 -0.8371816   0.29747784  0.6221539 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1574. State = [[-0.2159621   0.11437949  0.17038704  1.        ]]. Action = [[-0.06112128  0.7814561  -0.35527557  0.6489246 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1575. State = [[-0.22469026  0.11091413  0.17120653  1.        ]]. Action = [[-0.6431763  -0.8808831   0.32955968  0.76518714]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1576. State = [[-0.24245739  0.10811032  0.17751153  1.        ]]. Action = [[-0.37707472  0.587531    0.2566358   0.842927  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1577. State = [[-0.2569529   0.12686706  0.17670567  1.        ]]. Action = [[-0.20871627  0.7068399  -0.75159067  0.74777913]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1578. State = [[-0.26110062  0.12702963  0.1728659   1.        ]]. Action = [[ 0.04054761 -0.902181    0.21123171  0.68526447]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1579. State = [[-0.25915486  0.11645359  0.17444533  1.        ]]. Action = [[-0.57659614 -0.74389744  0.60651815  0.79742193]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1580. State = [[-0.25921783  0.11550077  0.17449427  1.        ]]. Action = [[-0.64257276 -0.9430109  -0.09527439  0.73934174]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 1581. State = [[-0.26267296  0.12442427  0.17112443  1.        ]]. Action = [[-0.06560576  0.7156575  -0.3716877   0.7369826 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1582. State = [[-0.26651615  0.1313984   0.1649571   1.        ]]. Action = [[-0.37315702 -0.61186755  0.7872162   0.6695249 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1583. State = [[-0.26702344  0.13295768  0.16461882  1.        ]]. Action = [[-0.6422987  -0.72256535  0.09122372  0.701005  ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1584. State = [[-0.2580278  -0.1731626   0.11436503  1.        ]]. Action = [[ 0.915383    0.9547851  -0.61272645 -0.02271837]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1585. State = [[-0.2536043  -0.20406516  0.09557658  1.        ]]. Action = [[ 0.48454845 -0.87702787 -0.34551632  0.56024075]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1586. State = [[-0.24473184 -0.21595092  0.08869585  1.        ]]. Action = [[-0.13624555  0.606447    0.45772147  0.7741878 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1587. State = [[-0.2438938  -0.20610842  0.09242129  1.        ]]. Action = [[-0.20915794  0.49573016  0.30545592  0.7837883 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1588. State = [[-0.2447396 -0.2022141  0.0970691  1.       ]]. Action = [[ 0.48099828 -0.5699681  -0.3870908   0.835088  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1589. State = [[-0.23409593 -0.21452057  0.10205072  1.        ]]. Action = [[ 0.46923637 -0.60220134  0.6285293   0.6558436 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1590. State = [[-0.22835241 -0.22932437  0.10494798  1.        ]]. Action = [[-0.3826332  -0.17729121 -0.4279428   0.743045  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1591. State = [[-0.23009206 -0.2409237   0.10544141  1.        ]]. Action = [[ 0.1638869  -0.39685786  0.36805117  0.77853143]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1592. State = [[-0.22067328 -0.23616223  0.11183397  1.        ]]. Action = [[0.681576   0.88848186 0.33705974 0.7718446 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1593. State = [[-0.19997226 -0.21166997  0.11058068  1.        ]]. Action = [[ 0.73029613  0.79784596 -0.89273435  0.7348635 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1594. State = [[-0.188654   -0.18801695  0.09102717  1.        ]]. Action = [[-0.43183565  0.4588766  -0.9409306   0.6821115 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1595. State = [[-0.18610874 -0.17608562  0.07185482  1.        ]]. Action = [[0.4499979  0.02760434 0.14188004 0.76320314]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1596. State = [[-0.18102975 -0.17604765  0.07009176  1.        ]]. Action = [[-0.2212823  -0.15398222  0.0545702   0.8067622 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1597. State = [[-0.18137802 -0.17618012  0.07003834  1.        ]]. Action = [[ 0.7350457   0.6922064  -0.64259374  0.8798245 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 1598. State = [[-0.17699608 -0.18747137  0.0711173   1.        ]]. Action = [[ 0.39541173 -0.76410836  0.11041462  0.94294465]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1599. State = [[-0.16559583 -0.20887509  0.07970329  1.        ]]. Action = [[ 0.36703944 -0.62970215  0.8093178   0.7075237 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1600. State = [[-0.15539218 -0.2256566   0.09204314  1.        ]]. Action = [[0.75509906 0.86778927 0.17877388 0.70937634]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 1601. State = [[-0.15937775 -0.23407562  0.08619874  1.        ]]. Action = [[-0.45643532 -0.34572434 -0.8163066   0.6190152 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1602. State = [[-0.16505119 -0.22683494  0.07097378  1.        ]]. Action = [[ 0.02419508  0.9373437  -0.619643    0.8473668 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1603. State = [[-0.17795551 -0.20326121  0.04839336  1.        ]]. Action = [[-0.9252171   0.83694005 -0.43959463  0.8581339 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1604. State = [[-0.19452918 -0.17453161  0.04293197  1.        ]]. Action = [[-0.23457873  0.7997043   0.7552295   0.8192816 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1605. State = [[-0.20304558 -0.16648588  0.06011893  1.        ]]. Action = [[-0.3639977 -0.6235469  0.8568319  0.7130728]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1606. State = [[-0.22436507 -0.16093627  0.07807007  1.        ]]. Action = [[-0.79255867  0.6823602   0.01008582  0.9317038 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1607. State = [[-0.24630688 -0.14075296  0.09416937  1.        ]]. Action = [[-0.5345258  0.6563277  0.9398236  0.9029895]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1608. State = [[-0.2643054  -0.11875409  0.1185819   1.        ]]. Action = [[-0.06498164  0.43130326  0.370057    0.6218829 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1609. State = [[-0.26824737 -0.10054994  0.12319097  1.        ]]. Action = [[ 0.2998445   0.47005367 -0.83815247  0.8495867 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1610. State = [[-0.26809126 -0.09022272  0.11664114  1.        ]]. Action = [[-0.4085039   0.80281377  0.7755692   0.75009465]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 1611. State = [[-0.26541266 -0.08861177  0.11049318  1.        ]]. Action = [[ 0.31583667 -0.14319313 -0.54175925  0.8268626 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1612. State = [[-0.25410682 -0.08020248  0.10637078  1.        ]]. Action = [[0.48669672 0.63681054 0.9715425  0.78466463]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1613. State = [[-0.24428312 -0.06748943  0.11630902  1.        ]]. Action = [[-0.45193398 -0.4744668   0.98030114  0.8119371 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 1614. State = [[-0.24393134 -0.06605423  0.10979588  1.        ]]. Action = [[ 0.06054461 -0.00166523 -0.95390534  0.92708194]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1615. State = [[-0.2401874  -0.06627588  0.09709886  1.        ]]. Action = [[-0.9223226   0.03087139  0.7445189   0.9023075 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 1616. State = [[-0.2338902  -0.05653898  0.10292228  1.        ]]. Action = [[0.36938918 0.61938894 0.893414   0.80708027]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1617. State = [[-0.21230856 -0.04486777  0.1261603   1.        ]]. Action = [[ 0.97088253 -0.1810537   0.79949     0.82848656]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1618. State = [[-0.19187205 -0.04471651  0.1384082   1.        ]]. Action = [[ 0.13821447  0.09305918 -0.62359434  0.55945694]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1619. State = [[-0.18651126 -0.04460683  0.13409051  1.        ]]. Action = [[ 0.86077285 -0.16975051  0.71563196  0.88118136]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 1620. State = [[-0.18699542 -0.04550112  0.13928221  1.        ]]. Action = [[-0.39929008 -0.09213883  0.62944245  0.37747478]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1621. State = [[-0.18733317 -0.04606409  0.14559208  1.        ]]. Action = [[ 0.44931543 -0.6319293   0.9772086   0.90658474]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 1622. State = [[-0.18761621 -0.0463197   0.14578247  1.        ]]. Action = [[0.8663807  0.13676345 0.8314686  0.42529142]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: No entry zone
Current timestep = 1623. State = [[-0.19254789 -0.05514515  0.15748125  1.        ]]. Action = [[-0.5292848 -0.5115667  0.9011018  0.8416381]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1624. State = [[-0.21071574 -0.07567998  0.17434524  1.        ]]. Action = [[-0.5508976  -0.65656275 -0.2987315   0.504473  ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1625. State = [[-0.21932088 -0.08387997  0.16799518  1.        ]]. Action = [[ 0.058761    0.3730607  -0.7212042   0.47736514]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1626. State = [[-0.21579905 -0.09101858  0.16100505  1.        ]]. Action = [[ 0.6593096  -0.7917069  -0.01590884  0.7103994 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1627. State = [[-0.20284837 -0.09014848  0.14740483  1.        ]]. Action = [[ 0.65667796  0.7683022  -0.9024739   0.88352585]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1628. State = [[-0.19410245 -0.0826875   0.13080467  1.        ]]. Action = [[-0.8133001  -0.05716103  0.4789288   0.86304736]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1629. State = [[-0.19595145 -0.08260322  0.13185197  1.        ]]. Action = [[ 0.84658575  0.14091444 -0.02940148  0.7683635 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 1630. State = [[-0.2043007  -0.08153063  0.1394211   1.        ]]. Action = [[-0.63680154  0.07702386  0.70245934  0.77215385]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1631. State = [[-0.2170476  -0.08073672  0.15043005  1.        ]]. Action = [[ 0.3740474  -0.19349527 -0.40720809  0.73659515]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1632. State = [[-0.20742446 -0.06958512  0.1531278   1.        ]]. Action = [[0.74279356 0.8033838  0.30993855 0.73803496]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1633. State = [[-0.20760018 -0.06301755  0.15879688  1.        ]]. Action = [[-0.78957915 -0.3252477   0.40078545  0.4740274 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1634. State = [[-0.21177751 -0.06652127  0.1640382   1.        ]]. Action = [[ 0.89460564 -0.6662628  -0.15660882  0.7417846 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 1635. State = [[-0.2055181  -0.06802773  0.16679163  1.        ]]. Action = [[ 0.8288076  -0.11813217  0.02510977  0.65567255]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1636. State = [[-0.20321132 -0.06691873  0.16080655  1.        ]]. Action = [[-0.21667808  0.20635152 -0.9401668   0.84204936]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1637. State = [[-0.20499513 -0.07853322  0.1376291   1.        ]]. Action = [[-0.12627184 -0.92632765 -0.723294    0.56594133]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1638. State = [[-0.20439607 -0.08628097  0.11346804  1.        ]]. Action = [[ 0.26058733  0.39553237 -0.47043812  0.60164   ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1639. State = [[-0.20158872 -0.06938846  0.09036212  1.        ]]. Action = [[ 0.02629328  0.89064384 -0.8849309   0.5189905 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1640. State = [[-0.19115461 -0.05890541  0.07625494  1.        ]]. Action = [[ 0.56360984 -0.38673544  0.70801425  0.88626146]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1641. State = [[-0.18089865 -0.06094928  0.07970001  1.        ]]. Action = [[ 0.8394823  -0.68411875  0.8447317   0.80130315]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 1642. State = [[-0.1795543  -0.06132492  0.08010432  1.        ]]. Action = [[ 0.25210655  0.35409272 -0.25066692  0.67890537]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 1643. State = [[-0.17945209 -0.06110892  0.07366713  1.        ]]. Action = [[ 0.11796629  0.04457712 -0.8196132   0.7059462 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1644. State = [[-0.17613813 -0.06033128  0.06263053  1.        ]]. Action = [[-0.20928091  0.1254772   0.18835235  0.9236431 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1645. State = [[-0.17748696 -0.05556498  0.07150071  1.        ]]. Action = [[-0.4349749   0.2127155   0.91943645  0.8121493 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1646. State = [[-0.18637682 -0.06123995  0.09451207  1.        ]]. Action = [[-0.4790343  -0.6828765   0.8204174   0.75513625]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1647. State = [[-0.20506585 -0.06887489  0.10797586  1.        ]]. Action = [[-0.39760137  0.2184856  -0.7083451   0.6202965 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1648. State = [[-0.22024964 -0.07922292  0.10683861  1.        ]]. Action = [[-0.5043358 -0.7345087  0.3807044  0.8354366]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1649. State = [[-0.23634174 -0.09282191  0.11312581  1.        ]]. Action = [[-0.75468045 -0.07394814  0.2847011   0.6844225 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1650. State = [[-0.25199813 -0.10952033  0.12308905  1.        ]]. Action = [[ 0.31189275 -0.8660173   0.1181308   0.22589207]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1651. State = [[-0.24982382 -0.12548982  0.12636118  1.        ]]. Action = [[-0.810522   -0.3769909  -0.01553792  0.70803213]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 1652. State = [[-0.24989125 -0.12782295  0.12645034  1.        ]]. Action = [[-0.971938   -0.4334513  -0.22767651  0.48195767]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 1653. State = [[-0.24887748 -0.1215295   0.12600465  1.        ]]. Action = [[ 0.3056965   0.548751   -0.06943774  0.73629475]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1654. State = [[-0.23621675 -0.11871833  0.13224542  1.        ]]. Action = [[ 0.78445697 -0.36972117  0.62657523  0.84903026]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1655. State = [[-0.23034951 -0.13680379  0.15390411  1.        ]]. Action = [[-0.6211698  -0.98925203  0.8813946   0.85866916]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1656. State = [[-0.23153798 -0.16995771  0.1868634   1.        ]]. Action = [[ 0.48255074 -0.97072244  0.7738049   0.6274338 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1657. State = [[-0.22398177 -0.18097907  0.19820952  1.        ]]. Action = [[ 0.24936974  0.8201585  -0.9713647   0.8709854 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1658. State = [[-0.2244608  -0.15890975  0.18991409  1.        ]]. Action = [[-0.68565536  0.95182633  0.45511293  0.72828865]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1659. State = [[-0.23840278 -0.13370366  0.19940205  1.        ]]. Action = [[-0.6839582   0.38900483  0.5886078   0.3113836 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1660. State = [[-0.24611533 -0.10891581  0.20452882  1.        ]]. Action = [[ 0.8472073   0.88983536 -0.82075846  0.5060713 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1661. State = [[-0.23593347 -0.07557042  0.1845462   1.        ]]. Action = [[ 0.445014    0.97488093 -0.9685871   0.901495  ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1662. State = [[-0.21451686 -0.04215295  0.16498786  1.        ]]. Action = [[0.8418777  0.6113956  0.3489089  0.72969365]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1663. State = [[-0.18723321 -0.02481697  0.17398621  1.        ]]. Action = [[0.6905706  0.04486978 0.7491492  0.7369962 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1664. State = [[-0.168596   -0.02078947  0.18594053  1.        ]]. Action = [[0.75734484 0.14307356 0.97373533 0.02529263]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 1665. State = [[-0.17573673 -0.01913207  0.17839012  1.        ]]. Action = [[-0.8807344   0.16811013 -0.88343173  0.8142948 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1666. State = [[-0.19386786 -0.02182816  0.17132393  1.        ]]. Action = [[-0.87640786 -0.32540536  0.5023396   0.4075272 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1667. State = [[-0.22277457 -0.0252472   0.16857679  1.        ]]. Action = [[-0.8070342   0.06895852 -0.57912946  0.5996475 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1668. State = [[-0.2553838  -0.01282755  0.1493029   1.        ]]. Action = [[-0.77942234  0.85412693 -0.8069563   0.6516955 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1669. State = [[-0.2731598   0.00857421  0.14336258  1.        ]]. Action = [[0.09556937 0.20489621 0.9673716  0.44638395]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1670. State = [[-0.26748827  0.00479529  0.1645809   1.        ]]. Action = [[ 0.7432828  -0.87106603  0.9270253   0.0827626 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1671. State = [[-0.26167163 -0.00884354  0.18429723  1.        ]]. Action = [[-0.631714   -0.5420611   0.9701518   0.71387446]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 1672. State = [[-0.25962853 -0.01282955  0.18679726  1.        ]]. Action = [[-0.7712982   0.15856266  0.34705055  0.45590556]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 1673. State = [[-2.5915006e-01  1.0988055e-05  1.9973880e-01  1.0000000e+00]]. Action = [[-0.07607859  0.7938762   0.9522357   0.86333346]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1674. State = [[-0.26108837  0.01388837  0.2208707   1.        ]]. Action = [[-0.64267606 -0.21638632 -0.4693973   0.54704165]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 1675. State = [[-0.26050857  0.01608391  0.22293617  1.        ]]. Action = [[-0.363163    0.09451294  0.23264027  0.5764499 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 1676. State = [[-0.25882277  0.00641908  0.21723643  1.        ]]. Action = [[ 0.2152772  -0.6283528  -0.83930063  0.8791008 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1677. State = [[-0.24681458  0.00741557  0.20224686  1.        ]]. Action = [[ 0.7649064  0.6610987 -0.5686052  0.7990656]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1678. State = [[-0.22108357  0.01315931  0.19391347  1.        ]]. Action = [[ 0.53842497 -0.21760833  0.48451877  0.5720804 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1679. State = [[-0.20999773  0.02616804  0.20339541  1.        ]]. Action = [[-0.06643605  0.84186816  0.54623604  0.708143  ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1680. State = [[-0.20869823  0.03730588  0.20665617  1.        ]]. Action = [[-0.29025996 -0.20546687 -0.4342901   0.34577775]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1681. State = [[-0.21008337  0.03868925  0.21152486  1.        ]]. Action = [[0.10042417 0.12101531 0.7172916  0.768332  ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1682. State = [[-0.21077758  0.02837835  0.2159848   1.        ]]. Action = [[ 0.10816813 -0.69990665 -0.59772694  0.8673501 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1683. State = [[-0.20338346  0.00563844  0.2163652   1.        ]]. Action = [[ 0.6275902  -0.81902385  0.1903739   0.6633698 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1684. State = [[-0.18407296 -0.01613588  0.2264341   1.        ]]. Action = [[ 0.5829263  -0.36117935  0.704134    0.7252097 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1685. State = [[-0.1720283  -0.03671516  0.23852879  1.        ]]. Action = [[-0.2171874  -0.57781845 -0.22730315  0.6208043 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1686. State = [[-0.26378998  0.10203852  0.11549298  1.        ]]. Action = [[-0.9064339  -0.59995776 -0.59776706  0.53092206]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1687. State = [[-0.26072544  0.11558674  0.10140926  1.        ]]. Action = [[-0.30776656 -0.85905087 -0.63521725  0.5136709 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1688. State = [[-0.2586922   0.12537135  0.10446119  1.        ]]. Action = [[0.17744958 0.5667994  0.48485172 0.6841812 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1689. State = [[-0.25493863  0.13365759  0.10551031  1.        ]]. Action = [[ 0.13907623 -0.18214428 -0.35254782  0.26493907]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1690. State = [[-0.24124639  0.13850014  0.11245033  1.        ]]. Action = [[0.8376701  0.42038882 0.8006729  0.6085639 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1691. State = [[-0.21733944  0.14323024  0.11568373  1.        ]]. Action = [[ 0.4789927  -0.22962141 -0.7748842   0.582572  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1692. State = [[-0.2112329   0.14701162  0.11280274  1.        ]]. Action = [[-0.6541077   0.20572567  0.858052    0.71035147]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1693. State = [[-0.214753    0.13900873  0.11700983  1.        ]]. Action = [[ 0.26960945 -0.61899984 -0.62784564  0.7001848 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1694. State = [[-0.22494482  0.14196071  0.1116662   1.        ]]. Action = [[-0.9506999   0.7573798  -0.12912703  0.17167008]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1695. State = [[-0.2376366   0.15457608  0.10876196  1.        ]]. Action = [[-0.8971556  -0.9144923   0.74399936  0.64361143]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 1696. State = [[-0.232104    0.15925065  0.10036656  1.        ]]. Action = [[ 0.9219723  0.298234  -0.8371112  0.6897756]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1697. State = [[-0.20880064  0.15277243  0.08014419  1.        ]]. Action = [[ 0.6358553  -0.7384515  -0.08250087  0.7041378 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1698. State = [[-0.19453591  0.1338026   0.07680922  1.        ]]. Action = [[-0.15906644 -0.6194669   0.10794711  0.5800983 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1699. State = [[-0.19326033  0.1199765   0.08484506  1.        ]]. Action = [[-0.38173628 -0.16081786  0.81566906  0.93788505]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1700. State = [[-0.19066387  0.10394882  0.10693155  1.        ]]. Action = [[ 0.41957068 -0.5975658   0.80113745  0.55623174]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1701. State = [[-0.18665501  0.09364877  0.12517422  1.        ]]. Action = [[ 0.90101886 -0.3046975  -0.12870425  0.5697777 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 1702. State = [[-0.18569624  0.09740664  0.14054172  1.        ]]. Action = [[-0.05795687  0.3574046   0.93591094  0.87271   ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1703. State = [[-0.18429261  0.10855946  0.17479554  1.        ]]. Action = [[0.1833291  0.46191216 0.98376846 0.85117245]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1704. State = [[-0.18346375  0.11540276  0.1977776   1.        ]]. Action = [[ 0.47565413 -0.45836347  0.5854708   0.73040485]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 1705. State = [[-0.18839537  0.11177617  0.19303647  1.        ]]. Action = [[-0.4854846  -0.43988693 -0.9426359   0.20286655]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1706. State = [[-0.20269513  0.10546643  0.18590194  1.        ]]. Action = [[-0.821735   -0.09774685  0.37353408  0.82369494]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1707. State = [[-0.21786746  0.10166423  0.18376634  1.        ]]. Action = [[-0.0513078   0.02757502 -0.3864106   0.89799917]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1708. State = [[-0.22652765  0.09705599  0.17655663  1.        ]]. Action = [[-0.2567652  -0.3249541  -0.24709308  0.8496611 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1709. State = [[-0.23021448  0.10048661  0.17174244  1.        ]]. Action = [[0.67385626 0.65197444 0.02603424 0.8689772 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1710. State = [[-0.22893274  0.11071841  0.17558405  1.        ]]. Action = [[-0.68779814  0.1168952   0.66657877  0.765105  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1711. State = [[-0.23957853  0.10991392  0.17321612  1.        ]]. Action = [[-0.17613828 -0.4688145  -0.9449129   0.6658937 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1712. State = [[-0.24099     0.10142237  0.16173314  1.        ]]. Action = [[ 0.410213   -0.18419737 -0.03552073  0.6251688 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1713. State = [[-0.2375602   0.09661575  0.16099192  1.        ]]. Action = [[-0.8477597  -0.26517856  0.68573177  0.66410625]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1714. State = [[-0.23075584  0.08863392  0.1682545   1.        ]]. Action = [[ 0.36549938 -0.4072804   0.82523704  0.865957  ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1715. State = [[-0.22076315  0.0842682   0.18057771  1.        ]]. Action = [[0.36050773 0.19052517 0.31683457 0.8527875 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1716. State = [[-0.20535678  0.08101564  0.19543916  1.        ]]. Action = [[ 0.54184306 -0.24974489  0.62077475  0.59826386]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1717. State = [[-0.19383253  0.08818913  0.22161132  1.        ]]. Action = [[-0.09657621  0.6764587   0.70785713  0.6592262 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1718. State = [[-0.18672752  0.10056274  0.23794919  1.        ]]. Action = [[ 0.35077655  0.31109786 -0.08927166  0.645692  ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1719. State = [[-0.18770587  0.09649067  0.23906581  1.        ]]. Action = [[-0.7242266  -0.75073594 -0.2706117   0.8165271 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1720. State = [[-0.18464164  0.07746806  0.23816077  1.        ]]. Action = [[ 0.9376154  -0.5184644   0.03525627  0.48820066]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1721. State = [[-0.1767538   0.07696127  0.23707867  1.        ]]. Action = [[ 0.07682514  0.7339649  -0.13129854  0.5713792 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1722. State = [[-0.17360577  0.08364348  0.23529322  1.        ]]. Action = [[-0.18548626 -0.07230598  0.10387313  0.8372276 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1723. State = [[-0.17393862  0.08414468  0.23523392  1.        ]]. Action = [[ 0.36708832  0.5080303  -0.73603475  0.89582944]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 1724. State = [[-0.18120389  0.09261399  0.22780855  1.        ]]. Action = [[-0.6492597   0.42263186 -0.82748145  0.74311876]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1725. State = [[-0.19695969  0.09233818  0.21437281  1.        ]]. Action = [[-0.8987563  -0.69202876  0.0025543   0.75359213]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1726. State = [[-0.22450443  0.09278926  0.21520588  1.        ]]. Action = [[-0.8643144   0.58937883  0.6913593   0.80538905]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1727. State = [[-0.24729085  0.11247642  0.22439763  1.        ]]. Action = [[ 0.22530699  0.79701304 -0.4182524   0.7148342 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1728. State = [[-0.2443405   0.12517817  0.23159218  1.        ]]. Action = [[ 0.5007349  -0.11852884  0.8153095   0.4720565 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1729. State = [[-0.23370041  0.12195276  0.23285717  1.        ]]. Action = [[ 0.4765091  -0.30601084 -0.5348032   0.6342399 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1730. State = [[-0.22759469  0.1098244   0.23948354  1.        ]]. Action = [[-0.41434062 -0.5612064   0.9406259   0.6362393 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1731. State = [[-0.23850037  0.11169314  0.25534177  1.        ]]. Action = [[-0.7692356   0.8444215   0.10329652  0.72723186]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1732. State = [[-0.2440255   0.11053712  0.26215172  1.        ]]. Action = [[ 0.74265766 -0.86778706 -0.27755475  0.56638455]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1733. State = [[-0.23974212  0.0993917   0.26106283  1.        ]]. Action = [[-0.6828766  -0.03437603 -0.83611673  0.6949365 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 1734. State = [[-0.24356036  0.08711112  0.25890073  1.        ]]. Action = [[-0.549082  -0.6059129 -0.2543978  0.9367311]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1735. State = [[-0.24757645  0.07642853  0.2573786   1.        ]]. Action = [[-0.75742126  0.01203752 -0.3310107   0.8626741 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 1736. State = [[-0.25289574  0.08255249  0.25367153  1.        ]]. Action = [[-0.09595251  0.7029867  -0.21485436  0.78758264]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1737. State = [[-0.24776694  0.08437856  0.24849716  1.        ]]. Action = [[ 0.78455484 -0.5783417  -0.2529583   0.87982845]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1738. State = [[-0.2359085   0.07759459  0.23859186  1.        ]]. Action = [[-0.7621436   0.22729516 -0.46851718  0.8349478 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 1739. State = [[-0.22746962  0.06673246  0.23136379  1.        ]]. Action = [[ 0.4098692  -0.50564116 -0.69546497  0.74050975]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1740. State = [[-0.21032996  0.05092907  0.22431776  1.        ]]. Action = [[ 0.54994464 -0.33792424  0.93961763  0.63608396]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1741. State = [[-0.2087267   0.05779793  0.23606792  1.        ]]. Action = [[-0.86797476  0.8737421   0.260679    0.7337543 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1742. State = [[-0.21869662  0.07536027  0.23359749  1.        ]]. Action = [[ 0.16298318  0.324031   -0.72204196  0.79088306]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1743. State = [[-0.22158682  0.07640535  0.22845128  1.        ]]. Action = [[-0.2500111  -0.38410658 -0.01669687  0.847651  ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1744. State = [[-0.22557013  0.08072861  0.22041418  1.        ]]. Action = [[-0.16928113  0.53272307 -0.6048137   0.92256904]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1745. State = [[-0.23542593  0.09840176  0.20832776  1.        ]]. Action = [[-0.43722343  0.6200814  -0.0739364   0.56536794]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1746. State = [[-0.24490897  0.10091866  0.20999783  1.        ]]. Action = [[-0.41465175 -0.7486791   0.4997661   0.68920135]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1747. State = [[-0.24723633  0.08583319  0.20673811  1.        ]]. Action = [[ 0.64030683 -0.28259647 -0.8068669   0.7763163 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1748. State = [[-0.2413416   0.08059726  0.19513516  1.        ]]. Action = [[-0.78569794  0.49377584  0.36928785  0.8023555 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 1749. State = [[-0.23403326  0.07780701  0.1967013   1.        ]]. Action = [[ 0.65609336 -0.06961393  0.2658857   0.6232343 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1750. State = [[-0.21900563  0.08702886  0.20268817  1.        ]]. Action = [[0.6142225  0.80355394 0.27793336 0.66794133]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1751. State = [[-0.20810005  0.0919623   0.2009802   1.        ]]. Action = [[-0.02361465 -0.3945861  -0.2356832   0.3915007 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1752. State = [[-0.21111621  0.07810678  0.20017421  1.        ]]. Action = [[-0.61054975 -0.7750202   0.17440212  0.72082424]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1753. State = [[-0.21271765  0.07684465  0.2127356   1.        ]]. Action = [[0.11226118 0.6462445  0.90325284 0.57459736]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1754. State = [[-0.20846766  0.0808177   0.22143206  1.        ]]. Action = [[ 0.7496445 -0.0345307 -0.5187554  0.8495164]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1755. State = [[-0.19934164  0.08275465  0.21762024  1.        ]]. Action = [[ 0.825371    0.17833686 -0.8328012   0.71623003]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 1756. State = [[-0.20181505  0.08666395  0.22143093  1.        ]]. Action = [[-0.7731811   0.11826479  0.4913733   0.8022746 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1757. State = [[-0.21735732  0.08053724  0.23661593  1.        ]]. Action = [[-0.8658275  -0.5035652   0.53958166  0.46736848]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1758. State = [[-0.2361532   0.07786965  0.26259193  1.        ]]. Action = [[0.12030411 0.30382836 0.92710567 0.7752286 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1759. State = [[-0.23906991  0.0688296   0.2738621   1.        ]]. Action = [[ 0.28586948 -0.65233624 -0.7960226   0.79192185]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1760. State = [[-0.23936975  0.0711511   0.26656398  1.        ]]. Action = [[-0.227673    0.86237013 -0.06933695  0.4101826 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1761. State = [[-0.24490598  0.09378381  0.26662028  1.        ]]. Action = [[-0.14745808  0.75426054  0.31026292  0.696493  ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1762. State = [[-0.2514998   0.10974854  0.2678251   1.        ]]. Action = [[-0.5906302   0.90507364  0.49407816  0.74901175]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 1763. State = [[-0.2543695   0.12100283  0.26188192  1.        ]]. Action = [[-0.08004826  0.5731752  -0.6682032   0.83770084]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1764. State = [[-0.2549858   0.12360529  0.24735613  1.        ]]. Action = [[-0.05071318 -0.558625   -0.45037007  0.81188214]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1765. State = [[-0.24915883  0.13042328  0.24569726  1.        ]]. Action = [[0.47194993 0.80230594 0.4232086  0.5299921 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1766. State = [[-0.23312737  0.14537087  0.2537649   1.        ]]. Action = [[0.84099615 0.28009522 0.27137518 0.73088515]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1767. State = [[-0.21663949  0.15646224  0.25381085  1.        ]]. Action = [[-0.8304831   0.15931463 -0.15075856  0.63792515]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 1768. State = [[-0.21564062  0.16905569  0.2485273   1.        ]]. Action = [[ 0.05891037  0.8051156  -0.49906236  0.7785926 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1769. State = [[-0.20073892  0.17267738  0.24692233  1.        ]]. Action = [[ 0.5857525  -0.69465774  0.5674877   0.48068643]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1770. State = [[-0.18799789  0.16912025  0.2586353   1.        ]]. Action = [[0.38873518 0.18075347 0.16103828 0.5777986 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1771. State = [[-0.17176396  0.17938228  0.26522094  1.        ]]. Action = [[0.7502389  0.69450045 0.05602539 0.72525513]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1772. State = [[-0.16174409  0.19833778  0.25430086  1.        ]]. Action = [[-0.8192051   0.31242716 -0.9128915   0.7950976 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1773. State = [[-0.16419312  0.19682206  0.24418639  1.        ]]. Action = [[-0.00589859 -0.6925253   0.32439113  0.7086389 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1774. State = [[-0.16595253  0.1958861   0.23790514  1.        ]]. Action = [[-0.04536408  0.44456697 -0.8165736   0.66801023]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1775. State = [[-0.17526399  0.20837286  0.21972334  1.        ]]. Action = [[-0.4885522   0.6505227  -0.17438471  0.55863595]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1776. State = [[-0.19013312  0.21615462  0.2223591   1.        ]]. Action = [[-0.8484837  -0.51741374  0.5388837   0.82740164]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1777. State = [[-0.20045081  0.2188244   0.21906516  1.        ]]. Action = [[ 0.64773524  0.6063247  -0.7510638   0.80715764]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1778. State = [[-0.19279435  0.23326278  0.20092803  1.        ]]. Action = [[ 0.72601974  0.5919348  -0.8766726   0.85156715]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1779. State = [[-0.16991378  0.23545252  0.1814621   1.        ]]. Action = [[ 0.6401143  -0.76793444  0.5357336   0.7914672 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1780. State = [[-0.15564959  0.22293653  0.17940994  1.        ]]. Action = [[-0.00277245 -0.17805201 -0.9362631   0.8471019 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1781. State = [[-0.15095763  0.21007416  0.1583228   1.        ]]. Action = [[-0.41549832 -0.6207138  -0.05162287  0.69386077]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1782. State = [[-0.15462263  0.20720261  0.14779103  1.        ]]. Action = [[-0.22263598  0.43499398 -0.8764396   0.69027925]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1783. State = [[-0.1551954   0.21928824  0.12809938  1.        ]]. Action = [[0.48559463 0.6573751  0.25631797 0.7763163 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1784. State = [[-0.14440468  0.21726702  0.12651053  1.        ]]. Action = [[ 0.50409174 -0.9235476  -0.15652514  0.8108845 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1785. State = [[-0.13197124  0.19252071  0.13030224  1.        ]]. Action = [[-0.22814953 -0.79219633  0.8228431   0.854784  ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1786. State = [[-0.12890497  0.19259667  0.1519963   1.        ]]. Action = [[0.07191408 0.95581293 0.87398934 0.61587906]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1787. State = [[-0.13382716  0.2181025   0.17838818  1.        ]]. Action = [[-0.11116982  0.7742908   0.63095593  0.20868313]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1788. State = [[-0.2569672   0.1566416   0.09575123  1.        ]]. Action = [[-0.52937967 -0.78354913 -0.22338766  0.90170693]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1789. State = [[-0.2537654   0.17348373  0.08344962  1.        ]]. Action = [[-0.49646926  0.08324623  0.7251382   0.79396117]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1790. State = [[-0.24796467  0.16674733  0.0906813   1.        ]]. Action = [[ 0.1172998 -0.4892454  0.8991113  0.5451672]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1791. State = [[-0.24453029  0.15943877  0.0986357   1.        ]]. Action = [[-0.6192082   0.78641343 -0.62277734  0.4108925 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1792. State = [[-0.24016523  0.14387803  0.09717289  1.        ]]. Action = [[ 0.1130228  -0.9704859  -0.33150518  0.85425997]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1793. State = [[-0.22984742  0.11689351  0.10572031  1.        ]]. Action = [[ 0.49203396 -0.6469739   0.83153     0.5960065 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1794. State = [[-0.22812577  0.10863498  0.11774465  1.        ]]. Action = [[-0.5536766   0.35814488  0.03161693  0.90091324]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1795. State = [[-0.22327386  0.11926921  0.12348874  1.        ]]. Action = [[0.8426652  0.6221137  0.18796325 0.57442725]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1796. State = [[-0.2165215   0.12758598  0.12882046  1.        ]]. Action = [[-0.9413887  -0.58444405 -0.22334349  0.44346428]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 1797. State = [[-0.20644696  0.13076787  0.13775526  1.        ]]. Action = [[0.5848098  0.00859833 0.5933676  0.6178268 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1798. State = [[-0.19659899  0.13408445  0.14374287  1.        ]]. Action = [[-0.24991393  0.05087733 -0.5591884   0.6671717 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1799. State = [[-0.20805658  0.13061179  0.13589345  1.        ]]. Action = [[-0.91703236 -0.39031953 -0.45748723  0.54281545]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1800. State = [[-0.22582172  0.1256267   0.13587405  1.        ]]. Action = [[-0.47866786 -0.06508172  0.84515846  0.79153633]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1801. State = [[-0.24477154  0.12553917  0.13815486  1.        ]]. Action = [[-0.43757617  0.1807574  -0.81145525  0.79193854]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1802. State = [[-0.2516717   0.13688716  0.13761891  1.        ]]. Action = [[0.5649971  0.71696305 0.9445355  0.57777   ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1803. State = [[-0.24682724  0.1584361   0.14034984  1.        ]]. Action = [[ 0.41391575  0.63961804 -0.7806575   0.9485891 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1804. State = [[-0.22993386  0.16406842  0.12323407  1.        ]]. Action = [[ 0.71926284 -0.45125973 -0.80793816  0.7487304 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1805. State = [[-0.2004396   0.16940668  0.11794959  1.        ]]. Action = [[0.9591472  0.43157804 0.944453   0.72586346]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1806. State = [[-0.16906476  0.17720139  0.13273951  1.        ]]. Action = [[ 0.7343776  -0.01637262 -0.02664745  0.7191719 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1807. State = [[-0.1558625   0.17170677  0.13718836  1.        ]]. Action = [[-0.74296886 -0.53229976  0.2833426   0.6428863 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1808. State = [[-0.15664162  0.16238022  0.13955139  1.        ]]. Action = [[-0.20583558 -0.45933592 -0.55131996  0.50399625]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 1809. State = [[-0.14740057  0.16094728  0.15124089  1.        ]]. Action = [[0.79004383 0.10380173 0.8092364  0.85858536]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1810. State = [[-0.1425614   0.17274643  0.16310915  1.        ]]. Action = [[-0.22910249  0.6582675  -0.39880842  0.7608819 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1811. State = [[-0.14646055  0.1797196   0.17118002  1.        ]]. Action = [[-0.32380295 -0.25830233  0.7627715   0.7999548 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1812. State = [[-0.1472007   0.18032034  0.18164428  1.        ]]. Action = [[ 0.30108213 -0.5486689   0.12383902  0.8400912 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 1813. State = [[-0.14731064  0.18030587  0.18163942  1.        ]]. Action = [[ 0.5239928  -0.6397158   0.97421527  0.62113404]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 1814. State = [[-0.15307285  0.17733063  0.19272602  1.        ]]. Action = [[-0.7800186  -0.29815185  0.6598692   0.80356336]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1815. State = [[-0.1730664   0.16226722  0.20003928  1.        ]]. Action = [[-0.47867608 -0.66484463 -0.85180986  0.8626493 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1816. State = [[-0.18653286  0.15065557  0.1903139   1.        ]]. Action = [[ 0.5893035  -0.10880852 -0.6701338   0.7413361 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 1817. State = [[-0.18692416  0.14843129  0.19004776  1.        ]]. Action = [[ 0.9264852  -0.82730675  0.16176057  0.19652987]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 1818. State = [[-0.19899209  0.14226219  0.17973512  1.        ]]. Action = [[-0.82238984 -0.24809706 -0.7172867   0.8597152 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1819. State = [[-0.20809798  0.13949315  0.17457426  1.        ]]. Action = [[0.8851831  0.42486894 0.9309416  0.48298728]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1820. State = [[-0.1992177   0.15643963  0.19231372  1.        ]]. Action = [[0.13099217 0.74590325 0.63201547 0.8996949 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1821. State = [[-0.18345931  0.16103919  0.22128125  1.        ]]. Action = [[ 0.785156   -0.6965269   0.97113967  0.8479018 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1822. State = [[-0.17262065  0.13924734  0.23531637  1.        ]]. Action = [[-0.36463076 -0.8907286  -0.949384    0.55161595]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1823. State = [[-0.16436507  0.11334378  0.23098896  1.        ]]. Action = [[ 0.8046601  -0.54330444  0.24191475  0.64602685]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1824. State = [[-0.15900935  0.08340354  0.24336214  1.        ]]. Action = [[-0.9268831  -0.97985923  0.9104357   0.6898911 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1825. State = [[-0.16261925  0.07049281  0.27096882  1.        ]]. Action = [[0.49890888 0.48263764 0.896755   0.65121746]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1826. State = [[-0.26662254  0.0945226   0.11465658  1.        ]]. Action = [[ 0.33558798 -0.28432274 -0.77753407 -0.03481358]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1827. State = [[-0.26368243  0.10572986  0.10035633  1.        ]]. Action = [[-0.19239897 -0.78685224 -0.88804555  0.6224743 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1828. State = [[-0.2584814   0.09334449  0.10271066  1.        ]]. Action = [[ 0.2655226  -0.82308614  0.4893123   0.40569854]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1829. State = [[-0.24399242  0.08538125  0.10627227  1.        ]]. Action = [[ 0.7810813   0.20490456 -0.17685193  0.80723405]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1830. State = [[-0.2206831   0.10128053  0.11288992  1.        ]]. Action = [[0.8531871  0.96813786 0.34379935 0.78750026]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1831. State = [[-0.20196967  0.12829375  0.11595451  1.        ]]. Action = [[ 0.1712048   0.71940875 -0.883755    0.9081259 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1832. State = [[-0.19137868  0.15295836  0.10461932  1.        ]]. Action = [[-0.01083463  0.44101667  0.91461337  0.8494437 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1833. State = [[-0.18715772  0.16488598  0.11429929  1.        ]]. Action = [[ 0.46361876 -0.5549668  -0.9044517   0.68013835]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 1834. State = [[-0.1884019   0.15705839  0.12270918  1.        ]]. Action = [[-0.8445383  -0.7464049   0.61723065  0.6186464 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1835. State = [[-0.2012383   0.1459308   0.13497035  1.        ]]. Action = [[ 0.5978979 -0.5843474 -0.6020629  0.7872665]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 1836. State = [[-0.2081176   0.1460817   0.14774643  1.        ]]. Action = [[-0.49548203  0.08439159  0.85162616  0.7746402 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1837. State = [[-0.21867125  0.15799665  0.18146202  1.        ]]. Action = [[-0.02888268  0.56677806  0.9561417   0.21287656]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1838. State = [[-0.22995959  0.17634638  0.21172349  1.        ]]. Action = [[-0.231529    0.5727469   0.5836959   0.63022983]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1839. State = [[-0.23375325  0.17314231  0.22771695  1.        ]]. Action = [[-0.07668412 -0.960409   -0.001423    0.8049123 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1840. State = [[-0.23010394  0.16745491  0.24208173  1.        ]]. Action = [[0.3679248 0.5192803 0.7535664 0.6475582]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1841. State = [[-0.21642932  0.15545973  0.25195652  1.        ]]. Action = [[ 0.9058417  -0.96108955 -0.56252205  0.7252507 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1842. State = [[-0.19417249  0.1293032   0.24839401  1.        ]]. Action = [[ 0.6100192  -0.6990989  -0.09138793  0.83468044]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1843. State = [[-0.16777208  0.11382452  0.2572464   1.        ]]. Action = [[ 0.7691809  -0.04336274  0.7907164   0.7793486 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1844. State = [[-0.15142794  0.1000919   0.26378694  1.        ]]. Action = [[-0.11385566 -0.5944668  -0.57801396  0.55248713]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1845. State = [[-0.14810824  0.0902648   0.25916216  1.        ]]. Action = [[0.1225332  0.05491078 0.12317276 0.7738359 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1846. State = [[-0.13789377  0.0800389   0.26740584  1.        ]]. Action = [[ 0.6385987 -0.5732387  0.5339658  0.6582687]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1847. State = [[-0.12425058  0.07672021  0.27198902  1.        ]]. Action = [[ 0.26513124  0.6274526  -0.5347051   0.6183455 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1848. State = [[-0.11006077  0.07097863  0.2528805   1.        ]]. Action = [[ 0.30215812 -0.8283453  -0.6806444   0.75689983]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1849. State = [[-0.08688556  0.05437713  0.25069177  1.        ]]. Action = [[ 0.80306554 -0.3269999   0.87920463  0.228621  ]]. Reward = [0.]
Curr episode timestep = 22
Above hoop
Current timestep = 1850. State = [[-0.05881158  0.03592054  0.27315125  1.        ]]. Action = [[ 0.9547236  -0.6455786   0.5251231   0.82826066]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Current timestep = 1851. State = [[-0.02424676  0.01966732  0.294947    1.        ]]. Action = [[ 0.9522865  -0.13287866  0.6360512   0.55724144]]. Reward = [0.]
Curr episode timestep = 24
Above hoop
Current timestep = 1852. State = [[0.00723432 0.00587775 0.3054053  1.        ]]. Action = [[ 0.85405743 -0.47844124 -0.3741715   0.64767075]]. Reward = [0.]
Curr episode timestep = 25
Above hoop
Current timestep = 1853. State = [[ 0.03645908 -0.00161415  0.2885327   1.        ]]. Action = [[ 0.46466398  0.10660923 -0.63304025  0.8473711 ]]. Reward = [0.]
Curr episode timestep = 26
Above hoop
Current timestep = 1854. State = [[ 0.05502424 -0.01270671  0.26189893  1.        ]]. Action = [[ 0.14831781 -0.7839652  -0.7711107   0.8073158 ]]. Reward = [0.]
Curr episode timestep = 27
Above hoop
Scene graph at timestep 1854 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1854 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1854 of 1
Current timestep = 1855. State = [[ 0.07089677 -0.02934164  0.24473101  1.        ]]. Action = [[ 0.83080745  0.76558447 -0.09893596  0.6728449 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Above hoop
Scene graph at timestep 1855 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1855 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1855 of -1
Current timestep = 1856. State = [[ 0.07591267 -0.02520543  0.25107178  1.        ]]. Action = [[0.48317385 0.34628272 0.4042771  0.8467097 ]]. Reward = [0.]
Curr episode timestep = 29
Above hoop
Scene graph at timestep 1856 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1856 is tensor(1.8487e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1856 of -1
Current timestep = 1857. State = [[ 0.08847651 -0.01626332  0.25895938  1.        ]]. Action = [[ 0.7135756   0.42754555 -0.14671975  0.80172443]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Scene graph at timestep 1857 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1857 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1857 of -1
Current timestep = 1858. State = [[ 0.08794296 -0.01648442  0.26044247  1.        ]]. Action = [[-0.5037492   0.05169046  0.17649615  0.7756293 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1858 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1858 is tensor(5.1395e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1858 of -1
Current timestep = 1859. State = [[ 0.08537588 -0.01564616  0.26814613  1.        ]]. Action = [[-0.63617027  0.08450782  0.29510617  0.75558174]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1859 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1859 is tensor(2.5694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1859 of -1
Current timestep = 1860. State = [[ 0.07116588 -0.01546302  0.27405915  1.        ]]. Action = [[-0.72238123 -0.1058901  -0.37516809  0.73863196]]. Reward = [0.]
Curr episode timestep = 33
Above hoop
Scene graph at timestep 1860 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 1860 is tensor(6.5511e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1860 of -1
Current timestep = 1861. State = [[ 0.04809878 -0.02993346  0.27515927  1.        ]]. Action = [[-0.7331979  -0.7051618   0.35034955  0.11349142]]. Reward = [0.]
Curr episode timestep = 34
Above hoop
Current timestep = 1862. State = [[ 0.02500451 -0.05288593  0.29111618  1.        ]]. Action = [[-0.3114131  -0.5952181   0.41882908  0.5188534 ]]. Reward = [0.]
Curr episode timestep = 35
Above hoop
Current timestep = 1863. State = [[ 0.0043047  -0.06657279  0.3149083   1.        ]]. Action = [[-0.986825    0.07466435  0.7896631   0.6682832 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1864. State = [[-0.03019784 -0.06827217  0.31963122  1.        ]]. Action = [[-0.22821772 -0.03424114 -0.9180862   0.86461186]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1865. State = [[-0.04052332 -0.05682287  0.31331715  1.        ]]. Action = [[-0.27362102  0.7427366   0.59595585  0.35772586]]. Reward = [0.]
Curr episode timestep = 38
Above hoop
Current timestep = 1866. State = [[-0.06075813 -0.0518377   0.32472464  1.        ]]. Action = [[-0.6457262  -0.49484658  0.1532836   0.8523884 ]]. Reward = [0.]
Curr episode timestep = 39
Above hoop
Current timestep = 1867. State = [[-0.07921113 -0.04507909  0.32598606  1.        ]]. Action = [[-0.2467289   0.78560686 -0.21648782  0.92276955]]. Reward = [0.]
Curr episode timestep = 40
Above hoop
Current timestep = 1868. State = [[-0.083776   -0.02658257  0.31983313  1.        ]]. Action = [[ 0.72995615  0.2906661  -0.320522    0.25811505]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Current timestep = 1869. State = [[-0.07166614 -0.0222243   0.30393845  1.        ]]. Action = [[ 0.91117203 -0.16564047 -0.59879345  0.80730534]]. Reward = [0.]
Curr episode timestep = 42
Above hoop
Current timestep = 1870. State = [[-0.06010672 -0.03164423  0.28148293  1.        ]]. Action = [[-0.3697381  -0.6432606  -0.36271012  0.85917497]]. Reward = [0.]
Curr episode timestep = 43
Above hoop
Current timestep = 1871. State = [[-0.05981484 -0.04859427  0.2834374   1.        ]]. Action = [[-0.59483063 -0.32268202  0.78610516  0.7199403 ]]. Reward = [0.]
Curr episode timestep = 44
Above hoop
Current timestep = 1872. State = [[-0.06452227 -0.0441578   0.28903902  1.        ]]. Action = [[ 0.29927218  0.89533615 -0.5526796   0.8201778 ]]. Reward = [0.]
Curr episode timestep = 45
Above hoop
Current timestep = 1873. State = [[-0.07391351 -0.03921584  0.27350938  1.        ]]. Action = [[-0.78167427 -0.4711436  -0.9162269   0.685683  ]]. Reward = [0.]
Curr episode timestep = 46
Above hoop
Current timestep = 1874. State = [[-0.08981031 -0.04117123  0.24379352  1.        ]]. Action = [[ 0.00960684  0.21628773 -0.7145399   0.57913816]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Current timestep = 1875. State = [[-0.09338835 -0.03918706  0.2242232   1.        ]]. Action = [[ 0.09493113 -0.50111705 -0.70760447  0.868274  ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 1876. State = [[-0.09431358 -0.0279703   0.22531861  1.        ]]. Action = [[-0.08696944  0.6544857   0.43161833  0.94926715]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1877. State = [[-0.08710504 -0.01830822  0.23243605  1.        ]]. Action = [[ 0.8372462  -0.25901222  0.48209858  0.8513839 ]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Current timestep = 1878. State = [[-0.08236272 -0.02372219  0.24480964  1.        ]]. Action = [[-0.3329727  -0.2283538   0.47719383  0.790431  ]]. Reward = [0.]
Curr episode timestep = 51
Above hoop
Current timestep = 1879. State = [[-0.08213034 -0.02668925  0.25526494  1.        ]]. Action = [[ 0.07904625  0.6875218  -0.6053016   0.74581766]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Above hoop
Current timestep = 1880. State = [[-0.08362857 -0.03703354  0.26736575  1.        ]]. Action = [[-0.14294332 -0.48223627  0.69054556  0.50497293]]. Reward = [0.]
Curr episode timestep = 53
Above hoop
Current timestep = 1881. State = [[-0.0769194  -0.03512188  0.29167268  1.        ]]. Action = [[0.83248925 0.59524703 0.74935734 0.6190281 ]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Current timestep = 1882. State = [[-0.07216407 -0.02966433  0.30661535  1.        ]]. Action = [[-0.94420874 -0.12930602 -0.17533994  0.47741997]]. Reward = [0.]
Curr episode timestep = 55
Above hoop
Current timestep = 1883. State = [[-0.08421037 -0.04332528  0.3018517   1.        ]]. Action = [[ 0.19314694 -0.9242752  -0.77509195  0.8559073 ]]. Reward = [0.]
Curr episode timestep = 56
Above hoop
Current timestep = 1884. State = [[-0.09067513 -0.0490948   0.29565904  1.        ]]. Action = [[-0.6363962   0.77581906  0.24036348  0.83193743]]. Reward = [0.]
Curr episode timestep = 57
Above hoop
Current timestep = 1885. State = [[-0.10794847 -0.05346498  0.28926247  1.        ]]. Action = [[-0.6775404  -0.8608818  -0.5336401   0.45376635]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1886. State = [[-0.11724818 -0.05760653  0.26786706  1.        ]]. Action = [[ 0.78314877  0.5493264  -0.89922744  0.8292284 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1887. State = [[-0.11904399 -0.04787979  0.24212967  1.        ]]. Action = [[-0.92888194  0.31216073 -0.5947289   0.7384335 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1888. State = [[-0.12499524 -0.05033341  0.22860168  1.        ]]. Action = [[ 0.79923713 -0.54563206  0.0153774   0.77017117]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1889. State = [[-0.11875778 -0.05540133  0.22527455  1.        ]]. Action = [[-0.7272407   0.06494153  0.08068502  0.5732831 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 1890. State = [[-0.1163291  -0.05577286  0.2252558   1.        ]]. Action = [[ 0.97633314  0.5100627  -0.4781785   0.62527835]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 1891. State = [[-0.12504795 -0.0691542   0.2278458   1.        ]]. Action = [[-0.8712078  -0.71910846  0.3176272   0.70747924]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1892. State = [[-0.13689728 -0.08384294  0.22968143  1.        ]]. Action = [[ 0.29862344 -0.32499087 -0.59778464  0.8553903 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 1893. State = [[-0.13964477 -0.08648502  0.22939536  1.        ]]. Action = [[-0.5143519  -0.81613725 -0.8605227   0.7317536 ]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: No entry zone
Current timestep = 1894. State = [[-0.14025868 -0.08684601  0.22919343  1.        ]]. Action = [[ 0.2809279   0.92541313 -0.386814    0.6075963 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 1895. State = [[-0.14268823 -0.09290541  0.24318802  1.        ]]. Action = [[-0.35571015 -0.25218534  0.98347616  0.5802965 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1896. State = [[-0.14727925 -0.09806012  0.2609001   1.        ]]. Action = [[-0.40038133 -0.43957907 -0.8420087   0.8180051 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 1897. State = [[-0.15025425 -0.09901921  0.2616776   1.        ]]. Action = [[ 0.62050676  0.85668087 -0.90224016  0.7330103 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: No entry zone
Current timestep = 1898. State = [[-0.1521939  -0.09956242  0.2606977   1.        ]]. Action = [[ 0.2544701  0.919193  -0.9000052  0.6465509]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 1899. State = [[-0.16089514 -0.09960625  0.25206423  1.        ]]. Action = [[-0.4511401   0.13582718 -0.9460143   0.8070104 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1900. State = [[-0.16912723 -0.09919541  0.24391285  1.        ]]. Action = [[ 0.21477306  0.39263272 -0.9805581   0.904093  ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 1901. State = [[-0.1786424  -0.11101473  0.23409614  1.        ]]. Action = [[-0.5054903 -0.7356944 -0.7452343  0.5507091]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1902. State = [[-0.19421054 -0.12266429  0.21924524  1.        ]]. Action = [[ 0.40496278  0.9319972  -0.52756304  0.9090359 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 1903. State = [[-0.20481694 -0.11316857  0.21514599  1.        ]]. Action = [[-0.8332023   0.96576035  0.24919987  0.6140561 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1904. State = [[-0.21985725 -0.07949409  0.21696627  1.        ]]. Action = [[0.06665385 0.9646199  0.43733943 0.8006115 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1905. State = [[-0.22831853 -0.04826562  0.221074    1.        ]]. Action = [[-0.3859985   0.44687867 -0.12040871  0.83384633]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1906. State = [[-0.23361145 -0.05076073  0.21613531  1.        ]]. Action = [[ 0.2935835  -0.8756825  -0.8060413   0.87032604]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1907. State = [[-0.22214632 -0.03919404  0.20339108  1.        ]]. Action = [[ 0.95574665  0.60976005 -0.38899815  0.6166781 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1908. State = [[-0.20097576 -0.03961453  0.1973318   1.        ]]. Action = [[ 0.51112497 -0.7161114   0.49050128  0.71114707]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1909. State = [[-0.1936182  -0.03772337  0.19016811  1.        ]]. Action = [[-0.2305255   0.40065074 -0.87338877  0.4236915 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1910. State = [[-0.19925389 -0.04103172  0.17571847  1.        ]]. Action = [[-0.75618774 -0.9529702  -0.2924335   0.4784875 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1911. State = [[-0.2048399  -0.04353229  0.16794437  1.        ]]. Action = [[ 0.34667277 -0.03981525 -0.27880418  0.8270861 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1912. State = [[-0.2073197  -0.05201625  0.16777855  1.        ]]. Action = [[-0.77705103 -0.7962776   0.5585382   0.7371161 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1913. State = [[-0.21659532 -0.06337433  0.17574869  1.        ]]. Action = [[-0.10562027 -0.04987192 -0.04888737  0.66746163]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1914. State = [[-0.22398934 -0.06221963  0.17377783  1.        ]]. Action = [[-0.4274428   0.07512152 -0.16787094  0.7557347 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1915. State = [[-0.23183435 -0.05532903  0.18171643  1.        ]]. Action = [[0.08177435 0.5462587  0.87782454 0.34200943]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1916. State = [[-0.22870567 -0.04297458  0.1939278   1.        ]]. Action = [[ 0.8375268  0.5502331 -0.3950814  0.7981489]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1917. State = [[-0.2149237  -0.04947428  0.20079438  1.        ]]. Action = [[ 0.67135894 -0.8574602   0.6602125   0.8258569 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1918. State = [[-0.2084362  -0.07455405  0.22262512  1.        ]]. Action = [[-0.5966133  -0.38688242  0.84498155  0.8348049 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1919. State = [[-0.21152452 -0.08685492  0.22952451  1.        ]]. Action = [[ 0.52567506 -0.19278342 -0.5938086   0.73846555]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1920. State = [[-0.19808263 -0.09264001  0.21916227  1.        ]]. Action = [[ 0.58831704 -0.21904552 -0.54366493  0.777459  ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1921. State = [[-0.1907405  -0.10102738  0.19813098  1.        ]]. Action = [[-0.9236093  -0.1266554  -0.5837398   0.54947424]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1922. State = [[-0.20364445 -0.10538398  0.18976934  1.        ]]. Action = [[-0.6667646  -0.36555493 -0.14719939  0.7010819 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1923. State = [[-0.20534356 -0.10895682  0.19090867  1.        ]]. Action = [[ 0.53378224 -0.29886115  0.32042313  0.8885188 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1924. State = [[-0.2092601  -0.11003449  0.1866075   1.        ]]. Action = [[-0.5692057  -0.12926108 -0.76529676  0.47282004]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1925. State = [[-0.21881615 -0.1090128   0.1751352   1.        ]]. Action = [[-0.60360515 -0.6004502  -0.05591786  0.57922757]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1926. State = [[-0.23796003 -0.10307223  0.1653245   1.        ]]. Action = [[-0.8334115   0.82125103 -0.7569279   0.832258  ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1927. State = [[-0.25134557 -0.06023211  0.14169058  1.        ]]. Action = [[ 0.42321396  0.23261917 -0.98116904  0.9649439 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1928. State = [[-0.25077832 -0.11971918  0.09340528  1.        ]]. Action = [[-0.26970065 -0.20721531  0.2646265   0.76912546]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1929. State = [[-0.25109583 -0.13264602  0.07798271  1.        ]]. Action = [[-0.73627543 -0.5052648   0.16056502  0.5768821 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1930. State = [[-0.25094846 -0.13268714  0.07800067  1.        ]]. Action = [[-0.9660678   0.17170143  0.519148    0.76483345]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 1931. State = [[-0.25148147 -0.12380747  0.07202838  1.        ]]. Action = [[-0.03696549  0.6916745  -0.5338545   0.5486648 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1932. State = [[-0.24287985 -0.12758645  0.06862834  1.        ]]. Action = [[ 0.66931975 -0.90341455  0.9633324   0.8666258 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1933. State = [[-0.2275445  -0.1310833   0.08904902  1.        ]]. Action = [[0.39885068 0.63756    0.7068002  0.7046094 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1934. State = [[-0.22215657 -0.1270376   0.0970425   1.        ]]. Action = [[-0.18588972 -0.16375017 -0.87459993  0.76565456]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1935. State = [[-0.22715862 -0.12251449  0.09079866  1.        ]]. Action = [[-0.6013906   0.39125705  0.50489664  0.80011773]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1936. State = [[-0.22465412 -0.13072412  0.10495084  1.        ]]. Action = [[ 0.5377058  -0.8400681   0.87482154  0.7348964 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1937. State = [[-0.21946466 -0.14220281  0.11892724  1.        ]]. Action = [[ 0.20103669 -0.17975998 -0.49609601  0.8126764 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1938. State = [[-0.22576226 -0.15264584  0.11562951  1.        ]]. Action = [[-0.83129704 -0.3251964  -0.12465489  0.7240617 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1939. State = [[-0.23012975 -0.16151378  0.11526775  1.        ]]. Action = [[-0.95825326  0.68003833  0.02099609  0.10014653]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 1940. State = [[-0.23855118 -0.15105557  0.11993453  1.        ]]. Action = [[-0.5532932  0.9504938  0.7397001  0.5210793]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1941. State = [[-0.25453657 -0.12512103  0.12538467  1.        ]]. Action = [[-0.12107301  0.7461848  -0.48930442  0.52282095]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1942. State = [[-0.25319755 -0.11294755  0.1245802   1.        ]]. Action = [[ 0.55056715 -0.28441572 -0.04496825  0.62412333]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1943. State = [[-0.2512539  -0.1128299   0.12474208  1.        ]]. Action = [[-0.417858    0.6505456   0.7237685   0.87077117]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 1944. State = [[-0.25112233 -0.11272349  0.12473584  1.        ]]. Action = [[-0.74207556  0.34043074 -0.0209536   0.5473161 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 1945. State = [[-0.2536109  -0.12228201  0.11728635  1.        ]]. Action = [[-0.24513239 -0.7162252  -0.68830705  0.6271286 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1946. State = [[-0.25926682 -0.12222385  0.09520738  1.        ]]. Action = [[-0.30678046  0.8274274  -0.4229166   0.91958094]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1947. State = [[-0.26147968 -0.11379428  0.0837524   1.        ]]. Action = [[-0.7432938   0.3567798  -0.21670467  0.82504714]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1948. State = [[-0.25356448 -0.11732155  0.08116341  1.        ]]. Action = [[ 0.84010863 -0.48772216 -0.17988324  0.7424345 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1949. State = [[-0.24889253 -0.12836023  0.06992529  1.        ]]. Action = [[-0.31182134 -0.48724902 -0.5521808   0.7902007 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1950. State = [[-0.24691728 -0.13480233  0.06530841  1.        ]]. Action = [[0.36578202 0.39763856 0.9596436  0.77603674]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1951. State = [[-0.24024658 -0.13377394  0.07573485  1.        ]]. Action = [[-0.6837703   0.11003876  0.96290636  0.82610106]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 1952. State = [[-0.24327219 -0.13208377  0.07792919  1.        ]]. Action = [[-0.47172064  0.09705627  0.21550488  0.59963155]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1953. State = [[-0.25059128 -0.12959     0.08118203  1.        ]]. Action = [[-0.16013014  0.07261217 -0.22837079  0.5352857 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1954. State = [[-0.2593605  -0.11382404  0.07242532  1.        ]]. Action = [[-0.37206292  0.8352152  -0.97881514  0.605613  ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1955. State = [[-0.2675785  -0.09891104  0.05513534  1.        ]]. Action = [[-0.7069538   0.69087124  0.74525225  0.66602945]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 1956. State = [[-0.26284102 -0.10165666  0.04842824  1.        ]]. Action = [[ 0.6490438  -0.52205294 -0.48654246  0.85452473]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1957. State = [[-0.25531456 -0.11846466  0.03619479  1.        ]]. Action = [[-0.02914393 -0.88056225 -0.08200043  0.62900853]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1958. State = [[-0.253539   -0.13598321  0.03423396  1.        ]]. Action = [[ 0.22869909  0.8187697  -0.62177795  0.95758045]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 1959. State = [[-0.25000238 -0.13856994  0.03741239  1.        ]]. Action = [[ 0.28745747 -0.0192048   0.5307584   0.75739956]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1960. State = [[-0.24645407 -0.13955793  0.03973302  1.        ]]. Action = [[-0.84828854 -0.7054652  -0.0155493   0.6333871 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 1961. State = [[-0.23956177 -0.15163068  0.05096989  1.        ]]. Action = [[ 0.42075253 -0.69508195  0.9253328   0.6937914 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1962. State = [[-0.22247715 -0.16095087  0.07547646  1.        ]]. Action = [[0.6337751  0.31789672 0.31427372 0.7856474 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1963. State = [[-0.21044967 -0.15529615  0.08243305  1.        ]]. Action = [[-0.17095596  0.44519734 -0.22247839  0.85796154]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1964. State = [[-0.20601489 -0.1350667   0.08520786  1.        ]]. Action = [[0.23642087 0.8717866  0.42920196 0.8014649 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1965. State = [[-0.19814345 -0.12974079  0.09466935  1.        ]]. Action = [[ 0.23294628 -0.927899    0.19214904  0.9346075 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1966. State = [[-0.19556703 -0.15105091  0.10229249  1.        ]]. Action = [[-0.26124585 -0.7262436   0.32092476  0.89888406]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1967. State = [[-0.19580302 -0.15935455  0.10242229  1.        ]]. Action = [[ 0.30388904  0.4154619  -0.8561633   0.7113848 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1968. State = [[-0.19124818 -0.1555398   0.09552832  1.        ]]. Action = [[0.5763658  0.02010465 0.93321717 0.6437739 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 1969. State = [[-0.19739147 -0.15959875  0.0946814   1.        ]]. Action = [[-0.82313937 -0.26232767  0.18038094  0.81054914]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1970. State = [[-0.20500618 -0.16142651  0.08875372  1.        ]]. Action = [[ 0.14763284  0.15738797 -0.585975    0.24529421]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1971. State = [[-0.21594481 -0.15306899  0.08016573  1.        ]]. Action = [[-0.7525629   0.6023563  -0.03194636  0.30632544]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1972. State = [[-0.22527999 -0.13050742  0.06801999  1.        ]]. Action = [[ 0.2720436  0.8812611 -0.9756992  0.8007736]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1973. State = [[-0.22293925 -0.10937768  0.04345419  1.        ]]. Action = [[ 0.59821296  0.11573303 -0.07622313  0.8588929 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1974. State = [[-0.2140272  -0.10302456  0.039361    1.        ]]. Action = [[ 0.47172654  0.1553483  -0.90399635  0.7577208 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 1975. State = [[-0.20363005 -0.10529233  0.03783524  1.        ]]. Action = [[ 0.68597317 -0.37556303 -0.13167447  0.51465917]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1976. State = [[-0.19215094 -0.11667737  0.03923768  1.        ]]. Action = [[-0.3533213  -0.36041498  0.8041035   0.8512517 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1977. State = [[-0.19171013 -0.12548676  0.04770911  1.        ]]. Action = [[-0.5626606  -0.39570534 -0.94873613  0.8544369 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 1978. State = [[-0.19215219 -0.12764378  0.04893824  1.        ]]. Action = [[-0.7395505  -0.8100278  -0.9174851   0.66512144]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 1979. State = [[-0.19363196 -0.14121518  0.05003431  1.        ]]. Action = [[ 0.02035928 -0.9188573  -0.08029193  0.65082896]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1980. State = [[-0.18676762 -0.1723483   0.06186711  1.        ]]. Action = [[ 0.5116224 -0.9435913  0.9796655  0.6268872]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1981. State = [[-0.18789722 -0.17934689  0.09255804  1.        ]]. Action = [[-0.9706255  0.9288547  0.9414246  0.6827047]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1982. State = [[-0.1933729  -0.17151605  0.12015776  1.        ]]. Action = [[ 0.49460912 -0.02500367  0.3832748   0.6761565 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1983. State = [[-0.19038863 -0.16993733  0.13196787  1.        ]]. Action = [[0.8176812  0.31812847 0.49644327 0.6092862 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 1984. State = [[-0.19019488 -0.16948396  0.1339826   1.        ]]. Action = [[ 0.63557863  0.76225257 -0.20624459  0.7758403 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 1985. State = [[-0.19026935 -0.16933455  0.13393506  1.        ]]. Action = [[ 0.96479785  0.10286045 -0.3413195   0.6274574 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 1986. State = [[-0.1989764  -0.18175294  0.13407072  1.        ]]. Action = [[-0.8563939  -0.8005995   0.02274311  0.71331096]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1987. State = [[-0.2060521  -0.18197675  0.12868163  1.        ]]. Action = [[ 0.4429735   0.7274349  -0.7975637   0.48475826]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1988. State = [[-0.20571002 -0.1617787   0.12087531  1.        ]]. Action = [[-0.2569523   0.7647151   0.12884545  0.81573296]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1989. State = [[-0.2006927  -0.14901161  0.1226953   1.        ]]. Action = [[ 0.5749979 -0.1043939  0.2975726  0.8368709]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1990. State = [[-0.19484879 -0.15418248  0.13568084  1.        ]]. Action = [[-0.0981217  -0.46431392  0.960917    0.93420696]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1991. State = [[-0.1949364  -0.15211213  0.14733526  1.        ]]. Action = [[-0.07831156  0.46974456 -0.89580977  0.92445004]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1992. State = [[-0.2055668  -0.13569039  0.13120085  1.        ]]. Action = [[-0.65054756  0.6878574  -0.696601    0.80564666]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1993. State = [[-0.21751794 -0.1168905   0.10431372  1.        ]]. Action = [[ 0.00287616  0.30860043 -0.9734959   0.8933346 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1994. State = [[-0.23229606 -0.10824895  0.08554122  1.        ]]. Action = [[-0.8962096   0.16063952  0.81193614  0.8926184 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1995. State = [[-0.24488069 -0.102876    0.09260438  1.        ]]. Action = [[-0.9459776   0.5707195  -0.9316854   0.78014123]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 1996. State = [[-0.24828531 -0.0907573   0.09193411  1.        ]]. Action = [[ 0.16595149  0.70952415 -0.10222715  0.5972209 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1997. State = [[-0.25006863 -0.07658619  0.09195453  1.        ]]. Action = [[-0.8727799   0.11304855 -0.9714862   0.5660341 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 1998. State = [[-0.24538031 -0.08167233  0.09262522  1.        ]]. Action = [[ 0.516726   -0.5817537  -0.04113477  0.8373308 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1999. State = [[-0.23993263 -0.08606334  0.09882008  1.        ]]. Action = [[-0.34587598  0.15956962  0.62908316  0.47512913]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2000. State = [[-0.24368371 -0.09435765  0.09954031  1.        ]]. Action = [[-0.16009259 -0.58240485 -0.93737996  0.8375815 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2001. State = [[-0.2471432  -0.10251374  0.08904947  1.        ]]. Action = [[-0.67410016 -0.2225771   0.31846857  0.84514046]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2002. State = [[-0.24977753 -0.10684323  0.08078218  1.        ]]. Action = [[ 0.1458888  -0.18417227 -0.9303215   0.388718  ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2003. State = [[-0.253643   -0.09774207  0.0578603   1.        ]]. Action = [[-0.15548319  0.9867053   0.15655375  0.84084284]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2004. State = [[-0.25193727 -0.07261157  0.06453066  1.        ]]. Action = [[0.10526216 0.8306705  0.8473716  0.51491976]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2005. State = [[-0.2504251  -0.05231042  0.07390688  1.        ]]. Action = [[ 0.8646219  -0.88762695 -0.87923604  0.885537  ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 2006. State = [[-0.2523461  -0.06018774  0.07528128  1.        ]]. Action = [[-0.23016542 -0.7109416   0.07387221  0.8833432 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2007. State = [[-0.25170696 -0.08456972  0.08600573  1.        ]]. Action = [[ 0.23517609 -0.9395867   0.74382675  0.5984864 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2008. State = [[-0.24962844 -0.1011569   0.10382163  1.        ]]. Action = [[-0.59291375  0.52209425 -0.38800275  0.84921336]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2009. State = [[-0.25029078 -0.10393675  0.10594457  1.        ]]. Action = [[-0.6686825  -0.87888765  0.85649395  0.8857832 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2010. State = [[-0.24072261 -0.09555223  0.10795165  1.        ]]. Action = [[0.8731358  0.67929673 0.14537251 0.72154367]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2011. State = [[-0.21294552 -0.08069178  0.12432837  1.        ]]. Action = [[0.83423495 0.409284   0.9351404  0.8117204 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2012. State = [[-0.18146458 -0.08047424  0.14732756  1.        ]]. Action = [[ 0.758131   -0.722255   -0.16035664  0.7332053 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2013. State = [[-0.16430986 -0.088052    0.1528215   1.        ]]. Action = [[ 0.58928585  0.782748   -0.94869184  0.9199021 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 2014. State = [[-0.16322695 -0.0887668   0.15309195  1.        ]]. Action = [[ 0.07504272 -0.5121067  -0.5457921   0.8694594 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 2015. State = [[-0.16291499 -0.08866539  0.15321401  1.        ]]. Action = [[0.5181401 0.7534206 0.4092412 0.7471235]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: No entry zone
Current timestep = 2016. State = [[-0.16283214 -0.08873647  0.15326239  1.        ]]. Action = [[0.7486572  0.9259261  0.94883084 0.8412976 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 2017. State = [[-0.16283214 -0.08873647  0.15326239  1.        ]]. Action = [[ 0.8530879  -0.13436252 -0.6341942   0.9176214 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 2018. State = [[-0.169548   -0.07872509  0.14480534  1.        ]]. Action = [[-0.6353205   0.81526613 -0.9473609   0.8996501 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2019. State = [[-0.17882091 -0.06785223  0.12343796  1.        ]]. Action = [[-0.3849511   0.0773536  -0.22909778  0.60961473]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2020. State = [[-0.1912135  -0.07049371  0.10664065  1.        ]]. Action = [[-0.53968203 -0.4564401  -0.7595129   0.8176917 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2021. State = [[-0.20523274 -0.07521897  0.09622313  1.        ]]. Action = [[-0.16085804 -0.00995988  0.9547124   0.9043703 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2022. State = [[-0.203096   -0.07083967  0.1120108   1.        ]]. Action = [[0.56805325 0.41186368 0.45771754 0.6651639 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2023. State = [[-0.19668096 -0.07218545  0.11856888  1.        ]]. Action = [[ 0.2855525  -0.5078408  -0.49221456  0.7869978 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2024. State = [[-0.19425704 -0.08246353  0.11734948  1.        ]]. Action = [[-0.11714965 -0.33363545 -0.04826891  0.5722976 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2025. State = [[-0.20482215 -0.07740469  0.11058608  1.        ]]. Action = [[-0.91449356  0.84629273 -0.34806305  0.28768814]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2026. State = [[-0.2118256  -0.0537746   0.10927704  1.        ]]. Action = [[0.6104802  0.90851927 0.61460614 0.80713034]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2027. State = [[-0.21732953 -0.01823573  0.12264372  1.        ]]. Action = [[-0.86646265  0.95249104  0.7180685   0.8464775 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2028. State = [[-0.22014071  0.00564379  0.13737603  1.        ]]. Action = [[ 0.889848    0.07535803 -0.25305212  0.20554936]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2029. State = [[-0.19903898  0.00479075  0.14869134  1.        ]]. Action = [[ 0.88345337 -0.40826845  0.87927806  0.7520206 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2030. State = [[-0.25583452 -0.12200851  0.1216099   1.        ]]. Action = [[ 0.45098615 -0.1763587  -0.06439954  0.46588504]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2031. State = [[-0.25774726 -0.13527012  0.10776038  1.        ]]. Action = [[-0.8960277   0.9076605  -0.95948136  0.56769884]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 2032. State = [[-0.2589324  -0.13801385  0.10448255  1.        ]]. Action = [[-0.12423879 -0.15814793 -0.23371542  0.2867968 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2033. State = [[-0.25302666 -0.01351833  0.1247951   1.        ]]. Action = [[ 0.09319782 -0.80060166  0.45464242 -0.09503782]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2034. State = [[-0.25109497 -0.0169972   0.11111771  1.        ]]. Action = [[-0.9561833  -0.97850347 -0.92389476  0.9223139 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 2035. State = [[-0.2532859  -0.03080237  0.10283389  1.        ]]. Action = [[-0.35267383 -0.9146857  -0.81168854  0.74238265]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2036. State = [[-0.25625235 -0.04754423  0.08089083  1.        ]]. Action = [[-0.80447805 -0.8876158   0.61776304  0.7766137 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2037. State = [[-0.25734133 -0.04957141  0.0779759   1.        ]]. Action = [[-0.6615004  -0.23608732 -0.32948375  0.49562633]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2038. State = [[-0.25711098 -0.05006206  0.07801622  1.        ]]. Action = [[-0.6821427  -0.29780847 -0.78767705  0.6444006 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2039. State = [[-0.25711098 -0.05006206  0.07801622  1.        ]]. Action = [[-0.73678714  0.7168492   0.30040622  0.6263447 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 2040. State = [[-0.2541503  -0.0552108   0.07882246  1.        ]]. Action = [[ 0.36731207 -0.2605294   0.27340972  0.84321606]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2041. State = [[-0.25155294 -0.05988892  0.07980537  1.        ]]. Action = [[-0.7498074   0.0740726  -0.5911931   0.54233956]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2042. State = [[-0.25143495 -0.06066303  0.07987462  1.        ]]. Action = [[-0.9681668   0.51391137 -0.47476053  0.8439996 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2043. State = [[-0.253651   -0.06330153  0.07252272  1.        ]]. Action = [[-0.10550761 -0.13657683 -0.8899134   0.7656411 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2044. State = [[-0.24333426 -0.06688021  0.06596685  1.        ]]. Action = [[0.9022572  0.08502936 0.8905393  0.72547793]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2045. State = [[-0.22999619 -0.05857986  0.07970987  1.        ]]. Action = [[-0.22941363  0.6863518   0.4128102   0.62400734]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2046. State = [[-0.22977652 -0.0394883   0.09450671  1.        ]]. Action = [[-0.08728838  0.5328665   0.37719822  0.7451832 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2047. State = [[-0.23422354 -0.01615671  0.09457868  1.        ]]. Action = [[-0.06806439  0.6499444  -0.9137392   0.85268545]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2048. State = [[-0.23584053 -0.00433764  0.08657403  1.        ]]. Action = [[-0.9772393  -0.50576645  0.49214518  0.79845715]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2049. State = [[-0.23867801 -0.00703327  0.09039944  1.        ]]. Action = [[-0.5995807  -0.38581705  0.6538445   0.878608  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2050. State = [[-0.24462365 -0.00966381  0.09589494  1.        ]]. Action = [[-0.62922144  0.30907416  0.01294625  0.87927413]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2051. State = [[-0.24472983 -0.00966791  0.09591281  1.        ]]. Action = [[-0.52131796 -0.58166605  0.73610127  0.7776432 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2052. State = [[-0.24581094 -0.01613963  0.09160724  1.        ]]. Action = [[ 0.0463903  -0.43692583 -0.58366656  0.8822881 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2053. State = [[-0.246075   -0.01691475  0.09270921  1.        ]]. Action = [[-0.13505954  0.41287875  0.79216146  0.6929941 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2054. State = [[-0.2412757  -0.01365988  0.10284148  1.        ]]. Action = [[0.5802951  0.1444807  0.04164577 0.7923815 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2055. State = [[-0.23929784 -0.01233217  0.10496057  1.        ]]. Action = [[-0.76850885  0.56751156  0.71865666  0.93755364]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2056. State = [[-2.3023592e-01  4.5970586e-04  1.0729236e-01  1.0000000e+00]]. Action = [[0.7510905  0.7568679  0.04830098 0.66715527]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2057. State = [[-0.21407433  0.0130182   0.11066373  1.        ]]. Action = [[ 0.17855024 -0.15502983 -0.24658775  0.7700596 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2058. State = [[-0.2069458   0.00964839  0.11102457  1.        ]]. Action = [[ 0.16941345 -0.30225837  0.33129978  0.6963105 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2059. State = [[-0.20969759  0.01749069  0.11592692  1.        ]]. Action = [[-0.9127975   0.70615685  0.24100256  0.37986803]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2060. State = [[-0.221271    0.04036042  0.11754322  1.        ]]. Action = [[ 0.2062707   0.69449186 -0.33610392  0.5185194 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2061. State = [[-0.21379386  0.06255228  0.11146707  1.        ]]. Action = [[ 0.86608887  0.57806504 -0.56204367  0.45150447]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2062. State = [[-0.18440342  0.07362226  0.10736813  1.        ]]. Action = [[ 0.9212929  -0.12720096  0.63021684  0.41617775]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2063. State = [[-0.16175814  0.07574257  0.11710166  1.        ]]. Action = [[ 0.6021948  -0.20431358 -0.18859792  0.92242944]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 2064. State = [[-0.16293986  0.06636596  0.11072832  1.        ]]. Action = [[-0.35868967 -0.61988616 -0.7370466   0.738552  ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2065. State = [[-0.16712514  0.05977985  0.09881876  1.        ]]. Action = [[ 0.88881946  0.32346034 -0.8439949   0.41584742]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 2066. State = [[-0.16769639  0.04831277  0.10008761  1.        ]]. Action = [[-0.40407217 -0.5463374   0.49320316  0.7569721 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2067. State = [[-0.18170014  0.0496609   0.10001368  1.        ]]. Action = [[-0.94445395  0.89274025 -0.38172174  0.7544043 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2068. State = [[-0.20479505  0.06351727  0.09142318  1.        ]]. Action = [[-0.21739233  0.01543307 -0.44351768  0.7404082 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2069. State = [[-0.21497649  0.06847461  0.09098789  1.        ]]. Action = [[-0.4493233   0.08218896  0.82776666  0.61649024]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2070. State = [[-0.23827265  0.0653233   0.11050742  1.        ]]. Action = [[-0.9559144  -0.35838318  0.66939974  0.7314273 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2071. State = [[-0.25241005  0.04951864  0.12697856  1.        ]]. Action = [[ 0.5252178  -0.6749054  -0.05224907  0.8220676 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2072. State = [[-0.24927592  0.02761858  0.12690051  1.        ]]. Action = [[ 0.14540064 -0.6086521  -0.3888533   0.74979496]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2073. State = [[-0.2409751   0.01679484  0.13229632  1.        ]]. Action = [[0.37278175 0.11726296 0.75463367 0.76498115]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2074. State = [[-0.22482634  0.00933165  0.13783866  1.        ]]. Action = [[ 0.9575193  -0.44683123 -0.46173984  0.74314094]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2075. State = [[-0.20538412 -0.01694539  0.14251664  1.        ]]. Action = [[ 0.26833367 -0.9510101   0.7466973   0.72534394]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2076. State = [[-0.19266886 -0.04725126  0.16322803  1.        ]]. Action = [[ 0.11049736 -0.64517     0.9603355   0.68325686]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2077. State = [[-0.18771084 -0.07105757  0.18035708  1.        ]]. Action = [[ 0.13825607 -0.5918674  -0.5759227   0.5328672 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2078. State = [[-0.18580975 -0.08632187  0.1770463   1.        ]]. Action = [[ 0.7703037  -0.5475511  -0.07184756  0.77193093]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 2079. State = [[-0.18793488 -0.09594594  0.17397837  1.        ]]. Action = [[-0.4760149  -0.48062038 -0.29088295  0.9442234 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2080. State = [[-0.20026053 -0.12152923  0.1777415   1.        ]]. Action = [[-0.8145172  -0.825972    0.7382617   0.54847264]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2081. State = [[-0.22390532 -0.13211939  0.1847616   1.        ]]. Action = [[-0.90063465  0.54643714 -0.14517212  0.5923325 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2082. State = [[-0.24041778 -0.12340552  0.18123789  1.        ]]. Action = [[ 0.2931962   0.27385902 -0.5713248   0.84027576]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2083. State = [[-0.24019444 -0.11977221  0.17535578  1.        ]]. Action = [[-0.8447523   0.6430752   0.9794669   0.68754673]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2084. State = [[-0.23870793 -0.10897414  0.17648707  1.        ]]. Action = [[0.16517603 0.52147007 0.44306314 0.73130417]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2085. State = [[-0.23316117 -0.09563896  0.16940787  1.        ]]. Action = [[ 0.5909202   0.15441418 -0.9373807   0.6178366 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2086. State = [[-0.22523157 -0.07462985  0.15450582  1.        ]]. Action = [[0.07308567 0.8317177  0.09774876 0.8892453 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2087. State = [[-0.22417313 -0.06814998  0.1461575   1.        ]]. Action = [[-0.2445491  -0.4349134  -0.63294566  0.45934868]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2088. State = [[-0.2256769  -0.06353404  0.12482285  1.        ]]. Action = [[ 0.23856461  0.437683   -0.6183013   0.7739811 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2089. State = [[-0.22963513 -0.04586323  0.11429448  1.        ]]. Action = [[-0.6111825  0.6798918  0.6583024  0.7570417]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2090. State = [[-0.22587605 -0.04131284  0.12022107  1.        ]]. Action = [[ 0.7562432  -0.6268841  -0.04776335  0.9227531 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2091. State = [[-0.22555526 -0.03640801  0.11450445  1.        ]]. Action = [[-0.42134905  0.7769916  -0.6542314   0.78000045]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2092. State = [[-0.2286361  -0.02850555  0.10733397  1.        ]]. Action = [[-0.9040332  -0.76965654  0.98345613  0.54007816]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 2093. State = [[-0.22724745 -0.02382312  0.1027547   1.        ]]. Action = [[ 0.323462   -0.02355736 -0.2641343   0.7347008 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2094. State = [[-0.23044258 -0.02449225  0.09924987  1.        ]]. Action = [[-0.7036593  -0.15973765  0.5023763   0.8782296 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2095. State = [[-0.24145523 -0.03548053  0.10598955  1.        ]]. Action = [[-0.6998931  -0.51332915  0.42981732  0.8846843 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2096. State = [[-0.2498263  -0.04657208  0.10815441  1.        ]]. Action = [[ 0.9163642  -0.11421508 -0.9511538   0.6840222 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2097. State = [[-0.24315505 -0.05415994  0.08642195  1.        ]]. Action = [[ 0.22516942 -0.1982159  -0.8579981   0.77514267]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2098. State = [[-0.2295383  -0.04814192  0.06592277  1.        ]]. Action = [[0.5089297 0.5762826 0.0779525 0.843776 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2099. State = [[-0.22412775 -0.02607562  0.07110231  1.        ]]. Action = [[-0.53714466  0.9712887   0.9553685   0.9020523 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2100. State = [[-0.23661195 -0.02091983  0.0859107   1.        ]]. Action = [[-0.7967978  -0.92155594  0.33915818  0.8573468 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2101. State = [[-0.24381055 -0.01941196  0.10640409  1.        ]]. Action = [[0.8096049  0.86829066 0.9530513  0.88897157]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2102. State = [[-0.22731295 -0.00804924  0.12725277  1.        ]]. Action = [[ 0.922915   -0.11708486 -0.4220509   0.5463362 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2103. State = [[-0.21446101  0.00191625  0.12161589  1.        ]]. Action = [[-0.32556397  0.57533586 -0.5159977   0.6589122 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2104. State = [[-0.22306672 -0.00298989  0.10892158  1.        ]]. Action = [[-0.74798095 -0.88045037 -0.23434949  0.6904615 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2105. State = [[-0.24014655 -0.02315952  0.09959289  1.        ]]. Action = [[-0.8674599  -0.6276061  -0.49572206  0.49660325]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2106. State = [[-0.24924743 -0.03697085  0.08517385  1.        ]]. Action = [[ 0.73571885  0.00609732 -0.14601427  0.9369724 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2107. State = [[-0.24444795 -0.05321022  0.08389774  1.        ]]. Action = [[ 0.03973997 -0.9661191   0.1258235   0.8771012 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2108. State = [[-0.2419212  -0.08198237  0.08874182  1.        ]]. Action = [[-0.3303811  -0.5419418   0.63723445  0.77762735]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2109. State = [[-0.24706304 -0.10948582  0.1000503   1.        ]]. Action = [[-0.44524866 -0.88839144  0.47222936  0.80579376]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2110. State = [[-0.25675213 -0.14152457  0.10870942  1.        ]]. Action = [[ 0.27501464 -0.8446172  -0.32935023  0.4459319 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2111. State = [[-0.25582626 -0.16462675  0.10827745  1.        ]]. Action = [[-0.7486219   0.6785792   0.5600306   0.70486426]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2112. State = [[-0.25590298 -0.16664475  0.10823655  1.        ]]. Action = [[-0.44700962 -0.20179892 -0.07789415  0.81802034]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2113. State = [[-0.25938192 -0.17373547  0.10157844  1.        ]]. Action = [[-0.06796533 -0.4093113  -0.66669893  0.9038813 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2114. State = [[-0.25955725 -0.18333675  0.09188055  1.        ]]. Action = [[-0.21783173 -0.1599642   0.10007751  0.80435085]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2115. State = [[-0.2582386  -0.17252557  0.08048539  1.        ]]. Action = [[-0.01142293  0.90836835 -0.97575754  0.840682  ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2116. State = [[-0.2580322 -0.1407468  0.0450192  1.       ]]. Action = [[ 0.29077864  0.8736602  -0.88490516  0.7454121 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2117. State = [[-0.25856596 -0.12141486  0.02103739  1.        ]]. Action = [[-0.73467493 -0.21760029 -0.5747206   0.84349597]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2118. State = [[-0.2471915  -0.12611796  0.0257159   1.        ]]. Action = [[ 0.8032613  -0.64496636  0.7814565   0.7870815 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2119. State = [[-0.22119893 -0.12445996  0.04271149  1.        ]]. Action = [[0.85352445 0.684451   0.7055032  0.69841504]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2120. State = [[-0.20193301 -0.11802944  0.05762999  1.        ]]. Action = [[ 0.83037364 -0.04811019 -0.7319893   0.50391316]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 2121. State = [[-0.19180208 -0.10733403  0.07072315  1.        ]]. Action = [[0.40592933 0.6070081  0.90754414 0.5794959 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2122. State = [[-0.1917303  -0.10359519  0.09711978  1.        ]]. Action = [[-0.9026666  -0.43786848  0.53262377  0.5541893 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2123. State = [[-0.1976578  -0.11075711  0.11066951  1.        ]]. Action = [[0.6503782  0.6806179  0.4024384  0.89757276]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 2124. State = [[-0.19928128 -0.10022831  0.10774937  1.        ]]. Action = [[ 0.31462872  0.76511955 -0.78605974  0.66943455]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2125. State = [[-0.19722702 -0.08773531  0.10192465  1.        ]]. Action = [[ 0.2731279  -0.13927138  0.12417138  0.7386012 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2126. State = [[-0.19981265 -0.08558509  0.10502484  1.        ]]. Action = [[-0.7517431   0.13038075  0.4508245   0.7605877 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2127. State = [[-0.20264299 -0.08353146  0.10634097  1.        ]]. Action = [[ 0.7911092   0.8105099  -0.56268024  0.7944989 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 2128. State = [[-0.20634717 -0.08026843  0.10244995  1.        ]]. Action = [[-0.17852843  0.24014044 -0.52272093  0.7266846 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2129. State = [[-0.22097394 -0.07251407  0.09417725  1.        ]]. Action = [[-0.6929539   0.29173994 -0.49060678  0.8963969 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2130. State = [[-0.23708098 -0.06881693  0.07587376  1.        ]]. Action = [[-0.04771161 -0.22877795 -0.5874752   0.89232635]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2131. State = [[-0.24831133 -0.06912743  0.05829271  1.        ]]. Action = [[-0.620074    0.15253997 -0.12024468  0.89600015]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2132. State = [[-0.2635511  -0.07964718  0.05128125  1.        ]]. Action = [[-0.16910017 -0.99479884 -0.26469386  0.8195751 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2133. State = [[-0.26226488 -0.09360636  0.05175675  1.        ]]. Action = [[0.577893   0.05009043 0.9457209  0.75307775]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2134. State = [[-0.2480869  -0.08524784  0.074163    1.        ]]. Action = [[0.73104525 0.6460402  0.8440757  0.80659175]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2135. State = [[-0.24612598 -0.08053698  0.12168838  1.        ]]. Action = [[0.29032433 0.35783362 0.73405254 0.7532902 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2136. State = [[-0.24522524 -0.09642313  0.10891766  1.        ]]. Action = [[ 0.09353685 -0.50401616 -0.10549515  0.8186953 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2137. State = [[-0.24338533 -0.09881784  0.10803291  1.        ]]. Action = [[-0.0943749   0.67218196  0.22012234  0.87570584]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2138. State = [[-0.24645762 -0.10485426  0.10381173  1.        ]]. Action = [[-0.41034544 -0.87210035 -0.39448452  0.85548854]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2139. State = [[-0.24525972 -0.11134289  0.09977613  1.        ]]. Action = [[0.64213085 0.39646697 0.3421222  0.65931344]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2140. State = [[-0.22560975 -0.12189176  0.11232049  1.        ]]. Action = [[ 0.9338664  -0.858572    0.9640564   0.75883985]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2141. State = [[-0.20954236 -0.13398     0.14108332  1.        ]]. Action = [[-0.5301097   0.01232886  0.85295725  0.819911  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2142. State = [[-0.22168009 -0.1257114   0.15328512  1.        ]]. Action = [[-0.64964634  0.8685243  -0.6654713   0.8280957 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2143. State = [[-0.24221535 -0.10442141  0.14458743  1.        ]]. Action = [[-0.81330943  0.59204936 -0.4224609   0.46810532]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2144. State = [[-0.26423737 -0.08760277  0.146674    1.        ]]. Action = [[-0.48327094  0.18574142  0.9856963   0.7600393 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2145. State = [[-0.27520588 -0.08690342  0.1549312   1.        ]]. Action = [[ 0.38358867 -0.49851698 -0.7614783   0.7383405 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2146. State = [[-0.27023676 -0.09933947  0.14093715  1.        ]]. Action = [[ 0.4498074  -0.60008186 -0.6157106   0.7612419 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2147. State = [[-0.2521132  -0.10317416  0.1319308   1.        ]]. Action = [[0.93888247 0.69261146 0.719614   0.6558931 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2148. State = [[-0.22910085 -0.10604351  0.13662708  1.        ]]. Action = [[ 0.30747354 -0.7315381  -0.15723705  0.539268  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2149. State = [[-0.21147665 -0.10464751  0.13176785  1.        ]]. Action = [[ 0.8485465   0.5871564  -0.75116664  0.9440894 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2150. State = [[-0.18081191 -0.0855057   0.12268138  1.        ]]. Action = [[0.84959674 0.89273167 0.56724477 0.7410104 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2151. State = [[-0.16123033 -0.07088632  0.12331596  1.        ]]. Action = [[-0.01649535 -0.19165468 -0.7503943   0.64384794]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2152. State = [[-0.15486453 -0.07114683  0.10967284  1.        ]]. Action = [[-0.1509018   0.95357084  0.9139298  -0.01218116]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 2153. State = [[-0.15608025 -0.06787894  0.11409415  1.        ]]. Action = [[-0.64432955  0.21405852  0.73299384  0.38152134]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2154. State = [[-0.16226481 -0.06504924  0.12316597  1.        ]]. Action = [[ 0.61935806 -0.01301086 -0.8687512   0.81253695]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 2155. State = [[-0.16464101 -0.06453951  0.12288141  1.        ]]. Action = [[0.17488444 0.9340292  0.24343467 0.8095002 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 2156. State = [[-0.16648299 -0.05844573  0.13403717  1.        ]]. Action = [[-0.3039862   0.33169568  0.90134275  0.67197895]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2157. State = [[-0.17028075 -0.05200158  0.15458566  1.        ]]. Action = [[ 0.874436   -0.9274784   0.91708994  0.7796309 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 2158. State = [[-0.17112136 -0.05155315  0.1592002   1.        ]]. Action = [[0.8044437  0.7514827  0.18009853 0.755365  ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 2159. State = [[-0.17126597 -0.05145051  0.15948713  1.        ]]. Action = [[ 0.7686949   0.6043732  -0.44679296  0.82409465]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 2160. State = [[-0.18080303 -0.0497723   0.1590648   1.        ]]. Action = [[-0.78876     0.10884345  0.05007017  0.2504815 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2161. State = [[-0.1967284  -0.06276637  0.17222095  1.        ]]. Action = [[-0.06950259 -0.9302082   0.8147129   0.758335  ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2162. State = [[-0.21362944 -0.08718444  0.18789664  1.        ]]. Action = [[-0.9195854  -0.5523301  -0.12055135  0.7511678 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2163. State = [[-0.23924224 -0.09852116  0.18619204  1.        ]]. Action = [[-0.52902216 -0.00177157 -0.5342149   0.755003  ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2164. State = [[-0.25582227 -0.10048703  0.17980622  1.        ]]. Action = [[-0.6043767   0.38266814 -0.5509184   0.01403832]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 2165. State = [[-0.2565376  -0.10116327  0.17958522  1.        ]]. Action = [[-0.944685    0.01051116  0.83006144  0.8973515 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 2166. State = [[-0.2573     -0.10102613  0.17925902  1.        ]]. Action = [[-0.43750328  0.8090062  -0.69319284  0.5005245 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 2167. State = [[-0.25727212 -0.10102098  0.17926915  1.        ]]. Action = [[-0.45099485  0.93259335  0.24002779  0.80363154]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 2168. State = [[-0.24927151 -0.1121014   0.18247929  1.        ]]. Action = [[ 0.8420509  -0.75412697  0.2597382   0.7078366 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2169. State = [[-0.24301754 -0.1245765   0.18442933  1.        ]]. Action = [[-0.94275045 -0.8578011   0.54376006  0.7307265 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2170. State = [[-0.23427074 -0.14164239  0.18861799  1.        ]]. Action = [[ 0.48762274 -0.97461015  0.12901151  0.8666005 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2171. State = [[-0.22587958 -0.15047482  0.1898401   1.        ]]. Action = [[2.8371811e-05 8.0190516e-01 1.4348471e-01 8.4663057e-01]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2172. State = [[-0.2238145  -0.13343689  0.18307285  1.        ]]. Action = [[ 0.0122292   0.6053946  -0.6513493   0.33330786]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2173. State = [[-0.22059438 -0.13186741  0.18526469  1.        ]]. Action = [[-0.07418197 -0.73744273  0.98877263  0.84449077]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2174. State = [[-0.22190504 -0.13041109  0.20806721  1.        ]]. Action = [[-0.30166775  0.66030383  0.7974212   0.8074876 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2175. State = [[-0.22383559 -0.12216425  0.22062273  1.        ]]. Action = [[ 0.42195952  0.04795969 -0.70502436  0.77198386]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2176. State = [[-0.22268385 -0.11930236  0.2094045   1.        ]]. Action = [[-0.15372026  0.14509273 -0.47437757  0.8167957 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2177. State = [[-0.21597292 -0.10230997  0.1925424   1.        ]]. Action = [[ 0.82826614  0.87208986 -0.91997325  0.9063003 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2178. State = [[-0.18855923 -0.08683567  0.1691305   1.        ]]. Action = [[ 0.82383823 -0.12901258  0.06417739  0.9037405 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2179. State = [[-0.1695244  -0.08541007  0.164995    1.        ]]. Action = [[ 0.48167014 -0.8497746   0.4497975   0.8439734 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 2180. State = [[-0.16798717 -0.08446441  0.1653745   1.        ]]. Action = [[ 0.703094    0.6636939  -0.7258866   0.86402774]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Current timestep = 2181. State = [[-0.16740389 -0.08429636  0.16556706  1.        ]]. Action = [[0.2021128  0.34984386 0.23913443 0.91553617]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 2182. State = [[-0.16726543 -0.0843172   0.16562429  1.        ]]. Action = [[ 0.50748014 -0.24990797  0.37798154  0.74234843]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 2183. State = [[-0.1758099  -0.08299794  0.1577961   1.        ]]. Action = [[-0.8307814   0.05075133 -0.709446    0.9218035 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2184. State = [[-0.181851   -0.09660391  0.151547    1.        ]]. Action = [[-0.01211166 -0.84691536  0.8952184   0.9050535 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2185. State = [[-0.19397286 -0.12608851  0.15731966  1.        ]]. Action = [[-0.6443568  -0.93067145 -0.1897884   0.68616986]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2186. State = [[-0.21041904 -0.152427    0.16307646  1.        ]]. Action = [[-0.6423756  -0.44616842  0.37052774  0.5258837 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2187. State = [[-0.22403325 -0.17459029  0.17406051  1.        ]]. Action = [[ 0.4016801  -0.7402222   0.2951057   0.77516675]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2188. State = [[-0.23065545 -0.1845364   0.17743813  1.        ]]. Action = [[-0.7597073   0.44175494  0.01906204  0.8167019 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2189. State = [[-0.23916735 -0.17692956  0.17295118  1.        ]]. Action = [[ 0.26337385  0.33306754 -0.94179887  0.9088354 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2190. State = [[-0.23997599 -0.17750302  0.15678331  1.        ]]. Action = [[-0.08788443 -0.43869984 -0.41255242  0.77587163]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2191. State = [[-0.24462165 -0.17892593  0.14066888  1.        ]]. Action = [[-0.16962886  0.21848965 -0.43051255  0.8538103 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2192. State = [[-0.243684   -0.16399224  0.11933932  1.        ]]. Action = [[ 0.5266359   0.83485126 -0.867268    0.9241209 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2193. State = [[-0.22900349 -0.13352266  0.09460347  1.        ]]. Action = [[ 0.6378051   0.740101   -0.06377757  0.47231364]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2194. State = [[-0.20987    -0.11699426  0.08215573  1.        ]]. Action = [[ 0.6106999  -0.00998139 -0.72472405  0.78096116]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2195. State = [[-0.20053713 -0.10667399  0.0719059   1.        ]]. Action = [[-0.70507246  0.54466677  0.9401622   0.8025873 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2196. State = [[-0.2191005  -0.09302668  0.07421999  1.        ]]. Action = [[-0.80184805  0.35516202 -0.6714672   0.84497285]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2197. State = [[-0.23528138 -0.08086393  0.07536047  1.        ]]. Action = [[-0.262272    0.3069136   0.7103219   0.90187836]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2198. State = [[-0.24179968 -0.07271047  0.08201592  1.        ]]. Action = [[-0.70744234 -0.53066164  0.8742267   0.7576618 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 2199. State = [[-0.24244694 -0.07196204  0.08237863  1.        ]]. Action = [[-0.892175   0.2229265 -0.5247054  0.9229882]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 2200. State = [[-0.24305707 -0.07786364  0.07585075  1.        ]]. Action = [[ 0.32546067 -0.5179936  -0.84090084  0.6217426 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2201. State = [[-0.24440691 -0.08130462  0.06626474  1.        ]]. Action = [[-0.75025463  0.8595849  -0.55087876  0.37058175]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 2202. State = [[-0.24407895 -0.08180444  0.0662405   1.        ]]. Action = [[-0.55582565  0.7427808  -0.33374047  0.86725736]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 2203. State = [[-0.23266758 -0.07229746  0.06865306  1.        ]]. Action = [[0.90353775 0.75672126 0.4043646  0.2980349 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2204. State = [[-0.21996412 -0.06280629  0.07086317  1.        ]]. Action = [[-0.31807017 -0.26517004 -0.9725276   0.83744526]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 2205. State = [[-0.20667952 -0.06293926  0.0643722   1.        ]]. Action = [[ 0.8940704  -0.2288773  -0.80649555  0.9185889 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2206. State = [[-0.17868836 -0.05054795  0.0507778   1.        ]]. Action = [[0.6564145  0.9136826  0.00159824 0.79693794]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2207. State = [[-0.16754894 -0.03365897  0.04972032  1.        ]]. Action = [[-0.36576742  0.10726607  0.2533666   0.57305336]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2208. State = [[-0.16756107 -0.01846334  0.06129803  1.        ]]. Action = [[-0.15438342  0.6309917   0.9762703   0.61146975]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2209. State = [[-0.17054592 -0.00514479  0.08038334  1.        ]]. Action = [[-1.7595291e-04  8.0761778e-01 -4.4940984e-01  6.7550969e-01]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 2210. State = [[-0.17455038 -0.00920707  0.07601681  1.        ]]. Action = [[-0.2505638  -0.40306342 -0.6906605   0.80784094]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2211. State = [[-0.17812403 -0.01268912  0.07021304  1.        ]]. Action = [[ 0.9191078  -0.24608874  0.58567595  0.814883  ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: No entry zone
Current timestep = 2212. State = [[-0.18101452 -0.02225733  0.07707232  1.        ]]. Action = [[-0.57954645 -0.38828886  0.85045505  0.8916049 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2213. State = [[-0.19355394 -0.04432159  0.07995926  1.        ]]. Action = [[-0.1106112  -0.90874314 -0.8442551   0.5636003 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2214. State = [[-0.19386789 -0.07485513  0.06470887  1.        ]]. Action = [[ 0.48150897 -0.9477483  -0.6009073   0.8951547 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2215. State = [[-0.19033998 -0.09684822  0.0524404   1.        ]]. Action = [[-0.23011774  0.6778251  -0.7280287   0.88268805]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 2216. State = [[-0.18965076 -0.09983283  0.05195024  1.        ]]. Action = [[0.6347251  0.22859585 0.54189277 0.29661798]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 2217. State = [[-0.18953665 -0.10021044  0.05196568  1.        ]]. Action = [[-0.78831506  0.6926315  -0.6299041   0.7138152 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 2218. State = [[-0.18953665 -0.10021044  0.05196568  1.        ]]. Action = [[0.64589643 0.9648926  0.08515978 0.72578895]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 2219. State = [[-0.1851334  -0.09503365  0.0550032   1.        ]]. Action = [[0.3011278  0.43939185 0.50452244 0.6879673 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2220. State = [[-0.18010695 -0.08092485  0.06284338  1.        ]]. Action = [[-0.20705402  0.6439029   0.4783523   0.80974483]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2221. State = [[-0.17819522 -0.06771115  0.07100125  1.        ]]. Action = [[-0.26372397 -0.582091   -0.87000704  0.5450511 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 2222. State = [[-0.18762536 -0.0589422   0.06815023  1.        ]]. Action = [[-0.63203424  0.48562515 -0.47353178  0.8578303 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2223. State = [[-0.20646739 -0.04011417  0.05836466  1.        ]]. Action = [[-0.6220083   0.5882131  -0.5951282   0.65268564]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2224. State = [[-0.22842482 -0.02387284  0.0463244   1.        ]]. Action = [[-0.96012515  0.26487637  0.21354389  0.76250887]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2225. State = [[-0.24627738 -0.01451474  0.04384254  1.        ]]. Action = [[ 0.27895355  0.10895681 -0.10437918  0.73954713]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2226. State = [[-0.24637075 -0.01254217  0.04383523  1.        ]]. Action = [[-0.9555955  -0.3463726   0.52977526  0.76274395]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2227. State = [[-0.24515226 -0.02147675  0.04331416  1.        ]]. Action = [[ 0.00684464 -0.7825969  -0.11836791  0.7862084 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2228. State = [[-0.24306917 -0.03020677  0.04297705  1.        ]]. Action = [[ 0.95667195 -0.6223368  -0.9588176   0.6856514 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2229. State = [[-0.24284078 -0.03126991  0.04304899  1.        ]]. Action = [[-0.24783105  0.05070734 -0.6594592   0.31968284]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 2230. State = [[-0.24281523 -0.03136425  0.04304899  1.        ]]. Action = [[-0.91839445  0.40972185  0.33551657  0.5779748 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 2231. State = [[-0.24281523 -0.03136425  0.04304899  1.        ]]. Action = [[ 0.34815478 -0.41092145 -0.9014436   0.8070669 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 2232. State = [[-0.24281523 -0.03136425  0.04304899  1.        ]]. Action = [[-0.7115005  0.7244042  0.6145189  0.8913884]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2233. State = [[-0.24281523 -0.03136425  0.04304899  1.        ]]. Action = [[ 0.58808553  0.8330666  -0.41094655  0.74476767]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2234. State = [[-0.24678046 -0.0349212   0.05066424  1.        ]]. Action = [[-0.362148   -0.11716455  0.95836055  0.71778774]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2235. State = [[-0.2605902  -0.04935603  0.07559793  1.        ]]. Action = [[-0.38292003 -0.6592468   0.93425655  0.7018676 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2236. State = [[-0.2688807  -0.05460211  0.1023264   1.        ]]. Action = [[0.3411262  0.47475612 0.36409807 0.46483266]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2237. State = [[-0.25518647  0.1123909   0.1241822   1.        ]]. Action = [[-0.2555325   0.18025684  0.18484879  0.91797256]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2238. State = [[-0.25107074  0.13756067  0.10685022  1.        ]]. Action = [[ 0.36686492  0.84225416 -0.55592954  0.7895744 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2239. State = [[-0.24107976  0.15599811  0.09101013  1.        ]]. Action = [[-0.9701789  -0.05424917  0.35628545  0.7641392 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 2240. State = [[-0.23766455  0.16845919  0.09551164  1.        ]]. Action = [[0.05449319 0.55575633 0.73008    0.41040802]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2241. State = [[-0.22844644  0.19216168  0.11215711  1.        ]]. Action = [[0.7108666 0.7458103 0.6715435 0.6261046]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2242. State = [[-0.21702503  0.19850785  0.13920791  1.        ]]. Action = [[-0.78082305 -0.73470867  0.8427516   0.8411646 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2243. State = [[-0.2179652   0.18227191  0.15645768  1.        ]]. Action = [[ 0.57050824 -0.4508236  -0.36127657  0.77284217]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2244. State = [[-0.21276331  0.17729607  0.16298965  1.        ]]. Action = [[0.12411475 0.39533412 0.5947139  0.7319522 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2245. State = [[-0.21467993  0.17452405  0.16429496  1.        ]]. Action = [[-0.6648293  -0.58785075 -0.45684206  0.76142347]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2246. State = [[-0.21423185  0.15848169  0.16527939  1.        ]]. Action = [[ 0.4077685 -0.533252   0.4009614  0.7065604]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2247. State = [[-0.21989806  0.1534128   0.18030526  1.        ]]. Action = [[-0.93904054  0.27439642  0.8716955   0.6902102 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2248. State = [[-0.24428087  0.1541305   0.1947765   1.        ]]. Action = [[-0.4590974  -0.1047067  -0.4015239   0.82391524]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2249. State = [[-0.2524485   0.15255824  0.19129907  1.        ]]. Action = [[-0.87949485 -0.84964913  0.57355165  0.7958143 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2250. State = [[-0.25099373  0.13717     0.1912681   1.        ]]. Action = [[ 0.16987956 -0.97817594 -0.1562618   0.759969  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2251. State = [[-0.2557516   0.11402069  0.18096158  1.        ]]. Action = [[-0.17791486 -0.42745256 -0.94489455  0.7773441 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2252. State = [[-0.26217908  0.10340312  0.16208604  1.        ]]. Action = [[-0.3200469  -0.19598818  0.74835205  0.8027334 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2253. State = [[-0.25765964  0.10767557  0.16235612  1.        ]]. Action = [[0.5945995  0.5897269  0.29904294 0.7317064 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2254. State = [[-0.25182676  0.11290748  0.16492364  1.        ]]. Action = [[-0.90115374  0.71854734 -0.3845383   0.66926074]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2255. State = [[-0.25159737  0.11363877  0.1650264   1.        ]]. Action = [[-0.4965453   0.6506114  -0.51015764  0.9339092 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2256. State = [[-0.25147086  0.11394038  0.16505615  1.        ]]. Action = [[-0.96303266 -0.8532587  -0.8724297   0.84179056]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 2257. State = [[-0.2514603   0.11401999  0.16505748  1.        ]]. Action = [[-0.9142932  -0.15912855 -0.628237    0.8332851 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2258. State = [[-0.2514603   0.11401999  0.16505748  1.        ]]. Action = [[-0.62738836  0.7539029  -0.10762095  0.72843003]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2259. State = [[-0.2514603   0.11401999  0.16505748  1.        ]]. Action = [[-0.48981225 -0.56624407 -0.9644804   0.54744124]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2260. State = [[-0.2524927   0.10661647  0.16102253  1.        ]]. Action = [[-0.31776857 -0.70399845 -0.5097872   0.9196799 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2261. State = [[-0.2529786   0.09884312  0.15329896  1.        ]]. Action = [[-0.4822228  -0.9253857  -0.6555223   0.74583244]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2262. State = [[-0.25320277  0.09756741  0.15295726  1.        ]]. Action = [[-0.3483904  0.3672521  0.8427267  0.7225988]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2263. State = [[-0.24451254  0.09957154  0.15634793  1.        ]]. Action = [[0.8310869  0.21277845 0.36535645 0.8299484 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2264. State = [[-0.2318297   0.10204346  0.16133426  1.        ]]. Action = [[0.43346763 0.0361712  0.06412899 0.89678764]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2265. State = [[-0.22860323  0.09571144  0.1560307   1.        ]]. Action = [[-0.54585886 -0.42603076 -0.74980056  0.6669407 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2266. State = [[-0.23604436  0.0895618   0.14343418  1.        ]]. Action = [[-0.6105302   0.04212248  0.34322608  0.7256665 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2267. State = [[-0.23682986  0.09340811  0.1500067   1.        ]]. Action = [[0.4758613  0.39339542 0.58130395 0.8379291 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2268. State = [[-0.22786252  0.08743515  0.1634626   1.        ]]. Action = [[ 0.562834   -0.6715063   0.25554574  0.76736486]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2269. State = [[-0.21220443  0.07218892  0.1632591   1.        ]]. Action = [[ 0.79348505 -0.32192278 -0.62290233  0.37261653]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2270. State = [[-0.19946119  0.06343013  0.14701626  1.        ]]. Action = [[-0.05095685  0.06799042 -0.9684416   0.72579503]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2271. State = [[-0.1929143   0.06346105  0.12623106  1.        ]]. Action = [[ 0.80631185 -0.87531054 -0.42519766  0.7942767 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 2272. State = [[-0.1925707   0.07701222  0.13246155  1.        ]]. Action = [[-0.20643997  0.9429772   0.8532572   0.7392876 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2273. State = [[-0.19972885  0.10050829  0.14062464  1.        ]]. Action = [[-0.4768175   0.32228184 -0.13191605  0.6933992 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2274. State = [[-0.20458809  0.10915424  0.14044932  1.        ]]. Action = [[ 0.6635145  -0.11670661  0.9349997   0.8357552 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: No entry zone
Current timestep = 2275. State = [[-0.21455677  0.11838266  0.14009973  1.        ]]. Action = [[-0.71428746  0.45810974  0.09486866  0.7429371 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2276. State = [[-0.23168045  0.13909468  0.14839739  1.        ]]. Action = [[-0.21828443  0.54501843  0.5577836   0.7191335 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2277. State = [[-0.23309907  0.15883657  0.16938795  1.        ]]. Action = [[0.7535169  0.6294465  0.6765791  0.73522496]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2278. State = [[-0.21899928  0.17062402  0.1796687   1.        ]]. Action = [[ 0.4102682  -0.0242058  -0.50575775  0.81995344]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2279. State = [[-0.20762733  0.17225324  0.18367182  1.        ]]. Action = [[ 0.3039893  -0.0287835   0.60545206  0.7505344 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2280. State = [[-0.19477615  0.16096798  0.18707198  1.        ]]. Action = [[ 0.15527546 -0.78210974 -0.55596656  0.6759745 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2281. State = [[-0.19849044  0.14511281  0.17961757  1.        ]]. Action = [[-0.8202447  -0.35747534 -0.20415592  0.788798  ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2282. State = [[-0.21459569  0.12689507  0.16596974  1.        ]]. Action = [[-0.6493453 -0.6561235 -0.8834589  0.9276649]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2283. State = [[-0.22444172  0.10381687  0.14844628  1.        ]]. Action = [[ 0.6632223  -0.5835916  -0.11440045  0.6727191 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2284. State = [[-0.20795923  0.08197647  0.13242584  1.        ]]. Action = [[ 0.8657992  -0.29058743 -0.9589242   0.78293943]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2285. State = [[-0.20056099  0.08050622  0.09813442  1.        ]]. Action = [[-0.49798274  0.6118183  -0.61516124  0.8847606 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2286. State = [[-0.21052507  0.0757045   0.07269917  1.        ]]. Action = [[-0.60238886 -0.9588012  -0.8904934   0.6374481 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2287. State = [[-0.2145806   0.07143851  0.05726247  1.        ]]. Action = [[0.17837298 0.6218355  0.8026606  0.38780177]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2288. State = [[-0.21104388  0.08838257  0.07312541  1.        ]]. Action = [[0.16679692 0.64752436 0.8220067  0.73702   ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2289. State = [[-0.21000239  0.11298291  0.10266621  1.        ]]. Action = [[0.07173705 0.6982999  0.9410362  0.8006836 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2290. State = [[-0.22044685  0.1355457   0.13187042  1.        ]]. Action = [[-0.8366661   0.38827527  0.55039954  0.42063034]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2291. State = [[-0.22538863  0.1366518   0.14794083  1.        ]]. Action = [[ 0.4882357  -0.6250867   0.14051831  0.6292441 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2292. State = [[-0.2246395   0.12381683  0.15766762  1.        ]]. Action = [[-0.4851985  -0.22885221  0.2767495   0.85453117]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2293. State = [[-0.22156304  0.10978154  0.170043    1.        ]]. Action = [[ 0.7753556  -0.5182714   0.2719394   0.66826737]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2294. State = [[-0.21879807  0.08661528  0.18571854  1.        ]]. Action = [[-0.63896316 -0.89234215  0.6386404   0.45904183]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2295. State = [[-0.21585225  0.07185967  0.20605688  1.        ]]. Action = [[0.9850931  0.2644596  0.25418508 0.71225333]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2296. State = [[-0.19409515  0.07682225  0.22100663  1.        ]]. Action = [[0.8543353  0.15266514 0.3579371  0.5194309 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2297. State = [[-0.17865299  0.08130875  0.23280868  1.        ]]. Action = [[-0.04787892  0.12087929  0.15153193  0.7807666 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2298. State = [[-0.17503196  0.08371515  0.23671733  1.        ]]. Action = [[ 0.8585849 -0.9929301 -0.7842098  0.5317297]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 2299. State = [[-0.17663975  0.08160069  0.24623373  1.        ]]. Action = [[-0.5446473  -0.13112795  0.7501118   0.53098464]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2300. State = [[-0.18617292  0.08965681  0.27119884  1.        ]]. Action = [[-0.33539766  0.72555923  0.88577867  0.49963546]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2301. State = [[-0.19259007  0.08593786  0.28199556  1.        ]]. Action = [[ 0.25333524 -0.9103469  -0.9584153   0.19984412]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2302. State = [[-0.19081005  0.06289457  0.2805324   1.        ]]. Action = [[-0.15454453 -0.7509335   0.2942611   0.79875445]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2303. State = [[-0.18917255  0.05484314  0.27729696  1.        ]]. Action = [[ 0.45474696  0.5522473  -0.6248346   0.7912204 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2304. State = [[-0.17468604  0.05002073  0.27352095  1.        ]]. Action = [[ 0.6852734 -0.5721851  0.6446444  0.8581935]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2305. State = [[-0.16583063  0.05357888  0.28089842  1.        ]]. Action = [[-0.37990993  0.7309334   0.27707648  0.59993494]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2306. State = [[-0.16222055  0.05437981  0.27737528  1.        ]]. Action = [[ 0.5075958  -0.6263319  -0.86316234  0.78393435]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2307. State = [[-0.15463814  0.03386356  0.2700313   1.        ]]. Action = [[-0.29830778 -0.8472756   0.17618966  0.7719259 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2308. State = [[-0.15484296  0.00306264  0.28786469  1.        ]]. Action = [[-0.22796553 -0.8483803   0.9882387   0.5694847 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2309. State = [[-0.15765959 -0.01099016  0.30456746  1.        ]]. Action = [[0.10001457 0.4230224  0.30677032 0.73068976]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2310. State = [[-0.16457933 -0.02416416  0.30553797  1.        ]]. Action = [[-0.3701557  -0.98530215 -0.7583953   0.7697815 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2311. State = [[-0.16822933 -0.03584979  0.29144803  1.        ]]. Action = [[ 0.54882836  0.2603923  -0.5037036   0.801473  ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2312. State = [[-0.15891474 -0.03454098  0.27064106  1.        ]]. Action = [[ 0.3801558   0.05934918 -0.76391697  0.64168024]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2313. State = [[-0.15763883 -0.03105942  0.24136499  1.        ]]. Action = [[-0.89274085  0.3869692  -0.75259316  0.6496891 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2314. State = [[-0.17552066 -0.02133829  0.21398646  1.        ]]. Action = [[-0.85564774  0.30233574 -0.79139566  0.82651925]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2315. State = [[-0.18446507 -0.02824768  0.19972852  1.        ]]. Action = [[ 0.6912291 -0.9747121  0.5221317  0.8571079]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2316. State = [[-0.18635528 -0.05837061  0.19840232  1.        ]]. Action = [[-0.74676144 -0.98453104 -0.6742182   0.4583391 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2317. State = [[-0.20975623 -0.07844876  0.1859094   1.        ]]. Action = [[-0.74497813  0.49730623 -0.1194222   0.8728123 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2318. State = [[-0.22558133 -0.07003064  0.17707224  1.        ]]. Action = [[-0.3750949   0.54787326 -0.33926362  0.908566  ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2319. State = [[-0.2425257  -0.04466962  0.17621383  1.        ]]. Action = [[-0.5218784  -0.5216009   0.34198987  0.5972526 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2320. State = [[-0.26081765 -0.01903159  0.18256506  1.        ]]. Action = [[ 0.03955317  0.17580914 -0.61017174  0.757766  ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2321. State = [[-2.6810068e-01  2.9914321e-05  1.8235524e-01  1.0000000e+00]]. Action = [[-0.613535    0.8484082  -0.52949315  0.4701928 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 2322. State = [[-0.26505736 -0.00403752  0.18633133  1.        ]]. Action = [[ 0.6173725  -0.82203037  0.04504943  0.670872  ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2323. State = [[-0.25895435 -0.01959946  0.18981484  1.        ]]. Action = [[ 0.263803   -0.4982915   0.1415751   0.71756506]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2324. State = [[-0.24958123 -0.03669666  0.1954065   1.        ]]. Action = [[0.09478652 0.52295876 0.32672    0.70386696]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2325. State = [[-0.24367003 -0.03370456  0.1973004   1.        ]]. Action = [[-0.7804665  -0.48010695  0.3258394   0.8787441 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2326. State = [[-0.24027546 -0.03655401  0.19665842  1.        ]]. Action = [[-0.8456297  -0.04345351 -0.9248058   0.6858978 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2327. State = [[-0.23799667 -0.0368775   0.19682129  1.        ]]. Action = [[-0.9944781   0.869282    0.7651026   0.72826314]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2328. State = [[-0.23760696 -0.03988187  0.19694608  1.        ]]. Action = [[-0.86198145 -0.9784492  -0.8023544   0.85925555]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2329. State = [[-0.24147993 -0.03221693  0.18502109  1.        ]]. Action = [[-0.3395028   0.6261785  -0.9941497   0.73510075]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2330. State = [[-0.2447452  -0.01495457  0.16588694  1.        ]]. Action = [[-0.80560493 -0.880741    0.30879056  0.67077553]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 2331. State = [[-0.2443612  -0.00377764  0.15417302  1.        ]]. Action = [[ 0.20742714  0.41362667 -0.77202916  0.68562484]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2332. State = [[-0.23460965 -0.00347693  0.14753498  1.        ]]. Action = [[ 0.14141655 -0.2703852   0.9737642   0.57013917]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2333. State = [[-0.22703035 -0.00202301  0.14666761  1.        ]]. Action = [[ 0.35460806 -0.11516821 -0.73468864  0.53247666]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2334. State = [[-0.2232654  -0.0027539   0.14485906  1.        ]]. Action = [[-0.45026934  0.31886053  0.7595844   0.6781509 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2335. State = [[-0.22654335 -0.00640946  0.14996317  1.        ]]. Action = [[-0.23149431 -0.9874556  -0.2640623   0.7199589 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2336. State = [[-0.23876251 -0.01575626  0.1581069   1.        ]]. Action = [[-0.547309   0.8940816  0.4931271  0.6463778]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2337. State = [[-0.25276756  0.00124018  0.1653193   1.        ]]. Action = [[-0.9251443   0.784551    0.31831694  0.5784025 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2338. State = [[-0.2521264   0.01198927  0.16888326  1.        ]]. Action = [[0.612905   0.5824734  0.01675236 0.8121338 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2339. State = [[-0.2601588   0.12574627  0.09331837  1.        ]]. Action = [[-0.9050689  -0.9861995  -0.50272053  0.8688872 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2340. State = [[-0.25700584  0.14417294  0.07903879  1.        ]]. Action = [[-0.25614965 -0.34282005 -0.72912824  0.36333346]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 2341. State = [[-0.25574434  0.13639782  0.07302298  1.        ]]. Action = [[ 0.13684511 -0.5828766  -0.47390795  0.8469708 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2342. State = [[-0.2548483   0.12795863  0.06154695  1.        ]]. Action = [[-0.8435974  -0.6987305  -0.93923914  0.4631518 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2343. State = [[-0.25436258  0.12624899  0.06132591  1.        ]]. Action = [[-0.7359194   0.45109427  0.5646372   0.7699684 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 2344. State = [[-0.2543773   0.12622772  0.06132893  1.        ]]. Action = [[-0.43904883  0.68670523  0.8179667   0.7507076 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 2345. State = [[-0.24648295  0.11476059  0.06849203  1.        ]]. Action = [[ 0.30700564 -0.69697315  0.91220474  0.81978035]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2346. State = [[-0.2411487   0.11584913  0.08762424  1.        ]]. Action = [[-0.14067173  0.8646952   0.71205306  0.6540661 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2347. State = [[-0.23180723  0.1243225   0.10386202  1.        ]]. Action = [[ 0.8099303  -0.29333192 -0.17829227  0.79091823]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2348. State = [[-0.22868742  0.1331289   0.11068639  1.        ]]. Action = [[-0.66256917  0.6762425   0.3642223   0.80350995]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2349. State = [[-0.23204337  0.14514141  0.10990029  1.        ]]. Action = [[ 0.38306546  0.15228963 -0.4395542   0.7733116 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2350. State = [[-0.22756448  0.14772657  0.10791969  1.        ]]. Action = [[-0.8590475   0.5427005  -0.8938709   0.74612427]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2351. State = [[-0.21675391  0.14901933  0.11256832  1.        ]]. Action = [[ 0.86718917 -0.03326058  0.4419496   0.7662945 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2352. State = [[-0.19556849  0.14423697  0.11229078  1.        ]]. Action = [[ 0.37195826 -0.4865104  -0.852179    0.83094263]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2353. State = [[-0.17664874  0.1333835   0.10552285  1.        ]]. Action = [[ 0.33054566 -0.2470321   0.7627672   0.54063654]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2354. State = [[-0.17790534  0.12837371  0.10531541  1.        ]]. Action = [[-0.9902605   0.00940347 -0.6473776   0.8681277 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2355. State = [[-0.18850404  0.1158518   0.09447781  1.        ]]. Action = [[-0.00341022 -0.74242085 -0.9592404   0.7412219 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2356. State = [[-0.19716658  0.09867842  0.07263758  1.        ]]. Action = [[-0.5111199 -0.3782087  0.4083321  0.8795451]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2357. State = [[-0.21312451  0.09679627  0.06704161  1.        ]]. Action = [[-0.6240769   0.48871982 -0.42401087  0.6234405 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2358. State = [[-0.22154467  0.10015029  0.05752665  1.        ]]. Action = [[ 0.79252625  0.02915621 -0.37585104  0.8013165 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2359. State = [[-0.21956305  0.10037964  0.04858121  1.        ]]. Action = [[-0.46677935 -0.1993345  -0.09627205  0.8094804 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2360. State = [[-0.22191143  0.10955074  0.05506118  1.        ]]. Action = [[0.21166873 0.84282255 0.94264674 0.55291843]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2361. State = [[-0.21888544  0.11433092  0.07160871  1.        ]]. Action = [[-0.09152746 -0.5704103   0.4729246   0.5875089 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2362. State = [[-0.22153474  0.1130757   0.08466957  1.        ]]. Action = [[-0.36243296  0.17996871  0.20366728  0.7803298 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2363. State = [[-0.22235967  0.11871197  0.0942001   1.        ]]. Action = [[0.6683049  0.5156157  0.22690976 0.5894315 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2364. State = [[-0.21055163  0.11994308  0.11119378  1.        ]]. Action = [[ 0.44795334 -0.47058338  0.7277943   0.8359444 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2365. State = [[-0.20741946  0.11194354  0.13993703  1.        ]]. Action = [[-0.88678163 -0.32872677  0.94591737  0.6293864 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2366. State = [[-0.21322313  0.09032798  0.15602279  1.        ]]. Action = [[ 0.3382343  -0.95598465 -0.72849864  0.56139755]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2367. State = [[-0.21455036  0.08373422  0.14707462  1.        ]]. Action = [[-0.2010346  0.7298852 -0.3461401  0.6500659]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2368. State = [[-0.2161048   0.08689551  0.13911021  1.        ]]. Action = [[ 0.2812152  -0.21393311 -0.2172494   0.7852776 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2369. State = [[-0.20246287  0.07075875  0.1438034   1.        ]]. Action = [[ 0.70856214 -0.9513376   0.8873365   0.860106  ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2370. State = [[-0.19092527  0.05580778  0.151777    1.        ]]. Action = [[ 0.79393077 -0.26915014  0.44579172  0.6903237 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 2371. State = [[-0.18452711  0.0384551   0.14880887  1.        ]]. Action = [[ 0.2588346  -0.83035    -0.63927054  0.81386805]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2372. State = [[-0.1784461   0.01954233  0.14294231  1.        ]]. Action = [[ 0.6222054  -0.60950387  0.8898382   0.5115118 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 2373. State = [[-0.17972642  0.00727567  0.14053695  1.        ]]. Action = [[-0.30733395 -0.6366236  -0.24625206  0.85690737]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2374. State = [[-0.17886348 -0.00602171  0.13872011  1.        ]]. Action = [[ 0.82925415 -0.9289526  -0.30259025  0.25827146]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: No entry zone
Current timestep = 2375. State = [[-0.19021106 -0.02154862  0.12786856  1.        ]]. Action = [[-0.8711438  -0.86762047 -0.8013687   0.35825777]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2376. State = [[-0.20886675 -0.03584639  0.11188438  1.        ]]. Action = [[-0.4868058   0.3273548   0.5586693   0.60475874]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2377. State = [[-0.21404727 -0.04178296  0.10771758  1.        ]]. Action = [[ 0.78656054 -0.43339658 -0.869292    0.5541153 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2378. State = [[-0.21480328 -0.05588412  0.09339329  1.        ]]. Action = [[-0.7845385  -0.45881265  0.15803957  0.61142576]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2379. State = [[-0.21444963 -0.06744292  0.08282206  1.        ]]. Action = [[ 0.8928635  -0.15633857 -0.8992626   0.6287098 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2380. State = [[-0.20172887 -0.07215903  0.0717969   1.        ]]. Action = [[ 0.3433267  -0.00946456  0.8424258   0.8496375 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2381. State = [[-0.19287266 -0.06545887  0.07117721  1.        ]]. Action = [[ 0.3339888   0.41446376 -0.8781332   0.7961482 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2382. State = [[-0.1905427  -0.05224545  0.06353226  1.        ]]. Action = [[-0.8151409   0.49603534  0.5562769   0.8683115 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2383. State = [[-0.20127167 -0.05230438  0.07055726  1.        ]]. Action = [[-0.46528494 -0.5509407   0.46501327  0.7589102 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2384. State = [[-0.20761886 -0.05094125  0.08612432  1.        ]]. Action = [[0.36330998 0.44437551 0.35070503 0.45648575]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2385. State = [[-0.20855649 -0.04867094  0.0912054   1.        ]]. Action = [[-0.45476794 -0.15379888 -0.38946962  0.7421348 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2386. State = [[-0.20903586 -0.04116848  0.09998459  1.        ]]. Action = [[0.51281106 0.46198523 0.974473   0.9130484 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2387. State = [[-0.20964496 -0.04189933  0.10679216  1.        ]]. Action = [[-0.30744505 -0.4800228  -0.9639474   0.7969253 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2388. State = [[-0.22289641 -0.03999781  0.09683074  1.        ]]. Action = [[-0.7077893   0.5501903   0.12642753  0.8120558 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2389. State = [[-0.2377469  -0.03821043  0.08637231  1.        ]]. Action = [[-0.17712343 -0.31586725 -0.94213015  0.53233385]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2390. State = [[-0.23719046 -0.04681573  0.07747331  1.        ]]. Action = [[ 0.5067084  -0.49418008  0.9809542   0.7842212 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2391. State = [[-0.23078045 -0.0540542   0.08524449  1.        ]]. Action = [[-0.8468932   0.11476791  0.85686827  0.44665182]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 2392. State = [[-0.23008512 -0.04919665  0.0848366   1.        ]]. Action = [[ 0.22695744  0.49627948 -0.27849972  0.796993  ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2393. State = [[-0.23309726 -0.05460645  0.0777271   1.        ]]. Action = [[-0.45685357 -0.6895382  -0.5848414   0.56387377]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2394. State = [[-0.23415783 -0.07236176  0.07195345  1.        ]]. Action = [[ 0.32538247 -0.4989618   0.23847759  0.8744607 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2395. State = [[-0.22179376 -0.0896941   0.06544323  1.        ]]. Action = [[ 0.89442825 -0.38951385 -0.845865    0.77739763]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2396. State = [[-0.21022831 -0.0950882   0.04820371  1.        ]]. Action = [[-0.46221447  0.24045253 -0.0605405   0.5679796 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2397. State = [[-0.221112   -0.08123181  0.04904334  1.        ]]. Action = [[-0.84575427  0.8885493   0.67009735  0.88233113]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2398. State = [[-0.2289099  -0.05567755  0.06468575  1.        ]]. Action = [[0.5080271  0.6363195  0.7826748  0.31350672]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2399. State = [[-0.22512363 -0.04004851  0.08025524  1.        ]]. Action = [[-0.10133362 -0.2695614  -0.95545197  0.7774019 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 2400. State = [[-0.22133356 -0.04690351  0.08335927  1.        ]]. Action = [[ 0.44265223 -0.69661784  0.17062318  0.73761725]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2401. State = [[-0.20953654 -0.04768771  0.10035753  1.        ]]. Action = [[0.55449605 0.53922844 0.86810124 0.76714814]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2402. State = [[-0.19087334 -0.05481689  0.1317142   1.        ]]. Action = [[ 0.52532804 -0.7835573   0.7654886   0.5975337 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2403. State = [[-0.18662195 -0.05867874  0.14681871  1.        ]]. Action = [[-0.8722953   0.5132179  -0.23638815  0.8712728 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2404. State = [[-0.1976162  -0.05056695  0.14066248  1.        ]]. Action = [[ 0.00851583  0.2615155  -0.78188     0.78959596]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2405. State = [[-0.20000422 -0.03061954  0.13252684  1.        ]]. Action = [[ 0.2778325   0.92760825 -0.04021168  0.5336096 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2406. State = [[-0.20209138 -0.01848369  0.12212155  1.        ]]. Action = [[-0.34921122 -0.39560235 -0.72221756  0.8482331 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2407. State = [[-0.2061147  -0.02025083  0.10319795  1.        ]]. Action = [[ 0.7461244  -0.37032628  0.7281263   0.67379355]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 2408. State = [[-0.20780042 -0.02880828  0.10167082  1.        ]]. Action = [[-0.38527465 -0.5461938   0.12958133  0.8543072 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2409. State = [[-0.21734925 -0.04749988  0.10538732  1.        ]]. Action = [[-0.63786566 -0.5945427   0.44950175  0.8105751 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2410. State = [[-0.24241167 -0.06124493  0.10882286  1.        ]]. Action = [[-0.9617472  -0.05571961 -0.09875214  0.7606523 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2411. State = [[-0.26353678 -0.06466726  0.10567532  1.        ]]. Action = [[-0.6809483  -0.91032153  0.02853417  0.78077483]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 2412. State = [[-0.26483816 -0.06398033  0.10070916  1.        ]]. Action = [[ 0.14209974  0.15471625 -0.6554269   0.75640666]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2413. State = [[-0.26130104 -0.06256341  0.08375381  1.        ]]. Action = [[ 0.7046493  -0.07339191 -0.9662881   0.35388112]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2414. State = [[-0.25585142 -0.06190158  0.05999226  1.        ]]. Action = [[-0.9432791   0.87186897 -0.5927714   0.61659193]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 2415. State = [[-0.25385934 -0.06138519  0.05699242  1.        ]]. Action = [[-0.85646796  0.9688221   0.79941344  0.46728742]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 2416. State = [[-0.25114408 -0.05285527  0.0623734   1.        ]]. Action = [[0.16813397 0.62896144 0.70777345 0.777658  ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2417. State = [[-0.24972793 -0.04453506  0.06676543  1.        ]]. Action = [[-0.7455164   0.3299799  -0.7727428   0.78051484]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 2418. State = [[-0.24807492 -0.03210024  0.07256924  1.        ]]. Action = [[-0.01732177  0.75290775  0.35713255  0.7506027 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2419. State = [[-0.2510668  -0.00472284  0.08177286  1.        ]]. Action = [[-0.13800204  0.74209106  0.14656365  0.72113466]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2420. State = [[-0.24646081  0.01586505  0.0825945   1.        ]]. Action = [[ 0.66405344  0.22522914 -0.4342171   0.6921946 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2421. State = [[-0.23336977  0.02311162  0.07426162  1.        ]]. Action = [[ 0.429335   -0.08739138 -0.98777395  0.798913  ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2422. State = [[-0.22689222  0.0200149   0.04690741  1.        ]]. Action = [[-0.45777547 -0.375502   -0.32887793  0.86595106]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2423. State = [[-2.2731934e-01 -6.2621594e-04  3.7689760e-02  1.0000000e+00]]. Action = [[ 0.19629776 -0.9749136   0.01219499  0.36109948]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2424. State = [[-0.22283699 -0.02052806  0.03572701  1.        ]]. Action = [[-0.97217333 -0.831835    0.6445857   0.821224  ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 2425. State = [[-0.21533947 -0.03630666  0.03668892  1.        ]]. Action = [[ 0.51741457 -0.8633647   0.11747098  0.39530206]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2426. State = [[-0.19544789 -0.05550767  0.04519685  1.        ]]. Action = [[ 0.7875595  -0.13827306  0.8108016   0.89510703]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2427. State = [[-0.17943104 -0.06048619  0.05563252  1.        ]]. Action = [[-0.7637129   0.51241016 -0.74450207  0.4715345 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2428. State = [[-0.17651619 -0.06073983  0.05683829  1.        ]]. Action = [[0.40693426 0.76531816 0.4053042  0.8605101 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 2429. State = [[-0.17535016 -0.06138828  0.05728957  1.        ]]. Action = [[0.7904662  0.05933809 0.28574812 0.61159587]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: No entry zone
Current timestep = 2430. State = [[-0.17504236 -0.06146044  0.0573618   1.        ]]. Action = [[0.37024927 0.9492345  0.40507197 0.8176451 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: No entry zone
Current timestep = 2431. State = [[-0.17651413 -0.05382829  0.0680095   1.        ]]. Action = [[-0.59728163  0.58461964  0.9090345   0.6343709 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2432. State = [[-0.18174373 -0.0467141   0.08727696  1.        ]]. Action = [[ 0.36047006  0.10655391 -0.68397725  0.77305424]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 2433. State = [[-0.18692163 -0.05730781  0.08233276  1.        ]]. Action = [[-0.06040007 -0.8180018  -0.972452    0.791389  ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2434. State = [[-0.19126593 -0.06993563  0.07078881  1.        ]]. Action = [[ 0.9569539   0.73442006 -0.02808177  0.74109864]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 2435. State = [[-0.19810973 -0.07553706  0.0675327   1.        ]]. Action = [[-0.6416363  -0.21096939 -0.06166905  0.8230287 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2436. State = [[-0.20600758 -0.08479495  0.05816889  1.        ]]. Action = [[-0.1731183  -0.24055839 -0.64036137  0.5936005 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2437. State = [[-0.21892509 -0.10076053  0.0458473   1.        ]]. Action = [[-0.5121818  -0.56849885  0.34670496  0.9036288 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2438. State = [[-0.22790545 -0.11275088  0.04494834  1.        ]]. Action = [[-0.7222239  -0.26709467 -0.91130537  0.7235744 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 2439. State = [[-0.23528856 -0.11447094  0.04319104  1.        ]]. Action = [[-0.33160472 -0.03264076 -0.30530745  0.82411623]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2440. State = [[-0.23937912 -0.10096134  0.04172263  1.        ]]. Action = [[0.6767975  0.90405345 0.22485507 0.73582196]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2441. State = [[-0.2528445  -0.03607459  0.12367813  1.        ]]. Action = [[-0.35845554  0.56579876  0.955498    0.7165499 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2442. State = [[-0.25379553 -0.0344369   0.11572155  1.        ]]. Action = [[-0.30277812  0.5016861   0.8541019   0.7848729 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2443. State = [[-0.25409603 -0.0155099   0.12780488  1.        ]]. Action = [[0.30848205 0.9278846  0.18865311 0.81901   ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2444. State = [[-0.25560716  0.00241088  0.1341373   1.        ]]. Action = [[-0.78465986 -0.0569818   0.13892126  0.82971025]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2445. State = [[-0.24621083  0.00303192  0.14502662  1.        ]]. Action = [[ 0.6366265  -0.18694425  0.83815014  0.82807386]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2446. State = [[-0.220205    0.01000135  0.16965865  1.        ]]. Action = [[0.84913313 0.5510547  0.31306303 0.6024511 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2447. State = [[-0.18841776  0.01825123  0.18889724  1.        ]]. Action = [[ 0.83749795 -0.06616884  0.5102718   0.6826048 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2448. State = [[-0.1636437   0.01184584  0.19646023  1.        ]]. Action = [[ 0.3168887  -0.53321123 -0.54776895  0.7599325 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2449. State = [[-0.15584734  0.00615008  0.1906143   1.        ]]. Action = [[ 0.7871982   0.18624735 -0.8266152   0.8232691 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 2450. State = [[-0.16431426 -0.00765367  0.18294781  1.        ]]. Action = [[-0.95697254 -0.7741531  -0.75793993  0.84880304]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2451. State = [[-0.17286932 -0.02219842  0.16848192  1.        ]]. Action = [[ 0.5279703  -0.97215056  0.905437    0.7075877 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 2452. State = [[-0.17650345 -0.01435496  0.15851457  1.        ]]. Action = [[-0.04803348  0.77204347 -0.7544757   0.7149067 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2453. State = [[-0.17839786 -0.01632151  0.13580586  1.        ]]. Action = [[-0.11467373 -0.85872746 -0.29681253  0.72185946]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2454. State = [[-0.17922992 -0.02738611  0.13571015  1.        ]]. Action = [[-0.27500916  0.01878488  0.9566958   0.5518031 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2455. State = [[-0.1918146  -0.04545836  0.15632965  1.        ]]. Action = [[-0.42123175 -0.92829156  0.6714127   0.492545  ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2456. State = [[-0.20757496 -0.07260298  0.18357     1.        ]]. Action = [[-0.31960166 -0.5765164   0.88429785  0.36808848]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2457. State = [[-0.214932   -0.07382981  0.19798051  1.        ]]. Action = [[ 0.39213467  0.8142798  -0.7469456   0.6824808 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2458. State = [[-0.21065293 -0.05509303  0.18610622  1.        ]]. Action = [[ 0.3813728   0.47993124 -0.46387875  0.5933466 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2459. State = [[-0.19785686 -0.04969973  0.1854785   1.        ]]. Action = [[ 0.35139203 -0.36179066  0.91446483  0.5317731 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2460. State = [[-0.19377996 -0.03864232  0.18409437  1.        ]]. Action = [[-0.31706452  0.9255676  -0.91739684  0.6429839 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2461. State = [[-0.20761584 -0.02541577  0.1656213   1.        ]]. Action = [[-0.8876585  -0.06060815 -0.99776196  0.6734437 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2462. State = [[-0.23094444 -0.01341281  0.14831632  1.        ]]. Action = [[-0.825153    0.68123436  0.8418056   0.84501886]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2463. State = [[-0.24808729  0.00135236  0.15668055  1.        ]]. Action = [[-0.8919762   0.95208216  0.9355614   0.6636237 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2464. State = [[-0.24916874  0.00357913  0.1567112   1.        ]]. Action = [[-0.849282    0.19331431 -0.61302596  0.8563862 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2465. State = [[-0.24918503  0.0036254   0.1567112   1.        ]]. Action = [[-0.6369186   0.5715494  -0.5075581   0.45160353]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 2466. State = [[-0.249274    0.00366951  0.1566887   1.        ]]. Action = [[-0.79456824 -0.6385085  -0.7894713   0.6286777 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2467. State = [[-0.24898447  0.00367995  0.15677865  1.        ]]. Action = [[ 0.25560617 -0.050394   -0.01599985  0.5656688 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2468. State = [[-0.23951653 -0.0050697   0.1495313   1.        ]]. Action = [[ 0.85727525 -0.5760121  -0.99623346  0.84494567]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2469. State = [[-0.22576727 -0.02462027  0.123453    1.        ]]. Action = [[ 0.24238002 -0.593395   -0.7333045   0.36863172]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2470. State = [[-0.21418135 -0.0341262   0.10818632  1.        ]]. Action = [[0.23993802 0.28441024 0.2912718  0.64718294]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2471. State = [[-0.20222779 -0.02853418  0.11796066  1.        ]]. Action = [[0.5533545  0.30274236 0.91286635 0.5623419 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2472. State = [[-0.19280656 -0.02654788  0.13745892  1.        ]]. Action = [[-0.59271187 -0.22693199  0.5024594   0.5904131 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2473. State = [[-0.1983069  -0.0279424   0.16261627  1.        ]]. Action = [[-0.32246917  0.00875163  0.9718391   0.5570078 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2474. State = [[-0.2154697  -0.03779615  0.182333    1.        ]]. Action = [[-0.83783317 -0.6018029  -0.26136994  0.72097135]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2475. State = [[-0.23715107 -0.03370342  0.17432529  1.        ]]. Action = [[-0.15084738  0.93558097 -0.9532453   0.5747025 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2476. State = [[-0.23126197 -0.03424845  0.17102756  1.        ]]. Action = [[ 0.942873   -0.92218375  0.9604137   0.71612334]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2477. State = [[-0.22418985 -0.04862851  0.18560311  1.        ]]. Action = [[-0.36226964 -0.27611881  0.58147573  0.9190357 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2478. State = [[-0.22615042 -0.0549778   0.19354358  1.        ]]. Action = [[ 0.3139671   0.2151357  -0.85532755  0.74479365]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2479. State = [[-0.22396886 -0.05572423  0.1885154   1.        ]]. Action = [[-0.07204443 -0.19076085  0.39386892  0.41852427]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2480. State = [[-0.21560897 -0.06025425  0.182981    1.        ]]. Action = [[ 0.8128104  -0.14277565 -0.93246716  0.59331083]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2481. State = [[-0.20124294 -0.05072366  0.16961524  1.        ]]. Action = [[-0.18175304  0.87022805  0.40771508  0.6582339 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2482. State = [[-0.20300634 -0.04191599  0.17652147  1.        ]]. Action = [[-0.5339755  -0.24312234  0.55320156  0.7685106 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2483. State = [[-0.21803229 -0.02975739  0.18539727  1.        ]]. Action = [[-0.71728724  0.7872772  -0.01313972  0.77092195]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2484. State = [[-0.2283677  -0.02411141  0.17978998  1.        ]]. Action = [[ 0.6745355 -0.5609469 -0.9963031  0.7600924]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2485. State = [[-0.21797617 -0.02989256  0.15670682  1.        ]]. Action = [[ 0.5207753   0.01499891 -0.72134334  0.53440666]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2486. State = [[-0.21284056 -0.01752122  0.1277264   1.        ]]. Action = [[-0.40531886  0.8999057  -0.87144655  0.7325443 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2487. State = [[-0.21726625 -0.0041812   0.10214271  1.        ]]. Action = [[-0.20356566 -0.06435519 -0.15717185  0.3733201 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2488. State = [[-0.22715421  0.01002017  0.0872367   1.        ]]. Action = [[-0.53007495  0.83561325 -0.7634583   0.7840545 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2489. State = [[-0.22931573  0.03964504  0.05876176  1.        ]]. Action = [[ 0.95848644  0.8910172  -0.9268735   0.8881556 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2490. State = [[-0.2169677   0.07203107  0.0317673   1.        ]]. Action = [[ 0.08975399  0.84002995 -0.19010139  0.7168913 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2491. State = [[-0.22001359  0.10343221  0.03206532  1.        ]]. Action = [[-0.7568046   0.6541389   0.9774538   0.49560046]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2492. State = [[-0.22968109  0.12335411  0.04162636  1.        ]]. Action = [[ 0.7931104   0.93853474 -0.23223269  0.8890668 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 2493. State = [[-0.23637679  0.11351816  0.05205708  1.        ]]. Action = [[-0.66274583 -0.7558942   0.81154394  0.6947057 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2494. State = [[-0.24992418  0.10048815  0.07065902  1.        ]]. Action = [[-0.71158785  0.19461918  0.32617283  0.79791534]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 2495. State = [[-0.25264677  0.09957946  0.0728654   1.        ]]. Action = [[-0.91904557  0.79032445  0.40688932  0.92315793]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 2496. State = [[-0.24685808  0.1122333   0.07934644  1.        ]]. Action = [[0.6909065  0.9446558  0.39141846 0.8425249 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2497. State = [[-0.22862466  0.12470227  0.08388206  1.        ]]. Action = [[ 0.9862001  -0.18550348 -0.8218128   0.7882068 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2498. State = [[-0.2077721   0.13010101  0.06976755  1.        ]]. Action = [[ 0.18910265  0.28001583 -0.15152365  0.5835171 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2499. State = [[-0.2024502   0.13272591  0.067689    1.        ]]. Action = [[ 0.92881155 -0.22353768  0.5117359   0.56447315]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 2500. State = [[-0.19520459  0.13399515  0.07240541  1.        ]]. Action = [[ 0.341586   -0.04720497  0.5972451   0.5729935 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2501. State = [[-0.18727276  0.14093015  0.0895356   1.        ]]. Action = [[-0.33293295  0.3166082   0.83078897  0.8652928 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2502. State = [[-0.19103211  0.14564472  0.10706957  1.        ]]. Action = [[ 0.5382453  -0.16537738  0.6942313   0.7232448 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 2503. State = [[-0.19172294  0.1454706   0.10852782  1.        ]]. Action = [[0.8438585  0.16510034 0.48028207 0.7401173 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 2504. State = [[-0.19840036  0.1488342   0.11838856  1.        ]]. Action = [[-0.86806715  0.03750288  0.7409431   0.5806899 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2505. State = [[-0.2049232   0.15968233  0.14033131  1.        ]]. Action = [[0.4670496  0.48707366 0.3598057  0.7224033 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2506. State = [[-0.20780039  0.16605952  0.14643882  1.        ]]. Action = [[-0.16869223  0.07748818 -0.54622054  0.8881465 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2507. State = [[-0.20248678  0.16297598  0.14326844  1.        ]]. Action = [[ 0.7165868  -0.3319555  -0.01829964  0.7548164 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2508. State = [[-0.18937346  0.16065443  0.14118584  1.        ]]. Action = [[ 0.3125143  -0.01181412 -0.15223283  0.873075  ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2509. State = [[-0.18021844  0.16077136  0.13527338  1.        ]]. Action = [[ 0.91251063 -0.63262016  0.04128933  0.7527313 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: No entry zone
Current timestep = 2510. State = [[-0.17723863  0.15139776  0.14513823  1.        ]]. Action = [[-0.31909162 -0.56179136  0.9760554   0.6223241 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2511. State = [[-0.18442002  0.14520893  0.15592563  1.        ]]. Action = [[-0.66584176  0.23339272 -0.5556653   0.23072433]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2512. State = [[-0.1957927   0.15663679  0.15790455  1.        ]]. Action = [[-0.32905507  0.45623624  0.47048616  0.84540033]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2513. State = [[-0.20522127  0.16616738  0.16067396  1.        ]]. Action = [[ 0.68226385 -0.07473516  0.92063975  0.7145587 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 2514. State = [[-0.21846038  0.17059211  0.17300539  1.        ]]. Action = [[-0.8655146   0.08679783  0.7640809   0.5906035 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2515. State = [[-0.23100708  0.17148621  0.18843035  1.        ]]. Action = [[ 0.70043457  0.07322133 -0.1843741   0.76596284]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2516. State = [[-0.21918167  0.15752508  0.19885607  1.        ]]. Action = [[ 0.56276417 -0.8918881   0.73526573  0.6348634 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2517. State = [[-0.21717234  0.14220682  0.22506407  1.        ]]. Action = [[-0.92225015 -0.07258886  0.96865106  0.45526195]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2518. State = [[-0.23366807  0.13855153  0.23958066  1.        ]]. Action = [[-0.42297256  0.10204697 -0.890521    0.56752455]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2519. State = [[-0.23573804  0.12247541  0.24199651  1.        ]]. Action = [[ 0.42881453 -0.99195975  0.65586317  0.7215828 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2520. State = [[-0.23973757  0.10978758  0.2426089   1.        ]]. Action = [[-0.5287412   0.2054764  -0.5421855   0.87592125]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2521. State = [[-0.24081022  0.1021616   0.23569992  1.        ]]. Action = [[ 0.6998911  -0.33176887 -0.18092424  0.9065676 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2522. State = [[-0.22196592  0.09185711  0.24171314  1.        ]]. Action = [[ 0.9007281  -0.32347584  0.8220974   0.62147117]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2523. State = [[-0.1949617   0.07371221  0.260975    1.        ]]. Action = [[ 0.903893   -0.69343936  0.4953022   0.7865677 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2524. State = [[-0.16456643  0.06054685  0.27837715  1.        ]]. Action = [[0.8151634  0.1268326  0.38058734 0.7939087 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2525. State = [[-0.1333748   0.07136708  0.29847738  1.        ]]. Action = [[0.89164746 0.6901388  0.8506899  0.7308241 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2526. State = [[-0.10815989  0.09520264  0.3109779   1.        ]]. Action = [[ 0.11464977  0.80716693 -0.3958512   0.7648783 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2527. State = [[-0.09338153  0.10116464  0.29831046  1.        ]]. Action = [[ 0.58633983 -0.58067036 -0.6504737   0.7527001 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2528. State = [[-0.07171535  0.08743999  0.2954967   1.        ]]. Action = [[ 0.53792787 -0.5483191   0.59941626  0.72230124]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2529. State = [[-0.05685962  0.08377867  0.3148161   1.        ]]. Action = [[0.03019011 0.4001739  0.7975693  0.7256224 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2530. State = [[-0.04901451  0.08092634  0.34316787  1.        ]]. Action = [[ 0.01352942 -0.4693544   0.7855785   0.6294873 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2531. State = [[-0.03868817  0.06868428  0.37577167  1.        ]]. Action = [[ 0.70292675 -0.27621424  0.8466952   0.726737  ]]. Reward = [0.]
Curr episode timestep = 89
Above hoop
Current timestep = 2532. State = [[-0.02739342  0.04959899  0.3870512   1.        ]]. Action = [[-0.0759123  -0.89027566 -0.93949723  0.75997424]]. Reward = [0.]
Curr episode timestep = 90
Above hoop
Current timestep = 2533. State = [[-0.02594516  0.02023735  0.3653269   1.        ]]. Action = [[-0.7422066  -0.7902275  -0.84674203  0.88455415]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 2534. State = [[-0.02871755  0.0113464   0.3454814   1.        ]]. Action = [[ 0.9793408   0.6118088  -0.11607355  0.44531047]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 2535. State = [[-0.02427925  0.02324485  0.32565624  1.        ]]. Action = [[-0.8979071   0.33816957 -0.9061545   0.30780423]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Current timestep = 2536. State = [[-0.03998557  0.0273528   0.31182477  1.        ]]. Action = [[-0.94466573 -0.21984512  0.10616183  0.7317641 ]]. Reward = [0.]
Curr episode timestep = 94
Above hoop
Current timestep = 2537. State = [[-0.04971372  0.02530702  0.31921384  1.        ]]. Action = [[ 0.7207527  -0.04614222  0.9615278   0.83560634]]. Reward = [0.]
Curr episode timestep = 95
Above hoop
Current timestep = 2538. State = [[-0.04863243  0.01594649  0.34252197  1.        ]]. Action = [[-0.2735678  -0.37816846  0.6968725   0.532439  ]]. Reward = [0.]
Curr episode timestep = 96
Above hoop
Current timestep = 2539. State = [[-0.0511298  -0.00431973  0.3610283   1.        ]]. Action = [[ 0.21342063 -0.87924683 -0.06325161  0.70623934]]. Reward = [0.]
Curr episode timestep = 97
Above hoop
Current timestep = 2540. State = [[-0.0551288  -0.01656951  0.3549479   1.        ]]. Action = [[-0.5614795   0.30774522 -0.8174504   0.8181126 ]]. Reward = [0.]
Curr episode timestep = 98
Above hoop
Current timestep = 2541. State = [[-0.0710007  -0.02930856  0.33445603  1.        ]]. Action = [[-0.6637763  -0.8087567  -0.93686223  0.77744234]]. Reward = [0.]
Curr episode timestep = 99
Above hoop
Current timestep = 2542. State = [[-0.09645649 -0.05361932  0.30651662  1.        ]]. Action = [[-0.7961809 -0.568518  -0.3848251  0.8018553]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2543. State = [[-0.26484627  0.04642544  0.10719158  1.        ]]. Action = [[-0.7351917  -0.10834312 -0.7554658   0.841377  ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2544. State = [[-0.25261098  0.05018958  0.08630814  1.        ]]. Action = [[ 0.82378113 -0.2825443  -0.74085385  0.82182825]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2545. State = [[-0.23637944  0.04803014  0.06685956  1.        ]]. Action = [[-0.37103152  0.5735153   0.3159547   0.18293107]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 2546. State = [[-0.22186957  0.05820275  0.07409284  1.        ]]. Action = [[0.8686607  0.7682731  0.9238925  0.74118316]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2547. State = [[-0.19365562  0.06586774  0.09001071  1.        ]]. Action = [[ 0.68549514 -0.3853134   0.35730994  0.75401115]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2548. State = [[-0.17676531  0.06882623  0.10952647  1.        ]]. Action = [[-0.16946977  0.4173131   0.91980946  0.7268994 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2549. State = [[-0.18081231  0.08408146  0.14383012  1.        ]]. Action = [[-0.6983876   0.48016     0.9368632   0.84153736]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2550. State = [[-0.19016011  0.09539159  0.16898163  1.        ]]. Action = [[0.88343203 0.8752551  0.71610653 0.7532562 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 2551. State = [[-0.19377251  0.09729367  0.17069978  1.        ]]. Action = [[0.8604814  0.30014157 0.06780231 0.8613831 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 2552. State = [[-0.20206751  0.11249763  0.17751426  1.        ]]. Action = [[-0.72312963  0.9099121   0.42523396  0.762563  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2553. State = [[-0.22877216  0.14237563  0.18880665  1.        ]]. Action = [[-0.8807193   0.5775187   0.05238008  0.48173094]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2554. State = [[-0.2457034   0.15709023  0.19236122  1.        ]]. Action = [[-0.91890043  0.8066006   0.13286614  0.8538327 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2555. State = [[-0.2521584   0.16572253  0.19896908  1.        ]]. Action = [[-0.17097652  0.31974542  0.36725068  0.4012146 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2556. State = [[-0.2570059   0.1710775   0.20393331  1.        ]]. Action = [[-0.5227164  -0.25238407  0.86767745  0.5344887 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2557. State = [[-0.25956517  0.16846539  0.20393205  1.        ]]. Action = [[-0.07474339 -0.25996202 -0.3726684   0.7902291 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2558. State = [[-0.2614886   0.16571859  0.20276898  1.        ]]. Action = [[-0.33757615 -0.3328985   0.46769106  0.752198  ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2559. State = [[-0.2624219   0.1646694   0.20225103  1.        ]]. Action = [[-0.62081575  0.09530604 -0.7646079   0.907693  ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2560. State = [[-0.26343217  0.16398598  0.2016999   1.        ]]. Action = [[-0.9209888  -0.4799416   0.22836185  0.79564905]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2561. State = [[-0.2637375   0.16367348  0.20153688  1.        ]]. Action = [[-0.7627017  -0.6465345  -0.41122162  0.7442192 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2562. State = [[-0.25784168  0.15647787  0.19580887  1.        ]]. Action = [[ 0.659057   -0.37260002 -0.5021411   0.7875874 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2563. State = [[-0.24855241  0.15315567  0.19019043  1.        ]]. Action = [[0.06131542 0.27461576 0.15427494 0.7406528 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2564. State = [[-0.24158953  0.14884387  0.18323793  1.        ]]. Action = [[ 0.47003627 -0.34173584 -0.79361093  0.6731677 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2565. State = [[-0.23668559  0.14549403  0.16342905  1.        ]]. Action = [[-0.47056973  0.01883197 -0.04464948  0.7704742 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2566. State = [[-0.23125635  0.1578527   0.15946928  1.        ]]. Action = [[ 0.7777238   0.9047868  -0.10015142  0.7317076 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2567. State = [[-0.2238502   0.16216074  0.1625932   1.        ]]. Action = [[-0.7266493  -0.70963466  0.7863369   0.8160416 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2568. State = [[-0.22635894  0.15797685  0.17921032  1.        ]]. Action = [[0.31888056 0.14994967 0.4606204  0.45729578]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2569. State = [[-0.21726984  0.1457483   0.18076275  1.        ]]. Action = [[ 0.8498938  -0.7054535  -0.9355166   0.72614884]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2570. State = [[-0.19260299  0.12203904  0.16370517  1.        ]]. Action = [[ 0.95278    -0.6865507  -0.53787386  0.76297724]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2571. State = [[-0.16888252  0.10739204  0.14850183  1.        ]]. Action = [[ 0.9610772  -0.06509769 -0.86187667  0.63137543]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 2572. State = [[-0.16487575  0.10008708  0.15068598  1.        ]]. Action = [[-0.2390238  -0.37129462  0.513216    0.81787395]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2573. State = [[-0.26520926  0.13841538  0.11720897  1.        ]]. Action = [[-0.95692354 -0.40760994  0.14601982 -0.05351931]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2574. State = [[-0.24786885  0.14563501  0.11072636  1.        ]]. Action = [[ 0.9076134  -0.63122135  0.81509376  0.6980083 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2575. State = [[-0.2310528   0.12414656  0.12054636  1.        ]]. Action = [[-0.01046467 -0.65478504  0.26775467  0.60686314]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2576. State = [[-0.21537466  0.09986102  0.13517568  1.        ]]. Action = [[ 0.7925832  -0.78577757  0.42221928  0.72741807]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2577. State = [[-0.20874934  0.08504792  0.13786907  1.        ]]. Action = [[-0.8103867  -0.10411096 -0.70111775  0.7648375 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2578. State = [[-0.22555692  0.08556434  0.13358547  1.        ]]. Action = [[-0.97884935  0.25518048  0.20297062  0.8600236 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2579. State = [[-0.2532724   0.08696829  0.14119127  1.        ]]. Action = [[-0.7765545  -0.04424667  0.9008676   0.770957  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2580. State = [[-0.2748086   0.08574115  0.1601048   1.        ]]. Action = [[-0.6770493  -0.5282663  -0.9713261   0.71741486]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 2581. State = [[-0.2750076   0.08413546  0.16498825  1.        ]]. Action = [[ 0.5215895  -0.07926697  0.30014646  0.366248  ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2582. State = [[-0.27164236  0.08338757  0.17155026  1.        ]]. Action = [[-0.7163291   0.04043829 -0.89968127  0.74179375]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2583. State = [[-0.2632334   0.06815543  0.1730825   1.        ]]. Action = [[ 0.63732445 -0.8614096  -0.26375818  0.8395581 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2584. State = [[-0.24512474  0.06367257  0.18459465  1.        ]]. Action = [[0.5053477  0.7166295  0.9747536  0.76904404]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2585. State = [[-0.23108149  0.07016753  0.19793205  1.        ]]. Action = [[-0.8596422  -0.94026244 -0.28936887  0.38974488]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2586. State = [[-0.2182035   0.05682273  0.21343534  1.        ]]. Action = [[ 0.570338   -0.96472794  0.8957293   0.84322596]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2587. State = [[-0.21641394  0.05426066  0.2304369   1.        ]]. Action = [[-0.8762289   0.67776513 -0.10924941  0.82209134]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2588. State = [[-0.22358516  0.05410606  0.2340137   1.        ]]. Action = [[-0.00185525 -0.5854632   0.04738998  0.287822  ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2589. State = [[-0.22810277  0.03146559  0.22810854  1.        ]]. Action = [[-0.06248742 -0.85428625 -0.9610329   0.79264116]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2590. State = [[-0.22222844  0.00190227  0.21553726  1.        ]]. Action = [[ 0.63543665 -0.9046734  -0.2198087   0.73546696]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2591. State = [[-0.20339137 -0.02295988  0.21245     1.        ]]. Action = [[ 0.8131678  -0.04041868  0.35072172  0.6066787 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2592. State = [[-0.19025183 -0.03931239  0.20601648  1.        ]]. Action = [[ 0.15623152 -0.6713067  -0.98483425  0.47951317]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2593. State = [[-0.18375698 -0.05774486  0.17283006  1.        ]]. Action = [[ 0.05544305 -0.16672033 -0.93225795  0.6293566 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2594. State = [[-0.18241528 -0.06222561  0.14742404  1.        ]]. Action = [[ 0.38437068 -0.18135041  0.33956337  0.82932997]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 2595. State = [[-0.18917486 -0.04984239  0.13904607  1.        ]]. Action = [[-0.73466915  0.93640804 -0.57078177  0.57029843]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2596. State = [[-0.19498955 -0.03030762  0.12363965  1.        ]]. Action = [[-0.28407955  0.27171063 -0.12976527  0.5882623 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2597. State = [[-0.1991786  -0.03706431  0.12116254  1.        ]]. Action = [[ 0.45669413 -0.9494212   0.32731462  0.8010833 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2598. State = [[-0.19354036 -0.04884428  0.13094549  1.        ]]. Action = [[-0.06966883 -0.01363361  0.77951705  0.74763715]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2599. State = [[-0.20208235 -0.06634717  0.14913328  1.        ]]. Action = [[-0.7407301  -0.820294    0.46040094  0.8728663 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2600. State = [[-0.20785165 -0.06883611  0.17264877  1.        ]]. Action = [[0.6646348 0.8281472 0.8074074 0.7393992]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2601. State = [[-0.20346342 -0.05943045  0.1891134   1.        ]]. Action = [[ 0.8847282  -0.01403993 -0.86756516  0.7709032 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Current timestep = 2602. State = [[-0.19507101 -0.07021809  0.19220322  1.        ]]. Action = [[ 0.66960037 -0.8793519  -0.16053677  0.66561043]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2603. State = [[-0.18321799 -0.09800693  0.1975074   1.        ]]. Action = [[ 0.19965374 -0.88331264  0.1627698   0.7677231 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2604. State = [[-0.17660916 -0.11665725  0.2002884   1.        ]]. Action = [[0.60524774 0.787029   0.2153008  0.7349162 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 2605. State = [[-0.17535454 -0.12733726  0.21175273  1.        ]]. Action = [[-0.15870714 -0.43530124  0.91323733  0.5773039 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2606. State = [[-0.18058631 -0.12362697  0.23559165  1.        ]]. Action = [[-0.67704326  0.9521867   0.7292192   0.6777612 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2607. State = [[-0.20135203 -0.11962922  0.24864173  1.        ]]. Action = [[-0.7962723  -0.5257394  -0.7334354   0.67208624]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2608. State = [[-0.22812758 -0.13332985  0.24062705  1.        ]]. Action = [[-0.8147938  -0.39336127 -0.0824914   0.21907282]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2609. State = [[-0.23496509 -0.13581978  0.23771152  1.        ]]. Action = [[ 0.7799065   0.41836894 -0.27266508  0.6743605 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2610. State = [[-0.22669482 -0.11608843  0.2408974   1.        ]]. Action = [[0.16613746 0.8956332  0.7599908  0.77453995]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2611. State = [[-0.23144664 -0.08707283  0.24142596  1.        ]]. Action = [[-0.6469869   0.6720921  -0.740506    0.59268904]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2612. State = [[-0.2349882  -0.05893869  0.24244985  1.        ]]. Action = [[0.37522006 0.8595898  0.8060355  0.5482937 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2613. State = [[-0.234988   -0.02431814  0.23925531  1.        ]]. Action = [[ 0.03433657  0.7702774  -0.9220769   0.7984593 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2614. State = [[-0.23627155 -0.0214201   0.23870333  1.        ]]. Action = [[-0.44579566 -0.822468    0.80380595  0.74907565]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2615. State = [[-0.23842692 -0.0346679   0.24739657  1.        ]]. Action = [[ 0.2158227  -0.26397586  0.03939223  0.27477503]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2616. State = [[-0.24067478 -0.03020459  0.24415603  1.        ]]. Action = [[-0.16131908  0.72971    -0.6600649   0.48024058]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2617. State = [[-0.24610752 -0.00571609  0.24134108  1.        ]]. Action = [[-0.09227902  0.9259019   0.56432045  0.73160076]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2618. State = [[-0.24943286  0.00213997  0.23837566  1.        ]]. Action = [[-0.02199954 -0.90521246 -0.82935333  0.73826814]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2619. State = [[-0.24109758 -0.01566093  0.22555038  1.        ]]. Action = [[ 0.61641014 -0.5218677  -0.42685562  0.7879778 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2620. State = [[-0.23441485 -0.01658539  0.20190634  1.        ]]. Action = [[-0.10315377  0.8633976  -0.8706735   0.6660781 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2621. State = [[-0.235007    0.00934855  0.17396829  1.        ]]. Action = [[-0.2787971   0.8825642  -0.55733126  0.74016285]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2622. State = [[-0.23585849  0.02790276  0.15945792  1.        ]]. Action = [[-0.773916   -0.6648092   0.5241363   0.47323442]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2623. State = [[-0.24612705  0.02513789  0.14662938  1.        ]]. Action = [[-0.6073069  -0.3451841  -0.9626003   0.84704566]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2624. State = [[-0.25677106  0.03232636  0.11382826  1.        ]]. Action = [[ 0.16988611  0.66901326 -0.70479846  0.3403089 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2625. State = [[-0.24676956  0.04267515  0.09805603  1.        ]]. Action = [[ 0.7654033  -0.24765623  0.42767715  0.69236803]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2626. State = [[-0.22987603  0.0290949   0.10729145  1.        ]]. Action = [[ 0.62570786 -0.80085623  0.6785989   0.7578865 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2627. State = [[-0.20380822  0.00533862  0.12523969  1.        ]]. Action = [[ 0.88433766 -0.61870426  0.6631206   0.8288162 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2628. State = [[-0.17592408 -0.02530621  0.15352617  1.        ]]. Action = [[ 0.6091933  -0.9835854   0.85686064  0.40558124]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2629. State = [[-0.16205786 -0.03981867  0.17144544  1.        ]]. Action = [[-0.1832385   0.4289937  -0.16710097  0.40020347]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2630. State = [[-0.16239499 -0.0383785   0.17140429  1.        ]]. Action = [[ 0.29421127  0.60925734 -0.9073005   0.6015141 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 2631. State = [[-0.16429137 -0.04391106  0.1730327   1.        ]]. Action = [[-0.3921355  -0.34650612  0.14362705  0.46831965]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2632. State = [[-0.17095768 -0.04202398  0.18366724  1.        ]]. Action = [[-0.61654854  0.4490106   0.6856531   0.4024043 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2633. State = [[-0.18544014 -0.03641756  0.20083869  1.        ]]. Action = [[ 0.43601334 -0.9319786  -0.85891634  0.4838364 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 2634. State = [[-0.19005041 -0.05086208  0.20881563  1.        ]]. Action = [[-0.25743508 -0.9883117   0.43817818  0.49259055]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2635. State = [[-0.20362815 -0.08376273  0.21654941  1.        ]]. Action = [[-0.3219415  -0.90437794 -0.66499424  0.6791886 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2636. State = [[-0.21169764 -0.09468691  0.21710502  1.        ]]. Action = [[-0.17078632  0.6391562   0.6569954   0.8217702 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2637. State = [[-0.2159767  -0.0785473   0.22355354  1.        ]]. Action = [[-0.166821    0.6404238   0.02658129  0.67950857]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2638. State = [[-0.21987028 -0.06719603  0.22117952  1.        ]]. Action = [[ 0.26126862 -0.15197837 -0.45334774  0.82927346]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2639. State = [[-0.22018138 -0.07132529  0.21538895  1.        ]]. Action = [[-0.04795176 -0.19945496 -0.49681985  0.6061795 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2640. State = [[-0.21445763 -0.06046173  0.19647428  1.        ]]. Action = [[ 0.7322402   0.9357575  -0.91188514  0.62148345]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2641. State = [[-0.20013285 -0.05740534  0.16215219  1.        ]]. Action = [[ 0.1759938 -0.8911737 -0.7537865  0.488245 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2642. State = [[-0.19343528 -0.06526009  0.14290161  1.        ]]. Action = [[0.81637    0.33069897 0.92391217 0.5743084 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 2643. State = [[-0.19693522 -0.05864167  0.13488184  1.        ]]. Action = [[-0.5170047  -0.05493248 -0.7602697   0.4986887 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2644. State = [[-0.19686505 -0.04835559  0.12776576  1.        ]]. Action = [[0.47847092 0.60414815 0.6334629  0.7140155 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2645. State = [[-0.1908355  -0.04239517  0.13034694  1.        ]]. Action = [[0.59277916 0.8906361  0.2110225  0.63022625]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 2646. State = [[-0.1899756  -0.03853564  0.13082631  1.        ]]. Action = [[0.9051616  0.953189   0.4581219  0.42988467]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: No entry zone
Current timestep = 2647. State = [[-0.19945565 -0.03367909  0.12456422  1.        ]]. Action = [[-0.83927286  0.08472109 -0.7783753   0.70950437]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2648. State = [[-0.20972487 -0.01342548  0.11916878  1.        ]]. Action = [[ 0.6737261  -0.62041044  0.8845501   0.47015882]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 2649. State = [[-0.218061   -0.01158223  0.11771595  1.        ]]. Action = [[-0.35889316 -0.9817146  -0.44603932  0.67055273]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2650. State = [[-0.22698334 -0.02911383  0.11495855  1.        ]]. Action = [[-0.37206936 -0.17078781  0.4412564   0.76824474]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2651. State = [[-0.23666966 -0.03091362  0.11812639  1.        ]]. Action = [[-0.57080597  0.7839234   0.2171936   0.7737131 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2652. State = [[-0.25147364 -0.0106866   0.12281437  1.        ]]. Action = [[-0.8869071   0.03150213 -0.6277945   0.7497828 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 2653. State = [[-0.2553036  -0.00150171  0.1300238   1.        ]]. Action = [[0.3541423  0.9919001  0.55295634 0.72235286]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2654. State = [[-0.25501895  0.00655387  0.13625072  1.        ]]. Action = [[-0.4406891   0.6852932   0.12871921  0.65308046]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2655. State = [[-0.24204136  0.01236674  0.14452665  1.        ]]. Action = [[0.9187386  0.94444036 0.82365465 0.29819214]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2656. State = [[-0.22721295  0.02037287  0.1526064   1.        ]]. Action = [[-0.6817959  -0.4603334   0.3832115   0.64123774]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 2657. State = [[-0.22835802  0.01563311  0.16111733  1.        ]]. Action = [[-0.4484719  -0.75024015  0.36376762  0.8062329 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2658. State = [[-0.24070182  0.01033663  0.17004003  1.        ]]. Action = [[-0.697645   -0.23412746 -0.34457803  0.49739742]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2659. State = [[-0.2555157   0.01955363  0.16907904  1.        ]]. Action = [[ 0.243819    0.9193953  -0.05952287  0.4753828 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2660. State = [[-0.2531401   0.04341732  0.16105665  1.        ]]. Action = [[ 0.7293775  0.5677066 -0.9004081  0.66905  ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2661. State = [[-0.24004582  0.05688274  0.14318815  1.        ]]. Action = [[-0.7552666   0.5424942  -0.7709464   0.74482787]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 2662. State = [[-0.22913796  0.05802494  0.1443681   1.        ]]. Action = [[ 0.5830085  -0.14676905  0.1042285   0.7563484 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2663. State = [[-0.22148734  0.06059623  0.1418926   1.        ]]. Action = [[-0.21526569 -0.46096814 -0.13618481  0.20001578]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2664. State = [[-0.22197734  0.06055591  0.14130607  1.        ]]. Action = [[-0.93839496  0.11114526 -0.19737244  0.57481337]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 2665. State = [[-0.20901996  0.06063586  0.14691556  1.        ]]. Action = [[0.92052877 0.7236898  0.7237673  0.85111046]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2666. State = [[-0.18725406  0.05770167  0.15586412  1.        ]]. Action = [[ 0.6924417  -0.44759607  0.12337971  0.7131777 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2667. State = [[-0.17101304  0.04164269  0.15761305  1.        ]]. Action = [[ 0.67069244 -0.6566343   0.9308152   0.8126744 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 2668. State = [[-0.16634203  0.03944711  0.15651768  1.        ]]. Action = [[-0.11234313  0.5633199   0.07261109  0.85591745]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2669. State = [[-0.1660132   0.04139544  0.1562653   1.        ]]. Action = [[ 0.5782826  -0.47966003  0.42476606  0.8843434 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 2670. State = [[-0.16611274  0.04142895  0.15618084  1.        ]]. Action = [[-0.03781474 -0.04932153 -0.82752365  0.5595777 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 2671. State = [[-0.16611274  0.04142895  0.15618084  1.        ]]. Action = [[ 0.06614113  0.6345494  -0.17636347  0.7264037 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 2672. State = [[-0.16611274  0.04142895  0.15618084  1.        ]]. Action = [[ 0.2949735  -0.3918445  -0.8456835   0.48501372]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 2673. State = [[-0.16580442  0.05694107  0.15740539  1.        ]]. Action = [[-0.1303109   0.9611803   0.11004221  0.84649324]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2674. State = [[-0.1684562   0.07321529  0.1581335   1.        ]]. Action = [[ 0.5854285   0.53373265 -0.90921104  0.7598522 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: No entry zone
Current timestep = 2675. State = [[-0.25197008 -0.10147275  0.11022716  1.        ]]. Action = [[-0.5054929  -0.5553577   0.87750244  0.85054755]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2676. State = [[-0.24845082 -0.10692053  0.0999352   1.        ]]. Action = [[0.4140799  0.529058   0.67408156 0.61779046]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2677. State = [[-0.23599811 -0.10565941  0.115381    1.        ]]. Action = [[ 0.5396011  -0.34409803  0.7515322   0.90552115]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2678. State = [[-0.22673719 -0.11823843  0.13766012  1.        ]]. Action = [[-0.3593065  -0.59742874  0.38248277  0.58044326]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2679. State = [[-0.23485649 -0.11607783  0.14372782  1.        ]]. Action = [[-0.4523852  0.9122648 -0.4591143  0.6723528]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2680. State = [[-0.24483168 -0.09163073  0.13380472  1.        ]]. Action = [[-0.16337556  0.8562901  -0.76184666  0.7798513 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2681. State = [[-0.24876837 -0.05773564  0.11167697  1.        ]]. Action = [[ 0.19179189  0.9380815  -0.861456    0.6600603 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2682. State = [[-0.24117534 -0.02897287  0.09416953  1.        ]]. Action = [[0.6694256  0.47879314 0.3263209  0.6215179 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2683. State = [[-0.23144789 -0.01755021  0.09546716  1.        ]]. Action = [[-0.71418965  0.232198    0.93161666  0.6498623 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2684. State = [[-0.22593862 -0.00244673  0.09767338  1.        ]]. Action = [[0.30371535 0.8811697  0.36226046 0.8109138 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2685. State = [[-0.22873907  0.01993457  0.09298541  1.        ]]. Action = [[-0.69027805  0.14526832 -0.9632454   0.86819136]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2686. State = [[-0.22584645  0.0157649   0.08169475  1.        ]]. Action = [[ 0.8509698  -0.6600361   0.33899367  0.28216672]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2687. State = [[-0.21900134  0.01442574  0.08368716  1.        ]]. Action = [[-0.6621187   0.42889667 -0.01128262  0.4865787 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2688. State = [[-0.23227581  0.01240343  0.08553232  1.        ]]. Action = [[-0.8707082  -0.39217067  0.46720564  0.7679242 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2689. State = [[-0.24588093  0.00894956  0.09692962  1.        ]]. Action = [[0.06923091 0.21941829 0.4265951  0.86194444]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2690. State = [[-0.24703781  0.01295305  0.10566801  1.        ]]. Action = [[-0.9509368   0.10430992 -0.60346496  0.80554545]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2691. State = [[-2.4287140e-01 -7.4157829e-04  1.0337866e-01  1.0000000e+00]]. Action = [[ 0.69454455 -0.9177979  -0.56325656  0.78853285]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2692. State = [[-0.23804833 -0.01549243  0.09921104  1.        ]]. Action = [[-0.60662425 -0.42766285  0.6800833   0.6282878 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2693. State = [[-0.23746276 -0.01752598  0.09912386  1.        ]]. Action = [[-0.94865865  0.06405377 -0.7419502   0.5915618 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2694. State = [[-0.24219176 -0.0110762   0.09860281  1.        ]]. Action = [[-0.61771095  0.5630199   0.17113638  0.5062908 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2695. State = [[-0.24369748 -0.01677663  0.10217116  1.        ]]. Action = [[ 0.06273556 -0.94348866  0.42340732  0.7804949 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2696. State = [[-0.24095663 -0.04228352  0.10026732  1.        ]]. Action = [[ 0.52728117 -0.7908892  -0.8723633   0.8852763 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2697. State = [[-0.2410727  -0.07513873  0.08420338  1.        ]]. Action = [[-0.44282162 -0.93860835 -0.60527676  0.37686098]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2698. State = [[-0.24493621 -0.08643303  0.06470964  1.        ]]. Action = [[-0.01473331  0.6865566  -0.2519518   0.6522713 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2699. State = [[-0.24328752 -0.06641477  0.06575549  1.        ]]. Action = [[0.07208335 0.96498275 0.9911101  0.63894606]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2700. State = [[-0.24219257 -0.04611905  0.07632608  1.        ]]. Action = [[-0.91782796 -0.18991083  0.85037374  0.76890683]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 2701. State = [[-0.24293417 -0.04954024  0.07649912  1.        ]]. Action = [[-0.12824804 -0.49280626 -0.2869221   0.8389299 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2702. State = [[-0.2415456  -0.04484851  0.08398622  1.        ]]. Action = [[0.32396936 0.6995208  0.79224765 0.9152545 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2703. State = [[-0.23994526 -0.03680218  0.09669161  1.        ]]. Action = [[-0.93034786  0.5700443   0.48855305  0.7752106 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2704. State = [[-0.22797823 -0.03119644  0.11109708  1.        ]]. Action = [[0.9064796  0.2624085  0.98885095 0.76767087]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2705. State = [[-0.21708089 -0.02608854  0.13330093  1.        ]]. Action = [[-0.2329055  -0.05034739 -0.06466788  0.40959275]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2706. State = [[-0.20922673 -0.01089756  0.12721401  1.        ]]. Action = [[ 0.8779912   0.98443687 -0.9526705   0.66956425]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2707. State = [[-0.2016472   0.02246561  0.11437618  1.        ]]. Action = [[-0.9273589   0.928982    0.35858917  0.59943867]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2708. State = [[-0.20877971  0.03564055  0.11215052  1.        ]]. Action = [[ 0.32657862 -0.65133166 -0.37418675  0.6708653 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2709. State = [[-0.19809063  0.03743127  0.1164058   1.        ]]. Action = [[0.7948139  0.5791575  0.46324694 0.5597137 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2710. State = [[-0.19068849  0.04774504  0.1322103   1.        ]]. Action = [[-0.8015402   0.12172067  0.8546159   0.6517811 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2711. State = [[-0.1954353   0.04919698  0.1587553   1.        ]]. Action = [[ 0.11156702 -0.17770368  0.80712783  0.60711575]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2712. State = [[-0.19000652  0.03217927  0.18944767  1.        ]]. Action = [[ 0.47273993 -0.9245787   0.79838586  0.6482775 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2713. State = [[-0.19325826  0.00210627  0.20419703  1.        ]]. Action = [[-0.9838652  -0.88327557 -0.5164844   0.5334046 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2714. State = [[-0.20460775 -0.01586844  0.20241801  1.        ]]. Action = [[0.5687338  0.8002938  0.34826732 0.6353438 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: No entry zone
Current timestep = 2715. State = [[-0.20778447 -0.01451764  0.2052196   1.        ]]. Action = [[-0.2718103   0.33078372  0.3589741   0.7900672 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2716. State = [[-0.22351031  0.00171063  0.20246094  1.        ]]. Action = [[-0.7145061   0.84789944 -0.59894305  0.70564556]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2717. State = [[-0.24085368  0.02509644  0.19804445  1.        ]]. Action = [[0.16824949 0.5110979  0.05290008 0.01169407]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2718. State = [[-0.25010493  0.05260674  0.19076017  1.        ]]. Action = [[-0.5150391   0.99349344 -0.827678    0.80991626]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2719. State = [[-0.25853395  0.08926263  0.16332594  1.        ]]. Action = [[ 0.43685186  0.95048976 -0.929705    0.72724485]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2720. State = [[-0.2471811   0.11320935  0.14169396  1.        ]]. Action = [[0.64219856 0.2481147  0.3615545  0.8840066 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2721. State = [[-0.2370188   0.12051435  0.14507082  1.        ]]. Action = [[-0.74726224  0.4929707   0.7076857   0.7907181 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 2722. State = [[-0.22824734  0.11832963  0.14852442  1.        ]]. Action = [[ 0.46180677 -0.43293226  0.42098975  0.62156093]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2723. State = [[-0.21534702  0.11559383  0.14694293  1.        ]]. Action = [[ 0.6027404   0.2065419  -0.68552035  0.7578423 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2724. State = [[-0.18798211  0.12456695  0.13704929  1.        ]]. Action = [[0.9191997  0.43251574 0.13701177 0.81866693]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2725. State = [[-0.17447273  0.13119997  0.14076808  1.        ]]. Action = [[-0.8598688  -0.2000379   0.39233017  0.5925821 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2726. State = [[-0.18822911  0.13681191  0.15673794  1.        ]]. Action = [[-0.8918953   0.23056054  0.9738467   0.64214325]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2727. State = [[-0.20648003  0.13984746  0.1786341   1.        ]]. Action = [[ 0.5544579  -0.41877747  0.82379496  0.82254255]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: No entry zone
Current timestep = 2728. State = [[-0.20502537  0.13483292  0.18409254  1.        ]]. Action = [[ 0.5521554  -0.3370793   0.05778885  0.4480965 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2729. State = [[-0.19515893  0.12728359  0.19480976  1.        ]]. Action = [[ 0.44625247 -0.11762446  0.61227775  0.4533894 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2730. State = [[-0.18353637  0.12787338  0.20731412  1.        ]]. Action = [[ 0.16417933  0.32642853 -0.0599764   0.56991315]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2731. State = [[-0.18477936  0.12185074  0.21217118  1.        ]]. Action = [[-0.9260475  -0.70787394  0.16436052  0.7693802 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2732. State = [[-0.19349551  0.1117531   0.21475911  1.        ]]. Action = [[ 0.72357845 -0.0703817  -0.2974012   0.7587395 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 2733. State = [[-0.19902836  0.11914882  0.22360565  1.        ]]. Action = [[-0.16882694  0.51765656  0.5514612   0.6580746 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2734. State = [[-0.21010351  0.12384922  0.22568141  1.        ]]. Action = [[-0.3405938  -0.17991173 -0.85410386  0.8654177 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2735. State = [[-0.22144315  0.11702407  0.20737839  1.        ]]. Action = [[-0.08645761 -0.2222892  -0.92119724  0.7348876 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2736. State = [[-0.22003496  0.09905618  0.17647557  1.        ]]. Action = [[ 0.45437145 -0.87630457 -0.8931369   0.54090023]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2737. State = [[-0.21760717  0.07528254  0.16139069  1.        ]]. Action = [[-0.58489215 -0.56736594  0.80935454  0.4248942 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2738. State = [[-0.21837379  0.04918113  0.16823776  1.        ]]. Action = [[ 0.5592439  -0.86486906 -0.35236597  0.743153  ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2739. State = [[-0.21438393  0.03358927  0.16266482  1.        ]]. Action = [[-0.323187    0.43537474 -0.3164947   0.85504675]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2740. State = [[-0.20989461  0.03426921  0.15647544  1.        ]]. Action = [[ 0.9599824  -0.04365671 -0.10872334  0.7688565 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2741. State = [[-0.19790858  0.03579777  0.15101826  1.        ]]. Action = [[0.8734206  0.43805003 0.6859902  0.84641886]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 2742. State = [[-0.18987046  0.03429856  0.14480595  1.        ]]. Action = [[ 0.4521978  -0.11117369 -0.4469546   0.77427244]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2743. State = [[-0.17863348  0.02328912  0.13057043  1.        ]]. Action = [[ 0.12505221 -0.49212503 -0.23597813  0.52073133]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2744. State = [[-0.17485155  0.02146583  0.13136156  1.        ]]. Action = [[-0.32499003  0.44340897  0.70635045  0.8231518 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2745. State = [[-0.17417307  0.0261345   0.13648489  1.        ]]. Action = [[ 0.7376299  0.9259627 -0.5340006  0.6107993]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: No entry zone
Current timestep = 2746. State = [[-0.17844094  0.03307129  0.13592105  1.        ]]. Action = [[-0.3617462   0.3459258  -0.1292345   0.64671874]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2747. State = [[-0.18482329  0.03929828  0.13450395  1.        ]]. Action = [[ 0.6495459  -0.12477911  0.36151183  0.76116073]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 2748. State = [[-0.19735672  0.04972082  0.13152313  1.        ]]. Action = [[-0.9001163   0.55678177 -0.09435356  0.6586764 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2749. State = [[-0.2096736   0.04945944  0.13354754  1.        ]]. Action = [[ 0.02057076 -0.81320405  0.24034858  0.6347084 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2750. State = [[-0.22161958  0.02694296  0.14932404  1.        ]]. Action = [[-0.68671066 -0.6939976   0.63116765  0.7364707 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2751. State = [[-0.24914281  0.01647134  0.1572507   1.        ]]. Action = [[-0.7867654   0.38429677 -0.4837774   0.67049384]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2752. State = [[-0.2637051   0.00522046  0.14846031  1.        ]]. Action = [[ 0.18357241 -0.8925871  -0.49030244  0.5110159 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2753. State = [[-0.25891545 -0.02396325  0.14119498  1.        ]]. Action = [[ 0.5853534  -0.6738157  -0.15689647  0.6243746 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2754. State = [[-0.24862096 -0.02576955  0.12589855  1.        ]]. Action = [[ 0.38032556  0.9743031  -0.7176525   0.10253847]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2755. State = [[-2.43801281e-01 -7.35555514e-05  1.17764086e-01  1.00000000e+00]]. Action = [[-0.1751942   0.11572957  0.83787966  0.51204145]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2756. State = [[-0.24518189  0.01251028  0.12652552  1.        ]]. Action = [[-0.87877893  0.4805374   0.25430477  0.8273139 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 2757. State = [[-0.24048233  0.01090785  0.12983432  1.        ]]. Action = [[ 0.46498847 -0.10475779  0.311754    0.610234  ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2758. State = [[-0.2214503  -0.00176359  0.13918531  1.        ]]. Action = [[ 0.83079565 -0.28220302  0.51876044  0.8673048 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2759. State = [[-0.19639158 -0.01015216  0.16207948  1.        ]]. Action = [[ 0.6694441 -0.1627475  0.9016572  0.8121171]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2760. State = [[-0.17553148 -0.00884622  0.18472686  1.        ]]. Action = [[0.27989638 0.46706557 0.20600986 0.64805174]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2761. State = [[-0.16625416 -0.00316064  0.19239353  1.        ]]. Action = [[ 0.83443594  0.05833638 -0.72022444  0.5960109 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 2762. State = [[-0.1709921   0.00217195  0.20108321  1.        ]]. Action = [[-0.8652641   0.21469927  0.46234202  0.74274445]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2763. State = [[-0.18136536  0.00535666  0.21312179  1.        ]]. Action = [[ 0.8869252   0.4946487  -0.35170734  0.18730688]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: No entry zone
Current timestep = 2764. State = [[-0.18428908  0.00616482  0.21553704  1.        ]]. Action = [[ 0.52737117 -0.9688767  -0.5258317   0.7708225 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: No entry zone
Current timestep = 2765. State = [[-0.19281222 -0.00921591  0.20965852  1.        ]]. Action = [[-0.35434544 -0.92783505 -0.7465267   0.59929466]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2766. State = [[-0.20657337 -0.01617897  0.19137517  1.        ]]. Action = [[-0.3465824   0.69328    -0.973099    0.53998613]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2767. State = [[-0.21789956 -0.00751745  0.16196641  1.        ]]. Action = [[-0.26662028  0.07133472 -0.5735455   0.63255405]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2768. State = [[-0.2271315   0.00981575  0.13676961  1.        ]]. Action = [[-0.19534671  0.890254   -0.71567416  0.4850825 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2769. State = [[-0.23952262  0.02742002  0.10835245  1.        ]]. Action = [[-0.7942054  -0.00925046 -0.631116    0.46957612]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2770. State = [[-0.25097406  0.04814968  0.08371847  1.        ]]. Action = [[ 0.6279328   0.9720025  -0.8408602   0.49948096]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2771. State = [[-0.25500494  0.08378059  0.05497142  1.        ]]. Action = [[-0.21669966  0.4755026  -0.5879479   0.6264651 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2772. State = [[-0.25806972  0.09574658  0.04042822  1.        ]]. Action = [[ 0.80531955  0.7907908  -0.7855786   0.39823163]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 2773. State = [[-0.25888154  0.09918656  0.03769768  1.        ]]. Action = [[-0.8778962   0.72561646 -0.27347648  0.42420268]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 2774. State = [[-0.25757974  0.10569218  0.03453588  1.        ]]. Action = [[ 0.1090107   0.62100184 -0.09482503  0.7573323 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2775. State = [[-0.25565368  0.11267956  0.02853762  1.        ]]. Action = [[-0.74547035 -0.65778124  0.77228785  0.5949781 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 2776. State = [[-0.25127313  0.11704051  0.02882604  1.        ]]. Action = [[0.2488904  0.23942184 0.41535604 0.8893783 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2777. State = [[-0.2601783   0.13547976  0.11760971  1.        ]]. Action = [[-0.50376993  0.96524763  0.9769714   0.47240055]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 2778. State = [[-0.25255024  0.16225742  0.11191735  1.        ]]. Action = [[0.33489263 0.7432375  0.8372848  0.67589664]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2779. State = [[-0.23927091  0.18148722  0.1171461   1.        ]]. Action = [[ 0.53873885  0.26011038 -0.42526984  0.7828462 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2780. State = [[-0.21849424  0.18500681  0.11771423  1.        ]]. Action = [[ 0.8143606  -0.19191748  0.07001078  0.83040905]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2781. State = [[-0.19507618  0.19489543  0.12882032  1.        ]]. Action = [[0.35635507 0.6420324  0.9475275  0.7318655 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2782. State = [[-0.17088649  0.19691257  0.1495089   1.        ]]. Action = [[ 0.548254   -0.61974627  0.24757338  0.6361785 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2783. State = [[-0.15470967  0.18430811  0.16769527  1.        ]]. Action = [[ 0.01011431 -0.32712257  0.7638657   0.50892293]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2784. State = [[-0.14529456  0.1816492   0.19342463  1.        ]]. Action = [[0.37903297 0.27326572 0.7026098  0.5367336 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2785. State = [[-0.13353671  0.17787237  0.22396262  1.        ]]. Action = [[-0.09646845 -0.5176422   0.92747474  0.71096325]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2786. State = [[-0.12704036  0.15416028  0.25928432  1.        ]]. Action = [[ 0.4533913  -0.90323156  0.8420954   0.45545328]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2787. State = [[-0.1231548   0.1256032   0.28294677  1.        ]]. Action = [[-0.72754985 -0.85166276 -0.0295105   0.5215876 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2788. State = [[-0.1317214   0.09567849  0.28176922  1.        ]]. Action = [[-0.11875445 -0.78826106 -0.61602414  0.4919107 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2789. State = [[-0.1462987   0.06460354  0.26952738  1.        ]]. Action = [[-0.8137283  -0.90448064 -0.605333    0.619851  ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2790. State = [[-0.16474427  0.04426303  0.25276133  1.        ]]. Action = [[-0.49884772 -0.5511038  -0.9819237   0.7388935 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 2791. State = [[-0.17286706  0.04857512  0.24300015  1.        ]]. Action = [[-0.292561    0.6310494  -0.63166493  0.68775237]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2792. State = [[-0.18069646  0.05489854  0.23203065  1.        ]]. Action = [[ 0.48587656 -0.40517586 -0.6926176   0.71493936]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 2793. State = [[-0.18282412  0.04398793  0.23090072  1.        ]]. Action = [[-0.29479122 -0.9393025   0.08225608  0.5830326 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2794. State = [[-0.19005206  0.03054525  0.22758673  1.        ]]. Action = [[ 0.8029822   0.03947222 -0.17104578  0.45732307]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 2795. State = [[-0.18327928  0.0117061   0.23339912  1.        ]]. Action = [[ 0.79216003 -0.91483396  0.28113878  0.5944407 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2796. State = [[-0.1767675  -0.00514879  0.22971366  1.        ]]. Action = [[-0.24989021  0.3107419  -0.72486705  0.840889  ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2797. State = [[-0.17765248 -0.00559783  0.21768652  1.        ]]. Action = [[ 0.66675735  0.68551147 -0.6990561   0.7675183 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: No entry zone
Current timestep = 2798. State = [[-0.17945766  0.00901186  0.20653245  1.        ]]. Action = [[ 0.06797659  0.9252825  -0.9676959   0.38135052]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2799. State = [[-0.18076462  0.03623145  0.17551535  1.        ]]. Action = [[-0.43478835  0.5518187  -0.29510117  0.70538497]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2800. State = [[-0.18451147  0.04947863  0.16730551  1.        ]]. Action = [[ 0.95848966  0.1791327  -0.838688    0.8536968 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 2801. State = [[-0.194364    0.04480182  0.16334829  1.        ]]. Action = [[-0.95507556 -0.53438497  0.03598809  0.76235867]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2802. State = [[-0.21711467  0.02974758  0.15703158  1.        ]]. Action = [[-0.17957067 -0.48826516  0.11800253  0.7295971 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2803. State = [[-0.23966825  0.03348644  0.15002365  1.        ]]. Action = [[-0.9393012   0.9584724  -0.87255603  0.6806412 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2804. State = [[-0.2577237   0.04628955  0.12546316  1.        ]]. Action = [[ 0.5049689  -0.02997184 -0.9277323   0.6757972 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2805. State = [[-0.2489804   0.03712548  0.0931307   1.        ]]. Action = [[ 0.6772281  -0.9100478  -0.5710502   0.72319126]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2806. State = [[-0.23838016  0.02648997  0.08122418  1.        ]]. Action = [[-0.10959888  0.5045264   0.58234143  0.8145195 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2807. State = [[-0.2298106   0.03401475  0.07793636  1.        ]]. Action = [[ 0.71126795  0.14448452 -0.66808784  0.5918212 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2808. State = [[-0.22576906  0.04170375  0.06007301  1.        ]]. Action = [[-0.36010432  0.15267432 -0.82978165  0.5340326 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2809. State = [[-0.21922413  0.05745942  0.03540491  1.        ]]. Action = [[ 0.8771086   0.84442186 -0.5031885   0.71078527]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2810. State = [[-0.20295511  0.07298619  0.02000206  1.        ]]. Action = [[ 0.985798   -0.7478277  -0.20444989  0.77042866]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 2811. State = [[-0.20232527  0.07481562  0.01976216  1.        ]]. Action = [[ 0.26568985  0.14444232 -0.06706661  0.6204083 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 2812. State = [[-0.20196678  0.0749417   0.01980039  1.        ]]. Action = [[-0.207533   -0.17934114 -0.87258565  0.5195248 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 2813. State = [[-0.19720426  0.06567571  0.02487119  1.        ]]. Action = [[ 0.0127666  -0.70723075  0.75360775  0.84768784]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2814. State = [[-0.20162816  0.05714048  0.03753081  1.        ]]. Action = [[-0.8852787   0.04927349  0.5853145   0.6095805 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2815. State = [[-0.22229338  0.05443348  0.05174261  1.        ]]. Action = [[-0.6104583  -0.04578269  0.08675301  0.8391938 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2816. State = [[-0.22906546  0.05505896  0.05630552  1.        ]]. Action = [[ 0.79709506  0.21811426 -0.21115565  0.48303723]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2817. State = [[-0.22321454  0.06217919  0.05528316  1.        ]]. Action = [[ 0.22222829  0.34301066 -0.27546525  0.5861542 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2818. State = [[-0.21730267  0.06782881  0.05222477  1.        ]]. Action = [[-0.23328966  0.55082583 -0.77308375  0.17784667]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 2819. State = [[-0.226083    0.08320709  0.05312952  1.        ]]. Action = [[-0.95419014  0.8563559   0.4074521   0.7649828 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2820. State = [[-0.24494042  0.10566457  0.05774023  1.        ]]. Action = [[-0.60052913  0.11470962  0.3480599   0.729481  ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2821. State = [[-0.25870508  0.11066633  0.06655735  1.        ]]. Action = [[-0.7878229   0.5426624  -0.7619325   0.63443744]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2822. State = [[-0.25688002  0.10063735  0.06544712  1.        ]]. Action = [[ 0.4803034  -0.6856077  -0.4418007   0.44089818]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2823. State = [[-0.25054303  0.08251247  0.06466039  1.        ]]. Action = [[-0.20225793 -0.4780708   0.03557324  0.76134086]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2824. State = [[-0.2495391   0.06740275  0.06052712  1.        ]]. Action = [[-0.19190025 -0.36684108 -0.25805342  0.39799368]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2825. State = [[-0.2509626   0.05966507  0.05552072  1.        ]]. Action = [[-0.75788265 -0.15557551  0.42761874  0.68014574]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 2826. State = [[-0.2513514   0.05859481  0.05532952  1.        ]]. Action = [[-0.72097766  0.36320734 -0.26489186  0.8437638 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 2827. State = [[-0.251199    0.04441334  0.05614788  1.        ]]. Action = [[ 0.0050528  -0.841908    0.14820397  0.54221475]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2828. State = [[-0.24346861  0.03797236  0.06674359  1.        ]]. Action = [[0.9620677  0.8320073  0.99273825 0.5131099 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2829. State = [[-0.22485982  0.05562956  0.08961051  1.        ]]. Action = [[0.7689328 0.6158494 0.3497957 0.6509528]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2830. State = [[-0.20789392  0.0790953   0.11034971  1.        ]]. Action = [[0.04531431 0.5856557  0.8070493  0.46758938]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2831. State = [[-0.19896469  0.09233132  0.13070157  1.        ]]. Action = [[0.40012467 0.02865314 0.17895317 0.800997  ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2832. State = [[-0.19125772  0.08271201  0.13677338  1.        ]]. Action = [[-0.4658407  -0.8672894   0.05222523  0.80465484]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2833. State = [[-0.19231313  0.06848518  0.13951735  1.        ]]. Action = [[0.7695446 0.5072789 0.9503348 0.5272839]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 2834. State = [[-0.20196922  0.07570143  0.15164237  1.        ]]. Action = [[-0.8442166   0.54913807  0.8396454   0.72470117]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2835. State = [[-0.2203927   0.08524718  0.1714658   1.        ]]. Action = [[ 0.84529865  0.94540477 -0.9700894   0.53974724]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 2836. State = [[-0.21442686  0.08556785  0.18074852  1.        ]]. Action = [[ 0.9011073  -0.03620666  0.64839816  0.5644734 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2837. State = [[-0.19695173  0.08832619  0.20565568  1.        ]]. Action = [[0.2877028  0.18377078 0.724318   0.76181793]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2838. State = [[-0.18702602  0.07588828  0.22207983  1.        ]]. Action = [[ 0.19897032 -0.90213823 -0.24275494  0.7830603 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2839. State = [[-0.18494377  0.05336136  0.22827084  1.        ]]. Action = [[-0.09922034 -0.5172466   0.386577    0.49294424]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2840. State = [[-0.1734721   0.03724452  0.2482992   1.        ]]. Action = [[ 0.66465354 -0.25862503  0.9783653   0.7641666 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2841. State = [[-0.15280487  0.01992528  0.28188857  1.        ]]. Action = [[ 0.56311107 -0.5922131   0.7565272   0.848557  ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2842. State = [[-0.14623994  0.01279213  0.29545498  1.        ]]. Action = [[-0.6334819   0.30097246 -0.7950036   0.7248666 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2843. State = [[-1.4495468e-01 -3.9270965e-04  2.9434332e-01  1.0000000e+00]]. Action = [[ 0.574172   -0.92776185  0.40116715  0.8309784 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2844. State = [[-0.14978413 -0.03185565  0.28797504  1.        ]]. Action = [[-0.89043677 -0.9642497  -0.94919175  0.8035048 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2845. State = [[-0.1558403  -0.05913778  0.26976663  1.        ]]. Action = [[ 0.4146657 -0.3583119 -0.380206   0.5275903]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2846. State = [[-0.15108761 -0.07008662  0.25958043  1.        ]]. Action = [[ 0.28685236  0.876179   -0.9474501   0.8171661 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 2847. State = [[-0.15280929 -0.0719274   0.25556603  1.        ]]. Action = [[-0.18403363  0.00483024 -0.29385722  0.69575953]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2848. State = [[-0.1596806  -0.08606631  0.24840868  1.        ]]. Action = [[-0.47279143 -0.8435495  -0.04593188  0.5173397 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2849. State = [[-0.16876417 -0.10230628  0.24168879  1.        ]]. Action = [[ 0.21703517  0.31846237 -0.70416147  0.4739536 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 2850. State = [[-0.1807922  -0.11300717  0.23524925  1.        ]]. Action = [[-0.7917306 -0.3894359 -0.5485562  0.5517459]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2851. State = [[-0.19170457 -0.10729247  0.23115449  1.        ]]. Action = [[0.27076244 0.9911064  0.765494   0.58083117]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2852. State = [[-0.20067394 -0.08277532  0.23198602  1.        ]]. Action = [[-0.7517059   0.6017531  -0.6200382   0.70708764]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2853. State = [[-0.21465406 -0.055997    0.21732979  1.        ]]. Action = [[ 0.0227164   0.8625245  -0.7125497   0.27717447]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2854. State = [[-0.2152789  -0.02266821  0.19457676  1.        ]]. Action = [[ 0.12727427  0.9484867  -0.8051767   0.6359688 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2855. State = [[-0.20779428 -0.01021269  0.17540203  1.        ]]. Action = [[ 0.28774834 -0.65870523  0.19340777  0.67056537]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2856. State = [[-2.1578857e-01 -3.1272642e-04  1.6599175e-01  1.0000000e+00]]. Action = [[-0.8510277   0.96550775 -0.7343666   0.77498794]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2857. State = [[-0.22140008  0.00472985  0.14314476  1.        ]]. Action = [[ 0.89269435 -0.63374496 -0.64281994  0.62581694]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2858. State = [[-0.21485457 -0.00149625  0.12908237  1.        ]]. Action = [[-0.5712707  -0.04619408  0.7633002   0.5756153 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2859. State = [[-0.212201   -0.01666119  0.12714653  1.        ]]. Action = [[ 0.43273616 -0.86449087 -0.9811306   0.70957744]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2860. State = [[-0.21522668 -0.0259057   0.11584762  1.        ]]. Action = [[-0.6585297   0.66157174  0.41373253  0.49442554]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2861. State = [[-0.23258378 -0.00624097  0.10903787  1.        ]]. Action = [[-0.445848   0.9481045 -0.8429579  0.5967233]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2862. State = [[-0.23736916  0.02510832  0.09546695  1.        ]]. Action = [[ 0.7063422   0.05795383 -0.2150327   0.53379536]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2863. State = [[-0.23619984  0.04136285  0.0954568   1.        ]]. Action = [[-0.12570077  0.8850739   0.73019385  0.6907767 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2864. State = [[-0.24279888  0.06430105  0.0957387   1.        ]]. Action = [[-0.42069465  0.59579444 -0.27801502  0.7807877 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2865. State = [[-0.24837142  0.08674455  0.10391715  1.        ]]. Action = [[-0.00602281  0.67376006  0.975055    0.8311523 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2866. State = [[-0.249752    0.09991273  0.115352    1.        ]]. Action = [[-0.9006992   0.8560252  -0.7193195   0.55063033]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 2867. State = [[-0.24962236  0.10076148  0.11635137  1.        ]]. Action = [[-0.85246354 -0.068075    0.2978555   0.8013097 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 2868. State = [[-0.23873241  0.10091683  0.12711298  1.        ]]. Action = [[0.8344455  0.16373658 0.9352863  0.6312746 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2869. State = [[-0.23596156  0.10757505  0.1378508   1.        ]]. Action = [[-0.55698395 -0.00852048 -0.7078458   0.8293371 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2870. State = [[-0.23595548  0.11910114  0.13631737  1.        ]]. Action = [[0.5416862  0.91737485 0.19141424 0.42869055]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2871. State = [[-0.21932694  0.12911274  0.13722073  1.        ]]. Action = [[ 0.97321296 -0.18888867 -0.1266082   0.5003109 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2872. State = [[-0.19236957  0.13956058  0.14794448  1.        ]]. Action = [[ 0.6340885  -0.09009027  0.75524783  0.8058286 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2873. State = [[-0.17594394  0.14001498  0.16383179  1.        ]]. Action = [[ 0.84604335 -0.8025459  -0.93166965  0.4141636 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 2874. State = [[-0.17854726  0.12458497  0.16626711  1.        ]]. Action = [[-0.6938825  -0.9578768   0.00439763  0.54208267]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2875. State = [[-0.18241534  0.10927335  0.16944022  1.        ]]. Action = [[0.30134094 0.7634499  0.8404727  0.5122509 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: No entry zone
Current timestep = 2876. State = [[-0.18668291  0.1126444   0.16285163  1.        ]]. Action = [[ 0.16992235  0.5305815  -0.57889163  0.65968287]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2877. State = [[-0.18831138  0.11172272  0.15494515  1.        ]]. Action = [[-0.28245854 -0.59977937  0.09138691  0.5591116 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2878. State = [[-0.19612314  0.11728612  0.15797642  1.        ]]. Action = [[-0.5203365   0.73563325  0.34810734  0.57323766]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2879. State = [[-0.26088583  0.11811467  0.10919094  1.        ]]. Action = [[-0.7029799  -0.06737524  0.7664695   0.7409675 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2880. State = [[-0.2457269   0.12011684  0.09683474  1.        ]]. Action = [[ 0.9258101  -0.7770735   0.0108465   0.58934057]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2881. State = [[-0.23019701  0.1100034   0.09404872  1.        ]]. Action = [[-0.6018432  -0.16083819  0.66796815  0.62289083]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 2882. State = [[-0.21646677  0.10844803  0.10158283  1.        ]]. Action = [[0.681306   0.02478564 0.7905867  0.69512415]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2883. State = [[-0.21287735  0.11624952  0.11014802  1.        ]]. Action = [[-0.97015464  0.5196085  -0.04472572  0.7275343 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2884. State = [[-0.22815399  0.12722367  0.10434338  1.        ]]. Action = [[-0.24735373  0.12549603 -0.8754262   0.8859196 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2885. State = [[-0.23963466  0.13941973  0.09395628  1.        ]]. Action = [[-0.33504814  0.5823567   0.21473026  0.6851008 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2886. State = [[-0.24519168  0.14706713  0.09461664  1.        ]]. Action = [[ 0.18516397 -0.28009903  0.13866377  0.63378215]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2887. State = [[-0.24455166  0.14563234  0.09499458  1.        ]]. Action = [[-0.56663597  0.1628704   0.28525043  0.49467027]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2888. State = [[-0.24242455  0.14648196  0.09571607  1.        ]]. Action = [[ 0.23963785  0.11167037 -0.1469047   0.50821996]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2889. State = [[-0.23618664  0.14945185  0.09119047  1.        ]]. Action = [[ 0.42028582  0.16045237 -0.43133837  0.7603681 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2890. State = [[-0.22670326  0.14203116  0.07609487  1.        ]]. Action = [[ 0.06320035 -0.72988856 -0.5650646   0.79178333]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2891. State = [[-0.21552193  0.11889728  0.06642841  1.        ]]. Action = [[ 0.39451087 -0.7144382   0.34440303  0.8515085 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2892. State = [[-0.21309887  0.09321333  0.06551113  1.        ]]. Action = [[-0.85546887 -0.82854474 -0.15277183  0.57757163]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2893. State = [[-0.23032755  0.08082496  0.06603532  1.        ]]. Action = [[-0.5789309   0.19984782  0.44139075  0.5845847 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2894. State = [[-0.24732286  0.08678488  0.0678371   1.        ]]. Action = [[-0.29011232  0.5232935  -0.48059082  0.76946187]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2895. State = [[-0.2574997   0.10164686  0.05659668  1.        ]]. Action = [[ 0.2884158  0.5548861 -0.6853901  0.8427708]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2896. State = [[-0.2609923   0.1108012   0.04212135  1.        ]]. Action = [[-0.24802518 -0.3425815   0.05915749  0.8792037 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2897. State = [[-0.2614319   0.11182117  0.039105    1.        ]]. Action = [[-0.13141775  0.28135097 -0.80920297  0.56121624]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 2898. State = [[-0.25069994  0.1055471   0.03753898  1.        ]]. Action = [[ 0.8251014  -0.5517638  -0.17677861  0.4962182 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2899. State = [[-0.23578373  0.09878414  0.03296994  1.        ]]. Action = [[-0.8831463  -0.49610585 -0.4646877   0.81050086]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 2900. State = [[-0.23323153  0.09845055  0.03297881  1.        ]]. Action = [[-0.9573881   0.5851114   0.80251145  0.73394203]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 2901. State = [[-0.2329198   0.09835236  0.03256007  1.        ]]. Action = [[-0.8591528  -0.26474214  0.6217289   0.6980238 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 2902. State = [[-0.23251729  0.09844452  0.03240897  1.        ]]. Action = [[-0.81631607  0.13740718  0.8682215   0.7010157 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 2903. State = [[-0.22723106  0.09734344  0.03969548  1.        ]]. Action = [[ 0.17891896 -0.01128471  0.86567664  0.5563173 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2904. State = [[-0.2292096   0.10650641  0.05093961  1.        ]]. Action = [[-0.6307742   0.61537755  0.13422143  0.5430989 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2905. State = [[-0.22871517  0.12653981  0.05547906  1.        ]]. Action = [[ 0.7962754   0.7016907  -0.05887491  0.7698487 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2906. State = [[-0.21625912  0.13086803  0.06396149  1.        ]]. Action = [[ 0.2558061  -0.52859765  0.50247693  0.7761986 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2907. State = [[-0.20875263  0.12773408  0.0725411   1.        ]]. Action = [[-0.9388014   0.17866957 -0.9152534   0.50045323]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 2908. State = [[-0.19953567  0.12661153  0.08456019  1.        ]]. Action = [[0.4760245  0.07954276 0.8541045  0.82117414]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2909. State = [[-0.18562469  0.11882352  0.11664195  1.        ]]. Action = [[ 0.17563462 -0.594594    0.9443983   0.7326789 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2910. State = [[-0.18021482  0.1102749   0.14066303  1.        ]]. Action = [[0.47859526 0.63213205 0.67763424 0.68059933]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Current timestep = 2911. State = [[-0.1768138   0.09737098  0.14418492  1.        ]]. Action = [[ 0.01549196 -0.6600744   0.08658755  0.38994122]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2912. State = [[-0.17493832  0.08639068  0.14683345  1.        ]]. Action = [[ 0.4909835  -0.3205797   0.48609638  0.6547899 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: No entry zone
Current timestep = 2913. State = [[-0.1746018   0.08504335  0.14729702  1.        ]]. Action = [[ 0.623999   -0.56708086  0.73428833  0.5442445 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: No entry zone
Current timestep = 2914. State = [[-0.1766739   0.08269525  0.15292767  1.        ]]. Action = [[-0.5421329  -0.13647199  0.43129253  0.8244879 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2915. State = [[-0.19041988  0.07569481  0.17244624  1.        ]]. Action = [[-0.7622649  -0.29996896  0.6444464   0.7092719 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2916. State = [[-0.20663443  0.07492958  0.1903522   1.        ]]. Action = [[0.2643857  0.47707438 0.18059516 0.30637193]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2917. State = [[-0.21071734  0.0684386   0.19241734  1.        ]]. Action = [[-0.33093274 -0.9011346  -0.61379933  0.759712  ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2918. State = [[-0.22309403  0.05779577  0.17894061  1.        ]]. Action = [[-0.40342677  0.24437213 -0.7531653   0.83188796]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2919. State = [[-0.2344741   0.06642125  0.16926484  1.        ]]. Action = [[-0.34042203  0.5169165   0.6609707   0.83792174]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2920. State = [[-0.24331222  0.074332    0.16675788  1.        ]]. Action = [[-0.29544562 -0.0888992  -0.6881379   0.53486824]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2921. State = [[-0.2506925   0.06024042  0.15806001  1.        ]]. Action = [[ 0.09562755 -0.92489356 -0.14484566  0.5685388 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2922. State = [[-0.2540706   0.04786743  0.15349115  1.        ]]. Action = [[-0.07116568  0.17180681  0.09378028  0.7721133 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2923. State = [[-0.25454375  0.04788955  0.15332277  1.        ]]. Action = [[-0.4512797   0.11759973 -0.8492617   0.81707525]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 2924. State = [[-0.25224745  0.03897057  0.14680111  1.        ]]. Action = [[ 0.19784653 -0.55170834 -0.56865317  0.78559995]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2925. State = [[-0.25178632  0.04046663  0.13861516  1.        ]]. Action = [[-0.09753281  0.8592117   0.3601966   0.70537007]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2926. State = [[-0.25502488  0.04990703  0.13829811  1.        ]]. Action = [[-0.7593874  0.4184531 -0.2811221  0.7750869]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 2927. State = [[-0.24996643  0.05279034  0.13081364  1.        ]]. Action = [[ 0.68600273  0.05909169 -0.94009155  0.23971665]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2928. State = [[-0.23312172  0.04846483  0.12188848  1.        ]]. Action = [[ 0.54922533 -0.57427967  0.724663    0.61580324]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2929. State = [[-0.22242878  0.03992712  0.12593311  1.        ]]. Action = [[-0.7508135   0.9402263  -0.60726255  0.5622847 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 2930. State = [[-0.21081154  0.04171385  0.13150874  1.        ]]. Action = [[0.74463546 0.27803802 0.3464105  0.786685  ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2931. State = [[-0.20163463  0.04644063  0.1321257   1.        ]]. Action = [[-0.42269766  0.14728403 -0.6845149   0.7881129 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2932. State = [[-0.20202336  0.05351445  0.12327857  1.        ]]. Action = [[ 0.39076757  0.24255145 -0.24911886  0.7352582 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2933. State = [[-0.1908086   0.05719209  0.10877228  1.        ]]. Action = [[ 0.53658295 -0.10692716 -0.7762603   0.778831  ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2934. State = [[-0.17639568  0.05759732  0.08890641  1.        ]]. Action = [[ 0.7589104  -0.74826914  0.48326826  0.3100065 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: No entry zone
Current timestep = 2935. State = [[-0.18016295  0.06748248  0.08720432  1.        ]]. Action = [[-0.6867614   0.6472757   0.2962662   0.70182943]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2936. State = [[-0.18601017  0.07859689  0.08746291  1.        ]]. Action = [[ 0.2895924  -0.23319823  0.179317    0.5761025 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 2937. State = [[-0.19418746  0.09169775  0.08075275  1.        ]]. Action = [[-0.40346086  0.73908484 -0.67315346  0.45129132]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2938. State = [[-0.20246468  0.10547098  0.07308944  1.        ]]. Action = [[ 0.6182041  -0.25419587  0.49946594  0.6976695 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 2939. State = [[-0.19810604  0.12206324  0.07436409  1.        ]]. Action = [[0.6538472  0.86569214 0.2743641  0.75689244]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2940. State = [[-0.19553275  0.13456753  0.07353416  1.        ]]. Action = [[-0.2672987  -0.09206182 -0.35018265  0.91004443]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2941. State = [[-0.19740504  0.13701843  0.07100749  1.        ]]. Action = [[ 0.66363883 -0.12638462 -0.02334213  0.46256554]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 2942. State = [[-0.1979021   0.13738684  0.0703571   1.        ]]. Action = [[ 0.84356034 -0.5242985   0.55204284  0.79845214]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: No entry zone
Current timestep = 2943. State = [[-0.19267367  0.15003715  0.07463015  1.        ]]. Action = [[0.44449532 0.86315894 0.5695751  0.81920815]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2944. State = [[-0.19494613  0.17292638  0.08745129  1.        ]]. Action = [[-0.98207366  0.24547815  0.75763345  0.7421782 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2945. State = [[-0.20496705  0.18910223  0.11665221  1.        ]]. Action = [[0.29672396 0.33875751 0.93922305 0.47712314]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2946. State = [[-0.20947868  0.18510343  0.14500184  1.        ]]. Action = [[-0.7212349 -0.7177938  0.5327095  0.4938233]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2947. State = [[-0.2318455   0.18471062  0.15578626  1.        ]]. Action = [[-0.8585726   0.55095506 -0.28607404  0.7883239 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2948. State = [[-0.2426245   0.18514448  0.15703467  1.        ]]. Action = [[ 0.46294498 -0.4743117   0.28748012  0.87493026]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2949. State = [[-0.23901178  0.17770728  0.16123962  1.        ]]. Action = [[-0.8341335  -0.00925219 -0.72010195  0.66771364]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 2950. State = [[-0.23784429  0.17588101  0.16284731  1.        ]]. Action = [[-0.6569677  -0.28843158 -0.7985933   0.74632525]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 2951. State = [[-0.23612823  0.16311748  0.16938257  1.        ]]. Action = [[-0.0410679  -0.8609906   0.32443213  0.8842623 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2952. State = [[-0.23588067  0.14852975  0.17436662  1.        ]]. Action = [[-0.9065218   0.0836345   0.79460764  0.7372689 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 2953. State = [[-0.22877976  0.13344553  0.18867192  1.        ]]. Action = [[ 0.60503864 -0.67575     0.8327391   0.34279513]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2954. State = [[-0.2188998   0.12487147  0.21405484  1.        ]]. Action = [[0.32650352 0.2932185  0.5334823  0.4373045 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2955. State = [[-0.21469189  0.12962122  0.21911095  1.        ]]. Action = [[ 0.08510256  0.2974558  -0.7682929   0.5254699 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2956. State = [[-0.21548803  0.12619595  0.21969077  1.        ]]. Action = [[-0.59666413 -0.5394092   0.6770593   0.8184339 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2957. State = [[-0.2245898   0.1088259   0.23812625  1.        ]]. Action = [[-0.73151076 -0.8512105   0.69621646  0.74352264]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2958. State = [[-0.2326626   0.08046032  0.2635602   1.        ]]. Action = [[ 0.8514209 -0.557064   0.5585604  0.8006058]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2959. State = [[-0.22582358  0.05895611  0.269336    1.        ]]. Action = [[ 0.03365862 -0.45605016 -0.706213    0.6406553 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2960. State = [[-0.22482203  0.05405774  0.253264    1.        ]]. Action = [[-0.05611521  0.37993455 -0.9813403   0.65312386]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2961. State = [[-0.2287307   0.04802198  0.2225373   1.        ]]. Action = [[-0.49136025 -0.5499705  -0.8368003   0.85781777]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2962. State = [[-0.22990952  0.05193298  0.20261984  1.        ]]. Action = [[0.5869746  0.74573064 0.06550872 0.54120934]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2963. State = [[-0.22494958  0.05919223  0.20561919  1.        ]]. Action = [[-0.14404976 -0.19552642  0.758291    0.78390646]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2964. State = [[-0.22460847  0.04514355  0.20668714  1.        ]]. Action = [[-0.04398471 -0.89875287 -0.8498234   0.6129528 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2965. State = [[-0.22531855  0.02859953  0.19813037  1.        ]]. Action = [[-0.16624564  0.05214548 -0.07780427  0.4832579 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2966. State = [[-0.22902706  0.01536513  0.18730332  1.        ]]. Action = [[-0.10381192 -0.6013398  -0.82889354  0.77228045]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2967. State = [[-0.22988822 -0.00794389  0.17524531  1.        ]]. Action = [[ 0.03189266 -0.6993836   0.733999    0.78353524]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2968. State = [[-0.22753936 -0.01135217  0.17247322  1.        ]]. Action = [[ 0.26424313  0.78526485 -0.6742935   0.48875523]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2969. State = [[-0.21891165  0.00468053  0.17058823  1.        ]]. Action = [[0.5255234  0.397321   0.69342804 0.6852021 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2970. State = [[-0.20464912  0.02601586  0.16808046  1.        ]]. Action = [[ 0.61032844  0.8752358  -0.6962112   0.6436602 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2971. State = [[-0.19126152  0.03215659  0.16367172  1.        ]]. Action = [[ 0.05351973 -0.7954468   0.5985961   0.65772104]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2972. State = [[-0.18801759  0.02629812  0.16799763  1.        ]]. Action = [[ 0.49036467  0.5836036  -0.93637997  0.6574749 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: No entry zone
Current timestep = 2973. State = [[-0.18740298  0.02576619  0.16844274  1.        ]]. Action = [[ 0.43428898 -0.01122445 -0.20374548  0.5959208 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: No entry zone
Current timestep = 2974. State = [[-0.18743047  0.02576404  0.16842984  1.        ]]. Action = [[ 0.39693952  0.02834916 -0.75365925  0.6274296 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 2975. State = [[-0.19048046  0.02539741  0.16842958  1.        ]]. Action = [[-0.5389865   0.04881263  0.01021123  0.65769553]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2976. State = [[-0.19367988  0.02489823  0.1696128   1.        ]]. Action = [[ 0.54625    -0.54691327  0.8668637   0.6908461 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 2977. State = [[-0.20071696  0.03766201  0.17866665  1.        ]]. Action = [[-0.5018463   0.8162581   0.71002126  0.74374497]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2978. State = [[-0.21469183  0.06215783  0.1912082   1.        ]]. Action = [[ 0.11309803  0.71936774 -0.18480986  0.58825076]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2979. State = [[-0.22121306  0.08640653  0.19478269  1.        ]]. Action = [[-0.01049769  0.6283145   0.36802387  0.6260239 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2980. State = [[-0.22945975  0.10077609  0.20765455  1.        ]]. Action = [[-0.80939674 -0.20873904  0.49606907  0.5735117 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2981. State = [[-0.25062147 -0.12908897  0.10470395  1.        ]]. Action = [[ 0.9502094  -0.93168867  0.64862347  0.6198504 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2982. State = [[-0.25232926 -0.14184256  0.09223813  1.        ]]. Action = [[-0.8092163   0.9778732  -0.38503873  0.76126313]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 2983. State = [[-0.25232926 -0.14184256  0.09223813  1.        ]]. Action = [[-0.54264367  0.75757813 -0.80668294  0.9130819 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 2984. State = [[-0.25232926 -0.14184256  0.09223813  1.        ]]. Action = [[-0.4679163  -0.09533942 -0.96873325  0.51428175]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 2985. State = [[-0.24492215 -0.12801346  0.08458449  1.        ]]. Action = [[ 0.6731547   0.9912927  -0.66623324  0.69140565]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2986. State = [[-0.22086559 -0.11386693  0.07436968  1.        ]]. Action = [[0.6405475  0.05819488 0.7287402  0.7254449 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2987. State = [[-0.21373348 -0.09681337  0.07684698  1.        ]]. Action = [[-0.51289237  0.9435482  -0.1515451   0.81738985]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2988. State = [[-0.22180733 -0.07111228  0.07254026  1.        ]]. Action = [[-0.44962716  0.40921283 -0.4705932   0.5151377 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2989. State = [[-0.22710311 -0.05999697  0.06869285  1.        ]]. Action = [[-0.6127037  -0.925672   -0.99375784  0.7102351 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 2990. State = [[-0.22749665 -0.05876277  0.06859248  1.        ]]. Action = [[-0.17722428  0.20033479 -0.99309725  0.6754043 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Current timestep = 2991. State = [[-0.22746252 -0.05857145  0.06857864  1.        ]]. Action = [[ 0.8063375  0.5105467 -0.9477255  0.8662858]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 2992. State = [[-0.22752674 -0.05849825  0.06856309  1.        ]]. Action = [[-0.3274483   0.8946171  -0.99476534  0.7429278 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 2993. State = [[-0.22752674 -0.05849825  0.06856309  1.        ]]. Action = [[-0.9562177   0.96095145  0.47672462  0.61204815]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 2994. State = [[-0.2275041  -0.05837113  0.06855394  1.        ]]. Action = [[-0.42105484  0.79479873 -0.8934302   0.93833494]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 2995. State = [[-0.2280121  -0.06626709  0.06272463  1.        ]]. Action = [[ 0.32562375 -0.6586011  -0.4985944   0.61387634]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2996. State = [[-0.22733939 -0.07303645  0.05219585  1.        ]]. Action = [[-0.9375098   0.88546133 -0.96443665  0.49671292]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 2997. State = [[-0.22719635 -0.07349358  0.05208648  1.        ]]. Action = [[ 0.19413829  0.82116413 -0.70941335  0.7384615 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 2998. State = [[-0.22719972 -0.07402311  0.05213466  1.        ]]. Action = [[ 0.89630055  0.08838058 -0.61058044  0.78402674]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 2999. State = [[-0.23123063 -0.07930021  0.04704843  1.        ]]. Action = [[-0.53114986 -0.29931414 -0.34954977  0.58618474]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 3000. State = [[-0.23744251 -0.08562267  0.03629831  1.        ]]. Action = [[ 0.49470675 -0.28231812 -0.58963764  0.83349764]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 3001. State = [[-0.23774754 -0.08710962  0.03598592  1.        ]]. Action = [[ 0.51707006  0.9456668  -0.5400288   0.77222633]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3002. State = [[-0.2377347  -0.08717865  0.0359924   1.        ]]. Action = [[-0.86048627  0.97868955 -0.23189604  0.68039143]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3003. State = [[-0.23782194 -0.08756373  0.03603121  1.        ]]. Action = [[ 0.11006212 -0.21351498 -0.21055877  0.74431074]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3004. State = [[-0.23783651 -0.08762798  0.03603771  1.        ]]. Action = [[ 0.72108555 -0.25625718 -0.95799935  0.73216414]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3005. State = [[-0.23788026 -0.08782071  0.03605726  1.        ]]. Action = [[-0.9540183  -0.78889453 -0.49941313  0.80103326]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3006. State = [[-0.23788026 -0.08782071  0.03605726  1.        ]]. Action = [[ 0.5401325   0.47904396 -0.22945344  0.56270695]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3007. State = [[-0.23788026 -0.08782071  0.03605726  1.        ]]. Action = [[ 0.29230034 -0.18707913 -0.8777252   0.7072314 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3008. State = [[-0.23788026 -0.08782071  0.03605726  1.        ]]. Action = [[-0.9300979  -0.6609163   0.57194567  0.49918818]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3009. State = [[-0.23788026 -0.08782071  0.03605726  1.        ]]. Action = [[-0.6777986   0.97064674 -0.89005864  0.6981058 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3010. State = [[-0.23757146 -0.07896198  0.03720783  1.        ]]. Action = [[0.30686116 0.74143183 0.3369044  0.8607304 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 3011. State = [[-0.23804368 -0.06908216  0.03804109  1.        ]]. Action = [[-0.90636563 -0.01314759 -0.7009493   0.5928242 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3012. State = [[-0.23785244 -0.06850208  0.03824452  1.        ]]. Action = [[ 0.8863915  -0.33092403 -0.26945156  0.5466901 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3013. State = [[-0.23785244 -0.06850208  0.03824452  1.        ]]. Action = [[-0.0518924   0.98223925 -0.4066609   0.64963484]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3014. State = [[-0.23592463 -0.05475933  0.04811985  1.        ]]. Action = [[-0.1513015   0.96046066  0.9292282   0.75802064]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 3015. State = [[-0.23484486 -0.03221706  0.0653512   1.        ]]. Action = [[ 0.52817464  0.4354732  -0.694306    0.42682672]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3016. State = [[-0.23373745 -0.02959621  0.06906207  1.        ]]. Action = [[-0.9343005   0.29277873 -0.29339623  0.5807862 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3017. State = [[-0.22336444 -0.03267461  0.07948853  1.        ]]. Action = [[ 0.89427805 -0.269688    0.7025813   0.54909897]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 3018. State = [[-0.20779213 -0.04680986  0.10261401  1.        ]]. Action = [[ 0.23662114 -0.78693825  0.51237535  0.79490113]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 3019. State = [[-0.20168006 -0.04875762  0.10856223  1.        ]]. Action = [[-0.03306484  0.72268665 -0.73549634  0.54009426]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 3020. State = [[-0.20759246 -0.05419197  0.10306226  1.        ]]. Action = [[-0.72923094 -0.8308081   0.24412     0.5813502 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 3021. State = [[-0.21632072 -0.08016291  0.10779293  1.        ]]. Action = [[-0.48438668 -0.8623815   0.383919    0.71347475]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 3022. State = [[-0.23152263 -0.087689    0.10929669  1.        ]]. Action = [[-0.08939385  0.6274234  -0.64886844  0.5452907 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 3023. State = [[-0.2366731  -0.08229766  0.10453585  1.        ]]. Action = [[-0.17173207  0.01813829  0.4061674   0.44598842]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 3024. State = [[-0.23729096 -0.07089384  0.11433312  1.        ]]. Action = [[0.09863138 0.73217034 0.63665104 0.57365656]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 3025. State = [[-0.23994142 -0.05202338  0.13543452  1.        ]]. Action = [[-0.4126792   0.23968077  0.77693856  0.82303655]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3026. State = [[-0.25287753 -0.03028132  0.15762144  1.        ]]. Action = [[-0.016159    0.9104148   0.25266492  0.5338664 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 3027. State = [[-0.26019144 -0.0128451   0.16141424  1.        ]]. Action = [[-0.18960595 -0.03671491 -0.635644    0.78486943]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 3028. State = [[-0.2550362  -0.0113353   0.15820195  1.        ]]. Action = [[ 0.8701284  -0.06910622  0.06514251  0.8762417 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 3029. State = [[-0.2446818  -0.00573181  0.15259348  1.        ]]. Action = [[ 0.25380325  0.2591952  -0.78522944  0.6864331 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 3030. State = [[-0.22649415  0.01273647  0.13123465  1.        ]]. Action = [[ 0.56684184  0.9312278  -0.24792975  0.7958276 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 3031. State = [[-0.20189638  0.04001758  0.13356403  1.        ]]. Action = [[0.8574091  0.63942397 0.97100854 0.7198305 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 3032. State = [[-0.17965896  0.0573035   0.15771504  1.        ]]. Action = [[0.10644722 0.02865243 0.9115472  0.50412846]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 3033. State = [[-0.17364149  0.05982478  0.18104455  1.        ]]. Action = [[-0.19201005 -0.09276032  0.31909096  0.55047727]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 3034. State = [[-0.1736711   0.05961165  0.18963598  1.        ]]. Action = [[ 0.614889    0.98499084 -0.59278685  0.7282814 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 3035. State = [[-0.17392999  0.05955524  0.18967855  1.        ]]. Action = [[0.70016694 0.4027561  0.18107319 0.6157539 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: No entry zone
Current timestep = 3036. State = [[-0.17681774  0.04682814  0.19890319  1.        ]]. Action = [[-0.60590255 -0.81536007  0.5576482   0.44954443]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 3037. State = [[-0.18383168  0.02107206  0.22596842  1.        ]]. Action = [[-0.1648295  -0.68912876  0.7210927   0.63846254]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 3038. State = [[-0.18860199  0.00496174  0.24461325  1.        ]]. Action = [[ 0.48883343 -0.8063299  -0.71513486  0.77093244]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: No entry zone
Current timestep = 3039. State = [[-0.1976974  -0.00233028  0.23621869  1.        ]]. Action = [[-0.56573915 -0.38202143 -0.994193    0.79266894]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 3040. State = [[-0.21276869 -0.00913173  0.22407487  1.        ]]. Action = [[ 0.79095185 -0.3878448  -0.85273385  0.7743449 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 3041. State = [[-0.2226549  -0.02446463  0.22758481  1.        ]]. Action = [[-0.7788558  -0.7823847   0.5544065   0.80767274]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 3042. State = [[-0.24555077 -0.05459516  0.22950117  1.        ]]. Action = [[-0.43149817 -0.9057548  -0.6597546   0.70213103]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 3043. State = [[-0.25039086 -0.08463082  0.21358384  1.        ]]. Action = [[ 0.7965207  -0.7720997  -0.91828465  0.7881696 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 3044. State = [[-0.23652603 -0.0924503   0.18268088  1.        ]]. Action = [[ 0.45264435  0.8674209  -0.3023445   0.6298001 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 3045. State = [[-0.23072916 -0.07274851  0.17132317  1.        ]]. Action = [[-0.31722057  0.4624685   0.06891918  0.7250391 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 3046. State = [[-0.23026368 -0.04948008  0.16845736  1.        ]]. Action = [[0.19501424 0.9794028  0.06588364 0.5011661 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 3047. State = [[-0.22964056 -0.022946    0.17591478  1.        ]]. Action = [[-0.00105143  0.2820351   0.8012444   0.7883053 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 3048. State = [[-0.234432   -0.01672872  0.18047692  1.        ]]. Action = [[-0.46686006 -0.23199743 -0.9565972   0.66106594]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 3049. State = [[-0.23418921 -0.02672467  0.16221718  1.        ]]. Action = [[ 0.5198877 -0.5070226 -0.9575202  0.6816044]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 3050. State = [[-0.22210822 -0.04567555  0.13477735  1.        ]]. Action = [[ 0.62160885 -0.74999505 -0.08300221  0.6126565 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 3051. State = [[-0.21536657 -0.06142979  0.11973862  1.        ]]. Action = [[-0.47224534  0.07195818 -0.6344635   0.86208344]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 3052. State = [[-0.22334988 -0.0734253   0.09573536  1.        ]]. Action = [[-0.4053247  -0.58436203 -0.83582604  0.36710763]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 3053. State = [[-0.23718211 -0.07069308  0.06477205  1.        ]]. Action = [[-0.64889145  0.9335699  -0.767228    0.8009449 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 3054. State = [[-0.2462193  -0.0543069   0.04131432  1.        ]]. Action = [[ 0.3997004  -0.84467036 -0.72163415  0.8071139 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3055. State = [[-0.24874534 -0.0494864   0.03662958  1.        ]]. Action = [[-0.03898758  0.10913837  0.06415427  0.8560698 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 3056. State = [[-0.24558207 -0.03791493  0.04420608  1.        ]]. Action = [[0.22059238 0.6593864  0.8550422  0.8133745 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 3057. State = [[-0.2521626  -0.0194158   0.06332687  1.        ]]. Action = [[-0.37610412  0.3873272   0.5455371   0.79236245]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 3058. State = [[-2.5182471e-01  2.8872452e-04  7.4562743e-02  1.0000000e+00]]. Action = [[ 0.9196534   0.5931523  -0.38505375  0.6473646 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 3059. State = [[-0.24474822  0.02429231  0.07383261  1.        ]]. Action = [[0.08806908 0.8019229  0.02226341 0.7385142 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 3060. State = [[-0.24331608  0.04042574  0.07391585  1.        ]]. Action = [[-0.95449805  0.32947993  0.392128    0.3681004 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3061. State = [[-0.23169346  0.05682155  0.07848152  1.        ]]. Action = [[0.73724294 0.80360484 0.403888   0.7431326 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 3062. State = [[-0.22327201  0.0814697   0.09480463  1.        ]]. Action = [[-0.7495284   0.3486234   0.89785504  0.12584889]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 3063. State = [[-0.23203412  0.09168746  0.11336208  1.        ]]. Action = [[-0.9637293   0.84687495 -0.85295546  0.6393722 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3064. State = [[-0.22786629  0.08698123  0.12610233  1.        ]]. Action = [[ 0.5531926  -0.35218132  0.78763914  0.5529047 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 3065. State = [[-0.20992835  0.08675114  0.14471658  1.        ]]. Action = [[ 0.6556269   0.43877983 -0.2168358   0.86502206]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 3066. State = [[-0.19868392  0.07883748  0.1520042   1.        ]]. Action = [[-0.3823828 -0.8866565  0.5283526  0.867607 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 3067. State = [[-0.20607078  0.05881503  0.15047422  1.        ]]. Action = [[-0.58014363 -0.48155951 -0.93766916  0.38909054]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 3068. State = [[-0.2151731   0.04781075  0.13393424  1.        ]]. Action = [[ 0.06991839 -0.05055732 -0.6375334   0.6515976 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 3069. State = [[-0.22144383  0.03719983  0.12396516  1.        ]]. Action = [[-0.6666134  -0.56149375  0.65072024  0.81711996]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 3070. State = [[-0.23177302  0.03509063  0.12939475  1.        ]]. Action = [[ 0.12882876  0.6711142  -0.1257897   0.7048929 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 3071. State = [[-0.23397224  0.03438779  0.12202049  1.        ]]. Action = [[-0.13801229 -0.5283149  -0.7642251   0.6857587 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 3072. State = [[-0.23580052  0.01794508  0.10104974  1.        ]]. Action = [[-0.11859208 -0.79906994 -0.62913895  0.7657604 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 3073. State = [[-0.2390518   0.00292425  0.08455993  1.        ]]. Action = [[-0.24686992  0.11926162 -0.14253908  0.7149637 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 3074. State = [[-0.24765606  0.00203927  0.08006954  1.        ]]. Action = [[-0.654865    0.56596756 -0.7876814   0.45300055]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3075. State = [[-0.251174    0.01394418  0.07690489  1.        ]]. Action = [[-0.08970881  0.82935643 -0.18505937  0.5701194 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 3076. State = [[-0.25599456  0.04398444  0.0688227   1.        ]]. Action = [[-0.21836674  0.9180697  -0.13684034  0.81817126]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 3077. State = [[-0.2592575   0.06183121  0.06391411  1.        ]]. Action = [[-0.51887083 -0.1129353   0.4139471   0.63471484]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3078. State = [[-0.25556493  0.06372347  0.06503456  1.        ]]. Action = [[ 0.5775151  -0.17871583  0.35197258  0.60015655]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 3079. State = [[-0.25321057  0.06309275  0.06601671  1.        ]]. Action = [[-0.5861265  -0.07684505 -0.7941306   0.78721666]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3080. State = [[-0.25302202  0.0630701   0.06621066  1.        ]]. Action = [[-0.48451757  0.85989285 -0.17890304  0.75498676]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3081. State = [[-0.25350687  0.07538479  0.06381039  1.        ]]. Action = [[ 0.05054164  0.9340991  -0.4479714   0.7866051 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 3082. State = [[-0.25161865  0.08974499  0.0650607   1.        ]]. Action = [[-0.10979462 -0.04050958  0.5873804   0.7852013 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 3083. State = [[-0.2626374   0.03503026  0.12138299  1.        ]]. Action = [[-0.67412055  0.48877263 -0.6205543   0.35485148]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 3084. State = [[-0.26058996  0.03144241  0.10232551  1.        ]]. Action = [[-0.06356758 -0.8020514  -0.46111524  0.6326591 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3085. State = [[-0.26279297  0.02148519  0.09034593  1.        ]]. Action = [[-0.3922336   0.7088399   0.50305784  0.8282082 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3086. State = [[-0.26390216  0.02259178  0.08682207  1.        ]]. Action = [[ 0.00173199  0.340829   -0.29084712  0.4532441 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3087. State = [[-0.26467025  0.02343312  0.07900289  1.        ]]. Action = [[-0.3156544   0.2735499  -0.32068592  0.7811444 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3088. State = [[-0.26408374  0.02353145  0.07736558  1.        ]]. Action = [[-0.21416378  0.4414301  -0.7554173   0.8454757 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3089. State = [[-0.26398617  0.02392548  0.07737029  1.        ]]. Action = [[-0.28962696  0.4526472   0.5998485   0.7752979 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3090. State = [[-0.262787    0.01435245  0.07775325  1.        ]]. Action = [[-0.06210411 -0.75256354  0.2073065   0.7631433 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3091. State = [[-0.26204172 -0.00144199  0.07883569  1.        ]]. Action = [[ 0.07517147 -0.41179097 -0.13456416  0.8458942 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 3092. State = [[-2.6061013e-01 -3.0294608e-04  7.1810298e-02  1.0000000e+00]]. Action = [[ 0.2458942  0.6497843 -0.7917568  0.8227639]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 3093. State = [[-0.2568513   0.00334839  0.0527467   1.        ]]. Action = [[-0.6551121   0.97815204  0.49433148  0.81890357]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3094. State = [[-0.24240947  0.00463941  0.05780701  1.        ]]. Action = [[0.9406712 0.0163821 0.7312783 0.7230644]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 3095. State = [[-0.22644061  0.00533968  0.06389686  1.        ]]. Action = [[-0.75209826  0.18285012  0.41665554  0.5543957 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3096. State = [[-0.22436337  0.00526469  0.06470841  1.        ]]. Action = [[ 0.5214428   0.68028784 -0.94917935  0.67796564]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3097. State = [[-0.21790713 -0.0065626   0.07268506  1.        ]]. Action = [[ 0.30539608 -0.83906925  0.665903    0.77575254]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 3098. State = [[-0.2113034  -0.02864382  0.08690342  1.        ]]. Action = [[-0.08231634 -0.7598041   0.01372504  0.6035043 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 3099. State = [[-0.20331086 -0.04156308  0.08239645  1.        ]]. Action = [[ 0.6353874   0.21545064 -0.77529436  0.73354745]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 3100. State = [[-0.20064418 -0.04135866  0.06529788  1.        ]]. Action = [[-0.62024885 -0.00181121 -0.8822822   0.7660829 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 3101. State = [[-0.21213396 -0.04251153  0.03887945  1.        ]]. Action = [[-0.22553062 -0.09487617 -0.3784839   0.45476222]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 3102. State = [[-0.20513777 -0.04589115  0.03222369  1.        ]]. Action = [[ 0.7783524  -0.16594887  0.46559596  0.667392  ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 3103. State = [[-0.19497977 -0.04448802  0.03581473  1.        ]]. Action = [[-0.32332683  0.47151232  0.21427941  0.48564887]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 3104. State = [[-0.19461752 -0.04199049  0.03690141  1.        ]]. Action = [[-0.7284579  -0.23277128 -0.6789928   0.870353  ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 3105. State = [[-0.19444662 -0.04130668  0.03706805  1.        ]]. Action = [[-0.8902814  0.9405935 -0.4446727  0.5293524]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 3106. State = [[-0.19450891 -0.04093843  0.0370683   1.        ]]. Action = [[ 0.4464004  -0.14815217 -0.97815216  0.7155483 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3107. State = [[-0.18889734 -0.04205516  0.04843175  1.        ]]. Action = [[ 0.37464392 -0.18959779  0.9118825   0.38591278]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 3108. State = [[-0.1832631  -0.03563119  0.07125879  1.        ]]. Action = [[-0.3235218   0.44990683  0.13412881  0.7651553 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 3109. State = [[-0.19985297 -0.0215803   0.06909116  1.        ]]. Action = [[-0.9562539   0.48064673 -0.6297048   0.7248088 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 3110. State = [[-2.2685821e-01 -1.4610915e-04  6.7785449e-02  1.0000000e+00]]. Action = [[-0.82437813  0.7814081   0.51625574  0.34718895]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 3111. State = [[-0.24922222  0.02744326  0.08200973  1.        ]]. Action = [[-0.31083965  0.51138926  0.63083696  0.49080324]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 3112. State = [[-0.26158395  0.04029219  0.0960085   1.        ]]. Action = [[-0.3503148  -0.07876003  0.15473986  0.5776975 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 3113. State = [[-0.2731682   0.03415633  0.09277337  1.        ]]. Action = [[ 0.07922077 -0.47701657 -0.96848834  0.80181885]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 3114. State = [[-0.27788332  0.03735247  0.07734003  1.        ]]. Action = [[ 0.08179212  0.740692   -0.2015776   0.75337994]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 3115. State = [[-0.27890763  0.04483212  0.07441104  1.        ]]. Action = [[-0.29628623  0.5948386  -0.40160626  0.33604538]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3116. State = [[-0.26861787  0.05760868  0.0812428   1.        ]]. Action = [[0.88328314 0.72985244 0.83005714 0.6960404 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 3117. State = [[-0.25644046  0.07126675  0.09032816  1.        ]]. Action = [[-0.34264535  0.60210013  0.21169007  0.82460964]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 3118. State = [[-0.2553313   0.07236011  0.09118824  1.        ]]. Action = [[-0.44192314  0.8570471  -0.14222789  0.68273365]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3119. State = [[-0.25466144  0.0725694   0.09154131  1.        ]]. Action = [[-0.8786933   0.8357494   0.15277767  0.5039358 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3120. State = [[-0.24817908  0.06258516  0.09888843  1.        ]]. Action = [[ 0.34863973 -0.78493905  0.6128051   0.50454986]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 3121. State = [[-0.23888788  0.05404533  0.1234277   1.        ]]. Action = [[0.03752542 0.10532653 0.96238804 0.7622193 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 3122. State = [[-0.23006974  0.06554727  0.14360641  1.        ]]. Action = [[ 0.43035245  0.7867935  -0.45203018  0.7657583 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 3123. State = [[-0.22745216  0.0847767   0.15084238  1.        ]]. Action = [[-0.2892745   0.50903296  0.8474257   0.80331326]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 3124. State = [[-0.21653026  0.09201249  0.16774717  1.        ]]. Action = [[ 0.86502254 -0.4314298   0.03727686  0.54355454]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 3125. State = [[-0.20759673  0.10248448  0.16875282  1.        ]]. Action = [[-0.17375815  0.9296639  -0.50055015  0.7739999 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 3126. State = [[-0.20428492  0.10995866  0.15245698  1.        ]]. Action = [[ 0.33090234 -0.34423566 -0.93471116  0.7218909 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 3127. State = [[-0.18841583  0.11287338  0.13614717  1.        ]]. Action = [[0.413795   0.2512424  0.34863806 0.83018124]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3128. State = [[-0.17815563  0.12420934  0.14269258  1.        ]]. Action = [[0.2469933  0.57283044 0.345937   0.3003943 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 3129. State = [[-0.17303692  0.13119991  0.14531994  1.        ]]. Action = [[-0.28349304 -0.32481265 -0.3413396   0.6527488 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 3130. State = [[-0.1776883  0.129637   0.1349325  1.       ]]. Action = [[-0.23619902 -0.00261253 -0.53906655  0.68813086]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 3131. State = [[-0.18118349  0.12865052  0.12450778  1.        ]]. Action = [[0.4996928  0.04539275 0.74423504 0.63312125]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 3132. State = [[-0.18338537  0.11932188  0.12288353  1.        ]]. Action = [[-0.35877323 -0.6689613   0.02675569  0.6922325 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 3133. State = [[-0.19771883  0.09825382  0.11775839  1.        ]]. Action = [[-0.95842713 -0.6558671  -0.16354358  0.6927035 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 3134. State = [[-0.21910371  0.0778085   0.10351216  1.        ]]. Action = [[-0.13401377 -0.45174074 -0.6313368   0.73904824]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 3135. State = [[-0.22665375  0.07788122  0.08525468  1.        ]]. Action = [[ 0.51506925  0.6642549  -0.952654    0.5984044 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 3136. State = [[-0.22544806  0.09691594  0.05204058  1.        ]]. Action = [[-0.03223211  0.8369496  -0.54596305  0.70166135]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 3137. State = [[-0.223189    0.11126406  0.03741442  1.        ]]. Action = [[-0.5075559  -0.62501836 -0.66013706  0.68858397]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3138. State = [[-0.22318453  0.1139068   0.03682099  1.        ]]. Action = [[ 0.26850283 -0.32114363 -0.42786336  0.7811903 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Current timestep = 3139. State = [[-0.21310143  0.12824273  0.03929129  1.        ]]. Action = [[0.7267461  0.88115036 0.2778852  0.27456164]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 3140. State = [[-0.20233808  0.141037    0.05098889  1.        ]]. Action = [[-0.31974286 -0.2834497   0.9693489   0.31110442]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 3141. State = [[-0.19885466  0.14237423  0.0651816   1.        ]]. Action = [[ 0.62210774  0.4685428  -0.67323923  0.56210685]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3142. State = [[-0.2123229   0.14828582  0.06104887  1.        ]]. Action = [[-0.8678287   0.38574767 -0.51034456  0.8201461 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 3143. State = [[-0.22538257  0.15674439  0.05882864  1.        ]]. Action = [[ 0.6507032   0.00536227 -0.8411638   0.636405  ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3144. State = [[-0.22437     0.14582357  0.05856714  1.        ]]. Action = [[ 0.06299329 -0.83240217 -0.04504132  0.5814065 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 3145. State = [[-0.21939631  0.13146013  0.06123741  1.        ]]. Action = [[ 0.5096327  -0.07627696  0.3351847   0.83260787]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 3146. State = [[-0.20733045  0.12346421  0.05982452  1.        ]]. Action = [[ 0.7887757  -0.1226958  -0.4959333   0.54491997]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 3147. State = [[-0.19165803  0.12239907  0.05788463  1.        ]]. Action = [[0.2685635  0.15853119 0.22602367 0.67001915]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 3148. State = [[-0.1955999   0.13081864  0.05446681  1.        ]]. Action = [[-0.81540966  0.46394515 -0.57529646  0.8246906 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 3149. State = [[-0.19972353  0.13246305  0.04296363  1.        ]]. Action = [[ 0.49230456 -0.31615794 -0.04203308  0.55635977]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 3150. State = [[-0.19053262  0.12159494  0.04307561  1.        ]]. Action = [[ 0.11915565 -0.6255711   0.37826312  0.7870704 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 3151. State = [[-0.18292229  0.11530422  0.0470763   1.        ]]. Action = [[0.3053     0.35596943 0.2392137  0.61874104]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 3152. State = [[-0.17859435  0.11681274  0.04893114  1.        ]]. Action = [[ 0.93232703 -0.09769344  0.6792948   0.8227477 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: No entry zone
Current timestep = 3153. State = [[-0.1776873   0.11711227  0.04927645  1.        ]]. Action = [[-0.6640015  0.115376  -0.5723055  0.738188 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3154. State = [[-0.17647952  0.11755006  0.04977891  1.        ]]. Action = [[-0.08171582  0.64460635 -0.53472006  0.5445051 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3155. State = [[-0.17566636  0.11790298  0.05018758  1.        ]]. Action = [[ 0.56292725 -0.0721851  -0.01695836  0.7731637 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: No entry zone
Current timestep = 3156. State = [[-0.17533562  0.10983473  0.06282555  1.        ]]. Action = [[-0.57268727 -0.608435    0.96803045  0.5652312 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 3157. State = [[-0.17953803  0.10139776  0.08349314  1.        ]]. Action = [[ 0.12694478 -0.47418177  0.96015334  0.70532656]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: No entry zone
Current timestep = 3158. State = [[-0.18182997  0.10031018  0.08427043  1.        ]]. Action = [[ 0.81794024 -0.7926755   0.43911815  0.63487136]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: No entry zone
Current timestep = 3159. State = [[-0.183775    0.10336473  0.09737544  1.        ]]. Action = [[-0.22731596  0.21116412  0.9804331   0.6995264 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 3160. State = [[-0.19087425  0.09163982  0.12570377  1.        ]]. Action = [[-0.26522398 -0.8740962   0.35527754  0.6935704 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 3161. State = [[-0.195855    0.0809226   0.14841725  1.        ]]. Action = [[0.16206634 0.26075768 0.76486397 0.651407  ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 3162. State = [[-0.19314092  0.0794023   0.17809299  1.        ]]. Action = [[ 0.14283049 -0.17368191  0.936764    0.7504432 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 3163. State = [[-0.19787619  0.06503814  0.19519146  1.        ]]. Action = [[-0.49054945 -0.70442766 -0.50349957  0.866081  ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 3164. State = [[-0.19839182  0.04071455  0.18945861  1.        ]]. Action = [[ 0.58337283 -0.71865654 -0.5002462   0.5672283 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 3165. State = [[-0.18683453  0.0093396   0.1790438   1.        ]]. Action = [[ 0.48074543 -0.86357766 -0.42135715  0.64880717]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 3166. State = [[-0.18306799  0.00247109  0.16863547  1.        ]]. Action = [[-0.6161088   0.9298303   0.21651089  0.63847613]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 3167. State = [[-0.18692765  0.01394852  0.16842154  1.        ]]. Action = [[ 0.668712   -0.14163041  0.15707374  0.65298414]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: No entry zone
Current timestep = 3168. State = [[-0.19317675  0.01477164  0.17789565  1.        ]]. Action = [[-0.6777429  -0.1409077   0.77026105  0.67617416]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 3169. State = [[-0.20734029  0.01431225  0.19082355  1.        ]]. Action = [[ 0.7130959  -0.353904    0.7775259   0.68153954]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: No entry zone
Current timestep = 3170. State = [[-2.1500850e-01 -2.4942629e-04  1.8711081e-01  1.0000000e+00]]. Action = [[-0.411245   -0.89711523 -0.5745478   0.5784004 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 3171. State = [[-0.23677534 -0.03035586  0.17774525  1.        ]]. Action = [[-0.78586197 -0.94452316 -0.41581738  0.4989096 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 3172. State = [[-0.2548461  -0.06660271  0.15952343  1.        ]]. Action = [[ 0.05112648 -0.7947182  -0.7390376   0.7238581 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 3173. State = [[-0.2604044  -0.07198755  0.13216546  1.        ]]. Action = [[ 0.00188541  0.98662245 -0.7635408   0.7456275 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 3174. State = [[-0.2585575  -0.05678317  0.11523397  1.        ]]. Action = [[0.05666113 0.07397974 0.39101243 0.6794902 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 3175. State = [[-0.25133103 -0.05036373  0.12504959  1.        ]]. Action = [[0.5695113 0.1282444 0.945606  0.6008774]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 3176. State = [[-0.24568363 -0.0381188   0.13415764  1.        ]]. Action = [[ 0.03809285  0.56231225 -0.6385724   0.7903166 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 3177. State = [[-0.24563454 -0.03600718  0.12066375  1.        ]]. Action = [[ 0.07222641 -0.5044629  -0.9931524   0.71278787]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 3178. State = [[-0.24787992 -0.02586531  0.09410454  1.        ]]. Action = [[-0.4333955   0.97446156 -0.2342825   0.36637592]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 3179. State = [[-0.24957201 -0.01043733  0.08509142  1.        ]]. Action = [[-0.7267528   0.40135217 -0.81634605  0.80340433]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Current timestep = 3180. State = [[-0.24441849 -0.01596622  0.08501216  1.        ]]. Action = [[ 0.6494899  -0.6304461  -0.00717312  0.7186277 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 3181. State = [[-0.24009277 -0.02477434  0.08522526  1.        ]]. Action = [[-0.67413247  0.9730356  -0.8359591   0.55895615]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3182. State = [[-0.2265441  -0.03110704  0.08330396  1.        ]]. Action = [[ 0.930135   -0.3105191  -0.27455807  0.6817069 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 3183. State = [[-0.21421327 -0.02572091  0.0752898   1.        ]]. Action = [[-0.57833445  0.8222511   0.42954385  0.6173785 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 3184. State = [[-0.21357818 -0.02135412  0.07980577  1.        ]]. Action = [[ 0.32388425 -0.5997701   0.28969288  0.650018  ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 3185. State = [[-0.25878906  0.08360108  0.12193592  1.        ]]. Action = [[-0.3446653   0.7397864   0.21986341  0.83236647]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 3186. State = [[-0.24531603  0.08968786  0.10217486  1.        ]]. Action = [[ 0.84681225 -0.3383093  -0.80505526  0.56163263]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3187. State = [[-0.22870131  0.09518686  0.08207859  1.        ]]. Action = [[-0.00912708  0.59674466  0.15119135  0.72632265]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3188. State = [[-0.23502462  0.11003249  0.07491827  1.        ]]. Action = [[-0.47984213  0.40815866 -0.76296896  0.730304  ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3189. State = [[-0.24271959  0.11911263  0.05613923  1.        ]]. Action = [[-0.80418235  0.9430971   0.23290062  0.80255485]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3190. State = [[-0.24195008  0.12076631  0.05372144  1.        ]]. Action = [[-0.64510316 -0.03014767 -0.2299555   0.775074  ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3191. State = [[-0.228491    0.11971162  0.05817053  1.        ]]. Action = [[ 0.90495896 -0.09794921  0.5403819   0.7686614 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 3192. State = [[-0.22037473  0.13028996  0.06363774  1.        ]]. Action = [[-0.4095471   0.6902522   0.15485084  0.63154197]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3193. State = [[-0.22473502  0.14294407  0.06569921  1.        ]]. Action = [[-0.03031558 -0.5733719  -0.7907819   0.7841921 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Current timestep = 3194. State = [[-0.23558551  0.14979963  0.06318802  1.        ]]. Action = [[-0.82776713  0.25370944 -0.35217845  0.07277799]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 3195. State = [[-0.23732305  0.15013781  0.07114163  1.        ]]. Action = [[ 0.49642694 -0.39714372  0.93587303  0.7830758 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 3196. State = [[-0.22539358  0.1381903   0.07915169  1.        ]]. Action = [[ 0.67022514 -0.29274964 -0.64997727  0.60695505]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 3197. State = [[-0.20749007  0.13895865  0.07216126  1.        ]]. Action = [[ 0.82855487  0.41041613 -0.2914217   0.44530082]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 3198. State = [[-0.1986403   0.14658341  0.06258384  1.        ]]. Action = [[-0.91806275  0.03383422 -0.04025829  0.76277065]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 3199. State = [[-0.20346755  0.14886627  0.05984601  1.        ]]. Action = [[ 0.5864012  0.8269205 -0.9041838  0.7862457]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3200. State = [[-0.1963723   0.13595375  0.06346778  1.        ]]. Action = [[ 0.6992936  -0.79383314  0.5072968   0.61376595]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 3201. State = [[-0.19001202  0.12668724  0.06902569  1.        ]]. Action = [[ 0.04332387  0.32936573 -0.11670953  0.70524657]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 3202. State = [[-0.18826418  0.12765048  0.06972701  1.        ]]. Action = [[ 0.6652589  -0.04799682 -0.23901331  0.5036032 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 3203. State = [[-0.19581579  0.12845829  0.06385763  1.        ]]. Action = [[-7.5793761e-01 -7.1811676e-04 -5.5010742e-01  5.7517290e-01]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 3204. State = [[-0.20259641  0.12945646  0.05734599  1.        ]]. Action = [[ 0.5937954  -0.09614164  0.7197559   0.6197947 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: No entry zone
Current timestep = 3205. State = [[-0.20555559  0.13115138  0.05717934  1.        ]]. Action = [[-0.26351213  0.02461863  0.22531474  0.7922752 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 3206. State = [[-0.21374035  0.13641371  0.06356224  1.        ]]. Action = [[-0.5217417   0.1301707   0.62617517  0.6331911 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 3207. State = [[-0.23248886  0.13623019  0.0702185   1.        ]]. Action = [[-0.4442895  -0.25823605 -0.4345351   0.85141134]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 3208. State = [[-0.24882221  0.1398016   0.05991591  1.        ]]. Action = [[-0.01022816  0.64763343 -0.68196976  0.8071413 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 3209. State = [[-0.25427154  0.14693418  0.04668088  1.        ]]. Action = [[ 0.16046178  0.04981494 -0.22317547  0.44001567]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 3210. State = [[-0.2547572   0.14962171  0.04318551  1.        ]]. Action = [[ 0.41465068  0.367203   -0.51655906  0.36727095]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3211. State = [[-0.24782208  0.14951748  0.04737414  1.        ]]. Action = [[ 0.42213356 -0.2205711   0.704864    0.539624  ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 3212. State = [[-0.242024    0.14503752  0.05171207  1.        ]]. Action = [[ 0.0447849  -0.14726979 -0.29951113  0.616912  ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 3213. State = [[-0.2404804   0.14358227  0.05211434  1.        ]]. Action = [[-0.14064538  0.07311428  0.19348502  0.7663636 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 3214. State = [[-0.24041048  0.14360696  0.05213336  1.        ]]. Action = [[-0.85946935  0.7028744  -0.09221375  0.36602545]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3215. State = [[-0.23694438  0.14937843  0.05349069  1.        ]]. Action = [[ 0.3265133   0.47858024 -0.06975073  0.7149782 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 3216. State = [[-0.23450536  0.15154062  0.05288374  1.        ]]. Action = [[-0.18154448 -0.37021887 -0.14219588  0.6182406 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 3217. State = [[-0.2333573   0.14421463  0.05126587  1.        ]]. Action = [[-0.00524992 -0.39331603  0.01512659  0.4349122 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 3218. State = [[-0.23600478  0.1425517   0.05849567  1.        ]]. Action = [[-0.57958037  0.15879345  0.8529246   0.43648124]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 3219. State = [[-0.23951215  0.14174308  0.08021784  1.        ]]. Action = [[ 0.14179659 -0.07500851  0.7638979   0.7583909 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 3220. State = [[-0.24043262  0.14061576  0.09864585  1.        ]]. Action = [[-0.7484974  -0.29582047 -0.08021939  0.67150307]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 3221. State = [[-0.24778546  0.15229248  0.1057068   1.        ]]. Action = [[-0.24727702  0.92210424  0.37442732  0.4380871 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 3222. State = [[-0.25231856  0.17749533  0.11885799  1.        ]]. Action = [[0.43721175 0.6358025  0.26163387 0.6173928 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 3223. State = [[-0.24624906  0.19432122  0.13474147  1.        ]]. Action = [[0.02499557 0.14871144 0.6737317  0.78610015]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 3224. State = [[-0.24160117  0.19139014  0.15584856  1.        ]]. Action = [[-0.04974079 -0.574949    0.4054283   0.75601184]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 3225. State = [[-0.24016221  0.18177684  0.166746    1.        ]]. Action = [[-0.57782996  0.102193   -0.07009387  0.6138389 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3226. State = [[-0.23360468  0.18174367  0.17042734  1.        ]]. Action = [[0.64242387 0.08326828 0.0761112  0.52857757]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 3227. State = [[-0.23177558  0.17992121  0.16943525  1.        ]]. Action = [[-0.7129959  -0.36979675 -0.6311887   0.60055137]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 3228. State = [[-0.23624006  0.17603834  0.15839659  1.        ]]. Action = [[ 0.03427064  0.05891001 -0.3008852   0.27264082]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 3229. State = [[-0.24034931  0.16360243  0.15218394  1.        ]]. Action = [[-0.33348393 -0.7606924  -0.00552338  0.80936027]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3230. State = [[-0.24963558  0.14012375  0.15746747  1.        ]]. Action = [[-0.45226085 -0.7023785   0.78485024  0.7621887 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 3231. State = [[-0.25779745  0.11635159  0.17026597  1.        ]]. Action = [[ 0.09958053 -0.45557952 -0.01738918  0.52038753]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 3232. State = [[-0.25219625  0.10737545  0.1714258   1.        ]]. Action = [[ 0.6988114   0.20165193 -0.28781533  0.5100223 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 3233. State = [[-0.25145304  0.11252719  0.16365276  1.        ]]. Action = [[-0.22810084  0.43713892 -0.5308208   0.8485588 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 3234. State = [[-0.25360683  0.12567101  0.16127385  1.        ]]. Action = [[-0.1400919   0.449852    0.8820487   0.57134175]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 3235. State = [[-0.25564587  0.13492388  0.16836023  1.        ]]. Action = [[-0.38697422  0.4826455   0.01961052  0.3779912 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3236. State = [[-0.25266328  0.13046567  0.16305247  1.        ]]. Action = [[ 0.39200926 -0.35658967 -0.6951638   0.66046095]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 3237. State = [[-0.24506524  0.12950319  0.15942506  1.        ]]. Action = [[0.20909202 0.01480114 0.19912732 0.7562437 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 3238. State = [[-0.23254551  0.11715307  0.15260136  1.        ]]. Action = [[ 0.7010081  -0.7240612  -0.93425673  0.6696937 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 3239. State = [[-0.2081984   0.08939364  0.12852581  1.        ]]. Action = [[ 0.46819246 -0.7191405  -0.33973253  0.80562544]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 3240. State = [[-0.19417135  0.07063646  0.12415895  1.        ]]. Action = [[ 0.01522529 -0.21503288  0.69158196  0.8639473 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 3241. State = [[-0.199571    0.05822331  0.12313787  1.        ]]. Action = [[-0.93951553 -0.46739876 -0.74887204  0.7901782 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 3242. State = [[-0.20824821  0.03890731  0.11677818  1.        ]]. Action = [[ 0.2560637  -0.6227288   0.15059543  0.74467397]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 3243. State = [[-0.20673695  0.01601792  0.10772374  1.        ]]. Action = [[-0.18966216 -0.38761687 -0.76480824  0.608544  ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 3244. State = [[-0.21740025 -0.00360865  0.09858843  1.        ]]. Action = [[-0.8662849  -0.59417397  0.548332    0.67812896]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 3245. State = [[-0.2420688  -0.01260336  0.10544117  1.        ]]. Action = [[-0.68764025  0.19585276  0.30499578  0.7171607 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 3246. State = [[-0.2548795  -0.02178523  0.12269048  1.        ]]. Action = [[ 0.13924634 -0.5921236   0.79940057  0.72721577]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 3247. State = [[-0.26628113 -0.02533593  0.12979575  1.        ]]. Action = [[-0.17537743  0.40823686 -0.9503844   0.5449996 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 3248. State = [[-0.27017674 -0.02160332  0.12006061  1.        ]]. Action = [[-0.76418     0.8966569   0.33571362  0.8249692 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Current timestep = 3249. State = [[-0.2696416  -0.01932612  0.11924455  1.        ]]. Action = [[-0.48699093 -0.1288237  -0.3849383   0.6522846 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Current timestep = 3250. State = [[-0.26953575 -0.0192479   0.11916171  1.        ]]. Action = [[-0.11651492  0.56754065 -0.6867071   0.77893734]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3251. State = [[-0.26442376 -0.02545824  0.11269173  1.        ]]. Action = [[ 0.66021276 -0.3566382  -0.5900014   0.5947298 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 3252. State = [[-0.25650164 -0.03330361  0.09625381  1.        ]]. Action = [[-0.5225776  0.6218504 -0.6739126  0.6651664]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Current timestep = 3253. State = [[-0.2512173  -0.04675298  0.0895851   1.        ]]. Action = [[ 0.13493276 -0.7177893  -0.3979739   0.8241308 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 3254. State = [[-0.24112576 -0.04812608  0.08126304  1.        ]]. Action = [[0.6976378  0.7279618  0.04552615 0.7934439 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 3255. State = [[-0.23255807 -0.03665043  0.07978115  1.        ]]. Action = [[-0.62368447 -0.6052089  -0.3375442   0.77092266]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3256. State = [[-0.2230073  -0.02445092  0.08446193  1.        ]]. Action = [[0.6142998 0.713645  0.6491437 0.6354008]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 3257. State = [[-0.22104515  0.00244441  0.08184111  1.        ]]. Action = [[-0.8156736  0.9152038 -0.7417257  0.7842908]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 3258. State = [[-0.22620636  0.02517187  0.07699234  1.        ]]. Action = [[0.37985754 0.24861288 0.4081564  0.87266135]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 3259. State = [[-0.22640584  0.02605548  0.07369623  1.        ]]. Action = [[-0.26806998 -0.51570493 -0.4998541   0.60368824]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 3260. State = [[-0.22267449  0.03554462  0.08063714  1.        ]]. Action = [[0.44925714 0.96136713 0.95242107 0.74043703]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 3261. State = [[-0.22417094  0.04818087  0.08328837  1.        ]]. Action = [[-0.25659847 -0.07753032 -0.90708965  0.59783757]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 3262. State = [[-0.22687817  0.04828567  0.07423048  1.        ]]. Action = [[-0.963101  -0.1706785 -0.5951361  0.4577726]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3263. State = [[-0.22796209  0.05917526  0.06689408  1.        ]]. Action = [[ 0.13460195  0.6551652  -0.8702509   0.7636131 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 3264. State = [[-0.22856675  0.07862175  0.03785441  1.        ]]. Action = [[-0.20882154  0.4398228  -0.46950507  0.5717958 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 3265. State = [[-0.2313954   0.08820548  0.02543852  1.        ]]. Action = [[-0.4907031  -0.8760294  -0.73824733  0.81733084]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3266. State = [[-0.23102807  0.08923625  0.0253977   1.        ]]. Action = [[ 0.36109912 -0.31090653 -0.37542439  0.8109338 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3267. State = [[-0.23095389  0.08925249  0.02541168  1.        ]]. Action = [[ 0.09864295  0.5400411  -0.69035304  0.7548828 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3268. State = [[-0.23097803  0.08939744  0.02539574  1.        ]]. Action = [[ 0.8167325  -0.45542526 -0.22941011  0.8636124 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Current timestep = 3269. State = [[-0.23097803  0.08939744  0.02539574  1.        ]]. Action = [[-0.80274636 -0.13974178 -0.61261976  0.8223684 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3270. State = [[-0.23098975  0.08946778  0.02538801  1.        ]]. Action = [[ 0.6329138   0.79893136 -0.21848363  0.69585085]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Current timestep = 3271. State = [[-0.22612955  0.09812195  0.02784957  1.        ]]. Action = [[0.36080587 0.46009517 0.41045356 0.8088329 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 3272. State = [[-0.22052321  0.1076259   0.03013492  1.        ]]. Action = [[-0.86526483 -0.36109126 -0.22620082  0.73160315]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3273. State = [[-0.20835924  0.12129121  0.03925903  1.        ]]. Action = [[0.872581   0.7351645  0.7028129  0.67034173]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 3274. State = [[-0.19431904  0.1252636   0.05360178  1.        ]]. Action = [[-0.46194082 -0.71329916  0.33372486  0.8158307 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 3275. State = [[-0.2016221   0.11463901  0.06612119  1.        ]]. Action = [[-0.78832626 -0.14579654  0.42976785  0.45417976]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 3276. State = [[-0.21515346  0.12289872  0.08543176  1.        ]]. Action = [[0.04714262 0.64739525 0.5522318  0.5093756 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 3277. State = [[-0.21819915  0.12897274  0.1094704   1.        ]]. Action = [[ 0.10487628 -0.18622673  0.75378096  0.72862625]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 3278. State = [[-0.21820791  0.11273701  0.11962319  1.        ]]. Action = [[-0.0033918  -0.90665466 -0.8335349   0.5524924 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 3279. State = [[-0.22220969  0.11097165  0.11670272  1.        ]]. Action = [[-0.42771888  0.86543393  0.5238172   0.67992496]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 3280. State = [[-0.22730823  0.11106915  0.11803251  1.        ]]. Action = [[ 0.10204005 -0.84731966 -0.27047133  0.53831685]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 3281. State = [[-0.22256227  0.09188102  0.12144879  1.        ]]. Action = [[ 0.23955798 -0.44383985  0.33466303  0.49976516]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 3282. State = [[-0.21858948  0.08641473  0.1259727   1.        ]]. Action = [[0.19061792 0.34463358 0.16896307 0.71735454]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 3283. State = [[-0.2285793   0.08147156  0.11925247  1.        ]]. Action = [[-0.98941785 -0.4319136  -0.8844959   0.7787682 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 3284. State = [[-0.23547171  0.06283375  0.1180929   1.        ]]. Action = [[ 0.24263644 -0.81335324  0.5687642   0.46776056]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 3285. State = [[-0.22615054  0.03784791  0.11815731  1.        ]]. Action = [[ 0.6619072  -0.66759497 -0.5307171   0.8749728 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 3286. State = [[-0.20686589  0.02812112  0.1198715   1.        ]]. Action = [[0.9249629  0.67312515 0.6786654  0.67468286]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 3287. State = [[-0.26324138  0.08940885  0.11396497  1.        ]]. Action = [[ 0.25057602 -0.25380057 -0.9258151   0.83027506]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 3288. State = [[-0.25739613  0.08737331  0.1005548   1.        ]]. Action = [[-0.0093618  -0.9008817   0.10669589  0.67801523]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3289. State = [[-0.25184563  0.07295754  0.09435225  1.        ]]. Action = [[ 0.52876735 -0.11516136 -0.7821871   0.7226505 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 3290. State = [[-0.24508911  0.06904212  0.07615837  1.        ]]. Action = [[-0.52687025  0.6255052   0.35366654  0.6975763 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 3291. State = [[-0.23068316  0.0633219   0.07914205  1.        ]]. Action = [[ 0.7721553  -0.30262268  0.4153558   0.63865006]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 3292. State = [[-0.21893136  0.0440758   0.08075861  1.        ]]. Action = [[-0.35656983 -0.8153254  -0.2324416   0.6006712 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 3293. State = [[-0.21609846  0.03311624  0.08154041  1.        ]]. Action = [[0.25124228 0.25579453 0.3918227  0.40626693]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 3294. State = [[-0.21506521  0.01925322  0.0846998   1.        ]]. Action = [[-0.3844272  -0.8713254   0.2151587   0.33835113]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3295. State = [[-0.2209548  -0.00403893  0.09893445  1.        ]]. Action = [[-0.58747464 -0.54066974  0.9655576   0.7438977 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 3296. State = [[-0.23397271 -0.01964321  0.11490288  1.        ]]. Action = [[ 0.25575447 -0.22869515 -0.814134    0.78854954]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 3297. State = [[-0.23418564 -0.02658755  0.10620401  1.        ]]. Action = [[ 0.2366662   0.08371818 -0.17647755  0.87292206]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 3298. State = [[-0.23252538 -0.03492237  0.09645746  1.        ]]. Action = [[-0.00852269 -0.49998522 -0.9577842   0.8520769 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 3299. State = [[-0.2325343  -0.04421483  0.0692809   1.        ]]. Action = [[-0.7391661  -0.6557982  -0.97518504  0.73814046]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Current timestep = 3300. State = [[-0.22684312 -0.04090828  0.06264819  1.        ]]. Action = [[ 0.48593807  0.23809898 -0.38238072  0.60150754]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 3301. State = [[-0.21808691 -0.03939829  0.05279168  1.        ]]. Action = [[-0.6165492  -0.45155752 -0.93854684  0.80203974]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Current timestep = 3302. State = [[-0.21764798 -0.03867491  0.05271559  1.        ]]. Action = [[ 0.85788274 -0.3488047  -0.84201765  0.65750396]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3303. State = [[-0.21762304 -0.03840565  0.05267799  1.        ]]. Action = [[-0.14378846  0.799582   -0.8880442   0.71841633]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3304. State = [[-0.22008106 -0.02709528  0.04689995  1.        ]]. Action = [[-0.16812718  0.6835458  -0.49137127  0.87600076]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 3305. State = [[-0.22284219 -0.03020344  0.04172205  1.        ]]. Action = [[-0.44624734 -0.89371675  0.87945163  0.67225313]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 3306. State = [[-0.23339073 -0.02786011  0.04847454  1.        ]]. Action = [[-0.5260955   0.9650314  -0.11459345  0.8833964 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 3307. State = [[-0.24237673 -0.01490801  0.04848     1.        ]]. Action = [[-0.18324971 -0.8156972  -0.9552527   0.7185528 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 3308. State = [[-0.24056502 -0.0129516   0.05726887  1.        ]]. Action = [[ 0.07847404 -0.03026807  0.77328944  0.26143384]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 3309. State = [[-0.2466815  -0.01457119  0.06624604  1.        ]]. Action = [[-0.43348742 -0.25040507 -0.5064367   0.6433158 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 3310. State = [[-0.2565226  -0.01544476  0.06418794  1.        ]]. Action = [[-0.724566    0.93677974  0.04689169  0.6679313 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 3311. State = [[-0.25769255 -0.0154419   0.06404051  1.        ]]. Action = [[-0.6180401  -0.11244029  0.01778758  0.619251  ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3312. State = [[-0.25319892 -0.00395826  0.06249635  1.        ]]. Action = [[ 0.7696085   0.862643   -0.32153702  0.8148422 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 3313. State = [[-0.24514596  0.00773933  0.06084206  1.        ]]. Action = [[ 0.33935618 -0.46992445 -0.9451082   0.8769672 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3314. State = [[-0.24346799  0.00930814  0.06093343  1.        ]]. Action = [[-0.5208732   0.75430703 -0.47240287  0.6640196 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 3315. State = [[-0.24338745  0.0095744   0.06091969  1.        ]]. Action = [[-0.50854707  0.44156384 -0.9992676   0.6967268 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3316. State = [[-0.24315965  0.00958271  0.06095292  1.        ]]. Action = [[-0.68136245 -0.7128411  -0.9078171   0.6874558 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Current timestep = 3317. State = [[-0.23401092  0.01000891  0.05850259  1.        ]]. Action = [[ 0.7221601 -0.0169996 -0.1944555  0.6955371]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 3318. State = [[-0.22229066  0.01033938  0.05109212  1.        ]]. Action = [[-0.9667728  -0.656512   -0.25608784  0.39141083]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3319. State = [[-0.22021866  0.01039165  0.050113    1.        ]]. Action = [[ 0.37399006 -0.17572153 -0.95211273  0.43940306]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 3320. State = [[-0.2196331   0.01048711  0.05020405  1.        ]]. Action = [[ 0.23967338 -0.32849693 -0.5172182   0.7079017 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 3321. State = [[-0.21929339  0.01705759  0.05585935  1.        ]]. Action = [[-0.25555563  0.41422772  0.76270735  0.3737197 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 3322. State = [[-0.220402    0.02673308  0.06234342  1.        ]]. Action = [[ 0.15191424  0.21933103 -0.36342365  0.6382544 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 3323. State = [[-0.22086516  0.03009439  0.05447584  1.        ]]. Action = [[ 0.1922338  -0.07007176 -0.679401    0.6910231 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 3324. State = [[-0.21693467  0.0236185   0.04164455  1.        ]]. Action = [[-0.04216576 -0.49968523 -0.12478334  0.76717687]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 3325. State = [[-0.21593738  0.01834094  0.0366119   1.        ]]. Action = [[-0.5305685  -0.02057648 -0.67358613  0.59375477]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3326. State = [[-0.2213073   0.01162421  0.0337598   1.        ]]. Action = [[-0.77971363 -0.41525722 -0.12743634  0.73823094]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 3327. State = [[-0.23878151  0.00478052  0.03187501  1.        ]]. Action = [[-0.8746601  -0.05161172  0.34693205  0.74003863]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 3328. State = [[-2.5704268e-01  8.5786823e-04  3.1137744e-02  1.0000000e+00]]. Action = [[-0.9065042   0.20062387 -0.97625947  0.69057965]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 3329. State = [[-0.2564536   0.00392226  0.04056114  1.        ]]. Action = [[0.08538759 0.30731273 0.80132675 0.54143   ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 3330. State = [[-0.2605805   0.00640055  0.05603904  1.        ]]. Action = [[-0.79866326 -0.90462846 -0.27359176  0.76945734]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3331. State = [[-0.26595867  0.01897083  0.05423227  1.        ]]. Action = [[-0.09996659  0.81217957 -0.5637804   0.6738715 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3332. State = [[-0.27188525  0.03269194  0.05171124  1.        ]]. Action = [[ 0.42323923 -0.5686767  -0.9232168   0.47299385]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3333. State = [[-0.2725358   0.03493791  0.05163783  1.        ]]. Action = [[-0.925293    0.23405027 -0.6130316   0.7590698 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Current timestep = 3334. State = [[-0.2726532   0.03536654  0.0516089   1.        ]]. Action = [[ 0.14833808  0.10567999 -0.6700475   0.77987814]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 3335. State = [[-0.27267295  0.03541083  0.05160891  1.        ]]. Action = [[-0.6182473   0.76547325 -0.14178574  0.64725757]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3336. State = [[-0.272676    0.03547573  0.05160257  1.        ]]. Action = [[ 0.5966449  -0.42832112 -0.9550701   0.6430011 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3337. State = [[-0.2726851   0.03567044  0.05158361  1.        ]]. Action = [[-0.7029196  -0.6505916   0.77547014  0.38117337]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3338. State = [[-0.2726851   0.03567044  0.05158361  1.        ]]. Action = [[-0.2104972  -0.04923952 -0.7199668   0.6419213 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3339. State = [[-0.26470244  0.02487028  0.04930801  1.        ]]. Action = [[ 0.76397014 -0.89345956 -0.3104037   0.729694  ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 3340. State = [[-0.23863736  0.02341845  0.04344661  1.        ]]. Action = [[ 0.9760432   0.89113593 -0.05224866  0.82864404]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 3341. State = [[-0.2212436   0.0332092   0.03889371  1.        ]]. Action = [[-0.7615297  0.845996  -0.90414    0.6551167]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3342. State = [[-0.21062851  0.04170379  0.04371573  1.        ]]. Action = [[0.6037942  0.40590763 0.62052226 0.8216009 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 3343. State = [[-0.19823682  0.05605121  0.04815795  1.        ]]. Action = [[-0.25010073  0.34113526 -0.27640522  0.87777734]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 3344. State = [[-0.20051163  0.06615138  0.04925648  1.        ]]. Action = [[0.02526915 0.17013693 0.29837978 0.5409279 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 3345. State = [[-0.20028663  0.07007033  0.0502304   1.        ]]. Action = [[-0.44857144 -0.07722819 -0.90001893  0.73089457]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3346. State = [[-0.18983889  0.06098393  0.05800914  1.        ]]. Action = [[ 0.6309552  -0.6741656   0.5621984   0.62151456]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 3347. State = [[-0.18233724  0.06103081  0.06835215  1.        ]]. Action = [[-0.39641547  0.58213973 -0.24384952  0.66510534]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 3348. State = [[-0.18640013  0.06562107  0.07504342  1.        ]]. Action = [[-0.3569278  -0.18909675  0.6218116   0.5583564 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 3349. State = [[-0.18948044  0.06225593  0.07798875  1.        ]]. Action = [[ 0.19979727 -0.20367873 -0.8059036   0.5292561 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 3350. State = [[-0.19897752  0.07259739  0.07477779  1.        ]]. Action = [[-0.8144243   0.87761414  0.5975683   0.7563677 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 3351. State = [[-0.21183269  0.09195974  0.07726473  1.        ]]. Action = [[-0.01550931  0.24015522 -0.3453797   0.51762843]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 3352. State = [[-0.22050968  0.09888449  0.08041631  1.        ]]. Action = [[-0.55804145 -0.04792368  0.52537084  0.5102701 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 3353. State = [[-0.2290709   0.1027009   0.08511087  1.        ]]. Action = [[-0.29268897  0.02579343  0.06854713  0.22006178]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 3354. State = [[-0.24811092  0.09552655  0.08062074  1.        ]]. Action = [[-0.5771241  -0.49581254 -0.78245646  0.7666967 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 3355. State = [[-0.2614272   0.0887908   0.07234533  1.        ]]. Action = [[-0.77240247  0.40966868  0.48092556  0.6607388 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Current timestep = 3356. State = [[-0.263007    0.0831968   0.06295643  1.        ]]. Action = [[ 0.3998654  -0.12981701 -0.7374262   0.72684336]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 3357. State = [[-0.26527175  0.07912296  0.04723712  1.        ]]. Action = [[-0.81063664  0.69633317 -0.84247965  0.41467428]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3358. State = [[-0.26490664  0.07871175  0.04707252  1.        ]]. Action = [[ 0.29995823  0.732882   -0.86102355  0.27379763]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3359. State = [[-0.26004383  0.09062583  0.05521252  1.        ]]. Action = [[0.32243943 0.9447098  0.93767405 0.6057937 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 3360. State = [[-0.25513935  0.11298177  0.07220089  1.        ]]. Action = [[-0.04008079  0.39343965  0.5388367   0.68188536]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 3361. State = [[-0.25426158  0.12227592  0.08430886  1.        ]]. Action = [[-0.98973376  0.7252184  -0.8607128   0.8337252 ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3362. State = [[-0.25091586  0.12553546  0.08211134  1.        ]]. Action = [[ 0.30628252  0.2416091  -0.50523293  0.41866553]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 3363. State = [[-0.24590859  0.12964687  0.07848132  1.        ]]. Action = [[-0.58102226  0.09245098  0.09695065  0.91413176]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 3364. State = [[-0.23631208  0.13514635  0.07149613  1.        ]]. Action = [[ 0.82149816  0.21427882 -0.7026976   0.5205679 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 3365. State = [[-0.21611477  0.13496123  0.06569712  1.        ]]. Action = [[ 0.1682725  -0.43892443  0.92414236  0.8440094 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 3366. State = [[-0.21495134  0.1339068   0.0706561   1.        ]]. Action = [[-0.6466724   0.2652496  -0.4874403   0.79123425]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 3367. State = [[-0.21594447  0.14006431  0.07667425  1.        ]]. Action = [[0.3579098  0.15929449 0.86402524 0.6495831 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 3368. State = [[-0.21844289  0.14463642  0.09529526  1.        ]]. Action = [[-0.6669722  -0.01901722  0.5010301   0.70728374]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 3369. State = [[-0.22322161  0.14972603  0.10654885  1.        ]]. Action = [[ 0.5688764   0.33747983 -0.19215214  0.5282253 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 3370. State = [[-0.22666688  0.1546971   0.11533984  1.        ]]. Action = [[-0.68084407 -0.03759068  0.7435341   0.6261003 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 3371. State = [[-0.23905243  0.16877732  0.12086451  1.        ]]. Action = [[-0.07351583  0.816594   -0.8912007   0.7642536 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 3372. State = [[-0.24785362  0.17828338  0.11220099  1.        ]]. Action = [[-0.3362322  -0.2924015   0.0629741   0.42566466]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 3373. State = [[-0.24942225  0.17792872  0.11163188  1.        ]]. Action = [[-0.9265432  -0.2887348   0.62799     0.54938745]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Current timestep = 3374. State = [[-0.25462362  0.18815415  0.11620633  1.        ]]. Action = [[-0.15109384  0.67367685  0.5602646   0.79958117]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 3375. State = [[-0.25805837  0.19005448  0.1210122   1.        ]]. Action = [[ 0.01294899 -0.7349783  -0.18745458  0.55623937]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 3376. State = [[-0.25656462  0.1817038   0.12176447  1.        ]]. Action = [[-0.45288813 -0.6172728  -0.72666085  0.624302  ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 3377. State = [[-0.2565056   0.18089955  0.12180004  1.        ]]. Action = [[-0.5863791  -0.70272404  0.18623924  0.8790829 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3378. State = [[-0.25843665  0.17504686  0.11613917  1.        ]]. Action = [[-0.1720255  -0.35966587 -0.5007086   0.49562216]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 3379. State = [[-0.25300696  0.1619426   0.11020488  1.        ]]. Action = [[ 0.73180795 -0.31610268 -0.1442675   0.8240273 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 3380. State = [[-0.2458536   0.1532075   0.10599311  1.        ]]. Action = [[-0.7077463   0.8563044   0.54772747  0.6263876 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 3381. State = [[-0.24170558  0.14858025  0.09944264  1.        ]]. Action = [[ 0.2327044  -0.07844561 -0.926628    0.6727252 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 3382. State = [[-0.24557437  0.15167952  0.06455478  1.        ]]. Action = [[-0.37258524  0.40590727 -0.83486414  0.6633676 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 3383. State = [[-0.24992667  0.15612046  0.0458838   1.        ]]. Action = [[-0.31067032 -0.22886002  0.5598134   0.7011328 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 3384. State = [[-0.24313751  0.16321132  0.04789978  1.        ]]. Action = [[ 0.7811198   0.63844705 -0.12553883  0.44607615]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 3385. State = [[-0.23241396  0.17700839  0.05283134  1.        ]]. Action = [[0.24601912 0.41887653 0.40976703 0.5870347 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 3386. State = [[-0.22548312  0.18700373  0.05743174  1.        ]]. Action = [[-0.61798775 -0.02170092 -0.8306005   0.7711499 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 3387. State = [[-0.22451869  0.18790072  0.05800534  1.        ]]. Action = [[-0.88777816 -0.39344168  0.5971453   0.86975646]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 3388. State = [[-0.22461589  0.17868398  0.0670151   1.        ]]. Action = [[-0.489825   -0.81664646  0.7905407   0.5227654 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 3389. State = [[-0.25747943 -0.07513142  0.10890486  1.        ]]. Action = [[ 0.41049254 -0.27271426 -0.43899584  0.682572  ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 3390. State = [[-0.2594169  -0.07466909  0.09357283  1.        ]]. Action = [[0.05716538 0.7679589  0.25208235 0.69084716]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3391. State = [[-0.25947312 -0.06520584  0.09358753  1.        ]]. Action = [[-0.3229525   0.59507895  0.82942367  0.6337881 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3392. State = [[-0.25356042 -0.04874687  0.0866037   1.        ]]. Action = [[ 0.62266946  0.9509361  -0.976173    0.6923801 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3393. State = [[-0.24038184 -0.02920559  0.06932674  1.        ]]. Action = [[-0.7783044   0.9028255  -0.65012866  0.55974627]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3394. State = [[-0.2394807  -0.02572766  0.06842954  1.        ]]. Action = [[-0.35581934 -0.07442665 -0.94223523  0.66028094]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 3395. State = [[-0.22765586 -0.01192216  0.06819934  1.        ]]. Action = [[ 0.84523356  0.7696502  -0.13801676  0.4910046 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 3396. State = [[-0.21303853 -0.00132044  0.0544397   1.        ]]. Action = [[ 0.12618864 -0.16396654 -0.59988064  0.8260871 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3397. State = [[-0.21368656  0.01105252  0.03781889  1.        ]]. Action = [[-0.61918175  0.8130965  -0.1838677   0.7518352 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 3398. State = [[-0.214653    0.02942527  0.03462441  1.        ]]. Action = [[0.5593543  0.146384   0.16167295 0.5045357 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 3399. State = [[-0.20857595  0.03354876  0.03513525  1.        ]]. Action = [[-0.13762069 -0.0930866  -0.6513967   0.80138624]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3400. State = [[-0.20719117  0.03395832  0.03516996  1.        ]]. Action = [[ 0.774493   -0.08664292 -0.71978176  0.77203524]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3401. State = [[-0.20310429  0.03069448  0.03579039  1.        ]]. Action = [[ 0.22830963 -0.31171155  0.11977482  0.79950356]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 3402. State = [[-0.20024154  0.0268618   0.03649447  1.        ]]. Action = [[-0.36130846  0.4867034  -0.83153     0.6988776 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Current timestep = 3403. State = [[-0.19425862  0.0110156   0.03854344  1.        ]]. Action = [[ 0.3132224  -0.9394819   0.21298158  0.6374619 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 3404. State = [[-0.18742901 -0.00290297  0.04105238  1.        ]]. Action = [[-0.5631643   0.21341574 -0.42500198  0.47990775]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3405. State = [[-0.1873903  -0.01436124  0.05024453  1.        ]]. Action = [[-0.53402305 -0.40437996  0.82377446  0.6266457 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 3406. State = [[-0.18602858 -0.0226239   0.06348026  1.        ]]. Action = [[ 0.0357492   0.20236075 -0.48956466  0.6419368 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Current timestep = 3407. State = [[-0.20147769 -0.01856497  0.06098768  1.        ]]. Action = [[-0.9282929   0.35490775 -0.5945515   0.63711774]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 3408. State = [[-0.22125837 -0.00604978  0.05574312  1.        ]]. Action = [[-0.18616474  0.50740075 -0.06654859  0.5506443 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 3409. State = [[-0.22305    -0.00208953  0.05705181  1.        ]]. Action = [[ 0.21716368 -0.43991542  0.41481423  0.73495686]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 3410. State = [[-0.21868476 -0.02110506  0.063846    1.        ]]. Action = [[ 0.10273147 -0.892205    0.28383172  0.5602281 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 3411. State = [[-0.22139242 -0.04238373  0.06333273  1.        ]]. Action = [[-0.41820216 -0.4613725  -0.5253927   0.5240226 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 3412. State = [[-0.23687786 -0.06645457  0.05611198  1.        ]]. Action = [[-0.47635633 -0.71199423 -0.3441484   0.5301957 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 3413. State = [[-0.24731757 -0.08418955  0.04870717  1.        ]]. Action = [[-0.6243247  -0.03788543 -0.6735615   0.66612315]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Current timestep = 3414. State = [[-0.2497218  -0.08647651  0.04815606  1.        ]]. Action = [[ 0.2609632  -0.22822994 -0.9641331   0.80438495]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 3415. State = [[-0.25056034 -0.09866646  0.05008633  1.        ]]. Action = [[-0.13090456 -0.7830914   0.37474513  0.6447011 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 3416. State = [[-0.24289557 -0.11594682  0.05539184  1.        ]]. Action = [[ 0.9425882  -0.01395583  0.28072882  0.32673597]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 3417. State = [[-0.23768485 -0.1180063   0.05990938  1.        ]]. Action = [[ 0.28823137  0.86658275 -0.9868928   0.5599071 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 3418. State = [[-0.23689558 -0.12961772  0.06021773  1.        ]]. Action = [[-0.1207872  -0.7624404  -0.14348942  0.6422646 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 3419. State = [[-0.23974653 -0.15167391  0.06825015  1.        ]]. Action = [[-0.533001  -0.2985499  0.8910457  0.7079792]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 3420. State = [[-0.24247617 -0.16748953  0.08458214  1.        ]]. Action = [[ 0.68254995 -0.5733228   0.07080173  0.7443819 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 3421. State = [[-0.23866835 -0.16632688  0.09084487  1.        ]]. Action = [[-0.25093687  0.98198533  0.16524792  0.19495332]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 3422. State = [[-0.24001442 -0.14678158  0.08891004  1.        ]]. Action = [[-0.19661427  0.43767703 -0.6802155   0.6210444 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 3423. State = [[-0.23942229 -0.12036113  0.07734155  1.        ]]. Action = [[ 0.41164374  0.95078063 -0.6558426   0.77649367]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 3424. State = [[-0.22691622 -0.09042803  0.06453565  1.        ]]. Action = [[0.8380203  0.6577852  0.01237392 0.6411402 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 3425. State = [[-0.21631287 -0.08722824  0.06080117  1.        ]]. Action = [[-0.5197425  -0.8346475   0.08907175  0.7366382 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 3426. State = [[-0.22945388 -0.08218797  0.06167461  1.        ]]. Action = [[-0.92997473  0.9466938   0.37912703  0.51098466]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 3427. State = [[-0.250555   -0.05513678  0.06359252  1.        ]]. Action = [[-0.4051807   0.9024453  -0.19992638  0.66955364]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 3428. State = [[-0.25699583 -0.04796148  0.06417882  1.        ]]. Action = [[ 0.14783895 -0.8906945   0.08752072  0.75411034]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 3429. State = [[-0.25870982 -0.05911393  0.06523459  1.        ]]. Action = [[-0.67169     0.65029144  0.70483375  0.5515317 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Current timestep = 3430. State = [[-0.2579936  -0.05564227  0.06528413  1.        ]]. Action = [[ 0.2705983   0.43927288 -0.14883792  0.70863366]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 3431. State = [[-0.2572285  -0.05194581  0.06518005  1.        ]]. Action = [[-0.34915715  0.4780208  -0.07908785  0.67205954]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3432. State = [[-0.25956443 -0.04692951  0.06072663  1.        ]]. Action = [[-0.18443918  0.23998833 -0.42289114  0.7210424 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 3433. State = [[-0.25468245 -0.02808927  0.0526819   1.        ]]. Action = [[ 0.69225025  0.8404989  -0.26420158  0.48006475]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3434. State = [[-0.24421945 -0.01109706  0.04466225  1.        ]]. Action = [[-0.65985507  0.5875524   0.81905425  0.5340371 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 3435. State = [[-0.24396828  0.00679447  0.04194696  1.        ]]. Action = [[-0.10891694  0.92926407 -0.18783045  0.5292146 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 3436. State = [[-0.2471116   0.03332441  0.04451773  1.        ]]. Action = [[-0.1442669   0.47892034  0.63126326  0.5867529 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 3437. State = [[-0.24918582  0.04730746  0.04680135  1.        ]]. Action = [[-0.33822167  0.73488796 -0.40005946  0.6615596 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 3438. State = [[-0.24912256  0.0483516   0.0469299   1.        ]]. Action = [[ 0.8838217 -0.6375815 -0.5469418  0.5941217]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 3439. State = [[-0.2421851  0.0516865  0.0524844  1.       ]]. Action = [[0.51596653 0.21837175 0.36743093 0.681875  ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 3440. State = [[-0.22990954  0.04469085  0.05978088  1.        ]]. Action = [[ 0.3424858  -0.77570325  0.06226873  0.45551598]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 3441. State = [[-0.22229409  0.03420095  0.06232964  1.        ]]. Action = [[-0.93878543 -0.38220763  0.72652173  0.4924748 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3442. State = [[-0.22069417  0.0315128   0.0630127   1.        ]]. Action = [[ 0.3161739   0.17146492 -0.9257217   0.68908715]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 3443. State = [[-0.22008672  0.03087718  0.06320967  1.        ]]. Action = [[-0.19802433  0.10609734 -0.75843245  0.6778619 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 3444. State = [[-0.21104853  0.04149427  0.07250801  1.        ]]. Action = [[0.57064307 0.7684721  0.643816   0.5916376 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 3445. State = [[-0.19886138  0.05483657  0.08694867  1.        ]]. Action = [[0.88621306 0.63959074 0.85706794 0.6323588 ]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Current timestep = 3446. State = [[-0.2024882   0.05298472  0.0832467   1.        ]]. Action = [[-0.67686254 -0.336231   -0.6213737   0.6351    ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 3447. State = [[-0.21955071  0.04305216  0.07456037  1.        ]]. Action = [[-0.8898138  -0.4521625  -0.05298126  0.4645325 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 3448. State = [[-0.24411507  0.03121542  0.07365375  1.        ]]. Action = [[-0.78889775 -0.2065745   0.42598724  0.67912126]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 3449. State = [[-0.26907453  0.03645159  0.07020174  1.        ]]. Action = [[-0.2737487   0.757311   -0.88317907  0.65509415]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 3450. State = [[-0.27818498  0.04753429  0.05910754  1.        ]]. Action = [[-0.868793   -0.31885964 -0.38066614  0.63674796]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 3451. State = [[-0.27691415  0.05733011  0.06562699  1.        ]]. Action = [[0.4981308  0.5726553  0.7905054  0.69020605]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 3452. State = [[-0.2671284   0.07699043  0.07128492  1.        ]]. Action = [[ 0.6220828   0.56748533 -0.4432919   0.5759039 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 3453. State = [[-0.25738552  0.09889778  0.06710397  1.        ]]. Action = [[ 0.19957149  0.67759573 -0.15763855  0.86206996]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 3454. State = [[-0.25201824  0.11323434  0.06568179  1.        ]]. Action = [[-0.43629038 -0.5353297   0.6267059   0.78087187]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3455. State = [[-0.25156397  0.11522224  0.06537097  1.        ]]. Action = [[-0.58296156 -0.95423967 -0.74387646  0.67074513]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3456. State = [[-0.2524334   0.11537487  0.05818542  1.        ]]. Action = [[ 0.04422402 -0.00343853 -0.61403406  0.80899334]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 3457. State = [[-0.24808021  0.10854702  0.04753865  1.        ]]. Action = [[-0.22911853 -0.6725229   0.4164076   0.51745   ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 3458. State = [[-0.25114372  0.11267308  0.05595611  1.        ]]. Action = [[-0.38849378  0.76976275  0.7261754   0.58380055]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 3459. State = [[-0.25687304  0.12468216  0.06868257  1.        ]]. Action = [[-0.85626245 -0.5732366   0.19373679  0.7060001 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 3460. State = [[-0.25833178  0.1262306   0.06947051  1.        ]]. Action = [[-0.85573435  0.63657236 -0.01513892  0.5735855 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 3461. State = [[-0.25846133  0.12629974  0.06945313  1.        ]]. Action = [[-0.14104909 -0.26665366 -0.86251384  0.34500098]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3462. State = [[-0.25846133  0.12629974  0.06945313  1.        ]]. Action = [[-0.32119352 -0.8228263   0.5109577   0.6049042 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 3463. State = [[-0.25861892  0.12650613  0.06946126  1.        ]]. Action = [[-0.815491  -0.0290978 -0.5889906  0.5146036]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 3464. State = [[-0.2502761   0.11162363  0.07496803  1.        ]]. Action = [[ 0.7200061  -0.9466451   0.3381343   0.67302895]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 3465. State = [[-0.23715918  0.10181503  0.08319819  1.        ]]. Action = [[ 0.44871187  0.41901863 -0.23156387  0.6257453 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 3466. State = [[-0.22814494  0.10538369  0.08526815  1.        ]]. Action = [[-0.8255613  -0.20588511 -0.66012     0.6364049 ]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Current timestep = 3467. State = [[-0.22702849  0.10554833  0.08560582  1.        ]]. Action = [[-0.8604302   0.26550305  0.7215345   0.64713836]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3468. State = [[-0.2234932   0.10540794  0.08926065  1.        ]]. Action = [[ 0.20382488 -0.05904984  0.4743955   0.7168857 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 3469. State = [[-0.21304505  0.11566637  0.10377851  1.        ]]. Action = [[0.24762619 0.63115716 0.63566875 0.6864333 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 3470. State = [[-0.21001248  0.1352864   0.1160703   1.        ]]. Action = [[-0.2263906   0.69232416 -0.1989246   0.7651589 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 3471. State = [[-0.2202235   0.15537295  0.11982638  1.        ]]. Action = [[-0.36864197  0.35667145  0.34684575  0.68595684]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 3472. State = [[-0.2229204   0.15891202  0.12668109  1.        ]]. Action = [[ 0.07196152 -0.4598902   0.27260947  0.7763772 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 3473. State = [[-0.2256547   0.1438928   0.14623018  1.        ]]. Action = [[-0.7589785  -0.65156686  0.7917801   0.4708209 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 3474. State = [[-0.24540728  0.13671617  0.15672459  1.        ]]. Action = [[-0.63723713  0.27985096 -0.78802437  0.6863847 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 3475. State = [[-0.25468042  0.1302641   0.15992121  1.        ]]. Action = [[ 0.06931186 -0.5850819   0.8845061   0.7358849 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 3476. State = [[-0.25418824  0.12168235  0.16981618  1.        ]]. Action = [[-0.7620811  -0.71990925 -0.29422665  0.708056  ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3477. State = [[-0.25361153  0.12548895  0.17239945  1.        ]]. Action = [[0.35522842 0.4444307  0.10532463 0.6811452 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 3478. State = [[-0.24843422  0.13675219  0.17851894  1.        ]]. Action = [[0.50936055 0.44735122 0.3167299  0.53291154]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 3479. State = [[-0.2407405   0.13942613  0.18632542  1.        ]]. Action = [[-0.1563704  -0.47676605  0.06325638  0.61859906]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 3480. State = [[-0.23497218  0.12411997  0.18678989  1.        ]]. Action = [[ 0.33358467 -0.6679972  -0.50136524  0.7442529 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 3481. State = [[-0.23656093  0.10732315  0.18233113  1.        ]]. Action = [[-0.62326664 -0.41939533 -0.05104178  0.45836484]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 3482. State = [[-0.2441452   0.08323541  0.17995332  1.        ]]. Action = [[-0.0842132  -0.90958536 -0.191069    0.770005  ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 3483. State = [[-0.23541042  0.05481073  0.17277494  1.        ]]. Action = [[ 0.69653046 -0.6056074  -0.91708815  0.76052976]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 3484. State = [[-0.2318673   0.02317501  0.14797571  1.        ]]. Action = [[-0.65607    -0.8062465  -0.10226285  0.82725334]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 3485. State = [[-0.25180924  0.01287516  0.13107198  1.        ]]. Action = [[-0.7205216  0.5553752 -0.9707659  0.7454457]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 3486. State = [[-0.2677982   0.02104548  0.10658254  1.        ]]. Action = [[-0.42368704 -0.510318   -0.8506769   0.6926465 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3487. State = [[-0.26824296  0.02199897  0.10432321  1.        ]]. Action = [[-0.9111241  -0.8629432   0.30854487  0.7497153 ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 3488. State = [[-0.26358497  0.03413648  0.10413318  1.        ]]. Action = [[0.65043354 0.8290844  0.16075265 0.6912272 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 3489. State = [[-0.24930212  0.04800501  0.11235814  1.        ]]. Action = [[ 0.75869536 -0.0786581   0.94444203  0.6656772 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 3490. State = [[-0.23734081  0.0510854   0.12540613  1.        ]]. Action = [[-0.91366976  0.43985057 -0.6100761   0.66360855]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 3491. State = [[-0.26074386  0.06953184  0.1196849   1.        ]]. Action = [[-0.34023672  0.4163165  -0.26638722  0.67760944]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 3492. State = [[-0.25613782  0.09134516  0.09947444  1.        ]]. Action = [[ 0.56795204  0.9336183  -0.8655626   0.5737989 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 3493. State = [[-0.24212201  0.11093937  0.07557783  1.        ]]. Action = [[-0.36657667  0.3650775  -0.14190304  0.44899178]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 3494. State = [[-0.2343737   0.10869639  0.06381203  1.        ]]. Action = [[ 0.4662633  -0.40204298 -0.6726456   0.8044753 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 3495. State = [[-0.22350243  0.10494816  0.04888714  1.        ]]. Action = [[-0.8617548  -0.23230004  0.7761793   0.716583  ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 3496. State = [[-0.22611122  0.08821785  0.05274276  1.        ]]. Action = [[-0.89359444 -0.95442045  0.8323436   0.7145796 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 3497. State = [[-0.23698413  0.07101365  0.06330018  1.        ]]. Action = [[-0.9453916  -0.21106005 -0.2553076   0.62167406]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 3498. State = [[-0.23889837  0.05796678  0.07389336  1.        ]]. Action = [[-0.29733002 -0.70207185  0.9488127   0.0382992 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 3499. State = [[-0.25351918  0.03142089  0.09708236  1.        ]]. Action = [[-0.45812201 -0.88565975  0.13375974  0.4815483 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 3500. State = [[-0.26266795  0.00582286  0.11584861  1.        ]]. Action = [[ 0.03236294 -0.4612484   0.9500458   0.5896276 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 3501. State = [[-0.26444283 -0.00536577  0.13596772  1.        ]]. Action = [[-0.19242465  0.08465958  0.30960596  0.6397321 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Current timestep = 3502. State = [[-0.2642091  -0.00690116  0.1390941   1.        ]]. Action = [[-0.6431049  -0.7245672   0.9039242   0.56831264]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 3503. State = [[-2.6760527e-01  4.1700987e-04  1.3164932e-01  1.0000000e+00]]. Action = [[ 0.01429212  0.5821959  -0.88756835  0.54853225]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 3504. State = [[-0.26414663  0.00328818  0.12838855  1.        ]]. Action = [[ 0.44941926 -0.37142497  0.6479838   0.66861343]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 3505. State = [[-0.25456914 -0.0155877   0.13703653  1.        ]]. Action = [[ 0.27702188 -0.9706865   0.40689492  0.3450613 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 3506. State = [[-0.24858274 -0.03464828  0.14429052  1.        ]]. Action = [[-0.9280947   0.5445249  -0.74087816  0.6638336 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Current timestep = 3507. State = [[-0.24801968 -0.03689194  0.14434022  1.        ]]. Action = [[-0.58521193 -0.5484569  -0.58800334  0.7889452 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 3508. State = [[-0.23599446 -0.03634901  0.1483573   1.        ]]. Action = [[0.97380567 0.08492827 0.15295017 0.40964925]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 3509. State = [[-0.2192321  -0.03475685  0.15559882  1.        ]]. Action = [[-0.84089214  0.7542982  -0.9157531   0.7706984 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 3510. State = [[-0.20831339 -0.04198273  0.15108939  1.        ]]. Action = [[ 0.65854096 -0.45489264 -0.78702664  0.6287756 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 3511. State = [[-0.20431408 -0.06748877  0.12880395  1.        ]]. Action = [[-0.9506718  -0.8467481  -0.80807185  0.817209  ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 3512. State = [[-0.22013628 -0.08048229  0.10329171  1.        ]]. Action = [[-0.510114    0.36547363 -0.5103189   0.68251157]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 3513. State = [[-0.22683884 -0.0876025   0.08265959  1.        ]]. Action = [[ 0.33186555 -0.6130165  -0.8221528   0.557915  ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 3514. State = [[-0.22650182 -0.10581589  0.05697269  1.        ]]. Action = [[ 0.15629947 -0.51104873 -0.15748894  0.7482598 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 3515. State = [[-0.21247417 -0.10407864  0.05175123  1.        ]]. Action = [[0.86015344 0.79551387 0.11703026 0.678444  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 3516. State = [[-0.2027203  -0.08445236  0.05852351  1.        ]]. Action = [[-0.59034437  0.6051222   0.94114566  0.5046787 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 3517. State = [[-0.20431367 -0.07244602  0.07206485  1.        ]]. Action = [[-0.59870046 -0.04221779 -0.8884292   0.67629814]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 3518. State = [[-0.21850066 -0.079271    0.06694005  1.        ]]. Action = [[-0.9800771  -0.62911904 -0.6491685   0.56425905]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 3519. State = [[-0.24076602 -0.09388562  0.05736989  1.        ]]. Action = [[-0.3748666  -0.4013884  -0.33616018  0.6408107 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 3520. State = [[-0.2480603  -0.09474988  0.04751201  1.        ]]. Action = [[ 0.48449492  0.54464483 -0.18699694  0.2183733 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 3521. State = [[-0.24678025 -0.08966753  0.04519127  1.        ]]. Action = [[-0.5244018  -0.97133917 -0.8496989   0.78393245]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 3522. State = [[-0.24619988 -0.0866276   0.04475351  1.        ]]. Action = [[ 0.29224503  0.19129121 -0.46793115  0.70544994]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 3523. State = [[-0.23832037 -0.08611774  0.04555156  1.        ]]. Action = [[ 0.642359   -0.05134594  0.23811305  0.6218374 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 3524. State = [[-0.23423737 -0.08765066  0.05327758  1.        ]]. Action = [[-0.6120363  -0.01816577  0.77845955  0.82131565]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 3525. State = [[-0.23606719 -0.08835896  0.06236185  1.        ]]. Action = [[ 0.07305181 -0.11070979 -0.2028228   0.75234413]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 3526. State = [[-0.2400753 -0.081519   0.0617574  1.       ]]. Action = [[-0.33142412  0.6498263  -0.08249354  0.626832  ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 3527. State = [[-0.24412867 -0.07382978  0.06126593  1.        ]]. Action = [[-0.49033397 -0.35750115 -0.837602    0.48072255]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 3528. State = [[-0.24320875 -0.08206969  0.06391731  1.        ]]. Action = [[ 0.24482226 -0.7336237   0.36997294  0.61840034]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 3529. State = [[-0.24302687 -0.09175225  0.06763276  1.        ]]. Action = [[-0.81300455  0.6228734  -0.8269334   0.36380374]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 3530. State = [[-0.2421893  -0.1077439   0.07942355  1.        ]]. Action = [[-0.07072586 -0.88048685  0.7772467   0.68796563]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 3531. State = [[-0.24574359 -0.11001998  0.09920339  1.        ]]. Action = [[-0.34638596  0.99108005  0.21197927  0.5190351 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 3532. State = [[-0.24775429 -0.0988617   0.11080054  1.        ]]. Action = [[ 0.70742035 -0.01648712  0.2405597   0.39929974]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 3533. State = [[-0.24291094 -0.09624761  0.11564454  1.        ]]. Action = [[-0.9858897  -0.66189283 -0.04570186  0.7231755 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 3534. State = [[-0.24186218 -0.09584919  0.11582547  1.        ]]. Action = [[-0.97831357 -0.74782014 -0.27159703  0.6744654 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 3535. State = [[-0.23689766 -0.08049397  0.11266427  1.        ]]. Action = [[ 0.39174724  0.89139724 -0.5545609   0.5415323 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 3536. State = [[-0.23238929 -0.05239119  0.10842113  1.        ]]. Action = [[-0.35075903  0.72241783 -0.07235593  0.7617272 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 3537. State = [[-0.23755775 -0.04213313  0.10049284  1.        ]]. Action = [[-0.0690009 -0.387056  -0.5640233  0.6091876]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 3538. State = [[-0.24076806 -0.03613199  0.0926764   1.        ]]. Action = [[-0.3094206   0.5166614   0.38286364  0.69695044]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 3539. State = [[-0.24519143 -0.03531336  0.09643069  1.        ]]. Action = [[-0.3392113  -0.4510448   0.35867584  0.6153946 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 3540. State = [[-0.25148642 -0.04150735  0.10509767  1.        ]]. Action = [[-0.1115697   0.01780951  0.238971    0.7013762 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 3541. State = [[-0.2542564  -0.04204182  0.10998411  1.        ]]. Action = [[-0.91397595 -0.7290432  -0.6225827   0.45305395]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Current timestep = 3542. State = [[-0.2542564  -0.04204182  0.10998411  1.        ]]. Action = [[-0.6385733  -0.90665305 -0.9423561   0.5789325 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 3543. State = [[-0.2542564  -0.04204182  0.10998411  1.        ]]. Action = [[-0.9480107   0.72786784  0.8378041   0.48522508]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Current timestep = 3544. State = [[-0.24910451 -0.05262299  0.10323048  1.        ]]. Action = [[ 0.73283577 -0.7263152  -0.9460824   0.18403769]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 3545. State = [[-0.24169327 -0.07847299  0.09394164  1.        ]]. Action = [[-0.0121296  -0.8605899   0.27290046  0.55788326]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 3546. State = [[-0.23204641 -0.10823075  0.08682127  1.        ]]. Action = [[ 0.6740352  -0.8382466  -0.87730175  0.73166907]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 3547. State = [[-0.2248151  -0.12315506  0.07722515  1.        ]]. Action = [[-0.73544663  0.46141863  0.9253988   0.69558764]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 3548. State = [[-0.23333569 -0.12248751  0.07989479  1.        ]]. Action = [[-0.07015508 -0.18399179 -0.66249835  0.76412845]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 3549. State = [[-0.23529401 -0.12270911  0.0769325   1.        ]]. Action = [[-0.81925863  0.63401425 -0.2561924   0.68053937]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Current timestep = 3550. State = [[-0.235396   -0.12268893  0.07691658  1.        ]]. Action = [[-0.995678   -0.7063614   0.37576258  0.7721391 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 3551. State = [[-0.235396   -0.12268893  0.07691658  1.        ]]. Action = [[-0.9359506   0.82199526  0.8248105   0.45722008]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Current timestep = 3552. State = [[-0.23378253 -0.10823819  0.07554231  1.        ]]. Action = [[ 0.37233078  0.9900358  -0.11056697  0.5778568 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 3553. State = [[-0.23031561 -0.09251963  0.07391654  1.        ]]. Action = [[-0.9165535  -0.4737981  -0.21897423  0.6967664 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Current timestep = 3554. State = [[-0.22832729 -0.08917315  0.06511226  1.        ]]. Action = [[ 0.30165267 -0.03044772 -0.90970397  0.7891164 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 3555. State = [[-0.22722584 -0.1019101   0.04265554  1.        ]]. Action = [[-0.36447644 -0.85622114  0.11870778  0.41206527]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 3556. State = [[-0.22726835 -0.11396844  0.04161349  1.        ]]. Action = [[-0.9789399  0.8682518  0.3914566  0.4808519]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Current timestep = 3557. State = [[-0.22726722 -0.11512783  0.04159616  1.        ]]. Action = [[ 0.09000325  0.6451235  -0.9309872   0.69799685]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Current timestep = 3558. State = [[-0.23531085 -0.10662815  0.04573773  1.        ]]. Action = [[-0.67365867  0.77615213  0.6910691   0.51794374]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 3559. State = [[-0.23769414 -0.08220073  0.05274331  1.        ]]. Action = [[ 0.78460026  0.94460714 -0.12200302  0.69820714]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 3560. State = [[-0.23417327 -0.06268645  0.05342241  1.        ]]. Action = [[ 0.26015544  0.58104825 -0.5284741   0.6569563 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Current timestep = 3561. State = [[-0.23890714 -0.04831861  0.05902898  1.        ]]. Action = [[-0.69468784  0.70444655  0.6162907   0.63081825]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 3562. State = [[-0.24011898 -0.04244444  0.07279772  1.        ]]. Action = [[ 0.25555444 -0.60759366  0.28323722  0.5529158 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 3563. State = [[-0.23833862 -0.04911287  0.07928838  1.        ]]. Action = [[-0.87601626 -0.27559435  0.9226515   0.63025975]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 3564. State = [[-0.24050711 -0.06584546  0.08439638  1.        ]]. Action = [[-0.37990475 -0.9464684   0.41971242  0.65638304]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 3565. State = [[-0.24577516 -0.06738492  0.09144458  1.        ]]. Action = [[ 0.41113114  0.9535086  -0.48219037  0.6331203 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 3566. State = [[-0.24444829 -0.05751717  0.09015006  1.        ]]. Action = [[-0.5985756  -0.8877887  -0.07235759  0.6581764 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Current timestep = 3567. State = [[-0.24720456 -0.05330395  0.09175991  1.        ]]. Action = [[-0.3995657   0.11349022  0.43880546  0.7955557 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 3568. State = [[-0.25502592 -0.04553211  0.10600888  1.        ]]. Action = [[-0.40532845  0.27079308  0.909904    0.6512561 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 3569. State = [[-0.2602883  -0.03913419  0.12754683  1.        ]]. Action = [[-0.64971167 -0.9444739  -0.72071654  0.6351733 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 3570. State = [[-0.26067516 -0.03847361  0.12824182  1.        ]]. Action = [[-0.41820836 -0.71759814  0.1037215   0.7134125 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 3571. State = [[-0.26076427 -0.03846187  0.12827508  1.        ]]. Action = [[-0.86587155  0.46744096  0.09308147  0.73829484]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 3572. State = [[-0.26076427 -0.03846187  0.12827508  1.        ]]. Action = [[-0.88884175  0.23841357  0.6807983   0.633844  ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 3573. State = [[-0.26074338 -0.03846164  0.12831204  1.        ]]. Action = [[-0.7936795  -0.74522835  0.89724326  0.77200437]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 3574. State = [[-0.26015857 -0.03066043  0.14040926  1.        ]]. Action = [[0.01683831 0.4456247  0.951807   0.6168504 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 3575. State = [[-0.26125363 -0.02148571  0.1644649   1.        ]]. Action = [[-0.87876004 -0.72449404  0.35988498  0.6373526 ]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Current timestep = 3576. State = [[-0.26102772 -0.01316122  0.18126774  1.        ]]. Action = [[0.12027335 0.3935784  0.96573925 0.69972706]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 3577. State = [[-0.26188076 -0.0058307   0.20695513  1.        ]]. Action = [[ 0.00496197 -0.02545464  0.34318066  0.431005  ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 3578. State = [[-0.26234448 -0.0053829   0.21675189  1.        ]]. Action = [[-0.8496666   0.9459454   0.36149883  0.5761423 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 3579. State = [[-0.26245576 -0.00531529  0.21684043  1.        ]]. Action = [[-0.29294002 -0.39449072 -0.6067362   0.2851379 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 3580. State = [[-0.2620381  -0.01277796  0.22121371  1.        ]]. Action = [[-0.02897918 -0.50513035  0.2994839   0.40064454]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 3581. State = [[-0.26404077 -0.02165449  0.22936103  1.        ]]. Action = [[-0.5423563  -0.97988737  0.68412924  0.5476705 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 3582. State = [[-0.26436213 -0.0230546   0.22969262  1.        ]]. Action = [[-0.89349556 -0.48603666  0.26898408  0.45613325]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 3583. State = [[-0.26438347 -0.0232794   0.2297986   1.        ]]. Action = [[-0.331046  -0.5803668  0.8394644  0.6098356]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 3584. State = [[-0.25802055 -0.02904767  0.23155008  1.        ]]. Action = [[ 0.5151198  -0.42232233 -0.21163446  0.5179471 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 3585. State = [[-0.24812749 -0.03615334  0.23832937  1.        ]]. Action = [[ 0.2988975  -0.1483882   0.64695525  0.7111056 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 3586. State = [[-0.24010178 -0.04327838  0.24287371  1.        ]]. Action = [[-0.06262934 -0.11437529 -0.5833849   0.67355216]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 3587. State = [[-0.23951058 -0.06062143  0.23197365  1.        ]]. Action = [[ 0.08091474 -0.8932403  -0.8447405   0.50292635]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 3588. State = [[-0.23697275 -0.07726959  0.21278135  1.        ]]. Action = [[-0.81509066  0.8357122  -0.79834217  0.5706229 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 3589. State = [[-0.2304526  -0.09332229  0.20903051  1.        ]]. Action = [[ 0.4286542  -0.8538729  -0.13595653  0.70518875]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 3590. State = [[-0.22208308 -0.10911611  0.20655829  1.        ]]. Action = [[0.16383696 0.11857104 0.13349319 0.6446738 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 3591. State = [[-0.2198088  -0.10064831  0.19901635  1.        ]]. Action = [[-0.09916234  0.6804755  -0.78018695  0.61303663]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 3592. State = [[-0.22582336 -0.08098739  0.18066707  1.        ]]. Action = [[-0.6998315   0.6560967  -0.08370793  0.6766783 ]]. Reward = [0.]
Curr episode timestep = 100
