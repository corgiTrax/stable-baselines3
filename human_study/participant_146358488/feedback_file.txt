Current timestep = 0. State = [[-0.25778034  0.00781093]]. Action = [[ 0.02368893 -0.03158506  0.04001088  0.69162905]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0429, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.2564483   0.00707999]]. Action = [[0.07373633 0.08091681 0.06756005 0.5634241 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0389, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25488514  0.00761247]]. Action = [[ 0.09737315 -0.09297818 -0.07287975  0.7404721 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0304, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25239858  0.00675375]]. Action = [[-0.04975582 -0.08748221  0.00551226 -0.94289124]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25175938  0.00420017]]. Action = [[-0.03770411  0.00626408 -0.02686328  0.9408541 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25187263  0.00248978]]. Action = [[ 0.02642245 -0.01754234 -0.08054997 -0.78538036]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25201234  0.00098327]]. Action = [[-0.08958351 -0.00627692  0.08311988  0.7870767 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.25223276 -0.00076014]]. Action = [[-0.04437388 -0.05475631  0.03463527  0.82439876]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.25261095 -0.00304695]]. Action = [[-0.01613992  0.03254435  0.08612885 -0.36171764]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.25302714 -0.00368745]]. Action = [[-0.08462992  0.04811313  0.07206645 -0.98434865]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25373265 -0.00295214]]. Action = [[ 0.03853441  0.0197413   0.02014067 -0.12030959]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.25399125 -0.0024672 ]]. Action = [[ 0.08501334 -0.09208038  0.08363893  0.74848723]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.2541354  -0.00375201]]. Action = [[-0.09679459  0.05888941 -0.00308506  0.8257098 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.25432524 -0.00352511]]. Action = [[-0.07299139  0.06944428  0.05570538 -0.4148904 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.2556898  -0.00125744]]. Action = [[ 0.07165524  0.08838286 -0.05515878  0.86750805]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.2568322   0.00177889]]. Action = [[ 0.05358364 -0.09650075  0.07312261 -0.3943267 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25695604  0.00148568]]. Action = [[-0.04066303 -0.08103337  0.07009096 -0.47142172]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25696093 -0.00073255]]. Action = [[ 0.07059401 -0.09491835 -0.00954337  0.17947435]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25707233 -0.00414847]]. Action = [[ 0.09145062  0.07857228  0.07934905 -0.7069146 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.25636795 -0.00416081]]. Action = [[ 0.07052606  0.01781956  0.09339178 -0.07393825]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25455576 -0.0041458 ]]. Action = [[ 0.00106108 -0.00629153  0.0482812  -0.11614805]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25337437 -0.00421927]]. Action = [[ 0.0409273  -0.06368825 -0.0707197  -0.17120326]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25186262 -0.00542926]]. Action = [[ 0.02552319 -0.05726453  0.05539069  0.8009926 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.2499068  -0.00744247]]. Action = [[0.0880824  0.00760049 0.07233959 0.6051024 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.24670298 -0.00846457]]. Action = [[ 0.0588819   0.0220478   0.04531992 -0.98616225]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.24280216 -0.00873703]]. Action = [[-0.00357002 -0.07182739 -0.04953902 -0.17610455]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.23931815 -0.01069022]]. Action = [[-3.6833435e-04 -7.5968295e-02  7.3689446e-03 -3.8235319e-01]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.2371524  -0.01405705]]. Action = [[-0.08044098 -0.09667422  0.03135235  0.7763022 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.23692133 -0.0190534 ]]. Action = [[ 0.01422672 -0.08352346  0.01487527  0.660892  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.23699042 -0.02437749]]. Action = [[-0.01756872 -0.05940478  0.05280597 -0.4849366 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.2374103 -0.029583 ]]. Action = [[ 0.04316296 -0.0276583  -0.07992089  0.39248252]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.23763774 -0.03312201]]. Action = [[-0.03816063 -0.03883727 -0.02424463  0.65806794]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.2382815  -0.03662593]]. Action = [[-0.09111346  0.08217967  0.00128467  0.7340598 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0007, grad_fn=<MseLossBackward0>)
