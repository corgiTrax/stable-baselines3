Current timestep = 0. State = [[-0.25747296  0.00759809]]. Action = [[ 0.02368855 -0.03158561  0.04001086  0.6916251 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0428, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25625098  0.00682318]]. Action = [[-0.09270248  0.0646274  -0.06983037 -0.9083965 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25642595  0.007482  ]]. Action = [[ 0.0431088  -0.06934617  0.09526784 -0.82669073]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25650987  0.00652685]]. Action = [[-0.06190487 -0.08646178 -0.03630088 -0.9427213 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25673994  0.00363559]]. Action = [[-0.08931493 -0.03190197 -0.00652941 -0.9672617 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.2575116   0.00106602]]. Action = [[-0.06249293  0.08957941 -0.0716176  -0.81830573]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25957215  0.00147791]]. Action = [[-0.09125363 -0.09415698 -0.00713614 -0.9420017 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.26257867 -0.00102917]]. Action = [[ 0.09933748 -0.0636981   0.0408236  -0.44709992]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.26327196 -0.00426803]]. Action = [[-0.04832998 -0.00338924  0.09150892  0.6864822 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.26394373 -0.00626572]]. Action = [[0.04460192 0.08833695 0.07067736 0.3944025 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.26441023 -0.00526156]]. Action = [[-0.04550258 -0.04602807 -0.05031934 -0.86290425]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.26468605 -0.00580788]]. Action = [[ 0.03651799  0.00397178 -0.07483701 -0.2650506 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.2646976  -0.00607026]]. Action = [[ 0.05022342 -0.02081194  0.09727707 -0.15003765]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.26473725 -0.00639531]]. Action = [[-0.0631019  -0.030653   -0.04483298 -0.10362488]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.26478276 -0.00750859]]. Action = [[-0.07688412 -0.03545725  0.07835067 -0.14898008]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.2658095  -0.00938718]]. Action = [[-0.09148765 -0.0593936  -0.09120604 -0.09064072]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.2674889  -0.01214686]]. Action = [[ 0.01078991  0.0192586  -0.06361694 -0.68861055]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.26872152 -0.01331536]]. Action = [[ 0.06849951  0.06100721  0.09753624 -0.94922763]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.26919317 -0.01259701]]. Action = [[ 0.0366763  -0.03063039  0.07328995  0.56137395]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.26917472 -0.01281824]]. Action = [[ 0.07679396 -0.01775225 -0.08513071  0.78840137]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.26913282 -0.01308104]]. Action = [[ 0.077348   -0.00850181 -0.08044545 -0.00444615]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.26850173 -0.01338924]]. Action = [[0.06750036 0.02742767 0.0957342  0.6963942 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.26693818 -0.0136137 ]]. Action = [[-0.04199674 -0.08214653  0.03716951  0.843143  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.2667912  -0.01483506]]. Action = [[ 0.00836929  0.01986049 -0.01196043 -0.9015365 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.26680252 -0.01519566]]. Action = [[-0.05922174 -0.05566074 -0.00682327  0.3903985 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.26697072 -0.01703887]]. Action = [[-0.04589999 -0.07173018  0.06384955  0.6699996 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.26732907 -0.02002214]]. Action = [[-0.07334874 -0.06978232 -0.01375969 -0.27928793]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.26816955 -0.02431175]]. Action = [[-0.04524998 -0.05386174 -0.06901638  0.6907828 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.269241   -0.02854564]]. Action = [[-0.07691265  0.03323691  0.03927196  0.96542645]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.2708562  -0.03046626]]. Action = [[-0.007499   -0.02707961 -0.0796818  -0.17836702]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.27206278 -0.03248478]]. Action = [[-0.0532234   0.01056969  0.08086684  0.4560591 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.2735912  -0.03394052]]. Action = [[ 0.00180759 -0.06609906 -0.0424611  -0.9663733 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.27419266 -0.03615573]]. Action = [[0.05827539 0.08108868 0.08132593 0.6699271 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.27444074 -0.03605301]]. Action = [[-0.02509135 -0.06834702 -0.03027477 -0.5049312 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.2746554  -0.03721376]]. Action = [[-0.0208324  -0.07501888  0.00500488 -0.16421026]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.27509478 -0.03953325]]. Action = [[0.02800336 0.08269788 0.0735574  0.64079595]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.27534187 -0.03978831]]. Action = [[ 0.07377262 -0.06637827  0.01001042 -0.78552115]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.27540535 -0.040693  ]]. Action = [[ 0.09230264  0.06775797 -0.04096412 -0.42593396]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.27516353 -0.04074952]]. Action = [[-0.01681421 -0.08631621 -0.04686276 -0.6871526 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.275129   -0.04178701]]. Action = [[ 0.05551655 -0.00358893 -0.03937523  0.30911446]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.2744917  -0.04261863]]. Action = [[ 0.03670699 -0.06791958 -0.04920672  0.48854995]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0027, grad_fn=<MseLossBackward0>)
