Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.33283383 -0.08530019]]. Action = [[ 0.05919518 -0.05022885  0.          0.01753342]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.32827216 -0.08361269]]. Action = [[ 0.07941671  0.04839007  0.         -0.508215  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32541144 -0.08398328]]. Action = [[ 0.02240024 -0.05187739  0.          0.0605756 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.32796615 -0.0831292 ]]. Action = [[-0.0650194   0.04303385  0.         -0.7176986 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.33477747 -0.08681659]]. Action = [[-0.09400087 -0.09524944  0.         -0.55304015]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.34229738 -0.08951566]]. Action = [[-0.08571257  0.01567943  0.          0.7501602 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.34299225 -0.09307053]]. Action = [[ 0.06539012 -0.06981484  0.          0.3592732 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of 1
Current timestep = 9. State = [[-0.33896312 -0.09811439]]. Action = [[ 0.05115712 -0.04872281  0.          0.9660847 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.34125546 -0.09593623]]. Action = [[-0.09367881  0.09747549  0.          0.9562684 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.34793574 -0.08932651]]. Action = [[-0.06636099  0.08790558  0.         -0.7746588 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.35245633 -0.09056675]]. Action = [[-0.0205902  -0.07998344  0.         -0.9537932 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.35142174 -0.09695402]]. Action = [[ 0.06858603 -0.08357798  0.          0.7992346 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of 1
Current timestep = 14. State = [[-0.34904945 -0.10319333]]. Action = [[ 0.03612975 -0.07323891  0.          0.9914725 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.34914002 -0.10755385]]. Action = [[-0.00910385 -0.03296125  0.         -0.21322316]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.35156968 -0.11432378]]. Action = [[-0.03352619 -0.09511235  0.         -0.4662441 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.3503742  -0.11494984]]. Action = [[ 0.06327911  0.07894935  0.         -0.04514998]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.3524275  -0.11038767]]. Action = [[-0.07426466  0.06373271  0.         -0.7987995 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.35570377 -0.1081973 ]]. Action = [[-0.01449598  0.01586638  0.         -0.61227727]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.1323e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.35271204 -0.10627191]]. Action = [[0.08608354 0.02448354 0.         0.3103485 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-0.35501102 -0.10490083]]. Action = [[-0.09338944  0.00911902  0.          0.03489482]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.35573485 -0.1087836 ]]. Action = [[ 0.06810559 -0.09270412  0.         -0.5360718 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.35006243 -0.11582094]]. Action = [[ 0.08809731 -0.09009238  0.          0.2065028 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.343563   -0.12277665]]. Action = [[ 0.07308612 -0.08330748  0.         -0.62273115]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.33757102 -0.12915267]]. Action = [[ 0.05875898 -0.06652127  0.         -0.7028109 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.33370644 -0.13494326]]. Action = [[ 0.01797774 -0.05327829  0.         -0.4863853 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.32810563 -0.14061922]]. Action = [[ 0.07460729 -0.04999409  0.         -0.8335734 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.327947   -0.14093696]]. Action = [[-0.07963566  0.07131896  0.          0.7177458 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.32594612 -0.13757168]]. Action = [[ 0.0772061   0.05621044  0.         -0.18619573]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.32152265 -0.13429005]]. Action = [[0.03797211 0.04063547 0.         0.9151566 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.32027033 -0.13055375]]. Action = [[-0.0069726   0.05363946  0.         -0.6771934 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.31584424 -0.13230997]]. Action = [[ 0.09162039 -0.07315674  0.          0.00815642]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.31361908 -0.13150449]]. Action = [[-0.03014021  0.06307799  0.         -0.41412687]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.31577697 -0.12708302]]. Action = [[-0.03489859  0.05320347  0.         -0.9824247 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.31445447 -0.1264547 ]]. Action = [[ 0.04783314 -0.02968664  0.          0.74320936]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.31646934 -0.1294241 ]]. Action = [[-0.07372199 -0.05044695  0.         -0.93425536]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.31943014 -0.13560559]]. Action = [[-0.02328431 -0.08773293  0.         -0.65297973]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[-0.3186109  -0.13751926]]. Action = [[ 0.0261556   0.02310929  0.         -0.6677866 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.3216306  -0.13375203]]. Action = [[-0.08595458  0.07295001  0.         -0.4829867 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.32153058 -0.13535304]]. Action = [[ 0.06286944 -0.08135904  0.         -0.45979333]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.3143743  -0.13975422]]. Action = [[ 0.09697468 -0.04056672  0.         -0.40651995]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.30844894 -0.13634124]]. Action = [[ 0.03734118  0.09616994  0.         -0.6715108 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.3069756  -0.13551229]]. Action = [[-0.01193266 -0.03997818  0.          0.89520514]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.30440852 -0.13636753]]. Action = [[ 0.0445999   0.00695129  0.         -0.25983584]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.29962942 -0.13855012]]. Action = [[ 0.05716745 -0.04645311  0.          0.56068444]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.2963461 -0.1371155]]. Action = [[0.01510642 0.05753209 0.         0.617563  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.29652888 -0.13108552]]. Action = [[-0.02799656  0.08973243  0.         -0.1781767 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.29329243 -0.1306683 ]]. Action = [[ 0.0772661  -0.05600893  0.         -0.70971084]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(9.2873e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.28658608 -0.12822846]]. Action = [[ 0.0714271   0.06927688  0.         -0.42163205]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.2809999 -0.1217021]]. Action = [[ 0.04839612  0.07501834  0.         -0.8339299 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.28319392 -0.11852476]]. Action = [[-0.09448094  0.00178598  0.          0.4297037 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.28646412 -0.12231229]]. Action = [[-0.02993742 -0.08535568  0.         -0.45468527]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.2901801  -0.12655434]]. Action = [[-0.07186637 -0.03397761  0.         -0.27332467]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.28928813 -0.13210791]]. Action = [[ 0.04749254 -0.08505278  0.          0.01740015]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.289478   -0.13890883]]. Action = [[-0.04933196 -0.07431152  0.          0.33919227]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.29044995 -0.14553288]]. Action = [[-0.01127589 -0.06390125  0.         -0.20525444]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.2932322  -0.14620885]]. Action = [[-0.06516351  0.05635882  0.          0.87274086]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.29118463 -0.14258927]]. Action = [[0.07465333 0.06274229 0.         0.27407444]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.28910053 -0.1378831 ]]. Action = [[-0.00325283  0.0615325   0.         -0.5585762 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.28618285 -0.13740622]]. Action = [[ 0.06040824 -0.03048173  0.          0.44698083]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of 1
Current timestep = 61. State = [[-0.28858    -0.13935415]]. Action = [[-0.08569698 -0.01994359  0.         -0.5944482 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.28970462 -0.14251317]]. Action = [[ 0.03336404 -0.04481505  0.          0.79600286]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.28680682 -0.1438045 ]]. Action = [[ 0.04818711  0.00235786  0.         -0.4855011 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.28776932 -0.14490907]]. Action = [[-0.04535669 -0.0206544   0.          0.2176373 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.2907055  -0.14647129]]. Action = [[-0.03117846 -0.00955661  0.          0.62722456]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
