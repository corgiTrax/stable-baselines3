Current timestep = 0. State = [[-0.25950497  0.00804204]]. Action = [[ 0.11840898 -0.15790617  0.20004636  0.69168067]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0408, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25950497  0.00804204]]. Action = [[-0.46353653  0.32314825 -0.34916887 -0.9083623 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25950497  0.00804204]]. Action = [[ 0.21552896 -0.3466853   0.47634804 -0.826576  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25950497  0.00804204]]. Action = [[-0.30966434 -0.43227798 -0.18149966 -0.9427109 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25950497  0.00804204]]. Action = [[-0.44665155 -0.15947115 -0.03258991 -0.9672477 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25950497  0.00804204]]. Action = [[-0.31268942  0.4479282  -0.35807666 -0.81817037]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25950497  0.00804204]]. Action = [[-0.4564156  -0.47082096 -0.03552467 -0.9419192 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.25950497  0.00804204]]. Action = [[ 0.49671233 -0.31845412  0.20412952 -0.4467029 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.25950497  0.00804204]]. Action = [[-0.24171576 -0.01671344  0.4575467   0.6864531 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.25950497  0.00804204]]. Action = [[0.22353876 0.4419564  0.35340536 0.3945017 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25950497  0.00804204]]. Action = [[-0.22763568 -0.23031488 -0.25166377 -0.862889  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.25950497  0.00804204]]. Action = [[ 0.1831733   0.02010322 -0.37424907 -0.2650448 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.25950497  0.00804204]]. Action = [[ 0.2517575  -0.10410103  0.48639572 -0.14997816]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.25950497  0.00804204]]. Action = [[-0.31564665 -0.15328395 -0.22436088 -0.10338044]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25950497  0.00804204]]. Action = [[-0.3846193  -0.17737707  0.3917765  -0.14887792]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.25950497  0.00804204]]. Action = [[-0.45761654 -0.2972384  -0.45612797 -0.09046263]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.2594301   0.00809346]]. Action = [[ 0.05437481  0.09701931 -0.3183054  -0.68888295]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.2592722   0.00866468]]. Action = [[ 0.34321034  0.30604768  0.4877354  -0.94940126]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25903302  0.00913517]]. Action = [[ 0.18386889 -0.15340662  0.36682665  0.56176996]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.25860807  0.00977929]]. Action = [[ 0.38442874 -0.08893967 -0.42589012  0.7887337 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25848615  0.00982174]]. Action = [[ 0.38719845 -0.04258612 -0.4025088  -0.00442988]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25827485  0.010193  ]]. Action = [[0.33800483 0.13754398 0.47882533 0.69688463]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.258015    0.01067017]]. Action = [[-0.21026865 -0.4112885   0.18628877  0.8436029 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.25788662  0.0115018 ]]. Action = [[ 0.0419715   0.09968513 -0.05966571 -0.9020108 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.25782442  0.01201019]]. Action = [[-0.29656523 -0.27892464 -0.03393617  0.39096308]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.25776106  0.01254528]]. Action = [[-0.22993287 -0.35933888  0.31987345  0.67083454]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.25767642  0.01327517]]. Action = [[-0.367265   -0.3496666  -0.06869465 -0.27999413]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.25766912  0.01367025]]. Action = [[-0.22687623 -0.2700979  -0.34551278  0.69186854]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.25766847  0.01407394]]. Action = [[-0.38526484  0.16702765  0.19715244  0.9658215 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.25747454  0.01470535]]. Action = [[-0.03798082 -0.1359567  -0.3989602  -0.17908609]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.2575154   0.01514749]]. Action = [[-0.26713696  0.05322909  0.4051597   0.45725417]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.25757745  0.01572841]]. Action = [[ 0.00857925 -0.33186546 -0.21273953 -0.9668865 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.25766504  0.0159534 ]]. Action = [[0.29183704 0.4067061  0.40753913 0.6714195 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.25759608  0.01614735]]. Action = [[-0.12652886 -0.34325218 -0.15157881 -0.5068649 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.2575691   0.01633438]]. Action = [[-0.10522804 -0.3765348   0.02555162 -0.16525823]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.2577524   0.01655773]]. Action = [[0.1397807  0.41466516 0.36886168 0.6423948 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.25774732  0.01680587]]. Action = [[ 0.36928618 -0.33343506  0.05070382 -0.7874516 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.2578322   0.01685113]]. Action = [[ 0.46181738  0.34008658 -0.20515245 -0.42790723]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.2578391   0.01697588]]. Action = [[-0.08530301 -0.43264055 -0.23478016 -0.68964   ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.25767726  0.01704923]]. Action = [[ 0.27750194 -0.0181779  -0.19728902  0.30944753]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.25775185  0.0171727 ]]. Action = [[ 0.18306744 -0.34085453 -0.24657777  0.48950982]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.25765923  0.01743112]]. Action = [[-0.09045577 -0.28307772  0.49199337 -0.22733474]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.25765923  0.01743112]]. Action = [[-0.25874415 -0.01247486 -0.19686037 -0.65647423]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.25774947  0.01741436]]. Action = [[ 0.20362622  0.314579   -0.31028992 -0.7840849 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.25765923  0.01743112]]. Action = [[-7.50094652e-04  3.72395992e-01  1.05603874e-01 -8.62488210e-01]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(9.9007e-05, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.25774947  0.01741436]]. Action = [[-0.29356223  0.10848463 -0.48590207 -0.48707318]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.25774947  0.01741436]]. Action = [[-0.3072604   0.28249258  0.30482864  0.24924076]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.25768232  0.01741574]]. Action = [[-0.25987148  0.07406473 -0.14379963  0.6956644 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 48. State = [[-0.25774947  0.01741436]]. Action = [[ 0.3148656  -0.27539423 -0.3978444   0.60629225]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.25768232  0.01741574]]. Action = [[-0.39648697 -0.08623305 -0.4808354  -0.7200063 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(6.6406e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.25774947  0.01741436]]. Action = [[-0.07547793 -0.02969247  0.34620035 -0.6123414 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 51. State = [[-0.25774947  0.01741436]]. Action = [[-0.46576065 -0.38747928  0.18579763  0.51437235]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.25774947  0.01741436]]. Action = [[ 0.33343875 -0.4262557  -0.44722056 -0.2932992 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.25774947  0.01741436]]. Action = [[0.323691   0.2935567  0.38646865 0.7995485 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.25774947  0.01741436]]. Action = [[ 0.15084487 -0.42999104  0.294317   -0.02511555]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.25774947  0.01741436]]. Action = [[-0.11879867  0.47685492  0.02145785  0.02590728]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.25774947  0.01741436]]. Action = [[0.46583533 0.30702972 0.45532    0.59001875]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.25774947  0.01741436]]. Action = [[-0.09153593 -0.24441645  0.17251301 -0.90685844]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.25774947  0.01741436]]. Action = [[ 0.25288737 -0.37244228 -0.34082866  0.9214616 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(2.6315e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.25772986  0.01737439]]. Action = [[ 0.05756825 -0.04644793 -0.05344886  0.9132986 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(2.5730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.25768927  0.01730503]]. Action = [[0.43250585 0.15909612 0.01703054 0.30175412]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.25774625  0.01714685]]. Action = [[-0.3270224  -0.23572862  0.17229277 -0.07593912]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.25775325  0.01703653]]. Action = [[ 0.46436012 -0.27938378 -0.0327346   0.41520822]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 63. State = [[-0.25777036  0.01708266]]. Action = [[-0.2339552  -0.08801264 -0.40589616  0.8777629 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.257684   0.0166019]]. Action = [[-0.07100266 -0.09203279  0.409719   -0.5307129 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.25754854  0.01575112]]. Action = [[ 0.30673337  0.05103332 -0.4764765  -0.6123854 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.2574764   0.01503103]]. Action = [[-0.32643473 -0.13088289 -0.46898106 -0.80528367]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.2574852   0.01487151]]. Action = [[-0.40918827  0.0309236  -0.44355476 -0.34654152]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.2576153   0.01456981]]. Action = [[-0.24650395  0.4756676   0.21952903  0.85767174]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.2576139  0.0143024]]. Action = [[ 0.40127116 -0.03269655  0.34422052  0.19852448]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.2575228   0.01350721]]. Action = [[ 0.0893929   0.24337006 -0.2778812  -0.17132545]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.25768495  0.01330459]]. Action = [[-0.45073286  0.40552485 -0.47828105  0.51050127]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.25769648  0.01287173]]. Action = [[ 0.39848995 -0.01259896 -0.27311367  0.64429355]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.25761214  0.01255894]]. Action = [[-0.32558146 -0.07988924 -0.15052366  0.11025167]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 74. State = [[-0.2577124   0.01238317]]. Action = [[ 0.32287896 -0.01966363 -0.2817717   0.23347902]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.25775817  0.01231036]]. Action = [[0.239111   0.07980001 0.04953563 0.9884076 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(2.1851e-05, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.2577582   0.01204308]]. Action = [[-0.24850127 -0.47189197 -0.12609738  0.594481  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.25769103  0.01177636]]. Action = [[ 0.26601523  0.11301678 -0.22192335  0.01227403]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.25780413  0.01170257]]. Action = [[ 0.23418641 -0.08270711 -0.20864576  0.3762728 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.25782183  0.01121159]]. Action = [[-0.3977591   0.2686144   0.44243246 -0.303425  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.25776753  0.01104629]]. Action = [[ 0.2301299  -0.4210937   0.45834267 -0.7864493 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(8.1341e-06, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.25786415  0.01092605]]. Action = [[ 0.13867414  0.04877681  0.29781735 -0.46738255]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.25785503  0.01065941]]. Action = [[0.16467297 0.23463619 0.46473598 0.08558881]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.25781092  0.01049424]]. Action = [[-0.21063754  0.10657555  0.48360538 -0.61668116]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(9.1539e-05, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.25790474  0.010313  ]]. Action = [[-0.43713567  0.35373604 -0.13078642 -0.62687665]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 85. State = [[-0.25783646  0.01001507]]. Action = [[ 0.12124902 -0.0695219   0.3085965  -0.48480916]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.2575956   0.00953204]]. Action = [[-0.09123641  0.37371993 -0.35999224  0.23424816]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.2573048  0.0089531]]. Action = [[-0.3459609   0.38899136 -0.45161837  0.2651167 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.256942    0.00750907]]. Action = [[ 0.12772405  0.03404772  0.40752423 -0.29867744]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.25656587  0.007585  ]]. Action = [[-0.00710905  0.3087926   0.14834523 -0.6435994 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.2555047   0.00780255]]. Action = [[ 0.36328232 -0.12450752 -0.12211886 -0.9279159 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(1.7677e-05, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.25468132  0.00769968]]. Action = [[-0.39034826 -0.2128402   0.145895   -0.14628565]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 92. State = [[-0.25135568  0.00753864]]. Action = [[-0.01377666  0.03290367  0.22097397  0.9039854 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(7.1628e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.2506584   0.00762414]]. Action = [[ 0.18943727 -0.26135623  0.43779087  0.26774716]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.25026885  0.00778947]]. Action = [[0.20694762 0.2911067  0.42196816 0.30882072]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.2501039   0.00775964]]. Action = [[-0.34308887 -0.10934958 -0.25751066  0.8487499 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(8.9400e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.24974723  0.00771322]]. Action = [[-0.4245187  -0.38096032 -0.17305821 -0.6841575 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(4.3245e-05, grad_fn=<MseLossBackward0>)
Current timestep = 97. State = [[-0.24932724  0.0077436 ]]. Action = [[ 0.1866166   0.20959514 -0.35617816 -0.6053473 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(8.3367e-05, grad_fn=<MseLossBackward0>)
Current timestep = 98. State = [[-0.24930279  0.00776073]]. Action = [[-0.41510233  0.44222152 -0.4418475  -0.09461379]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.24911304  0.00776015]]. Action = [[-0.1635074   0.31465286  0.32622647 -0.46019697]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.24861075  0.00779976]]. Action = [[-0.34046552 -0.19028372 -0.07203349 -0.40503973]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.24876957  0.0078266 ]]. Action = [[ 0.47658873 -0.32225773 -0.3504923   0.6643276 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(2.8907e-05, grad_fn=<MseLossBackward0>)
Current timestep = 102. State = [[-0.24863912  0.00787335]]. Action = [[-0.21039653  0.07008183  0.27816677 -0.819568  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(4.5898e-05, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.24828157  0.00795971]]. Action = [[-0.1327973  -0.232833   -0.39402318  0.11008453]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.248469    0.00785923]]. Action = [[ 0.47950256  0.10321206 -0.41671866 -0.7629113 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(5.5451e-05, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.24811544  0.00789818]]. Action = [[ 0.23543954  0.18219239 -0.4444742  -0.7826824 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 106. State = [[-0.24822202  0.00793022]]. Action = [[-0.31547728  0.44071096 -0.11596972  0.05527866]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.24816921  0.0078742 ]]. Action = [[ 0.08279216 -0.40308374 -0.4610109   0.31172276]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.24791394  0.00789855]]. Action = [[0.00245464 0.13713098 0.3895682  0.9677286 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 109. State = [[-0.24813865  0.00791565]]. Action = [[ 0.1333952  -0.12833372  0.22240543 -0.80202967]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(4.8246e-05, grad_fn=<MseLossBackward0>)
Current timestep = 110. State = [[-0.24780595  0.00794578]]. Action = [[-0.29127517 -0.3657189  -0.39459762  0.01344752]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.24772912  0.00794608]]. Action = [[ 0.38770986 -0.00139168  0.3364445  -0.913521  ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(5.8775e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.24793762  0.00791604]]. Action = [[-0.28169724 -0.07426941  0.11375737  0.22025537]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 113. State = [[-0.24780595  0.00794578]]. Action = [[ 0.48795617 -0.14638841 -0.4325799   0.5952523 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(4.3818e-05, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.24788664  0.00793086]]. Action = [[ 0.26856863 -0.12340719 -0.24161279  0.59950435]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.24779627  0.00794595]]. Action = [[-0.17141563 -0.1918214  -0.4347978   0.940269  ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(1.8921e-05, grad_fn=<MseLossBackward0>)
Current timestep = 116. State = [[-0.24779627  0.00794595]]. Action = [[ 3.5092580e-01  1.8695134e-01 -4.8373806e-01  1.9264221e-04]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.24781948  0.00793099]]. Action = [[-0.26843232  0.30596542  0.0777908   0.25604558]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.24779627  0.00794595]]. Action = [[-0.05182779  0.14944875  0.3112768  -0.04046828]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.24781948  0.00793099]]. Action = [[ 0.46873808 -0.28793916 -0.24498329  0.09593642]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.24781948  0.00793099]]. Action = [[0.05004042 0.3787272  0.22322309 0.8730513 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(3.4503e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.24781948  0.00793099]]. Action = [[ 0.38296652 -0.43201712  0.32718235  0.9657761 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 122. State = [[-0.24781948  0.00793099]]. Action = [[ 0.15678322 -0.1203084  -0.43709415  0.89208424]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(1.3890e-05, grad_fn=<MseLossBackward0>)
Current timestep = 123. State = [[-0.24781948  0.00793099]]. Action = [[-0.2679694  -0.08221933  0.23019749 -0.43590832]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(2.4539e-05, grad_fn=<MseLossBackward0>)
Current timestep = 124. State = [[-0.24781948  0.00793099]]. Action = [[ 0.25290924  0.42959702 -0.30802727  0.60740805]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.24781948  0.00793099]]. Action = [[ 0.3973297  0.4358133 -0.2661032 -0.9154442]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(7.3784e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.24781948  0.00793099]]. Action = [[-0.48493376 -0.01730591  0.17316031  0.32176518]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(3.3239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.24781948  0.00793099]]. Action = [[ 0.34376252  0.27581573  0.44165874 -0.7167066 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(2.9165e-06, grad_fn=<MseLossBackward0>)
Current timestep = 128. State = [[-0.24781948  0.00793099]]. Action = [[0.12550735 0.22747278 0.23139709 0.35716522]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.24781948  0.00793099]]. Action = [[-0.10367194  0.3570571   0.01331925  0.698189  ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(9.9975e-05, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.24781948  0.00793099]]. Action = [[ 0.3929808  -0.31231305 -0.29267308  0.02247131]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.24781948  0.00793099]]. Action = [[-0.34219283 -0.24594605 -0.07429338 -0.77254355]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.24787122  0.00809653]]. Action = [[-0.08057618  0.06740308  0.46520483 -0.82046044]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(4.9585e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.24805658  0.0085252 ]]. Action = [[ 0.11995387 -0.28699803  0.39079404 -0.9281672 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(7.7440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.24814966  0.00893675]]. Action = [[ 0.2514016  -0.37503576  0.0897634  -0.6361995 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(2.3384e-06, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.24850956  0.01013545]]. Action = [[-0.07296818  0.09213048 -0.00298169 -0.53727436]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(5.0172e-05, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.24881497  0.01095938]]. Action = [[0.23746967 0.33396518 0.31950855 0.26762545]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.24981222  0.01384519]]. Action = [[ 0.04518294 -0.07744992 -0.14189357 -0.4987347 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.24994811  0.01375424]]. Action = [[-0.01418772 -0.17755124 -0.23982793  0.8090737 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(6.7811e-05, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.24983852  0.01350109]]. Action = [[-0.31314695  0.39229608 -0.3852551  -0.7203437 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(5.1391e-05, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.2498258   0.01339772]]. Action = [[-0.25693938  0.42403686 -0.25219405 -0.54618937]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(6.8768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.24989788  0.01380261]]. Action = [[ 0.21240884 -0.0685049   0.24207973 -0.46373093]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(3.5977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 142. State = [[-0.2499423   0.01384936]]. Action = [[-0.1181936  -0.17459399 -0.2840096  -0.41377646]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 143. State = [[-0.24987829  0.01353276]]. Action = [[-0.46858034 -0.21255833  0.04633421  0.32807302]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(3.0336e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.24990796  0.01368351]]. Action = [[-0.06462657 -0.23455447 -0.04796904  0.43587327]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 145. State = [[-0.24995446  0.01388118]]. Action = [[0.34363818 0.3832096  0.40153122 0.606076  ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.24994165  0.01377781]]. Action = [[-0.36958417 -0.42678994  0.35384655  0.6289544 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(1.8645e-06, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.24989119  0.01363658]]. Action = [[-0.1337361  -0.15062636 -0.3451704   0.2302171 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 148. State = [[-0.24990796  0.01368351]]. Action = [[-0.29575694 -0.25308722  0.21231043  0.8125833 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(2.9331e-05, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.24995446  0.01388118]]. Action = [[-0.03725612 -0.25482118 -0.17539573  0.67400515]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.24993768  0.01383425]]. Action = [[0.17691487 0.45946413 0.36901712 0.53576803]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.24990796  0.01368351]]. Action = [[-0.4482275   0.47105587  0.2995941   0.7944906 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(1.3701e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.24994165  0.01377781]]. Action = [[-0.2758347   0.38703263 -0.01913503 -0.7714878 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(5.4045e-05, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.24992488  0.01373088]]. Action = [[0.25758082 0.29674882 0.0046069  0.877687  ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(2.3799e-07, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.24990796  0.01368351]]. Action = [[-0.34372294 -0.26400542 -0.41628483  0.80417466]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.24992488  0.01373088]]. Action = [[ 0.09829664 -0.26624766  0.08285707  0.817343  ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(4.2831e-06, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.24993768  0.01383425]]. Action = [[ 0.48948604  0.4173252   0.46044976 -0.9963584 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(2.8994e-05, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.24994165  0.01377781]]. Action = [[-0.34281072 -0.09329671  0.1871357   0.66720057]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(4.4169e-06, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.24992488  0.01373088]]. Action = [[ 0.23141009  0.31637847  0.09407818 -0.83288705]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(5.3842e-05, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.24992488  0.01373088]]. Action = [[ 0.4485339   0.15122509 -0.01247457 -0.39232445]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(6.4234e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.24992488  0.01373088]]. Action = [[-0.04738435  0.15133607 -0.35411265 -0.751326  ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.24992488  0.01373088]]. Action = [[-0.06036058  0.4181832   0.24558318 -0.73293465]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(5.4998e-05, grad_fn=<MseLossBackward0>)
Current timestep = 162. State = [[-0.24992488  0.01373088]]. Action = [[-0.40855795 -0.10482854 -0.48701113  0.67348576]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.24992488  0.01373088]]. Action = [[ 0.22415596  0.32116336 -0.34625155  0.15844357]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.2499209   0.01378732]]. Action = [[-0.17332321  0.21041256  0.10042459 -0.6259376 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(3.4233e-05, grad_fn=<MseLossBackward0>)
Current timestep = 165. State = [[-0.24992488  0.01373088]]. Action = [[ 0.16067469  0.4400797  -0.42340353 -0.44647807]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(9.6121e-05, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.2499209   0.01378732]]. Action = [[-0.36554676 -0.14572418 -0.39223945 -0.36505997]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(7.7034e-05, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.24992488  0.01373088]]. Action = [[0.29241776 0.11207283 0.09175622 0.0069108 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(6.2240e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.24992488  0.01373088]]. Action = [[-0.22349203  0.02615643 -0.02535596  0.6690177 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(7.5071e-06, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.24992488  0.01373088]]. Action = [[ 0.08085024  0.44141924 -0.3117309  -0.81387305]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(7.7912e-05, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.24992488  0.01373088]]. Action = [[ 0.44792336 -0.18682837 -0.09789354  0.9176383 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(6.3359e-06, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.24992488  0.01373088]]. Action = [[-0.10017797  0.31727594  0.11940539  0.9048445 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(4.7270e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.24992488  0.01373088]]. Action = [[ 0.06885231  0.26549685  0.25176454 -0.9419709 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(1.1345e-05, grad_fn=<MseLossBackward0>)
Current timestep = 173. State = [[-0.24992488  0.01373088]]. Action = [[ 0.24046409 -0.3917693  -0.2310257   0.6979414 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(7.5730e-05, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.24992488  0.01373088]]. Action = [[ 0.09787625 -0.26925567  0.3184576  -0.95076096]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(4.4855e-05, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.24992488  0.01373088]]. Action = [[-0.45350048 -0.18645042 -0.2412971  -0.9010455 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.24992488  0.01373088]]. Action = [[ 0.25773728  0.30226088  0.4889326  -0.643642  ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(1.1206e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.24992488  0.01373088]]. Action = [[ 0.36080337 -0.07224759  0.376518   -0.09911281]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(1.0035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.24992488  0.01373088]]. Action = [[-0.17706949 -0.31810647  0.01406336 -0.60889846]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.24992488  0.01373088]]. Action = [[-0.26474518 -0.03929627 -0.4332979   0.64043665]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(6.9493e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of -1
Current timestep = 180. State = [[-0.24992488  0.01373088]]. Action = [[-0.41786242  0.26562083  0.01597077  0.10267222]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 181. State = [[-0.24992488  0.01373088]]. Action = [[-0.07022947 -0.3213383   0.07122219  0.16490746]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(2.7670e-05, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.24992488  0.01373088]]. Action = [[-0.11481664  0.3081144  -0.17376599  0.16274548]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-0.24992488  0.01373088]]. Action = [[-0.34966865 -0.15411347 -0.01714939 -0.6284482 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.24992488  0.01373088]]. Action = [[-0.458116   -0.01093987  0.40446806  0.78682494]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(5.1069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.24994801  0.01371552]]. Action = [[ 0.1678344  0.4591211 -0.1959315  0.8509449]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(1.5131e-06, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.24994801  0.01371552]]. Action = [[ 0.04133695 -0.27249837  0.08673334  0.45236194]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(4.6933e-06, grad_fn=<MseLossBackward0>)
Current timestep = 187. State = [[-0.24994801  0.01371552]]. Action = [[ 0.27616817 -0.27741948  0.3110661   0.99152374]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(7.0023e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.24994801  0.01371552]]. Action = [[ 0.2907219  -0.2835802  -0.22150809 -0.2863239 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(7.7938e-05, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.24994801  0.01371552]]. Action = [[ 0.30592108  0.3925594  -0.46129867  0.1835357 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(5.8534e-05, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.24994801  0.01371552]]. Action = [[ 0.01344275 -0.25540447  0.06239766  0.90578103]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(1.9304e-05, grad_fn=<MseLossBackward0>)
Current timestep = 191. State = [[-0.24994801  0.01371552]]. Action = [[ 0.48800457 -0.1405552   0.10895336  0.62707543]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(3.2713e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.24994801  0.01371552]]. Action = [[ 0.21548688  0.11045027  0.01773894 -0.94911   ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(7.3947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.24994801  0.01371552]]. Action = [[ 0.14007735 -0.11709684  0.09744418  0.69148874]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(2.5143e-05, grad_fn=<MseLossBackward0>)
Current timestep = 194. State = [[-0.24994801  0.01371552]]. Action = [[ 0.35480058  0.34026343  0.39278924 -0.72101116]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(2.4761e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.24994801  0.01371552]]. Action = [[-0.20191938 -0.21158424 -0.39170325 -0.91802037]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 196. State = [[-0.24994801  0.01371552]]. Action = [[-0.28466204 -0.20250428  0.3751052   0.70085764]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(9.5663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.24994801  0.01371552]]. Action = [[ 0.45124382 -0.29963636 -0.29584682 -0.04567277]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(1.8707e-05, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.24994801  0.01371552]]. Action = [[-0.06227559  0.42987478 -0.18880457 -0.6640644 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(6.0035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.24994801  0.01371552]]. Action = [[-0.48452193  0.1407131  -0.43984777 -0.33707023]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(4.6121e-05, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.24994801  0.01371552]]. Action = [[ 0.23612791 -0.34660515  0.14740944  0.428028  ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(6.3688e-05, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.24994801  0.01371552]]. Action = [[ 0.02077931 -0.3877399   0.38885665 -0.48318183]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.24994801  0.01371552]]. Action = [[-0.19520345 -0.3703879  -0.3740763   0.35124767]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(6.5641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[-0.24994801  0.01371552]]. Action = [[-0.08077526  0.39130467  0.08482987  0.31894577]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(4.2384e-05, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.24994801  0.01371552]]. Action = [[ 0.38672322  0.42159456 -0.40601447  0.86935663]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(3.7896e-06, grad_fn=<MseLossBackward0>)
Current timestep = 205. State = [[-0.24999604  0.01367347]]. Action = [[-0.09759131 -0.04415423  0.00934011  0.10733807]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(2.6449e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.2503248   0.01333995]]. Action = [[-0.07810664  0.0129407  -0.01503283 -0.77422667]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.2508527   0.01325238]]. Action = [[ 0.24825901  0.1595422  -0.43682298  0.74665666]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(1.3736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.25110644  0.01323989]]. Action = [[ 0.08856946  0.3135906  -0.24691856 -0.9426694 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.25140923  0.01317941]]. Action = [[-0.41263914 -0.05940294 -0.39162338 -0.9674308 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.25166112  0.01312898]]. Action = [[-0.2357145  -0.14032042 -0.01466897 -0.34246922]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(5.5793e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.2521358   0.01312254]]. Action = [[ 0.17084742 -0.21916264 -0.14498419  0.5179329 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(6.4716e-07, grad_fn=<MseLossBackward0>)
Current timestep = 212. State = [[-0.25220314  0.01312163]]. Action = [[ 0.25651586 -0.45403042 -0.35238397 -0.37356663]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(8.3366e-05, grad_fn=<MseLossBackward0>)
Current timestep = 213. State = [[-0.25233775  0.0131198 ]]. Action = [[ 0.3730768  -0.0791842   0.35605925  0.00859451]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(6.3885e-05, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.25240463  0.01311889]]. Action = [[ 0.41375345  0.0657869  -0.05869767 -0.34752607]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(1.9354e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.25254333  0.01306129]]. Action = [[-0.32436207  0.445588    0.39179164 -0.7974934 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(9.4989e-06, grad_fn=<MseLossBackward0>)
Current timestep = 216. State = [[-0.25266552  0.01298884]]. Action = [[-0.24295217  0.28238487  0.22324836  0.10561728]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(2.7712e-05, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.2526082   0.01306623]]. Action = [[-0.3738765  -0.38272005 -0.23145062  0.1304506 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(5.8065e-06, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.25275314  0.01313422]]. Action = [[-0.15901375  0.3583904   0.25177276 -0.17152613]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(2.2201e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.25310475  0.01324953]]. Action = [[-0.11538261  0.08145118 -0.21997353 -0.81240636]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.2539002   0.01393677]]. Action = [[ 0.10153097 -0.45046204  0.1806001   0.22277904]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(1.6303e-05, grad_fn=<MseLossBackward0>)
Current timestep = 221. State = [[-0.25446212  0.01431311]]. Action = [[-0.37543237 -0.15146512  0.17467862 -0.7208867 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(6.7108e-05, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.25495094  0.01492295]]. Action = [[-0.21205539 -0.4588468  -0.1952365   0.81808865]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(2.4764e-05, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.25530648  0.01544397]]. Action = [[-0.28993115  0.42223227 -0.02027082 -0.5084352 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(4.5052e-06, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.2560581   0.01596769]]. Action = [[-0.08717304  0.22746772 -0.30824432 -0.8702156 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.2570157   0.01688046]]. Action = [[-0.18158683  0.21298367  0.43573785  0.14391029]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(3.2325e-05, grad_fn=<MseLossBackward0>)
Current timestep = 226. State = [[-0.25757605  0.01725609]]. Action = [[-0.2581275  -0.24209297 -0.06895751  0.06270456]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(5.6977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.25799352  0.0174155 ]]. Action = [[ 0.19554341 -0.42232865  0.3653952  -0.22619659]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(9.4647e-06, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.25834334  0.01758507]]. Action = [[-0.02573001 -0.14019239 -0.16473132 -0.86641204]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(7.6773e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.2587406   0.01779028]]. Action = [[-0.38869005  0.14363825 -0.4743718   0.24238741]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.25881281  0.01782275]]. Action = [[ 0.35727286 -0.3912773  -0.17302078 -0.6211988 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(4.5308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.25965387  0.01800856]]. Action = [[ 0.01128179  0.02142704 -0.0139004  -0.04552734]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(8.4799e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.25979728  0.01824667]]. Action = [[-0.44960797  0.26903558  0.25667048 -0.12739515]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(7.2040e-05, grad_fn=<MseLossBackward0>)
Current timestep = 233. State = [[-0.2598122   0.01837639]]. Action = [[-0.14217809  0.4040172   0.44276106  0.62488735]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(8.8247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.2597999   0.01858672]]. Action = [[ 0.08588719 -0.2652268   0.20103407 -0.17309844]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(2.8415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.26002404  0.01860373]]. Action = [[ 0.06902283 -0.3115325  -0.20935562 -0.762179  ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(8.0057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.2600704   0.01880375]]. Action = [[-0.45807472 -0.42814213 -0.02093121 -0.17673379]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(5.7599e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.26028076  0.01892263]]. Action = [[-0.35521454  0.3123883   0.09744996 -0.28991568]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(8.6714e-05, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.2604116   0.01888699]]. Action = [[ 0.33102942  0.4402234  -0.4020362  -0.2826153 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(7.0865e-05, grad_fn=<MseLossBackward0>)
Current timestep = 239. State = [[-0.26068768  0.01936707]]. Action = [[ 0.10653734  0.0996691  -0.05643117 -0.38287628]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(1.8366e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.2609068   0.02023953]]. Action = [[ 0.14558876  0.2520507  -0.3093875   0.975428  ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(4.4301e-05, grad_fn=<MseLossBackward0>)
Current timestep = 241. State = [[-0.26148173  0.02187303]]. Action = [[ 0.04560024 -0.0108501  -0.06703502  0.28009176]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.26150253  0.02210489]]. Action = [[ 0.47306478  0.23695832 -0.30273816 -0.0265699 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 243. State = [[-0.26152015  0.02215064]]. Action = [[-0.05339044  0.23948914 -0.19463396 -0.46721685]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(6.8341e-05, grad_fn=<MseLossBackward0>)
Current timestep = 244. State = [[-0.26157853  0.0222268 ]]. Action = [[ 0.28496194  0.15179092  0.27084935 -0.59308666]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(6.2679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.2616109   0.02237392]]. Action = [[-0.3362017   0.45236987 -0.4006009  -0.9114127 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(6.7464e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.26162055  0.02253724]]. Action = [[-0.36932787 -0.01720697 -0.09040594 -0.21889108]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.261679    0.02261336]]. Action = [[-0.26081827  0.2664175  -0.11392391  0.3461156 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 248. State = [[-0.2616503   0.02274045]]. Action = [[ 0.04064488 -0.43040675  0.4178189  -0.962579  ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(3.7896e-05, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.2616503   0.02274045]]. Action = [[-0.31357786  0.42151356 -0.28642085 -0.36487925]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(9.5427e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.26167035  0.02278033]]. Action = [[ 0.22003138 -0.36465037 -0.15723056 -0.3707466 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.26166463  0.02289128]]. Action = [[-0.47897005 -0.2200532   0.4363504   0.9079524 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(4.4690e-06, grad_fn=<MseLossBackward0>)
Current timestep = 252. State = [[-0.26163602  0.02301838]]. Action = [[-0.37223428 -0.28857973  0.3068415  -0.44257843]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(8.2572e-05, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.26163316  0.02307405]]. Action = [[-0.3880056  -0.4806445   0.22851634 -0.16877222]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(5.2455e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.26162753  0.023185  ]]. Action = [[-0.42165518 -0.13832155  0.43428725  0.7182739 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(1.4343e-06, grad_fn=<MseLossBackward0>)
Current timestep = 255. State = [[-0.26165044  0.02316921]]. Action = [[ 0.47232592  0.21985334 -0.29219335  0.6630075 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(4.3088e-05, grad_fn=<MseLossBackward0>)
Current timestep = 256. State = [[-0.26165044  0.02316921]]. Action = [[-0.45379826 -0.39798987  0.14948624 -0.480789  ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(8.6453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.26162472  0.02324066]]. Action = [[-0.3001965   0.39368308  0.38593566 -0.6248274 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.2616448   0.02328053]]. Action = [[ 0.36101782 -0.2967982  -0.16129124 -0.7244787 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 259. State = [[-0.26160216  0.02341679]]. Action = [[ 0.08308184  0.05698115 -0.34304225  0.47590423]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(8.9666e-05, grad_fn=<MseLossBackward0>)
Current timestep = 260. State = [[-0.26106673  0.02384079]]. Action = [[0.00475603 0.43473458 0.14741611 0.36689723]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.25968248  0.0249373 ]]. Action = [[0.01389992 0.1155479  0.19034743 0.1478455 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(6.1872e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.2594882   0.02636179]]. Action = [[ 0.38862455 -0.3496606   0.11881369  0.7258427 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(7.5756e-05, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.25946057  0.02699943]]. Action = [[-0.39430663  0.15691674 -0.32851288  0.07216918]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.2593479   0.02799912]]. Action = [[-0.48796645 -0.17704272  0.36110485 -0.8078498 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(7.4291e-05, grad_fn=<MseLossBackward0>)
Current timestep = 265. State = [[-0.2591974   0.02885244]]. Action = [[-0.04536194 -0.2351399   0.34048736  0.95511246]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(1.9770e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.25913653  0.03005905]]. Action = [[-0.33103633  0.31237113  0.25532842  0.76489305]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 267. State = [[-0.25904256  0.03079229]]. Action = [[ 0.31587958  0.4067319  -0.4160352  -0.81690943]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.2591927   0.03128159]]. Action = [[0.29066253 0.20031881 0.19313693 0.2481153 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(8.9158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.2591167   0.03181919]]. Action = [[-0.34322226 -0.05025202 -0.35536546 -0.4826464 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.2589472   0.03213547]]. Action = [[ 0.4013549  -0.27904785  0.3330559  -0.19253832]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(2.4350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 271. State = [[-0.2588996   0.03343994]]. Action = [[-0.12995693  0.00104392  0.21479046  0.03154576]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.25900084  0.03391557]]. Action = [[-0.12515098 -0.267882   -0.05835095 -0.9623378 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 273. State = [[-0.25916016  0.03415207]]. Action = [[-0.01458979 -0.21044996  0.36910343  0.8578327 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(4.4305e-05, grad_fn=<MseLossBackward0>)
Current timestep = 274. State = [[-0.25939324  0.03439737]]. Action = [[-0.09595445 -0.4239937   0.47160506 -0.49656963]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.25942662  0.03484453]]. Action = [[ 0.00844675  0.25800192  0.29778588 -0.6338975 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of -1
Current timestep = 276. State = [[-0.25966078  0.0352226 ]]. Action = [[-0.39704442  0.38161993  0.11518586 -0.7145654 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.2597358   0.03546354]]. Action = [[ 0.27917516  0.17747152  0.25211394 -0.79238546]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-0.25979298  0.03560174]]. Action = [[ 0.434016    0.26212418 -0.45006335  0.40512514]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.25987262  0.03572347]]. Action = [[-0.17338115 -0.22262466 -0.30047628 -0.9675415 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 280. State = [[-0.25990963  0.03587207]]. Action = [[ 0.25599283 -0.12279725 -0.4959468  -0.28823894]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 281. State = [[-0.25992674  0.03592443]]. Action = [[-0.19539261  0.24474972 -0.42401785  0.40472102]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.26003402  0.03604455]]. Action = [[ 0.29513758 -0.33834532 -0.00853071  0.11463463]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.26012006  0.03637866]]. Action = [[ 0.0674389   0.02381945 -0.22373804 -0.27197373]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.260138   0.0364809]]. Action = [[-0.36039    -0.46443754  0.2513916   0.7068174 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 285. State = [[-0.26013914  0.03642455]]. Action = [[-0.3092254  0.3792563 -0.3003262  0.7686939]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 286. State = [[-0.26015842  0.03647086]]. Action = [[ 0.28942704  0.16342032 -0.20000869  0.01083815]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 287. State = [[-0.2601764  0.0365731]]. Action = [[-0.04426384  0.44264877 -0.47174582  0.8820324 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.2601764  0.0365731]]. Action = [[-0.04095209  0.17765051  0.48024368 -0.7927384 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 289. State = [[-0.2601764  0.0365731]]. Action = [[ 0.13850838 -0.34766185  0.00932199  0.9310286 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.2601764  0.0365731]]. Action = [[-0.1250441  -0.37177145 -0.14494896  0.37445974]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.2601764  0.0365731]]. Action = [[ 0.09100467  0.49285305  0.18015975 -0.23572552]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.2601764  0.0365731]]. Action = [[ 0.04639244  0.43558872 -0.38662922  0.71231794]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 293. State = [[-0.2601764  0.0365731]]. Action = [[ 0.43027484  0.16668802 -0.474707    0.08466125]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.26010498  0.03663437]]. Action = [[ 0.13278902  0.0923779   0.02641088 -0.06085491]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.2595025   0.03741872]]. Action = [[-0.45665115  0.40038633 -0.42477882 -0.45161492]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.25883466  0.03792952]]. Action = [[-0.40071335 -0.11133671 -0.3249675   0.0570457 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.25853822  0.0381958 ]]. Action = [[ 0.32137895  0.01454407 -0.4160669   0.40749228]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.2581362  0.0386048]]. Action = [[-0.13668233  0.24788254 -0.31553587 -0.7474728 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.25667763  0.03986669]]. Action = [[ 0.01938409 -0.46175346 -0.39415875 -0.13979185]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 300. State = [[-0.25629872  0.04018864]]. Action = [[-0.25939727  0.27171206  0.02461088 -0.7695804 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.25557384  0.04087939]]. Action = [[ 0.44222367 -0.44071904  0.45317286 -0.86570716]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 302. State = [[-0.25540262  0.04103567]]. Action = [[ 0.30069256 -0.22831243 -0.46419173  0.39457345]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.25521538  0.04121768]]. Action = [[-0.26430842  0.0207414   0.43252808 -0.82897156]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.2550908   0.04153927]]. Action = [[-0.4400001   0.21407169 -0.07461661  0.6242446 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.25520232  0.04154023]]. Action = [[ 0.10467261 -0.3462844  -0.37401655 -0.5133315 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.25506374  0.04168515]]. Action = [[ 0.48773634  0.46482468 -0.43538874  0.60558414]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 307. State = [[-0.25507754  0.04202387]]. Action = [[ 0.31933367 -0.08814222  0.05783707  0.41850317]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 308. State = [[-0.25480902  0.0420501 ]]. Action = [[-0.1329382   0.1596657  -0.20882249  0.34530985]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of 1
Current timestep = 309. State = [[-0.254535    0.04225416]]. Action = [[ 0.283646   -0.0975641   0.40156424 -0.6793287 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 310. State = [[-0.25485834  0.0422483 ]]. Action = [[-0.33242917  0.36641324  0.4690932   0.88860965]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 311. State = [[-0.25468275  0.04233935]]. Action = [[-0.16391256 -0.22111681  0.45385098 -0.28743613]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.2547241   0.04242628]]. Action = [[ 0.42820746  0.15560973 -0.12840962 -0.853206  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.25472373  0.0424835 ]]. Action = [[-0.41405088 -0.03778768 -0.49627337 -0.5539624 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.2544971   0.04257277]]. Action = [[-0.38243484  0.28829026 -0.21259373  0.7300279 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of 1
Current timestep = 315. State = [[-0.2545467   0.04255414]]. Action = [[ 0.4185211   0.24950868 -0.25646922  0.06978166]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.2546353   0.04253235]]. Action = [[ 0.25853443  0.04230756 -0.4886187   0.38033998]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 317. State = [[-0.25457335  0.0427777 ]]. Action = [[-0.06811151  0.04312873 -0.15067679  0.6050451 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.25484017  0.04328992]]. Action = [[-0.303844    0.08585215 -0.38837746  0.4428115 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.25553367  0.04511563]]. Action = [[-0.09634146  0.11271966 -0.1264711  -0.49114656]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.25620356  0.04691061]]. Action = [[-0.3245713  -0.01119497 -0.128721    0.3427801 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 321. State = [[-0.2565344   0.04778278]]. Action = [[ 0.36720067  0.4311571  -0.28339478 -0.96541196]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 322. State = [[-0.2568721   0.04862669]]. Action = [[ 0.413916   -0.2513588   0.21869105  0.830981  ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.25753874  0.04997936]]. Action = [[-0.34600538 -0.22779614  0.08467257 -0.7778259 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 324. State = [[-0.25802377  0.051278  ]]. Action = [[ 0.38582152 -0.18136126 -0.19853872  0.5566628 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.25825173  0.05166633]]. Action = [[ 0.01657033  0.35997117 -0.35162127  0.456941  ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 326. State = [[-0.25847918  0.05211756]]. Action = [[-0.4737901  -0.44602337 -0.36878732 -0.50150675]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.25873572  0.05274362]]. Action = [[-0.2579176   0.06547314  0.34397972  0.96174884]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 328. State = [[-0.25896522  0.05325153]]. Action = [[ 0.21815836 -0.35202003 -0.45612627  0.63692963]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.2595025   0.05462119]]. Action = [[ 0.12724072  0.09472156 -0.06205264 -0.93036145]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.25935003  0.05548525]]. Action = [[-0.42868263 -0.4837083   0.14462125 -0.8414266 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 331. State = [[-0.2590207   0.05607341]]. Action = [[ 0.3391286  -0.30806348 -0.46503022 -0.68642706]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.25901067  0.05642818]]. Action = [[-0.16239679 -0.40195265 -0.2879589  -0.33901155]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.25893158  0.05716387]]. Action = [[ 0.16783619 -0.10996825  0.05556339 -0.1747272 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.25884086  0.05784984]]. Action = [[-0.14596766 -0.265802   -0.38464352  0.853639  ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.25867957  0.05847353]]. Action = [[ 0.16315496 -0.3891764   0.38435245  0.77927434]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.258584    0.05871409]]. Action = [[ 0.4174776  -0.16540736 -0.26404107  0.53880227]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.2582489  0.0589266]]. Action = [[-0.21881783  0.3756696   0.41194177 -0.1775139 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.25829437  0.05906229]]. Action = [[-0.47033295 -0.40233245  0.2053371   0.43002582]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.25843713  0.05909643]]. Action = [[-0.16416186  0.06575954 -0.23452514 -0.9555526 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.25796756  0.05947842]]. Action = [[-0.31736532  0.3478387  -0.07788727 -0.42440963]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 341. State = [[-0.2582114   0.05946093]]. Action = [[ 0.21806133 -0.11085713 -0.21481943  0.870939  ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.2581487   0.05951664]]. Action = [[ 0.2225393  -0.10943556  0.05000681 -0.46166933]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.2581182   0.05973899]]. Action = [[-0.20887282 -0.02481171  0.3913467  -0.8237438 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.2583267   0.05963348]]. Action = [[-0.21856457  0.43028724 -0.29501298 -0.17587477]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 345. State = [[-0.25816894  0.05977289]]. Action = [[ 0.42753315  0.11073518 -0.32862705  0.27861702]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 346. State = [[-0.25824913  0.05987012]]. Action = [[-0.13133854  0.32896197 -0.07452267 -0.5396965 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.2582686   0.05979515]]. Action = [[-0.14636245 -0.02435273 -0.20603722  0.24769306]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.2582473   0.05981258]]. Action = [[ 0.21072888  0.12508583 -0.07826582 -0.89442986]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 349. State = [[-0.25827476  0.05980973]]. Action = [[ 0.4649142   0.12224728 -0.4618702   0.4557073 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 350. State = [[-0.2582473   0.05981258]]. Action = [[0.34927082 0.33441162 0.0035134  0.8963561 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.2582473   0.05981258]]. Action = [[ 0.25289017  0.49057496  0.05250746 -0.37897587]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.2582905   0.05990516]]. Action = [[ 0.00581264  0.18604189 -0.47931835  0.18042111]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 353. State = [[-0.2583335  0.0599973]]. Action = [[-0.18880552 -0.02953634  0.16726029  0.9367144 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.25835708  0.06010135]]. Action = [[-0.39966503  0.11689138  0.28362006 -0.14168084]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.25837675  0.06008987]]. Action = [[-0.45486352 -0.27194443  0.05617452  0.9563582 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.25842783  0.06019105]]. Action = [[0.43791223 0.43100536 0.0619508  0.08370137]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 357. State = [[-0.25839847  0.06013637]]. Action = [[0.37863064 0.0888626  0.3575586  0.8606627 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 358. State = [[-0.25839847  0.06013637]]. Action = [[-0.0468286  -0.2645018  -0.04088706 -0.268394  ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.25839847  0.06013637]]. Action = [[-0.03220409  0.24098086  0.3008225  -0.11331952]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.25839847  0.06013637]]. Action = [[-0.33790365  0.12044072 -0.26559272  0.3451426 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 361. State = [[-0.25839847  0.06013637]]. Action = [[ 0.0124824  -0.3105901   0.08819813 -0.9030804 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.25839847  0.06013637]]. Action = [[ 0.00176358 -0.47792584 -0.15626353 -0.40345287]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 363. State = [[-0.25839847  0.06013637]]. Action = [[-0.09725699 -0.18428409 -0.12574202  0.55556107]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.25829726  0.06004656]]. Action = [[ 0.02515811 -0.0996967  -0.20562845  0.8025689 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 365. State = [[-0.25757545  0.05950051]]. Action = [[ 0.03127617  0.08566278  0.23131764 -0.44385475]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 366. State = [[-0.25665712  0.06026323]]. Action = [[-0.07221854  0.09820479 -0.06957245  0.76997805]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 367. State = [[-0.257284    0.06136371]]. Action = [[-0.45834708  0.45683855 -0.36744434  0.3649075 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.25742885  0.06237724]]. Action = [[ 0.42765433 -0.33215722  0.30141437 -0.20571423]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 369. State = [[-0.25738308  0.06279437]]. Action = [[-0.32481036  0.1565522   0.10137075  0.8680972 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.25745603  0.06294465]]. Action = [[ 0.1333304   0.18940926 -0.37800324  0.10816813]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 371. State = [[-0.25804392  0.06379601]]. Action = [[ 0.48635173  0.2770356   0.17039073 -0.7524066 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 372. State = [[-0.25789994  0.06434549]]. Action = [[ 0.32082546  0.38469088 -0.24148113  0.24213886]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 373. State = [[-0.2577301   0.06454556]]. Action = [[-0.1915904  -0.40253246  0.49367    -0.19385469]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.25818387  0.06491508]]. Action = [[-0.4103794  -0.1294289  -0.41466865 -0.78038466]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.2581806   0.06532485]]. Action = [[ 0.10607272  0.4131478  -0.13417417  0.10837579]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.25787553  0.06548073]]. Action = [[-0.00888109 -0.3188845   0.10422152  0.5361372 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 377. State = [[-0.25819838  0.06571499]]. Action = [[-0.16398904  0.24887288 -0.18537328 -0.63598967]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.25827846  0.06598222]]. Action = [[-0.26935413 -0.4667159  -0.3684149  -0.7046874 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 379. State = [[-0.2581539   0.06600875]]. Action = [[-0.08785775 -0.4181737   0.15152091  0.74346495]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 380. State = [[-0.25828126  0.06618392]]. Action = [[0.36728692 0.27569175 0.41144365 0.54843473]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.25828522  0.06623176]]. Action = [[0.43131346 0.3906203  0.4849847  0.6528716 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 382. State = [[-0.2583034   0.06622999]]. Action = [[-0.36422265 -0.17878515  0.19978273  0.76988864]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 383. State = [[-0.25832313  0.06638337]]. Action = [[ 0.16421336  0.08454496  0.40800422 -0.3280638 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 384. State = [[-0.25832048  0.06632559]]. Action = [[-0.25802827  0.01517469 -0.31641573  0.00787294]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.25832048  0.06632559]]. Action = [[-0.42613742 -0.18726087 -0.06669301  0.8935412 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 386. State = [[-0.2583453   0.06642944]]. Action = [[ 0.41628343  0.3131392  -0.35285488  0.15574777]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.2583506   0.06638034]]. Action = [[-0.20647407 -0.08970782  0.46303797 -0.83785796]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.2584395   0.06656502]]. Action = [[-3.0147204e-01  6.6161156e-06  4.2884022e-01  9.4941986e-01]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.25841835  0.06658252]]. Action = [[ 0.42164755  0.159805   -0.04107088  0.11036992]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.25846297  0.06667507]]. Action = [[-0.06099427 -0.09727401 -0.36252835 -0.68737656]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.25839078  0.06642091]]. Action = [[ 0.15791559  0.13968682 -0.48876023 -0.81441647]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 392. State = [[-0.25829133  0.06616973]]. Action = [[-0.2616012  -0.28474724  0.21936172  0.4343344 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.25824684  0.06607715]]. Action = [[ 0.11559105 -0.21517986  0.05225611 -0.9071138 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 394. State = [[-0.25827152  0.06601285]]. Action = [[-0.37188536 -0.17494696  0.3797984  -0.84211165]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.25833812  0.0661515 ]]. Action = [[-0.2153846  -0.02826369 -0.2785359  -0.90551305]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 396. State = [[-0.25833288  0.06603634]]. Action = [[-0.15372387 -0.15076664 -0.14182016 -0.83463895]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.25826094  0.06594676]]. Action = [[-0.39819038 -0.44240543 -0.44343892  0.07405865]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 398. State = [[-0.25831074  0.06599027]]. Action = [[ 0.21623182  0.366848   -0.34890983  0.57483387]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.25828066  0.06593549]]. Action = [[ 0.04054391 -0.337863   -0.28921235  0.17626286]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 400. State = [[-0.2582806   0.06577042]]. Action = [[-0.43461072  0.08265984  0.16080976 -0.35043836]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.25825053  0.06571563]]. Action = [[ 0.34971553 -0.36215547  0.3610515  -0.70088416]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.25830033  0.06575916]]. Action = [[ 0.16005945  0.48671752  0.36300397 -0.26074338]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.25827286  0.06576214]]. Action = [[ 0.13772482  0.45035815  0.27554458 -0.45783097]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 404. State = [[-0.25829774  0.06570177]]. Action = [[-0.3375997  -0.04242963 -0.41891944  0.978889  ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.25829262  0.06558622]]. Action = [[ 0.2532673   0.19907427  0.48091185 -0.68636525]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.25829262  0.06558622]]. Action = [[-0.18900087 -0.0607262   0.41353786  0.43831444]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.25829262  0.06558622]]. Action = [[-0.32523793 -0.0925214  -0.36585394 -0.26541913]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.25829005  0.06552844]]. Action = [[-0.4506189   0.2870093  -0.15994781  0.8822274 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.25829005  0.06552844]]. Action = [[-0.44746372  0.14221066 -0.28572023  0.34577847]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 410. State = [[-0.25829005  0.06552844]]. Action = [[ 0.13643456  0.3005445  -0.21059698  0.12467253]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.25826257  0.0655314 ]]. Action = [[ 0.3770268   0.08876032 -0.24577975  0.36304975]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 412. State = [[-0.25826257  0.0655314 ]]. Action = [[ 0.49102622 -0.13907117  0.27494287 -0.50147426]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.2582875   0.06547066]]. Action = [[ 0.43633926 -0.4011942   0.13094759 -0.8024044 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.25826266  0.06536637]]. Action = [[-0.45538393 -0.08983892 -0.37464213 -0.0186792 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.2582352   0.06536932]]. Action = [[-0.34257734  0.37677372 -0.4857916  -0.62749785]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.25828493  0.06541288]]. Action = [[ 0.30009484 -0.4573288  -0.16813532 -0.81825477]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.25828493  0.06541288]]. Action = [[ 0.34229875  0.28510755  0.01879406 -0.9209597 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.2582824   0.06535509]]. Action = [[-0.22728899 -0.4624796  -0.30808488 -0.77740103]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.25828493  0.06541288]]. Action = [[ 0.39606428  0.27494746 -0.0630551  -0.41615534]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.25825492  0.06535805]]. Action = [[-0.22463971 -0.17700833 -0.11665815 -0.47644484]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 421. State = [[-0.25825492  0.06535805]]. Action = [[-0.09259897 -0.46659088  0.27226877 -0.7114877 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.2582824   0.06535509]]. Action = [[ 0.41272277 -0.12097499  0.10738492  0.70474195]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.25825492  0.06535805]]. Action = [[ 0.03607517 -0.41982022 -0.39917004 -0.20916581]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of -1
Current timestep = 424. State = [[-0.2582824   0.06535509]]. Action = [[ 0.456299    0.11313289  0.0341633  -0.46345174]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 425. State = [[-0.25825492  0.06535805]]. Action = [[-0.35632864 -0.04407027 -0.1243037  -0.83142745]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.2582824   0.06535509]]. Action = [[-0.36409667  0.35516876 -0.4032299   0.8527603 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.2582824   0.06535509]]. Action = [[-0.47544974 -0.06811315  0.323277    0.12012911]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.2582824   0.06535509]]. Action = [[0.1649639  0.01075763 0.12536836 0.44977033]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 429. State = [[-0.2582824   0.06535509]]. Action = [[ 0.48480195  0.18769073 -0.277333    0.1451881 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.2582824   0.06535509]]. Action = [[ 0.20587134 -0.14051408  0.2700659   0.28160012]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of -1
Current timestep = 431. State = [[-0.2582824   0.06535509]]. Action = [[-0.28192765 -0.26889443  0.01113522 -0.6792902 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 432. State = [[-0.2582824   0.06535509]]. Action = [[0.0979327  0.41615516 0.4355185  0.07008457]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.2582824   0.06535509]]. Action = [[ 0.42393112  0.4624722  -0.31543761 -0.1672988 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.2582824   0.06535509]]. Action = [[ 0.2301445   0.3165872  -0.07988262 -0.0959968 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.2582824   0.06535509]]. Action = [[-0.4232927  -0.10937601 -0.38756612  0.7899251 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.2582824   0.06535509]]. Action = [[0.35316378 0.14250225 0.3629775  0.37453473]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.2582824   0.06535509]]. Action = [[ 0.44222224  0.27449238 -0.46375114  0.24809659]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.2582824   0.06535509]]. Action = [[-0.30061746  0.46766406 -0.06732887 -0.90831053]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.2582824   0.06535509]]. Action = [[-0.42714235  0.38025272 -0.09215075  0.07377422]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.2582824   0.06535509]]. Action = [[-0.35595983  0.13094515 -0.16184944  0.38679302]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 441. State = [[-0.2582824   0.06535509]]. Action = [[ 0.15568137  0.4416654   0.3833015  -0.27186918]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.2582824   0.06535509]]. Action = [[-0.3380375   0.19527972 -0.22962126  0.03099453]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.25825796  0.06508269]]. Action = [[-0.10277867 -0.08582553 -0.04808861  0.45996523]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.2582426   0.06428146]]. Action = [[ 0.13846135  0.06042755 -0.21586382 -0.28174925]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 445. State = [[-0.25817192  0.06372213]]. Action = [[-0.37964168  0.40859556  0.21173298  0.9177232 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 446. State = [[-0.2581385   0.06330188]]. Action = [[ 0.10425687 -0.25754243 -0.09319964 -0.44241083]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 447. State = [[-0.25825453  0.06302889]]. Action = [[0.11481214 0.21491653 0.28893292 0.7199936 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 448. State = [[-0.25821304  0.06246319]]. Action = [[ 0.2921505   0.3930397   0.14732665 -0.11845684]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.25817645  0.06201304]]. Action = [[0.3848499  0.13795888 0.12162173 0.71315193]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 450. State = [[-0.25824896  0.06179371]]. Action = [[-0.16136086 -0.16584301 -0.42299286 -0.92598253]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 451. State = [[-0.25822115  0.06157469]]. Action = [[ 0.16375715 -0.15023977 -0.16733313 -0.25711662]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 452. State = [[-0.25819123  0.06129704]]. Action = [[ 0.3142807   0.12532043 -0.40009487 -0.24427396]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.25818318  0.06106278]]. Action = [[-0.45597845  0.38271993 -0.0988785   0.4119599 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.25820303  0.06105152]]. Action = [[-0.08903059 -0.45433018 -0.32884932  0.572044  ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.25819525  0.06082078]]. Action = [[ 0.30312562  0.08213288 -0.19401222  0.5765419 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 456. State = [[-0.2582664   0.06076229]]. Action = [[-0.06280872  0.10544211  0.11014044 -0.23110455]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.2585804   0.06126951]]. Action = [[ 0.20440733  0.2398724  -0.37974468  0.8530562 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 458. State = [[-0.25971878  0.06334785]]. Action = [[ 0.00770533  0.0462684  -0.28255498  0.7555225 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.26014477  0.06437828]]. Action = [[ 0.4085083   0.4116488  -0.32576597 -0.10918856]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 460. State = [[-0.26029927  0.06470071]]. Action = [[-0.27898183 -0.34020042  0.05081207  0.47511995]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.2606717   0.06527871]]. Action = [[-0.44694006 -0.18555278  0.01770556 -0.63033986]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 462. State = [[-0.2609822   0.06592268]]. Action = [[-0.28868854 -0.46691325 -0.21409497  0.6129321 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 463. State = [[-0.2612069   0.06643956]]. Action = [[-0.19395489  0.17803329  0.38306963  0.337808  ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 464. State = [[-0.2613812   0.06675951]]. Action = [[ 0.3943777   0.1930964  -0.43130088 -0.36778295]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 465. State = [[-0.261545    0.06708532]]. Action = [[ 0.47831202 -0.0780336   0.3845312  -0.2687111 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.2617671   0.06748051]]. Action = [[ 0.49236715 -0.1525166  -0.17651707  0.86317945]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.26190642  0.0677068 ]]. Action = [[-0.15067366  0.18461746 -0.08963573 -0.05012262]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 468. State = [[-0.2619461   0.06784746]]. Action = [[-0.01685002 -0.3161801   0.07418609 -0.8706451 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.26201636  0.06804312]]. Action = [[0.2933014  0.20986056 0.42887723 0.03744125]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.2621002   0.06811087]]. Action = [[ 0.38045108  0.2157954  -0.04274577 -0.36238438]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 471. State = [[-0.26224062  0.0685012 ]]. Action = [[ 0.03887969 -0.29929364 -0.25478408 -0.31051302]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.26224062  0.0685012 ]]. Action = [[ 0.34996802  0.0316568  -0.36262116  0.9478848 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.26228267  0.06853526]]. Action = [[-0.38777697 -0.05299422 -0.13734186 -0.7229257 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.262308    0.06863863]]. Action = [[ 0.3979783  -0.06255144 -0.13159567  0.554312  ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 475. State = [[-0.26233062  0.06868472]]. Action = [[ 0.45193875  0.43827444 -0.03049397  0.9343264 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 476. State = [[-0.26232767  0.06862701]]. Action = [[-0.08653855 -0.39732647 -0.4588559  -0.34339303]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.2624646   0.06879524]]. Action = [[-0.02327594 -0.10045457 -0.17290038 -0.5864783 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 478. State = [[-0.26236862  0.0682419 ]]. Action = [[ 0.10151529 -0.15245444 -0.29965085 -0.23729503]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 479. State = [[-0.2623575   0.06660112]]. Action = [[0.0021261  0.0005002  0.1992799  0.23406792]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 480. State = [[-0.26231307  0.06603041]]. Action = [[ 0.00681186  0.21440458 -0.24352807  0.54295087]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.2622258   0.06559575]]. Action = [[-0.33397406  0.15061414 -0.1768083   0.9508096 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.26228502  0.06539632]]. Action = [[ 0.4138704   0.19113773 -0.1526461  -0.593383  ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 483. State = [[-0.26230946  0.06502675]]. Action = [[ 0.47125602  0.44994903 -0.46224973 -0.89661187]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 484. State = [[-0.2622725   0.06463122]]. Action = [[-0.35616454 -0.18029106 -0.40880966  0.15263557]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[-0.262193    0.06389861]]. Action = [[-0.09164342 -0.0221099  -0.31660256  0.9644915 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 486. State = [[-0.2624426  0.0636358]]. Action = [[ 0.20201468 -0.11858168  0.20699209  0.48842108]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.2629629   0.06339752]]. Action = [[-0.4397172  -0.32362184 -0.00959685  0.9671767 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 488. State = [[-0.26313198  0.06329957]]. Action = [[ 0.44005948 -0.23898745  0.02446687  0.7148062 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 489. State = [[-0.26333743  0.06304397]]. Action = [[ 0.05321187 -0.24463037  0.21681702  0.61856973]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(9.6833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 490. State = [[-0.26374587  0.06277693]]. Action = [[ 0.13795543  0.32713437 -0.30414373 -0.68044734]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(7.5774e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.26406464  0.06259352]]. Action = [[ 0.39366305  0.3296079  -0.17737746  0.8909017 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.26412076  0.06251848]]. Action = [[-0.3490004  -0.41009215 -0.46330786  0.54543495]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 493. State = [[-0.26434192  0.062376  ]]. Action = [[-0.41281992 -0.25157544  0.11784297  0.58922577]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.2645748   0.06223136]]. Action = [[ 0.10725582 -0.19318917  0.08741039  0.26258373]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[-0.2647276   0.06218456]]. Action = [[ 0.28026795  0.3392638   0.22270453 -0.9662637 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.2647572   0.06209442]]. Action = [[-0.2991505  -0.07658398 -0.3329653   0.08350325]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.26481053  0.06195086]]. Action = [[-0.37056968 -0.31901     0.40715325 -0.62735695]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.2648537   0.06197422]]. Action = [[ 0.2488221   0.35878396 -0.00039819  0.32868838]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.2649864   0.06193954]]. Action = [[-0.07494932  0.42897344 -0.35542667  0.6932869 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.2650259   0.06191577]]. Action = [[-0.4646048  -0.31012288 -0.07684848  0.00400341]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 501. State = [[-0.26504558  0.06190366]]. Action = [[-0.45790824  0.3079995   0.48866397  0.32261586]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.26508218  0.06184078]]. Action = [[-0.18214518 -0.26060385 -0.06278881  0.9717783 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 503. State = [[-0.26516443  0.06177422]]. Action = [[ 0.03693867  0.19782633 -0.20009175 -0.03607845]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.26530036  0.06193082]]. Action = [[ 0.39104593 -0.00451347 -0.15110034  0.9161289 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.26538575  0.06173227]]. Action = [[-0.09798273  0.31126654  0.37727654  0.23214114]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.26536646  0.0618072 ]]. Action = [[-0.39651406  0.3726058   0.4357667   0.85172987]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.2654101   0.06189832]]. Action = [[ 0.42707205  0.4013356  -0.37846762  0.03223395]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 508. State = [[-0.26540804  0.06184085]]. Action = [[ 0.47550446  0.15141016 -0.21731788  0.76928973]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 509. State = [[-0.265404    0.06172632]]. Action = [[0.3383031  0.02945298 0.20625877 0.0577662 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 510. State = [[-0.26547018  0.06171814]]. Action = [[0.36495316 0.38302058 0.37472773 0.22043979]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.26547018  0.06171814]]. Action = [[ 0.49411118 -0.44193408 -0.29994157  0.2051121 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 512. State = [[-0.26547018  0.06171814]]. Action = [[-0.44790724 -0.3943612  -0.41039214 -0.3493955 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.26547018  0.06171814]]. Action = [[ 0.35202837 -0.20709735  0.15641975 -0.10057163]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 514. State = [[-0.26547018  0.06171814]]. Action = [[ 0.03576148  0.22113371  0.18438601 -0.03865635]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.26547018  0.06171814]]. Action = [[-0.47614822 -0.33801886 -0.35130218 -0.3183267 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.26547018  0.06171814]]. Action = [[-0.34439573  0.07032514  0.4774351   0.10027456]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 517. State = [[-0.26547018  0.06171814]]. Action = [[-0.42927918 -0.48648262 -0.26299903  0.6943686 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.26547018  0.06171814]]. Action = [[-0.3550482  -0.42653164  0.01203299  0.04602993]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.26547018  0.06171814]]. Action = [[ 0.44646513  0.27190912 -0.41197127 -0.16277462]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 520. State = [[-0.26560253  0.06170179]]. Action = [[ 0.42284638  0.17682493  0.01520073 -0.32117045]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 521. State = [[-0.26560053  0.06164435]]. Action = [[ 0.29567623  0.06441516 -0.03065148  0.25415266]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 522. State = [[-0.26557505  0.06170456]]. Action = [[-0.33336848 -0.30090496  0.19192988  0.5916791 ]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 522 of -1
Current timestep = 523. State = [[-0.26568803  0.06161872]]. Action = [[ 0.09357816 -0.3592989  -0.11356077 -0.48733306]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 524. State = [[-0.26560053  0.06164435]]. Action = [[0.34971488 0.24643284 0.06722105 0.90948117]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 525. State = [[-0.26560053  0.06164435]]. Action = [[-0.3215072  -0.4677346   0.402763    0.32060695]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 526. State = [[-0.26568803  0.06161872]]. Action = [[ 0.42728794 -0.29242957  0.4151411  -0.5293969 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 526 of -1
Current timestep = 527. State = [[-0.26566672  0.06163618]]. Action = [[ 0.46631265  0.31867933  0.28947365 -0.12631512]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.26566672  0.06163618]]. Action = [[ 0.40093374 -0.17852777 -0.492955   -0.94494873]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.26566672  0.06163618]]. Action = [[-0.03453454 -0.36145204  0.3442769   0.9929817 ]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.26566672  0.06163618]]. Action = [[-0.387854    0.01566112  0.17596757  0.58497584]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.26566672  0.06163618]]. Action = [[-0.30783215 -0.10185623  0.05303061  0.8658886 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(9.6828e-05, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.26566672  0.06163618]]. Action = [[-0.19615632 -0.41559508  0.09183729  0.7440158 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.26566672  0.06163618]]. Action = [[-0.1466589   0.19949865 -0.14208186 -0.77657294]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(7.6838e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.26566672  0.06163618]]. Action = [[ 0.3987739  -0.45884013 -0.47121677  0.76263666]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 535. State = [[-0.26566672  0.06163618]]. Action = [[ 0.21013087 -0.18825656  0.49354362 -0.80843854]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.26566672  0.06163618]]. Action = [[ 0.03368795 -0.24154323 -0.38038906 -0.30180216]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.26566672  0.06163618]]. Action = [[ 0.4180708  -0.45433092 -0.2522351  -0.04964101]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of -1
Current timestep = 538. State = [[-0.26566672  0.06163618]]. Action = [[ 0.48720837 -0.35822752  0.26953775 -0.76876265]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 539. State = [[-0.26566672  0.06163618]]. Action = [[ 0.40924025  0.40699482  0.20724237 -0.6419314 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(7.4876e-05, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.26566672  0.06163618]]. Action = [[ 0.44490916  0.48299152  0.01519758 -0.6152701 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(5.4467e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 540 of -1
Current timestep = 541. State = [[-0.26566672  0.06163618]]. Action = [[-0.46672338  0.06186074  0.00596493 -0.468426  ]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.26576075  0.06176369]]. Action = [[-0.11483639  0.03745079  0.16536504  0.9019859 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(5.9184e-05, grad_fn=<MseLossBackward0>)
Current timestep = 543. State = [[-0.2662322   0.06213439]]. Action = [[-0.49099058  0.15263009  0.3544172   0.02087486]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.26671278  0.06250304]]. Action = [[ 0.17106354 -0.06570619 -0.06221661  0.72272253]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 545. State = [[-0.26732627  0.06294087]]. Action = [[-0.10133913 -0.4372328  -0.18680468  0.8751036 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 546. State = [[-0.267636    0.06297027]]. Action = [[-0.4806535  -0.19506246  0.17648745  0.46083283]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.26817152  0.06337003]]. Action = [[ 0.33866084 -0.40424958 -0.46081522 -0.5079778 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.26851887  0.06369015]]. Action = [[ 0.13667262 -0.360607    0.29777652 -0.8245296 ]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(6.6866e-05, grad_fn=<MseLossBackward0>)
Current timestep = 549. State = [[-0.26910207  0.06390359]]. Action = [[-0.16258448 -0.04819548  0.06462097  0.5554919 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(3.8526e-05, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.26951194  0.06406253]]. Action = [[-0.36123377 -0.0511865  -0.30111128 -0.6291469 ]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(9.2470e-05, grad_fn=<MseLossBackward0>)
Current timestep = 551. State = [[-0.2698664   0.06412635]]. Action = [[-0.11437771 -0.48034003  0.27202868  0.49177384]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 552. State = [[-0.27013764  0.06440315]]. Action = [[-0.28883466 -0.1400995   0.35854876  0.75921893]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(8.9258e-05, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.2705324   0.06443797]]. Action = [[-0.45826596  0.17148358 -0.4709747   0.47881305]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 554. State = [[-0.2709786   0.06431299]]. Action = [[ 0.21416634 -0.44998708  0.09133089  0.34269238]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(9.8019e-05, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.27135637  0.06441656]]. Action = [[ 0.05708647  0.44978988 -0.10333648 -0.2049464 ]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 556. State = [[-0.2717928   0.06450181]]. Action = [[-0.41706344  0.17265719  0.43726617 -0.85374045]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.2722416   0.06437131]]. Action = [[ 0.04803371 -0.2875875  -0.09567654  0.595706  ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(8.2648e-05, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.27241912  0.06436284]]. Action = [[-0.16331905 -0.07987937 -0.31582752  0.56926095]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(8.9596e-05, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.27259094  0.06433962]]. Action = [[ 0.23276359 -0.16357869  0.3337019  -0.86449164]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 560. State = [[-0.27297157  0.06427541]]. Action = [[ 0.38699472 -0.22719038 -0.28839335  0.84548926]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.2732179   0.06424379]]. Action = [[-0.282427    0.12973797 -0.32304978  0.19466186]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 562. State = [[-0.27330488  0.06424828]]. Action = [[ 0.32970202 -0.20379621 -0.00626978  0.24002981]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.2739599   0.06416264]]. Action = [[0.11551511 0.03706145 0.25901473 0.06691134]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 564. State = [[-0.27402598  0.06415397]]. Action = [[ 0.30788106  0.0506084  -0.3160366  -0.8372784 ]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 565. State = [[-0.27391097  0.06418334]]. Action = [[ 0.2082845  -0.46457577 -0.10414585 -0.31558335]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.27384445  0.06419207]]. Action = [[ 0.48371732  0.39086902  0.05619156 -0.02331334]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.27402598  0.06415397]]. Action = [[0.41253465 0.49648607 0.15103585 0.9174501 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 568. State = [[-0.27393845  0.06418037]]. Action = [[ 0.4327585   0.10490632 -0.45981145 -0.56233   ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.27391097  0.06418334]]. Action = [[-0.38145587  0.44954634  0.32629144  0.3874271 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.27402598  0.06415397]]. Action = [[-0.49208194 -0.3475855   0.02969599  0.9670179 ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.27391097  0.06418334]]. Action = [[ 0.07653064 -0.287506   -0.16836923 -0.7513583 ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(4.3805e-05, grad_fn=<MseLossBackward0>)
Current timestep = 572. State = [[-0.27391097  0.06418334]]. Action = [[ 0.3766278   0.25996912 -0.43400413 -0.93385655]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 573. State = [[-0.27393845  0.06418037]]. Action = [[-0.49706182 -0.24284416 -0.30915937 -0.44921118]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.27391097  0.06418334]]. Action = [[-0.22636503  0.23494089  0.03257281 -0.7870661 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(9.8226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 575. State = [[-0.27393845  0.06418037]]. Action = [[ 0.04036409  0.4133191   0.0747419  -0.03100234]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(0.0001, grad_fn=<MseLossBackward0>)
