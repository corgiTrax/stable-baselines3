Current timestep = 0. State = [[-0.25791246  0.00909695]]. Action = [[ 0.05924907 -0.07895854  0.10003346  0.69161487]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0418, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25611892  0.00669194]]. Action = [[-0.23176551  0.16159177 -0.17457157 -0.90834266]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0203, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.2578065   0.00901536]]. Action = [[ 0.10777026 -0.17336698  0.23816895 -0.82670105]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.2580155  0.0057519]]. Action = [[-0.15481004 -0.21615396 -0.09073761 -0.94270116]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2602584  -0.00142151]]. Action = [[-0.22329102 -0.07974368 -0.0163338  -0.9672671 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.2658179  -0.00745479]]. Action = [[-0.1562309   0.22389305 -0.17906356 -0.81840515]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.27286604 -0.00663791]]. Action = [[-0.22811669 -0.23533046 -0.01784825 -0.9420009 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.28156918 -0.01180353]]. Action = [[ 0.24833518 -0.15900283  0.10211319 -0.44662964]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.2857716  -0.01936572]]. Action = [[-0.12114502 -0.00839999  0.22881788  0.68654716]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.2897768  -0.02415531]]. Action = [[0.11081654 0.22057694 0.17682138 0.39465654]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.2909056  -0.02176391]]. Action = [[-0.11415258 -0.11471838 -0.12570253 -0.86252207]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.29239076 -0.02290568]]. Action = [[ 0.09048593  0.00989583 -0.18702753 -0.26439488]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.29236948 -0.02326943]]. Action = [[ 0.12477124 -0.05174313  0.24319714 -0.14956695]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.29208636 -0.02443396]]. Action = [[-0.15795825 -0.07624465 -0.11193106 -0.10317028]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.29280066 -0.02771554]]. Action = [[-0.19224572 -0.08825207  0.19586608 -0.14874756]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.29681012 -0.03265611]]. Action = [[-0.22864102 -0.14797121 -0.2279381  -0.09052092]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.30478156 -0.04038483]]. Action = [[ 0.02617097  0.04792094 -0.15891035 -0.688313  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.30901796 -0.04387618]]. Action = [[ 0.17050242  0.15207493  0.24376023 -0.94917595]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.30921894 -0.04227071]]. Action = [[ 0.09070167 -0.07585645  0.18278232  0.5608492 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.30857912 -0.04246509]]. Action = [[ 0.19125229 -0.04383661 -0.21262155  0.7878113 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.30420503 -0.04327862]]. Action = [[ 0.19261521 -0.02088733 -0.20087093 -0.00515866]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.2973874  -0.04393052]]. Action = [[0.16782728 0.06825501 0.2391628  0.69502115]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.29068527 -0.04405492]]. Action = [[-0.10524267 -0.20487665  0.09233847  0.8419082 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.2883196  -0.04768186]]. Action = [[ 0.01997206  0.04912406 -0.03010666 -0.9012147 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.28644395 -0.04946773]]. Action = [[-0.14814807 -0.13888377 -0.01728109  0.38740635]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.28709963 -0.0544961 ]]. Action = [[-0.11515613 -0.17900194  0.15895686  0.6670451 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.28979632 -0.06354517]]. Action = [[-0.18338375 -0.17412424 -0.03437841 -0.28067625]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.29513165 -0.0746676 ]]. Action = [[-0.11380254 -0.13441662 -0.17189443  0.6870606 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.30089062 -0.08475235]]. Action = [[-0.19228688  0.08176795  0.09751767  0.9643948 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.3082195  -0.09053152]]. Action = [[-0.02098976 -0.06783569 -0.198336   -0.18027318]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.3132463  -0.09540568]]. Action = [[-0.13408801  0.02527642  0.2012695   0.4501592 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.3209682  -0.09817981]]. Action = [[ 0.00106803 -0.1645245  -0.10505539 -0.9655238 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.3256543  -0.10251248]]. Action = [[0.1417203  0.20115542 0.2022869  0.66334414]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.32511094 -0.10136973]]. Action = [[-0.06624271 -0.17013435 -0.07466033 -0.50499696]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.32516024 -0.10406215]]. Action = [[-0.05622214 -0.18693052  0.0125519  -0.16869074]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.32642862 -0.10971256]]. Action = [[0.06404155 0.20495594 0.1826033  0.631799  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of -1
Current timestep = 36. State = [[-0.32653224 -0.10937981]]. Action = [[ 0.18012023 -0.16588281  0.02481532 -0.7841305 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.32455918 -0.11142279]]. Action = [[ 0.22890699  0.16636515 -0.10121189 -0.42886513]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.31957906 -0.11095495]]. Action = [[-0.04849237 -0.2156536  -0.11597189 -0.68736666]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.3166915  -0.11471237]]. Action = [[ 0.13183743 -0.01303667 -0.09758687  0.29617107]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.3137994 -0.1177338]]. Action = [[ 0.08326745 -0.17111097 -0.1221637   0.4750135 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.3093561  -0.12465888]]. Action = [[-0.05276017 -0.14354526  0.24568051 -0.23454565]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.30730462 -0.1324967 ]]. Action = [[-0.13436024 -0.01271136 -0.09807888 -0.65616924]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.3071318  -0.13876015]]. Action = [[ 0.09187359  0.15068012 -0.15441726 -0.78243685]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 44. State = [[-0.30688474 -0.13842882]]. Action = [[-0.01104595  0.18039966  0.0504064  -0.86055166]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.30680406 -0.13478842]]. Action = [[-0.15234701  0.04439515 -0.24266542 -0.49183762]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.30793104 -0.13104698]]. Action = [[-0.15932368  0.13239068  0.14953521  0.23094785]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.31162587 -0.12514521]]. Action = [[-0.13764115  0.02563474 -0.07341918  0.6804776 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 48. State = [[-0.31551903 -0.12110744]]. Action = [[ 0.14682373 -0.14430912 -0.19883493  0.5890666 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.31578794 -0.12167419]]. Action = [[-0.2020694  -0.05465925 -0.24025533 -0.7231821 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 50. State = [[-0.31818175 -0.12371379]]. Action = [[-0.05228662 -0.02809545  0.17043954 -0.6189266 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.3208617  -0.12594788]]. Action = [[-0.23415163 -0.19806305  0.08935583  0.49431598]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, True, False, False]
State prediction error at timestep 51 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.3261215  -0.13225117]]. Action = [[ 0.1547091  -0.21612495 -0.22361672 -0.30974293]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, True, False, False]
State prediction error at timestep 52 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.327804   -0.14154425]]. Action = [[0.14868665 0.13383168 0.19083744 0.78678274]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.32668278 -0.14380847]]. Action = [[ 0.05638915 -0.21819283  0.14367032 -0.04854298]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.3254028  -0.15037964]]. Action = [[-0.07682401  0.23582801  0.00689197  0.00130498]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.32517514 -0.14940302]]. Action = [[0.22881836 0.13839537 0.22635847 0.56890917]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.32283568 -0.14595975]]. Action = [[-0.06525825 -0.13620536  0.08213246 -0.90897083]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.32232085 -0.14701918]]. Action = [[ 0.10766584 -0.19407946 -0.17138737  0.9152427 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.32025927 -0.15142076]]. Action = [[ 0.00594321 -0.04564694 -0.02992007  0.90658474]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 60. State = [[-0.31828013 -0.15568376]]. Action = [[0.20861489 0.0559321  0.00510693 0.27567327]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.31480598 -0.1570635 ]]. Action = [[-0.17488211 -0.13545491  0.0826036  -0.10072684]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.31443992 -0.16149254]]. Action = [[ 0.22777018 -0.15560105 -0.01933053  0.39030683]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.31166193 -0.16775794]]. Action = [[-0.1340217  -0.06968763 -0.20318483  0.868593  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.31155932 -0.17481959]]. Action = [[-0.05871648 -0.07265085  0.20290944 -0.5450313 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.3119334  -0.18090959]]. Action = [[ 0.13567841 -0.00473578 -0.23814486 -0.6242135 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.3111105 -0.1851998]]. Action = [[-0.17551778 -0.09255275 -0.23441492 -0.8104401 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.31234226 -0.19066763]]. Action = [[-0.21133086 -0.01681191 -0.22182947 -0.36673045]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.31583095 -0.19508612]]. Action = [[-0.14025089  0.23348695  0.10536349  0.8462151 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.32005903 -0.19307382]]. Action = [[ 0.18971145 -0.04942185  0.1683774   0.16894674]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.31996235 -0.19260558]]. Action = [[ 0.01965541  0.09216833 -0.14084889 -0.19787335]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.31963626 -0.19027735]]. Action = [[-0.2290002   0.18781239 -0.23903334  0.48294342]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.3223265  -0.18430178]]. Action = [[ 0.18844986 -0.04166988 -0.13856797  0.62082624]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.32240394 -0.18097775]]. Action = [[-0.17488652 -0.07382351 -0.07870151  0.07760572]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.323267   -0.18114395]]. Action = [[ 0.14582056 -0.04586792 -0.14267333  0.20024347]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.32305914 -0.18139246]]. Action = [[0.10013565 0.00278464 0.01967877 0.9871017 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.32209364 -0.18206836]]. Action = [[-0.13979122 -0.23918113 -0.06656362  0.56858516]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.32323477 -0.18836999]]. Action = [[ 0.11575061  0.01914227 -0.11322561 -0.02303493]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.32198077 -0.19023935]]. Action = [[ 0.09895703 -0.07689047 -0.106682    0.34237337]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.32049143 -0.19298413]]. Action = [[-0.20548296  0.10387522  0.21896243 -0.33624762]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.32168502 -0.19408241]]. Action = [[ 0.09779871 -0.21981739  0.22735924 -0.79963934]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.32203582 -0.19927369]]. Action = [[ 0.05030203 -0.01392052  0.14406061 -0.4988134 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.32120398 -0.20297144]]. Action = [[0.06428176 0.08485705 0.23067808 0.03964233]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.3200385  -0.20341963]]. Action = [[-0.1194175   0.01607955  0.24085969 -0.64519346]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.3204367  -0.20352715]]. Action = [[-0.22199407  0.1554094  -0.0682441  -0.65687954]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 85. State = [[-0.32370257 -0.20092602]]. Action = [[ 0.04343325 -0.06933294  0.14944392 -0.5256078 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.32425308 -0.20036325]]. Action = [[-0.06160638  0.16857687 -0.17986685  0.17905807]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.3255224  -0.19857898]]. Action = [[-0.1804807   0.17847884 -0.22520383  0.20782757]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.3295396  -0.19558865]]. Action = [[ 0.04766777 -0.01665622  0.20037371 -0.35733843]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.33119687 -0.19366728]]. Action = [[-0.01980148  0.13194394  0.06891006 -0.68367463]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.33146977 -0.19074205]]. Action = [[ 0.17246097 -0.090333   -0.06394637 -0.9382462 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.33087403 -0.188797  ]]. Action = [[-0.20018303 -0.12940864  0.06703705 -0.2201823 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 92. State = [[-0.33167675 -0.18962383]]. Action = [[-0.02282119 -0.0126403   0.1039899   0.89019656]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 93. State = [[-0.3320908 -0.1900323]]. Action = [[ 0.07939675 -0.14921717  0.21549982  0.1912409 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.33239487 -0.19048022]]. Action = [[0.08828652 0.12617072 0.20681357 0.23202562]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.332166   -0.19036008]]. Action = [[-0.17853746 -0.07814333 -0.13108635  0.82514346]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.33313417 -0.1916092 ]]. Action = [[-0.215593   -0.19945532 -0.09084287 -0.7308994 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 97. State = [[-0.33690512 -0.19615872]]. Action = [[ 0.07685739  0.08364314 -0.17901468 -0.66159886]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 98. State = [[-0.33878607 -0.19833213]]. Action = [[-0.21144493  0.21483076 -0.22067833 -0.1840288 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.3431626  -0.19701011]]. Action = [[-0.0963576   0.1418314   0.15449828 -0.5312089 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.34958062 -0.19383143]]. Action = [[-0.17807849 -0.11197075 -0.04445164 -0.48085153]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.35595644 -0.19224147]]. Action = [[ 0.23556358 -0.17131947 -0.17732872  0.61234856]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.35574546 -0.19370925]]. Action = [[-0.11949053  0.01464677  0.12784356 -0.8494756 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of -1
Current timestep = 103. State = [[-0.35700458 -0.19533071]]. Action = [[-0.08381161 -0.13000521 -0.19852097  0.01834309]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.35931364 -0.19837852]]. Action = [[ 0.23715043  0.03174561 -0.20948718 -0.8005927 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.3585147  -0.19912085]]. Action = [[ 0.09873676  0.07236171 -0.22281489 -0.81742936]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.3574983 -0.1994484]]. Action = [[-0.16843596  0.2141569  -0.06974615 -0.03669882]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.35913557 -0.19837049]]. Action = [[ 0.0191038  -0.20626846 -0.2309397   0.22751915]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.35929787 -0.19867644]]. Action = [[-0.02108714  0.04946631  0.18460524  0.9617753 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.35927996 -0.1988087 ]]. Action = [[ 0.0443868  -0.08025093  0.0942862  -0.8330623 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.35940772 -0.19911031]]. Action = [[-0.15864411 -0.18920258 -0.20058589 -0.07742256]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
State prediction error at timestep 110 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.3614938 -0.2024693]]. Action = [[ 0.18282396 -0.01906615  0.15420243 -0.92772293]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.36024618 -0.20396133]]. Action = [[-0.15469818 -0.05401836  0.0374707   0.13310218]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 113. State = [[-0.3614268  -0.20642334]]. Action = [[ 0.24250987 -0.08802576 -0.2184707   0.5345794 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.35827658 -0.20929345]]. Action = [[ 0.11639485 -0.07695782 -0.13220783  0.5395415 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.35446188 -0.21249276]]. Action = [[-0.10434335 -0.10894844 -0.21974726  0.9294325 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
State prediction error at timestep 115 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-0.35408694 -0.21623129]]. Action = [[ 0.16285151  0.07633361 -0.24222235 -0.08884877]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.3514032  -0.21810982]]. Action = [[-0.14836937  0.13945931  0.01867044  0.17176783]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.35144013 -0.21811864]]. Action = [[-0.04659656  0.0576655   0.14008039 -0.12904781]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.3523414  -0.21788363]]. Action = [[ 0.2313515  -0.15284362 -0.13464324  0.00768483]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.3503158 -0.2191113]]. Action = [[0.00439829 0.18000156 0.09327418 0.8521347 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of -1
Current timestep = 121. State = [[-0.3494975  -0.21888085]]. Action = [[ 0.1821818  -0.21856004  0.14959288  0.9601821 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.3454532  -0.22144948]]. Action = [[ 0.06003559 -0.07376768 -0.22135335  0.87457323]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.3421685 -0.2234961]]. Action = [[-0.14658684 -0.05543208  0.0984385  -0.50756335]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.342409   -0.22439645]]. Action = [[ 0.11181632  0.20903397 -0.16297115  0.5513576 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.34170774 -0.22350438]]. Action = [[ 0.19122869  0.2126565  -0.14356609 -0.9304161 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.33831593 -0.21946755]]. Action = [[-0.243332   -0.02347961  0.07021946  0.2406187 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.33891234 -0.2182304 ]]. Action = [[ 0.16195822  0.12554413  0.21585923 -0.76129794]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.33720234 -0.21544571]]. Action = [[0.04653496 0.10025987 0.10158366 0.27731597]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.33540446 -0.21232441]]. Action = [[-0.06709367  0.16942078 -0.00863348  0.65177584]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, True, False, False]
State prediction error at timestep 129 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.33499336 -0.20829812]]. Action = [[ 0.18956643 -0.1636489  -0.15438831 -0.07027042]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, True, False, False]
State prediction error at timestep 130 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.33260828 -0.20600905]]. Action = [[-0.17869979 -0.13282093 -0.05043893 -0.8104904 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.33236608 -0.20610365]]. Action = [[-0.05505756  0.01863691  0.22989625 -0.85130775]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, True, False, False]
State prediction error at timestep 132 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.33268946 -0.2060191 ]]. Action = [[ 0.04506648 -0.15218858  0.18853822 -0.94144136]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.33275118 -0.20676465]]. Action = [[ 0.11345106 -0.19287705  0.03143129 -0.69141173]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.33074766 -0.20936067]]. Action = [[-0.05103384  0.0307292  -0.01438667 -0.60297525]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, True, False, False]
State prediction error at timestep 135 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.33046684 -0.2103643 ]]. Action = [[0.10623831 0.15688616 0.15042698 0.17993748]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.32964587 -0.2102664 ]]. Action = [[ 0.00754854 -0.05357847 -0.08116341 -0.5673884 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, True, False, False]
State prediction error at timestep 137 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.32932466 -0.21033539]]. Action = [[-0.02191542 -0.10179934 -0.12750126  0.77623105]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, True, False, False]
State prediction error at timestep 138 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.32870468 -0.2107902 ]]. Action = [[-0.16477641  0.18850768 -0.19571468 -0.7635013 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.3297738  -0.21025527]]. Action = [[-0.13857533  0.20620209 -0.13270451 -0.6092681 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, True, False, False]
State prediction error at timestep 140 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.3332763  -0.20811862]]. Action = [[ 0.09337407 -0.05062152  0.11117548 -0.53448886]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, True, False, False]
State prediction error at timestep 141 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.33385146 -0.2076489 ]]. Action = [[-0.07268614 -0.10166571 -0.14724836 -0.48877746]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, True, False, False]
State prediction error at timestep 142 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.33470306 -0.20789017]]. Action = [[-0.2358153  -0.1198498   0.01300418  0.24385786]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, True, False, False]
State prediction error at timestep 143 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.33850372 -0.21065558]]. Action = [[-0.04666232 -0.13049056 -0.03301589  0.3588965 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, True, False, False]
State prediction error at timestep 144 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 145. State = [[-0.34223917 -0.21488976]]. Action = [[0.16316032 0.18280238 0.1958285  0.54511786]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.34252098 -0.21491328]]. Action = [[-0.19083488 -0.2177903   0.17057168  0.5702095 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, True, False, False]
State prediction error at timestep 146 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.34454876 -0.21753834]]. Action = [[-0.08013037 -0.09207764 -0.17538649  0.13925397]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, True, False, False]
State prediction error at timestep 147 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 148. State = [[-0.34762168 -0.22125496]]. Action = [[-0.15673727 -0.14006568  0.09757656  0.77867305]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.351938   -0.22624692]]. Action = [[-0.03308173 -0.14108647 -0.09382981  0.6187813 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.35633245 -0.23132083]]. Action = [[0.07511166 0.2260797  0.17867085 0.46288288]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.35740945 -0.2318853 ]]. Action = [[-0.22654988  0.2328276   0.14227277  0.75547755]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 152. State = [[-0.3607211  -0.23008385]]. Action = [[-0.14717454  0.18431818 -0.01800811 -0.81034607]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.36605296 -0.22638898]]. Action = [[ 0.11785927  0.13389093 -0.00650294  0.85237527]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.36945337 -0.22275175]]. Action = [[-0.17870034 -0.14615057 -0.20923226  0.7645848 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.37301278 -0.22209315]]. Action = [[ 0.03574932 -0.14727166  0.03144088  0.77941096]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.37392038 -0.22295892]]. Action = [[ 0.24411976  0.201689    0.22734109 -0.99710345]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, True, False, False]
State prediction error at timestep 156 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.37343448 -0.22173935]]. Action = [[-0.17807929 -0.06641725  0.0827294   0.6024699 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.3739371 -0.2220344]]. Action = [[ 0.10521662  0.14492285  0.03546196 -0.86478037]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(5.9504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.37346917 -0.22120994]]. Action = [[ 0.22164917  0.05632508 -0.01765239 -0.48450255]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, True, False, False]
State prediction error at timestep 159 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.37111717 -0.21871488]]. Action = [[-0.03638265  0.05665937 -0.18061762 -0.79801995]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, True, False, False]
State prediction error at timestep 160 is tensor(7.0676e-05, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.3698008  -0.21692118]]. Action = [[-0.04285467  0.20271754  0.11007506 -0.7829983 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, True, False, False]
State prediction error at timestep 161 is tensor(1.2275e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.3694007  -0.21371248]]. Action = [[-0.20860949 -0.07108393 -0.2435428   0.60739255]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, True, False, False]
State prediction error at timestep 162 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.37217906 -0.21201622]]. Action = [[ 0.10147989  0.14909476 -0.17784956  0.04496586]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, True, False, False]
State prediction error at timestep 163 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.37287784 -0.20910953]]. Action = [[-0.09856164  0.09001336  0.03390929 -0.69314456]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, True, False, False]
State prediction error at timestep 164 is tensor(7.7297e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.37429625 -0.20494403]]. Action = [[ 0.06802395  0.21623772 -0.21382558 -0.53615344]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, True, False, False]
State prediction error at timestep 165 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.3739341  -0.19989526]]. Action = [[-0.18949592 -0.08786872 -0.19963577 -0.46290535]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, True, False, False]
State prediction error at timestep 166 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.37430692 -0.19421315]]. Action = [[ 0.13698238  0.04156935  0.02757192 -0.10837686]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, True, False, False]
State prediction error at timestep 167 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.3736538  -0.18956766]]. Action = [[-0.12369572 -0.00119194 -0.0300657   0.6043675 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.37466544 -0.18519585]]. Action = [[ 0.02589011  0.21825933 -0.16347082 -0.85258967]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, True, False, False]
State prediction error at timestep 169 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.3747861  -0.17921259]]. Action = [[ 0.22124201 -0.1042937  -0.06539492  0.9003091 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, True, False, False]
State prediction error at timestep 170 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.3711518  -0.17572635]]. Action = [[-0.06522876  0.1529578   0.03923312  0.8851111 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, True, False, False]
State prediction error at timestep 171 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.36941293 -0.17222041]]. Action = [[ 0.01910025  0.12648612  0.10691676 -0.95564187]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, True, False, False]
State prediction error at timestep 172 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 173. State = [[-0.3684772  -0.16820924]]. Action = [[ 0.10849845 -0.200357   -0.12779106  0.63988006]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, True, False, False]
State prediction error at timestep 173 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 173 of -1
Current timestep = 174. State = [[-0.36624408 -0.16609293]]. Action = [[ 0.03398582 -0.14168388  0.14241552 -0.962636  ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, True, False, False]
State prediction error at timestep 174 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.36550876 -0.16647804]]. Action = [[-0.22985919 -0.10089585 -0.13289979 -0.92375124]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.3660645 -0.1682078]]. Action = [[ 0.11829555  0.14804858  0.24247909 -0.71151817]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, True, False, False]
State prediction error at timestep 176 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.3656463 -0.1678346]]. Action = [[ 0.17399913 -0.04307689  0.17457455 -0.21197402]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, True, False, False]
State prediction error at timestep 177 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.36407554 -0.16731681]]. Action = [[-0.10168245 -0.16418707 -0.01441452 -0.68112105]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, True, False, False]
State prediction error at timestep 178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.36419135 -0.16852848]]. Action = [[-0.14312015 -0.02564535 -0.21925291  0.5771203 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, True, False, False]
State prediction error at timestep 179 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.36548296 -0.17038015]]. Action = [[-0.21364695  0.13053358 -0.01331764 -0.00623637]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, True, False, False]
State prediction error at timestep 180 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.36885896 -0.17051566]]. Action = [[-0.04783864 -0.16523716  0.01386428  0.05973423]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, True, False, False]
State prediction error at timestep 181 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.37116957 -0.17302115]]. Action = [[-0.069323    0.15289551 -0.10227242  0.05916178]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, True, False, False]
State prediction error at timestep 182 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.3739436 -0.1730345]]. Action = [[-0.18165895 -0.08206016 -0.02873105 -0.69456846]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, True, False, False]
State prediction error at timestep 183 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.37868467 -0.17400731]]. Action = [[-0.23138116 -0.00953948  0.19150478  0.75179327]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, True, False, False]
State prediction error at timestep 184 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.3833743  -0.17283703]]. Action = [[ 0.0754689   0.22990304 -0.11189251  0.82724595]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, True, False, False]
State prediction error at timestep 185 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.38560686 -0.17073369]]. Action = [[ 0.01100895 -0.14023791  0.02292132  0.37740457]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, True, False, False]
State prediction error at timestep 186 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.3867454 -0.1707076]]. Action = [[ 0.13256735 -0.14255331  0.13964128  0.9906497 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, True, False, False]
State prediction error at timestep 187 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.38624477 -0.17207125]]. Action = [[ 0.14045686 -0.14550734 -0.12304145 -0.37420732]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.3838189  -0.17552646]]. Action = [[ 0.14862245  0.19651079 -0.23174934  0.09751534]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, True, False, False]
State prediction error at timestep 189 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.3810504 -0.17501  ]]. Action = [[-0.00147818 -0.131457    0.01194856  0.8939588 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, True, False, False]
State prediction error at timestep 190 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 191. State = [[-0.37923723 -0.17637146]]. Action = [[ 0.24386674 -0.07408452  0.03545952  0.58085847]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.37304515 -0.179713  ]]. Action = [[ 0.10216513  0.05306995 -0.0090913  -0.95938414]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.3675554  -0.18191773]]. Action = [[ 0.06359291 -0.06240299  0.03051847  0.6562085 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, True, False, False]
State prediction error at timestep 193 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.3632324  -0.18367968]]. Action = [[ 0.17454314  0.17009619  0.1863397  -0.76267684]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, True, False, False]
State prediction error at timestep 194 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.35785204 -0.18211263]]. Action = [[-0.10769348 -0.10970904 -0.19940658 -0.93258244]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, True, False, False]
State prediction error at timestep 195 is tensor(1.8171e-05, grad_fn=<MseLossBackward0>)
Current timestep = 196. State = [[-0.35593346 -0.18211174]]. Action = [[-0.14799806 -0.10512316  0.17653662  0.67260075]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, True, False, False]
State prediction error at timestep 196 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.35546568 -0.18260314]]. Action = [[ 0.22469926 -0.15331383 -0.15528868 -0.11173844]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.35191956 -0.18471251]]. Action = [[-0.0388564   0.2156232  -0.10588557 -0.7049152 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, True, False, False]
State prediction error at timestep 198 is tensor(9.5270e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.35120136 -0.1844869 ]]. Action = [[-0.24291766  0.0693537  -0.2212566  -0.39348197]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, True, False, False]
State prediction error at timestep 199 is tensor(7.1094e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.35345814 -0.18329284]]. Action = [[ 0.11225581 -0.176012    0.0554049   0.38893163]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, True, False, False]
State prediction error at timestep 200 is tensor(7.1488e-05, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.3537978 -0.1838473]]. Action = [[ 0.00189996 -0.19605514  0.18311119 -0.52954566]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, True, False, False]
State prediction error at timestep 201 is tensor(1.3885e-05, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.35372683 -0.1871984 ]]. Action = [[-0.10556364 -0.18737473 -0.19085044  0.31255448]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, True, False, False]
State prediction error at timestep 202 is tensor(9.8212e-05, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.35504878 -0.19337697]]. Action = [[-0.04949342  0.19683489  0.02308819  0.2805122 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.35566086 -0.19536929]]. Action = [[ 0.19011658  0.21191463 -0.20536563  0.8637421 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, True, False, False]
State prediction error at timestep 204 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.35457692 -0.19372882]]. Action = [[-0.05811004 -0.02278297 -0.01396725  0.06361878]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.35451198 -0.19326882]]. Action = [[-0.0485543   0.00640982 -0.02552207 -0.79884905]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, True, False, False]
State prediction error at timestep 206 is tensor(1.5166e-06, grad_fn=<MseLossBackward0>)
Current timestep = 207. State = [[-0.35516798 -0.19259062]]. Action = [[ 0.11733434  0.08093759 -0.21930559  0.73465073]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, True, False, False]
State prediction error at timestep 207 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.3539891  -0.19089809]]. Action = [[ 0.03512377  0.15863249 -0.13260628 -0.95084286]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, True, False, False]
State prediction error at timestep 208 is tensor(5.3545e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.3527798  -0.18816273]]. Action = [[-0.20975572 -0.02971065 -0.19830255 -0.9724659 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, True, False, False]
State prediction error at timestep 209 is tensor(5.6325e-05, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.3551126 -0.1868078]]. Action = [[-0.12563074 -0.07072103 -0.02507292 -0.38706625]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, True, False, False]
State prediction error at timestep 210 is tensor(8.0272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.35703212 -0.18654199]]. Action = [[ 0.07746041 -0.11060333 -0.08581965  0.49373043]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, True, False, False]
State prediction error at timestep 211 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.35728517 -0.18732712]]. Action = [[ 0.12191051 -0.22792256 -0.18023151 -0.41809082]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, True, False, False]
State prediction error at timestep 212 is tensor(7.1828e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.35650864 -0.19047236]]. Action = [[ 0.18303698 -0.03982198  0.1629883  -0.0364064 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, True, False, False]
State prediction error at timestep 213 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.35279146 -0.19352701]]. Action = [[ 0.20454594  0.03348726 -0.0457754  -0.39381182]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, True, False, False]
State prediction error at timestep 214 is tensor(9.5271e-05, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.3480369  -0.19550724]]. Action = [[-0.16744763  0.22378892  0.18323216 -0.82155037]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, True, False, False]
State prediction error at timestep 215 is tensor(2.4008e-05, grad_fn=<MseLossBackward0>)
Current timestep = 216. State = [[-0.3482582  -0.19461516]]. Action = [[-0.12795772  0.14228457  0.0923236   0.05967307]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.35117307 -0.19216503]]. Action = [[-0.19076237 -0.19329448 -0.12496123  0.08366942]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, True, False, False]
State prediction error at timestep 217 is tensor(3.6199e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.35416815 -0.19190611]]. Action = [[-0.08646405  0.18008488  0.10766345 -0.22478467]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.35721016 -0.19007385]]. Action = [[-0.06468177  0.03968209 -0.11925854 -0.8373292 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, True, False, False]
State prediction error at timestep 219 is tensor(9.2627e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.35949945 -0.18898161]]. Action = [[ 0.04441479 -0.2265037   0.07184851  0.1747371 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, True, False, False]
State prediction error at timestep 220 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 221. State = [[-0.36025503 -0.1901067 ]]. Action = [[-0.1911968  -0.07872391  0.06924018 -0.755672  ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, True, False, False]
State prediction error at timestep 221 is tensor(3.5508e-05, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.3627242  -0.19295198]]. Action = [[-0.11190931 -0.23059101 -0.10702643  0.8072448 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, True, False, False]
State prediction error at timestep 222 is tensor(1.6225e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.36675376 -0.19874325]]. Action = [[-0.1498234  0.2113407 -0.025031  -0.5595566]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, True, False, False]
State prediction error at timestep 223 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of -1
Current timestep = 224. State = [[-0.37333906 -0.19951482]]. Action = [[-0.05001076  0.1123634  -0.15851915 -0.8904304 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, True, False, False]
State prediction error at timestep 224 is tensor(2.5004e-05, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.37702    -0.19726105]]. Action = [[-0.09678407  0.10497656  0.20981055  0.08544922]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, True, False, False]
State prediction error at timestep 225 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 226. State = [[-0.3817582  -0.19443673]]. Action = [[-0.13435353 -0.12426923 -0.04731351  0.00026631]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, True, False, False]
State prediction error at timestep 226 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.3863428  -0.19431238]]. Action = [[ 0.09242859 -0.21273865  0.17002499 -0.2917596 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, True, False, False]
State prediction error at timestep 227 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.38875386 -0.19826102]]. Action = [[-0.01989982 -0.07307041 -0.09127358 -0.88857967]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, True, False, False]
State prediction error at timestep 228 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.39153814 -0.20473196]]. Action = [[-0.19759539  0.07007456 -0.2363647   0.18351996]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, True, False, False]
State prediction error at timestep 229 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.3932915  -0.21238983]]. Action = [[ 0.17545778 -0.19702305 -0.09464332 -0.67141795]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, True, False, False]
State prediction error at timestep 230 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.39457342 -0.2194124 ]]. Action = [[-0.00198555  0.00906709 -0.0201886  -0.11682397]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, True, False, False]
State prediction error at timestep 231 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.3962066  -0.22579499]]. Action = [[-0.22647513  0.13396627  0.11244056 -0.20092821]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, True, False, False]
State prediction error at timestep 232 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 232 of -1
Current timestep = 233. State = [[-0.397291   -0.23042633]]. Action = [[-0.07873049  0.20196778  0.21379498  0.58957505]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, True, False, False]
State prediction error at timestep 233 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.39798602 -0.23355481]]. Action = [[ 0.03457612 -0.13326362  0.08456242 -0.2506233 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.39852116 -0.23525798]]. Action = [[ 0.0255926  -0.15608568 -0.10997337 -0.8029254 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.39864957 -0.23562364]]. Action = [[-0.23042819 -0.21413606 -0.02185941 -0.25845695]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, True, False, False]
State prediction error at timestep 236 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 237. State = [[-0.39864513 -0.23570529]]. Action = [[-0.18213259  0.15643722  0.0348461  -0.3712107 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.39849564 -0.23560673]]. Action = [[ 0.1591199   0.22024876 -0.199633   -0.36593723]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, True, False, False]
State prediction error at timestep 238 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.39628777 -0.23355941]]. Action = [[ 0.04259679  0.05060452 -0.0373846  -0.4637692 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, True, False, False]
State prediction error at timestep 239 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.39420128 -0.23180757]]. Action = [[ 0.06196004  0.12670344 -0.15488848  0.97417593]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, True, False, False]
State prediction error at timestep 240 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 241. State = [[-0.39322588 -0.23107773]]. Action = [[ 0.01091674 -0.00432366 -0.04160225  0.20307195]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, True, False, False]
State prediction error at timestep 241 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 242. State = [[-0.39271742 -0.23069844]]. Action = [[ 0.23486876  0.11923671 -0.15118282 -0.11830264]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, True, False, False]
State prediction error at timestep 242 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of 1
Current timestep = 243. State = [[-0.3898675 -0.2284052]]. Action = [[-0.03925121  0.12050769 -0.10050124 -0.546756  ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, True, False, False]
State prediction error at timestep 243 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.38693044 -0.22617823]]. Action = [[ 0.13231498  0.07676342  0.12109008 -0.66136044]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.38346827 -0.22343327]]. Action = [[-0.17483503  0.22618717 -0.19763498 -0.9318525 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, True, False, False]
State prediction error at timestep 245 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.38089222 -0.22166231]]. Action = [[-0.19035426 -0.0079973  -0.05259731 -0.3123144 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.37877032 -0.22046234]]. Action = [[-0.14041351  0.13310751 -0.06385097  0.26874232]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.37731886 -0.21984345]]. Action = [[ 0.0046227  -0.21505953  0.19795084 -0.9718057 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, True, False, False]
State prediction error at timestep 248 is tensor(2.4114e-05, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.37660158 -0.22046243]]. Action = [[-0.1654704   0.21036315 -0.1439027  -0.4512639 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, True, False, False]
State prediction error at timestep 249 is tensor(5.9104e-05, grad_fn=<MseLossBackward0>)
Current timestep = 250. State = [[-0.3756239 -0.2209373]]. Action = [[ 0.09567121 -0.18258092 -0.08531132 -0.45620054]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, True, False, False]
State prediction error at timestep 250 is tensor(2.5125e-05, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.37332025 -0.22293249]]. Action = [[-0.24057077 -0.11113065  0.20738047  0.89655423]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, True, False, False]
State prediction error at timestep 251 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.37177652 -0.22455819]]. Action = [[-0.19285055 -0.14546165  0.13407141 -0.5221902 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(5.8977e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 1
Current timestep = 253. State = [[-0.3732672 -0.2291483]]. Action = [[-0.20008904 -0.24043822  0.09296665 -0.26152664]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, True, False, False]
State prediction error at timestep 253 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of 1
Current timestep = 254. State = [[-0.3757224  -0.23712729]]. Action = [[-0.21521328 -0.07185072  0.20482323  0.6785557 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, True, False, False]
State prediction error at timestep 254 is tensor(8.1834e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 254 of 1
Current timestep = 255. State = [[-0.37786284 -0.2437317 ]]. Action = [[ 0.23365408  0.10679442 -0.14887731  0.61518204]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, True, False, False]
State prediction error at timestep 255 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 255 of 1
Current timestep = 256. State = [[-0.3779912  -0.24692759]]. Action = [[-0.22955818 -0.19998765  0.05157405 -0.55685633]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, True, False, False]
State prediction error at timestep 256 is tensor(6.8113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.37786692 -0.24787003]]. Action = [[-0.16095246  0.1949445   0.17462346 -0.6862897 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(3.0224e-05, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.37760147 -0.24816117]]. Action = [[ 0.17015818 -0.15058662 -0.09136006 -0.77322817]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, True, False, False]
State prediction error at timestep 258 is tensor(3.7339e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[-0.3740287 -0.2511302]]. Action = [[ 0.02254689  0.02429321 -0.17232643  0.40392494]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, True, False, False]
State prediction error at timestep 259 is tensor(9.8205e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of 1
Current timestep = 260. State = [[-0.37132543 -0.2533579 ]]. Action = [[-0.01675344  0.21603304  0.04963395  0.2846632 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, True, False, False]
State prediction error at timestep 260 is tensor(1.5789e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 1
Current timestep = 261. State = [[-0.37056205 -0.2525437 ]]. Action = [[-0.0122674   0.05365399  0.07077682  0.05242825]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, True, False, False]
State prediction error at timestep 261 is tensor(1.0842e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of 1
Current timestep = 262. State = [[-0.36954603 -0.25149527]]. Action = [[ 0.18578196 -0.17647384  0.03671792  0.6834947 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, True, False, False]
State prediction error at timestep 262 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.36617514 -0.25194827]]. Action = [[-0.20375924  0.07502323 -0.1649645  -0.0258832 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, True, False, False]
State prediction error at timestep 263 is tensor(2.2774e-05, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.3665297  -0.25191015]]. Action = [[-0.24478172 -0.09125185  0.1612019  -0.84515774]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, True, False, False]
State prediction error at timestep 264 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 265. State = [[-0.36935624 -0.2528505 ]]. Action = [[-0.04148966 -0.11964707  0.15033284  0.94891334]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, True, False, False]
State prediction error at timestep 265 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.37235457 -0.2549535 ]]. Action = [[-0.17562954  0.15420857  0.10618743  0.7294462 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, True, False, False]
State prediction error at timestep 266 is tensor(3.7595e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of 1
Current timestep = 267. State = [[-0.37681141 -0.2542602 ]]. Action = [[ 0.14584082  0.20222372 -0.20476903 -0.8525129 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, True, False, False]
State prediction error at timestep 267 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 267 of 1
Current timestep = 268. State = [[-0.37893406 -0.25126   ]]. Action = [[0.13202581 0.09810728 0.07661235 0.16120338]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, True, False, False]
State prediction error at timestep 268 is tensor(8.8860e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.3777034  -0.24819052]]. Action = [[-0.1813107  -0.02680057 -0.17500728 -0.55810195]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, True, False, False]
State prediction error at timestep 269 is tensor(1.9230e-06, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.3769263  -0.24646007]]. Action = [[ 0.19335929 -0.14018218  0.14915818 -0.28283018]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, True, False, False]
State prediction error at timestep 270 is tensor(6.8782e-06, grad_fn=<MseLossBackward0>)
Current timestep = 271. State = [[-0.374685   -0.24497165]]. Action = [[-0.08284508 -0.00112428  0.08981776 -0.05812854]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, True, False, False]
State prediction error at timestep 271 is tensor(6.9096e-06, grad_fn=<MseLossBackward0>)
Current timestep = 272. State = [[-0.3742038  -0.24464078]]. Action = [[-0.08069916 -0.13449755 -0.03705736 -0.97051436]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, True, False, False]
State prediction error at timestep 272 is tensor(3.7719e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.3742996  -0.24573193]]. Action = [[-0.02717207 -0.10595435  0.17001301  0.8400912 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, True, False, False]
State prediction error at timestep 273 is tensor(4.5297e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.37531024 -0.24817991]]. Action = [[-0.06713332 -0.2118373   0.22976929 -0.5643629 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(3.3682e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.3771101  -0.25376007]]. Action = [[-0.01631677  0.12742513  0.13335133 -0.6880422 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, True, False, False]
State prediction error at timestep 275 is tensor(2.3728e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.37855512 -0.25810322]]. Action = [[-0.20553726  0.18960407  0.04510903 -0.75888455]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, True, False, False]
State prediction error at timestep 276 is tensor(5.4957e-05, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.37939757 -0.2607844 ]]. Action = [[ 0.12406254  0.08752918  0.11095589 -0.8265178 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, True, False, False]
State prediction error at timestep 277 is tensor(5.9194e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.37857208 -0.26032746]]. Action = [[ 0.21099687  0.13000813 -0.22008339  0.34317005]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, True, False, False]
State prediction error at timestep 278 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.37630528 -0.25821948]]. Action = [[-0.10485816 -0.11003046 -0.14500472 -0.9741324 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, True, False, False]
State prediction error at timestep 279 is tensor(2.1036e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.37605697 -0.2582008 ]]. Action = [[ 0.11066592 -0.05990845 -0.24691553 -0.36259276]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, True, False, False]
State prediction error at timestep 280 is tensor(2.5720e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of 1
Current timestep = 281. State = [[-0.3744359 -0.2592407]]. Action = [[-0.11547562  0.12240955 -0.20551123  0.3448429 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, True, False, False]
State prediction error at timestep 281 is tensor(3.6504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.3745614 -0.2589193]]. Action = [[ 0.13157713 -0.1667208  -0.00842777  0.03993022]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, True, False, False]
State prediction error at timestep 282 is tensor(2.7879e-05, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.37285322 -0.26035675]]. Action = [[ 0.0111123   0.01453876 -0.10637045 -0.3462125 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, True, False, False]
State prediction error at timestep 283 is tensor(2.3938e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.3715126  -0.26161906]]. Action = [[-0.18991262 -0.2309979   0.1140551   0.67487454]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, True, False, False]
State prediction error at timestep 284 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.37309107 -0.2653636 ]]. Action = [[-0.16738303  0.18977433 -0.14159426  0.7433766 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, True, False, False]
State prediction error at timestep 285 is tensor(4.8942e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.3756248  -0.26611608]]. Action = [[ 0.1279602   0.08436418 -0.09341219 -0.06715542]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, True, False, False]
State prediction error at timestep 286 is tensor(1.0109e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of 1
Current timestep = 287. State = [[-0.37577334 -0.26561588]]. Action = [[-0.04445575  0.22111863 -0.23099236  0.8693831 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, True, False, False]
State prediction error at timestep 287 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.37777585 -0.26207718]]. Action = [[-0.04273199  0.09189814  0.23603922 -0.8264902 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, True, False, False]
State prediction error at timestep 288 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[-0.37911633 -0.25918657]]. Action = [[ 0.04760155 -0.16888407  0.00398302  0.92393994]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, True, False, False]
State prediction error at timestep 289 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 289 of 1
Current timestep = 290. State = [[-0.3790201  -0.25924402]]. Action = [[-0.08261624 -0.1811315  -0.06621322  0.3096224 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, True, False, False]
State prediction error at timestep 290 is tensor(4.3259e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.37968004 -0.26025403]]. Action = [[ 0.02391812  0.24626672  0.08284676 -0.3139379 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, True, False, False]
State prediction error at timestep 291 is tensor(2.9352e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.3795959  -0.25953153]]. Action = [[ 0.00176752  0.21775317 -0.18306744  0.6780262 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, True, False, False]
State prediction error at timestep 292 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.3804493  -0.25531355]]. Action = [[ 0.20888829  0.08747712 -0.23251325  0.00473166]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, True, False, False]
State prediction error at timestep 293 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.37760493 -0.25076833]]. Action = [[ 0.04661059  0.05174512  0.01233104 -0.14310813]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, True, False, False]
State prediction error at timestep 294 is tensor(1.9307e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of 1
Current timestep = 295. State = [[-0.37427115 -0.24657954]]. Action = [[-0.23125969  0.20085067 -0.20367116 -0.5201968 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, True, False, False]
State prediction error at timestep 295 is tensor(5.1548e-05, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.3723608  -0.24390036]]. Action = [[-0.2067783  -0.04780883 -0.15197326 -0.02588451]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, True, False, False]
State prediction error at timestep 296 is tensor(2.0917e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[-0.37089485 -0.24186496]]. Action = [[ 0.14828703  0.01462311 -0.19903766  0.33945346]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, True, False, False]
State prediction error at timestep 297 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.3688851  -0.24025589]]. Action = [[-0.08563462  0.12798578 -0.14754401 -0.7866131 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, True, False, False]
State prediction error at timestep 298 is tensor(7.5572e-05, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.3691617  -0.23807198]]. Action = [[-0.0093274  -0.22852781 -0.18748336 -0.22356331]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, True, False, False]
State prediction error at timestep 299 is tensor(4.1801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 300. State = [[-0.3694041  -0.23811719]]. Action = [[-0.14273943  0.13991237  0.0099352  -0.80526584]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, True, False, False]
State prediction error at timestep 300 is tensor(4.9846e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.37094492 -0.23683663]]. Action = [[ 0.21648628 -0.21688172  0.2191332  -0.8877546 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, True, False, False]
State prediction error at timestep 301 is tensor(1.7526e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.369417   -0.23817973]]. Action = [[ 0.13734335 -0.10514626 -0.22629502  0.32035208]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, True, False, False]
State prediction error at timestep 302 is tensor(6.0607e-05, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.36662737 -0.2403608 ]]. Action = [[-0.14472316  0.01926768  0.20688754 -0.85602254]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, True, False, False]
State prediction error at timestep 303 is tensor(6.4215e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.36671433 -0.24096783]]. Action = [[-0.223571    0.11282575 -0.03633851  0.5679331 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, True, False, False]
State prediction error at timestep 304 is tensor(5.9333e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 1
Current timestep = 305. State = [[-0.3691394  -0.23973207]]. Action = [[ 0.03376848 -0.16622402 -0.17756368 -0.5779777 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, True, False, False]
State prediction error at timestep 305 is tensor(5.6169e-05, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.36938158 -0.24056104]]. Action = [[ 0.24261475  0.23270929 -0.20998748  0.5442014 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, True, False, False]
State prediction error at timestep 306 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.36766946 -0.2389107 ]]. Action = [[ 0.14681876 -0.03429256  0.02286997  0.33827603]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, True, False, False]
State prediction error at timestep 307 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[-0.36399698 -0.23772527]]. Action = [[-0.08276311  0.08680186 -0.09860674  0.2584914 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, True, False, False]
State prediction error at timestep 308 is tensor(3.8234e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of 1
Current timestep = 309. State = [[-0.3622939  -0.23572375]]. Action = [[ 0.12714633 -0.03915319  0.1887629  -0.72777134]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, True, False, False]
State prediction error at timestep 309 is tensor(6.4683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 310. State = [[-0.36019835 -0.23398861]]. Action = [[-0.17478733  0.18583924  0.22796124  0.8654938 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, True, False, False]
State prediction error at timestep 310 is tensor(8.4236e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of 1
Current timestep = 311. State = [[-0.360622   -0.23107032]]. Action = [[-0.09721783 -0.10159156  0.21834737 -0.37585634]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, True, False, False]
State prediction error at timestep 311 is tensor(5.1238e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.36080125 -0.23029718]]. Action = [[ 0.2071212   0.08531624 -0.06432319 -0.8781519 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, True, False, False]
State prediction error at timestep 312 is tensor(6.2159e-05, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.35913193 -0.22853673]]. Action = [[-0.21118712 -0.00910215 -0.24700788 -0.6203273 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, True, False, False]
State prediction error at timestep 313 is tensor(9.2729e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.36004725 -0.2273283 ]]. Action = [[-0.19697395  0.14922836 -0.10391834  0.6788801 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, True, False, False]
State prediction error at timestep 314 is tensor(1.7060e-05, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.36358917 -0.22441639]]. Action = [[ 0.20084465  0.13076794 -0.12464207 -0.03145224]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, True, False, False]
State prediction error at timestep 315 is tensor(5.5286e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of 1
Current timestep = 316. State = [[-0.3638274  -0.22052541]]. Action = [[ 0.11167097  0.03053007 -0.24196418  0.29232478]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, True, False, False]
State prediction error at timestep 316 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.36288306 -0.21658345]]. Action = [[-0.05253179  0.03082624 -0.07757734  0.5408251 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, True, False, False]
State prediction error at timestep 317 is tensor(7.7813e-05, grad_fn=<MseLossBackward0>)
Current timestep = 318. State = [[-0.36356345 -0.21301672]]. Action = [[-0.16134194  0.05155608 -0.18832377  0.36420774]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, True, False, False]
State prediction error at timestep 318 is tensor(5.4436e-05, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.36585015 -0.20922647]]. Action = [[-0.06570601  0.06453905 -0.06753771 -0.56187975]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, True, False, False]
State prediction error at timestep 319 is tensor(9.0578e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.36899912 -0.20507917]]. Action = [[-0.17042895  0.00361159 -0.06889285  0.2614956 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, True, False, False]
State prediction error at timestep 320 is tensor(7.0537e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.37292266 -0.20119895]]. Action = [[ 0.17084974  0.21710414 -0.13903488 -0.97208035]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, True, False, False]
State prediction error at timestep 321 is tensor(4.0972e-05, grad_fn=<MseLossBackward0>)
Current timestep = 322. State = [[-0.37259978 -0.1953271 ]]. Action = [[ 0.19758216 -0.11775309  0.09064585  0.8059864 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, True, False, False]
State prediction error at timestep 322 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.37000403 -0.19275436]]. Action = [[-0.1796195  -0.10576877  0.02739111 -0.8133837 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, True, False, False]
State prediction error at timestep 323 is tensor(3.6343e-05, grad_fn=<MseLossBackward0>)
Current timestep = 324. State = [[-0.37129447 -0.19205862]]. Action = [[ 0.18196785 -0.08220308 -0.10058224  0.50254154]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, True, False, False]
State prediction error at timestep 324 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.3708973  -0.19231519]]. Action = [[-0.00974233  0.18315697 -0.17122556  0.39620042]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, True, False, False]
State prediction error at timestep 325 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.3689786  -0.18908393]]. Action = [[-0.2374681  -0.22046317 -0.17943642 -0.56201786]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, True, False, False]
State prediction error at timestep 326 is tensor(2.7249e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.37133807 -0.19015364]]. Action = [[-0.13829687  0.04099023  0.15460283  0.9575485 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, True, False, False]
State prediction error at timestep 327 is tensor(5.6209e-05, grad_fn=<MseLossBackward0>)
Current timestep = 328. State = [[-0.37445393 -0.18973935]]. Action = [[ 0.09325409 -0.17034331 -0.22353087  0.5957265 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, True, False, False]
State prediction error at timestep 328 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of 1
Current timestep = 329. State = [[-0.37504384 -0.19226094]]. Action = [[ 0.04721084  0.05556679 -0.03873028 -0.94168   ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, True, False, False]
State prediction error at timestep 329 is tensor(8.3285e-05, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.37522346 -0.19358519]]. Action = [[-0.21639137 -0.24088237  0.05580002 -0.86458856]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, True, False, False]
State prediction error at timestep 330 is tensor(9.5893e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of -1
Current timestep = 331. State = [[-0.37522066 -0.19458188]]. Action = [[ 0.15826195 -0.14684844 -0.2283717  -0.7264705 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, True, False, False]
State prediction error at timestep 331 is tensor(6.4339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.37444153 -0.19754025]]. Action = [[-0.09293625 -0.19631338 -0.14138234 -0.40359354]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, True, False, False]
State prediction error at timestep 332 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.37514952 -0.20375395]]. Action = [[ 0.06830955 -0.04467438  0.01308843 -0.24440843]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, True, False, False]
State prediction error at timestep 333 is tensor(6.8654e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.3757182 -0.2095083]]. Action = [[-0.08545205 -0.12382981 -0.18732037  0.8351257 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, True, False, False]
State prediction error at timestep 334 is tensor(4.3256e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.3769176 -0.2165146]]. Action = [[ 0.06564602 -0.18865688  0.17496756  0.75189424]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, True, False, False]
State prediction error at timestep 335 is tensor(1.9564e-05, grad_fn=<MseLossBackward0>)
Current timestep = 336. State = [[-0.37734658 -0.22470137]]. Action = [[ 0.20144278 -0.07092509 -0.1312615   0.48895288]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, True, False, False]
State prediction error at timestep 336 is tensor(7.6520e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.37469792 -0.23204726]]. Action = [[-0.12027672  0.1916619   0.1903969  -0.24309105]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, True, False, False]
State prediction error at timestep 337 is tensor(1.1288e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of -1
Current timestep = 338. State = [[-0.3745212  -0.23399107]]. Action = [[-0.23599578 -0.19490911  0.08139366  0.37321973]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, True, False, False]
State prediction error at timestep 338 is tensor(7.7194e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.37463963 -0.23411077]]. Action = [[-0.09548745  0.04516295 -0.11841963 -0.9606861 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, True, False, False]
State prediction error at timestep 339 is tensor(5.1907e-06, grad_fn=<MseLossBackward0>)
Current timestep = 340. State = [[-0.3749539  -0.23388992]]. Action = [[-0.16651233  0.17883456 -0.04914449 -0.47522426]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, True, False, False]
State prediction error at timestep 340 is tensor(2.7107e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.37868202 -0.23061693]]. Action = [[ 0.09182355 -0.04198363 -0.11012837  0.85324955]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, True, False, False]
State prediction error at timestep 341 is tensor(7.4146e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.37934726 -0.2296421 ]]. Action = [[ 0.09376106 -0.04159488  0.00764298 -0.5077869 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, True, False, False]
State prediction error at timestep 342 is tensor(9.3531e-06, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.37870058 -0.2295135 ]]. Action = [[-1.17156565e-01 -1.40741467e-04  1.77645802e-01 -8.40833545e-01]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, True, False, False]
State prediction error at timestep 343 is tensor(4.5825e-06, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.37899676 -0.2292357 ]]. Action = [[-0.12164421  0.21639317 -0.14671817 -0.23290408]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, True, False, False]
State prediction error at timestep 344 is tensor(3.7339e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.38241875 -0.2257599 ]]. Action = [[ 0.20562136  0.06422007 -0.16222464  0.22110355]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, True, False, False]
State prediction error at timestep 345 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.38096792 -0.22240332]]. Action = [[-0.081121    0.16777748 -0.04855107 -0.5782597 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, True, False, False]
State prediction error at timestep 346 is tensor(7.1464e-05, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.38163662 -0.2174159 ]]. Action = [[-0.08798516 -0.00240059 -0.10698108  0.19055676]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, True, False, False]
State prediction error at timestep 347 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 348. State = [[-0.38354248 -0.21400945]]. Action = [[ 0.08653831  0.06966019 -0.04992369 -0.90413165]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, True, False, False]
State prediction error at timestep 348 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.38359308 -0.21027285]]. Action = [[ 0.22763407  0.06768334 -0.22746885  0.40691173]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, True, False, False]
State prediction error at timestep 349 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of -1
Current timestep = 350. State = [[-0.37905395 -0.20509806]]. Action = [[ 0.1607207   0.16903174 -0.01275143  0.8830292 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, True, False, False]
State prediction error at timestep 350 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.37472513 -0.1975097 ]]. Action = [[ 0.10883063  0.24509203  0.00996304 -0.4256059 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, True, False, False]
State prediction error at timestep 351 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.37160534 -0.18581727]]. Action = [[-0.01477695  0.09661445 -0.23722807  0.12294519]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, True, False, False]
State prediction error at timestep 352 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of -1
Current timestep = 353. State = [[-0.37041306 -0.17541084]]. Action = [[-0.10601625 -0.00887257  0.06499827  0.9281864 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, True, False, False]
State prediction error at timestep 353 is tensor(9.9293e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.3716474  -0.16931996]]. Action = [[-0.20241064  0.06249681  0.12330908 -0.20074224]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, True, False, False]
State prediction error at timestep 354 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.3757941  -0.16356592]]. Action = [[-0.22794694 -0.13071175  0.01355976  0.95025015]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, True, False, False]
State prediction error at timestep 355 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of 1
Current timestep = 356. State = [[-0.37762383 -0.15933974]]. Action = [[0.2111964  0.21549392 0.0167127  0.01954281]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, True, False, False]
State prediction error at timestep 356 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of 1
Current timestep = 357. State = [[-0.37519783 -0.1512112 ]]. Action = [[0.17695731 0.04849973 0.16347826 0.84234405]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, True, False, False]
State prediction error at timestep 357 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.37227467 -0.143392  ]]. Action = [[-0.03950706 -0.12736885 -0.02971734 -0.3279835 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, True, False, False]
State prediction error at timestep 358 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.37091824 -0.14039108]]. Action = [[-0.03259772  0.12249908  0.13471293 -0.177001  ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, True, False, False]
State prediction error at timestep 359 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.3700821  -0.13711059]]. Action = [[-0.17395604  0.06325924 -0.13210395  0.28809762]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, True, False, False]
State prediction error at timestep 360 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.3717526  -0.13404357]]. Action = [[-0.01132144 -0.15153314  0.03179461 -0.9146155 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, True, False, False]
State prediction error at timestep 361 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.3724412  -0.13406974]]. Action = [[-0.01655419 -0.23809694 -0.08074825 -0.454935  ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, True, False, False]
State prediction error at timestep 362 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 363. State = [[-0.37227854 -0.13734494]]. Action = [[-0.06352131 -0.08802266 -0.06625305  0.5139413 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, True, False, False]
State prediction error at timestep 363 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 364. State = [[-0.37203553 -0.14122398]]. Action = [[-0.00550787 -0.0458456  -0.10278973  0.78202295]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, True, False, False]
State prediction error at timestep 364 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 1
Current timestep = 365. State = [[-0.37193754 -0.14461099]]. Action = [[-0.00263926  0.04576388  0.10214502 -0.49243987]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, True, False, False]
State prediction error at timestep 365 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.37269983 -0.14579676]]. Action = [[-0.05199254  0.05213675 -0.03954378  0.74696684]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, True, False, False]
State prediction error at timestep 366 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of 1
Current timestep = 367. State = [[-0.37440702 -0.14511514]]. Action = [[-0.22975871  0.22823983 -0.17894053  0.31437612]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, True, False, False]
State prediction error at timestep 367 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of 1
Current timestep = 368. State = [[-0.37529993 -0.14478323]]. Action = [[ 0.20512831 -0.16182782  0.13758832 -0.26303947]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, True, False, False]
State prediction error at timestep 368 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 369. State = [[-0.37474996 -0.14694132]]. Action = [[-0.16908358  0.08200279  0.04068938  0.85464025]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, True, False, False]
State prediction error at timestep 369 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.37516862 -0.14756247]]. Action = [[ 0.04648334  0.09858    -0.18353447  0.04880941]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, True, False, False]
State prediction error at timestep 370 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of 1
Current timestep = 371. State = [[-0.37555876 -0.14713499]]. Action = [[ 0.24087897  0.14166579  0.07345298 -0.78025323]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, True, False, False]
State prediction error at timestep 371 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of 1
Current timestep = 372. State = [[-0.37393692 -0.14352639]]. Action = [[ 0.14391893  0.193919   -0.11758155  0.18466568]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, True, False, False]
State prediction error at timestep 372 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.37122464 -0.13791928]]. Action = [[-0.11069956 -0.1965343   0.24507609 -0.25528657]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, True, False, False]
State prediction error at timestep 373 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.37092263 -0.13772877]]. Action = [[-0.20934431 -0.05481534 -0.20156585 -0.80742896]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, True, False, False]
State prediction error at timestep 374 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of 1
Current timestep = 375. State = [[-0.37179884 -0.13795586]]. Action = [[ 0.02963343  0.20842242 -0.0676212   0.04452384]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, True, False, False]
State prediction error at timestep 375 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.37280443 -0.1362823 ]]. Action = [[-0.02748777 -0.15070383  0.0421899   0.49182224]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, True, False, False]
State prediction error at timestep 376 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.3735332 -0.136248 ]]. Action = [[-0.10064167  0.13168013 -0.09109384 -0.6794442 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, True, False, False]
State prediction error at timestep 377 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 378. State = [[-0.37588122 -0.13526464]]. Action = [[-0.14835304 -0.2306453  -0.17792203 -0.7429445 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, True, False, False]
State prediction error at timestep 378 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 379. State = [[-0.37936255 -0.13652384]]. Action = [[-0.06638688 -0.20321824  0.06393909  0.71506083]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, True, False, False]
State prediction error at timestep 379 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of 1
Current timestep = 380. State = [[-0.38110974 -0.14083873]]. Action = [[0.1673213  0.14529812 0.1946019  0.4999131 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, True, False, False]
State prediction error at timestep 380 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of 1
Current timestep = 381. State = [[-0.38023838 -0.14160779]]. Action = [[0.20538881 0.19877777 0.23874694 0.612401  ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, True, False, False]
State prediction error at timestep 381 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of 1
Current timestep = 382. State = [[-0.3785163  -0.13925071]]. Action = [[-0.19026099 -0.07509631  0.08587229  0.7409532 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, True, False, False]
State prediction error at timestep 382 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.37728807 -0.13710864]]. Action = [[ 0.05630699  0.05542088  0.19194895 -0.4019512 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, True, False, False]
State prediction error at timestep 383 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.37613028 -0.13507144]]. Action = [[-0.1436345   0.02214667 -0.15274939 -0.07455313]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, True, False, False]
State prediction error at timestep 384 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.37641284 -0.13403632]]. Action = [[-0.21707281 -0.07884167 -0.03823625  0.8790879 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, True, False, False]
State prediction error at timestep 385 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.37659746 -0.13341932]]. Action = [[ 0.196718    0.1631763  -0.17055915  0.07552898]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, True, False, False]
State prediction error at timestep 386 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.3755445  -0.12882149]]. Action = [[-0.11975138 -0.02929686  0.2237562  -0.86415684]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, True, False, False]
State prediction error at timestep 387 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.37569723 -0.12658857]]. Action = [[-0.16182028  0.01517361  0.2027879   0.94307315]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, True, False, False]
State prediction error at timestep 388 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.3772294  -0.12494879]]. Action = [[ 0.20072913  0.09206378 -0.02836075  0.03261209]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.37674326 -0.1222148 ]]. Action = [[-0.05121216 -0.032353   -0.17596288 -0.7303574 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.37681407 -0.12018026]]. Action = [[ 0.05683222  0.08327025 -0.2422138  -0.84196585]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 392. State = [[-0.3768404  -0.11726829]]. Action = [[-0.1429353  -0.12853712  0.09060943  0.3776033 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.37692508 -0.11722244]]. Action = [[ 0.03622147 -0.09166029  0.01175553 -0.92156297]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.37688696 -0.11754119]]. Action = [[-0.19201113 -0.07066113  0.1730581  -0.8647233 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.37689567 -0.11751419]]. Action = [[-0.12120457  0.00327897 -0.13738112 -0.9197904 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.3777005 -0.117424 ]]. Action = [[-0.09257412 -0.05786304 -0.07565695 -0.8577587 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 397. State = [[-0.37960932 -0.11793544]]. Action = [[-0.20344244 -0.21552554 -0.216771    0.00810468]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 398. State = [[-0.38036737 -0.1180049 ]]. Action = [[ 0.08899751  0.18964964 -0.16984317  0.534055  ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.380553   -0.11695977]]. Action = [[ 0.0005492  -0.15636903 -0.14176829  0.11355388]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.380511   -0.11737669]]. Action = [[-0.21946248  0.05820668  0.06323472 -0.41021264]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 401. State = [[-0.38055304 -0.11765619]]. Action = [[ 0.16122082 -0.16985637  0.16402882 -0.7372006 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.37931368 -0.12003005]]. Action = [[ 0.0610671   0.24382794  0.16552007 -0.32152414]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.37832785 -0.11901583]]. Action = [[ 0.05009952  0.22737163  0.12063807 -0.51041746]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.37765887 -0.11410251]]. Action = [[-0.17448199 -0.00311305 -0.20331548  0.9773586 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.3775455 -0.1102753]]. Action = [[ 0.10997605  0.11246428  0.23556209 -0.72242963]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.37564498 -0.10606787]]. Action = [[-0.10562384 -0.01332822  0.19459951  0.39128542]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[-0.37598354 -0.10267983]]. Action = [[-0.16805187 -0.02995211 -0.17603663 -0.32330263]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 407 of 1
Current timestep = 408. State = [[-0.37695652 -0.10174872]]. Action = [[-0.22587095  0.15188599 -0.07905731  0.871367  ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of 1
Current timestep = 409. State = [[-0.37750313 -0.10151737]]. Action = [[-0.22426651  0.08319211 -0.13688661  0.29665995]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 410. State = [[-0.37783918 -0.10118627]]. Action = [[ 0.0514777   0.1570813  -0.10155806  0.06954062]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.37784308 -0.09795445]]. Action = [[ 0.1772902   0.05578384 -0.11761637  0.3156724 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 412. State = [[-0.37603214 -0.09395493]]. Action = [[ 0.24396801 -0.05768697  0.12358403 -0.54279894]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of 1
Current timestep = 413. State = [[-0.3680088  -0.09271622]]. Action = [[ 0.2109307  -0.19516677  0.05515468 -0.8218163 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of 1
Current timestep = 414. State = [[-0.35724726 -0.09615012]]. Action = [[-0.22746223 -0.0348974  -0.17936146 -0.07185054]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.35540742 -0.09744834]]. Action = [[-0.17385995  0.1907816  -0.24002928 -0.65839124]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(9.1120e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of 1
Current timestep = 416. State = [[-0.3569097 -0.0961654]]. Action = [[ 0.13693321 -0.2264694  -0.08099724 -0.83444864]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.35641292 -0.09732629]]. Action = [[ 0.15954104  0.14618999  0.00419766 -0.9282178 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.35291684 -0.09807663]]. Action = [[-0.11923337 -0.2294144  -0.14638685 -0.7963428 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of 1
Current timestep = 419. State = [[-0.3521935  -0.10025939]]. Action = [[ 0.18890649  0.14067268 -0.03306775 -0.45685792]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.34855574 -0.10106375]]. Action = [[-0.11773333 -0.08212447 -0.05760616 -0.5147241 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(8.3168e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.348223   -0.10189026]]. Action = [[-0.05578327 -0.2318804   0.12208337 -0.73606604]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(9.7594e-05, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.34839383 -0.10691014]]. Action = [[ 0.19805157 -0.05471024  0.04361022  0.6753367 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.34516189 -0.11139918]]. Action = [[ 0.00501159 -0.20729777 -0.19198976 -0.26192868]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(3.2823e-05, grad_fn=<MseLossBackward0>)
Current timestep = 424. State = [[-0.34182072 -0.11891458]]. Action = [[ 0.22283435  0.06075945  0.00906178 -0.5074856 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(4.5817e-05, grad_fn=<MseLossBackward0>)
Current timestep = 425. State = [[-0.33514062 -0.12395902]]. Action = [[-0.18004173 -0.01702735 -0.06326841 -0.84893   ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.3342556  -0.12618354]]. Action = [[-0.18391699  0.17878067 -0.19474648  0.8345046 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, True, False, False]
State prediction error at timestep 426 is tensor(5.4592e-06, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.33617243 -0.12532596]]. Action = [[-0.2372525  -0.02953714  0.14497864  0.05529225]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, True, False, False]
State prediction error at timestep 427 is tensor(1.6737e-05, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.342417   -0.12391987]]. Action = [[0.06581074 0.00952077 0.04844725 0.39412272]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(4.0885e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.34462157 -0.12308957]]. Action = [[ 0.23985702  0.09694701 -0.13462773  0.07474685]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.34411773 -0.12283041]]. Action = [[ 0.08564299 -0.06558648  0.11743033  0.21130908]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(3.7807e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.3420242  -0.12364195]]. Action = [[-0.14738487 -0.13030672 -0.00447199 -0.72180486]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.34197915 -0.12458035]]. Action = [[ 0.03125364  0.20899948  0.20642173 -0.01391315]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(4.1292e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.34207904 -0.12326094]]. Action = [[ 0.20318389  0.23160246 -0.15283737 -0.25274223]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(1.3433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.33803436 -0.11527613]]. Action = [[ 0.09761429  0.16092187 -0.04503787 -0.18671888]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(3.7144e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.3350229  -0.10939331]]. Action = [[-0.21338339 -0.04828866 -0.18758191  0.75578403]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(9.2395e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of 1
Current timestep = 436. State = [[-0.3351536  -0.10647293]]. Action = [[0.16365248 0.07715109 0.1667555  0.29191923]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(5.2845e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.33338243 -0.1017601 ]]. Action = [[ 0.21446222  0.14170194 -0.22736472  0.15621161]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.32712096 -0.09575173]]. Action = [[-0.15778437  0.23441541 -0.03754091 -0.9271375 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(2.6351e-05, grad_fn=<MseLossBackward0>)
Current timestep = 439. State = [[-0.32457238 -0.08599531]]. Action = [[-0.2157887   0.19276851 -0.04794277 -0.0249905 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(3.5809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.32716447 -0.07512873]]. Action = [[-0.183266    0.07297099 -0.07895623  0.30457747]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(6.9137e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of 1
Current timestep = 441. State = [[-0.33259958 -0.06467934]]. Action = [[ 0.06149307  0.2220614   0.18056577 -0.36427605]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 441 of 1
Current timestep = 442. State = [[-0.33460283 -0.05249726]]. Action = [[-0.17508668  0.10433048 -0.10888658 -0.06554449]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[-0.3385382  -0.04252091]]. Action = [[-0.06473008 -0.03379741 -0.02388993  0.38869596]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of 1
Current timestep = 444. State = [[-0.3420058  -0.03602846]]. Action = [[ 0.05455118  0.0381957  -0.1012127  -0.3674792 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 445. State = [[-0.3436088  -0.03089244]]. Action = [[-0.19405437  0.20555937  0.09831381  0.90644574]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 446. State = [[-0.34791446 -0.02425837]]. Action = [[ 0.03871268 -0.12195091 -0.04272367 -0.51151675]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of 1
Current timestep = 447. State = [[-0.34973815 -0.02013681]]. Action = [[0.04464966 0.11105913 0.13650802 0.6846187 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 447 of 1
Current timestep = 448. State = [[-0.35094547 -0.01660115]]. Action = [[ 0.13669854  0.19688573  0.0695385  -0.19218707]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.35139844 -0.00958213]]. Action = [[0.18642953 0.07208729 0.05769122 0.6805527 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 450. State = [[-0.34741214 -0.00318601]]. Action = [[-0.0900538 -0.0780566 -0.2039003 -0.9357998]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of -1
Current timestep = 451. State = [[-0.34663588 -0.00055864]]. Action = [[ 0.07109565 -0.07065092 -0.07678288 -0.31345642]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.34428313 -0.00069831]]. Action = [[ 0.14929846  0.06482452 -0.19176629 -0.29688036]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 453. State = [[-3.4013757e-01  1.3756815e-04]]. Action = [[-0.22941051  0.19100642 -0.04525082  0.3669355 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[-0.34148955  0.0046903 ]]. Action = [[-0.05538248 -0.22537525 -0.1554048   0.53665805]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.34200433  0.00482657]]. Action = [[ 0.14292341  0.04495165 -0.09029391  0.54278684]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 456. State = [[-0.341701    0.00480104]]. Action = [[-0.04387401  0.05732891  0.0504654  -0.2714039 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of 0
Current timestep = 457. State = [[-0.3416849   0.00499523]]. Action = [[ 0.08995295  0.12364724 -0.18134807  0.83795094]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of 0
Current timestep = 458. State = [[-0.3406956   0.00737357]]. Action = [[-0.01094763  0.03099877 -0.13316517  0.73257625]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 459. State = [[-0.3400222   0.00986668]]. Action = [[ 0.19820866  0.20746535 -0.1544776  -0.14945102]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 460. State = [[-0.3349509   0.01650368]]. Action = [[-0.15021199 -0.16227227  0.02040929  0.43646896]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 0
Current timestep = 461. State = [[-0.3343429   0.01849148]]. Action = [[-0.22657686 -0.08099619  0.00450158 -0.649861  ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of 0
Current timestep = 462. State = [[-0.33657876  0.01822126]]. Action = [[-0.15596238 -0.23084368 -0.10242775  0.57821965]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of 0
Current timestep = 463. State = [[-0.34134609  0.01604702]]. Action = [[-0.11280337  0.10028115  0.17880744  0.293203  ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of 0
Current timestep = 464. State = [[-0.34689206  0.01582477]]. Action = [[ 0.18844807  0.10814714 -0.20922722 -0.40026903]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 465. State = [[-0.3472252   0.01670272]]. Action = [[ 0.2369672  -0.02264361  0.17803401 -0.30498266]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.343114    0.01679665]]. Action = [[ 0.24534479 -0.06019975 -0.08848622  0.8447349 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.33515733  0.01624536]]. Action = [[-0.0937008   0.10451338 -0.04985371 -0.09309006]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 467 of 0
Current timestep = 468. State = [[-0.33311737  0.0165824 ]]. Action = [[-0.02930576 -0.1468776   0.02399394 -0.8746625 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 468 of 0
Current timestep = 469. State = [[-0.33219677  0.01580184]]. Action = [[ 0.13136208  0.1153188   0.20245636 -0.00690407]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of 0
Current timestep = 470. State = [[-0.32918793  0.01616717]]. Action = [[ 0.1800094   0.11750102 -0.02926987 -0.39179194]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 471. State = [[-0.32210547  0.01858114]]. Action = [[-0.00051469 -0.13910377 -0.12522282 -0.3415891 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.31651878  0.01843037]]. Action = [[ 0.16340595  0.02775195 -0.17591242  0.9386101 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of 1
Current timestep = 473. State = [[-0.31050602  0.01815805]]. Action = [[-0.19965889 -0.01475786 -0.07077771 -0.7316856 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[-0.31046465  0.01824244]]. Action = [[ 0.19085145 -0.02026324 -0.06731489  0.51365376]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of 1
Current timestep = 475. State = [[-0.3069184   0.01812802]]. Action = [[ 0.22165546  0.22022733 -0.01984341  0.92276454]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of 1
Current timestep = 476. State = [[-0.29948232  0.02138712]]. Action = [[-0.057951   -0.19331473 -0.2247409  -0.37214112]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(5.9329e-05, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.29611292  0.02120072]]. Action = [[-0.02664021 -0.04086106 -0.08308707 -0.6034921 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 478. State = [[-0.29485223  0.02040613]]. Action = [[ 0.03585362 -0.06754543 -0.14245392 -0.27264142]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(3.1160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 479. State = [[-0.29337528  0.01942344]]. Action = [[-0.012989    0.00803271  0.09245741  0.18656063]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(6.8469e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of 1
Current timestep = 480. State = [[-0.2925075   0.01897216]]. Action = [[-0.01028131  0.11182332 -0.1138809   0.49987936]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(1.1502e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of 1
Current timestep = 481. State = [[-0.29269367  0.01925889]]. Action = [[-0.17243634  0.08092299 -0.08145171  0.9415846 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(7.2109e-05, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.29344803  0.02114131]]. Action = [[ 0.2011145   0.10059482 -0.06981725 -0.61153305]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 483. State = [[-0.29216596  0.02385036]]. Action = [[ 0.23332351  0.2253868  -0.2261541  -0.8985515 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 1
Current timestep = 484. State = [[-0.2857913   0.03041261]]. Action = [[-0.1826515  -0.08203363 -0.19658652  0.10787976]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of 1
Current timestep = 485. State = [[-0.2849612   0.03303498]]. Action = [[-0.05728205 -0.00310494 -0.14933802  0.95808244]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(4.5583e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of 1
Current timestep = 486. State = [[-0.28526968  0.03364212]]. Action = [[ 0.08921459 -0.05088849  0.09756008  0.4491359 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(1.2847e-05, grad_fn=<MseLossBackward0>)
Current timestep = 487. State = [[-0.28478912  0.0336658 ]]. Action = [[-0.22179852 -0.1553001  -0.00443748  0.961478  ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(4.7072e-05, grad_fn=<MseLossBackward0>)
Current timestep = 488. State = [[-0.28616858  0.0327654 ]]. Action = [[ 0.21629137 -0.11143607  0.01060665  0.6869067 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(1.3166e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 1
Current timestep = 489. State = [[-0.28480816  0.02966863]]. Action = [[ 0.01405969 -0.113989    0.10016114  0.5865667 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(9.1452e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of 1
Current timestep = 490. State = [[-0.2835274   0.02628092]]. Action = [[ 0.05712506  0.16705048 -0.14493382 -0.68982005]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.28131554  0.02672691]]. Action = [[ 0.19162732  0.1684669  -0.08498499  0.8775747 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(4.6683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.27608964  0.02841005]]. Action = [[-0.1802317  -0.20027347 -0.22735536  0.51352787]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of 1
Current timestep = 493. State = [[-0.27607116  0.02786493]]. Action = [[-0.21001951 -0.11673853  0.0507015   0.5596671 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of 1
Current timestep = 494. State = [[-0.2779472   0.02543602]]. Action = [[ 0.04272631 -0.08655721  0.03562286  0.22847307]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(9.3366e-05, grad_fn=<MseLossBackward0>)
Current timestep = 495. State = [[-0.27817157  0.02236943]]. Action = [[ 0.13266343  0.17325276  0.09901977 -0.9661099 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.27790707  0.02225251]]. Action = [[-0.15690207 -0.02759428 -0.16120587  0.05129433]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.27813947  0.0221605 ]]. Action = [[-0.1905307  -0.1511292   0.19279149 -0.6383516 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[-0.2816361   0.01977681]]. Action = [[ 0.11771515  0.18161488 -0.00732666  0.2972672 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(6.5093e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of 1
Current timestep = 499. State = [[-0.2825989   0.02116617]]. Action = [[-0.04706246  0.21486574 -0.17255601  0.67114687]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(4.9711e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.28476837  0.02498342]]. Action = [[-0.23406374 -0.14666961 -0.0426418  -0.02209508]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of 1
Current timestep = 501. State = [[-0.29049346  0.0259325 ]]. Action = [[-0.23098685  0.15608692  0.24154949  0.29552543]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(6.1739e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of 1
Current timestep = 502. State = [[-0.30119434  0.02845402]]. Action = [[-0.09897059 -0.12113731 -0.03628394  0.9684974 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(2.3545e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.31047776  0.02877191]]. Action = [[ 0.01107225  0.10294113 -0.0992524  -0.05729985]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(9.3175e-05, grad_fn=<MseLossBackward0>)
Current timestep = 504. State = [[-0.31481472  0.03086586]]. Action = [[ 0.19382715  0.00594088 -0.07635424  0.9091891 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(5.6887e-05, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.31472552  0.03217622]]. Action = [[-0.05596289  0.15703815  0.17747933  0.21045709]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(8.5616e-05, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.3164596   0.03645436]]. Action = [[-0.20140895  0.18655497  0.2099812   0.84221506]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(8.2455e-05, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.32025105  0.04418296]]. Action = [[ 0.21239397  0.2006033  -0.18370621  0.0128963 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 508. State = [[-0.32128406  0.05431428]]. Action = [[ 0.23737675  0.08209801 -0.10571015  0.7577435 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.31593603  0.06292774]]. Action = [[0.16595703 0.02463177 0.09366366 0.03967571]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.30800784  0.06873934]]. Action = [[0.17951131 0.19279921 0.17853463 0.20296073]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.297919    0.07687253]]. Action = [[ 0.24691021 -0.21561418 -0.14406888  0.1888938 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.2867609   0.07822361]]. Action = [[-0.22556794 -0.18894064 -0.199354   -0.36027545]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.28483704  0.07590578]]. Action = [[ 0.17167163 -0.08972988  0.07331151 -0.11337841]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(3.3582e-05, grad_fn=<MseLossBackward0>)
Current timestep = 514. State = [[-0.28176883  0.07256538]]. Action = [[ 0.0088127   0.11891642  0.08761936 -0.05052513]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(4.6959e-05, grad_fn=<MseLossBackward0>)
Current timestep = 515. State = [[-0.2793353   0.07252056]]. Action = [[-0.23888917 -0.15733147 -0.16800092 -0.3266471 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.2798881   0.07045019]]. Action = [[-0.17725848  0.04920247  0.23624942  0.09135115]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(2.5862e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of 1
Current timestep = 517. State = [[-0.28320777  0.06945127]]. Action = [[-0.2172178  -0.24138731 -0.12305406  0.6896244 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(2.5684e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.29103124  0.06350562]]. Action = [[-0.18289539 -0.20559499  0.008995    0.04083073]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(2.2969e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of 1
Current timestep = 519. State = [[-0.29936305  0.05521498]]. Action = [[ 0.22032332  0.14545527 -0.19888635 -0.16537672]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(7.0008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of 1
Current timestep = 520. State = [[-0.30060405  0.05272622]]. Action = [[ 0.2070795   0.10241067  0.01054147 -0.321383  ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(3.3518e-05, grad_fn=<MseLossBackward0>)
Current timestep = 521. State = [[-0.29835057  0.05308772]]. Action = [[ 0.13782403  0.05046773 -0.01128109  0.2536657 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 522. State = [[-0.29447335  0.0535826 ]]. Action = [[-0.17452247 -0.13351943  0.09278217  0.59073627]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(1.5883e-05, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.29443794  0.05221713]]. Action = [[ 0.03120497 -0.16578886 -0.05194436 -0.48426694]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 1
Current timestep = 524. State = [[-0.29410428  0.04816548]]. Action = [[0.16591772 0.1357767  0.03179914 0.9079933 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(1.4379e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of 1
Current timestep = 525. State = [[-0.29198408  0.04783817]]. Action = [[-0.170468   -0.22942913  0.19413772  0.31794465]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of 1
Current timestep = 526. State = [[-0.29195935  0.04314312]]. Action = [[ 0.20876157 -0.12904836  0.19998914 -0.5266496 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 527. State = [[-0.28933758  0.0365253 ]]. Action = [[ 0.23081124  0.16859514  0.13391858 -0.12856722]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(5.1240e-05, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.28235662  0.03642083]]. Action = [[ 0.19388425 -0.06910825 -0.24526098 -0.9429171 ]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.27311397  0.03519132]]. Action = [[-0.03597011 -0.16812922  0.15974611  0.99225986]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of 1
Current timestep = 530. State = [[-0.26812288  0.0311634 ]]. Action = [[-0.20162244  0.02690208  0.07460368  0.57211006]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of 1
Current timestep = 531. State = [[-0.2685323   0.02808704]]. Action = [[-0.16582948 -0.03192216  0.01509875  0.85776913]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.27003565  0.02464384]]. Action = [[-0.11428399 -0.20009996  0.0325343   0.73158073]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.2738156   0.01839658]]. Action = [[-0.09090318  0.1125522  -0.07649431 -0.77337605]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of 1
Current timestep = 534. State = [[-0.2804877  0.0141918]]. Action = [[ 0.1928364  -0.22540832 -0.23324274  0.7492627 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(7.9141e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.2801253   0.00912248]]. Action = [[ 0.08962268 -0.07832104  0.24497417 -0.80483663]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.27897897  0.00359348]]. Action = [[-0.00192021 -0.10671535 -0.18730491 -0.3074839 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.2783977  -0.00260528]]. Action = [[ 0.20399934 -0.22311229 -0.12642474 -0.06131405]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of -1
Current timestep = 538. State = [[-0.27255526 -0.01232445]]. Action = [[ 0.24291417 -0.16995336  0.12037557 -0.7649694 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of 1
Current timestep = 539. State = [[-0.26155898 -0.02172342]]. Action = [[ 0.19940558  0.20576954  0.09019715 -0.6398287 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.24876206 -0.0227882 ]]. Action = [[ 2.1942466e-01  2.4158150e-01 -5.8594346e-04 -6.1408335e-01]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 541. State = [[-0.23714666 -0.02091728]]. Action = [[-0.23593518  0.04007456 -0.00417094 -0.47183532]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.2343633  -0.01984039]]. Action = [[-0.07388088  0.02713805  0.07215649  0.8924711 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[-0.2352621  -0.01904172]]. Action = [[-0.24625975  0.08223593  0.16714013  0.00277531]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of 1
Current timestep = 544. State = [[-0.23886438 -0.01643859]]. Action = [[ 0.07142907 -0.02518193 -0.03373137  0.70349216]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of 1
Current timestep = 545. State = [[-0.24008103 -0.01517308]]. Action = [[-0.06665692 -0.21546757 -0.09149796  0.8628783 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 546. State = [[-0.24138995 -0.01763639]]. Action = [[-0.24181247 -0.09112844  0.08057526  0.43570185]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.24719216 -0.02212906]]. Action = [[ 0.16185981 -0.19855118 -0.22682333 -0.51315564]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.24855787 -0.0295101 ]]. Action = [[ 0.05483091 -0.17613244  0.14078963 -0.82171273]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[-0.24971443 -0.03844089]]. Action = [[-0.09476903 -0.02048139  0.02925855  0.5305984 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[-0.25140175 -0.04428319]]. Action = [[-0.18792768 -0.02264236 -0.14375706 -0.6295294 ]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.25440025 -0.04952148]]. Action = [[-0.07024378 -0.23914376  0.12921232  0.46540284]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.25786328 -0.05859337]]. Action = [[-0.15370512 -0.06682464  0.17223948  0.73846316]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.26378688 -0.06756386]]. Action = [[-0.23154585  0.08571863 -0.23179062  0.45054543]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 554. State = [[-0.27220502 -0.07100248]]. Action = [[ 0.09760809 -0.22290264  0.04272071  0.3127631 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 554 of 0
Current timestep = 555. State = [[-0.2783471  -0.07637542]]. Action = [[ 0.01651019  0.22378898 -0.04861976 -0.22290564]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 0
Current timestep = 556. State = [[-0.28165448 -0.0756074 ]]. Action = [[-0.21285872  0.08753905  0.21184939 -0.8511763 ]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of 0
Current timestep = 557. State = [[-0.28726715 -0.07405029]]. Action = [[ 0.01136807 -0.13838749 -0.04660469  0.5649775 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 557 of -1
Current timestep = 558. State = [[-0.29112324 -0.07464711]]. Action = [[-0.0938696  -0.03420496 -0.15088761  0.5378407 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.29478565 -0.07578529]]. Action = [[ 0.10648215 -0.0750923   0.15498471 -0.8624526 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 560. State = [[-0.29509228 -0.07799825]]. Action = [[ 0.18872598 -0.10654721 -0.13834038  0.8275871 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.29392534 -0.08064702]]. Action = [[-0.15135458  0.0696404  -0.15537617  0.16357231]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.29464087 -0.08234355]]. Action = [[ 0.15748999 -0.09447646 -0.00932869  0.20950544]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 562 of -1
Current timestep = 563. State = [[-0.29383135 -0.08395904]]. Action = [[0.04403895 0.02511859 0.11479738 0.04092848]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.2926609 -0.0841185]]. Action = [[ 0.14527595  0.03198352 -0.15275675 -0.83460736]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.2893792  -0.08455057]]. Action = [[ 0.09176201 -0.23018703 -0.05513227 -0.32572246]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.28391674 -0.08749742]]. Action = [[ 0.24114853  0.19655013  0.01797155 -0.04028803]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.27430996 -0.08725543]]. Action = [[0.20193964 0.24817842 0.06216896 0.9078927 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.26241398 -0.08293176]]. Action = [[ 0.21308762  0.05822426 -0.22535254 -0.5614269 ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 568 of 1
Current timestep = 569. State = [[-0.2510723  -0.08019394]]. Action = [[-0.19836548  0.22498667  0.14897117  0.3642192 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.24747713 -0.07537497]]. Action = [[-0.24671482 -0.16930117  0.00748023  0.9619392 ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.24846481 -0.07447764]]. Action = [[ 0.02260816 -0.13883957 -0.0827602  -0.74746734]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 572. State = [[-0.24909046 -0.07512991]]. Action = [[ 0.182118    0.13259336 -0.2111875  -0.93026584]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 573. State = [[-0.24801429 -0.0740998 ]]. Action = [[-0.24879564 -0.1167029  -0.14844358 -0.45563686]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of 1
Current timestep = 574. State = [[-0.24907653 -0.07490937]]. Action = [[-0.12724887  0.12001932  0.01029626 -0.78434634]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[-0.25156063 -0.0738178 ]]. Action = [[ 0.00316018  0.20723894  0.0299221  -0.0539462 ]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of 1
Current timestep = 576. State = [[-0.2542595  -0.06926341]]. Action = [[-0.01292524  0.12227345  0.18325222  0.8671763 ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.2567999  -0.06258522]]. Action = [[-0.10309538  0.08875233 -0.07541853  0.35542846]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, True, False]
State prediction error at timestep 577 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of 1
Current timestep = 578. State = [[-0.26100612 -0.05617633]]. Action = [[-0.23431624 -0.08803669 -0.19274923  0.62730086]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 578 of -1
Current timestep = 579. State = [[-0.26973063 -0.05262541]]. Action = [[-0.1236648   0.14234883  0.11544168 -0.5913067 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, True, False]
State prediction error at timestep 579 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 580. State = [[-0.2781695  -0.04821104]]. Action = [[ 0.14158708 -0.17547801  0.1962111   0.13333082]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 581. State = [[-0.27931842 -0.04838816]]. Action = [[-0.05940728  0.19820487 -0.12571807 -0.97472376]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 582. State = [[-0.28117582 -0.04564076]]. Action = [[-0.2204384   0.15651095  0.03241748 -0.53953815]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, True, False]
State prediction error at timestep 582 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 582 of -1
Current timestep = 583. State = [[-0.28632095 -0.0401182 ]]. Action = [[0.06534794 0.13446745 0.20903584 0.11877906]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.289031   -0.03231726]]. Action = [[ 0.03788364 -0.00854975  0.10777023  0.786252  ]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[-0.2906193  -0.02602896]]. Action = [[-0.23492208  0.10674661 -0.17988726  0.82939315]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(4.8381e-05, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.29517382 -0.01979273]]. Action = [[ 0.17189243 -0.20499606  0.17508525 -0.83803594]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 587. State = [[-0.2960817  -0.01979913]]. Action = [[ 0.15439546 -0.16162685 -0.22379504 -0.27315056]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, True, False]
State prediction error at timestep 587 is tensor(6.7525e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[-0.29505965 -0.02344788]]. Action = [[-0.02635491 -0.2221154   0.13077247  0.85667133]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, True, False]
State prediction error at timestep 588 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.29529634 -0.03004747]]. Action = [[-0.06867009  0.21990138  0.04451838 -0.32660472]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.29581758 -0.03080184]]. Action = [[-0.2172625   0.17283088 -0.21078694 -0.7424872 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, True, False]
State prediction error at timestep 590 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 591. State = [[-0.2989488  -0.02817065]]. Action = [[ 0.08651155 -0.02228057  0.14915785 -0.79422027]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.30009598 -0.02675479]]. Action = [[ 0.09761536 -0.22104725  0.12601545 -0.8604151 ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.30043706 -0.02950588]]. Action = [[-0.08792068 -0.05304199 -0.12468395  0.10819161]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, True, False]
State prediction error at timestep 593 is tensor(3.6005e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.30098772 -0.03275339]]. Action = [[0.11821836 0.16232121 0.00757545 0.9197707 ]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, True, False]
State prediction error at timestep 594 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 595. State = [[-0.300919   -0.03273894]]. Action = [[-0.11611241 -0.22189644 -0.14094687 -0.75626034]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, True, False]
State prediction error at timestep 595 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 596. State = [[-0.3014072  -0.03567214]]. Action = [[-0.1917346  -0.0190789  -0.17727959  0.8402946 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(9.2484e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.30526784 -0.03935877]]. Action = [[-0.18964447 -0.22136408 -0.24006213 -0.1648286 ]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, True, False]
State prediction error at timestep 597 is tensor(3.1768e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.3131865 -0.0458212]]. Action = [[-0.22256124  0.2181516  -0.13196701 -0.27726078]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.32095382 -0.04522624]]. Action = [[-0.01699255  0.01861724  0.0535216  -0.52654994]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(8.5175e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.32521784 -0.04421194]]. Action = [[ 0.06876653 -0.12979451 -0.13873461 -0.90267414]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(5.3316e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.3257675  -0.04485122]]. Action = [[-0.22643577  0.06783321 -0.04139622 -0.7502915 ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(8.5655e-05, grad_fn=<MseLossBackward0>)
Current timestep = 602. State = [[-0.32946545 -0.04537856]]. Action = [[-0.24772985 -0.24117321 -0.00729971 -0.72532314]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(2.2587e-05, grad_fn=<MseLossBackward0>)
Current timestep = 603. State = [[-0.33614245 -0.04908833]]. Action = [[ 0.11955613  0.19224513 -0.05676036  0.24019325]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.3384074  -0.04813259]]. Action = [[ 0.16595024  0.15929475  0.17165494 -0.23907089]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.33861142 -0.04610947]]. Action = [[ 0.06941977 -0.07641882 -0.08793092  0.8225894 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(6.0670e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of -1
Current timestep = 606. State = [[-0.3384302  -0.04554508]]. Action = [[-0.05019833  0.1791929  -0.03381132  0.49388385]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 607. State = [[-0.3385691  -0.04310112]]. Action = [[ 0.12078148 -0.23293215  0.16704088  0.02165389]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(7.5339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 608. State = [[-0.33684072 -0.04359724]]. Action = [[ 0.08246237 -0.16902034  0.07274997 -0.22332615]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(5.9091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.33414507 -0.04579555]]. Action = [[-0.0838072   0.1897259   0.12660718  0.95039344]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(7.5531e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of -1
Current timestep = 610. State = [[-0.3338382  -0.04568535]]. Action = [[-0.09977591  0.0197688  -0.06542367 -0.89406353]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(6.3603e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.3341204  -0.04518919]]. Action = [[-0.10849687  0.11150521  0.11942318 -0.55264294]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(2.1052e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.3357508 -0.043543 ]]. Action = [[ 0.08694941 -0.11013749  0.1091916   0.9010024 ]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, True, False]
State prediction error at timestep 612 is tensor(3.0239e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.33587182 -0.04334394]]. Action = [[0.165595   0.20465392 0.12580222 0.14559412]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 614. State = [[-0.33477056 -0.04133186]]. Action = [[ 0.24761039 -0.12601112  0.06650013 -0.9023384 ]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, True, False]
State prediction error at timestep 614 is tensor(9.0551e-05, grad_fn=<MseLossBackward0>)
Current timestep = 615. State = [[-0.3298689  -0.04185985]]. Action = [[-0.09107903 -0.21584406  0.17755258 -0.8311159 ]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(7.2713e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.32880148 -0.04402075]]. Action = [[ 0.18287498 -0.22592886  0.10554275  0.84272337]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, True, False]
State prediction error at timestep 616 is tensor(3.2170e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of -1
Current timestep = 617. State = [[-0.32455212 -0.04883941]]. Action = [[0.13524973 0.20811445 0.15367824 0.46444654]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(9.4944e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.31982523 -0.04915139]]. Action = [[-0.00334324 -0.21198787  0.01621926  0.29865623]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, True, False]
State prediction error at timestep 618 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 619. State = [[-0.31751946 -0.05223998]]. Action = [[-0.18336007 -0.10370122 -0.11392206  0.9419904 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(8.1629e-05, grad_fn=<MseLossBackward0>)
Current timestep = 620. State = [[-0.31808096 -0.05585127]]. Action = [[-0.0519558   0.00455147 -0.23564763  0.43357491]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(4.3213e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of -1
Current timestep = 621. State = [[-0.3191812  -0.05871524]]. Action = [[-0.07092369 -0.14028814 -0.12220371 -0.5596671 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.3207511  -0.06346605]]. Action = [[-0.06643733 -0.1866277   0.21900946  0.20712173]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.3225163  -0.06974868]]. Action = [[-0.12830432 -0.00419992  0.18902445  0.1432792 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(9.7566e-05, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.32417473 -0.0741695 ]]. Action = [[ 0.2367962  -0.22085634 -0.08958612  0.3695693 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(8.1748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 625. State = [[-0.32192594 -0.08017322]]. Action = [[0.07915181 0.21486449 0.04966232 0.8008535 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(8.0559e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of -1
Current timestep = 626. State = [[-0.31883058 -0.08187925]]. Action = [[ 0.03520343  0.06608868  0.14589733 -0.13756561]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(4.1992e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.3172671  -0.08246005]]. Action = [[-0.22262804  0.00379878  0.01276773  0.8627362 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.318693   -0.08316718]]. Action = [[-0.21422005 -0.12357572 -0.09982251  0.33948934]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 629. State = [[-0.32331687 -0.08597264]]. Action = [[ 0.02706778  0.15221232 -0.01188181 -0.23284835]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(6.7644e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.3253823  -0.08601359]]. Action = [[-0.10123232 -0.21544631 -0.20112556  0.9771631 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(1.5569e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.32813877 -0.0889686 ]]. Action = [[-0.10926601 -0.05824797  0.1434716   0.88696647]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 632. State = [[-0.33148426 -0.09192982]]. Action = [[-0.20501435  0.02142614 -0.1109035   0.46501827]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(7.8764e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.33740732 -0.0944316 ]]. Action = [[-0.04018086 -0.03555484 -0.09849346  0.4568982 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(6.6734e-05, grad_fn=<MseLossBackward0>)
Current timestep = 634. State = [[-0.3413848  -0.09648735]]. Action = [[-0.15945143  0.04296997  0.0918451  -0.12270844]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(3.9550e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.34555176 -0.0975449 ]]. Action = [[ 0.2195029   0.20569032  0.09559321 -0.81597584]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(5.1274e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.34632024 -0.09649178]]. Action = [[ 0.0987497  -0.16561066 -0.22458987 -0.83062625]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(1.1131e-05, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.34594536 -0.09752549]]. Action = [[-0.16637355  0.05975282 -0.00889708  0.82521105]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(4.2165e-05, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.3468158  -0.09760028]]. Action = [[0.14891958 0.1566788  0.02135739 0.7519363 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(1.4631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.34626818 -0.09603512]]. Action = [[ 0.08656496  0.21017122  0.07419908 -0.9293122 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(6.0361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.34500566 -0.09102397]]. Action = [[-0.20203692 -0.20497292  0.1699925   0.5444505 ]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(6.1616e-06, grad_fn=<MseLossBackward0>)
Current timestep = 641. State = [[-0.34636247 -0.09125638]]. Action = [[-0.20768279  0.14259383 -0.18051185 -0.11079001]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(3.9664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of -1
Current timestep = 642. State = [[-0.3509502  -0.08975614]]. Action = [[ 0.22256017  0.23061097  0.23129416 -0.90376437]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[-0.35045433 -0.08642262]]. Action = [[ 0.1799714  -0.14029716 -0.10680568  0.89578176]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(9.8022e-06, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.34882838 -0.08432287]]. Action = [[ 0.11516538 -0.01080714 -0.09702033  0.963115  ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, True, False]
State prediction error at timestep 644 is tensor(1.1362e-05, grad_fn=<MseLossBackward0>)
Current timestep = 645. State = [[-0.34629497 -0.08294506]]. Action = [[-0.16498305 -0.11103547 -0.07507753  0.91117835]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(8.7313e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[-0.34617046 -0.08339795]]. Action = [[-0.23646319  0.03624713  0.17375338  0.90716136]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(5.9347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[-0.34962252 -0.08325367]]. Action = [[-0.13166654  0.03707409 -0.11383757 -0.25198543]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(5.6651e-05, grad_fn=<MseLossBackward0>)
Current timestep = 648. State = [[-0.35295895 -0.08280113]]. Action = [[-0.24623352  0.11301017  0.17324924  0.18863368]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(5.8794e-05, grad_fn=<MseLossBackward0>)
Current timestep = 649. State = [[-0.35966504 -0.08076626]]. Action = [[-0.06101817  0.23035485  0.16619515 -0.9165514 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.36738396 -0.07717493]]. Action = [[ 0.07864398 -0.18443361  0.06894261 -0.20047832]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.3713313  -0.07670831]]. Action = [[ 0.09714958  0.10952857  0.22304893 -0.8546715 ]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 652. State = [[-0.3711544 -0.0746793]]. Action = [[-0.23763482  0.05127788  0.05029336  0.8781915 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, True, False]
State prediction error at timestep 652 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 653. State = [[-0.3710499  -0.07360306]]. Action = [[ 0.18514478 -0.24319345  0.06316292  0.984164  ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.36992243 -0.07492114]]. Action = [[ 0.02447969  0.14798474 -0.17360708  0.70562387]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, True, False]
State prediction error at timestep 654 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 654 of -1
Current timestep = 655. State = [[-0.3691652  -0.07415372]]. Action = [[0.13932055 0.15416527 0.09631658 0.18235898]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of -1
Current timestep = 656. State = [[-0.36687574 -0.07104266]]. Action = [[-0.20181406 -0.17852652 -0.14792705  0.13096428]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.36713538 -0.07096574]]. Action = [[ 0.03434137 -0.15874174 -0.14661291  0.02072537]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, True, False]
State prediction error at timestep 657 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 658. State = [[-0.3669306  -0.07241336]]. Action = [[ 0.08551899  0.10573792  0.15495157 -0.36853957]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.36665308 -0.07251275]]. Action = [[-0.22225153 -0.17920676 -0.0444576  -0.23326397]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.3673701  -0.07514285]]. Action = [[ 0.12397411 -0.18797348  0.20827615  0.46879625]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(8.5102e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of -1
Current timestep = 661. State = [[-0.3672052  -0.08003296]]. Action = [[-0.19877464  0.15399677 -0.14726208  0.55955124]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of -1
Current timestep = 662. State = [[-0.3685076  -0.08138631]]. Action = [[ 0.23851115 -0.16607311  0.03615171  0.66901994]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(6.4368e-05, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.36742297 -0.08471545]]. Action = [[-0.03498274  0.09141999 -0.05732238  0.80900645]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(9.2759e-05, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.3669891  -0.08680896]]. Action = [[-0.01360203 -0.05578972  0.14852971 -0.8635371 ]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 665. State = [[-0.3664414  -0.08884722]]. Action = [[ 0.22973692  0.0385195  -0.15808137  0.9366423 ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of -1
Current timestep = 666. State = [[-0.36339965 -0.09093352]]. Action = [[-0.18943442 -0.20345509 -0.10550317  0.23403406]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(4.7426e-05, grad_fn=<MseLossBackward0>)
Current timestep = 667. State = [[-0.36316383 -0.09447023]]. Action = [[0.13247973 0.0870744  0.06963038 0.36612654]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.36215648 -0.09690023]]. Action = [[ 0.1674957  -0.18773311  0.07534045 -0.3108704 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, False, True, False]
State prediction error at timestep 668 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 669. State = [[-0.35754195 -0.10154292]]. Action = [[ 0.21461317 -0.01918685 -0.02224699 -0.91601974]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of -1
Current timestep = 670. State = [[-0.3501203  -0.10662566]]. Action = [[0.10442793 0.02606189 0.01691395 0.15956342]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(3.5413e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of -1
Current timestep = 671. State = [[-0.34373623 -0.1092472 ]]. Action = [[ 0.03583553  0.22423753 -0.2235412   0.37216735]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(6.9791e-06, grad_fn=<MseLossBackward0>)
Current timestep = 672. State = [[-0.34112856 -0.10911526]]. Action = [[ 0.13312843 -0.17161714  0.15263784 -0.07370663]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(1.9721e-06, grad_fn=<MseLossBackward0>)
Current timestep = 673. State = [[-0.3364193  -0.11070733]]. Action = [[-0.12762606 -0.18608123  0.12077719 -0.93173313]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(5.4819e-05, grad_fn=<MseLossBackward0>)
Current timestep = 674. State = [[-0.33580577 -0.1134569 ]]. Action = [[ 0.09142169 -0.1185042   0.15249336 -0.5690831 ]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, False, True, False]
State prediction error at timestep 674 is tensor(3.0386e-05, grad_fn=<MseLossBackward0>)
Current timestep = 675. State = [[-0.3336132  -0.11737047]]. Action = [[ 0.07647848  0.15888146 -0.05342008  0.4055513 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, False, True, False]
State prediction error at timestep 675 is tensor(2.5682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 676. State = [[-0.3312505  -0.11896615]]. Action = [[ 0.14282069 -0.16630325 -0.2208497   0.45247805]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, False, True, False]
State prediction error at timestep 676 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 677. State = [[-0.32557452 -0.12183452]]. Action = [[ 0.23113853 -0.04290041 -0.13429509 -0.21007878]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 678. State = [[-0.31766236 -0.1247616 ]]. Action = [[ 0.15120122 -0.13656177 -0.1627201   0.24272597]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 679. State = [[-0.30989993 -0.1290652 ]]. Action = [[-0.02085836 -0.05962572 -0.09574954 -0.9105144 ]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, True, False, False]
State prediction error at timestep 679 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.30648863 -0.13286091]]. Action = [[-0.01532133  0.19359612 -0.21825895  0.13281918]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, True, False, False]
State prediction error at timestep 680 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 680 of 1
Current timestep = 681. State = [[-0.30495718 -0.13329406]]. Action = [[-0.05354545 -0.22027327 -0.16342466 -0.90926325]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, True, False, False]
State prediction error at timestep 681 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.30530468 -0.13661551]]. Action = [[-0.08704317 -0.14186944  0.12628144 -0.9694892 ]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [True, False, False, True, False, False]
State prediction error at timestep 682 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.30633566 -0.14203133]]. Action = [[ 0.04846856 -0.18253979 -0.05153912  0.54521966]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [True, False, False, True, False, False]
State prediction error at timestep 683 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of 1
Current timestep = 684. State = [[-0.30548865 -0.1484452 ]]. Action = [[ 0.20189565  0.1842539  -0.09096614  0.5166253 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [True, False, False, True, False, False]
State prediction error at timestep 684 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of 1
Current timestep = 685. State = [[-0.30050224 -0.1499048 ]]. Action = [[-0.07393439  0.06092036 -0.2214839  -0.4183036 ]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [True, False, False, True, False, False]
State prediction error at timestep 685 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.299241   -0.15000884]]. Action = [[-0.19413128  0.0956161  -0.15677877 -0.50141513]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [True, False, False, True, False, False]
State prediction error at timestep 686 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.30058578 -0.14945646]]. Action = [[-0.13770035  0.14440852 -0.21614185 -0.7514567 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [True, False, False, True, False, False]
State prediction error at timestep 687 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 687 of 1
Current timestep = 688. State = [[-0.30367345 -0.1472885 ]]. Action = [[-0.05244115  0.22323996  0.19272527  0.13094842]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [True, False, False, True, False, False]
State prediction error at timestep 688 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[-0.30662832 -0.14326166]]. Action = [[-0.15391524 -0.10176024  0.19095722 -0.9573003 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [True, False, False, True, False, False]
State prediction error at timestep 689 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 690. State = [[-0.31133527 -0.14078894]]. Action = [[ 0.07253262 -0.2273685   0.03816801  0.4976554 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, True, False, False]
State prediction error at timestep 690 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 691. State = [[-0.3126473  -0.14243971]]. Action = [[ 0.08914301  0.04940197 -0.07260591  0.20717788]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, True, False, False]
State prediction error at timestep 691 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 692. State = [[-0.31247565 -0.14256042]]. Action = [[ 0.04123288 -0.00626206  0.08498675 -0.6642379 ]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [True, False, False, True, False, False]
State prediction error at timestep 692 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 693. State = [[-0.3124468  -0.14296569]]. Action = [[-0.22047417 -0.21315517  0.17966819 -0.842846  ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [True, False, False, True, False, False]
State prediction error at timestep 693 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 694. State = [[-0.31494072 -0.14712234]]. Action = [[-0.19006854 -0.00507346 -0.16080593  0.4637661 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [True, False, False, True, False, False]
State prediction error at timestep 694 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 695. State = [[-0.31909782 -0.15042979]]. Action = [[ 0.18022126  0.17993835 -0.21597053  0.21933627]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [True, False, False, True, False, False]
State prediction error at timestep 695 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.31954986 -0.1500862 ]]. Action = [[-0.23686595  0.1833772   0.00538209 -0.37500477]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [True, False, False, True, False, False]
State prediction error at timestep 696 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 696 of 1
Current timestep = 697. State = [[-0.32370898 -0.14735831]]. Action = [[0.10428825 0.1302787  0.14506578 0.31130898]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, True, False, False]
State prediction error at timestep 697 is tensor(6.2936e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 697 of -1
Current timestep = 698. State = [[-0.32496974 -0.143866  ]]. Action = [[-0.23306517 -0.2372347   0.2030046  -0.04710233]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, True, False, False]
State prediction error at timestep 698 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of -1
Current timestep = 699. State = [[-0.32864457 -0.1439123 ]]. Action = [[ 0.18657023  0.01764697 -0.15331097  0.4672935 ]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, True, False, False]
State prediction error at timestep 699 is tensor(9.8009e-05, grad_fn=<MseLossBackward0>)
Current timestep = 700. State = [[-0.3286644  -0.14377244]]. Action = [[ 0.0440242  -0.09818496  0.06253669  0.1335957 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, True, False, False]
State prediction error at timestep 700 is tensor(9.2157e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of -1
Current timestep = 701. State = [[-0.32861665 -0.1441522 ]]. Action = [[-0.00453565 -0.23894025 -0.17400864  0.32940936]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, True, False, False]
State prediction error at timestep 701 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 701 of -1
Current timestep = 702. State = [[-0.3285879  -0.14786656]]. Action = [[ 0.23574132  0.22307172 -0.05283785 -0.1886664 ]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [True, False, False, True, False, False]
State prediction error at timestep 702 is tensor(4.2903e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 702 of -1
Current timestep = 703. State = [[-0.32498893 -0.14850444]]. Action = [[ 0.2215805  -0.02497673  0.13467291 -0.7776544 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [True, False, False, True, False, False]
State prediction error at timestep 703 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.31906193 -0.14876431]]. Action = [[-0.23376319  0.1747359   0.13671726 -0.14734173]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [True, False, False, True, False, False]
State prediction error at timestep 704 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[-0.31922066 -0.14751078]]. Action = [[-0.02655366 -0.0088928   0.21706527 -0.42112672]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, True, False, False]
State prediction error at timestep 705 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of 0
Current timestep = 706. State = [[-0.3194818 -0.1467816]]. Action = [[0.12044245 0.08960393 0.05838591 0.53380513]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, True, False, False]
State prediction error at timestep 706 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 707. State = [[-0.31884322 -0.14480242]]. Action = [[-0.00214985  0.01729783  0.13197276 -0.3816222 ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, True, False, False]
State prediction error at timestep 707 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 707 of 1
Current timestep = 708. State = [[-0.31822813 -0.14354953]]. Action = [[ 0.18846798 -0.12383442 -0.08201715  0.92004335]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, True, False, False]
State prediction error at timestep 708 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 709. State = [[-0.3152553  -0.14376993]]. Action = [[-0.21933565  0.14074361  0.03461361 -0.29307467]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, True, False, False]
State prediction error at timestep 709 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.3154605  -0.14300966]]. Action = [[ 0.1780217   0.02101851 -0.11474761 -0.6328187 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, True, False, False]
State prediction error at timestep 710 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 711. State = [[-0.3146132 -0.1414633]]. Action = [[ 0.12084895  0.20306808 -0.18835932  0.97262526]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, True, False, False]
State prediction error at timestep 711 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 711 of 1
Current timestep = 712. State = [[-0.3118472  -0.13597591]]. Action = [[ 0.22460538  0.08101183  0.06767124 -0.67173404]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, True, False, False]
State prediction error at timestep 712 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of 1
Current timestep = 713. State = [[-0.30592704 -0.13034113]]. Action = [[ 0.12440121  0.01892385 -0.12941942 -0.28600574]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, True, False, False]
State prediction error at timestep 713 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.29980728 -0.12686609]]. Action = [[ 0.05740103 -0.15444241 -0.19087659  0.35627818]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, True, False, False]
State prediction error at timestep 714 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 715. State = [[-0.29310268 -0.12618873]]. Action = [[ 0.11461914 -0.20192264 -0.1282612   0.05891144]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, True, False, False]
State prediction error at timestep 715 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.2865378  -0.12798843]]. Action = [[ 0.20859367  0.00355753  0.01821396 -0.1036061 ]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, True, False, False]
State prediction error at timestep 716 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.27820432 -0.12876016]]. Action = [[-0.09955803  0.03888294  0.00294968 -0.07839113]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, True, False, False]
State prediction error at timestep 717 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.27448556 -0.12923022]]. Action = [[ 0.11973161 -0.22554807 -0.03172348 -0.33206505]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, True, False, False]
State prediction error at timestep 718 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.2690334  -0.13087131]]. Action = [[-0.09814809 -0.2011604  -0.18328846 -0.52531415]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, True, False, False]
State prediction error at timestep 719 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 720. State = [[-0.26710123 -0.13624708]]. Action = [[-0.05381936 -0.23383427 -0.02823874 -0.8909746 ]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, True, False, False]
State prediction error at timestep 720 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of 1
Current timestep = 721. State = [[-0.2678422  -0.14330678]]. Action = [[ 0.1633924   0.1516766  -0.12600367  0.68476486]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [True, False, False, True, False, False]
State prediction error at timestep 721 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 721 of 1
Current timestep = 722. State = [[-0.26509136 -0.14477648]]. Action = [[-0.07148445 -0.17994016 -0.04110585 -0.16619551]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [True, False, False, True, False, False]
State prediction error at timestep 722 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.2635974  -0.14791591]]. Action = [[ 0.12748194  0.08256382  0.05513459 -0.1227079 ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [True, False, False, True, False, False]
State prediction error at timestep 723 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 724. State = [[-0.2602625  -0.14900146]]. Action = [[-0.18453078 -0.15605132 -0.09253381  0.5151129 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [True, False, False, True, False, False]
State prediction error at timestep 724 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 725. State = [[-0.2622223 -0.1531329]]. Action = [[-0.10610285 -0.18512419  0.06311774 -0.88641787]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [True, False, False, True, False, False]
State prediction error at timestep 725 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[-0.2663318  -0.15989384]]. Action = [[ 0.1764983  -0.0579368  -0.21223323  0.6840787 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [True, False, False, True, False, False]
State prediction error at timestep 726 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 726 of 1
Current timestep = 727. State = [[-0.26410472 -0.1653736 ]]. Action = [[ 0.16615826 -0.07536116 -0.10521126 -0.62215686]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [True, False, False, True, False, False]
State prediction error at timestep 727 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 728. State = [[-0.25788152 -0.1696783 ]]. Action = [[-0.03384188  0.05526796  0.22462764  0.7241169 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [True, False, False, True, False, False]
State prediction error at timestep 728 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.2545908  -0.17150249]]. Action = [[-0.1420778   0.22484589  0.10228521  0.85349584]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [True, False, False, True, False, False]
State prediction error at timestep 729 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.25535607 -0.17171794]]. Action = [[-0.03016976 -0.10795978  0.13162047  0.69800806]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [True, False, False, True, False, False]
State prediction error at timestep 730 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of 1
Current timestep = 731. State = [[-0.25693706 -0.1750535 ]]. Action = [[-0.02945797  0.18374845  0.13853261 -0.43767393]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [True, False, False, True, False, False]
State prediction error at timestep 731 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 732. State = [[-0.25711232 -0.17375717]]. Action = [[ 0.19631433  0.17058736 -0.03673166 -0.8797937 ]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [True, False, False, True, False, False]
State prediction error at timestep 732 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 733. State = [[-0.25552073 -0.16850853]]. Action = [[-0.10141601 -0.24252169 -0.21297409  0.8112488 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [True, False, False, True, False, False]
State prediction error at timestep 733 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.25599584 -0.17062552]]. Action = [[-0.0192882   0.09374928 -0.15393727 -0.928474  ]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [True, False, False, True, False, False]
State prediction error at timestep 734 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[-0.2560983  -0.17033885]]. Action = [[ 0.1563335  -0.06572773 -0.19572179 -0.1641134 ]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [True, False, False, True, False, False]
State prediction error at timestep 735 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 736. State = [[-0.2544005  -0.17124248]]. Action = [[ 0.09201318 -0.15785645 -0.11025137 -0.6760879 ]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [True, False, False, True, False, False]
State prediction error at timestep 736 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.25070646 -0.17558023]]. Action = [[ 0.20578116 -0.10019875 -0.16769995  0.81987965]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [True, False, False, True, False, False]
State prediction error at timestep 737 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of 1
Current timestep = 738. State = [[-0.24279341 -0.18091328]]. Action = [[-0.11999995  0.05950993 -0.08013678 -0.06588614]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [True, False, False, True, False, False]
State prediction error at timestep 738 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 739. State = [[-0.23886362 -0.18229538]]. Action = [[ 0.09656706  0.19538084  0.13766053 -0.88064075]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [True, False, False, True, False, False]
State prediction error at timestep 739 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 740. State = [[-0.23579174 -0.17845163]]. Action = [[-0.05175276  0.23606643  0.16056454 -0.9051873 ]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [True, False, False, True, False, False]
State prediction error at timestep 740 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.23473755 -0.16987833]]. Action = [[ 0.03583854  0.1156854  -0.00291663  0.348675  ]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [True, False, False, True, False, False]
State prediction error at timestep 741 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.23404823 -0.16254832]]. Action = [[-0.14375769 -0.17368495  0.01183602  0.10631466]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [True, False, False, True, False, False]
State prediction error at timestep 742 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of 1
Current timestep = 743. State = [[-0.23471524 -0.16185662]]. Action = [[-0.07805152  0.2448627   0.17068768  0.13041508]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [True, False, False, True, False, False]
State prediction error at timestep 743 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 744. State = [[-0.23533143 -0.15379228]]. Action = [[-0.0837447  -0.21616688  0.09284329  0.9802418 ]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [True, False, False, True, False, False]
State prediction error at timestep 744 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of 1
Current timestep = 745. State = [[-0.23730133 -0.15625148]]. Action = [[ 0.08804718  0.1819222   0.09577769 -0.6962822 ]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [True, False, False, True, False, False]
State prediction error at timestep 745 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 745 of 1
Current timestep = 746. State = [[-0.23721936 -0.15348427]]. Action = [[ 0.15891314  0.05805194  0.18758696 -0.8672144 ]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [True, False, False, True, False, False]
State prediction error at timestep 746 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 747. State = [[-0.23588994 -0.1498595 ]]. Action = [[ 0.2158516  -0.09721017  0.10115784 -0.21975964]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [True, False, False, True, False, False]
State prediction error at timestep 747 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 748. State = [[-0.23107748 -0.1501111 ]]. Action = [[-0.15471812 -0.128089    0.04297724 -0.09687304]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, True, False, False]
State prediction error at timestep 748 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of 1
Current timestep = 749. State = [[-0.23051329 -0.15227443]]. Action = [[-0.24585538  0.1812954  -0.10238951  0.04667187]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [True, False, False, True, False, False]
State prediction error at timestep 749 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[-0.23222716 -0.15006042]]. Action = [[ 0.1969012   0.21437377  0.02625462 -0.9565103 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, True, False, False]
State prediction error at timestep 750 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.23175216 -0.14320251]]. Action = [[ 0.13254493 -0.22513136 -0.12516706  0.4537325 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, True, False, False]
State prediction error at timestep 751 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of 1
Current timestep = 752. State = [[-0.22980665 -0.1439261 ]]. Action = [[-0.21813628  0.05843219 -0.02136384 -0.8074438 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, True, False, False]
State prediction error at timestep 752 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.22996336 -0.1441154 ]]. Action = [[ 0.14020932 -0.15384044 -0.07216293 -0.5261564 ]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, True, False, False]
State prediction error at timestep 753 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-0.22985233 -0.14617836]]. Action = [[ 0.17119637  0.21077204  0.10034031 -0.2599504 ]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, True, False, False]
State prediction error at timestep 754 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 755. State = [[-0.22627091 -0.14316255]]. Action = [[-0.19526878  0.14778885 -0.06046547 -0.7108299 ]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, True, False, False]
State prediction error at timestep 755 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of 1
Current timestep = 756. State = [[-0.22637957 -0.13769591]]. Action = [[-0.10609141  0.17870378  0.01274908  0.20317161]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, True, False, False]
State prediction error at timestep 756 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 756 of 1
Current timestep = 757. State = [[-0.22715725 -0.12914693]]. Action = [[ 0.0392395   0.12803811 -0.02927272  0.459347  ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, True, False, False]
State prediction error at timestep 757 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of 1
Current timestep = 758. State = [[-0.22785804 -0.1202192 ]]. Action = [[ 0.0902251   0.07193202 -0.09285846  0.51772666]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, False, True, False]
State prediction error at timestep 758 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.22851163 -0.11234546]]. Action = [[ 0.00329313  0.14577836 -0.17627394  0.47151315]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, False, True, False]
State prediction error at timestep 759 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 760. State = [[-0.22882447 -0.1037695 ]]. Action = [[ 0.06276172  0.03891119 -0.20772433 -0.44727385]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, False, True, False]
State prediction error at timestep 760 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.2282778  -0.09764659]]. Action = [[0.0668084  0.19510269 0.12219128 0.73675835]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, False, True, False]
State prediction error at timestep 761 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[-0.22579587 -0.08853017]]. Action = [[ 0.21123332 -0.00209408  0.07272983 -0.96339077]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, False, True, False]
State prediction error at timestep 762 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of 1
Current timestep = 763. State = [[-0.21944536 -0.08330157]]. Action = [[ 0.09805378 -0.13828579  0.18843296 -0.9470948 ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, False, True, False]
State prediction error at timestep 763 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.21184832 -0.08384188]]. Action = [[ 0.24572745 -0.22512974  0.23332384 -0.78259265]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, False, True, False]
State prediction error at timestep 764 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.20000611 -0.08826093]]. Action = [[ 0.18478993  0.24328381 -0.1845748  -0.3839357 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, False, True, False]
State prediction error at timestep 765 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 766. State = [[-0.18739395 -0.08560005]]. Action = [[ 0.03086263  0.06383261 -0.0612314  -0.139947  ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.17805825 -0.08319129]]. Action = [[-0.09388727 -0.22546998 -0.16654797  0.25681877]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, False, True, False]
State prediction error at timestep 767 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of 1
Current timestep = 768. State = [[-0.17374295 -0.0869366 ]]. Action = [[ 0.04223573 -0.14090112 -0.04215091 -0.80552006]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, False, True, False]
State prediction error at timestep 768 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 1
Current timestep = 769. State = [[-0.17242908 -0.09217046]]. Action = [[-0.2122282   0.07546729 -0.01098189  0.8010156 ]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, False, True, False]
State prediction error at timestep 769 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[-0.17296445 -0.09351186]]. Action = [[ 0.14424807  0.19511014 -0.08591361  0.5883362 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, False, True, False]
State prediction error at timestep 770 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[-0.17279895 -0.08955415]]. Action = [[ 0.22463426 -0.01877597  0.2118161  -0.39377236]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[-0.16783674 -0.08848663]]. Action = [[-0.0458346  -0.06568661  0.0281556  -0.2490319 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 1
Current timestep = 773. State = [[-0.1640977  -0.08836218]]. Action = [[ 0.02404183  0.20403355 -0.2144589  -0.7499003 ]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, False, True, False]
State prediction error at timestep 773 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 773 of 1
Current timestep = 774. State = [[-0.1616443  -0.08364535]]. Action = [[-2.2662249e-01  2.4535012e-01  5.1397085e-04 -6.7179292e-01]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, False, True, False]
State prediction error at timestep 774 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[-0.16375503 -0.07229269]]. Action = [[-0.12505083  0.23900169 -0.14509334 -0.32292116]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, False, True, False]
State prediction error at timestep 775 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[-0.16641977 -0.06018241]]. Action = [[-0.1245884  -0.15567856  0.1057274  -0.26187205]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, False, True, False]
State prediction error at timestep 776 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 776 of 1
Current timestep = 777. State = [[-0.17008092 -0.05686484]]. Action = [[-0.08383656  0.0353837   0.04871473  0.01096356]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, False, True, False]
State prediction error at timestep 777 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 1
Current timestep = 778. State = [[-0.17346239 -0.05476969]]. Action = [[ 0.16472554 -0.11318481  0.2161791  -0.00214761]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, False, True, False]
State prediction error at timestep 778 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of 1
Current timestep = 779. State = [[-0.17375639 -0.05559491]]. Action = [[ 0.15492153  0.11274439 -0.08537658  0.9095768 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.17231433 -0.05428438]]. Action = [[ 0.11463329 -0.15521534 -0.07459979 -0.43819034]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, False, True, False]
State prediction error at timestep 780 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 781. State = [[-0.16911298 -0.05561074]]. Action = [[-0.1478182  -0.04573703 -0.14129345 -0.624937  ]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, False, True, False]
State prediction error at timestep 781 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 781 of 1
Current timestep = 782. State = [[-0.16938367 -0.05804861]]. Action = [[-0.19881292 -0.0486421   0.20782295  0.00024688]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, False, True, False]
State prediction error at timestep 782 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[-0.17183135 -0.06286116]]. Action = [[ 0.06171951 -0.23206499 -0.2396845   0.94677067]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, False, True, False]
State prediction error at timestep 783 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 783 of 1
Current timestep = 784. State = [[-0.17315683 -0.07169653]]. Action = [[-0.15058789 -0.24723387 -0.09025335  0.13651717]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, False, True, False]
State prediction error at timestep 784 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of 1
Current timestep = 785. State = [[-0.17667997 -0.08477026]]. Action = [[-0.17670992 -0.23012944 -0.12799558 -0.5981256 ]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, False, True, False]
State prediction error at timestep 785 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[-0.18366846 -0.09826964]]. Action = [[ 0.06232226 -0.11242887  0.05129671 -0.22538006]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, False, True, False]
State prediction error at timestep 786 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 787. State = [[-0.1882257  -0.11002801]]. Action = [[0.04374063 0.05217502 0.06203684 0.80246663]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[-0.18895409 -0.11413135]]. Action = [[ 0.03283122  0.02955952 -0.13132286  0.62725997]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, False, True, False]
State prediction error at timestep 788 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 788 of 1
Current timestep = 789. State = [[-0.18881817 -0.11558034]]. Action = [[-0.14819744 -0.03998706  0.12917808  0.9125905 ]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, False, True, False]
State prediction error at timestep 789 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of 1
Current timestep = 790. State = [[-0.19202931 -0.11842208]]. Action = [[0.0500083  0.01463893 0.23930252 0.870111  ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, False, True, False]
State prediction error at timestep 790 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.19338821 -0.11919864]]. Action = [[-0.22933438  0.22678426  0.1718077   0.7799871 ]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, False, True, False]
State prediction error at timestep 791 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[-0.19875932 -0.11496353]]. Action = [[-0.00396156  0.03348005  0.16099298  0.06566381]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, False, True, False]
State prediction error at timestep 792 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 793. State = [[-0.20208488 -0.11132237]]. Action = [[-0.03911506  0.10689065 -0.07928851  0.5196588 ]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, False, True, False]
State prediction error at timestep 793 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 793 of 1
Current timestep = 794. State = [[-0.20410113 -0.10588247]]. Action = [[ 0.04627493  0.2260868  -0.15714082 -0.711094  ]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, False, True, False]
State prediction error at timestep 794 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[-0.20486747 -0.09671336]]. Action = [[ 0.18029088 -0.20092557 -0.18351144 -0.9526752 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, False, True, False]
State prediction error at timestep 795 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 796. State = [[-0.20347893 -0.09581742]]. Action = [[ 0.10435802  0.15410966 -0.15685798 -0.6537747 ]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, False, True, False]
State prediction error at timestep 796 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[-0.20184699 -0.09236481]]. Action = [[-0.10331431 -0.08546861 -0.09353904 -0.6696178 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, False, True, False]
State prediction error at timestep 797 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 797 of 1
Current timestep = 798. State = [[-0.20193295 -0.09253943]]. Action = [[-0.10636014 -0.14550608  0.2127881   0.44223952]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, False, True, False]
State prediction error at timestep 798 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.20270725 -0.09624236]]. Action = [[ 0.2056185  -0.22626348 -0.22826943 -0.9140535 ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, False, True, False]
State prediction error at timestep 799 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[-0.20093529 -0.10322591]]. Action = [[-0.12372789  0.11355639 -0.06689627  0.16328561]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, False, True, False]
State prediction error at timestep 800 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 801. State = [[-0.20060875 -0.10423147]]. Action = [[ 0.07908386  0.09738129 -0.0618978   0.33601022]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, False, True, False]
State prediction error at timestep 801 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.20048907 -0.10326513]]. Action = [[ 0.00827205 -0.0777092  -0.21226984 -0.0589276 ]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, False, True, False]
State prediction error at timestep 802 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of 1
Current timestep = 803. State = [[-0.20057632 -0.10351861]]. Action = [[-0.20568669  0.05019182 -0.1111095   0.11005127]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, False, True, False]
State prediction error at timestep 803 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of 1
Current timestep = 804. State = [[-0.20236465 -0.1033235 ]]. Action = [[-0.14898388  0.05443364 -0.22201265  0.81866455]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, False, True, False]
State prediction error at timestep 804 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[-0.20553629 -0.10297953]]. Action = [[-0.21942979 -0.09136817 -0.09888335  0.7644707 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[-0.21515907 -0.10600901]]. Action = [[-0.00334007 -0.24268483  0.12046027  0.00166583]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, False, True, False]
State prediction error at timestep 806 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of 1
Current timestep = 807. State = [[-0.2224707  -0.11303961]]. Action = [[ 0.22082222  0.1373297  -0.22128299 -0.60938483]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, False, True, False]
State prediction error at timestep 807 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of 0
Current timestep = 808. State = [[-0.2222023  -0.11342046]]. Action = [[ 0.09266266  0.10233527  0.05873707 -0.1084249 ]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, False, True, False]
State prediction error at timestep 808 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of -1
Current timestep = 809. State = [[-0.22187112 -0.11143529]]. Action = [[-0.24636818  0.02923548 -0.14401245  0.02450669]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, False, True, False]
State prediction error at timestep 809 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 809 of -1
Current timestep = 810. State = [[-0.22361776 -0.10974696]]. Action = [[ 0.21383062 -0.00910537  0.1723764   0.9519428 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, False, True, False]
State prediction error at timestep 810 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of -1
Current timestep = 811. State = [[-0.22373827 -0.10981324]]. Action = [[-0.18888931 -0.23702939 -0.1708414  -0.5800346 ]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, False, True, False]
State prediction error at timestep 811 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[-0.22524653 -0.1143747 ]]. Action = [[-0.11433239  0.04464912 -0.08431546  0.43485177]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[-0.22811313 -0.11564511]]. Action = [[ 0.08168226  0.062612    0.06003025 -0.6579445 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, False, True, False]
State prediction error at timestep 813 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 814. State = [[-0.2283904  -0.11509091]]. Action = [[-0.15153193  0.24306852 -0.2358518  -0.5129493 ]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, False, True, False]
State prediction error at timestep 814 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of -1
Current timestep = 815. State = [[-0.23071776 -0.10951782]]. Action = [[ 0.22416782 -0.21992916 -0.02032028  0.16652286]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of -1
Current timestep = 816. State = [[-0.2300786  -0.10972003]]. Action = [[ 0.11386323  0.24718088  0.11336842 -0.18923414]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, False, True, False]
State prediction error at timestep 816 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of -1
Current timestep = 817. State = [[-0.2281124  -0.10457346]]. Action = [[-0.0793855   0.14174694 -0.08384058 -0.97167766]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 818. State = [[-0.22833553 -0.09687505]]. Action = [[ 0.1132704   0.22058758 -0.11535695 -0.39047992]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, False, True, False]
State prediction error at timestep 818 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[-0.22761126 -0.08700579]]. Action = [[-0.10862264 -0.24755032  0.02133206  0.5988424 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, False, True, False]
State prediction error at timestep 819 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[-0.22722976 -0.08691813]]. Action = [[ 0.16887367 -0.00222044  0.07065505  0.87030995]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, False, True, False]
State prediction error at timestep 820 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of -1
Current timestep = 821. State = [[-0.22580045 -0.08657493]]. Action = [[-0.15777944  0.20313936 -0.18452801 -0.86299556]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, False, True, False]
State prediction error at timestep 821 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[-0.22616355 -0.08239327]]. Action = [[-0.11720213 -0.17463471  0.0064263  -0.8781262 ]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, False, True, False]
State prediction error at timestep 822 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 822 of -1
Current timestep = 823. State = [[-0.22816128 -0.08351225]]. Action = [[-0.16555613 -0.09358115 -0.13266066 -0.26714242]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, False, True, False]
State prediction error at timestep 823 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 824. State = [[-0.23233335 -0.08716895]]. Action = [[-0.1727306  -0.16009875  0.21726298 -0.03861201]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, False, True, False]
State prediction error at timestep 824 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 825. State = [[-0.23915696 -0.09352151]]. Action = [[-0.18774642  0.22797993  0.06460887 -0.09399766]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, False, True, False]
State prediction error at timestep 825 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[-0.24850604 -0.09156894]]. Action = [[ 0.21341589 -0.043134   -0.17340364  0.6287451 ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, False, True, False]
State prediction error at timestep 826 is tensor(6.3467e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of -1
Current timestep = 827. State = [[-0.24958958 -0.09165715]]. Action = [[ 0.21023953 -0.22440654 -0.152677    0.13748395]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, False, True, False]
State prediction error at timestep 827 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 827 of -1
Current timestep = 828. State = [[-0.24757381 -0.09637576]]. Action = [[ 0.0693872  -0.13610753 -0.02112573  0.42235255]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, False, True, False]
State prediction error at timestep 828 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[-0.24498087 -0.10287239]]. Action = [[ 0.10041806  0.08968875 -0.12403053 -0.99506825]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, False, True, False]
State prediction error at timestep 829 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 830. State = [[-0.24158847 -0.10433671]]. Action = [[-0.07386753 -0.01907653  0.19630879 -0.11006629]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, False, True, False]
State prediction error at timestep 830 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 831. State = [[-0.24090736 -0.10507262]]. Action = [[-0.03082249 -0.00269899  0.11704874  0.15647757]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, False, True, False]
State prediction error at timestep 831 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[-0.24093436 -0.10618381]]. Action = [[-0.21377611 -0.22640906  0.21131036 -0.6421391 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, False, True, False]
State prediction error at timestep 832 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of -1
Current timestep = 833. State = [[-0.24474862 -0.11550023]]. Action = [[ 0.05355439 -0.12733886 -0.0822714   0.8814888 ]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, False, True, False]
State prediction error at timestep 833 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[-0.24597387 -0.12351947]]. Action = [[-0.2239979  -0.0557933  -0.12177925  0.4060626 ]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, False, True, False]
State prediction error at timestep 834 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 835. State = [[-0.25161603 -0.13050935]]. Action = [[-0.03153451 -0.04360434  0.18945152  0.04376602]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, True, False, False]
State prediction error at timestep 835 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of -1
Current timestep = 836. State = [[-0.25624692 -0.13566075]]. Action = [[0.0816862  0.00099921 0.19426376 0.840009  ]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, True, False, False]
State prediction error at timestep 836 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of -1
Current timestep = 837. State = [[-0.25737223 -0.13852857]]. Action = [[-0.17880805 -0.06620002 -0.21992987 -0.95819724]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, True, False, False]
State prediction error at timestep 837 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[-0.26156703 -0.1428267 ]]. Action = [[ 0.07043439  0.11565831  0.23663819 -0.46425545]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, True, False, False]
State prediction error at timestep 838 is tensor(8.6883e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of -1
Current timestep = 839. State = [[-0.2617313 -0.1421632]]. Action = [[ 0.22646803  0.18539685  0.19091785 -0.69694746]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, True, False, False]
State prediction error at timestep 839 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of -1
Current timestep = 840. State = [[-0.2592627  -0.13687049]]. Action = [[ 0.13894612  0.14747381  0.05589986 -0.18672585]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, True, False, False]
State prediction error at timestep 840 is tensor(5.9419e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of -1
Current timestep = 841. State = [[-0.2554351  -0.12902586]]. Action = [[-0.19840692  0.15394723  0.00306833 -0.49169338]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, True, False, False]
State prediction error at timestep 841 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 842. State = [[-0.2559145 -0.1208021]]. Action = [[-0.15558226  0.13105804 -0.04796401  0.04273915]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, False, True, False]
State prediction error at timestep 842 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of -1
Current timestep = 843. State = [[-0.25860232 -0.11191512]]. Action = [[-0.03653115  0.02787572  0.10354504  0.44534206]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, False, True, False]
State prediction error at timestep 843 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 843 of -1
Current timestep = 844. State = [[-0.26047027 -0.10592667]]. Action = [[ 0.11088926 -0.16323578 -0.21769926  0.14730382]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, False, True, False]
State prediction error at timestep 844 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of -1
Current timestep = 845. State = [[-0.2604516  -0.10626376]]. Action = [[-0.14428844 -0.00965934 -0.12096247 -0.65415454]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, False, True, False]
State prediction error at timestep 845 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 846. State = [[-0.26191214 -0.10750036]]. Action = [[-0.1607491  -0.10539767  0.02474025  0.77763045]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, False, True, False]
State prediction error at timestep 846 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of -1
Current timestep = 847. State = [[-0.2669178  -0.11093578]]. Action = [[-0.11283569  0.21923679  0.11030364  0.5703845 ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, False, True, False]
State prediction error at timestep 847 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of -1
Current timestep = 848. State = [[-0.27304187 -0.10754685]]. Action = [[-0.01519655 -0.22527735 -0.0846743  -0.5524319 ]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, False, True, False]
State prediction error at timestep 848 is tensor(2.3572e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of -1
Current timestep = 849. State = [[-0.27920088 -0.11091208]]. Action = [[ 0.07860175 -0.1575476  -0.03403202 -0.91176295]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, False, True, False]
State prediction error at timestep 849 is tensor(4.9459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[-0.28099212 -0.11779859]]. Action = [[-0.16489232 -0.20918708 -0.16069219 -0.5441851 ]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, False, True, False]
State prediction error at timestep 850 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of -1
Current timestep = 851. State = [[-0.28654617 -0.1280182 ]]. Action = [[0.14892584 0.19914323 0.23519021 0.8402028 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, True, False, False]
State prediction error at timestep 851 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of -1
Current timestep = 852. State = [[-0.28629124 -0.12780206]]. Action = [[-0.17159508  0.18292826  0.15474835 -0.6766696 ]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, True, False, False]
State prediction error at timestep 852 is tensor(3.3035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.28887996 -0.12389733]]. Action = [[-0.16651681 -0.15068513 -0.16435869 -0.02337462]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, False, True, False]
State prediction error at timestep 853 is tensor(4.3083e-05, grad_fn=<MseLossBackward0>)
Current timestep = 854. State = [[-0.29535645 -0.11703144]]. Action = [[ 0.04633224 -0.05034955 -0.19257164  0.59931016]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, False, True, False]
State prediction error at timestep 854 is tensor(4.7462e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 854 of -1
Current timestep = 855. State = [[-0.29808566 -0.11389463]]. Action = [[ 0.0091846   0.14522904 -0.11923993  0.7926426 ]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, False, True, False]
State prediction error at timestep 855 is tensor(3.4571e-06, grad_fn=<MseLossBackward0>)
Current timestep = 856. State = [[-0.29944965 -0.10770211]]. Action = [[-0.06637433 -0.02485377 -0.13234642 -0.8991496 ]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, False, True, False]
State prediction error at timestep 856 is tensor(5.6598e-06, grad_fn=<MseLossBackward0>)
Current timestep = 857. State = [[-0.30120665 -0.10358889]]. Action = [[ 0.13230091  0.07417601 -0.21914947 -0.41176653]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(1.1763e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of -1
Current timestep = 858. State = [[-0.3024765  -0.09878405]]. Action = [[-0.10528132 -0.12865649 -0.23422669 -0.4296131 ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(4.5299e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of -1
Current timestep = 859. State = [[-0.3043614  -0.09757763]]. Action = [[ 0.11490482 -0.13213882  0.01519334  0.7879981 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, False, True, False]
State prediction error at timestep 859 is tensor(8.7813e-05, grad_fn=<MseLossBackward0>)
Current timestep = 860. State = [[-0.30477116 -0.09666051]]. Action = [[ 0.16546315 -0.22467619 -0.05753237 -0.25778854]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, False, True, False]
State prediction error at timestep 860 is tensor(1.3115e-05, grad_fn=<MseLossBackward0>)
Current timestep = 861. State = [[-0.30202472 -0.10254838]]. Action = [[ 0.07346204  0.06239113 -0.06694636  0.01137912]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, False, True, False]
State prediction error at timestep 861 is tensor(8.2706e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of -1
Current timestep = 862. State = [[-0.2999873  -0.10595853]]. Action = [[-0.20564625  0.22642446 -0.13172129 -0.7290753 ]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, False, True, False]
State prediction error at timestep 862 is tensor(4.3140e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 862 of -1
Current timestep = 863. State = [[-0.30101913 -0.103109  ]]. Action = [[ 0.16774136 -0.23980522 -0.16829096 -0.21301913]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, False, True, False]
State prediction error at timestep 863 is tensor(3.4727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of -1
Current timestep = 864. State = [[-0.30004132 -0.10525159]]. Action = [[-0.09193265  0.07299489  0.21593034 -0.304286  ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(1.5647e-05, grad_fn=<MseLossBackward0>)
Current timestep = 865. State = [[-0.2998562  -0.10567228]]. Action = [[-0.13367583 -0.02444133  0.02548587 -0.08770728]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, False, True, False]
State prediction error at timestep 865 is tensor(1.9527e-05, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[-0.30129918 -0.1052336 ]]. Action = [[-0.10996878 -0.03003223  0.06733674  0.14216506]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, False, True, False]
State prediction error at timestep 866 is tensor(1.9215e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of -1
Current timestep = 867. State = [[-0.30417553 -0.10601974]]. Action = [[-0.19430803 -0.19647852  0.01575181  0.5441147 ]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, False, True, False]
State prediction error at timestep 867 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of -1
Current timestep = 868. State = [[-0.30890867 -0.10968658]]. Action = [[-0.05896954 -0.03850552 -0.2151355  -0.1723569 ]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, False, True, False]
State prediction error at timestep 868 is tensor(2.1954e-05, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[-0.31146392 -0.11176938]]. Action = [[ 0.18142661  0.22362414 -0.14895675 -0.71209687]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, False, True, False]
State prediction error at timestep 869 is tensor(3.2266e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of -1
Current timestep = 870. State = [[-0.31137502 -0.1106029 ]]. Action = [[0.20984429 0.15376598 0.20985186 0.98771834]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, False, True, False]
State prediction error at timestep 870 is tensor(4.7060e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of -1
Current timestep = 871. State = [[-0.30917937 -0.10805138]]. Action = [[-0.06923309  0.18204272 -0.08280966  0.29580545]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, False, True, False]
State prediction error at timestep 871 is tensor(2.2818e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 871 of -1
Current timestep = 872. State = [[-0.30941075 -0.10344727]]. Action = [[ 0.05532342  0.07909513 -0.17735793  0.675241  ]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(2.3176e-05, grad_fn=<MseLossBackward0>)
Current timestep = 873. State = [[-0.30984282 -0.09850147]]. Action = [[-0.02742636 -0.17444569  0.16739923 -0.835747  ]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(1.3802e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of -1
Current timestep = 874. State = [[-0.3102568  -0.09459987]]. Action = [[-0.04669812  0.24252343 -0.1769777   0.6565881 ]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, False, True, False]
State prediction error at timestep 874 is tensor(4.5989e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of -1
Current timestep = 875. State = [[-0.3108616  -0.09044737]]. Action = [[ 0.12027904 -0.13915478  0.09567496 -0.8856956 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, False, True, False]
State prediction error at timestep 875 is tensor(1.5637e-05, grad_fn=<MseLossBackward0>)
Current timestep = 876. State = [[-0.30943692 -0.09010793]]. Action = [[-0.14690308 -0.1772559  -0.22746064 -0.8772122 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(1.8290e-05, grad_fn=<MseLossBackward0>)
Current timestep = 877. State = [[-0.31009185 -0.08942181]]. Action = [[ 0.21511304 -0.17408249  0.20951322  0.6097765 ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of -1
Current timestep = 878. State = [[-0.308662   -0.09314367]]. Action = [[-0.09968647 -0.23542404  0.10079545  0.61622274]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of -1
Current timestep = 879. State = [[-0.309332   -0.09916671]]. Action = [[-0.23329213 -0.0163393   0.21887445 -0.76512074]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, False, True, False]
State prediction error at timestep 879 is tensor(6.4684e-05, grad_fn=<MseLossBackward0>)
Current timestep = 880. State = [[-0.31180573 -0.10262503]]. Action = [[-0.0641409  -0.05028203 -0.15197472  0.97688496]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, False, True, False]
State prediction error at timestep 880 is tensor(7.4531e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of -1
Current timestep = 881. State = [[-0.3134434  -0.10440285]]. Action = [[ 0.08581024 -0.00781435  0.03821069 -0.6404366 ]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, False, True, False]
State prediction error at timestep 881 is tensor(3.6063e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of -1
Current timestep = 882. State = [[-0.3143576  -0.10541759]]. Action = [[-0.06516334  0.12772846 -0.06635751  0.2657634 ]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(1.4143e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of -1
Current timestep = 883. State = [[-0.3147595  -0.10504752]]. Action = [[ 0.1489436   0.22719377 -0.21204184  0.6176995 ]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, False, True, False]
State prediction error at timestep 883 is tensor(3.3591e-05, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[-0.3145018  -0.10381441]]. Action = [[-0.06651938  0.12439632  0.2382454  -0.789673  ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, False, True, False]
State prediction error at timestep 884 is tensor(3.9026e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 884 of -1
Current timestep = 885. State = [[-0.31458893 -0.10308863]]. Action = [[ 0.03671521  0.12463427 -0.04507291  0.54216194]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, False, True, False]
State prediction error at timestep 885 is tensor(3.1808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 885 of -1
Current timestep = 886. State = [[-0.31518814 -0.09941966]]. Action = [[ 0.09263289 -0.18905194 -0.17409742  0.9593947 ]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(1.4523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.3141612  -0.09809303]]. Action = [[ 0.10398981  0.04521993 -0.16008298  0.5046122 ]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, False, True, False]
State prediction error at timestep 887 is tensor(4.2874e-05, grad_fn=<MseLossBackward0>)
Current timestep = 888. State = [[-0.31171206 -0.09887271]]. Action = [[-0.00928828 -0.16909593 -0.2002731   0.84477973]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(4.2523e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of -1
Current timestep = 889. State = [[-0.31000516 -0.10021838]]. Action = [[ 0.23984101 -0.2232591   0.18935096  0.44220626]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, False, True, False]
State prediction error at timestep 889 is tensor(8.9314e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 889 of -1
Current timestep = 890. State = [[-0.306199   -0.10505307]]. Action = [[-0.04931739  0.0766837   0.07916546 -0.3781166 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, False, True, False]
State prediction error at timestep 890 is tensor(6.7123e-05, grad_fn=<MseLossBackward0>)
Current timestep = 891. State = [[-0.3042028  -0.10717575]]. Action = [[-0.19672105  0.17085266  0.20877966 -0.83453083]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, False, True, False]
State prediction error at timestep 891 is tensor(2.2659e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 891 of -1
Current timestep = 892. State = [[-0.30615568 -0.10414851]]. Action = [[-0.18502967 -0.10239497 -0.17750491  0.40162098]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, False, True, False]
State prediction error at timestep 892 is tensor(6.0381e-05, grad_fn=<MseLossBackward0>)
Current timestep = 893. State = [[-0.3093728  -0.10281316]]. Action = [[-0.00977679 -0.16536628  0.15097645 -0.1898176 ]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, False, True, False]
State prediction error at timestep 893 is tensor(1.6303e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of 1
Current timestep = 894. State = [[-0.31063062 -0.10351609]]. Action = [[ 0.12436086  0.12799078 -0.07588801 -0.84528977]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(2.0320e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.31027046 -0.10242235]]. Action = [[ 0.22676063 -0.16833583 -0.22281814  0.5798876 ]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(2.7385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 896. State = [[-0.3083832  -0.10493321]]. Action = [[-0.20297538 -0.07794672  0.04317841  0.32958162]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(8.4951e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of 1
Current timestep = 897. State = [[-0.30978367 -0.1056978 ]]. Action = [[-0.14107972  0.03476053  0.11089182  0.3714136 ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(3.0835e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.31139013 -0.10625675]]. Action = [[-0.19616461  0.14526123  0.16361994  0.47320533]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, False, True, False]
State prediction error at timestep 898 is tensor(3.8549e-05, grad_fn=<MseLossBackward0>)
Current timestep = 899. State = [[-0.31495705 -0.105152  ]]. Action = [[-0.23774302  0.22302496  0.00600451 -0.3407247 ]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(3.9440e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of 1
Current timestep = 900. State = [[-0.32209793 -0.10139854]]. Action = [[ 0.00189659  0.19360358 -0.19258724 -0.9206006 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, False, True, False]
State prediction error at timestep 900 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 901. State = [[-0.2597552   0.00829367]]. Action = [[-0.1811937  -0.10362034  0.15788797  0.22853231]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Current timestep = 902. State = [[-0.26098636  0.00535188]]. Action = [[-0.15939286 -0.1551551  -0.16492133  0.89078116]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(3.0207e-05, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[-0.27735433 -0.02601467]]. Action = [[ 0.22020495  0.20924863 -0.07780148  0.17330122]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 904. State = [[-0.30236968 -0.06694726]]. Action = [[ 0.17341    -0.015881    0.11493829 -0.8070525 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 905. State = [[-0.31851745 -0.09473239]]. Action = [[-0.09664246 -0.19935052  0.11246076 -0.5123915 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 906. State = [[-0.32576078 -0.11525086]]. Action = [[-0.0400784  -0.22411638 -0.11564508 -0.7869879 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 906 is [True, False, False, False, True, False]
State prediction error at timestep 906 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 907. State = [[-0.3295499  -0.13269615]]. Action = [[-0.19240585  0.167261   -0.21191713 -0.35229576]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 907 is [True, False, False, True, False, False]
State prediction error at timestep 907 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 908. State = [[-0.3335806  -0.13903615]]. Action = [[-0.05227487  0.22923714  0.11094755 -0.02195889]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 908 is [True, False, False, True, False, False]
State prediction error at timestep 908 is tensor(1.9546e-05, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.33766648 -0.13811265]]. Action = [[-0.12095706 -0.22177806  0.15249449 -0.8621967 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 909 is [True, False, False, True, False, False]
State prediction error at timestep 909 is tensor(3.6321e-05, grad_fn=<MseLossBackward0>)
Current timestep = 910. State = [[-0.3425162  -0.14236608]]. Action = [[ 0.16519874 -0.17813577  0.09524381  0.75639343]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 910 is [True, False, False, True, False, False]
State prediction error at timestep 910 is tensor(5.1277e-05, grad_fn=<MseLossBackward0>)
Current timestep = 911. State = [[-0.34360322 -0.15016967]]. Action = [[-0.1413626  -0.08965343 -0.22862537 -0.17606103]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 911 is [True, False, False, True, False, False]
State prediction error at timestep 911 is tensor(4.4367e-05, grad_fn=<MseLossBackward0>)
Current timestep = 912. State = [[-0.34522778 -0.15753071]]. Action = [[-0.23190175  0.22130227  0.06570935 -0.8250201 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 912 is [True, False, False, True, False, False]
State prediction error at timestep 912 is tensor(3.6352e-06, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[-0.34916812 -0.15806934]]. Action = [[ 0.14219537 -0.15803349 -0.21493138 -0.38047552]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 913 is [True, False, False, True, False, False]
State prediction error at timestep 913 is tensor(1.9160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 914. State = [[-0.35031277 -0.15995799]]. Action = [[0.1321252  0.1716637  0.19684592 0.6995405 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 914 is [True, False, False, True, False, False]
State prediction error at timestep 914 is tensor(8.7046e-05, grad_fn=<MseLossBackward0>)
Current timestep = 915. State = [[-0.35044545 -0.15946351]]. Action = [[-0.21321148  0.07604611 -0.23198363 -0.966514  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 915 is [True, False, False, True, False, False]
State prediction error at timestep 915 is tensor(1.2588e-05, grad_fn=<MseLossBackward0>)
Current timestep = 916. State = [[-0.3525095  -0.15842202]]. Action = [[-0.15384467  0.1800673  -0.12920798  0.03925145]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 916 is [True, False, False, True, False, False]
State prediction error at timestep 916 is tensor(1.0593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 917. State = [[-0.35610965 -0.156108  ]]. Action = [[ 0.22791538  0.19471735 -0.1750353   0.7352891 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 917 is [True, False, False, True, False, False]
State prediction error at timestep 917 is tensor(8.1563e-05, grad_fn=<MseLossBackward0>)
Current timestep = 918. State = [[-0.35483435 -0.15220267]]. Action = [[ 0.23783004  0.09520817 -0.07968776  0.9040961 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 918 is [True, False, False, True, False, False]
State prediction error at timestep 918 is tensor(4.8480e-05, grad_fn=<MseLossBackward0>)
Current timestep = 919. State = [[-0.35179135 -0.14832224]]. Action = [[-0.10747859 -0.10118401 -0.07441142  0.85287976]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 919 is [True, False, False, True, False, False]
State prediction error at timestep 919 is tensor(6.7900e-05, grad_fn=<MseLossBackward0>)
Current timestep = 920. State = [[-0.35121614 -0.14779793]]. Action = [[-0.05696079 -0.21532439  0.03462863  0.60926914]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 920 is [True, False, False, True, False, False]
State prediction error at timestep 920 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 921. State = [[-0.35149387 -0.1487045 ]]. Action = [[-0.18228963  0.07217669 -0.15864882 -0.9001737 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 921 is [True, False, False, True, False, False]
State prediction error at timestep 921 is tensor(5.8779e-06, grad_fn=<MseLossBackward0>)
Current timestep = 922. State = [[-0.3541331  -0.14902784]]. Action = [[ 0.04227364 -0.17817187 -0.23748402 -0.97091264]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 922 is [True, False, False, True, False, False]
State prediction error at timestep 922 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 923. State = [[-0.35550416 -0.15122671]]. Action = [[-0.23315984  0.05089375  0.129201   -0.28692138]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 923 is [True, False, False, True, False, False]
State prediction error at timestep 923 is tensor(4.5461e-06, grad_fn=<MseLossBackward0>)
Current timestep = 924. State = [[-0.3597337  -0.15284945]]. Action = [[-0.20681965  0.04647312  0.01050767 -0.6508985 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 924 is [True, False, False, True, False, False]
State prediction error at timestep 924 is tensor(8.4832e-06, grad_fn=<MseLossBackward0>)
Current timestep = 925. State = [[-0.36635405 -0.15331708]]. Action = [[-0.19144487  0.14969385 -0.233603    0.8474333 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 925 is [True, False, False, True, False, False]
State prediction error at timestep 925 is tensor(2.7276e-05, grad_fn=<MseLossBackward0>)
Current timestep = 926. State = [[-0.3749614  -0.15133072]]. Action = [[ 0.18296361 -0.20504776 -0.07334462 -0.8248245 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 926 is [True, False, False, True, False, False]
State prediction error at timestep 926 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 927. State = [[-0.37738475 -0.15306355]]. Action = [[-0.22115538  0.06946713  0.18221918  0.4145751 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 927 is [True, False, False, True, False, False]
State prediction error at timestep 927 is tensor(1.3866e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of -1
Current timestep = 928. State = [[-0.37829092 -0.15661691]]. Action = [[-0.11045277 -0.17399782 -0.13871439 -0.08546257]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 928 is [True, False, False, True, False, False]
State prediction error at timestep 928 is tensor(2.4061e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 928 of -1
Current timestep = 929. State = [[-0.3806989  -0.16242479]]. Action = [[-0.05080026 -0.04041052 -0.08429614  0.8512187 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 929 is [True, False, False, True, False, False]
State prediction error at timestep 929 is tensor(4.6748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[-0.38268042 -0.16867197]]. Action = [[-0.0172423  -0.14385478  0.16005373 -0.8202105 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 930 is [True, False, False, True, False, False]
State prediction error at timestep 930 is tensor(3.4071e-05, grad_fn=<MseLossBackward0>)
Current timestep = 931. State = [[-0.38457575 -0.1764935 ]]. Action = [[ 0.0829871  -0.05206676 -0.1073488   0.1199187 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 931 is [True, False, False, True, False, False]
State prediction error at timestep 931 is tensor(4.2329e-05, grad_fn=<MseLossBackward0>)
Current timestep = 932. State = [[-0.38551402 -0.18420905]]. Action = [[ 0.09048834  0.00260285 -0.08046345 -0.6462926 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 932 is [True, False, False, True, False, False]
State prediction error at timestep 932 is tensor(7.8994e-06, grad_fn=<MseLossBackward0>)
Current timestep = 933. State = [[-0.38636595 -0.1901767 ]]. Action = [[-0.11304075  0.2277661   0.12879899  0.086622  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 933 is [True, False, False, True, False, False]
State prediction error at timestep 933 is tensor(1.5249e-05, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[-0.3870051  -0.19455585]]. Action = [[-0.12058264  0.15695602 -0.21087886 -0.28183442]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 934 is [True, False, False, True, False, False]
State prediction error at timestep 934 is tensor(3.3518e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of -1
Current timestep = 935. State = [[-0.38767305 -0.19823287]]. Action = [[-0.05244461 -0.19134474 -0.12481304  0.4301709 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 935 is [True, False, False, True, False, False]
State prediction error at timestep 935 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 935 of -1
Current timestep = 936. State = [[-0.38798708 -0.20104906]]. Action = [[ 0.2133758  -0.09764373  0.18042481 -0.3793661 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 936 is [True, False, False, True, False, False]
State prediction error at timestep 936 is tensor(3.1674e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of -1
Current timestep = 937. State = [[-0.38708863 -0.20289539]]. Action = [[ 0.18020764 -0.069399    0.16536376  0.00474501]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 937 is [True, False, False, True, False, False]
State prediction error at timestep 937 is tensor(2.1742e-06, grad_fn=<MseLossBackward0>)
Current timestep = 938. State = [[-0.38317662 -0.20488471]]. Action = [[ 0.23206419 -0.08047326  0.05978438 -0.14554489]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 938 is [True, False, False, True, False, False]
State prediction error at timestep 938 is tensor(2.0426e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of -1
Current timestep = 939. State = [[-0.37704432 -0.20728353]]. Action = [[ 0.11244655  0.11261433 -0.17561653  0.11679339]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 939 is [True, False, False, True, False, False]
State prediction error at timestep 939 is tensor(6.6616e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of -1
Current timestep = 940. State = [[-0.37167245 -0.20685801]]. Action = [[ 0.1324225  -0.1787651   0.07370603 -0.38659722]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 940 is [True, False, False, True, False, False]
State prediction error at timestep 940 is tensor(3.1276e-05, grad_fn=<MseLossBackward0>)
Current timestep = 941. State = [[-0.36606684 -0.20841435]]. Action = [[ 0.10120577 -0.19019198 -0.04405029  0.71981144]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 941 is [True, False, False, True, False, False]
State prediction error at timestep 941 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 942. State = [[-0.35968167 -0.21230182]]. Action = [[-0.09119895 -0.10186884 -0.215673   -0.7728911 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 942 is [True, False, False, True, False, False]
State prediction error at timestep 942 is tensor(1.3307e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 942 of 1
Current timestep = 943. State = [[-0.3566464  -0.21586633]]. Action = [[-0.23753342 -0.03413308 -0.16456978 -0.5011577 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 943 is [True, False, False, True, False, False]
State prediction error at timestep 943 is tensor(2.4808e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of 1
Current timestep = 944. State = [[-0.35872793 -0.22043727]]. Action = [[-0.07388306  0.14386314  0.08597136  0.36794734]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 944 is [True, False, False, True, False, False]
State prediction error at timestep 944 is tensor(1.2185e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of 1
Current timestep = 945. State = [[-0.36078355 -0.22178692]]. Action = [[-0.03851315  0.05632287 -0.09753215 -0.07069325]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 945 is [True, False, False, True, False, False]
State prediction error at timestep 945 is tensor(2.4221e-05, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[-0.36222395 -0.22180952]]. Action = [[-0.14639398 -0.18472752  0.07986262  0.85732245]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 946 is [True, False, False, True, False, False]
State prediction error at timestep 946 is tensor(6.7741e-06, grad_fn=<MseLossBackward0>)
Current timestep = 947. State = [[-0.36609823 -0.225586  ]]. Action = [[-0.24354424 -0.09582075  0.20531106 -0.48692   ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 947 is [True, False, False, True, False, False]
State prediction error at timestep 947 is tensor(3.7381e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of 1
Current timestep = 948. State = [[-0.37358704 -0.2319393 ]]. Action = [[-0.19903754 -0.15894422  0.07843599 -0.1208908 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 948 is [True, False, False, True, False, False]
State prediction error at timestep 948 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of 1
Current timestep = 949. State = [[-0.38191843 -0.24179779]]. Action = [[ 0.14312577 -0.15949918  0.22967142 -0.85413665]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 949 is [True, False, False, True, False, False]
State prediction error at timestep 949 is tensor(3.7302e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 949 of -1
Current timestep = 950. State = [[-0.38515705 -0.25123703]]. Action = [[-0.091371   -0.18898343 -0.17483969 -0.14529377]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 950 is [True, False, False, True, False, False]
State prediction error at timestep 950 is tensor(8.8504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 951. State = [[-0.3880994  -0.25958675]]. Action = [[-0.21289869  0.06879258 -0.06842147 -0.10996515]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 951 is [True, False, False, True, False, False]
State prediction error at timestep 951 is tensor(9.9098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 952. State = [[-0.3907185  -0.26592174]]. Action = [[-0.18503721 -0.03373076  0.22291797  0.6287074 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 952 is [True, False, False, True, False, False]
State prediction error at timestep 952 is tensor(5.4533e-05, grad_fn=<MseLossBackward0>)
Current timestep = 953. State = [[-0.3925998  -0.27082366]]. Action = [[-0.19583468 -0.09340766 -0.14199558  0.36524272]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 953 is [True, False, False, True, False, False]
State prediction error at timestep 953 is tensor(6.4780e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 953 of -1
Current timestep = 954. State = [[-0.39374974 -0.27379936]]. Action = [[ 0.01104942 -0.15831184 -0.1299228   0.567929  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 954 is [True, False, False, True, False, False]
State prediction error at timestep 954 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of -1
Current timestep = 955. State = [[-0.39442578 -0.2751349 ]]. Action = [[ 0.09346223 -0.05089769 -0.08704886  0.34015286]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 955 is [True, False, False, True, False, False]
State prediction error at timestep 955 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 955 of -1
Current timestep = 956. State = [[-0.3943613  -0.27561837]]. Action = [[-0.22714879  0.09107059  0.09325689  0.85416794]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 956 is [True, False, False, True, False, False]
State prediction error at timestep 956 is tensor(9.2733e-05, grad_fn=<MseLossBackward0>)
Current timestep = 957. State = [[-0.39431584 -0.27577198]]. Action = [[-0.19485931  0.21828234  0.01012582  0.95029235]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 957 is [True, False, False, True, False, False]
State prediction error at timestep 957 is tensor(8.0021e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of -1
Current timestep = 958. State = [[-0.3941988  -0.27587962]]. Action = [[ 0.14690828 -0.08540806 -0.02833882 -0.34159982]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 958 is [True, False, False, True, False, False]
State prediction error at timestep 958 is tensor(1.8534e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 958 of -1
Current timestep = 959. State = [[-0.39186496 -0.27763513]]. Action = [[ 0.15763503 -0.11179671 -0.038058    0.59217846]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 959 is [True, False, False, True, False, False]
State prediction error at timestep 959 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[-0.3879264  -0.28025687]]. Action = [[ 0.02971148 -0.20933689  0.03246796 -0.42189336]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 960 is [True, False, False, True, False, False]
State prediction error at timestep 960 is tensor(3.1667e-05, grad_fn=<MseLossBackward0>)
Current timestep = 961. State = [[-0.38379896 -0.285568  ]]. Action = [[-0.12977882  0.21216816 -0.13241403  0.01067281]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 961 is [True, False, False, True, False, False]
State prediction error at timestep 961 is tensor(4.6345e-06, grad_fn=<MseLossBackward0>)
Current timestep = 962. State = [[-0.3820817 -0.2886829]]. Action = [[ 0.08999878  0.17287469 -0.14834298 -0.6592874 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 962 is [True, False, False, True, False, False]
State prediction error at timestep 962 is tensor(6.9150e-05, grad_fn=<MseLossBackward0>)
Current timestep = 963. State = [[-0.3801975  -0.28804973]]. Action = [[ 0.20716965 -0.03417346  0.19012123 -0.9333846 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 963 is [True, False, False, True, False, False]
State prediction error at timestep 963 is tensor(3.6728e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 963 of 1
Current timestep = 964. State = [[-0.37496835 -0.28840286]]. Action = [[-0.21412192 -0.20810974 -0.09097481  0.45795953]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 964 is [True, False, False, True, False, False]
State prediction error at timestep 964 is tensor(7.6319e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 1
Current timestep = 965. State = [[-0.37195337 -0.2888253 ]]. Action = [[ 0.09285459  0.09089172 -0.19477245  0.60908675]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 965 is [True, False, False, True, False, False]
State prediction error at timestep 965 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 966. State = [[-0.3696609  -0.28796443]]. Action = [[ 0.01270175  0.24116275 -0.01275988 -0.4412768 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 966 is [True, False, False, True, False, False]
State prediction error at timestep 966 is tensor(9.1107e-05, grad_fn=<MseLossBackward0>)
Current timestep = 967. State = [[-0.36681253 -0.28476375]]. Action = [[ 0.05539042 -0.16811569 -0.19449148  0.02829266]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 967 is [True, False, False, True, False, False]
State prediction error at timestep 967 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 968. State = [[-0.3634845  -0.28357318]]. Action = [[ 0.12899914  0.00989169 -0.20950717  0.47708547]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 968 is [True, False, False, True, False, False]
State prediction error at timestep 968 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of 1
Current timestep = 969. State = [[-0.36007836 -0.2827859 ]]. Action = [[-0.05485539 -0.19156335 -0.04830773 -0.7997014 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 969 is [True, False, False, True, False, False]
State prediction error at timestep 969 is tensor(4.1443e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 969 of 1
Current timestep = 970. State = [[-0.3579877 -0.2845275]]. Action = [[-0.00806774 -0.07885492  0.06116572 -0.22692716]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 970 is [True, False, False, True, False, False]
State prediction error at timestep 970 is tensor(4.2486e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.35623214 -0.2863416 ]]. Action = [[ 0.11168939  0.13313061  0.13572949 -0.41178966]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 971 is [True, False, False, True, False, False]
State prediction error at timestep 971 is tensor(2.1722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.35516155 -0.28650296]]. Action = [[ 0.02583909  0.02381364 -0.22654365  0.08540261]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 972 is [True, False, False, True, False, False]
State prediction error at timestep 972 is tensor(2.0019e-05, grad_fn=<MseLossBackward0>)
Current timestep = 973. State = [[-0.35393134 -0.286265  ]]. Action = [[ 0.02831402 -0.19749705  0.02501523 -0.49868357]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 973 is [True, False, False, True, False, False]
State prediction error at timestep 973 is tensor(8.7805e-06, grad_fn=<MseLossBackward0>)
Current timestep = 974. State = [[-0.35147688 -0.28812194]]. Action = [[-0.10696724  0.19395074 -0.17855607  0.9349271 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 974 is [True, False, False, True, False, False]
State prediction error at timestep 974 is tensor(2.6881e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 974 of 1
Current timestep = 975. State = [[-0.3517052  -0.28757042]]. Action = [[-0.24293621  0.1860528   0.05218101  0.69538724]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 975 is [True, False, False, True, False, False]
State prediction error at timestep 975 is tensor(3.7191e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[-0.35567525 -0.2846029 ]]. Action = [[-0.22308475 -0.08766098 -0.18595333  0.18625975]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 976 is [True, False, False, True, False, False]
State prediction error at timestep 976 is tensor(4.8166e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 1
Current timestep = 977. State = [[-0.36017534 -0.28342867]]. Action = [[ 0.01164329  0.2308647  -0.11345601 -0.86173344]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 977 is [True, False, False, True, False, False]
State prediction error at timestep 977 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 978. State = [[-0.36395833 -0.2793731 ]]. Action = [[ 0.18457043  0.07397443 -0.12081161 -0.60397166]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 978 is [True, False, False, True, False, False]
State prediction error at timestep 978 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 979. State = [[-0.36420745 -0.27621174]]. Action = [[-0.11664239 -0.21253598 -0.11996129 -0.2841863 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 979 is [True, False, False, True, False, False]
State prediction error at timestep 979 is tensor(2.6259e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 979 of -1
Current timestep = 980. State = [[-0.3645836 -0.2763826]]. Action = [[ 0.0673539  -0.21629491  0.22931439  0.9118295 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 980 is [True, False, False, True, False, False]
State prediction error at timestep 980 is tensor(1.8975e-05, grad_fn=<MseLossBackward0>)
Current timestep = 981. State = [[-0.36457086 -0.2781337 ]]. Action = [[-0.1953224   0.12934634 -0.23867634 -0.78318924]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 981 is [True, False, False, True, False, False]
State prediction error at timestep 981 is tensor(4.4086e-05, grad_fn=<MseLossBackward0>)
Current timestep = 982. State = [[-0.36670384 -0.2787202 ]]. Action = [[-0.19940503  0.14632821 -0.22200987 -0.75180435]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 982 is [True, False, False, True, False, False]
State prediction error at timestep 982 is tensor(5.6135e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of 1
Current timestep = 983. State = [[-0.37191102 -0.27694935]]. Action = [[-0.03811511 -0.18580073  0.0221042  -0.8378873 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 983 is [True, False, False, True, False, False]
State prediction error at timestep 983 is tensor(6.5642e-05, grad_fn=<MseLossBackward0>)
Current timestep = 984. State = [[-0.37515363 -0.27899393]]. Action = [[-0.04634282 -0.2300976  -0.18170652  0.603789  ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 984 is [True, False, False, True, False, False]
State prediction error at timestep 984 is tensor(3.2728e-05, grad_fn=<MseLossBackward0>)
Current timestep = 985. State = [[-0.37815085 -0.28405234]]. Action = [[ 0.18883407  0.05967179 -0.03706616 -0.7593597 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 985 is [True, False, False, True, False, False]
State prediction error at timestep 985 is tensor(4.5359e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 985 of 1
Current timestep = 986. State = [[-0.37811553 -0.2865783 ]]. Action = [[-0.00742584  0.21806324  0.24112213 -0.47132516]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 986 is [True, False, False, True, False, False]
State prediction error at timestep 986 is tensor(5.0739e-05, grad_fn=<MseLossBackward0>)
Current timestep = 987. State = [[-0.37814188 -0.28619358]]. Action = [[ 0.03753731  0.21820241  0.16879371 -0.13116717]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 987 is [True, False, False, True, False, False]
State prediction error at timestep 987 is tensor(4.8057e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 1
Current timestep = 988. State = [[-0.3772105  -0.28175268]]. Action = [[ 0.20301121 -0.21251537 -0.02430066  0.094118  ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 988 is [True, False, False, True, False, False]
State prediction error at timestep 988 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 989. State = [[-0.3750061 -0.2820232]]. Action = [[-0.1337434  -0.04804394 -0.15773888  0.21804678]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 989 is [True, False, False, True, False, False]
State prediction error at timestep 989 is tensor(5.9741e-05, grad_fn=<MseLossBackward0>)
Current timestep = 990. State = [[-0.37513542 -0.28211063]]. Action = [[-0.15669693  0.05165344  0.10527542 -0.56911945]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 990 is [True, False, False, True, False, False]
State prediction error at timestep 990 is tensor(7.0864e-05, grad_fn=<MseLossBackward0>)
Current timestep = 991. State = [[-0.37601906 -0.28199697]]. Action = [[-0.17946352 -0.06017768 -0.08764644  0.59928036]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 991 is [True, False, False, True, False, False]
State prediction error at timestep 991 is tensor(3.2604e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 0
Current timestep = 992. State = [[-0.37651953 -0.2818525 ]]. Action = [[-0.23429742  0.21299574  0.02144694  0.5520654 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 992 is [True, False, False, True, False, False]
State prediction error at timestep 992 is tensor(3.7432e-06, grad_fn=<MseLossBackward0>)
Current timestep = 993. State = [[-0.37655807 -0.28181323]]. Action = [[ 0.18651325 -0.07545114  0.10877654  0.73149014]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 993 is [True, False, False, True, False, False]
State prediction error at timestep 993 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 0
Current timestep = 994. State = [[-0.3754818  -0.28253743]]. Action = [[0.04574102 0.16054317 0.09896696 0.43582964]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 994 is [True, False, False, True, False, False]
State prediction error at timestep 994 is tensor(4.1201e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 994 of 0
Current timestep = 995. State = [[-0.37471318 -0.2816163 ]]. Action = [[-0.08741823  0.24506834  0.22712111 -0.862958  ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 995 is [True, False, False, True, False, False]
State prediction error at timestep 995 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 996. State = [[-0.37532339 -0.2787104 ]]. Action = [[-0.20674877  0.17670828  0.01782992 -0.25194544]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 996 is [True, False, False, True, False, False]
State prediction error at timestep 996 is tensor(8.9265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of 0
Current timestep = 997. State = [[-0.37589782 -0.276523  ]]. Action = [[ 0.05219388  0.10776621  0.05496335 -0.692816  ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 997 is [True, False, False, True, False, False]
State prediction error at timestep 997 is tensor(8.2433e-05, grad_fn=<MseLossBackward0>)
Current timestep = 998. State = [[-0.37473276 -0.27367517]]. Action = [[ 0.2343215  -0.23074234  0.0047771   0.5044167 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 998 is [True, False, False, True, False, False]
State prediction error at timestep 998 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 998 of 0
Current timestep = 999. State = [[-0.37015724 -0.27310893]]. Action = [[ 0.12403485 -0.07185152 -0.14750871 -0.48023784]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 999 is [True, False, False, True, False, False]
State prediction error at timestep 999 is tensor(9.4302e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1000. State = [[-0.36527222 -0.27361065]]. Action = [[ 0.13672882  0.13757887 -0.03711121 -0.54538465]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1000 is [True, False, False, True, False, False]
State prediction error at timestep 1000 is tensor(9.8593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1001. State = [[-0.36197844 -0.27208537]]. Action = [[ 0.2060523  -0.18273216  0.02230176  0.21772802]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1001 is [True, False, False, True, False, False]
State prediction error at timestep 1001 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of 0
Current timestep = 1002. State = [[-0.3545931  -0.27292943]]. Action = [[-0.01765136  0.18888235  0.1921995   0.29502237]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1002 is [True, False, False, True, False, False]
State prediction error at timestep 1002 is tensor(2.2043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of 1
Current timestep = 1003. State = [[-0.35064813 -0.27077562]]. Action = [[ 0.02300078  0.15078294 -0.13231933  0.10749018]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1003 is [True, False, False, True, False, False]
State prediction error at timestep 1003 is tensor(7.4272e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1004. State = [[-0.34777144 -0.26741672]]. Action = [[ 0.15050781  0.09991738 -0.03401177  0.7926543 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1004 is [True, False, False, True, False, False]
State prediction error at timestep 1004 is tensor(8.9066e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1005. State = [[-0.34271216 -0.26243111]]. Action = [[ 0.09552515 -0.22210042  0.14994937 -0.45494437]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1005 is [True, False, False, True, False, False]
State prediction error at timestep 1005 is tensor(5.5272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1006. State = [[-0.3377452 -0.2622097]]. Action = [[ 0.2367484  -0.20531681  0.02547652  0.65932417]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1006 is [True, False, False, True, False, False]
State prediction error at timestep 1006 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.32779154 -0.26656476]]. Action = [[ 0.19220608 -0.19752754 -0.16127394 -0.11241585]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1007 is [True, False, False, True, False, False]
State prediction error at timestep 1007 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.3143343  -0.27197897]]. Action = [[0.02771482 0.07732719 0.19276679 0.5421498 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1008 is [True, False, False, True, False, False]
State prediction error at timestep 1008 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1008 of 1
Current timestep = 1009. State = [[-0.3061952  -0.27324426]]. Action = [[ 0.0127866  -0.2336989  -0.09356016  0.4844346 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1009 is [True, False, False, True, False, False]
State prediction error at timestep 1009 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1010. State = [[-0.29943377 -0.27417225]]. Action = [[ 0.11420035  0.00703603  0.19856703 -0.14167929]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1010 is [True, False, False, True, False, False]
State prediction error at timestep 1010 is tensor(1.6420e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of 1
Current timestep = 1011. State = [[-0.29146168 -0.27485707]]. Action = [[-0.12258807  0.0384272  -0.23589668  0.39311397]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1011 is [True, False, False, True, False, False]
State prediction error at timestep 1011 is tensor(6.9036e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1011 of 1
Current timestep = 1012. State = [[-0.28787187 -0.27499694]]. Action = [[ 0.14570919  0.0484243   0.0830037  -0.50060517]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1012 is [True, False, False, True, False, False]
State prediction error at timestep 1012 is tensor(2.6758e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1012 of 1
Current timestep = 1013. State = [[-0.28324884 -0.2731809 ]]. Action = [[0.19341311 0.01457623 0.12582693 0.2713455 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1013 is [True, False, False, True, False, False]
State prediction error at timestep 1013 is tensor(3.2414e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of 1
Current timestep = 1014. State = [[-0.27641872 -0.27090067]]. Action = [[-0.07477269 -0.00312784  0.03217828  0.7835382 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1014 is [True, False, False, True, False, False]
State prediction error at timestep 1014 is tensor(5.9558e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1015. State = [[-0.2734987 -0.2708358]]. Action = [[-0.16572975 -0.14042358  0.18619037 -0.73229086]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1015 is [True, False, False, True, False, False]
State prediction error at timestep 1015 is tensor(2.7713e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1016. State = [[-0.2746731  -0.27327627]]. Action = [[-0.0082981  -0.05868304  0.04254147 -0.29375345]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1016 is [True, False, False, True, False, False]
State prediction error at timestep 1016 is tensor(1.7064e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[-0.27585545 -0.2747893 ]]. Action = [[ 0.07778746  0.24191338 -0.05381456 -0.64356136]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1017 is [True, False, False, True, False, False]
State prediction error at timestep 1017 is tensor(5.2838e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of 1
Current timestep = 1018. State = [[-0.27474645 -0.27078843]]. Action = [[0.04082063 0.19383326 0.20903254 0.9452797 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1018 is [True, False, False, True, False, False]
State prediction error at timestep 1018 is tensor(1.4453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1019. State = [[-0.27268234 -0.26383373]]. Action = [[ 0.0110102  -0.02618973 -0.23296277 -0.39416182]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1019 is [True, False, False, True, False, False]
State prediction error at timestep 1019 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1019 of 1
Current timestep = 1020. State = [[-0.27181473 -0.25949162]]. Action = [[-0.13346098  0.2229912  -0.21170202 -0.84196144]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1020 is [True, False, False, True, False, False]
State prediction error at timestep 1020 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of 1
Current timestep = 1021. State = [[-0.27150723 -0.25253344]]. Action = [[-0.08633699  0.00476158 -0.0341599   0.1181339 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1021 is [True, False, False, True, False, False]
State prediction error at timestep 1021 is tensor(4.9064e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of 1
Current timestep = 1022. State = [[-0.27293193 -0.24791874]]. Action = [[-0.07838577  0.13544318 -0.1773097  -0.29937518]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1022 is [True, False, False, True, False, False]
State prediction error at timestep 1022 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1023. State = [[-0.27522668 -0.24196728]]. Action = [[-0.21709539  0.1513294  -0.00204696  0.82641983]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1023 is [True, False, False, True, False, False]
State prediction error at timestep 1023 is tensor(5.9350e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1024. State = [[-0.28208092 -0.23450729]]. Action = [[-0.23059075  0.15858853 -0.20879687  0.44574213]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1024 is [True, False, False, True, False, False]
State prediction error at timestep 1024 is tensor(5.1661e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of 1
Current timestep = 1025. State = [[-0.29279613 -0.2253238 ]]. Action = [[ 0.04440349  0.06018412  0.1046651  -0.68622524]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1025 is [True, False, False, True, False, False]
State prediction error at timestep 1025 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1025 of 1
Current timestep = 1026. State = [[-0.29983172 -0.21738072]]. Action = [[-0.20706235  0.13937566  0.06036752 -0.25331497]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1026 is [True, False, False, True, False, False]
State prediction error at timestep 1026 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1027. State = [[-0.30963475 -0.21047205]]. Action = [[-0.1086953   0.01927209  0.18084672 -0.12864059]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1027 is [True, False, False, True, False, False]
State prediction error at timestep 1027 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of -1
Current timestep = 1028. State = [[-0.31645146 -0.20927711]]. Action = [[-0.06148513 -0.12666011 -0.17454152  0.9305961 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1028 is [True, False, False, True, False, False]
State prediction error at timestep 1028 is tensor(7.3015e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1029. State = [[-0.31986782 -0.21112481]]. Action = [[-0.07849438  0.1074174  -0.13640998  0.8018973 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1029 is [True, False, False, True, False, False]
State prediction error at timestep 1029 is tensor(5.9260e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[-0.32267538 -0.21116868]]. Action = [[ 0.13777637  0.0013423  -0.03072986 -0.37377417]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1030 is [True, False, False, True, False, False]
State prediction error at timestep 1030 is tensor(1.4276e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of -1
Current timestep = 1031. State = [[-0.32282242 -0.21105835]]. Action = [[ 0.02601719 -0.18612579 -0.14239287 -0.9115788 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1031 is [True, False, False, True, False, False]
State prediction error at timestep 1031 is tensor(8.6004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[-0.32280028 -0.21161021]]. Action = [[ 0.20657507 -0.16656193 -0.07330227 -0.6833436 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1032 is [True, False, False, True, False, False]
State prediction error at timestep 1032 is tensor(3.9126e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1033. State = [[-0.31921867 -0.21411225]]. Action = [[0.18549615 0.10465422 0.02199107 0.56612015]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1033 is [True, False, False, True, False, False]
State prediction error at timestep 1033 is tensor(5.9239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1034. State = [[-0.31536263 -0.21558025]]. Action = [[-0.11060752 -0.05303517  0.06759381 -0.05645674]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1034 is [True, False, False, True, False, False]
State prediction error at timestep 1034 is tensor(1.8429e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of -1
Current timestep = 1035. State = [[-0.31528226 -0.21599326]]. Action = [[-0.12545799  0.08374491  0.09704667  0.8579507 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1035 is [True, False, False, True, False, False]
State prediction error at timestep 1035 is tensor(1.7778e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1035 of -1
Current timestep = 1036. State = [[-0.3157788  -0.21619508]]. Action = [[-0.00188889 -0.17713815 -0.13487585  0.54550385]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1036 is [True, False, False, True, False, False]
State prediction error at timestep 1036 is tensor(1.8459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1037. State = [[-0.31667104 -0.21770386]]. Action = [[ 0.20880467  0.06284642 -0.0878244   0.74470687]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1037 is [True, False, False, True, False, False]
State prediction error at timestep 1037 is tensor(2.4927e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of -1
Current timestep = 1038. State = [[-0.31544426 -0.21825944]]. Action = [[-0.14206906 -0.22896764 -0.19182847 -0.29591835]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1038 is [True, False, False, True, False, False]
State prediction error at timestep 1038 is tensor(2.3626e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of -1
Current timestep = 1039. State = [[-0.3158712 -0.2213592]]. Action = [[ 0.12630123 -0.02591051  0.17971823 -0.7105117 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1039 is [True, False, False, True, False, False]
State prediction error at timestep 1039 is tensor(1.3891e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1040. State = [[-0.31401843 -0.22359289]]. Action = [[-0.06326509 -0.08001541  0.12913275  0.9896369 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1040 is [True, False, False, True, False, False]
State prediction error at timestep 1040 is tensor(1.5441e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of -1
Current timestep = 1041. State = [[-0.31379563 -0.22620906]]. Action = [[ 0.0998424  -0.20818648 -0.18256687  0.23357034]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1041 is [True, False, False, True, False, False]
State prediction error at timestep 1041 is tensor(5.2873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1042. State = [[-0.3109908  -0.23133452]]. Action = [[-0.1326944   0.02567106 -0.13930376 -0.5262055 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1042 is [True, False, False, True, False, False]
State prediction error at timestep 1042 is tensor(1.0732e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1043. State = [[-0.31162158 -0.23433569]]. Action = [[-0.2472188   0.05639696  0.0113754   0.8380759 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1043 is [True, False, False, True, False, False]
State prediction error at timestep 1043 is tensor(1.9255e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[-0.3161439  -0.23804009]]. Action = [[-0.15644553 -0.16369256 -0.19852567 -0.03247821]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1044 is [True, False, False, True, False, False]
State prediction error at timestep 1044 is tensor(4.5433e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1045. State = [[-0.32204694 -0.24335717]]. Action = [[-0.13314281 -0.21875355  0.13697463  0.55721986]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1045 is [True, False, False, True, False, False]
State prediction error at timestep 1045 is tensor(4.8504e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1046. State = [[-0.327365   -0.24915735]]. Action = [[ 0.20250583  0.15726489 -0.1431481   0.5393139 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1046 is [True, False, False, True, False, False]
State prediction error at timestep 1046 is tensor(2.0668e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1047. State = [[-0.32711306 -0.2506204 ]]. Action = [[-0.09107557  0.16403729  0.06813192 -0.33044684]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1047 is [True, False, False, True, False, False]
State prediction error at timestep 1047 is tensor(2.2077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1048. State = [[-0.3279843 -0.2510159]]. Action = [[-0.21868455 -0.1824689  -0.18258512 -0.9378837 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1048 is [True, False, False, True, False, False]
State prediction error at timestep 1048 is tensor(1.6902e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1048 of -1
Current timestep = 1049. State = [[-0.33209532 -0.2547138 ]]. Action = [[-0.14583768 -0.21240492  0.06773296 -0.15275323]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1049 is [True, False, False, True, False, False]
State prediction error at timestep 1049 is tensor(6.6140e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1050. State = [[-0.3376477  -0.25992087]]. Action = [[-0.18705179 -0.03439228 -0.17808792 -0.0696764 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1050 is [True, False, False, True, False, False]
State prediction error at timestep 1050 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1050 of -1
Current timestep = 1051. State = [[-0.34433565 -0.26524976]]. Action = [[-0.00928305  0.21339792  0.02686238  0.68824863]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1051 is [True, False, False, True, False, False]
State prediction error at timestep 1051 is tensor(1.1071e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1051 of -1
Current timestep = 1052. State = [[-0.34860748 -0.2668791 ]]. Action = [[0.21564847 0.18782884 0.03402707 0.7673383 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1052 is [True, False, False, True, False, False]
State prediction error at timestep 1052 is tensor(1.4297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1053. State = [[-0.34924114 -0.26625565]]. Action = [[ 0.15158355 -0.18347691  0.17539665  0.722183  ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1053 is [True, False, False, True, False, False]
State prediction error at timestep 1053 is tensor(4.1182e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1053 of -1
Current timestep = 1054. State = [[-0.347488   -0.26791456]]. Action = [[0.076599   0.04669476 0.07982087 0.5750778 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1054 is [True, False, False, True, False, False]
State prediction error at timestep 1054 is tensor(8.0523e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1054 of -1
Current timestep = 1055. State = [[-0.34593743 -0.26913878]]. Action = [[ 0.15383065 -0.19260305  0.20811963  0.6193657 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1055 is [True, False, False, True, False, False]
State prediction error at timestep 1055 is tensor(9.4985e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1056. State = [[-0.34169516 -0.27244034]]. Action = [[ 0.16913211 -0.00860508 -0.1576226   0.28069067]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1056 is [True, False, False, True, False, False]
State prediction error at timestep 1056 is tensor(8.6582e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1056 of -1
Current timestep = 1057. State = [[-0.3371235  -0.27419218]]. Action = [[ 0.06372023 -0.1252609   0.00513482 -0.49636835]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1057 is [True, False, False, True, False, False]
State prediction error at timestep 1057 is tensor(1.6332e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of -1
Current timestep = 1058. State = [[-0.33315337 -0.27643216]]. Action = [[-0.14715746 -0.1370514  -0.04070193 -0.03052616]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1058 is [True, False, False, True, False, False]
State prediction error at timestep 1058 is tensor(8.8325e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1059. State = [[-0.3324453 -0.2785559]]. Action = [[ 0.05640697 -0.10048702 -0.05418809 -0.77458566]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1059 is [True, False, False, True, False, False]
State prediction error at timestep 1059 is tensor(4.0877e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[-0.33172518 -0.28496012]]. Action = [[ 0.01703164  0.03673667 -0.22917269  0.19002128]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1060 is [True, False, False, True, False, False]
State prediction error at timestep 1060 is tensor(4.4502e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1060 of -1
Current timestep = 1061. State = [[-0.33154592 -0.28923848]]. Action = [[-0.07564205  0.10160476 -0.24700153  0.8017843 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1061 is [True, False, False, True, False, False]
State prediction error at timestep 1061 is tensor(5.4592e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1062. State = [[-0.33295304 -0.29114106]]. Action = [[-0.24586356  0.04495504 -0.01097406 -0.04796147]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1062 is [True, False, False, True, False, False]
State prediction error at timestep 1062 is tensor(4.2295e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of -1
Current timestep = 1063. State = [[-0.33634344 -0.2912737 ]]. Action = [[-0.03723645  0.23749611 -0.23836072 -0.13926387]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1063 is [True, False, False, True, False, False]
State prediction error at timestep 1063 is tensor(5.2365e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1063 of -1
Current timestep = 1064. State = [[-0.33907494 -0.28859618]]. Action = [[ 0.0384416  -0.06842618  0.08510131 -0.10884297]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1064 is [True, False, False, True, False, False]
State prediction error at timestep 1064 is tensor(1.4466e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1065. State = [[-0.33995065 -0.28770646]]. Action = [[-0.03289388 -0.07067928  0.07792506 -0.19489771]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1065 is [True, False, False, True, False, False]
State prediction error at timestep 1065 is tensor(1.6820e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1066. State = [[-0.3410121  -0.28745478]]. Action = [[-0.19659789  0.20181149  0.03538474  0.3427031 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1066 is [True, False, False, True, False, False]
State prediction error at timestep 1066 is tensor(4.8192e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1066 of -1
Current timestep = 1067. State = [[-0.34564197 -0.28475574]]. Action = [[-0.0040216   0.03418836 -0.18983789  0.09160864]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1067 is [True, False, False, True, False, False]
State prediction error at timestep 1067 is tensor(5.3145e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1068. State = [[-0.34808    -0.28295198]]. Action = [[-0.22347698  0.04722926 -0.14696261 -0.6491963 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1068 is [True, False, False, True, False, False]
State prediction error at timestep 1068 is tensor(5.0451e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1069. State = [[-0.35312748 -0.2819405 ]]. Action = [[ 0.20688766 -0.24166033  0.1574052  -0.7035443 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1069 is [True, False, False, True, False, False]
State prediction error at timestep 1069 is tensor(3.2669e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1069 of -1
Current timestep = 1070. State = [[-0.3527399 -0.2826306]]. Action = [[ 0.05694646  0.05048627 -0.16189677 -0.5634701 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1070 is [True, False, False, True, False, False]
State prediction error at timestep 1070 is tensor(4.5868e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1071. State = [[-0.35245967 -0.28268197]]. Action = [[-0.1990498   0.20942277 -0.02451563 -0.7062015 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1071 is [True, False, False, True, False, False]
State prediction error at timestep 1071 is tensor(6.9620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1071 of -1
Current timestep = 1072. State = [[-0.35415778 -0.28152356]]. Action = [[ 0.01020902 -0.03397506 -0.21667679  0.72250676]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1072 is [True, False, False, True, False, False]
State prediction error at timestep 1072 is tensor(3.2124e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1072 of -1
Current timestep = 1073. State = [[-0.35529143 -0.28120124]]. Action = [[-0.20205    -0.15435527 -0.11735503  0.14725316]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1073 is [True, False, False, True, False, False]
State prediction error at timestep 1073 is tensor(2.0568e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[-0.35927984 -0.2833434 ]]. Action = [[-0.15572958 -0.06254345  0.13631779  0.4596038 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1074 is [True, False, False, True, False, False]
State prediction error at timestep 1074 is tensor(2.5235e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1075. State = [[-0.36488152 -0.28647766]]. Action = [[ 0.07770535  0.14896998 -0.1215834  -0.8384098 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1075 is [True, False, False, True, False, False]
State prediction error at timestep 1075 is tensor(3.1635e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1076. State = [[-0.3666105  -0.28632513]]. Action = [[ 0.20968926 -0.00806616 -0.14655857 -0.31319082]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1076 is [True, False, False, True, False, False]
State prediction error at timestep 1076 is tensor(1.4454e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1076 of -1
Current timestep = 1077. State = [[-0.3664569 -0.2858433]]. Action = [[-0.23003614 -0.08667505  0.2186104  -0.64775175]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1077 is [True, False, False, True, False, False]
State prediction error at timestep 1077 is tensor(1.7905e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1077 of -1
Current timestep = 1078. State = [[-0.3679048 -0.2868056]]. Action = [[ 0.12696987 -0.23791705 -0.15202612 -0.3630941 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1078 is [True, False, False, True, False, False]
State prediction error at timestep 1078 is tensor(9.6475e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1079. State = [[-0.36697277 -0.2892302 ]]. Action = [[ 0.19528452 -0.18029334 -0.15031132  0.8683971 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1079 is [True, False, False, True, False, False]
State prediction error at timestep 1079 is tensor(3.3688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1080. State = [[-0.3624198  -0.29397362]]. Action = [[ 0.1206001   0.14379826 -0.06509484  0.19837093]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1080 is [True, False, False, True, False, False]
State prediction error at timestep 1080 is tensor(2.1767e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1080 of -1
Current timestep = 1081. State = [[-0.35977167 -0.29622045]]. Action = [[ 0.21574283 -0.0461781   0.2199991  -0.6929745 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1081 is [True, False, False, True, False, False]
State prediction error at timestep 1081 is tensor(6.4300e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1081 of -1
Current timestep = 1082. State = [[-0.35539603 -0.29807156]]. Action = [[-0.1774852   0.20094693  0.06161606  0.1709441 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1082 is [True, False, False, True, False, False]
State prediction error at timestep 1082 is tensor(1.1103e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1082 of -1
Current timestep = 1083. State = [[-0.3555764  -0.29735866]]. Action = [[ 0.19935256 -0.22525609 -0.17209227  0.8744817 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1083 is [True, False, False, True, False, False]
State prediction error at timestep 1083 is tensor(2.3552e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1084. State = [[-0.3533144  -0.29835305]]. Action = [[-0.00156914  0.02363971 -0.1370502   0.50562644]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1084 is [True, False, False, True, False, False]
State prediction error at timestep 1084 is tensor(3.9272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1085. State = [[-0.35203204 -0.2985903 ]]. Action = [[ 0.02849472  0.13489455 -0.23776005 -0.5120412 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1085 is [True, False, False, True, False, False]
State prediction error at timestep 1085 is tensor(4.2223e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of -1
Current timestep = 1086. State = [[-0.3509893  -0.29756162]]. Action = [[0.0659433  0.10964397 0.10371429 0.7182455 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1086 is [True, False, False, True, False, False]
State prediction error at timestep 1086 is tensor(8.3541e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1086 of -1
Current timestep = 1087. State = [[-0.34910968 -0.29544738]]. Action = [[ 0.12706727 -0.22003019  0.11844286  0.89554954]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1087 is [True, False, False, True, False, False]
State prediction error at timestep 1087 is tensor(2.1207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of -1
Current timestep = 1088. State = [[-0.34569505 -0.2967469 ]]. Action = [[ 0.12793082 -0.07731813 -0.1617251   0.82258797]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1088 is [True, False, False, True, False, False]
State prediction error at timestep 1088 is tensor(1.8403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1089. State = [[-0.3410608 -0.2989906]]. Action = [[-0.21956839  0.16093683 -0.2110689  -0.9115362 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1089 is [True, False, False, True, False, False]
State prediction error at timestep 1089 is tensor(3.2455e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1090. State = [[-0.3413097 -0.2987734]]. Action = [[ 0.17197037  0.0055753   0.17895186 -0.00412816]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1090 is [True, False, False, True, False, False]
State prediction error at timestep 1090 is tensor(3.7980e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1091. State = [[-0.34081373 -0.2986371 ]]. Action = [[-0.09484664 -0.17495267  0.2176086  -0.0503968 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1091 is [True, False, False, True, False, False]
State prediction error at timestep 1091 is tensor(1.7243e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1092. State = [[-0.34044823 -0.30121312]]. Action = [[ 0.20771372 -0.07384329  0.00120279  0.46532404]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1092 is [True, False, False, True, False, False]
State prediction error at timestep 1092 is tensor(6.3290e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1093. State = [[-0.33723995 -0.30433065]]. Action = [[-0.07074915 -0.1185862   0.0493772  -0.85957325]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1093 is [True, False, False, True, False, False]
State prediction error at timestep 1093 is tensor(1.9076e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of -1
Current timestep = 1094. State = [[-0.336271   -0.30891174]]. Action = [[-0.01156816  0.12309706  0.12816131 -0.979608  ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1094 is [True, False, False, True, False, False]
State prediction error at timestep 1094 is tensor(6.4549e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1095. State = [[-0.33641258 -0.30906853]]. Action = [[-0.23803112  0.00069723 -0.06382143 -0.4962715 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1095 is [True, False, False, True, False, False]
State prediction error at timestep 1095 is tensor(6.7859e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1096. State = [[-0.33764327 -0.30931   ]]. Action = [[0.10038817 0.06072405 0.04082441 0.12129807]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1096 is [True, False, False, True, False, False]
State prediction error at timestep 1096 is tensor(1.0710e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1097. State = [[-0.33736572 -0.30863464]]. Action = [[-0.19233607  0.21812028  0.23901063 -0.7655897 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1097 is [True, False, False, True, False, False]
State prediction error at timestep 1097 is tensor(8.9517e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of -1
Current timestep = 1098. State = [[-0.34078658 -0.30518156]]. Action = [[0.16224056 0.11470985 0.21039209 0.62618494]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1098 is [True, False, False, True, False, False]
State prediction error at timestep 1098 is tensor(1.6387e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1099. State = [[-0.34077764 -0.3004436 ]]. Action = [[-0.09020615  0.174299   -0.18176903  0.49454796]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1099 is [True, False, False, True, False, False]
State prediction error at timestep 1099 is tensor(2.5602e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1099 of -1
Current timestep = 1100. State = [[-0.34228462 -0.29538485]]. Action = [[-0.01117808 -0.1808659   0.08526286  0.12918341]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1100 is [True, False, False, True, False, False]
State prediction error at timestep 1100 is tensor(1.3950e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of -1
Current timestep = 1101. State = [[-0.34235924 -0.29477462]]. Action = [[-0.24091949  0.00117663  0.05668446  0.44408453]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1101 is [True, False, False, True, False, False]
State prediction error at timestep 1101 is tensor(1.3683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1102. State = [[-0.34602773 -0.2941299 ]]. Action = [[-0.01801257 -0.00161137 -0.12962021 -0.63864326]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1102 is [True, False, False, True, False, False]
State prediction error at timestep 1102 is tensor(4.5520e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of -1
Current timestep = 1103. State = [[-0.34881863 -0.2942414 ]]. Action = [[-0.09237108  0.0133146  -0.19083725 -0.8240547 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1103 is [True, False, False, True, False, False]
State prediction error at timestep 1103 is tensor(4.5481e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of -1
Current timestep = 1104. State = [[-0.35121456 -0.29458684]]. Action = [[ 0.24124697 -0.12661716 -0.24263172  0.67190933]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1104 is [True, False, False, True, False, False]
State prediction error at timestep 1104 is tensor(3.2344e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1104 of -1
Current timestep = 1105. State = [[-0.34987435 -0.29552603]]. Action = [[-0.19201215 -0.21159062  0.23227978 -0.99375105]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1105 is [True, False, False, True, False, False]
State prediction error at timestep 1105 is tensor(3.8780e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1106. State = [[-0.35171428 -0.29764536]]. Action = [[ 0.09720665  0.06582761 -0.15660924 -0.8938535 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1106 is [True, False, False, True, False, False]
State prediction error at timestep 1106 is tensor(3.5221e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1106 of -1
Current timestep = 1107. State = [[-0.35180163 -0.29794922]]. Action = [[-0.16805843  0.02852356  0.09785968  0.8698745 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1107 is [True, False, False, True, False, False]
State prediction error at timestep 1107 is tensor(1.6283e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1107 of -1
Current timestep = 1108. State = [[-0.35350806 -0.2989628 ]]. Action = [[-0.21111688  0.20619792 -0.01559234 -0.8460612 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1108 is [True, False, False, True, False, False]
State prediction error at timestep 1108 is tensor(5.3768e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1108 of -1
Current timestep = 1109. State = [[-0.35802972 -0.29814473]]. Action = [[-0.12019414 -0.06959836 -0.05270678  0.6365973 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1109 is [True, False, False, True, False, False]
State prediction error at timestep 1109 is tensor(4.2424e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1110. State = [[-0.36298606 -0.29906902]]. Action = [[-0.13840334  0.20242158 -0.20236285  0.70094657]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1110 is [True, False, False, True, False, False]
State prediction error at timestep 1110 is tensor(1.9753e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1110 of -1
Current timestep = 1111. State = [[-0.36846533 -0.29781172]]. Action = [[ 0.11135548  0.0574612  -0.15300652 -0.42900348]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1111 is [True, False, False, True, False, False]
State prediction error at timestep 1111 is tensor(4.8249e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of -1
Current timestep = 1112. State = [[-0.36938345 -0.29678977]]. Action = [[ 0.22731179 -0.17628698  0.06038389 -0.38597357]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1112 is [True, False, False, True, False, False]
State prediction error at timestep 1112 is tensor(4.1999e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1113. State = [[-0.36807328 -0.2972023 ]]. Action = [[-0.06927621  0.24035245 -0.01609996 -0.44235337]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1113 is [True, False, False, True, False, False]
State prediction error at timestep 1113 is tensor(5.0942e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1114. State = [[-0.3682742  -0.29615515]]. Action = [[ 0.07815266 -0.08172664  0.02483401 -0.2613623 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1114 is [True, False, False, True, False, False]
State prediction error at timestep 1114 is tensor(6.9462e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1115. State = [[-0.36810613 -0.29547626]]. Action = [[-0.09320632  0.23529282  0.10349354 -0.01536345]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1115 is [True, False, False, True, False, False]
State prediction error at timestep 1115 is tensor(5.4653e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1115 of -1
Current timestep = 1116. State = [[-0.3693561  -0.29293808]]. Action = [[-0.13779156  0.11971352 -0.11685491 -0.63515157]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1116 is [True, False, False, True, False, False]
State prediction error at timestep 1116 is tensor(4.6373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of -1
Current timestep = 1117. State = [[-0.37337545 -0.28854638]]. Action = [[ 0.01650542 -0.03216216  0.22609895  0.74435043]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1117 is [True, False, False, True, False, False]
State prediction error at timestep 1117 is tensor(1.4856e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1118. State = [[-0.37602267 -0.28557166]]. Action = [[-0.17753102 -0.08413932  0.15327007  0.02232766]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1118 is [True, False, False, True, False, False]
State prediction error at timestep 1118 is tensor(4.9356e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1119. State = [[-0.37862554 -0.28422916]]. Action = [[ 0.06353462 -0.20616114  0.22407755 -0.17099124]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1119 is [True, False, False, True, False, False]
State prediction error at timestep 1119 is tensor(3.4981e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of -1
Current timestep = 1120. State = [[-0.37902027 -0.2849666 ]]. Action = [[-0.22443917  0.15821949  0.09338471 -0.62879276]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1120 is [True, False, False, True, False, False]
State prediction error at timestep 1120 is tensor(4.2699e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1120 of -1
Current timestep = 1121. State = [[-0.37923908 -0.2852773 ]]. Action = [[-0.09775859  0.07835832 -0.16468541 -0.67488885]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1121 is [True, False, False, True, False, False]
State prediction error at timestep 1121 is tensor(1.2319e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1121 of -1
Current timestep = 1122. State = [[-0.38035056 -0.28561068]]. Action = [[-0.05540508  0.04080826  0.21644914 -0.8351604 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1122 is [True, False, False, True, False, False]
State prediction error at timestep 1122 is tensor(1.0287e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1123. State = [[-0.38146096 -0.28573555]]. Action = [[-0.17429757 -0.18182665 -0.03703165  0.11028719]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1123 is [True, False, False, True, False, False]
State prediction error at timestep 1123 is tensor(5.7300e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1123 of -1
Current timestep = 1124. State = [[-0.38201573 -0.28569216]]. Action = [[-0.20145775 -0.05829927 -0.11495093 -0.5302895 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1124 is [True, False, False, True, False, False]
State prediction error at timestep 1124 is tensor(2.2502e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of -1
Current timestep = 1125. State = [[-0.38255498 -0.28557232]]. Action = [[ 0.08096561 -0.08983903 -0.09894796 -0.3338501 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1125 is [True, False, False, True, False, False]
State prediction error at timestep 1125 is tensor(4.0814e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of -1
Current timestep = 1126. State = [[-0.38251224 -0.28558224]]. Action = [[ 0.04599419 -0.12825255 -0.1430784  -0.05843329]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1126 is [True, False, False, True, False, False]
State prediction error at timestep 1126 is tensor(3.9231e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1127. State = [[-0.38191885 -0.28655294]]. Action = [[ 0.24791795 -0.03689393 -0.02245542 -0.9552386 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1127 is [True, False, False, True, False, False]
State prediction error at timestep 1127 is tensor(1.6176e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1128. State = [[-0.37836668 -0.28937992]]. Action = [[ 0.15008655 -0.18991305  0.12709111  0.02834272]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1128 is [True, False, False, True, False, False]
State prediction error at timestep 1128 is tensor(3.4234e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1128 of -1
Current timestep = 1129. State = [[-0.3729366  -0.29439327]]. Action = [[ 0.0615862   0.03492934 -0.15919589  0.25463486]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1129 is [True, False, False, True, False, False]
State prediction error at timestep 1129 is tensor(5.0382e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of -1
Current timestep = 1130. State = [[-0.36919016 -0.29743338]]. Action = [[-0.05079061 -0.18944931  0.12290674 -0.46870077]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1130 is [True, False, False, True, False, False]
State prediction error at timestep 1130 is tensor(6.3170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1131. State = [[-0.36690205 -0.30040723]]. Action = [[-0.04160804  0.1393412  -0.21481773 -0.59891284]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1131 is [True, False, False, True, False, False]
State prediction error at timestep 1131 is tensor(1.4137e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1131 of -1
Current timestep = 1132. State = [[-0.36691418 -0.3003193 ]]. Action = [[-0.02590849  0.06150818  0.1111868   0.30532515]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1132 is [True, False, False, True, False, False]
State prediction error at timestep 1132 is tensor(1.2572e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[-0.36716807 -0.3003519 ]]. Action = [[-0.05632693 -0.02611932  0.07727495  0.5883591 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1133 is [True, False, False, True, False, False]
State prediction error at timestep 1133 is tensor(3.9377e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 1
Current timestep = 1134. State = [[-0.36747903 -0.30059952]]. Action = [[ 0.20929694 -0.0359187   0.1624875  -0.9386744 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1134 is [True, False, False, True, False, False]
State prediction error at timestep 1134 is tensor(2.4154e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1135. State = [[-0.3658245  -0.30130827]]. Action = [[ 0.08353296 -0.18696061  0.09817898 -0.10509807]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1135 is [True, False, False, True, False, False]
State prediction error at timestep 1135 is tensor(4.2739e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1136. State = [[-0.3624988  -0.30425957]]. Action = [[ 0.21223098 -0.08730513  0.05576429  0.2540269 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1136 is [True, False, False, True, False, False]
State prediction error at timestep 1136 is tensor(4.4259e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1137. State = [[-0.35740075 -0.30699462]]. Action = [[-0.04185514  0.16195238  0.07925007 -0.02727085]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1137 is [True, False, False, True, False, False]
State prediction error at timestep 1137 is tensor(2.2210e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of 1
Current timestep = 1138. State = [[-0.35653365 -0.3066332 ]]. Action = [[-0.0994229  -0.0833459   0.04241633 -0.72187364]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1138 is [True, False, False, True, False, False]
State prediction error at timestep 1138 is tensor(2.1694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1138 of 1
Current timestep = 1139. State = [[-0.35670125 -0.3068678 ]]. Action = [[-0.09584761 -0.12039231 -0.1031421   0.47365665]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1139 is [True, False, False, True, False, False]
State prediction error at timestep 1139 is tensor(1.7093e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of 1
Current timestep = 1140. State = [[-0.35763195 -0.30817977]]. Action = [[-0.2104662  -0.16668844 -0.09407467 -0.23440051]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1140 is [True, False, False, True, False, False]
State prediction error at timestep 1140 is tensor(6.6952e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1140 of 1
Current timestep = 1141. State = [[-0.3617798  -0.31201068]]. Action = [[-0.00335939 -0.05513656  0.16436452 -0.42792964]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1141 is [True, False, False, True, False, False]
State prediction error at timestep 1141 is tensor(6.3409e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1142. State = [[-0.36424327 -0.3154928 ]]. Action = [[ 0.06532368 -0.11758161  0.0968473   0.2751602 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1142 is [True, False, False, True, False, False]
State prediction error at timestep 1142 is tensor(2.8668e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1143. State = [[-0.36355683 -0.3190384 ]]. Action = [[ 0.15885401 -0.21772887  0.06855878 -0.04202187]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1143 is [True, False, False, True, False, False]
State prediction error at timestep 1143 is tensor(3.5859e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1144. State = [[-0.35899267 -0.3240558 ]]. Action = [[ 0.03486511  0.19394687 -0.0992552   0.96954775]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1144 is [True, False, False, True, False, False]
State prediction error at timestep 1144 is tensor(5.1954e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of 1
Current timestep = 1145. State = [[-0.35691023 -0.3266146 ]]. Action = [[-0.09731749 -0.05450222 -0.14087059  0.74808264]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1145 is [True, False, False, True, False, False]
State prediction error at timestep 1145 is tensor(2.6174e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1146. State = [[-0.3589779 -0.3301961]]. Action = [[-0.16310014  0.11395037 -0.03471479 -0.41220605]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1146 is [True, False, False, True, False, False]
State prediction error at timestep 1146 is tensor(8.7118e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1146 of 1
Current timestep = 1147. State = [[-0.36066306 -0.33123562]]. Action = [[ 0.15768224 -0.21892118 -0.21230595 -0.43370205]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1147 is [True, False, False, True, False, False]
State prediction error at timestep 1147 is tensor(3.2957e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1148. State = [[-0.3598892 -0.332755 ]]. Action = [[ 0.1996414  -0.21635665 -0.21066278  0.21259642]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1148 is [True, False, False, True, False, False]
State prediction error at timestep 1148 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1149. State = [[-0.35963684 -0.34395912]]. Action = [[ 0.07899359 -0.24880181  0.04265085  0.63210285]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1149 is [True, False, False, True, False, False]
State prediction error at timestep 1149 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1150. State = [[-0.35977376 -0.3592972 ]]. Action = [[0.03169376 0.15589434 0.10359859 0.85515404]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1150 is [True, False, False, True, False, False]
State prediction error at timestep 1150 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1151. State = [[-0.35893437 -0.36506078]]. Action = [[ 0.18980312 -0.22854756 -0.0924857  -0.59827524]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1151 is [True, False, False, True, False, False]
State prediction error at timestep 1151 is tensor(1.2131e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1152. State = [[-0.3558774  -0.37411824]]. Action = [[0.18477699 0.21306476 0.23526815 0.76896155]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1152 is [True, False, False, True, False, False]
State prediction error at timestep 1152 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1153. State = [[-0.3485537  -0.37578896]]. Action = [[-0.12798223  0.08700386  0.08672926  0.8662095 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1153 is [True, False, False, True, False, False]
State prediction error at timestep 1153 is tensor(8.2359e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of -1
Current timestep = 1154. State = [[-0.34643254 -0.37515494]]. Action = [[ 0.24627995  0.12879917 -0.08221534 -0.80043274]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1154 is [True, False, False, True, False, False]
State prediction error at timestep 1154 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1155. State = [[-0.34026706 -0.37128508]]. Action = [[ 0.18461436  0.03266084 -0.19750242 -0.5471904 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1155 is [True, False, False, True, False, False]
State prediction error at timestep 1155 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1156. State = [[-0.33098683 -0.36778054]]. Action = [[ 0.22824985 -0.19658232  0.00652066 -0.58385926]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1156 is [True, False, False, True, False, False]
State prediction error at timestep 1156 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1157. State = [[-0.31905997 -0.36963922]]. Action = [[ 0.19336262  0.11445519  0.03373423 -0.43895757]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1157 is [True, False, False, True, False, False]
State prediction error at timestep 1157 is tensor(9.2987e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1158. State = [[-0.30621374 -0.36938083]]. Action = [[ 0.05912483 -0.2287401   0.16160184  0.14980638]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1158 is [True, False, False, True, False, False]
State prediction error at timestep 1158 is tensor(6.6919e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of -1
Current timestep = 1159. State = [[-0.29585108 -0.37414405]]. Action = [[-0.00727861  0.21691489 -0.03641003 -0.83046156]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1159 is [True, False, False, True, False, False]
State prediction error at timestep 1159 is tensor(6.2943e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1160. State = [[-0.28958088 -0.37369654]]. Action = [[-0.16313522 -0.00125924  0.16959035  0.6447227 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1160 is [True, False, False, True, False, False]
State prediction error at timestep 1160 is tensor(6.4470e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1161. State = [[-0.28932843 -0.37347233]]. Action = [[ 0.11699635  0.0088205   0.04145497 -0.10197061]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1161 is [True, False, False, True, False, False]
State prediction error at timestep 1161 is tensor(1.4194e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1162. State = [[-0.28760758 -0.37177432]]. Action = [[ 0.1656838   0.08857644 -0.22330007 -0.08322918]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1162 is [True, False, False, True, False, False]
State prediction error at timestep 1162 is tensor(1.8310e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1163. State = [[-0.2821729 -0.3681468]]. Action = [[ 0.15273422  0.00736666 -0.1854814  -0.62080884]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1163 is [True, False, False, True, False, False]
State prediction error at timestep 1163 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1164. State = [[-0.2749066 -0.3659562]]. Action = [[-0.06529792 -0.22861281  0.00556052 -0.3772598 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1164 is [True, False, False, True, False, False]
State prediction error at timestep 1164 is tensor(4.8753e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1164 of 1
Current timestep = 1165. State = [[-0.27220702 -0.3692953 ]]. Action = [[-0.11113313  0.0032196  -0.19295532 -0.5127415 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1165 is [True, False, False, True, False, False]
State prediction error at timestep 1165 is tensor(8.9220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1166. State = [[-0.27233177 -0.37126157]]. Action = [[ 0.14818138 -0.19562341  0.01659346 -0.7175278 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1166 is [True, False, False, True, False, False]
State prediction error at timestep 1166 is tensor(4.0686e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1167. State = [[-0.2683239 -0.376817 ]]. Action = [[ 0.21219027  0.16160569 -0.0384232  -0.5373021 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1167 is [True, False, False, True, False, False]
State prediction error at timestep 1167 is tensor(1.1066e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1168. State = [[-0.26009506 -0.3777362 ]]. Action = [[ 0.06375062  0.22896743  0.17247003 -0.21606958]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1168 is [True, False, False, True, False, False]
State prediction error at timestep 1168 is tensor(4.2565e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[-0.25180048 -0.37321782]]. Action = [[ 0.24096692 -0.09377059  0.24542409  0.6438874 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1169 is [True, False, False, True, False, False]
State prediction error at timestep 1169 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1170. State = [[-0.2436998 -0.3720134]]. Action = [[-0.15474291 -0.12374789 -0.17920713  0.15430558]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1170 is [True, False, False, True, False, False]
State prediction error at timestep 1170 is tensor(9.4537e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of 1
Current timestep = 1171. State = [[-0.23973186 -0.37530437]]. Action = [[ 0.04924142 -0.18617892  0.18720019 -0.9770788 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1171 is [True, False, False, True, False, False]
State prediction error at timestep 1171 is tensor(6.8365e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1172. State = [[-0.23715705 -0.38069388]]. Action = [[-0.2044976   0.17349386  0.17652863 -0.732502  ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1172 is [True, False, False, True, False, False]
State prediction error at timestep 1172 is tensor(3.8114e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1173. State = [[-0.23832025 -0.38192993]]. Action = [[-0.24347569  0.00479048  0.11123586 -0.7163878 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1173 is [True, False, False, True, False, False]
State prediction error at timestep 1173 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.24372356 -0.38454133]]. Action = [[ 0.20894313  0.00640652 -0.066071    0.05361736]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1174 is [True, False, False, True, False, False]
State prediction error at timestep 1174 is tensor(3.9917e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1175. State = [[-0.24368219 -0.38435566]]. Action = [[ 0.01579994 -0.2227821  -0.15250683 -0.04537141]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1175 is [True, False, False, True, False, False]
State prediction error at timestep 1175 is tensor(6.7331e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1176. State = [[-0.24414185 -0.38792717]]. Action = [[ 0.06863308 -0.12816304  0.10894236 -0.90873194]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1176 is [True, False, False, True, False, False]
State prediction error at timestep 1176 is tensor(8.5138e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1177. State = [[-0.24333479 -0.39378434]]. Action = [[-0.11432487 -0.05071901  0.06777436 -0.36999953]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1177 is [True, False, False, True, False, False]
State prediction error at timestep 1177 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1178. State = [[-0.24510556 -0.39935365]]. Action = [[-0.0014271  -0.13825577 -0.15041383 -0.23724216]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1178 is [True, False, False, True, False, False]
State prediction error at timestep 1178 is tensor(3.8361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1178 of 1
Current timestep = 1179. State = [[-0.2475218  -0.40560207]]. Action = [[-0.23296948 -0.22141987  0.04849839 -0.39170247]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1179 is [True, False, False, True, False, False]
State prediction error at timestep 1179 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1180. State = [[-0.25468448 -0.41675273]]. Action = [[ 0.09030619 -0.0408213   0.11565742 -0.83128774]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1180 is [True, False, False, True, False, False]
State prediction error at timestep 1180 is tensor(9.4158e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1181. State = [[-0.25741583 -0.42306033]]. Action = [[-0.12511161  0.15581623 -0.09348759 -0.7574283 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1181 is [True, False, False, True, False, False]
State prediction error at timestep 1181 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of 0
Current timestep = 1182. State = [[-0.25947994 -0.42571884]]. Action = [[ 0.14449131 -0.23542011  0.00098151 -0.1606633 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1182 is [True, False, False, True, False, False]
State prediction error at timestep 1182 is tensor(2.8056e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1183. State = [[-0.26021636 -0.430375  ]]. Action = [[-1.7329670e-01 -4.6600401e-04 -1.2632622e-01 -9.8626870e-01]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1183 is [True, False, False, True, False, False]
State prediction error at timestep 1183 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1183 of -1
Current timestep = 1184. State = [[-0.26417494 -0.43533072]]. Action = [[-0.04320496 -0.1667614   0.09113896  0.1718651 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1184 is [True, False, False, True, False, False]
State prediction error at timestep 1184 is tensor(8.7035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[-0.26899144 -0.44240108]]. Action = [[-0.19694807  0.16214976 -0.2260877  -0.5064176 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1185 is [True, False, False, True, False, False]
State prediction error at timestep 1185 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1186. State = [[-0.273813   -0.44545662]]. Action = [[-0.14510432 -0.00707492 -0.10665689  0.44430673]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1186 is [True, False, False, True, False, False]
State prediction error at timestep 1186 is tensor(2.2369e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1187. State = [[-0.2797487  -0.44715613]]. Action = [[0.2272512  0.15031028 0.17763066 0.9593661 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1187 is [True, False, False, True, False, False]
State prediction error at timestep 1187 is tensor(7.9009e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1187 of -1
Current timestep = 1188. State = [[-0.27876094 -0.44502994]]. Action = [[-0.03937671  0.07417995 -0.16941133 -0.13290817]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1188 is [True, False, False, True, False, False]
State prediction error at timestep 1188 is tensor(8.6922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1188 of -1
Current timestep = 1189. State = [[-0.27775958 -0.44289675]]. Action = [[ 0.21421081  0.19982302 -0.1145094   0.5236676 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1189 is [True, False, False, True, False, False]
State prediction error at timestep 1189 is tensor(6.8550e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1190. State = [[-0.27275205 -0.43546253]]. Action = [[ 0.23849857 -0.08948478 -0.02448046 -0.75731367]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1190 is [True, False, False, True, False, False]
State prediction error at timestep 1190 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1190 of -1
Current timestep = 1191. State = [[-0.2651584 -0.4297706]]. Action = [[ 0.1424816   0.05597347 -0.03822833 -0.3405813 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1191 is [True, False, False, True, False, False]
State prediction error at timestep 1191 is tensor(5.9163e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of -1
Current timestep = 1192. State = [[-0.25744185 -0.42440325]]. Action = [[ 0.1804688  -0.09475809  0.21544152  0.5825052 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1192 is [True, False, False, True, False, False]
State prediction error at timestep 1192 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of -1
Current timestep = 1193. State = [[-0.24923816 -0.42372173]]. Action = [[-0.20043163 -0.05668083 -0.07448617 -0.42370868]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1193 is [True, False, False, True, False, False]
State prediction error at timestep 1193 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1194. State = [[-0.24728476 -0.4259141 ]]. Action = [[ 0.16395146 -0.12917376 -0.03415254 -0.22062969]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1194 is [True, False, False, True, False, False]
State prediction error at timestep 1194 is tensor(1.1892e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of -1
Current timestep = 1195. State = [[-0.24242976 -0.4297412 ]]. Action = [[-0.09949532  0.23221648  0.1363492   0.9298563 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1195 is [True, False, False, True, False, False]
State prediction error at timestep 1195 is tensor(1.0734e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1195 of -1
Current timestep = 1196. State = [[-0.23986343 -0.42809492]]. Action = [[-0.02437252  0.08425972  0.02398586 -0.01718211]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1196 is [True, False, False, True, False, False]
State prediction error at timestep 1196 is tensor(5.3759e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1196 of -1
Current timestep = 1197. State = [[-0.23842095 -0.4253047 ]]. Action = [[ 0.24400008 -0.13564391 -0.1030421  -0.90404475]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1197 is [True, False, False, True, False, False]
State prediction error at timestep 1197 is tensor(5.4265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1197 of -1
Current timestep = 1198. State = [[-0.23484612 -0.42547923]]. Action = [[ 0.07787162 -0.24839114  0.16306084  0.78320026]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1198 is [True, False, False, True, False, False]
State prediction error at timestep 1198 is tensor(2.1084e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1199. State = [[-0.23012148 -0.43071905]]. Action = [[ 0.03423327 -0.14333326  0.04609147 -0.36015785]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1199 is [True, False, False, True, False, False]
State prediction error at timestep 1199 is tensor(2.3364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of -1
Current timestep = 1200. State = [[-0.22433479 -0.4377702 ]]. Action = [[ 0.17846072  0.09172675  0.03738421 -0.6929941 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1200 is [True, False, False, True, False, False]
State prediction error at timestep 1200 is tensor(3.5531e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of -1
Current timestep = 1201. State = [[-0.2165646 -0.4410115]]. Action = [[-0.22543351  0.06420612  0.23027673 -0.08598876]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1201 is [True, False, False, True, False, False]
State prediction error at timestep 1201 is tensor(6.2649e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1201 of -1
Current timestep = 1202. State = [[-0.21642043 -0.44232526]]. Action = [[-0.09784272 -0.01482055  0.16085178  0.0095849 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1202 is [True, False, False, True, False, False]
State prediction error at timestep 1202 is tensor(5.6495e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1203. State = [[-0.21833333 -0.44381532]]. Action = [[ 0.01471743 -0.18193847 -0.04222959  0.0039432 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1203 is [True, False, False, True, False, False]
State prediction error at timestep 1203 is tensor(3.2830e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1203 of -1
Current timestep = 1204. State = [[-0.22061348 -0.44792762]]. Action = [[-0.1067642   0.05149758  0.22494471 -0.9252305 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1204 is [True, False, False, True, False, False]
State prediction error at timestep 1204 is tensor(5.9015e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1205. State = [[-0.22254184 -0.45048058]]. Action = [[-0.03427504 -0.15021247  0.16991022 -0.9551376 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1205 is [True, False, False, True, False, False]
State prediction error at timestep 1205 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of 0
Current timestep = 1206. State = [[-0.22607416 -0.4557928 ]]. Action = [[-0.22531267 -0.02957219 -0.21415812 -0.67144823]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1206 is [True, False, False, True, False, False]
State prediction error at timestep 1206 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of 0
Current timestep = 1207. State = [[-0.23228    -0.46344608]]. Action = [[ 0.0283502  -0.20624834 -0.04357976  0.36622357]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1207 is [True, False, False, True, False, False]
State prediction error at timestep 1207 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1207 of 0
Current timestep = 1208. State = [[-0.23737061 -0.47118333]]. Action = [[-0.0051818  -0.06708363  0.02328515  0.5355536 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1208 is [True, False, False, True, False, False]
State prediction error at timestep 1208 is tensor(3.8935e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1209. State = [[-0.24079287 -0.4763776 ]]. Action = [[ 0.12173653 -0.14334597  0.18981206  0.9344424 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1209 is [True, False, False, True, False, False]
State prediction error at timestep 1209 is tensor(9.0730e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1210. State = [[-0.2410368  -0.48251584]]. Action = [[ 0.10788029 -0.15379913 -0.20062391 -0.10961515]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1210 is [True, False, False, True, False, False]
State prediction error at timestep 1210 is tensor(4.1356e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1211. State = [[-0.23826161 -0.49045154]]. Action = [[-0.10487235  0.08935136 -0.11820844 -0.19361061]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1211 is [True, False, False, True, False, False]
State prediction error at timestep 1211 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of -1
Current timestep = 1212. State = [[-0.23701611 -0.49385187]]. Action = [[-0.10003942  0.11960346 -0.21386042 -0.87506497]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1212 is [True, False, False, True, False, False]
State prediction error at timestep 1212 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1212 of -1
Current timestep = 1213. State = [[-0.23803158 -0.4940711 ]]. Action = [[-0.10883938  0.03384674  0.18594947  0.93419707]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1213 is [True, False, False, True, False, False]
State prediction error at timestep 1213 is tensor(1.7010e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1213 of -1
Current timestep = 1214. State = [[-0.24030347 -0.4951678 ]]. Action = [[-0.08721469 -0.06969759  0.20551145 -0.67289865]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1214 is [True, False, False, True, False, False]
State prediction error at timestep 1214 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1215. State = [[-0.24382283 -0.4981934 ]]. Action = [[ 0.04792798  0.16183355 -0.05224901 -0.65896547]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1215 is [True, False, False, True, False, False]
State prediction error at timestep 1215 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1215 of -1
Current timestep = 1216. State = [[-0.2451729  -0.49810696]]. Action = [[-0.18448104 -0.2417314   0.11420333 -0.7337606 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1216 is [True, False, False, True, False, False]
State prediction error at timestep 1216 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1216 of -1
Current timestep = 1217. State = [[-0.25055856 -0.5027472 ]]. Action = [[ 0.1346722   0.13362283 -0.01735768 -0.28923047]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1217 is [True, False, False, True, False, False]
State prediction error at timestep 1217 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of -1
Current timestep = 1218. State = [[-0.2508058  -0.50229114]]. Action = [[ 0.05815941 -0.14291757 -0.05352288  0.33420563]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1218 is [True, False, False, True, False, False]
State prediction error at timestep 1218 is tensor(7.5049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1219. State = [[-0.2509162 -0.5025039]]. Action = [[ 0.20437175 -0.00750662 -0.1172744   0.15828216]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1219 is [True, False, False, True, False, False]
State prediction error at timestep 1219 is tensor(2.8717e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of -1
Current timestep = 1220. State = [[-0.24648534 -0.50223505]]. Action = [[ 0.17203718 -0.20230532 -0.02978705 -0.61427426]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1220 is [True, False, False, True, False, False]
State prediction error at timestep 1220 is tensor(7.1758e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1220 of -1
Current timestep = 1221. State = [[-0.24189655 -0.5054612 ]]. Action = [[ 0.23985595 -0.10731459 -0.02692342  0.27803302]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1221 is [True, False, False, True, False, False]
State prediction error at timestep 1221 is tensor(6.5108e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1222. State = [[-0.23281415 -0.5107939 ]]. Action = [[-0.23265752  0.02572697 -0.04882908 -0.9606335 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1222 is [True, False, False, True, False, False]
State prediction error at timestep 1222 is tensor(8.1769e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of -1
Current timestep = 1223. State = [[-0.22961225 -0.51545733]]. Action = [[-0.13676654  0.13170424 -0.19715936  0.29255414]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1223 is [True, False, False, True, False, False]
State prediction error at timestep 1223 is tensor(4.1503e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1224. State = [[-0.23041558 -0.5158893 ]]. Action = [[ 0.19876844 -0.12550367  0.11899322  0.10828733]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1224 is [True, False, False, True, False, False]
State prediction error at timestep 1224 is tensor(2.8585e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1224 of -1
Current timestep = 1225. State = [[-0.22871709 -0.51655227]]. Action = [[-0.02516265  0.23017517 -0.04783802  0.5855682 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1225 is [True, False, False, True, False, False]
State prediction error at timestep 1225 is tensor(1.0865e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of -1
Current timestep = 1226. State = [[-0.22748636 -0.51461583]]. Action = [[-0.1912923  -0.21968068  0.06240323 -0.37321103]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1226 is [True, False, False, True, False, False]
State prediction error at timestep 1226 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of -1
Current timestep = 1227. State = [[-0.23087482 -0.51846045]]. Action = [[-0.04287897  0.10909402 -0.12365708 -0.8963305 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1227 is [True, False, False, True, False, False]
State prediction error at timestep 1227 is tensor(6.5033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1228. State = [[-0.23227541 -0.51935416]]. Action = [[-0.21845037 -0.01019438  0.1705682  -0.9828676 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1228 is [True, False, False, True, False, False]
State prediction error at timestep 1228 is tensor(6.8229e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1228 of -1
Current timestep = 1229. State = [[-0.23835748 -0.5215336 ]]. Action = [[-0.20357376  0.09203932 -0.15393926 -0.09127903]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1229 is [True, False, False, True, False, False]
State prediction error at timestep 1229 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1230. State = [[-0.24782617 -0.5215263 ]]. Action = [[-0.18208504  0.23322749  0.02948439  0.00611401]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1230 is [True, False, False, True, False, False]
State prediction error at timestep 1230 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1230 of -1
Current timestep = 1231. State = [[-0.25728592 -0.5167888 ]]. Action = [[0.07322907 0.00635812 0.05612111 0.9280875 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1231 is [True, False, False, True, False, False]
State prediction error at timestep 1231 is tensor(7.5957e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1232. State = [[-0.2630416 -0.5121586]]. Action = [[-0.13104214  0.0274089  -0.14380485 -0.9571964 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1232 is [True, False, False, True, False, False]
State prediction error at timestep 1232 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1233. State = [[-0.26877865 -0.508733  ]]. Action = [[ 0.006475    0.02562416 -0.18629138 -0.97803867]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1233 is [True, False, False, True, False, False]
State prediction error at timestep 1233 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1233 of -1
Current timestep = 1234. State = [[-0.2714179 -0.5065948]]. Action = [[ 0.19892615 -0.13555348  0.16182226  0.32048726]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1234 is [True, False, False, True, False, False]
State prediction error at timestep 1234 is tensor(2.9188e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1234 of -1
Current timestep = 1235. State = [[-0.27118796 -0.5065815 ]]. Action = [[-0.22827469  0.16948348  0.0703755  -0.44479883]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1235 is [True, False, False, True, False, False]
State prediction error at timestep 1235 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1236. State = [[-0.2743174 -0.5041557]]. Action = [[-0.02989078  0.00392225 -0.2088061  -0.98342246]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1236 is [True, False, False, True, False, False]
State prediction error at timestep 1236 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1237. State = [[-0.27741358 -0.5019405 ]]. Action = [[ 0.14334899 -0.03745806 -0.15410484 -0.57991624]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1237 is [True, False, False, True, False, False]
State prediction error at timestep 1237 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of -1
Current timestep = 1238. State = [[-0.27718765 -0.5014241 ]]. Action = [[ 0.05363375 -0.14524107 -0.18427481 -0.05456924]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1238 is [True, False, False, True, False, False]
State prediction error at timestep 1238 is tensor(2.0035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of -1
Current timestep = 1239. State = [[-0.27676514 -0.501668  ]]. Action = [[ 0.22363463 -0.0287521   0.03750634 -0.9902143 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1239 is [True, False, False, True, False, False]
State prediction error at timestep 1239 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1240. State = [[-0.27259243 -0.5025103 ]]. Action = [[-0.10799226 -0.19878061  0.15257353 -0.24926627]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1240 is [True, False, False, True, False, False]
State prediction error at timestep 1240 is tensor(6.4142e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of -1
Current timestep = 1241. State = [[-0.27160063 -0.5069369 ]]. Action = [[ 0.10635906  0.11480731 -0.19228812  0.8306093 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1241 is [True, False, False, True, False, False]
State prediction error at timestep 1241 is tensor(2.9755e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of -1
Current timestep = 1242. State = [[-0.2689847 -0.5077241]]. Action = [[-0.23076412  0.06099719 -0.05011311  0.44685185]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1242 is [True, False, False, True, False, False]
State prediction error at timestep 1242 is tensor(1.4095e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1243. State = [[-0.27018967 -0.50849956]]. Action = [[-0.2151996   0.11802071  0.2455343   0.8061466 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1243 is [True, False, False, True, False, False]
State prediction error at timestep 1243 is tensor(1.4924e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1243 of -1
Current timestep = 1244. State = [[-0.2760496  -0.50714326]]. Action = [[-0.2378128   0.19687128  0.04416814 -0.85856074]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1244 is [True, False, False, True, False, False]
State prediction error at timestep 1244 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of -1
Current timestep = 1245. State = [[-0.28626657 -0.5018759 ]]. Action = [[ 0.10901257  0.0525476  -0.20326672  0.8815448 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1245 is [True, False, False, True, False, False]
State prediction error at timestep 1245 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1246. State = [[-0.29119593 -0.49695504]]. Action = [[-0.14791167  0.05960375 -0.00946978 -0.86113816]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1246 is [True, False, False, True, False, False]
State prediction error at timestep 1246 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1246 of -1
Current timestep = 1247. State = [[-0.29688323 -0.49388048]]. Action = [[-0.03297873 -0.04886043  0.1522387   0.2284199 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1247 is [True, False, False, True, False, False]
State prediction error at timestep 1247 is tensor(9.5158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1247 of -1
Current timestep = 1248. State = [[-0.30107695 -0.49179864]]. Action = [[-1.1292815e-01  1.5314239e-01  4.9448013e-04  8.2729053e-01]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1248 is [True, False, False, True, False, False]
State prediction error at timestep 1248 is tensor(1.7491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1249. State = [[-0.30635288 -0.48772493]]. Action = [[-0.0874097   0.04609025  0.10936385 -0.42718804]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1249 is [True, False, False, True, False, False]
State prediction error at timestep 1249 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1249 of -1
Current timestep = 1250. State = [[-0.31271893 -0.48292154]]. Action = [[-0.24274647  0.13924247  0.10745052 -0.94371015]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1250 is [True, False, False, True, False, False]
State prediction error at timestep 1250 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1251. State = [[-0.32296914 -0.4786002 ]]. Action = [[ 0.03960064 -0.22445798  0.20843983 -0.61891186]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1251 is [True, False, False, True, False, False]
State prediction error at timestep 1251 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1252. State = [[-0.33052972 -0.47868744]]. Action = [[ 0.12456122  0.08587116  0.18841922 -0.90003985]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1252 is [True, False, False, True, False, False]
State prediction error at timestep 1252 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1252 of -1
Current timestep = 1253. State = [[-0.33084285 -0.4779508 ]]. Action = [[-0.17605609  0.01826972 -0.2266593  -0.23675442]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1253 is [True, False, False, True, False, False]
State prediction error at timestep 1253 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1254. State = [[-0.33504865 -0.47718456]]. Action = [[ 0.05289894 -0.20789586 -0.0181545  -0.19288641]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1254 is [True, False, False, True, False, False]
State prediction error at timestep 1254 is tensor(3.2821e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1254 of -1
Current timestep = 1255. State = [[-0.33731627 -0.47977182]]. Action = [[ 0.04049587 -0.0893617  -0.03907557  0.6593789 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1255 is [True, False, False, True, False, False]
State prediction error at timestep 1255 is tensor(1.6914e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1256. State = [[-0.338802   -0.48270044]]. Action = [[-0.10435963 -0.20634775 -0.10302076 -0.5992028 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1256 is [True, False, False, True, False, False]
State prediction error at timestep 1256 is tensor(8.8463e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1257. State = [[-0.34283718 -0.48930603]]. Action = [[0.11492658 0.08808511 0.18954417 0.45249033]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1257 is [True, False, False, True, False, False]
State prediction error at timestep 1257 is tensor(2.4163e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1257 of -1
Current timestep = 1258. State = [[-0.3432296  -0.49105394]]. Action = [[-0.1528655  -0.09964111 -0.14421526 -0.26314557]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1258 is [True, False, False, True, False, False]
State prediction error at timestep 1258 is tensor(7.2645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1258 of -1
Current timestep = 1259. State = [[-0.34574336 -0.4952091 ]]. Action = [[ 0.14222783  0.18979579 -0.13899776  0.5150955 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 1259 is [True, False, False, True, False, False]
State prediction error at timestep 1259 is tensor(3.0276e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1260. State = [[-0.3449335  -0.49438697]]. Action = [[-0.2427211  -0.04632661 -0.08245054 -0.21836627]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 1260 is [True, False, False, True, False, False]
State prediction error at timestep 1260 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1260 of -1
Current timestep = 1261. State = [[-0.34838068 -0.49541068]]. Action = [[ 0.09783232  0.22777599  0.13762543 -0.78469956]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 1261 is [True, False, False, True, False, False]
State prediction error at timestep 1261 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1261 of -1
Current timestep = 1262. State = [[-0.348121  -0.4925016]]. Action = [[-0.09729418  0.00014991  0.00524083  0.03595507]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 1262 is [True, False, False, True, False, False]
State prediction error at timestep 1262 is tensor(2.0410e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1263. State = [[-0.35026708 -0.49108002]]. Action = [[-0.16240571 -0.09845997 -0.10003354  0.6455958 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 1263 is [True, False, False, True, False, False]
State prediction error at timestep 1263 is tensor(2.8323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1263 of -1
Current timestep = 1264. State = [[-0.35612714 -0.49262136]]. Action = [[-0.23693344  0.17203528 -0.06624156 -0.7754443 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 1264 is [True, False, False, True, False, False]
State prediction error at timestep 1264 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1264 of -1
Current timestep = 1265. State = [[-0.36614308 -0.49038085]]. Action = [[-0.01797223  0.21025693 -0.11942117 -0.4367987 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 1265 is [True, False, False, True, False, False]
State prediction error at timestep 1265 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1266. State = [[-0.37506995 -0.48088017]]. Action = [[-0.15043856  0.23063362  0.19016027 -0.37022913]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 1266 is [True, False, False, True, False, False]
State prediction error at timestep 1266 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1267. State = [[-0.38290155 -0.4714146 ]]. Action = [[ 0.04664189 -0.22697718 -0.16474244  0.47750807]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 1267 is [True, False, False, True, False, False]
State prediction error at timestep 1267 is tensor(3.4644e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1267 of -1
Current timestep = 1268. State = [[-0.38873306 -0.46738935]]. Action = [[ 0.09603453  0.02141556 -0.16040435 -0.39165992]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 1268 is [True, False, False, True, False, False]
State prediction error at timestep 1268 is tensor(5.6897e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1268 of -1
Current timestep = 1269. State = [[-0.39000827 -0.46529654]]. Action = [[ 0.10641655  0.19038361 -0.0494473   0.58183753]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 1269 is [True, False, False, True, False, False]
State prediction error at timestep 1269 is tensor(7.1528e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1270. State = [[-0.38631624 -0.45954788]]. Action = [[ 0.07453883  0.03828573 -0.03184283  0.18286204]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 1270 is [True, False, False, True, False, False]
State prediction error at timestep 1270 is tensor(1.4401e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1270 of -1
Current timestep = 1271. State = [[-0.38195488 -0.4536878 ]]. Action = [[0.24018091 0.1835615  0.19542882 0.48773324]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 1271 is [True, False, False, True, False, False]
State prediction error at timestep 1271 is tensor(5.2664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1271 of -1
Current timestep = 1272. State = [[-0.3737148 -0.4452706]]. Action = [[-0.0663808   0.00220862 -0.04207267 -0.34954083]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 1272 is [True, False, False, True, False, False]
State prediction error at timestep 1272 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1273. State = [[-0.36920628 -0.43965554]]. Action = [[ 0.21420571  0.02213722  0.04511371 -0.82716155]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 1273 is [True, False, False, True, False, False]
State prediction error at timestep 1273 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1273 of -1
Current timestep = 1274. State = [[-0.36287242 -0.43364856]]. Action = [[ 0.20217276 -0.06854931 -0.16889533 -0.5604208 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 1274 is [True, False, False, True, False, False]
State prediction error at timestep 1274 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1274 of -1
Current timestep = 1275. State = [[-0.35447878 -0.43041146]]. Action = [[ 0.07094568 -0.0559341   0.02459323 -0.05546963]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 1275 is [True, False, False, True, False, False]
State prediction error at timestep 1275 is tensor(6.7269e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1276. State = [[-0.34851292 -0.42988035]]. Action = [[-0.21698888 -0.12153718 -0.18567656 -0.76827186]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 1276 is [True, False, False, True, False, False]
State prediction error at timestep 1276 is tensor(6.3491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1277. State = [[-0.34893173 -0.43235448]]. Action = [[ 0.09254256  0.19860834  0.21628428 -0.9512126 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 1277 is [True, False, False, True, False, False]
State prediction error at timestep 1277 is tensor(2.9808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1277 of -1
Current timestep = 1278. State = [[-0.3460675  -0.42979717]]. Action = [[ 0.22751045  0.22988     0.2182796  -0.2821753 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 1278 is [True, False, False, True, False, False]
State prediction error at timestep 1278 is tensor(2.2683e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1278 of -1
Current timestep = 1279. State = [[-0.3386742  -0.42269462]]. Action = [[-0.07782479 -0.10710634 -0.24350572  0.6344788 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 1279 is [True, False, False, True, False, False]
State prediction error at timestep 1279 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1280. State = [[-0.33583775 -0.41998765]]. Action = [[ 0.20816952  0.04363912  0.19151449 -0.4778949 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 1280 is [True, False, False, True, False, False]
State prediction error at timestep 1280 is tensor(7.4573e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1280 of -1
Current timestep = 1281. State = [[-0.3298524 -0.4163419]]. Action = [[ 0.0787631  -0.15440805 -0.1237368  -0.8479102 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 1281 is [True, False, False, True, False, False]
State prediction error at timestep 1281 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1281 of -1
Current timestep = 1282. State = [[-0.3242601 -0.4172209]]. Action = [[ 0.15438813  0.18739587 -0.08619705  0.30563843]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 1282 is [True, False, False, True, False, False]
State prediction error at timestep 1282 is tensor(2.5674e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1283. State = [[-0.31577888 -0.4140374 ]]. Action = [[ 0.0483706  -0.24147217  0.04646677 -0.56213665]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 1283 is [True, False, False, True, False, False]
State prediction error at timestep 1283 is tensor(7.4052e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1284. State = [[-0.31012127 -0.41710576]]. Action = [[-0.05279762 -0.14240636  0.19572592  0.8136406 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 1284 is [True, False, False, True, False, False]
State prediction error at timestep 1284 is tensor(5.8908e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1284 of 1
Current timestep = 1285. State = [[-0.30653495 -0.42282936]]. Action = [[ 0.2477575   0.03227681 -0.19177222  0.14413548]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 1285 is [True, False, False, True, False, False]
State prediction error at timestep 1285 is tensor(3.4619e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1286. State = [[-0.29886538 -0.42554134]]. Action = [[ 0.09861311  0.22879261 -0.08418477  0.89903903]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 1286 is [True, False, False, True, False, False]
State prediction error at timestep 1286 is tensor(3.9279e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1286 of 1
Current timestep = 1287. State = [[-0.28980508 -0.42180797]]. Action = [[ 0.0146054  -0.06034991  0.09575647 -0.6233838 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 1287 is [True, False, False, True, False, False]
State prediction error at timestep 1287 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1287 of 1
Current timestep = 1288. State = [[-0.28511262 -0.42092252]]. Action = [[ 0.13617656 -0.21838352 -0.21065082 -0.96654195]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 1288 is [True, False, False, True, False, False]
State prediction error at timestep 1288 is tensor(8.4429e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1288 of 1
Current timestep = 1289. State = [[-0.27905273 -0.4261679 ]]. Action = [[-0.20550355 -0.18041717  0.02230233 -0.5993972 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 1289 is [True, False, False, True, False, False]
State prediction error at timestep 1289 is tensor(6.3777e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1289 of 1
Current timestep = 1290. State = [[-0.27971077 -0.43371156]]. Action = [[-0.13623255  0.19775623 -0.0169645   0.559906  ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 1290 is [True, False, False, True, False, False]
State prediction error at timestep 1290 is tensor(1.4195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1290 of 1
Current timestep = 1291. State = [[-0.28136432 -0.4350446 ]]. Action = [[ 0.04984969 -0.126773   -0.09188715  0.06580698]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 1291 is [True, False, False, True, False, False]
State prediction error at timestep 1291 is tensor(4.8245e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1292. State = [[-0.28264257 -0.43730196]]. Action = [[-0.19760881  0.07108545  0.03727466  0.33170497]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 1292 is [True, False, False, True, False, False]
State prediction error at timestep 1292 is tensor(3.6568e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1292 of 1
Current timestep = 1293. State = [[-0.28681302 -0.4385118 ]]. Action = [[-0.13934022  0.23434922  0.24042594  0.6856265 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 1293 is [True, False, False, True, False, False]
State prediction error at timestep 1293 is tensor(2.9102e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1293 of 1
Current timestep = 1294. State = [[-0.29163438 -0.43577695]]. Action = [[ 0.11864716 -0.08206129  0.03279379 -0.6318552 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 1294 is [True, False, False, True, False, False]
State prediction error at timestep 1294 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1294 of 1
Current timestep = 1295. State = [[-0.29249874 -0.43577382]]. Action = [[-0.19107105  0.02071512 -0.10282803 -0.26754218]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 1295 is [True, False, False, True, False, False]
State prediction error at timestep 1295 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1296. State = [[-0.297206  -0.4354704]]. Action = [[ 0.04235977 -0.14324823 -0.18783589  0.3009367 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 1296 is [True, False, False, True, False, False]
State prediction error at timestep 1296 is tensor(1.3663e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1297. State = [[-0.3007137  -0.43717504]]. Action = [[ 0.17204303 -0.06388527  0.12291765  0.47015452]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 1297 is [True, False, False, True, False, False]
State prediction error at timestep 1297 is tensor(5.5849e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1297 of 1
Current timestep = 1298. State = [[-0.3004656 -0.4380184]]. Action = [[-0.13899897 -0.16286838  0.0386529  -0.26369977]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 1298 is [True, False, False, True, False, False]
State prediction error at timestep 1298 is tensor(4.4471e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1298 of 1
Current timestep = 1299. State = [[-0.3036091  -0.44304854]]. Action = [[ 0.11560202  0.04111892 -0.0468878  -0.31210363]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 1299 is [True, False, False, True, False, False]
State prediction error at timestep 1299 is tensor(1.5868e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1300. State = [[-0.30327675 -0.44519696]]. Action = [[-0.12083989 -0.19390059 -0.05829619 -0.13328803]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 1300 is [True, False, False, True, False, False]
State prediction error at timestep 1300 is tensor(2.4403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1301. State = [[-0.3060861  -0.45070758]]. Action = [[ 0.08413306 -0.16670658  0.17762944  0.8093951 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 1301 is [True, False, False, True, False, False]
State prediction error at timestep 1301 is tensor(6.1874e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1301 of 1
Current timestep = 1302. State = [[-0.30676895 -0.4583793 ]]. Action = [[-0.03463058 -0.05072398  0.02269253 -0.7043228 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 1302 is [True, False, False, True, False, False]
State prediction error at timestep 1302 is tensor(6.5833e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1302 of 1
Current timestep = 1303. State = [[-0.30754656 -0.46617046]]. Action = [[-0.20158507 -0.04726365  0.08287045  0.84024286]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 1303 is [True, False, False, True, False, False]
State prediction error at timestep 1303 is tensor(1.4268e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1304. State = [[-0.3124632  -0.47169125]]. Action = [[-0.02241655  0.13675892 -0.11479697 -0.78200316]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 1304 is [True, False, False, True, False, False]
State prediction error at timestep 1304 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1305. State = [[-0.31366068 -0.47275543]]. Action = [[ 0.20607525  0.04504362  0.03520587 -0.08263946]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 1305 is [True, False, False, True, False, False]
State prediction error at timestep 1305 is tensor(2.2894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1305 of 1
Current timestep = 1306. State = [[-0.31215125 -0.4716099 ]]. Action = [[-0.10791123 -0.21116917 -0.19624431 -0.21652722]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 1306 is [True, False, False, True, False, False]
State prediction error at timestep 1306 is tensor(1.1929e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1306 of 1
Current timestep = 1307. State = [[-0.31421408 -0.4750815 ]]. Action = [[-0.23588458  0.04493436 -0.00136317  0.9627278 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 1307 is [True, False, False, True, False, False]
State prediction error at timestep 1307 is tensor(8.1153e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1308. State = [[-0.31943366 -0.4796056 ]]. Action = [[-0.22632591  0.04511142 -0.09343246  0.0275749 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 1308 is [True, False, False, True, False, False]
State prediction error at timestep 1308 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1308 of 1
Current timestep = 1309. State = [[-0.3273878 -0.4823723]]. Action = [[-0.03055106 -0.08280584  0.16036329 -0.8328811 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 1309 is [True, False, False, True, False, False]
State prediction error at timestep 1309 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1309 of 1
Current timestep = 1310. State = [[-0.33401114 -0.4864943 ]]. Action = [[ 0.03656656  0.08013296 -0.10351284  0.51868916]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 1310 is [True, False, False, True, False, False]
State prediction error at timestep 1310 is tensor(2.4444e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1311. State = [[-0.33696154 -0.48713282]]. Action = [[-0.19937621  0.0555566  -0.14199495 -0.55680513]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 1311 is [True, False, False, True, False, False]
State prediction error at timestep 1311 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1311 of 1
Current timestep = 1312. State = [[-0.3442868  -0.48530725]]. Action = [[-0.1914494  -0.04296549 -0.14648014 -0.029796  ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 1312 is [True, False, False, True, False, False]
State prediction error at timestep 1312 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1312 of 1
Current timestep = 1313. State = [[-0.35488784 -0.48460972]]. Action = [[ 0.2252624   0.01210213  0.11621374 -0.9339031 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 1313 is [True, False, False, True, False, False]
State prediction error at timestep 1313 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1314. State = [[-0.35598895 -0.48483878]]. Action = [[-0.2156263  -0.1395922  -0.07478467  0.9293153 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 1314 is [True, False, False, True, False, False]
State prediction error at timestep 1314 is tensor(2.4642e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1314 of 1
Current timestep = 1315. State = [[-0.36227956 -0.4885029 ]]. Action = [[-0.05667275 -0.21220599  0.24110854 -0.12436569]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 1315 is [True, False, False, True, False, False]
State prediction error at timestep 1315 is tensor(6.4059e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1315 of 1
Current timestep = 1316. State = [[-0.36855024 -0.4946734 ]]. Action = [[-0.03906827  0.13013798  0.16875798  0.4897964 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 1316 is [True, False, False, True, False, False]
State prediction error at timestep 1316 is tensor(2.5066e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1317. State = [[-0.37201536 -0.4960712 ]]. Action = [[ 0.23089686  0.06045687 -0.1409275   0.07847643]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 1317 is [True, False, False, True, False, False]
State prediction error at timestep 1317 is tensor(7.2024e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1317 of 1
Current timestep = 1318. State = [[-0.36982507 -0.49473605]]. Action = [[ 0.10907337 -0.12595123  0.09265703  0.7252834 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 1318 is [True, False, False, True, False, False]
State prediction error at timestep 1318 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1319. State = [[-0.36550847 -0.4961529 ]]. Action = [[ 0.23874152 -0.23897439 -0.14998063 -0.67058676]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 1319 is [True, False, False, True, False, False]
State prediction error at timestep 1319 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1320. State = [[-0.35786712 -0.5018815 ]]. Action = [[ 0.04897586 -0.21877268  0.04129407  0.630429  ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 1320 is [True, False, False, True, False, False]
State prediction error at timestep 1320 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1320 of 1
Current timestep = 1321. State = [[-0.35084477 -0.5103764 ]]. Action = [[ 0.1404388   0.21209157 -0.15294853 -0.46449268]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 1321 is [True, False, False, True, False, False]
State prediction error at timestep 1321 is tensor(3.9705e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1322. State = [[-0.3431114  -0.51263857]]. Action = [[-0.10270269 -0.1358838  -0.10610132  0.9529238 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 1322 is [True, False, False, True, False, False]
State prediction error at timestep 1322 is tensor(2.8931e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1323. State = [[-0.34093395 -0.51556784]]. Action = [[ 0.2255665  -0.10699521 -0.20914285  0.3235097 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 1323 is [True, False, False, True, False, False]
State prediction error at timestep 1323 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1324. State = [[-0.33405417 -0.519588  ]]. Action = [[ 0.16823816  0.11236796 -0.12395972  0.20430732]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 1324 is [True, False, False, True, False, False]
State prediction error at timestep 1324 is tensor(9.8401e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1325. State = [[-0.32472515 -0.5218504 ]]. Action = [[ 0.08531207  0.01152447  0.19811633 -0.67707664]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 1325 is [True, False, False, True, False, False]
State prediction error at timestep 1325 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1326. State = [[-0.3170172  -0.52254575]]. Action = [[-0.10153699 -0.05721295 -0.21387719 -0.9662748 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 1326 is [True, False, False, True, False, False]
State prediction error at timestep 1326 is tensor(0.0001, grad_fn=<MseLossBackward0>)
