Current timestep = 0. State = [[-0.33200356  0.07271446]]. Action = [[0.00541566 0.09844042 0.         0.7259095 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0728, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33765858  0.07564745]]. Action = [[-0.08993776 -0.0094047   0.          0.9253125 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0648, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.33873436  0.07375087]]. Action = [[ 0.06052893 -0.05036081  0.          0.02254677]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.33406782  0.07549009]]. Action = [[ 0.0804879   0.04935644  0.         -0.5039493 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0370, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.33111173  0.07515169]]. Action = [[ 0.02323902 -0.05198834  0.          0.06520462]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.3336523   0.07605299]]. Action = [[-0.06594089  0.04402239  0.         -0.7165636 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0192, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.3404982   0.07239919]]. Action = [[-0.09445891 -0.09541254  0.         -0.55163175]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0172, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.34806493  0.06975873]]. Action = [[-0.08651658  0.01674875  0.          0.7542231 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0314, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.3487415  0.0662501]]. Action = [[ 0.06659523 -0.06996306  0.          0.36502624]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0296, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.344605    0.06122589]]. Action = [[ 0.05228078 -0.04850364  0.          0.9673922 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0176, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.34685206  0.06342745]]. Action = [[-0.09423779  0.0977206   0.          0.9580705 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.3535939   0.07009483]]. Action = [[-0.06764689  0.0887687   0.         -0.7774557 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.35818166  0.068878  ]]. Action = [[-0.02134483 -0.08022624  0.         -0.955386  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.357128    0.06248068]]. Action = [[ 0.06951638 -0.08381344  0.          0.8053243 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.35471255  0.0562351 ]]. Action = [[ 0.03642271 -0.07331226  0.          0.99213016]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.35482895  0.05191045]]. Action = [[-0.00998229 -0.0321989   0.         -0.21287024]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.35734308  0.04515296]]. Action = [[-0.03481996 -0.09524327  0.         -0.46999443]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.35619718  0.04455035]]. Action = [[ 0.06345893  0.08002204  0.         -0.04349875]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.35830095  0.04921931]]. Action = [[-0.07529083  0.0651663   0.         -0.8052634 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.3616775   0.05155191]]. Action = [[-0.01562006  0.0174247   0.         -0.62026036]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.3587381   0.05363678]]. Action = [[0.08626472 0.0260647  0.         0.31375027]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.36107     0.05516472]]. Action = [[-0.09375616  0.01054572  0.          0.03166902]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.3618421   0.05136047]]. Action = [[ 0.06803507 -0.09286536  0.         -0.54774433]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.3562033   0.04434509]]. Action = [[ 0.08814909 -0.09026853  0.          0.20188606]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.3497388   0.03740248]]. Action = [[ 0.07297248 -0.0834576   0.         -0.6371082 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.34378877  0.03104583]]. Action = [[ 0.05844457 -0.06646217  0.         -0.7165698 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.33998606  0.02528988]]. Action = [[ 0.01725103 -0.05295527  0.         -0.5040044 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.33443058  0.01965936]]. Action = [[ 0.07439459 -0.04956023  0.         -0.8439672 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.33429712  0.0194266 ]]. Action = [[-0.07995008  0.07241974  0.          0.71481645]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.33233368  0.02291624]]. Action = [[ 0.07694603  0.05753786  0.         -0.20810843]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.32796618  0.02634157]]. Action = [[0.03742055 0.04211733 0.         0.91423655]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.32677415  0.03022512]]. Action = [[-0.00744186  0.05501559  0.         -0.6934837 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.3223976   0.02857809]]. Action = [[ 0.09140796 -0.07252084  0.         -0.01692754]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.3202032   0.02950635]]. Action = [[-0.03023296  0.06430451  0.         -0.43713135]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of -1
Current timestep = 34. State = [[-0.32237676  0.03408321]]. Action = [[-0.03482145  0.05471426  0.         -0.9834916 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.32109657  0.03492006]]. Action = [[ 0.0472557  -0.02733342  0.          0.73025405]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of -1
Current timestep = 36. State = [[-0.32312465  0.03219523]]. Action = [[-0.0731885  -0.04809514  0.         -0.937019  ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.3260687   0.02619776]]. Action = [[-0.02292933 -0.08670982  0.         -0.6659845 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.32527304  0.02453247]]. Action = [[ 0.0258904   0.02616487  0.         -0.6786668 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.32829463  0.02853839]]. Action = [[-0.08529007  0.07416365  0.         -0.49786788]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.32820794  0.02714203]]. Action = [[ 0.06223632 -0.07916638  0.         -0.47335035]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.32111454  0.02312282]]. Action = [[ 0.09674483 -0.03582124  0.         -0.4191662 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.31527182  0.02679236]]. Action = [[ 0.03709317  0.0962512   0.         -0.67376995]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.31379992  0.0279976 ]]. Action = [[-0.01107495 -0.03429965  0.          0.8846302 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(1.8065e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.3112405   0.02771086]]. Action = [[ 0.04426541  0.01278218  0.         -0.26855677]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.30651972  0.02616115]]. Action = [[ 0.05663063 -0.04010892  0.          0.53925955]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.30327317  0.02811118]]. Action = [[0.01558201 0.06080414 0.         0.5975522 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.30340296  0.03444983]]. Action = [[-0.02641177  0.0902904   0.         -0.18074864]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of -1
Current timestep = 48. State = [[-0.30015782  0.03537535]]. Action = [[ 0.07648215 -0.04903908  0.         -0.69805765]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(3.1249e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.29360336  0.0383439 ]]. Action = [[ 0.07061297  0.07188875  0.         -0.41144264]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.28817537  0.04524394]]. Action = [[ 0.04790884  0.07712203  0.         -0.8204959 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.2904344  0.0490685]]. Action = [[-0.09370343  0.01025311  0.          0.4179448 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.29366356  0.04596465]]. Action = [[-0.02810814 -0.08124532  0.         -0.43557835]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of -1
Current timestep = 53. State = [[-0.2972643   0.04256858]]. Action = [[-0.06981336 -0.02448823  0.         -0.25503105]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.29638407  0.03775993]]. Action = [[ 0.04660351 -0.08059502  0.          0.02909517]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.29651415  0.03165997]]. Action = [[-0.04718876 -0.06774168  0.          0.3410411 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.29738095  0.02587046]]. Action = [[-0.01028438 -0.05578712  0.         -0.17790437]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.3000432   0.02591721]]. Action = [[-0.06301561  0.06063855  0.          0.86527216]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.29803222  0.03011592]]. Action = [[0.07283116 0.06624048 0.         0.28875422]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.29602906  0.03535287]]. Action = [[-0.00309052  0.06509946  0.         -0.5184825 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.29323068  0.03665657]]. Action = [[ 0.05820014 -0.02036295  0.          0.45821416]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.2956404   0.03575731]]. Action = [[-0.0841277  -0.00997018  0.         -0.5506213 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.29683727  0.03364782]]. Action = [[ 0.03129803 -0.03507666  0.          0.7944038 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.29419908  0.03335484]]. Action = [[ 0.04549428  0.01129968  0.         -0.43341428]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.29528454  0.03329424]]. Action = [[-0.04438785 -0.01071407  0.          0.25335062]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.29824018  0.03284159]]. Action = [[-3.1080686e-02  2.7224422e-05  0.0000000e+00  6.3837421e-01]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.303907    0.03028511]]. Action = [[-0.09009418 -0.04339439  0.          0.83660674]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.30916083  0.02371452]]. Action = [[-0.04517923 -0.09168368  0.         -0.86057544]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.30879715  0.01834261]]. Action = [[ 0.04574548 -0.03653355  0.          0.09003139]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.3051684   0.01699275]]. Action = [[0.05438722 0.00799064 0.         0.03984189]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.29963318  0.01433294]]. Action = [[ 0.08341002 -0.05081475  0.         -0.6411432 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(9.5336e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.3006773   0.01042087]]. Action = [[-0.07531996 -0.03231976  0.          0.5916934 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.30575603  0.00717117]]. Action = [[-0.05174939 -0.02101796  0.         -0.5494784 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.30544585  0.00430788]]. Action = [[ 0.05206216 -0.02167697  0.          0.84222984]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.30515304 -0.00152552]]. Action = [[-0.01342889 -0.0852934   0.          0.6853478 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.30228254 -0.00644038]]. Action = [[ 0.07031284 -0.02615242  0.          0.17346323]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.2963501  -0.01005012]]. Action = [[ 0.07943361 -0.03679076  0.          0.2391454 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.29625136 -0.01381018]]. Action = [[-0.05091912 -0.02994373  0.          0.7368109 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.29361966 -0.01188528]]. Action = [[ 0.08149233  0.08527165  0.         -0.8689882 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.29188824 -0.00980655]]. Action = [[-0.01056205  0.00654141  0.          0.49344182]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.2881469  -0.01301119]]. Action = [[ 0.08147631 -0.06184937  0.         -0.6757772 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.28916475 -0.01157999]]. Action = [[-0.08469594  0.0886127   0.         -0.31675875]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.2903537  -0.00721604]]. Action = [[ 0.0368833   0.03918499  0.         -0.30542982]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.28732657 -0.00748513]]. Action = [[ 0.05692025 -0.04520543  0.          0.70227444]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of -1
Current timestep = 84. State = [[-0.2878421  -0.00392785]]. Action = [[-0.03947507  0.08750451  0.          0.41824818]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.29441723 -0.00213501]]. Action = [[-0.09311665 -0.02550938  0.         -0.18072444]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-3.0059695e-01 -5.4255041e-05]]. Action = [[-0.04835773  0.04679144  0.         -0.02345401]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.3067414   0.00295134]]. Action = [[-0.07163161  0.02372412  0.         -0.8872062 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.3082128  0.0074452]]. Action = [[0.03568492 0.05911935 0.         0.162184  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.30659458  0.01486062]]. Action = [[0.03528406 0.08349428 0.         0.8402889 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.30480543  0.01809068]]. Action = [[ 0.03501143 -0.02340581  0.         -0.3067094 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.30581295  0.01424845]]. Action = [[-0.02322188 -0.09826272  0.          0.7990804 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of -1
Current timestep = 92. State = [[-0.30951196  0.0089315 ]]. Action = [[-0.04830756 -0.0627652   0.          0.12094736]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.30931747  0.01104592]]. Action = [[ 0.04364277  0.08020239  0.         -0.8038029 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(1.0047e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.30965078  0.01148064]]. Action = [[-0.02515828 -0.04465627  0.         -0.60987496]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.3100498   0.01434306]]. Action = [[ 0.00986286  0.07750105  0.         -0.8995726 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(7.5127e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.31121048  0.01393389]]. Action = [[-0.02353384 -0.05762506  0.         -0.60331213]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.3085424   0.01197082]]. Action = [[ 0.070751   -0.01096109  0.         -0.04766619]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.3062877   0.01270822]]. Action = [[ 0.00442292  0.01713154  0.         -0.5998781 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.30527318  0.01739801]]. Action = [[0.01520742 0.08082726 0.         0.15865743]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.30183885  0.02229305]]. Action = [[ 0.06217829  0.03874362  0.         -0.5760633 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.29570073  0.02883524]]. Action = [[ 0.0901582   0.08278351  0.         -0.8822595 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.29367077  0.03470835]]. Action = [[-0.0094519   0.03993601  0.         -0.49640703]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of -1
Current timestep = 103. State = [[-0.29108047  0.03368153]]. Action = [[ 0.05905772 -0.07272185  0.          0.46678245]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.28602022  0.0336577 ]]. Action = [[ 0.06297351  0.01837248  0.         -0.2758999 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.2801497   0.03144348]]. Action = [[ 0.06790289 -0.07299171  0.         -0.21524668]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.27614984  0.03477504]]. Action = [[ 0.01809931  0.09764596  0.         -0.10104781]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.27208892  0.03683561]]. Action = [[ 0.05452771 -0.02709975  0.          0.55941904]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(8.1036e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.2660282   0.03716706]]. Action = [[0.07015801 0.00659802 0.         0.58787763]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.2638039   0.03879898]]. Action = [[-0.0215693   0.02030096  0.          0.52914405]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.26429313  0.04389152]]. Action = [[-0.02491983  0.08546781  0.          0.50494885]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.2655679   0.05050677]]. Action = [[-0.03164105  0.07242174  0.          0.16014576]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.26548317  0.04908315]]. Action = [[ 0.00246777 -0.0898325   0.         -0.7987793 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.26064172  0.04318456]]. Action = [[ 0.07515723 -0.08087635  0.          0.7534853 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.26036817  0.04061092]]. Action = [[-0.07134031 -0.0028384   0.         -0.11235863]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.25891456  0.04493467]]. Action = [[0.05118532 0.0895442  0.         0.5188246 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-0.25247008  0.04370558]]. Action = [[ 0.09683908 -0.08866785  0.          0.44304323]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.2489851   0.03931946]]. Action = [[-0.0044087  -0.04120788  0.          0.8576002 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.24781315  0.04217437]]. Action = [[ 0.00530191  0.09038954  0.         -0.62113094]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.24966174  0.04777111]]. Action = [[-0.05403317  0.06446064  0.         -0.16239929]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.2487596   0.04783227]]. Action = [[ 0.04193691 -0.04260886  0.          0.21299982]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of -1
Current timestep = 121. State = [[-0.24258195  0.04270651]]. Action = [[ 0.09060705 -0.0837364   0.          0.25768077]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.23796155  0.04068163]]. Action = [[0.02295909 0.01132102 0.         0.6684222 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.2399733   0.03721723]]. Action = [[-0.07782979 -0.06486795  0.         -0.363513  ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.23872198  0.0384182 ]]. Action = [[0.05017491 0.07661282 0.         0.10207725]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of -1
Current timestep = 125. State = [[-0.23350745  0.04321812]]. Action = [[ 0.06756625  0.05523207  0.         -0.7454895 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of -1
Current timestep = 126. State = [[-0.22796582  0.04305313]]. Action = [[ 0.06291033 -0.04362198  0.         -0.05658686]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.2241683   0.04554848]]. Action = [[0.02717473 0.06997598 0.         0.67048633]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.22450393  0.04695167]]. Action = [[-0.03604712 -0.01620714  0.         -0.72894824]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.22337775  0.04415551]]. Action = [[ 0.02755881 -0.05127265  0.         -0.915325  ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.22206877  0.04082085]]. Action = [[-0.00632673 -0.03605777  0.          0.8994324 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.21924569  0.03954763]]. Action = [[ 0.0412521   0.00103501  0.         -0.87173915]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of -1
Current timestep = 132. State = [[-0.22135936  0.03753572]]. Action = [[-0.08768039 -0.03338175  0.          0.8357148 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.2204214   0.03607384]]. Action = [[ 5.0968774e-02 -3.2575428e-04  0.0000000e+00 -4.2148542e-01]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.21872088  0.04012722]]. Action = [[-0.00586711  0.08829299  0.          0.16818058]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.21568362  0.04588864]]. Action = [[ 0.05692845  0.06077442  0.         -0.04340929]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.21505609  0.0444021 ]]. Action = [[-0.02477983 -0.07835262  0.         -0.87144023]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of -1
Current timestep = 137. State = [[-0.21634163  0.0424034 ]]. Action = [[-0.02257764  0.00159383  0.          0.02098048]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.21420468  0.03886198]]. Action = [[ 0.0488688  -0.06980922  0.          0.8141012 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of -1
Current timestep = 139. State = [[-0.20817012  0.03216727]]. Action = [[ 0.08616679 -0.09004081  0.         -0.00776649]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.20259118  0.02916622]]. Action = [[ 0.04916102  0.00554637  0.         -0.4089489 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.2037951   0.03215624]]. Action = [[-0.07565592  0.07851774  0.         -0.6328606 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.20548047  0.03529344]]. Action = [[-0.00046755  0.02942305  0.          0.07744467]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.20720913  0.03122443]]. Action = [[-0.03658811 -0.09496297  0.          0.644516  ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of -1
Current timestep = 144. State = [[-0.20385076  0.03116766]]. Action = [[0.08715435 0.06216987 0.         0.26509714]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.20544524  0.0295107 ]]. Action = [[-0.08741311 -0.06690505  0.          0.41745043]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.21090084  0.03087016]]. Action = [[-0.07049177  0.07393     0.          0.669466  ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.21011066  0.02869127]]. Action = [[ 0.06850445 -0.0857603   0.          0.11973751]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.2055686   0.02614064]]. Action = [[ 0.04588377  0.00333576  0.         -0.7473986 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.2012065   0.02155177]]. Action = [[ 0.04767127 -0.08286054  0.         -0.93358487]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.19458647  0.02219513]]. Action = [[0.0933686  0.07259775 0.         0.4175378 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.18874376  0.02272374]]. Action = [[ 0.05330098 -0.02246772  0.         -0.5626638 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.18841195  0.0175999 ]]. Action = [[-0.04296484 -0.08024333  0.          0.24317753]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.18784177  0.01920415]]. Action = [[0.01574864 0.097436   0.         0.7415829 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.18868059  0.0235157 ]]. Action = [[-0.04002708  0.04124393  0.         -0.84940946]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.19083178  0.02051296]]. Action = [[-0.03663889 -0.08263262  0.         -0.2986021 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.19161434  0.0197454 ]]. Action = [[-0.0090659   0.03791172  0.         -0.21697396]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.19012707  0.02224145]]. Action = [[0.02613049 0.03096082 0.         0.5879245 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.18977137  0.02360599]]. Action = [[-0.01476602  0.00340156  0.         -0.5704616 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.18995596  0.02880214]]. Action = [[-0.00107854  0.09449591  0.         -0.4561476 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.1902412  0.0312402]]. Action = [[-0.00524098 -0.02065676  0.          0.30919325]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.18973295  0.03315691]]. Action = [[ 0.01350868  0.032055    0.         -0.7369262 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.18743625  0.03494898]]. Action = [[ 0.04207871 -0.00048739  0.          0.14995551]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.18645145  0.03450393]]. Action = [[-0.00247915 -0.02675078  0.          0.6412629 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.19042677  0.03611245]]. Action = [[-0.08443388  0.03761684  0.         -0.11281121]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.19489263  0.03496641]]. Action = [[-0.04466075 -0.05203743  0.          0.8930329 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.19544105  0.02982147]]. Action = [[ 0.01632638 -0.07778439  0.          0.52153754]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.19716232  0.0303301 ]]. Action = [[-0.04388319  0.0596923   0.          0.42462564]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.19431844  0.0274629 ]]. Action = [[ 0.09663927 -0.09261568  0.          0.5634353 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.18688785  0.02306291]]. Action = [[ 0.09475496 -0.02814259  0.         -0.922036  ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.18001701  0.0171225 ]]. Action = [[ 0.07387225 -0.08552455  0.         -0.751318  ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.18077524  0.01533063]]. Action = [[-0.08848612  0.04561187  0.         -0.8810508 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.17956929  0.01234419]]. Action = [[ 0.07890116 -0.06546838  0.          0.9060601 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.17880161  0.00578768]]. Action = [[-0.03558688 -0.06841254  0.          0.8867078 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of -1
Current timestep = 174. State = [[-0.17958021 -0.0014161 ]]. Action = [[-0.0114805  -0.06865193  0.         -0.8939892 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.18006447 -0.00065306]]. Action = [[-0.0156711   0.09458055  0.         -0.76128906]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.17875293  0.00024656]]. Action = [[ 0.02815831 -0.00767741  0.         -0.29083288]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.17646869 -0.00102096]]. Action = [[ 0.02531572 -0.00477659  0.          0.2930702 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.1797793  -0.00506079]]. Action = [[-0.0906339  -0.05986396  0.          0.36951733]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.18118295 -0.01010172]]. Action = [[ 0.01424567 -0.04410191  0.          0.9496367 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of -1
Current timestep = 180. State = [[-0.1788754  -0.00846615]]. Action = [[ 0.03736604  0.07770274  0.         -0.77246535]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.17401962 -0.00842795]]. Action = [[ 0.0781794  -0.03697915  0.         -0.7878289 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.17122146 -0.00676834]]. Action = [[0.01066469 0.05692358 0.         0.37204635]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.1740121  -0.00052483]]. Action = [[-0.06599773  0.09519283  0.          0.36983204]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.1802471   0.00131509]]. Action = [[-0.09017725 -0.02354301  0.          0.75713325]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.18754819 -0.00350898]]. Action = [[-0.09576537 -0.08904957  0.          0.2647097 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.18663172 -0.00743421]]. Action = [[ 0.08386014 -0.02850427  0.         -0.09298867]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.18794242 -0.0061618 ]]. Action = [[-0.0762053   0.04722733  0.          0.2244575 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.19045067 -0.00043534]]. Action = [[0.00165502 0.08322731 0.         0.91861165]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.1937034   0.00476956]]. Action = [[-0.0466859   0.04076419  0.          0.77472425]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.19941063  0.00215915]]. Action = [[-0.07050228 -0.09273569  0.          0.7211797 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.20023894  0.00364026]]. Action = [[ 0.04675255  0.07427389  0.         -0.6084478 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.19903646  0.00777403]]. Action = [[0.02296233 0.02406268 0.         0.753108  ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.19959702  0.01425695]]. Action = [[-0.00211661  0.09302262  0.          0.17220938]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.19841546  0.01579191]]. Action = [[ 0.04991884 -0.04953836  0.          0.8054248 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.19917552  0.01739602]]. Action = [[-0.02587344  0.03749979  0.         -0.6531192 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.19816938  0.01490825]]. Action = [[ 0.05219594 -0.08915915  0.          0.7906424 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.19740131  0.0115765 ]]. Action = [[-0.00484745 -0.0258228   0.          0.04271924]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.19720325  0.00811961]]. Action = [[ 0.00830702 -0.05214699  0.          0.44081342]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.20006351  0.00771791]]. Action = [[-0.06578998  0.03116841  0.         -0.799816  ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.19853419  0.00956094]]. Action = [[0.07135115 0.02640643 0.         0.8162676 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.19511235  0.01364087]]. Action = [[ 0.03230897  0.06563789  0.         -0.7510519 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.19051449  0.01403104]]. Action = [[ 0.07505435 -0.03626696  0.          0.5052595 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[-0.18474247  0.01652916]]. Action = [[ 0.07356697  0.06321751  0.         -0.31401736]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.18500933  0.01998693]]. Action = [[-0.05319858  0.02686074  0.          0.20053232]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.18657587  0.01980842]]. Action = [[-0.00973906 -0.0273699   0.          0.56704414]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.19083942  0.02165654]]. Action = [[-0.09071372  0.04744039  0.          0.38341665]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.19606261  0.02070055]]. Action = [[-0.06486535 -0.05135395  0.          0.45759654]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.20194861  0.02381688]]. Action = [[-0.08842828  0.08693994  0.         -0.3585354 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.20286438  0.02218351]]. Action = [[ 0.04267908 -0.09612959  0.          0.31692147]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.20080377  0.02048651]]. Action = [[ 0.01116921  0.01595954  0.         -0.9137666 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.19593804  0.01621613]]. Action = [[ 0.0843038  -0.09433252  0.         -0.41868418]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.19297834  0.0145658 ]]. Action = [[-3.1925738e-04  2.7530529e-02  0.0000000e+00  3.9022279e-01]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.1907818   0.01571263]]. Action = [[0.03574493 0.01649234 0.         0.5012671 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.19215424  0.01337506]]. Action = [[-0.05489489 -0.04972976  0.         -0.7147908 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.19029075  0.00979976]]. Action = [[ 0.06188057 -0.03100348  0.         -0.6835481 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
