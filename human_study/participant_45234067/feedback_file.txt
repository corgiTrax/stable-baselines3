Current timestep = 0. State = [[-0.2570971   0.00616745]]. Action = [[ 0.02367785 -0.03158766  0.04000842  0.6916356 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0427, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25646392  0.00544549]]. Action = [[0.07372472 0.08090978 0.06755843 0.56342375]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25508317  0.00577181]]. Action = [[ 0.09737056 -0.09297547 -0.07288133  0.7404984 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0304, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25273615  0.00483395]]. Action = [[-0.04975105 -0.08747726  0.00550655 -0.9429119 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25249648  0.00232326]]. Action = [[-0.03770284  0.00626429 -0.02686714  0.9408599 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0200, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25262204  0.00094095]]. Action = [[ 0.02640819 -0.01754098 -0.08055089 -0.7854126 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.2526544  -0.00051048]]. Action = [[-0.08958083 -0.00627812  0.08312202  0.78708565]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.2528922  -0.00215329]]. Action = [[-0.04437445 -0.0547477   0.03463908  0.8243971 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.25321668 -0.00461804]]. Action = [[-0.01612729  0.03258101  0.08612562 -0.36151993]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.2535606  -0.00519632]]. Action = [[-0.08462853  0.04811741  0.07206769 -0.9843385 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.2546493 -0.0045067]]. Action = [[ 0.03852255  0.01973508  0.02015036 -0.12030143]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.25486723 -0.00404889]]. Action = [[ 0.08500449 -0.09207466  0.08364386  0.7484665 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.2549487 -0.0054904]]. Action = [[-0.09679319  0.05887575 -0.00307036  0.82568073]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.2551898  -0.00538451]]. Action = [[-0.07299019  0.06942894  0.05571859 -0.41485465]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25691754 -0.00288916]]. Action = [[ 0.07163744  0.08837221 -0.05514275  0.8674588 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-2.5795043e-01  1.4777298e-04]]. Action = [[ 0.05356307 -0.09649627  0.07312597 -0.39420915]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25799656 -0.00047452]]. Action = [[-0.04067041 -0.08102162  0.07009435 -0.47129476]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.2580813  -0.00273314]]. Action = [[ 0.07057584 -0.09491205 -0.0095245   0.17949343]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.2580131  -0.00608311]]. Action = [[ 0.09144407  0.07856169  0.07934768 -0.7067544 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.2568056  -0.00615702]]. Action = [[ 0.07051394  0.01782222  0.09338949 -0.07380849]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25520456 -0.00612317]]. Action = [[ 0.00105598 -0.00628538  0.04828604 -0.11602521]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25349736 -0.00623267]]. Action = [[ 0.04091748 -0.06367087 -0.07070073 -0.17104924]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.2515652  -0.00726668]]. Action = [[ 0.02551991 -0.05725157  0.05539287  0.80088687]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.2493148  -0.00964781]]. Action = [[0.08808186 0.00759514 0.07234211 0.60505223]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.24611977 -0.01060326]]. Action = [[ 0.05888664  0.02203177  0.04533473 -0.9861329 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.24157086 -0.01095022]]. Action = [[-0.00356148 -0.07181868 -0.04951105 -0.1758526 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.2385536  -0.01290197]]. Action = [[-3.5299361e-04 -7.5963631e-02  7.3886737e-03 -3.8213658e-01]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.2365     -0.01608517]]. Action = [[-0.08043504 -0.09666999  0.03136428  0.7762716 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.2362661  -0.02116214]]. Action = [[ 0.01424038 -0.08351447  0.01489147  0.66087866]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.23645355 -0.0264669 ]]. Action = [[-0.01755282 -0.0593993   0.05281072 -0.48473275]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.23691334 -0.03165972]]. Action = [[ 0.04317678 -0.02766515 -0.07990559  0.3925029 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.23707858 -0.03527699]]. Action = [[-0.03814469 -0.0388381  -0.02422349  0.6580157 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.23736206 -0.03850479]]. Action = [[-0.09110995  0.08215547  0.00129988  0.7339889 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.2375922  -0.03917366]]. Action = [[-0.002722    0.00777378  0.04855634  0.64811444]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.23770612 -0.03911462]]. Action = [[ 0.09156785 -0.02538202 -0.08356691 -0.968944  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(7.3318e-05, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.2376199  -0.03965429]]. Action = [[-0.04807795 -0.08844988 -0.04154871 -0.17492414]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.23786347 -0.04220941]]. Action = [[ 0.05969601 -0.04148616 -0.01789509 -0.40556282]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.23785159 -0.04455583]]. Action = [[ 0.07794441 -0.00922301  0.04908098  0.4558655 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.23633716 -0.04640034]]. Action = [[ 0.08627719 -0.0751676  -0.01846547 -0.8676516 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.23344564 -0.04929401]]. Action = [[ 0.04375226 -0.03387758  0.06201272 -0.37882137]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.23007816 -0.05225851]]. Action = [[-0.05812951  0.07128956  0.04060597 -0.69645435]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.22949135 -0.05228373]]. Action = [[ 0.01951911  0.00640176 -0.00818164 -0.5608456 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.22916381 -0.05211667]]. Action = [[0.00244761 0.05337992 0.02713025 0.4866774 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.22901522 -0.05140255]]. Action = [[-0.04645127  0.0848342   0.03168903 -0.22632062]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 44. State = [[-0.22938585 -0.04927015]]. Action = [[ 0.03205181  0.08548155  0.05232372 -0.962509  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.22971585 -0.0463031 ]]. Action = [[-0.02279697 -0.02655182  0.0571861  -0.42442918]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.22983825 -0.04525226]]. Action = [[ 0.07296365  0.06990727 -0.09102231 -0.62211925]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.22948965 -0.04302116]]. Action = [[-0.08288828 -0.07488607  0.08381663  0.41453207]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 48. State = [[-0.22943608 -0.04303054]]. Action = [[-0.05133417 -0.00564808 -0.04848862 -0.9416436 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.22953549 -0.0429832 ]]. Action = [[-0.07791276 -0.06883679  0.02039119  0.71283233]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 50. State = [[-0.22987649 -0.0439623 ]]. Action = [[-0.04768042  0.04033352  0.098698   -0.7787949 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 51. State = [[-0.23084182 -0.0440919 ]]. Action = [[ 0.04593215 -0.08886702 -0.01608735  0.4135672 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.23111568 -0.04588692]]. Action = [[-0.06118914 -0.0892102  -0.01220691 -0.93526137]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.23179308 -0.04917265]]. Action = [[ 0.0950773  -0.03618217 -0.08461383  0.17357075]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.23203245 -0.05203508]]. Action = [[ 0.0840558  -0.07555554 -0.06222951 -0.62879527]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.23165119 -0.05584094]]. Action = [[-0.02402575  0.00500026 -0.04737977 -0.2696842 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 56. State = [[-0.231691   -0.05820265]]. Action = [[-0.04819114 -0.06384301  0.04772422  0.9370955 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.23195182 -0.06127175]]. Action = [[-0.02801906  0.06842827  0.02526153  0.1442498 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.23207274 -0.06161484]]. Action = [[ 0.01177342 -0.03962985 -0.03386016  0.31790364]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.23227686 -0.06246962]]. Action = [[-0.01330265  0.06794441 -0.09601583  0.20558822]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 60. State = [[-0.23218672 -0.06222628]]. Action = [[ 0.01310033  0.03822916 -0.08563871 -0.43727112]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.23228055 -0.06153093]]. Action = [[ 0.0693857  -0.08104581  0.05279721 -0.9078165 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.2321646  -0.06193906]]. Action = [[ 0.08203251  0.03101025 -0.03368092  0.79631186]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 63. State = [[-0.23159623 -0.06182912]]. Action = [[-0.00165399 -0.0011737   0.08986077 -0.4903708 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.23102751 -0.06165749]]. Action = [[-0.02368353  0.01095388  0.05959295 -0.8148019 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.23079368 -0.06157022]]. Action = [[-0.07893504  0.03355438 -0.05764794 -0.12190115]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.23087214 -0.06125432]]. Action = [[ 0.01423329 -0.01095057  0.04635832 -0.04927558]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.23097332 -0.06094608]]. Action = [[-0.00926475  0.06602349 -0.05566532 -0.4193946 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.23112194 -0.05978149]]. Action = [[-0.0873054  -0.09414583  0.05839861  0.06878352]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.23116793 -0.06032205]]. Action = [[ 0.05666689 -0.03104816  0.02942938 -0.736774  ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 70. State = [[-0.23112673 -0.06103714]]. Action = [[-0.04824917  0.06997667 -0.09243257  0.92926764]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.23122862 -0.06076192]]. Action = [[-0.07186352 -0.07143968 -0.03568109 -0.65767837]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.23273702 -0.06177857]]. Action = [[-0.09692886 -0.03474922  0.08139084  0.6460502 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 73. State = [[-0.23485106 -0.06321312]]. Action = [[-0.05389768  0.0796366   0.09050048 -0.82237905]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 74. State = [[-0.237343   -0.06236908]]. Action = [[-0.04287552 -0.02828016  0.06792933 -0.80456954]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.23879656 -0.06286239]]. Action = [[ 0.091448    0.00335634 -0.08497289 -0.41205335]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.2388126  -0.06306807]]. Action = [[ 0.04807795 -0.07157837  0.09552071  0.90454614]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.23893568 -0.06424198]]. Action = [[0.01658225 0.01696368 0.04660548 0.87564826]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.23898959 -0.06456025]]. Action = [[-0.01649016  0.09234887 -0.09111866  0.63987267]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.23915806 -0.06369201]]. Action = [[-0.01970205  0.0484959  -0.07692368 -0.32501668]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.23946981 -0.06190377]]. Action = [[ 0.06994472  0.08819658  0.01394232 -0.5836059 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.2398701  -0.05822085]]. Action = [[-0.03132728  0.09255403 -0.04130503  0.3054477 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.24056362 -0.05330661]]. Action = [[-0.0819432   0.06690431 -0.09902111 -0.502288  ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 83. State = [[-0.24151614 -0.04854627]]. Action = [[ 0.09903743  0.07248307  0.06906303 -0.2405566 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.2422114  -0.04356559]]. Action = [[ 0.07995904  0.04882502 -0.02278788 -0.816082  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(3.4170e-05, grad_fn=<MseLossBackward0>)
Current timestep = 85. State = [[-0.24139819 -0.03919545]]. Action = [[ 0.02994392  0.06072327  0.06976972 -0.7077368 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.24058498 -0.03523487]]. Action = [[-0.04926914 -0.03886992 -0.030007    0.5127852 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.2408432  -0.03402184]]. Action = [[ 0.0392539   0.06835728  0.01036648 -0.69284403]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.24075945 -0.03145588]]. Action = [[ 0.03366957 -0.04586207  0.09427045 -0.3014508 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.23995456 -0.03118958]]. Action = [[-0.02495421  0.01132455  0.09134784  0.8031515 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.23997149 -0.03067832]]. Action = [[-0.03211971 -0.05829129  0.0417868   0.08928394]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.23989277 -0.03073078]]. Action = [[-0.09787337  0.06094041  0.09196156  0.8464596 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 92. State = [[-0.2401391  -0.02967926]]. Action = [[-1.5481561e-04  5.9776373e-02 -3.7323490e-02 -2.2896391e-01]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 93. State = [[-0.24064685 -0.02760558]]. Action = [[-0.04869418 -0.09314715  0.07545724  0.10189211]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.24089484 -0.0278938 ]]. Action = [[-0.05530361  0.09323292  0.09752614 -0.94838023]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.24206287 -0.02593376]]. Action = [[ 0.05899177  0.07594436 -0.08943257  0.31423676]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.24283075 -0.02300796]]. Action = [[-0.02195805  0.07058125  0.04432247 -0.5793503 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 97. State = [[-0.24376446 -0.01940502]]. Action = [[ 0.05784705 -0.02840643  0.04293592  0.56344354]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 98. State = [[-0.24410252 -0.01788391]]. Action = [[ 0.01982294 -0.02505206 -0.02742671 -0.7213074 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.24401401 -0.01787711]]. Action = [[0.00590483 0.01841973 0.07998069 0.21631789]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.24400233 -0.01754161]]. Action = [[ 0.06451879 -0.07803619  0.08242843  0.63179827]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.24322426 -0.01786899]]. Action = [[0.06088024 0.07474849 0.00145844 0.98216736]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(4.9900e-05, grad_fn=<MseLossBackward0>)
Current timestep = 102. State = [[-0.24210136 -0.01747614]]. Action = [[ 0.06799688  0.07610431 -0.0609142  -0.27413774]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.24009232 -0.01569046]]. Action = [[ 0.02697051 -0.03417848 -0.01685192  0.32582164]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.23808102 -0.01546428]]. Action = [[-0.08976513 -0.07552458  0.01011944  0.676492  ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.23799579 -0.01582017]]. Action = [[ 0.05698862 -0.03461875  0.02740119 -0.22461188]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 106. State = [[-0.23725608 -0.01656855]]. Action = [[ 0.08399326 -0.07849486  0.09219391  0.9409244 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(1.0032e-05, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.23501727 -0.01849977]]. Action = [[-0.04146425  0.06003668  0.05453715 -0.86202455]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.23394717 -0.01817802]]. Action = [[ 0.06040017  0.0049561  -0.08112617 -0.24395609]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 109. State = [[-0.23169737 -0.01793349]]. Action = [[ 0.08932567 -0.04681374 -0.00400835 -0.511566  ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 110. State = [[-0.22786745 -0.01858539]]. Action = [[ 0.03743125 -0.00715223 -0.07245488 -0.42551506]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.22417855 -0.01906911]]. Action = [[-0.00577094 -0.00083705  0.08086497  0.08361161]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.22234373 -0.01940443]]. Action = [[ 0.02130677  0.08202875 -0.0397713   0.8724842 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(8.0162e-05, grad_fn=<MseLossBackward0>)
Current timestep = 113. State = [[-0.21932271 -0.01887964]]. Action = [[-0.00669567 -0.02579029 -0.09156437 -0.5402679 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.21823466 -0.01899867]]. Action = [[-0.08670288 -0.0226583  -0.03447585  0.2292527 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 115. State = [[-0.21835029 -0.01916123]]. Action = [[-0.07042862  0.06918205 -0.06768925 -0.67163295]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 116. State = [[-0.21868525 -0.01802277]]. Action = [[ 0.09252956 -0.08508389 -0.00889425  0.9306189 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.21861136 -0.01888931]]. Action = [[-0.07084306  0.04889257 -0.04917851  0.13889289]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.21866535 -0.01868781]]. Action = [[ 0.02517008 -0.00649761  0.03588646 -0.3983118 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.21871705 -0.0185993 ]]. Action = [[ 0.00811458  0.04796315 -0.02952557  0.5129585 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.21881722 -0.01813663]]. Action = [[0.01098479 0.01046698 0.09778    0.5904405 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(5.1819e-05, grad_fn=<MseLossBackward0>)
Current timestep = 121. State = [[-0.21887152 -0.01765209]]. Action = [[-0.08572584  0.03818298 -0.03577475 -0.21083337]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 122. State = [[-0.21925752 -0.01624635]]. Action = [[-0.0494476  -0.05012426  0.05550279 -0.30541956]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 123. State = [[-0.21935421 -0.01623402]]. Action = [[ 0.01755171 -0.05203275 -0.02364214  0.3115939 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 124. State = [[-0.21949217 -0.01694448]]. Action = [[-0.08455736  0.02960429 -0.07824697 -0.07188952]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.22046374 -0.01723335]]. Action = [[ 0.05526703 -0.02953536 -0.07883503  0.31770205]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.22052717 -0.0177544 ]]. Action = [[-0.07854488  0.04517724 -0.01542129  0.904264  ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(9.7533e-05, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.22174986 -0.0173822 ]]. Action = [[-0.07317285 -0.07472467  0.06823268 -0.425915  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 128. State = [[-0.2240392  -0.01891066]]. Action = [[-0.04349136  0.06429181 -0.0460869   0.8897457 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(5.4094e-05, grad_fn=<MseLossBackward0>)
Current timestep = 129. State = [[-0.22664061 -0.0180424 ]]. Action = [[0.0081173  0.01067649 0.02005982 0.30043674]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.22834045 -0.0170976 ]]. Action = [[-0.00133702  0.08278526 -0.06498422  0.8996954 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(5.5110e-05, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.22983515 -0.01416638]]. Action = [[ 0.07886861  0.05846443 -0.05309289  0.6871884 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.23074529 -0.01112123]]. Action = [[-0.09883377  0.09702838 -0.04788541  0.7970526 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 133. State = [[-0.23232909 -0.00613041]]. Action = [[ 0.0919771  -0.03432404  0.07638989 -0.18916476]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.23287472 -0.00450685]]. Action = [[ 0.02636117 -0.07993081 -0.06719573 -0.05600983]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.23279266 -0.00483363]]. Action = [[ 0.03135278 -0.01513396  0.08165254  0.10094273]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.23272234 -0.00530553]]. Action = [[-0.08423333  0.03832375 -0.02780666 -0.92233527]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.23277223 -0.00512194]]. Action = [[-0.07736878  0.05219104  0.04718197 -0.6170986 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.2335675  -0.00360605]]. Action = [[ 0.03905234  0.0558655   0.03243599 -0.29633367]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.2345018  -0.00104205]]. Action = [[-0.06722689  0.05206623  0.03984568  0.46359158]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.23549007  0.00193523]]. Action = [[ 0.04696352 -0.00885226 -0.04970837 -0.8873377 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.23589776  0.00319796]]. Action = [[-0.03839827 -0.01847709  0.07106408 -0.14509523]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 142. State = [[-0.23604903  0.00366934]]. Action = [[ 0.0721186   0.05086531 -0.09442663 -0.56203055]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 143. State = [[-0.2363883   0.00470212]]. Action = [[-0.05062179 -0.04401491 -0.09312435  0.952127  ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.23641616  0.00457399]]. Action = [[ 0.02047398 -0.08790411 -0.00768672  0.75087094]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 145. State = [[-0.23651257  0.00258849]]. Action = [[-0.06558771 -0.09368966 -0.06618702  0.00103247]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.23727416 -0.00162311]]. Action = [[-0.0475879  -0.09302261 -0.02553549 -0.18023711]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.23890498 -0.00617627]]. Action = [[ 0.01881647  0.0801738   0.03465327 -0.28219926]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 148. State = [[-0.23973803 -0.00710671]]. Action = [[ 0.04703449  0.09273789 -0.00787349 -0.00228369]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.24012536 -0.00540341]]. Action = [[-0.04774372  0.0726575   0.02948674 -0.53462744]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.2410209  -0.00237189]]. Action = [[-0.08360679  0.00495021 -0.02979042  0.7198602 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-2.4253651e-01 -1.8676637e-04]]. Action = [[-0.09178414 -0.0853344   0.09153611 -0.10862708]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 152. State = [[-0.2453728  -0.00082881]]. Action = [[ 0.06074388 -0.00198159 -0.04948195 -0.96671   ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.24691129 -0.0015174 ]]. Action = [[-0.01558344 -0.05858942 -0.01683772 -0.01105016]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.24812575 -0.00394435]]. Action = [[ 0.08904818 -0.08302847  0.04439042  0.49449503]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 155. State = [[-0.24825032 -0.00710847]]. Action = [[0.09034541 0.08738964 0.07672048 0.37756515]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.24815187 -0.00705244]]. Action = [[ 0.0907458   0.03184941  0.09529056 -0.7270008 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.24694684 -0.0068186 ]]. Action = [[-0.06703229 -0.00233629  0.01423129  0.9060652 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.24699762 -0.00673481]]. Action = [[-0.08760699  0.09098441 -0.0174536  -0.353153  ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.24783067 -0.00425537]]. Action = [[ 0.00534979 -0.09587547 -0.08572068 -0.1497494 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.24793088 -0.00466048]]. Action = [[-0.07220718 -0.01503354  0.01080173  0.6131246 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.24834839 -0.00503188]]. Action = [[-0.00348866  0.08199816 -0.04125038  0.47478497]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 162. State = [[-0.24876623 -0.00360267]]. Action = [[ 0.08718169  0.06540909 -0.04801755  0.25345075]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.24925382 -0.00159666]]. Action = [[-0.00348478 -0.03358699 -0.09795384  0.65128136]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 164. State = [[-0.24937238 -0.00121785]]. Action = [[-0.0516509  -0.09372105 -0.03116639  0.719903  ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 165. State = [[-0.24929601 -0.00211424]]. Action = [[-0.04132407 -0.01716323 -0.03349762 -0.63063526]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.24952911 -0.00367126]]. Action = [[ 0.05190653 -0.08544724  0.08505524 -0.0442366 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.24949853 -0.00612355]]. Action = [[-0.07149278  0.08393457 -0.09332401 -0.62799203]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.250145   -0.00616757]]. Action = [[-0.07703003 -0.08200619 -0.08847903  0.3033594 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.25173748 -0.00793375]]. Action = [[-0.0025327   0.03306437 -0.01145263 -0.91091305]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.2526484  -0.00858525]]. Action = [[-0.01371274  0.00113616  0.06722798 -0.87266374]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.2531651  -0.00894057]]. Action = [[-0.06227791 -0.09618795  0.02047601  0.8094189 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.25498256 -0.01155685]]. Action = [[-0.05754808  0.07577542 -0.07480947  0.46191525]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 173. State = [[-0.25701195 -0.01149101]]. Action = [[ 0.06869317 -0.03328308 -0.07202466  0.07217538]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.2573579  -0.01226766]]. Action = [[0.05170607 0.02771998 0.00306149 0.91096795]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(8.5520e-05, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.25721246 -0.01229717]]. Action = [[ 0.05659426  0.02648029 -0.08947682  0.8351685 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.25728098 -0.01189834]]. Action = [[ 0.07943306  0.07816882  0.02356438 -0.768154  ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 177. State = [[-0.25708342 -0.01014081]]. Action = [[-0.09576836  0.02161123  0.01686444  0.16394413]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.25777963 -0.0079744 ]]. Action = [[-0.03034178  0.04664128  0.03663728 -0.71395636]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.25867897 -0.00499916]]. Action = [[-0.04767286  0.09503949 -0.02762133  0.2021339 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.26010814 -0.00072697]]. Action = [[-0.03760971 -0.09523782  0.01341884  0.524719  ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 181. State = [[-2.6051673e-01 -2.4986791e-04]]. Action = [[-0.03888329 -0.02531485 -0.09196075  0.20649564]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.2613442 -0.0004128]]. Action = [[-0.01402779  0.03898602  0.05980716 -0.6717535 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-2.6201978e-01  1.7600095e-04]]. Action = [[-0.00385576  0.00240934  0.04029603  0.13187063]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.26256704  0.00076828]]. Action = [[ 0.02087537  0.06318329 -0.00582734 -0.32411653]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.26327506  0.00287485]]. Action = [[ 0.03399611  0.02120831 -0.07388993  0.28299713]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.26367477  0.00458218]]. Action = [[-0.0325477   0.06929124  0.01756854 -0.01116037]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 187. State = [[-0.2647562  0.0078011]]. Action = [[0.09695417 0.08739965 0.0434707  0.23913145]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.26550797  0.01133975]]. Action = [[-0.09736314 -0.03999465 -0.01809912  0.44806695]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.26622963  0.01323661]]. Action = [[ 0.01333376  0.07277936  0.02190451 -0.5513815 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.26725397  0.01596983]]. Action = [[ 0.06451879  0.01165392  0.08156735 -0.75675094]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 191. State = [[-0.26752517  0.01767697]]. Action = [[-0.01833234  0.09271032  0.05419569  0.41710806]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.2685445   0.02092272]]. Action = [[-0.04136893 -0.01713333  0.02364655 -0.6971282 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.26926672  0.02299654]]. Action = [[ 0.08540557 -0.06684394  0.03094152  0.32454848]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 194. State = [[-0.26824328  0.02326252]]. Action = [[-0.06523199  0.03827418  0.01795677 -0.59063834]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 195. State = [[-0.2682019   0.02339586]]. Action = [[ 0.06536142 -0.08157124  0.08921177  0.846848  ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 196. State = [[-0.2672792   0.02307886]]. Action = [[0.02727974 0.08776499 0.09193175 0.70997226]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.2666903   0.02356907]]. Action = [[ 0.04242379 -0.01299106  0.02020273 -0.73266   ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.26543593  0.02398279]]. Action = [[ 0.07263654 -0.01018794  0.05919959 -0.86705154]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.263092    0.02463434]]. Action = [[ 0.05255444 -0.00468366  0.07447634  0.12164176]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.2607346  0.0249826]]. Action = [[-0.06358394  0.06507874 -0.05120034  0.46047318]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.2607874   0.02548518]]. Action = [[-0.08216269 -0.07887877 -0.04742641  0.20902228]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.26095712  0.02516817]]. Action = [[-0.0571892  -0.08173528  0.03516536 -0.2013349 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.26094228  0.02201657]]. Action = [[-0.03365614 -0.02848306 -0.02685367 -0.6263448 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.26174718  0.020947  ]]. Action = [[-0.0007935   0.09415121  0.02354205  0.7861116 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 205. State = [[-0.26206633  0.02152599]]. Action = [[-0.01809178  0.06347468 -0.05515442 -0.9631322 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.2628288   0.02340763]]. Action = [[0.0468943  0.04534084 0.0145869  0.9821161 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 207. State = [[-0.26341483  0.02514468]]. Action = [[ 0.08215108 -0.09711372  0.09010325  0.72032714]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.2632147   0.02486376]]. Action = [[ 0.01033251 -0.03632665  0.0465324   0.4155922 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.26272726  0.02398715]]. Action = [[-6.6457421e-02 -5.7162344e-04  2.5177546e-02  8.2897556e-01]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.26269704  0.02377751]]. Action = [[-0.03869062  0.04779399 -0.01253279 -0.42163163]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.2627398   0.02381053]]. Action = [[ 0.06759434 -0.07395747 -0.07055056 -0.43742937]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 212. State = [[-0.26267004  0.0234348 ]]. Action = [[-0.02645791  0.0866872  -0.01506193 -0.80120414]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 213. State = [[-0.262766    0.02367919]]. Action = [[ 0.09699888 -0.02300012 -0.02825101  0.73191404]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.26267236  0.02375554]]. Action = [[ 0.02077664  0.00489865  0.01434223 -0.9330215 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.26255965  0.02376175]]. Action = [[ 0.01720166 -0.05712626  0.06257608 -0.8361679 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 216. State = [[-0.26193595  0.02303244]]. Action = [[-0.00951583  0.02305081 -0.05971766 -0.34334987]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.26168644  0.02290683]]. Action = [[-0.05385634 -0.09356766  0.05471259  0.3492918 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.26111066  0.02077342]]. Action = [[-0.07637037 -0.07817254 -0.01408751  0.09818625]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.2610422   0.01764744]]. Action = [[ 0.09003254 -0.07107081 -0.00702571 -0.88699657]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 220. State = [[-0.26039463  0.01310382]]. Action = [[-0.06037942 -0.05194098  0.0764206  -0.66276467]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 221. State = [[-0.26039732  0.00910801]]. Action = [[ 9.4651245e-02  3.0674040e-04 -1.8078551e-02 -3.3353066e-01]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.26017004  0.00663084]]. Action = [[ 0.05627459 -0.09398205  0.09858029 -0.82888544]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.25871277  0.00255697]]. Action = [[-0.08570459 -0.01626682  0.01353816 -0.8242963 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-2.5841948e-01  1.6207081e-04]]. Action = [[ 0.02421993  0.0671972  -0.07455115  0.33843613]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-2.5805405e-01  8.8334309e-05]]. Action = [[-0.05828898 -0.08532074 -0.0088941   0.3754195 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 226. State = [[-0.25810763 -0.00202511]]. Action = [[0.01398204 0.09356076 0.01015872 0.10830081]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.25811166 -0.0017444 ]]. Action = [[ 0.0926952   0.05694542 -0.09375454  0.05786121]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.258027   -0.00113057]]. Action = [[ 0.09678239  0.04382331 -0.02841956  0.15722167]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.25573996  0.00039765]]. Action = [[ 0.09077375  0.040227   -0.08316582 -0.97496456]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.25218824  0.00225578]]. Action = [[-0.08399291 -0.09329048  0.0628546   0.27892828]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.2511566   0.00230759]]. Action = [[-0.04475377 -0.00109182  0.00781103  0.1416241 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.25123346  0.00218651]]. Action = [[0.00655737 0.01740922 0.0775924  0.75158894]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(2.3576e-05, grad_fn=<MseLossBackward0>)
Current timestep = 233. State = [[-0.25129303  0.00213598]]. Action = [[-0.07518665 -0.08909848  0.03934229  0.6389104 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(2.0615e-05, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.25119066  0.00084656]]. Action = [[-0.04737077  0.09887419  0.06180427  0.1571852 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.2514433   0.00148736]]. Action = [[-0.08022614 -0.08068706 -0.02453351  0.7244842 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(2.3637e-05, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.25284544  0.00059435]]. Action = [[-0.07282676  0.01434996  0.08820131 -0.9184264 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(6.9607e-05, grad_fn=<MseLossBackward0>)
Current timestep = 237. State = [[-0.25525105  0.00054431]]. Action = [[-0.08637468  0.09115148 -0.04915508  0.79838955]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(4.3208e-05, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.2588456   0.00239448]]. Action = [[ 0.08966423 -0.04689381  0.02334474 -0.00982362]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 239. State = [[-0.25895286  0.00242711]]. Action = [[ 3.4653135e-02 -1.0532141e-04  1.6733363e-02 -1.0930854e-01]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 240. State = [[-0.25899494  0.00229006]]. Action = [[ 0.06103978 -0.07611031  0.0541276   0.8957341 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(2.8697e-05, grad_fn=<MseLossBackward0>)
Current timestep = 241. State = [[-0.25873232  0.00089152]]. Action = [[ 0.06905245 -0.00770634 -0.01265518  0.98388946]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(3.8117e-05, grad_fn=<MseLossBackward0>)
Current timestep = 242. State = [[-2.5815195e-01 -1.3323779e-04]]. Action = [[ 0.00097774 -0.03292802  0.01643987  0.25496304]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 243. State = [[-0.25794673 -0.00149017]]. Action = [[ 0.0301415   0.0858741  -0.04113464  0.40140402]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 244. State = [[-0.257363   -0.00142826]]. Action = [[ 0.05067796 -0.07879575  0.03010536 -0.84552866]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(5.7031e-05, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.25620246 -0.00214816]]. Action = [[ 0.07157198 -0.0202907   0.02572597  0.8343735 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(4.4056e-06, grad_fn=<MseLossBackward0>)
Current timestep = 246. State = [[-0.2540568  -0.00331264]]. Action = [[-0.03641643 -0.02857057 -0.06337527 -0.92476064]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(9.1312e-05, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.25364944 -0.00493139]]. Action = [[-0.08484554 -0.07615616  0.0127906   0.13489795]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(9.0626e-05, grad_fn=<MseLossBackward0>)
Current timestep = 248. State = [[-0.25370574 -0.00790313]]. Action = [[ 0.0904346   0.03585257 -0.04954031  0.59022427]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(1.0625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.2532424  -0.00877971]]. Action = [[ 0.01903943 -0.08270191 -0.0640286  -0.18113631]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(6.1901e-05, grad_fn=<MseLossBackward0>)
Current timestep = 250. State = [[-0.25211388 -0.01163772]]. Action = [[ 0.03667421 -0.02469651  0.00352577  0.17547762]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(9.7561e-05, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.2509735  -0.01446487]]. Action = [[-0.0721882   0.09017584 -0.07903418  0.9005928 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(1.4076e-05, grad_fn=<MseLossBackward0>)
Current timestep = 252. State = [[-0.25088814 -0.01408059]]. Action = [[ 0.05589456  0.01790549 -0.05732128 -0.06640506]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(4.5071e-05, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.25054947 -0.0141081 ]]. Action = [[-0.08412458 -0.09103472  0.02873874 -0.15069103]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(1.0091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 254. State = [[-0.250504   -0.01495435]]. Action = [[-0.06362368 -0.02257556 -0.02718411  0.10286987]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(5.2877e-05, grad_fn=<MseLossBackward0>)
Current timestep = 255. State = [[-0.25064352 -0.01608518]]. Action = [[-0.03374741 -0.0012822  -0.06135741  0.2358836 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(7.3630e-05, grad_fn=<MseLossBackward0>)
Current timestep = 256. State = [[-0.25095052 -0.01668195]]. Action = [[-0.0809031   0.02686719  0.06557236  0.79911065]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(5.1530e-05, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.25276116 -0.01669463]]. Action = [[-0.0243193   0.09427888 -0.08429591  0.6850457 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(2.1369e-05, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.2542677  -0.01528465]]. Action = [[ 0.08174212 -0.09308054  0.01095526 -0.65348333]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(1.2901e-05, grad_fn=<MseLossBackward0>)
Current timestep = 259. State = [[-0.25415102 -0.01607091]]. Action = [[ 0.0133491  -0.05953004  0.09664214 -0.81867784]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(2.0415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 260. State = [[-0.253892   -0.01755586]]. Action = [[ 0.07689049  0.09711172 -0.08485198 -0.6406717 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(5.2273e-06, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.2538976  -0.01750325]]. Action = [[ 0.02598897 -0.06741316  0.06289337  0.08465397]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(7.9376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 262. State = [[-0.2534368  -0.01789543]]. Action = [[ 0.08510528 -0.01192905  0.05676074 -0.58091396]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(2.5395e-05, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.2517046  -0.01876795]]. Action = [[ 0.08245269 -0.07697041 -0.04525845 -0.89021146]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(1.2037e-05, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.249245   -0.02144369]]. Action = [[ 0.06837312  0.03911839  0.02465707 -0.80664146]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(2.9121e-05, grad_fn=<MseLossBackward0>)
Current timestep = 265. State = [[-0.24617076 -0.02228511]]. Action = [[-0.07035241 -0.01272553  0.09452281 -0.33093208]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(4.0486e-05, grad_fn=<MseLossBackward0>)
Current timestep = 266. State = [[-0.24493492 -0.02241253]]. Action = [[0.05935634 0.0128078  0.04877374 0.33579922]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(7.8875e-05, grad_fn=<MseLossBackward0>)
Current timestep = 267. State = [[-0.2432616  -0.02264504]]. Action = [[ 0.08700978 -0.07107837  0.07527501  0.8729274 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.23964542 -0.02442463]]. Action = [[ 0.04413403 -0.02939391 -0.05708054  0.58477426]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 269. State = [[-0.23574159 -0.02663516]]. Action = [[ 0.03364115 -0.03175026 -0.00088332 -0.44285834]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.2324905  -0.02915033]]. Action = [[ 2.4355948e-05  1.2046300e-02 -9.2152067e-02 -3.6288393e-01]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 271. State = [[-0.23108864 -0.03027549]]. Action = [[ 0.01842991 -0.03769309 -0.02155752 -0.21040738]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 272. State = [[-0.22983286 -0.03205469]]. Action = [[ 0.07007734 -0.0258766   0.06171928 -0.26113153]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 273. State = [[-0.22759315 -0.03387962]]. Action = [[ 0.01868179  0.07712395 -0.02996924 -0.7406948 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 274. State = [[-0.22510554 -0.03362402]]. Action = [[ 0.03738592  0.06809705  0.0923615  -0.8931732 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.22233427 -0.03239753]]. Action = [[ 0.06269652  0.03787685 -0.07284848  0.8631216 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.21882933 -0.03069068]]. Action = [[ 0.05202699  0.04769016 -0.0342416   0.49649704]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.21591729 -0.02884091]]. Action = [[-0.09474404 -0.05999948 -0.0085931   0.4350648 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 278. State = [[-0.2154686  -0.02859508]]. Action = [[ 0.0584671   0.07307921 -0.03395113 -0.72158414]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.21393347 -0.02680574]]. Action = [[ 0.05932758  0.07978345  0.03783072 -0.33841014]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 280. State = [[-0.21111238 -0.02362789]]. Action = [[ 0.07930601 -0.00933361 -0.09129817 -0.91296405]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 281. State = [[-0.20753683 -0.02209821]]. Action = [[ 0.05058923 -0.00911029 -0.01620691 -0.5745208 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.20367935 -0.02153946]]. Action = [[-0.02529436  0.00719633 -0.07439892 -0.49440634]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.2015238  -0.02087292]]. Action = [[-0.04887224  0.01317187  0.01655463  0.7365376 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 284. State = [[-0.20163603 -0.02023964]]. Action = [[-0.0755269   0.02096055  0.00447796 -0.49203146]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 285. State = [[-0.20219134 -0.01874985]]. Action = [[-0.05962471  0.05705383 -0.08852442 -0.90840477]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 286. State = [[-0.20326379 -0.01577773]]. Action = [[ 0.04696887  0.03652029 -0.0522181   0.03296161]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 287. State = [[-0.20396788 -0.01324464]]. Action = [[-0.05273231  0.04862633  0.07135115  0.1704253 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.20482826 -0.00987667]]. Action = [[ 0.06233714  0.0780609   0.05185796 -0.49638897]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 289. State = [[-0.20597775 -0.00603521]]. Action = [[-0.02355876 -0.01528905 -0.02430712  0.17618656]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.20647809 -0.00421718]]. Action = [[-0.00411946 -0.03258994 -0.06499231 -0.284612  ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.20652303 -0.00406255]]. Action = [[-0.07388306 -0.04867734 -0.04330553 -0.20681989]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 292. State = [[-0.20650616 -0.00426488]]. Action = [[-0.03891971  0.06723649 -0.06896771 -0.6916065 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 293. State = [[-0.20711005 -0.00305328]]. Action = [[ 0.0242121  -0.02488326 -0.03815733 -0.8135485 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.20722288 -0.00278674]]. Action = [[ 0.04287288  0.09817145 -0.06399304 -0.4794653 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 295. State = [[-0.20790021 -0.00050385]]. Action = [[-0.00911947 -0.00395137 -0.05191307  0.36368036]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.20828636  0.00095811]]. Action = [[ 0.04677404  0.01955892  0.08854068 -0.362422  ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.2085607   0.00176951]]. Action = [[0.05437625 0.01364068 0.07959407 0.84209657]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.20856135  0.0022374 ]]. Action = [[ 0.0984518  -0.03114878  0.03222961 -0.7225458 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.20498677  0.00245815]]. Action = [[-0.0371068   0.02153863 -0.07941345  0.06594706]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 300. State = [[-0.2047173   0.00273941]]. Action = [[ 0.05087567  0.07011845  0.07213741 -0.2642802 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 301. State = [[-0.2038925   0.00463749]]. Action = [[-0.03330901  0.08770265 -0.07715374  0.8945694 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 302. State = [[-0.204219    0.00838481]]. Action = [[ 0.05657995 -0.02727774  0.00415881  0.13366556]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.20336248  0.00995574]]. Action = [[ 0.03956511  0.07211179  0.00969426 -0.93015677]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.20179872  0.01230273]]. Action = [[-0.04643928 -0.08544613  0.03889035  0.72436   ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.20109166  0.01216678]]. Action = [[ 0.01900955 -0.02440231  0.05799844  0.98870015]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.20083186  0.01223541]]. Action = [[ 0.0319112   0.07125611 -0.097456   -0.75372815]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 307. State = [[-0.19956101  0.0130719 ]]. Action = [[-0.02026219 -0.04481113  0.07122896 -0.2417599 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 308. State = [[-0.19926214  0.01288707]]. Action = [[-0.05861095 -0.08146467 -0.02028595  0.6102135 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 309. State = [[-0.19919583  0.0118282 ]]. Action = [[-0.0564063  -0.04371747 -0.05148932 -0.36819673]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 310. State = [[-0.19950251  0.01016381]]. Action = [[ 0.03382763  0.06803172 -0.08872142 -0.6679262 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 311. State = [[-0.19951504  0.01028061]]. Action = [[ 0.09676672  0.01086782 -0.05303333  0.5645504 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.1990122   0.01052801]]. Action = [[-0.06730044  0.05624268  0.03289784  0.6676457 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.19938082  0.01156313]]. Action = [[ 0.05634872  0.09018487 -0.09695851  0.240592  ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.19897124  0.01419325]]. Action = [[0.09915655 0.07619155 0.04580405 0.960822  ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.19669656  0.01749198]]. Action = [[-0.08005939 -0.02505912  0.00965363 -0.5703985 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.19519848  0.02002803]]. Action = [[ 0.07891989 -0.02176531  0.03207774  0.49990988]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 317. State = [[-0.19234231  0.0207689 ]]. Action = [[-0.02902643 -0.00641069  0.07627781 -0.5312221 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 318. State = [[-0.19068284  0.02112869]]. Action = [[-0.02811939 -0.05937873  0.02440584  0.91694355]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.19065496  0.0203906 ]]. Action = [[ 0.02687115 -0.08276464 -0.03154047  0.80610824]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 320. State = [[-0.18990521  0.0181982 ]]. Action = [[ 0.07894535 -0.04643493  0.09665617 -0.61123234]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 321. State = [[-0.18822934  0.01577872]]. Action = [[ 0.03883702 -0.06150622 -0.02504233  0.6908196 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 322. State = [[-0.18600142  0.01326022]]. Action = [[ 0.05275538  0.09706403 -0.09574125  0.67091084]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.18429145  0.01391921]]. Action = [[-0.00631401  0.00755979 -0.08503719 -0.17598057]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 324. State = [[-0.18346448  0.01413269]]. Action = [[-0.0364505   0.03372162 -0.05554608 -0.5764575 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.18312147  0.01460109]]. Action = [[-0.00614931 -0.04868149 -0.00699002 -0.3004707 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 326. State = [[-0.18317659  0.01432982]]. Action = [[ 0.0736594  -0.02058572 -0.00654581 -0.421952  ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 327. State = [[-0.18203646  0.01405846]]. Action = [[-0.05571422  0.08134291  0.06557391 -0.10514361]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 328. State = [[-0.18192421  0.01489581]]. Action = [[ 0.08640983 -0.02905733 -0.00855982 -0.04418856]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.18065575  0.01497927]]. Action = [[-0.09402576  0.04296074 -0.01574479  0.01045907]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.18106222  0.01560102]]. Action = [[-0.02143412  0.01838769 -0.06476942  0.8217194 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 331. State = [[-0.18143728  0.01626927]]. Action = [[-0.07544179 -0.07388145  0.04250152 -0.84457743]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.18149093  0.01613577]]. Action = [[-0.02929804  0.03146558  0.0305388  -0.41145754]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 333. State = [[-0.1816925   0.01627819]]. Action = [[-0.04830156  0.09255863 -0.05714792  0.7050886 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.18270813  0.01886152]]. Action = [[-0.07643659 -0.08689196 -0.03285215  0.7345693 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.1837765  0.0182429]]. Action = [[-0.05964224 -0.01266173  0.08329608 -0.8468122 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 336. State = [[-0.18541002  0.01752323]]. Action = [[ 0.01902219 -0.06687716 -0.07277013 -0.8076032 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.18628693  0.01585962]]. Action = [[-0.07730292  0.05219579 -0.07404465  0.9263729 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.18790434  0.01602604]]. Action = [[-0.01090427 -0.03786777 -0.01034237 -0.22218549]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.18891384  0.01501946]]. Action = [[ 0.09791542 -0.02299061  0.082946   -0.73430634]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 340. State = [[-0.18889998  0.01416311]]. Action = [[-0.08044125  0.03765474  0.03489614 -0.19323236]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 341. State = [[-0.1893      0.01421167]]. Action = [[ 0.01943183  0.0649618  -0.04738959 -0.4766376 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.18986985  0.01592354]]. Action = [[ 4.2300381e-02  4.1139789e-02 -7.8482926e-04  8.6225224e-01]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.19043311  0.01774667]]. Action = [[-0.05758795 -0.01878762  0.08891628 -0.5239773 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.19077908  0.01864173]]. Action = [[-0.08547417 -0.05572268  0.09371234  0.5365921 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 345. State = [[-0.19211935  0.01781688]]. Action = [[ 0.04305857 -0.0550673   0.08062317  0.47266567]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 346. State = [[-0.19231692  0.01583719]]. Action = [[ 0.06549441 -0.01373094 -0.00939441 -0.95615035]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.19240658  0.01405751]]. Action = [[-0.01859935  0.01317636  0.04745553  0.69657755]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 348. State = [[-0.19248569  0.01322736]]. Action = [[ 0.08717711  0.08867862 -0.08544468 -0.60309345]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 349. State = [[-0.19278558  0.01424677]]. Action = [[ 0.014966    0.04774516 -0.04925821 -0.01650125]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 350. State = [[-0.19329245  0.01630336]]. Action = [[0.08230216 0.08200433 0.04231613 0.02822793]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.1926882   0.01928612]]. Action = [[-0.05286058 -0.09781225 -0.09228572 -0.85218245]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 352. State = [[-0.19267263  0.01936836]]. Action = [[-0.09204401  0.08049244  0.00746529  0.8509407 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 353. State = [[-0.19349454  0.02163926]]. Action = [[ 0.00032756  0.0270602  -0.02817903 -0.10037935]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.19423096  0.02357465]]. Action = [[ 0.0577868  -0.09086605  0.0786267   0.59308815]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.19411725  0.02292655]]. Action = [[ 0.02947288 -0.06823848 -0.0565544   0.986948  ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.19361535  0.02117996]]. Action = [[ 0.05897357  0.06457645 -0.02853788 -0.4742297 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 357. State = [[-0.19339035  0.02102771]]. Action = [[-0.07830611 -0.08363396  0.0563465   0.22258174]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 358. State = [[-0.19327173  0.0196131 ]]. Action = [[-7.8596026e-02 -9.3004137e-02 -4.0404499e-05  7.4797487e-01]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.19342506  0.01626145]]. Action = [[-0.00616881  0.00845277 -0.03124797 -0.4510783 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 360. State = [[-0.19341911  0.01472707]]. Action = [[-0.05930423 -0.00302775 -0.08090286 -0.836255  ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 361. State = [[-0.19400737  0.01310138]]. Action = [[ 0.00317711 -0.06936351  0.06522238  0.7804289 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.1944158   0.01054167]]. Action = [[-0.08874106 -0.02797019  0.03639138  0.2943933 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 363. State = [[-0.19616403  0.0073467 ]]. Action = [[-0.04700425 -0.01172073  0.08963058  0.27632582]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 364. State = [[-0.19781671  0.0051983 ]]. Action = [[ 0.04256433 -0.02896686  0.08760283  0.1395204 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 365. State = [[-0.19806118  0.00359554]]. Action = [[-0.07242124  0.08327568 -0.00893763  0.7618909 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 366. State = [[-0.19923565  0.00434844]]. Action = [[ 0.08717182 -0.05256316 -0.0384611   0.8525094 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 367. State = [[-0.1990986   0.00408147]]. Action = [[-0.04882608 -0.0736269   0.06054614  0.00310802]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 368. State = [[-0.19918247  0.00195075]]. Action = [[-0.07718174 -0.07595409 -0.03581953 -0.9172626 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 369. State = [[-0.20049684 -0.00166291]]. Action = [[-0.03319396 -0.08432576  0.08399751  0.64809775]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.20226496 -0.00664865]]. Action = [[0.05171365 0.05959513 0.06696881 0.6639519 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 371. State = [[-0.20302399 -0.00853138]]. Action = [[ 0.04342432 -0.07976319 -0.05926359  0.9215833 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 372. State = [[-0.20307107 -0.01070009]]. Action = [[-0.03693937  0.07217481 -0.0088024   0.8788419 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 373. State = [[-0.20337059 -0.01066377]]. Action = [[-0.04067854 -0.08333315  0.05941191  0.5033376 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.20480947 -0.01232692]]. Action = [[ 0.03034727 -0.08682296 -0.06969225 -0.26821017]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.20575947 -0.01588985]]. Action = [[-0.04865517  0.03488661 -0.09378614 -0.7548392 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.20725667 -0.01767846]]. Action = [[-0.01315685 -0.0840684   0.05158073 -0.50329304]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 377. State = [[-0.20891261 -0.02126008]]. Action = [[0.03243535 0.0194605  0.076571   0.5909215 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 378. State = [[-0.20940958 -0.02296089]]. Action = [[-0.03678662  0.0039739  -0.0195518   0.83228517]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 379. State = [[-0.21034889 -0.02360308]]. Action = [[-0.03807158  0.04476429  0.0514761   0.67216444]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 380. State = [[-0.21144119 -0.02339856]]. Action = [[ 0.00693661 -0.07552956 -0.05432679 -0.8239251 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 381. State = [[-0.21225119 -0.02474451]]. Action = [[-0.06099922 -0.03260848 -0.06711809 -0.07699156]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 382. State = [[-0.21385604 -0.0262007 ]]. Action = [[-0.0559802   0.02035812  0.08370297 -0.10240149]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 383. State = [[-0.21590783 -0.02674278]]. Action = [[-0.03266643 -0.02677884  0.05894706 -0.02005172]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 384. State = [[-0.21872775 -0.02769071]]. Action = [[ 0.07556567 -0.05819365  0.0139666  -0.6586112 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 385. State = [[-0.21960773 -0.02916728]]. Action = [[-0.06499246  0.09356716 -0.08389935  0.9016502 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 386. State = [[-0.22090921 -0.0288759 ]]. Action = [[ 0.05894703 -0.07325383 -0.08960732  0.36540878]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.22062768 -0.02970506]]. Action = [[-0.03595805 -0.06559286 -0.03067861 -0.72219574]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.22120228 -0.03252614]]. Action = [[ 0.04520945 -0.09302548 -0.02036729 -0.19649762]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.22107747 -0.03730277]]. Action = [[-0.07979327 -0.05867026 -0.03822774  0.47353673]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.22237787 -0.04352596]]. Action = [[ 0.02152348  0.0504358  -0.09316213  0.8319571 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 391. State = [[-0.22255287 -0.04406339]]. Action = [[ 0.0136575  -0.00237646  0.07829008 -0.8318389 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 392. State = [[-0.22266716 -0.04420281]]. Action = [[ 0.03667185  0.04950965  0.04839087 -0.6312342 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.22269976 -0.04405317]]. Action = [[ 0.05741776  0.02386725 -0.04880866 -0.674043  ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 394. State = [[-0.22269659 -0.04385152]]. Action = [[ 0.03143952  0.04812007 -0.03497582  0.6836473 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.22255386 -0.0422751 ]]. Action = [[-0.092524    0.07899388  0.0963654   0.51476526]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 396. State = [[-0.22304684 -0.03948493]]. Action = [[-0.01588707  0.08525688 -0.04969781  0.8942833 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 397. State = [[-0.22379267 -0.03523607]]. Action = [[-0.06047831 -0.03895907  0.07691865  0.82564783]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 398. State = [[-0.2242289  -0.03394717]]. Action = [[ 0.03243507 -0.03743354  0.04175355  0.5348234 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.22431256 -0.03375866]]. Action = [[-0.02502259  0.02675308 -0.07788689  0.01372337]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 400. State = [[-0.22441497 -0.03327405]]. Action = [[0.07772494 0.09951211 0.02040664 0.58572173]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 401. State = [[-0.22482413 -0.03010103]]. Action = [[-0.01029526  0.07355908 -0.01200412  0.49983072]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.22538994 -0.0266722 ]]. Action = [[-0.06391034 -0.0481039   0.09763693  0.87204146]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.22558714 -0.02548155]]. Action = [[-0.0214486  -0.00509815 -0.05765365 -0.33624136]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(9.4780e-05, grad_fn=<MseLossBackward0>)
Current timestep = 404. State = [[-0.22571987 -0.02502289]]. Action = [[ 0.06510451 -0.06064881 -0.07413004  0.08680069]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(7.6406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 405. State = [[-0.22567245 -0.02521191]]. Action = [[-0.04140638 -0.01794428  0.00623287  0.90235066]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.22565083 -0.02563374]]. Action = [[ 3.9287783e-02 -1.2196608e-02  1.4594942e-04 -4.9429518e-01]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.2256361  -0.02588657]]. Action = [[ 0.02322684  0.00738961 -0.03014113 -0.85473907]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.22550796 -0.02621596]]. Action = [[-0.08136285 -0.07999573 -0.08097212  0.5690057 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 409. State = [[-0.22551303 -0.0282522 ]]. Action = [[ 0.04258075 -0.01220646  0.00704503 -0.19922554]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 410. State = [[-0.2253149  -0.02935812]]. Action = [[ 0.06744652  0.00083176 -0.07595656 -0.57575536]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.22500946 -0.03006134]]. Action = [[-0.04099967 -0.09393722  0.02775755 -0.9351791 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 412. State = [[-0.22440888 -0.03295266]]. Action = [[-0.05523876 -0.00075684  0.02614272 -0.5145669 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 413. State = [[-0.22470926 -0.0346464 ]]. Action = [[ 0.09805115  0.09228604 -0.00603113 -0.90436035]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(4.9710e-05, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.22440037 -0.03426616]]. Action = [[ 0.04240463  0.08894963 -0.07718045  0.79680824]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.22435048 -0.03287382]]. Action = [[-0.0341469  -0.03400701 -0.04493199 -0.09723753]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(9.6377e-05, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.22423647 -0.03288807]]. Action = [[ 0.07389138 -0.04926223  0.05372231  0.7024019 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.22328638 -0.0329232 ]]. Action = [[-0.06833842 -0.06014882 -0.06310736  0.87841284]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.22216867 -0.03402769]]. Action = [[ 0.07133878  0.0067721   0.02607162 -0.30392504]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.22093506 -0.03448249]]. Action = [[-0.01547596  0.00427651  0.03165307 -0.26350582]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.22062877 -0.03481717]]. Action = [[-0.09074836  0.00879224  0.09281924 -0.5194022 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 421. State = [[-0.22080424 -0.03502143]]. Action = [[-0.06545733 -0.0131553   0.05383586  0.90896654]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.22209175 -0.03600634]]. Action = [[-0.09056477 -0.09343635  0.00175675  0.32065797]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.22389688 -0.03906163]]. Action = [[-0.04274406 -0.04684571 -0.03396644  0.886212  ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 424. State = [[-0.22664925 -0.04172258]]. Action = [[-0.05242212  0.02192913 -0.00887658  0.73913956]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 425. State = [[-0.23042217 -0.04262299]]. Action = [[ 0.03212471  0.09667382 -0.07366855  0.8541882 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.23215534 -0.04153592]]. Action = [[-0.05731085  0.02264084 -0.06382654 -0.66104347]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.23398717 -0.04057648]]. Action = [[-0.08674761 -0.09807057 -0.05970434  0.07289839]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.23730633 -0.04157033]]. Action = [[-0.00925952  0.00566745 -0.02214188 -0.9962109 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 429. State = [[-0.23982756 -0.04212386]]. Action = [[-0.06914964  0.07512832 -0.0857294   0.8350489 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.24320045 -0.04105755]]. Action = [[-0.04567164  0.08408878  0.02753072  0.7582059 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.2457776  -0.03828927]]. Action = [[-0.07803021 -0.07294844  0.07368691 -0.2527051 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(7.5947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 432. State = [[-0.24846807 -0.03839333]]. Action = [[-0.02636785  0.08030815  0.01836669 -0.69226515]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.2512643  -0.03621318]]. Action = [[ 4.9890578e-04  8.8101394e-02 -7.1227744e-02  8.6327600e-01]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.25266212 -0.03265641]]. Action = [[ 0.06653301  0.04439088  0.04367635 -0.6971203 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 435. State = [[-0.25338173 -0.02951067]]. Action = [[-0.04786671 -0.08582233 -0.03558002  0.01783121]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(9.8119e-05, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.2536042  -0.02935847]]. Action = [[-0.08034077  0.05801918 -0.09428601 -0.8140956 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.25499165 -0.02790843]]. Action = [[-0.01514248  0.068372    0.07144574  0.90084887]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.2567695  -0.02452998]]. Action = [[ 0.08361521  0.07561073 -0.01817026  0.46739793]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 439. State = [[-0.25774238 -0.02066195]]. Action = [[0.01217459 0.07491786 0.07937755 0.56548   ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.25881767 -0.01693615]]. Action = [[-0.06787447 -0.04508771 -0.08060661  0.92620444]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 441. State = [[-0.2593786  -0.01526318]]. Action = [[-0.07080326 -0.00154758  0.09575719 -0.86908513]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.26068497 -0.01429779]]. Action = [[ 0.02464245  0.06640901 -0.01114126  0.37169123]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 443. State = [[-0.26177472 -0.01143309]]. Action = [[-0.07850166  0.0859114   0.03904288  0.40321314]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 444. State = [[-0.2635023  -0.00716553]]. Action = [[ 0.01982117  0.09224004 -0.04991864  0.57472956]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 445. State = [[-0.265384   -0.00183968]]. Action = [[ 0.05268186 -0.05528273 -0.07659307  0.69402194]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(7.9419e-05, grad_fn=<MseLossBackward0>)
Current timestep = 446. State = [[-0.26586488 -0.00077859]]. Action = [[ 0.07555989 -0.01476137 -0.06229976 -0.47942233]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 447. State = [[-0.26579854 -0.00074827]]. Action = [[-0.06779836  0.0782456   0.08480365  0.92025757]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 448. State = [[-0.26656443  0.00135541]]. Action = [[ 0.06050699 -0.08728176 -0.09685787 -0.58053505]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 449. State = [[-0.2664625   0.00103414]]. Action = [[-0.04587015 -0.02436459  0.03726131  0.7156472 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 450. State = [[-2.6646373e-01  1.4361773e-04]]. Action = [[ 0.06503899  0.06924007 -0.04730337  0.35217702]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 451. State = [[-0.26650244  0.00038665]]. Action = [[-0.04662162 -0.07577141 -0.06959858 -0.75122654]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 452. State = [[-0.26654744 -0.00056966]]. Action = [[ 0.08681441 -0.02616086  0.08158744 -0.7499488 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 453. State = [[-0.2662942 -0.0013544]]. Action = [[-0.01007716  0.08322173  0.05405702 -0.9142905 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.26634866 -0.00132398]]. Action = [[-0.04823863 -0.09705024 -0.09813143  0.24759996]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.26636872 -0.00253016]]. Action = [[-0.09560984 -0.0867526   0.04882997  0.59691834]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 456. State = [[-0.26661825 -0.00567269]]. Action = [[ 0.04242044 -0.09449231  0.02466829  0.9517524 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 457. State = [[-0.26646045 -0.00955031]]. Action = [[-0.03812874  0.03921432 -0.003877    0.7916429 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 458. State = [[-0.26694867 -0.01062322]]. Action = [[ 0.01088969  0.08331127  0.0489179  -0.5909188 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 459. State = [[-0.2671737  -0.00996411]]. Action = [[ 0.03607749  0.09041084 -0.05013316  0.38241196]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 460. State = [[-0.26778904 -0.00792917]]. Action = [[-0.03889865  0.06132858  0.05802888  0.13454032]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.2685887 -0.0053224]]. Action = [[ 0.03360107 -0.00051855  0.00542275 -0.35207498]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 462. State = [[-0.26903903 -0.0039972 ]]. Action = [[-0.09297336 -0.02883444  0.09065891 -0.33354175]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 463. State = [[-0.269842   -0.00346199]]. Action = [[ 3.2130629e-04 -7.6788463e-02  6.0089864e-02 -8.4078604e-01]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 464. State = [[-0.27059394 -0.00366387]]. Action = [[ 0.06036279  0.07975782  0.06615091 -0.6378335 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 465. State = [[-0.2706692  -0.00343899]]. Action = [[ 0.00986149  0.00643466 -0.0397307   0.934242  ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.2707083  -0.00332816]]. Action = [[-0.06191469  0.06737035 -0.04351383 -0.98169166]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.27148825 -0.00087566]]. Action = [[ 0.08644404  0.07274521  0.01871624 -0.67531925]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 468. State = [[-0.2721385   0.00232046]]. Action = [[-0.02200798  0.0474977   0.00288968 -0.8019638 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.2729556   0.00550431]]. Action = [[ 0.09613893 -0.06432364 -0.07239355 -0.67625344]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 470. State = [[-0.27210414  0.00598382]]. Action = [[ 0.08415686  0.00301509 -0.07299445 -0.18806201]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 471. State = [[-0.27005914  0.00644976]]. Action = [[-0.09023928 -0.0156367  -0.02301611  0.80730045]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(9.1346e-05, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.26938275  0.00660549]]. Action = [[ 0.04852097 -0.01865137 -0.06356096 -0.6338516 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 473. State = [[-0.26858374  0.00673637]]. Action = [[-0.08923309  0.04315824  0.04988744 -0.11461329]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.268766    0.00715535]]. Action = [[-0.02924206  0.06255288 -0.03954525 -0.41792727]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 475. State = [[-0.26937655  0.00899058]]. Action = [[-0.08356771  0.06037083 -0.08705616  0.38523293]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 476. State = [[-0.27074936  0.01270752]]. Action = [[ 0.02036326  0.07397408 -0.08530106 -0.27176106]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.27223682  0.01670667]]. Action = [[-0.07570609  0.09118976 -0.04304202  0.4054762 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 478. State = [[-0.27444237  0.02212615]]. Action = [[-0.009867   -0.04321239 -0.00058386  0.2393533 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 479. State = [[-0.27546442  0.0242916 ]]. Action = [[ 0.09198835 -0.0176747   0.0376066   0.02506065]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 480. State = [[-0.27536142  0.02447466]]. Action = [[0.02285985 0.05853034 0.08080929 0.5843618 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.27568814  0.02582615]]. Action = [[-0.0068894   0.04007497 -0.05621262  0.62390924]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.2760106   0.02754164]]. Action = [[ 0.08449943 -0.06247345 -0.07803938  0.7022368 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 483. State = [[-0.27504528  0.02745421]]. Action = [[ 0.04397497 -0.09450597  0.06777018  0.65632176]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 484. State = [[-0.27381933  0.02575949]]. Action = [[ 0.07852583 -0.08054654  0.07330956 -0.80718756]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 485. State = [[-0.27130532  0.02279431]]. Action = [[ 0.02452133 -0.05386036  0.08621115 -0.5897225 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 486. State = [[-0.26878563  0.02012543]]. Action = [[ 0.03681353  0.05889382  0.05648074 -0.5483258 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 487. State = [[-0.26660487  0.01982138]]. Action = [[-0.08092636  0.0471063  -0.07134666  0.83450556]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(7.0622e-05, grad_fn=<MseLossBackward0>)
Current timestep = 488. State = [[-0.26643816  0.02021881]]. Action = [[-0.05714988  0.0888266   0.08681966 -0.93599474]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 489. State = [[-0.26740462  0.02259395]]. Action = [[-0.02910037  0.09576011  0.08304561 -0.6896011 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 490. State = [[-0.2686985  0.0259439]]. Action = [[-0.08157012 -0.08732619  0.02729192 -0.8367725 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.2689777  0.0263781]]. Action = [[ 0.0593252  -0.03755156  0.07926983  0.937094  ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(7.3403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.26900464  0.02613491]]. Action = [[-0.05422554  0.06360371 -0.09081221  0.08331859]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 493. State = [[-0.26945102  0.02699297]]. Action = [[-0.05235106  0.0768939   0.03341826 -0.8358541 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.27064052  0.0299804 ]]. Action = [[0.0580349  0.05270205 0.05129933 0.39364433]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 495. State = [[-0.27170265  0.03266225]]. Action = [[-0.04328793  0.0648771   0.08365083  0.59092665]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.27303687  0.03586581]]. Action = [[ 0.0958632   0.0719187  -0.04568358 -0.26260448]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.27402073  0.03982756]]. Action = [[-0.0744697  -0.05659337  0.08202937 -0.5602453 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.2743151   0.04055722]]. Action = [[ 0.08570046 -0.01798604  0.05759162 -0.88993245]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 499. State = [[-0.27417472  0.04035899]]. Action = [[ 0.0053974  -0.07211862  0.05402515  0.2660774 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.27369547  0.03995533]]. Action = [[ 0.00722782  0.02034898 -0.02547933 -0.93041784]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 501. State = [[-0.27356762  0.03988453]]. Action = [[-0.02565577  0.05003     0.09231514  0.3659761 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.2735205   0.04018998]]. Action = [[ 0.08002073  0.04300158 -0.07241833 -0.33877188]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 503. State = [[-0.27266482  0.04110605]]. Action = [[-0.0015634  -0.03723471 -0.00907483  0.86892164]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 504. State = [[-0.27247462  0.04133388]]. Action = [[-0.09029538  0.09135015  0.08579626 -0.74180144]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.27320752  0.04326368]]. Action = [[-0.03292149 -0.07634135  0.02701939 -0.7271104 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.27320608  0.04320263]]. Action = [[-0.00792152 -0.05398431 -0.09532595  0.90901244]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.2731279   0.04289978]]. Action = [[-0.03923485  0.04139102 -0.06575224 -0.3065666 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 508. State = [[-0.2732237   0.04281287]]. Action = [[-0.04735613 -0.05705452  0.08500689 -0.95223695]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 509. State = [[-0.273328    0.04210194]]. Action = [[-0.0892456  -0.0104062   0.08000865 -0.6745201 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 510. State = [[-0.27478617  0.04151096]]. Action = [[ 0.005312    0.0280159  -0.09117743  0.3721161 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.27639964  0.04141727]]. Action = [[-0.00518116  0.02088519  0.09728541 -0.8635414 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 512. State = [[-0.27755487  0.04175011]]. Action = [[ 0.08551273  0.07502199  0.03058582 -0.15917456]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.27817875  0.04337764]]. Action = [[ 0.0060459   0.04782046  0.04118631 -0.07746726]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 514. State = [[-0.2789842   0.04542154]]. Action = [[0.04865211 0.04073567 0.01424682 0.7609706 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 515. State = [[-0.27954942  0.04788745]]. Action = [[-0.04374217  0.05986298  0.07123662  0.8931248 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.2807068   0.05068489]]. Action = [[ 0.09335168 -0.08988004  0.03567237  0.63027954]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 517. State = [[-0.28037348  0.05082572]]. Action = [[-0.05969698  0.03519095  0.07371929  0.28313613]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.28053877  0.05117735]]. Action = [[-0.06555447  0.01882821  0.07379574  0.6721114 ]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.28086707  0.05196634]]. Action = [[-0.00542849 -0.08218084  0.06745344 -0.05745465]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 520. State = [[-0.28085563  0.05156526]]. Action = [[ 0.09683044 -0.0206667  -0.00913428  0.7093792 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 521. State = [[-0.28048503  0.05103562]]. Action = [[0.03357974 0.06790801 0.02377429 0.0401417 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 522. State = [[-0.28009745  0.05136969]]. Action = [[ 0.01880185  0.08016364 -0.05711246 -0.32184768]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.2795041   0.05306034]]. Action = [[ 0.06799795 -0.02239327  0.00580299 -0.35156333]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 524. State = [[-0.27774483  0.05361051]]. Action = [[ 0.0476016   0.08138663  0.06778187 -0.46117353]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 525. State = [[-0.2742192   0.05571176]]. Action = [[ 0.07195988 -0.09175856 -0.07127406  0.9279628 ]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 526. State = [[-0.27093276  0.05591701]]. Action = [[-0.06588638 -0.09893063 -0.01000004 -0.57877463]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(4.7111e-05, grad_fn=<MseLossBackward0>)
Current timestep = 527. State = [[-0.2702438   0.05404557]]. Action = [[-0.07956392 -0.03402801  0.08719081 -0.5656534 ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(5.5938e-05, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.27007437  0.05213489]]. Action = [[ 0.09767466 -0.0464437   0.01470164  0.2556156 ]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.26916072  0.05033529]]. Action = [[0.07294608 0.04252531 0.05341838 0.6088021 ]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(9.3682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.26771447  0.05011739]]. Action = [[ 0.05212218  0.05878728 -0.00909106  0.5214267 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.26646435  0.05063843]]. Action = [[ 0.08699322 -0.0601525   0.03368217 -0.52293426]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(7.3682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.2637748   0.05004529]]. Action = [[ 0.02760268  0.03325208  0.08200087 -0.30477995]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(7.7952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.26165614  0.05033328]]. Action = [[ 0.06232261  0.05689353 -0.07105488 -0.42842507]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(7.4449e-05, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.25851306  0.05111746]]. Action = [[-0.05851036  0.04426933  0.06944507  0.71291006]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(2.8730e-05, grad_fn=<MseLossBackward0>)
Current timestep = 535. State = [[-0.25712824  0.05253382]]. Action = [[ 0.07214854  0.08879579 -0.02409739  0.9219308 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.25409314  0.05541014]]. Action = [[ 0.09801     0.00974212 -0.04343249  0.7688812 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.24923436  0.05741417]]. Action = [[0.02749891 0.00557176 0.02711124 0.59328294]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(7.9385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 538. State = [[-0.2441151   0.05904673]]. Action = [[ 0.06540256 -0.05210602 -0.02949141 -0.57300836]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(1.5524e-05, grad_fn=<MseLossBackward0>)
Current timestep = 539. State = [[-0.24076834  0.05972015]]. Action = [[-0.0168676   0.0378618  -0.03326654 -0.85221756]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(6.8762e-06, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.2390682   0.06005575]]. Action = [[-0.04832379 -0.07102333 -0.08409175  0.2510717 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 541. State = [[-0.23900184  0.05971017]]. Action = [[ 0.08781112 -0.0818371  -0.07245736 -0.38568735]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(4.8955e-05, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.237571    0.05742268]]. Action = [[-0.09077123 -0.01349715 -0.09651936 -0.2517255 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(7.3559e-05, grad_fn=<MseLossBackward0>)
Current timestep = 543. State = [[-0.23728305  0.05638804]]. Action = [[-0.02110303  0.01361839 -0.02126519  0.5656102 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(7.2634e-05, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.2371444   0.05617942]]. Action = [[-0.02675422 -0.02542437 -0.05102892 -0.22081894]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(7.8776e-05, grad_fn=<MseLossBackward0>)
Current timestep = 545. State = [[-0.23703174  0.05538381]]. Action = [[ 0.05644167 -0.07238223  0.00864676  0.26322365]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(7.5297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 546. State = [[-0.23611514  0.0532996 ]]. Action = [[ 0.08902711 -0.03138618 -0.05260476 -0.74711573]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(3.5638e-05, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.23418257  0.0506891 ]]. Action = [[ 0.04601829 -0.08362938 -0.00173961  0.53942466]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.23194933  0.04663551]]. Action = [[ 0.04317888 -0.03036563 -0.08684374 -0.68529797]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 549. State = [[-0.23008291  0.04290651]]. Action = [[-0.03766791 -0.03608735  0.01770741 -0.6185344 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.22886798  0.0402227 ]]. Action = [[-0.08312199 -0.09089217 -0.05659018  0.29899716]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(6.1684e-05, grad_fn=<MseLossBackward0>)
Current timestep = 551. State = [[-0.22811764  0.03561866]]. Action = [[-0.0156261  -0.07066555 -0.06699896  0.03902531]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(4.6312e-05, grad_fn=<MseLossBackward0>)
Current timestep = 552. State = [[-0.22836784  0.03067234]]. Action = [[-0.05628658 -0.09274568 -0.07816266  0.35014558]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(8.8702e-05, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.22941518  0.02577552]]. Action = [[ 0.0102044   0.05682733 -0.02181225  0.9498358 ]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 554. State = [[-0.2297065   0.02455572]]. Action = [[-0.01283883 -0.07254465  0.07823429  0.8608968 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.22996646  0.02185297]]. Action = [[-0.08265293 -0.03998065  0.00832395  0.52652264]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 556. State = [[-0.23124415  0.0188957 ]]. Action = [[-0.04859714 -0.08754957 -0.04324437 -0.24251157]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.23299813  0.0133553 ]]. Action = [[ 0.09035695 -0.03758205  0.02839472  0.6027732 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.23284703  0.00932762]]. Action = [[-0.09374096  0.03638632 -0.06205846  0.5787386 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(8.1864e-05, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.23373789  0.00800307]]. Action = [[-0.04185855 -0.06127531 -0.03171408  0.6842712 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 560. State = [[-0.23464258  0.00606355]]. Action = [[-0.0120875  -0.06144152 -0.09186681 -0.21839601]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(3.8646e-05, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.23585051  0.0027267 ]]. Action = [[ 0.08501983 -0.0398288   0.08857507  0.53373504]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 562. State = [[-0.23581326 -0.00059984]]. Action = [[-0.08773118 -0.04656091 -0.04543823 -0.20231068]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(6.4051e-05, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.23788436 -0.00427364]]. Action = [[-0.0504191  -0.02458908 -0.06915621  0.5707406 ]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 564. State = [[-0.24090314 -0.00779431]]. Action = [[ 0.08256287 -0.07112586  0.02836464 -0.30479753]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(1.9749e-05, grad_fn=<MseLossBackward0>)
Current timestep = 565. State = [[-0.24154171 -0.01079361]]. Action = [[0.08968278 0.08515766 0.0446464  0.769434  ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.24070297 -0.01118913]]. Action = [[-0.06750748 -0.05709493 -0.09631261  0.1600045 ]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(3.2264e-06, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.24070093 -0.01213422]]. Action = [[-0.00166608 -0.08624662 -0.01137    -0.00064832]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(4.6655e-07, grad_fn=<MseLossBackward0>)
Current timestep = 568. State = [[-0.24046381 -0.01490192]]. Action = [[0.06297689 0.02108233 0.09652431 0.35086   ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(1.9606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.23923296 -0.01652811]]. Action = [[ 0.04169295 -0.04181109  0.07213051  0.28561914]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(1.9249e-05, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.23773812 -0.01892122]]. Action = [[-0.06638924 -0.04981748 -0.05079563 -0.71356136]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(7.6234e-05, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.23790501 -0.02192093]]. Action = [[-0.03479807 -0.03856689  0.07736883  0.157987  ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(2.7703e-05, grad_fn=<MseLossBackward0>)
Current timestep = 572. State = [[-0.23821428 -0.02487573]]. Action = [[ 0.08503831 -0.04458755  0.01319494  0.22015846]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(2.7643e-05, grad_fn=<MseLossBackward0>)
Current timestep = 573. State = [[-0.23682512 -0.02944738]]. Action = [[-0.05660401 -0.03429435 -0.00453296 -0.75097513]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(6.9914e-05, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.2374042 -0.0316976]]. Action = [[-0.0738387  -0.05729315  0.00847602 -0.6841953 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(3.9390e-05, grad_fn=<MseLossBackward0>)
Current timestep = 575. State = [[-0.23906942 -0.03434847]]. Action = [[0.05552513 0.09530216 0.05069111 0.07822251]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(4.0141e-05, grad_fn=<MseLossBackward0>)
Current timestep = 576. State = [[-0.23933981 -0.03431931]]. Action = [[-0.01923355 -0.09150364 -0.01306008  0.95074606]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 577. State = [[-0.23965599 -0.03631443]]. Action = [[-0.03121829 -0.0750856  -0.00143981 -0.24461365]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, True, False]
State prediction error at timestep 577 is tensor(5.6056e-06, grad_fn=<MseLossBackward0>)
Current timestep = 578. State = [[-0.24039808 -0.03964103]]. Action = [[ 0.02840782 -0.00325666  0.01115153  0.95462704]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 579. State = [[-0.24048218 -0.04213456]]. Action = [[-0.03258232 -0.07044168  0.06631111 -0.01271468]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, True, False]
State prediction error at timestep 579 is tensor(2.2184e-05, grad_fn=<MseLossBackward0>)
Current timestep = 580. State = [[-0.24099962 -0.04535327]]. Action = [[0.05750997 0.07908478 0.00568032 0.47533   ]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(6.0302e-05, grad_fn=<MseLossBackward0>)
Current timestep = 581. State = [[-0.24104527 -0.04584128]]. Action = [[-0.08167379 -0.04268863 -0.05023803 -0.2063694 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(2.1443e-06, grad_fn=<MseLossBackward0>)
Current timestep = 582. State = [[-0.24160138 -0.04684485]]. Action = [[-0.02523495  0.04427183 -0.09015255 -0.79164916]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, True, False]
State prediction error at timestep 582 is tensor(1.9706e-05, grad_fn=<MseLossBackward0>)
Current timestep = 583. State = [[-0.24197045 -0.04677348]]. Action = [[-0.00879355  0.09029452  0.06103624  0.84295726]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(7.4239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 584. State = [[-0.24237186 -0.04528419]]. Action = [[ 0.09640581  0.06644709 -0.04997313 -0.35748345]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(9.7898e-05, grad_fn=<MseLossBackward0>)
Current timestep = 585. State = [[-0.2426848  -0.04339505]]. Action = [[ 0.09712129 -0.02874109 -0.09016925 -0.09168231]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(8.2640e-05, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.24238944 -0.04259431]]. Action = [[ 0.06310344  0.04029775 -0.05669561 -0.03618854]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(8.9934e-05, grad_fn=<MseLossBackward0>)
Current timestep = 587. State = [[-0.24094267 -0.04088807]]. Action = [[-0.08152144  0.05184109  0.08774891  0.24267244]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, True, False]
State prediction error at timestep 587 is tensor(6.3157e-06, grad_fn=<MseLossBackward0>)
Current timestep = 588. State = [[-0.24097331 -0.03896211]]. Action = [[ 0.0653696  -0.03471706  0.00382887  0.34748006]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, True, False]
State prediction error at timestep 588 is tensor(1.8048e-05, grad_fn=<MseLossBackward0>)
Current timestep = 589. State = [[-0.2408295  -0.03835602]]. Action = [[-0.02328987 -0.03683598  0.01851732 -0.72707546]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(9.3102e-06, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.2408478  -0.03835406]]. Action = [[-0.05975248  0.04179647  0.09247246 -0.45136046]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, True, False]
State prediction error at timestep 590 is tensor(1.9729e-05, grad_fn=<MseLossBackward0>)
Current timestep = 591. State = [[-0.24096937 -0.03820227]]. Action = [[ 0.08208857 -0.07226644 -0.07335933  0.18807387]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(7.2252e-06, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.24077281 -0.03875886]]. Action = [[ 0.05807746 -0.09289306  0.01520999 -0.3590299 ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(5.2854e-05, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.23950638 -0.04124035]]. Action = [[-0.04038637 -0.00430664  0.01083275  0.2875619 ]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, True, False]
State prediction error at timestep 593 is tensor(3.4977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 594. State = [[-0.23896277 -0.04264132]]. Action = [[ 0.05004842 -0.09434726 -0.03212309  0.22333097]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, True, False]
State prediction error at timestep 594 is tensor(3.4770e-05, grad_fn=<MseLossBackward0>)
Current timestep = 595. State = [[-0.23788868 -0.0461238 ]]. Action = [[-0.02946639 -0.05412102  0.07718544 -0.762852  ]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, True, False]
State prediction error at timestep 595 is tensor(4.2667e-05, grad_fn=<MseLossBackward0>)
Current timestep = 596. State = [[-0.23699205 -0.04951437]]. Action = [[ 0.01284709  0.06713795  0.09024658 -0.8273978 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(2.2438e-05, grad_fn=<MseLossBackward0>)
Current timestep = 597. State = [[-0.23656623 -0.04992532]]. Action = [[ 0.09596004 -0.07704858  0.08913118  0.30916917]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, True, False]
State prediction error at timestep 597 is tensor(1.5792e-05, grad_fn=<MseLossBackward0>)
Current timestep = 598. State = [[-0.23498167 -0.05200167]]. Action = [[-0.03917575 -0.03776192  0.03840562  0.32353663]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(4.4672e-05, grad_fn=<MseLossBackward0>)
Current timestep = 599. State = [[-0.23349972 -0.05484962]]. Action = [[-0.08915647 -0.07465087 -0.08457677 -0.9626164 ]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(2.6259e-05, grad_fn=<MseLossBackward0>)
Current timestep = 600. State = [[-0.23273112 -0.05849133]]. Action = [[ 0.06981819  0.01666313 -0.07654423 -0.8735806 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(2.8711e-05, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.23227257 -0.06072728]]. Action = [[ 0.02796679 -0.07018715 -0.03302501 -0.83825326]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(2.5284e-05, grad_fn=<MseLossBackward0>)
Current timestep = 602. State = [[-0.23142578 -0.06377343]]. Action = [[-0.05195142  0.06998528 -0.09340265 -0.21227467]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(1.4788e-05, grad_fn=<MseLossBackward0>)
Current timestep = 603. State = [[-0.23133467 -0.06395596]]. Action = [[-0.07980309 -0.03250867 -0.08593605 -0.48595268]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(6.4888e-06, grad_fn=<MseLossBackward0>)
Current timestep = 604. State = [[-0.2318658 -0.0644888]]. Action = [[ 0.05744839 -0.02369473  0.09080417 -0.05746931]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(1.9794e-05, grad_fn=<MseLossBackward0>)
Current timestep = 605. State = [[-0.23173088 -0.06524159]]. Action = [[-0.03060693  0.0250157   0.0549083   0.9792793 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 606. State = [[-0.23173955 -0.0651928 ]]. Action = [[-0.00842209 -0.00893884 -0.08016931 -0.7791198 ]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(1.7846e-05, grad_fn=<MseLossBackward0>)
Current timestep = 607. State = [[-0.23189303 -0.06541047]]. Action = [[-0.06585827  0.06671494  0.08010986 -0.78060925]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(4.8497e-05, grad_fn=<MseLossBackward0>)
Current timestep = 608. State = [[-0.23269412 -0.06521302]]. Action = [[-0.08132131 -0.0838282   0.01668962  0.30715883]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(4.5018e-05, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.23427993 -0.06690095]]. Action = [[ 0.04559725 -0.08100947  0.07018495 -0.7456675 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(3.0590e-05, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.2350451  -0.06969713]]. Action = [[ 0.03302104 -0.08789671 -0.06787486  0.05924785]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(2.3244e-05, grad_fn=<MseLossBackward0>)
Current timestep = 611. State = [[-0.23488593 -0.07356072]]. Action = [[ 0.0956363   0.05730324 -0.05473474 -0.893593  ]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(9.2471e-05, grad_fn=<MseLossBackward0>)
Current timestep = 612. State = [[-0.23462349 -0.0745491 ]]. Action = [[-0.06812461 -0.00185729 -0.00251397  0.64414203]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, True, False]
State prediction error at timestep 612 is tensor(8.3293e-05, grad_fn=<MseLossBackward0>)
Current timestep = 613. State = [[-0.23451355 -0.07484123]]. Action = [[-0.05997107 -0.04220868 -0.0831889  -0.55840975]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(6.4625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 614. State = [[-0.2351961  -0.07626894]]. Action = [[-0.07622837 -0.00998349  0.02450605  0.88774383]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, True, False]
State prediction error at timestep 614 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 615. State = [[-0.23618126 -0.07752319]]. Action = [[ 0.05216969 -0.05487732  0.04133485  0.5846827 ]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(9.6035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 616. State = [[-0.23629618 -0.08004525]]. Action = [[-0.03502227 -0.04912354  0.06163139 -0.13685411]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, True, False]
State prediction error at timestep 616 is tensor(2.9191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 617. State = [[-0.23664354 -0.08298252]]. Action = [[-0.07296364  0.07892001 -0.03875625  0.61716294]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(4.5458e-05, grad_fn=<MseLossBackward0>)
Current timestep = 618. State = [[-0.23755252 -0.08327559]]. Action = [[ 0.04820446 -0.06227598  0.08692008 -0.02872634]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, True, False]
State prediction error at timestep 618 is tensor(5.3710e-05, grad_fn=<MseLossBackward0>)
Current timestep = 619. State = [[-0.2378644 -0.084383 ]]. Action = [[ 0.03989666  0.07263877  0.09506599 -0.6415946 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(6.5447e-05, grad_fn=<MseLossBackward0>)
Current timestep = 620. State = [[-0.23787715 -0.08415946]]. Action = [[-0.02516951  0.06595541 -0.03394505  0.7304549 ]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(2.9511e-05, grad_fn=<MseLossBackward0>)
Current timestep = 621. State = [[-0.2380341  -0.08320642]]. Action = [[ 0.01410808 -0.09946611 -0.08141793  0.02315879]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(2.6722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.23799978 -0.08377475]]. Action = [[-0.059715    0.08079291  0.02938134 -0.5982141 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(6.5018e-05, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.23827526 -0.08332755]]. Action = [[-0.06092714 -0.08720297  0.07987385 -0.2791338 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(1.6510e-05, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.23936298 -0.08444659]]. Action = [[-0.01133621 -0.08518376 -0.03776374  0.6921828 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 625. State = [[-0.24058263 -0.08763923]]. Action = [[-0.07474872 -0.03680171 -0.08522134  0.65471935]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(8.6382e-05, grad_fn=<MseLossBackward0>)
Current timestep = 626. State = [[-0.24307357 -0.09123085]]. Action = [[-0.01465029  0.01210441  0.06838106  0.6379583 ]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.24455343 -0.09328351]]. Action = [[ 0.04572361 -0.06020486 -0.07335563 -0.9548545 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.24569874 -0.09503628]]. Action = [[ 0.03865875 -0.06916144 -0.05217759  0.86993814]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 629. State = [[-0.24609719 -0.09801692]]. Action = [[-0.0838093   0.05486927  0.05930812 -0.3276736 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 630. State = [[-0.24786314 -0.09869906]]. Action = [[-0.05449864 -0.05426637 -0.06503341  0.73859906]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(7.5653e-05, grad_fn=<MseLossBackward0>)
Current timestep = 631. State = [[-0.25127915 -0.09998942]]. Action = [[ 0.01529104  0.03340345 -0.05105569 -0.59063333]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 632. State = [[-0.25367475 -0.10008074]]. Action = [[0.03047252 0.00392196 0.05302048 0.77828074]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(4.7358e-05, grad_fn=<MseLossBackward0>)
Current timestep = 633. State = [[-0.2542142  -0.10013583]]. Action = [[ 0.04061782 -0.01699742  0.07447111  0.4243543 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(6.4829e-05, grad_fn=<MseLossBackward0>)
Current timestep = 634. State = [[-0.25419357 -0.10019364]]. Action = [[-0.02928416  0.03822163 -0.09640652  0.90738785]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(5.9411e-05, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.2543612  -0.10014242]]. Action = [[ 0.01688321  0.00770895 -0.00817768 -0.92508906]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.2543     -0.10019559]]. Action = [[ 0.09654845 -0.04838789  0.03208651  0.94787574]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(6.4492e-05, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.25391197 -0.10043398]]. Action = [[ 0.04510475 -0.08063967 -0.02039547  0.68129635]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(7.4708e-05, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.25252378 -0.10283241]]. Action = [[-0.03256635  0.00295583  0.07842258 -0.5046346 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.25176454 -0.10399384]]. Action = [[ 0.05642099  0.00817589 -0.01835243  0.1127336 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 640. State = [[-0.2509855  -0.10460614]]. Action = [[ 0.04456539 -0.0476398  -0.08073356 -0.68386954]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 641. State = [[-0.24974099 -0.10621729]]. Action = [[-0.04426087  0.06182713  0.03631046  0.25384283]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(8.8241e-05, grad_fn=<MseLossBackward0>)
Current timestep = 642. State = [[-0.24960308 -0.10634238]]. Action = [[ 0.04951049 -0.04531999 -0.00714744  0.72007704]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(4.7992e-05, grad_fn=<MseLossBackward0>)
Current timestep = 643. State = [[-0.2490195  -0.10703806]]. Action = [[ 0.03425079  0.02952505 -0.07622381  0.74573493]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(1.2853e-05, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.24886127 -0.10729022]]. Action = [[-0.07750693 -0.07327599  0.01932404  0.74628246]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, True, False]
State prediction error at timestep 644 is tensor(4.0614e-05, grad_fn=<MseLossBackward0>)
Current timestep = 645. State = [[-0.24828936 -0.10886162]]. Action = [[ 0.0269371   0.0911792  -0.07087711 -0.4372263 ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 646. State = [[-0.24831916 -0.10863069]]. Action = [[-0.02247347 -0.02655286 -0.02928995  0.01844168]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 647. State = [[-0.24828105 -0.10894618]]. Action = [[-0.02307608 -0.07346299 -0.04721627  0.3034618 ]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(3.3387e-05, grad_fn=<MseLossBackward0>)
Current timestep = 648. State = [[-0.24812278 -0.11024798]]. Action = [[-0.06939276  0.04845477 -0.05126706  0.7490034 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(1.4809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 649. State = [[-0.24844946 -0.11049203]]. Action = [[-0.09894227 -0.07028429  0.02968322 -0.95603794]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 650. State = [[-0.2503937  -0.11180251]]. Action = [[ 0.08151907 -0.07250439 -0.07246028  0.8136878 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(7.1351e-05, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.25074774 -0.11512056]]. Action = [[ 0.05405018 -0.03903316 -0.02693702  0.19433427]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(7.1398e-05, grad_fn=<MseLossBackward0>)
Current timestep = 652. State = [[-0.25052238 -0.11786687]]. Action = [[ 0.07991216  0.03193497 -0.02100366  0.8650472 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, True, False]
State prediction error at timestep 652 is tensor(4.8703e-05, grad_fn=<MseLossBackward0>)
Current timestep = 653. State = [[-0.25028452 -0.11885595]]. Action = [[ 0.04293933 -0.08261462 -0.06277303 -0.3961408 ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 654. State = [[-0.24987783 -0.1219606 ]]. Action = [[ 0.00204089 -0.01934315 -0.06760596 -0.22189724]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, True, False]
State prediction error at timestep 654 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 655. State = [[-0.24940212 -0.124598  ]]. Action = [[ 0.09014001  0.03578807 -0.07483552  0.5897932 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(1.1501e-05, grad_fn=<MseLossBackward0>)
Current timestep = 656. State = [[-0.24764517 -0.12505557]]. Action = [[0.04642159 0.05222731 0.0693133  0.85368264]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, True, False, False]
State prediction error at timestep 656 is tensor(4.1422e-05, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.24497999 -0.12424458]]. Action = [[-0.0225822  -0.03163577  0.03506834  0.46827507]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, True, False]
State prediction error at timestep 657 is tensor(9.8088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 658. State = [[-0.24405211 -0.12437201]]. Action = [[0.04207654 0.07002748 0.02839629 0.53118944]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(3.9753e-06, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.24134286 -0.12373909]]. Action = [[0.06731007 0.02726813 0.0561393  0.18507862]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(8.0793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.23741753 -0.12258782]]. Action = [[ 0.02064989  0.02139755 -0.04558306  0.3555863 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(1.8640e-05, grad_fn=<MseLossBackward0>)
Current timestep = 661. State = [[-0.23308414 -0.12054903]]. Action = [[ 0.04463387 -0.05816045  0.04924316 -0.61710435]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 662. State = [[-0.23171745 -0.1208709 ]]. Action = [[ 0.08329945 -0.04031689  0.08798987 -0.0113073 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(6.7342e-05, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.22928035 -0.12174782]]. Action = [[ 0.01066619 -0.00941082 -0.07344163  0.08042622]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(3.8681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.22761586 -0.12219089]]. Action = [[-0.08089319  0.08602481  0.07905621 -0.57254845]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(6.1371e-05, grad_fn=<MseLossBackward0>)
Current timestep = 665. State = [[-0.22763066 -0.12160981]]. Action = [[-0.07591084 -0.07218611 -0.01661082 -0.34870815]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(3.2998e-05, grad_fn=<MseLossBackward0>)
Current timestep = 666. State = [[-0.22765452 -0.12194554]]. Action = [[0.07891054 0.04548917 0.05470604 0.08592212]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(6.0894e-05, grad_fn=<MseLossBackward0>)
Current timestep = 667. State = [[-0.22762963 -0.12181632]]. Action = [[ 0.08793374  0.0102273  -0.02246464  0.17499804]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(4.4656e-05, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.22604333 -0.12152582]]. Action = [[ 0.05816314 -0.06807855  0.09205487 -0.32486892]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, False, True, False]
State prediction error at timestep 668 is tensor(3.9372e-05, grad_fn=<MseLossBackward0>)
Current timestep = 669. State = [[-0.22344269 -0.12213783]]. Action = [[ 0.01980867 -0.08753108  0.0294717  -0.29757178]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(2.9899e-05, grad_fn=<MseLossBackward0>)
Current timestep = 670. State = [[-0.22105569 -0.12420591]]. Action = [[ 0.04895859  0.04618219 -0.08785018 -0.41917717]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 671. State = [[-0.21815597 -0.12482568]]. Action = [[-0.02765027 -0.07873269 -0.02754188 -0.2671622 ]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(4.3256e-05, grad_fn=<MseLossBackward0>)
Current timestep = 672. State = [[-0.21675594 -0.12651464]]. Action = [[ 0.08908417  0.05447274 -0.02368577  0.22266543]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, True, False, False]
State prediction error at timestep 672 is tensor(1.8538e-05, grad_fn=<MseLossBackward0>)
Current timestep = 673. State = [[-0.21306181 -0.12683149]]. Action = [[ 0.06130894  0.02773789 -0.04587751  0.44478714]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, True, False, False]
State prediction error at timestep 673 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 674. State = [[-0.20890246 -0.12679173]]. Action = [[-0.03438758 -0.06227345  0.03503013  0.11997569]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, True, False, False]
State prediction error at timestep 674 is tensor(5.2797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 675. State = [[-0.2076772  -0.12763959]]. Action = [[-0.07027055 -0.09224091  0.07213082  0.37980604]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, True, False, False]
State prediction error at timestep 675 is tensor(6.3038e-05, grad_fn=<MseLossBackward0>)
Current timestep = 676. State = [[-0.20759344 -0.13129482]]. Action = [[ 0.05353171  0.01598446 -0.08265007 -0.89386404]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, True, False, False]
State prediction error at timestep 676 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 677. State = [[-0.20753989 -0.13220099]]. Action = [[-0.0397166  -0.04131636 -0.0942306  -0.6693758 ]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, True, False, False]
State prediction error at timestep 677 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 678. State = [[-0.20741795 -0.13379674]]. Action = [[ 0.0303565  -0.02344902 -0.00874411 -0.53950214]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, True, False, False]
State prediction error at timestep 678 is tensor(5.0829e-05, grad_fn=<MseLossBackward0>)
Current timestep = 679. State = [[-0.20727251 -0.13530926]]. Action = [[ 0.0766887   0.09541879  0.07953744 -0.51488686]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, True, False, False]
State prediction error at timestep 679 is tensor(7.7973e-05, grad_fn=<MseLossBackward0>)
Current timestep = 680. State = [[-0.2055089  -0.13490282]]. Action = [[ 0.07046511  0.00587181 -0.09071403 -0.794958  ]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, True, False, False]
State prediction error at timestep 680 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 681. State = [[-0.20272328 -0.13493276]]. Action = [[-0.08867196 -0.09427999  0.04037719 -0.43353188]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, True, False, False]
State prediction error at timestep 681 is tensor(9.8653e-05, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[-0.20265453 -0.13601053]]. Action = [[-0.0760524   0.00075281 -0.0016766  -0.16186798]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [True, False, False, True, False, False]
State prediction error at timestep 682 is tensor(6.8683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[-0.2027654  -0.13681255]]. Action = [[-0.00534125  0.00121357 -0.07435976  0.6140946 ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [True, False, False, True, False, False]
State prediction error at timestep 683 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 684. State = [[-0.20296724 -0.13767342]]. Action = [[-0.07080446 -0.08638441 -0.05495444 -0.6924057 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [True, False, False, True, False, False]
State prediction error at timestep 684 is tensor(9.7703e-05, grad_fn=<MseLossBackward0>)
Current timestep = 685. State = [[-0.20314269 -0.14093515]]. Action = [[ 0.00035246 -0.07450615  0.0785692   0.31959546]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [True, False, False, True, False, False]
State prediction error at timestep 685 is tensor(5.6311e-05, grad_fn=<MseLossBackward0>)
Current timestep = 686. State = [[-0.20326056 -0.14560851]]. Action = [[-0.01648246 -0.0896932   0.03595727 -0.36159134]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [True, False, False, True, False, False]
State prediction error at timestep 686 is tensor(3.9526e-05, grad_fn=<MseLossBackward0>)
Current timestep = 687. State = [[-0.2034283  -0.15112872]]. Action = [[ 0.04083493 -0.04484082  0.09426858 -0.7117679 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [True, False, False, True, False, False]
State prediction error at timestep 687 is tensor(3.0568e-05, grad_fn=<MseLossBackward0>)
Current timestep = 688. State = [[-0.20337918 -0.1559374 ]]. Action = [[ 0.05979385 -0.07577744  0.03194112 -0.10538137]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [True, False, False, True, False, False]
State prediction error at timestep 688 is tensor(1.7861e-05, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[-0.20347732 -0.1603961 ]]. Action = [[-0.02786791  0.0839043   0.0017198  -0.03777593]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [True, False, False, True, False, False]
State prediction error at timestep 689 is tensor(3.6170e-05, grad_fn=<MseLossBackward0>)
Current timestep = 690. State = [[-0.20348309 -0.16074412]]. Action = [[ 0.00934577  0.0594291  -0.02214844  0.83463573]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, True, False, False]
State prediction error at timestep 690 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 691. State = [[-0.20350626 -0.16058126]]. Action = [[ 0.04150914  0.02647836 -0.04453886  0.34327543]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, True, False, False]
State prediction error at timestep 691 is tensor(5.1409e-05, grad_fn=<MseLossBackward0>)
Current timestep = 692. State = [[-0.20333925 -0.15957364]]. Action = [[-0.04341465  0.0586236  -0.09621808 -0.36566794]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [True, False, False, True, False, False]
State prediction error at timestep 692 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 693. State = [[-0.20322523 -0.15789819]]. Action = [[-0.09086294 -0.02870926  0.03755813 -0.4038638 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [True, False, False, True, False, False]
State prediction error at timestep 693 is tensor(8.4069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 694. State = [[-0.20325553 -0.15800977]]. Action = [[-0.01764186  0.01306842 -0.0009277   0.4311427 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [True, False, False, True, False, False]
State prediction error at timestep 694 is tensor(5.3762e-05, grad_fn=<MseLossBackward0>)
Current timestep = 695. State = [[-0.2034432 -0.1579989]]. Action = [[-0.09553921 -0.06581995 -0.08124472  0.9583659 ]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [True, False, False, True, False, False]
State prediction error at timestep 695 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 696. State = [[-0.20530893 -0.1587855 ]]. Action = [[ 0.00936781 -0.06117739 -0.01857416 -0.75316966]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [True, False, False, True, False, False]
State prediction error at timestep 696 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 697. State = [[-0.20635863 -0.16052614]]. Action = [[ 0.06012877 -0.02189209 -0.04065589 -0.6271603 ]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, True, False, False]
State prediction error at timestep 697 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 698. State = [[-0.2063498  -0.16165306]]. Action = [[ 0.06573451 -0.07203342  0.08320322  0.39214766]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, True, False, False]
State prediction error at timestep 698 is tensor(1.9337e-05, grad_fn=<MseLossBackward0>)
Current timestep = 699. State = [[-0.20635134 -0.16421193]]. Action = [[-0.09026893 -0.05463755  0.02509613 -0.48464984]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, True, False, False]
State prediction error at timestep 699 is tensor(3.7689e-05, grad_fn=<MseLossBackward0>)
Current timestep = 700. State = [[-0.20665589 -0.16792323]]. Action = [[ 0.06868843 -0.09382157 -0.04146236  0.7625375 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, True, False, False]
State prediction error at timestep 700 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 701. State = [[-0.20677647 -0.17209451]]. Action = [[ 0.03367954  0.02633622 -0.08745076  0.5556021 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, True, False, False]
State prediction error at timestep 701 is tensor(6.3628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 702. State = [[-0.20681444 -0.17411107]]. Action = [[ 0.09148646  0.04894684  0.01088462 -0.10201877]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [True, False, False, True, False, False]
State prediction error at timestep 702 is tensor(6.6657e-05, grad_fn=<MseLossBackward0>)
Current timestep = 703. State = [[-0.20643412 -0.17424928]]. Action = [[0.08144612 0.06192379 0.01892518 0.84263253]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [True, False, False, True, False, False]
State prediction error at timestep 703 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.20405549 -0.17262125]]. Action = [[-0.01185094  0.08404081  0.02290586  0.71611893]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [True, False, False, True, False, False]
State prediction error at timestep 704 is tensor(7.9932e-05, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[-0.20265907 -0.16974682]]. Action = [[ 0.07146936  0.03411169  0.02591496 -0.33138382]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, True, False, False]
State prediction error at timestep 705 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 706. State = [[-0.20072976 -0.16755664]]. Action = [[ 0.00064657 -0.0761445   0.04371577  0.5531713 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, True, False, False]
State prediction error at timestep 706 is tensor(8.3211e-05, grad_fn=<MseLossBackward0>)
Current timestep = 707. State = [[-0.19971704 -0.16769311]]. Action = [[ 0.0641469  -0.03146517  0.02907407  0.8671104 ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, True, False, False]
State prediction error at timestep 707 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 708. State = [[-0.19800381 -0.16830029]]. Action = [[-0.06982045  0.07006361 -0.04877044  0.04589713]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, True, False, False]
State prediction error at timestep 708 is tensor(3.8264e-05, grad_fn=<MseLossBackward0>)
Current timestep = 709. State = [[-0.19755732 -0.16749127]]. Action = [[ 0.02531537 -0.07790592  0.01192167  0.49578488]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, True, False, False]
State prediction error at timestep 709 is tensor(9.9348e-05, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.19761784 -0.16825843]]. Action = [[ 0.03101862 -0.08746442  0.0276758   0.9346645 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, True, False, False]
State prediction error at timestep 710 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 711. State = [[-0.19711992 -0.17008322]]. Action = [[-0.01475789  0.05851123  0.01237061 -0.62705076]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, True, False, False]
State prediction error at timestep 711 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 712. State = [[-0.19666322 -0.17048539]]. Action = [[ 0.03634939 -0.09639952 -0.06641355  0.8606174 ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, True, False, False]
State prediction error at timestep 712 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 713. State = [[-0.1959108  -0.17231873]]. Action = [[ 0.08113565 -0.00728473  0.03883798 -0.42825472]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, True, False, False]
State prediction error at timestep 713 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 714. State = [[-0.1935837 -0.1735747]]. Action = [[-0.06442446  0.05686168 -0.07909442  0.80245066]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, True, False, False]
State prediction error at timestep 714 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 715. State = [[-0.1930805 -0.1735061]]. Action = [[ 0.02808709  0.08259172 -0.03491673 -0.24323118]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, True, False, False]
State prediction error at timestep 715 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.19243963 -0.1724394 ]]. Action = [[ 0.04231165 -0.06911753  0.05104253 -0.4341718 ]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, True, False, False]
State prediction error at timestep 716 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.19121763 -0.17265794]]. Action = [[-0.02542911  0.05917747 -0.05832084  0.4536575 ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, True, False, False]
State prediction error at timestep 717 is tensor(5.7232e-05, grad_fn=<MseLossBackward0>)
Current timestep = 718. State = [[-0.19082342 -0.17210527]]. Action = [[-0.03464852  0.06746203 -0.08229872  0.6623521 ]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, True, False, False]
State prediction error at timestep 718 is tensor(8.6122e-05, grad_fn=<MseLossBackward0>)
Current timestep = 719. State = [[-0.19072802 -0.17049529]]. Action = [[ 0.05790254 -0.08821479  0.03792007  0.09632885]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, True, False, False]
State prediction error at timestep 719 is tensor(1.4717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 720. State = [[-0.19026938 -0.17065904]]. Action = [[-0.02544767  0.01520966  0.05402737 -0.77579993]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, True, False, False]
State prediction error at timestep 720 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[-0.1897988 -0.1707158]]. Action = [[-0.08075441 -0.04437171  0.07087555 -0.17489392]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [True, False, False, True, False, False]
State prediction error at timestep 721 is tensor(4.1459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 722. State = [[-0.19001809 -0.17168255]]. Action = [[-0.09080703 -0.09418913  0.09333111 -0.12896168]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [True, False, False, True, False, False]
State prediction error at timestep 722 is tensor(1.4188e-05, grad_fn=<MseLossBackward0>)
Current timestep = 723. State = [[-0.19034892 -0.17486961]]. Action = [[ 0.03320076  0.06363516  0.00700705 -0.17771333]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [True, False, False, True, False, False]
State prediction error at timestep 723 is tensor(7.0387e-05, grad_fn=<MseLossBackward0>)
Current timestep = 724. State = [[-0.1905876  -0.17512642]]. Action = [[ 0.08620887 -0.06394893  0.07059791 -0.6128213 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [True, False, False, True, False, False]
State prediction error at timestep 724 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 725. State = [[-0.19048855 -0.1762068 ]]. Action = [[ 0.0738408  -0.06870915 -0.04155776  0.94576204]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [True, False, False, True, False, False]
State prediction error at timestep 725 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 726. State = [[-0.18984382 -0.17817731]]. Action = [[ 0.00374228  0.08552323 -0.00777127  0.36478364]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [True, False, False, True, False, False]
State prediction error at timestep 726 is tensor(6.1366e-05, grad_fn=<MseLossBackward0>)
Current timestep = 727. State = [[-0.18875512 -0.17830457]]. Action = [[-0.07609387 -0.06164208  0.04598198 -0.9266363 ]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [True, False, False, True, False, False]
State prediction error at timestep 727 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 728. State = [[-0.18881841 -0.17919704]]. Action = [[0.02857421 0.07284122 0.04035392 0.72074866]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [True, False, False, True, False, False]
State prediction error at timestep 728 is tensor(9.2026e-05, grad_fn=<MseLossBackward0>)
Current timestep = 729. State = [[-0.18882763 -0.17885098]]. Action = [[-0.07801406 -0.04321615 -0.00468276 -0.03204668]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [True, False, False, True, False, False]
State prediction error at timestep 729 is tensor(3.7964e-05, grad_fn=<MseLossBackward0>)
Current timestep = 730. State = [[-0.1888562  -0.17901519]]. Action = [[-0.09557325 -0.00306334  0.07684217 -0.28618127]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [True, False, False, True, False, False]
State prediction error at timestep 730 is tensor(7.0715e-05, grad_fn=<MseLossBackward0>)
Current timestep = 731. State = [[-0.18978302 -0.18012296]]. Action = [[-0.09381512 -0.01792637 -0.0943878   0.54398227]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [True, False, False, True, False, False]
State prediction error at timestep 731 is tensor(3.3310e-05, grad_fn=<MseLossBackward0>)
Current timestep = 732. State = [[-0.19165276 -0.18143778]]. Action = [[ 0.01145048 -0.07143218 -0.00740355  0.59761584]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [True, False, False, True, False, False]
State prediction error at timestep 732 is tensor(6.0230e-05, grad_fn=<MseLossBackward0>)
Current timestep = 733. State = [[-0.19296966 -0.18356079]]. Action = [[ 0.04821341  0.07819701 -0.02004001  0.17102408]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [True, False, False, True, False, False]
State prediction error at timestep 733 is tensor(8.1147e-05, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[-0.19297358 -0.18345779]]. Action = [[ 0.02219765  0.06467582 -0.09330244  0.04575682]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [True, False, False, True, False, False]
State prediction error at timestep 734 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 735. State = [[-0.19297238 -0.18238676]]. Action = [[ 0.08226693  0.07456351 -0.02217901  0.1704135 ]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [True, False, False, True, False, False]
State prediction error at timestep 735 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 736. State = [[-0.19260783 -0.17930435]]. Action = [[ 0.0098081   0.08786536 -0.01505847 -0.10970753]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [True, False, False, True, False, False]
State prediction error at timestep 736 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 737. State = [[-0.19229706 -0.175213  ]]. Action = [[-0.05577961  0.02302797 -0.01855416  0.8387077 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [True, False, False, True, False, False]
State prediction error at timestep 737 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[-0.1920999  -0.17225468]]. Action = [[ 0.0786574   0.03152118 -0.01445885 -0.14342338]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [True, False, False, True, False, False]
State prediction error at timestep 738 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 739. State = [[-0.19175608 -0.1694483 ]]. Action = [[ 0.05140425  0.04285233  0.0152723  -0.64761055]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [True, False, False, True, False, False]
State prediction error at timestep 739 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 740. State = [[-0.19118541 -0.16640995]]. Action = [[-0.01563488  0.02464113 -0.05063397  0.494241  ]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [True, False, False, True, False, False]
State prediction error at timestep 740 is tensor(7.1516e-05, grad_fn=<MseLossBackward0>)
Current timestep = 741. State = [[-0.1909689  -0.16408135]]. Action = [[ 0.03365543 -0.07293211 -0.08379081 -0.04147297]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [True, False, False, True, False, False]
State prediction error at timestep 741 is tensor(7.0407e-05, grad_fn=<MseLossBackward0>)
Current timestep = 742. State = [[-0.18999942 -0.16400567]]. Action = [[ 0.071932    0.0831194   0.03920466 -0.8176846 ]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [True, False, False, True, False, False]
State prediction error at timestep 742 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 743. State = [[-0.18751621 -0.16174163]]. Action = [[0.07709413 0.04998148 0.0883779  0.28198922]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [True, False, False, True, False, False]
State prediction error at timestep 743 is tensor(6.4786e-05, grad_fn=<MseLossBackward0>)
Current timestep = 744. State = [[-0.1834505  -0.15912175]]. Action = [[ 0.03612491 -0.06487389  0.050085   -0.51739407]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [True, False, False, True, False, False]
State prediction error at timestep 744 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 745. State = [[-0.179701  -0.1587366]]. Action = [[ 0.09378538  0.06466443  0.06992973 -0.11531508]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [True, False, False, True, False, False]
State prediction error at timestep 745 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 746. State = [[-0.17552525 -0.1573266 ]]. Action = [[-0.08690194 -0.04683312 -0.01723273 -0.80392355]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [True, False, False, True, False, False]
State prediction error at timestep 746 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 747. State = [[-0.17522137 -0.15740229]]. Action = [[-0.00117772  0.03820706 -0.06947766  0.81587505]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [True, False, False, True, False, False]
State prediction error at timestep 747 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 748. State = [[-0.17507114 -0.15702096]]. Action = [[-0.06652668  0.09002242 -0.0858907   0.16353583]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, True, False, False]
State prediction error at timestep 748 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 749. State = [[-0.1752735  -0.15450025]]. Action = [[ 0.0650168  -0.03568853  0.00407038 -0.9455773 ]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [True, False, False, True, False, False]
State prediction error at timestep 749 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 750. State = [[-0.17529306 -0.15375601]]. Action = [[-0.06005764  0.01917294  0.07565322 -0.858996  ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, True, False, False]
State prediction error at timestep 750 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 751. State = [[-0.17520642 -0.15309408]]. Action = [[ 0.01481172 -0.05586489  0.00155331 -0.33339655]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, True, False, False]
State prediction error at timestep 751 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 752. State = [[-0.17520759 -0.15316081]]. Action = [[-0.03058279  0.08742362 -0.04306048 -0.6915524 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, True, False, False]
State prediction error at timestep 752 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 753. State = [[-0.17522703 -0.1516759 ]]. Action = [[-0.01905482  0.02873886  0.03260652  0.21796799]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, True, False, False]
State prediction error at timestep 753 is tensor(6.9148e-05, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-0.1752497  -0.15011074]]. Action = [[ 0.05752478 -0.04829435  0.00825355  0.89409006]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, True, False, False]
State prediction error at timestep 754 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 755. State = [[-0.17520511 -0.15013781]]. Action = [[ 0.07344855  0.06338792 -0.08796824 -0.7774065 ]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, True, False, False]
State prediction error at timestep 755 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 756. State = [[-0.1745393 -0.1485466]]. Action = [[-0.02981515  0.04573505 -0.04304838  0.28602135]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, True, False, False]
State prediction error at timestep 756 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 757. State = [[-0.17415294 -0.14639783]]. Action = [[ 0.09320527 -0.0960076   0.01651218  0.9079977 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, True, False, False]
State prediction error at timestep 757 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 758. State = [[-0.17274927 -0.14679696]]. Action = [[ 0.07579257 -0.0232361   0.03321125  0.75668   ]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, True, False, False]
State prediction error at timestep 758 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 759. State = [[-0.16997772 -0.14710121]]. Action = [[ 0.06295631  0.0190722  -0.02542195  0.5802109 ]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, True, False, False]
State prediction error at timestep 759 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 760. State = [[-0.16405159 -0.14630844]]. Action = [[ 0.08959689  0.01613537 -0.01631998 -0.427271  ]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, True, False, False]
State prediction error at timestep 760 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.16015026 -0.14573656]]. Action = [[ 0.08433505  0.03685277 -0.05645787  0.8328687 ]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, True, False, False]
State prediction error at timestep 761 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 762. State = [[-0.15461071 -0.14474592]]. Action = [[-0.0950267   0.06505165 -0.06870022 -0.8831086 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, True, False, False]
State prediction error at timestep 762 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 763. State = [[-0.15226701 -0.14319776]]. Action = [[-0.02538636 -0.0987008  -0.08516997  0.752241  ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, True, False, False]
State prediction error at timestep 763 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[-0.15224962 -0.14386293]]. Action = [[-0.02914832 -0.06057828 -0.07849987  0.02848864]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, True, False, False]
State prediction error at timestep 764 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.15228505 -0.14573872]]. Action = [[-0.02266935 -0.04631991  0.01204874  0.07451439]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, True, False, False]
State prediction error at timestep 765 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 766. State = [[-0.15233816 -0.14768049]]. Action = [[-0.06026685  0.03480601  0.0790657   0.0076201 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, True, False, False]
State prediction error at timestep 766 is tensor(6.5997e-05, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.15283588 -0.14850593]]. Action = [[-0.07482446 -0.08976135 -0.06331365  0.02379858]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, True, False, False]
State prediction error at timestep 767 is tensor(7.6085e-05, grad_fn=<MseLossBackward0>)
Current timestep = 768. State = [[-0.15407129 -0.151295  ]]. Action = [[-0.09360766  0.06101569 -0.0584814   0.6921905 ]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, True, False, False]
State prediction error at timestep 768 is tensor(7.9981e-05, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[-0.15557134 -0.15215512]]. Action = [[ 0.08597682 -0.08415262  0.04139533  0.40859938]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, True, False, False]
State prediction error at timestep 769 is tensor(7.6669e-05, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[-0.15559338 -0.15434037]]. Action = [[ 0.0753131  -0.08861684 -0.06716867  0.93469524]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, True, False, False]
State prediction error at timestep 770 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[-0.155481   -0.15796521]]. Action = [[-0.05813129 -0.04030452  0.00674455  0.6898407 ]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, True, False, False]
State prediction error at timestep 771 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[-0.15548179 -0.16104731]]. Action = [[ 0.02062325  0.07578001 -0.07253845  0.4870224 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, True, False, False]
State prediction error at timestep 772 is tensor(7.1715e-05, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[-0.15568328 -0.16126372]]. Action = [[ 0.03721464 -0.08186536 -0.05369767 -0.27773666]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, True, False, False]
State prediction error at timestep 773 is tensor(9.9645e-05, grad_fn=<MseLossBackward0>)
Current timestep = 774. State = [[-0.15576342 -0.16323234]]. Action = [[-0.04795115 -0.07231672 -0.04506767  0.889786  ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, True, False, False]
State prediction error at timestep 774 is tensor(9.9116e-05, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[-0.15573615 -0.16685814]]. Action = [[-0.02978566 -0.07438029  0.05664084  0.90414524]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, True, False, False]
State prediction error at timestep 775 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[-0.15585609 -0.17103066]]. Action = [[ 0.09297358 -0.06717886  0.04403741 -0.30980223]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, True, False, False]
State prediction error at timestep 776 is tensor(5.3534e-05, grad_fn=<MseLossBackward0>)
Current timestep = 777. State = [[-0.15603396 -0.17544267]]. Action = [[0.03061522 0.08103377 0.05512371 0.5186548 ]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, True, False, False]
State prediction error at timestep 777 is tensor(6.1467e-05, grad_fn=<MseLossBackward0>)
Current timestep = 778. State = [[-0.15601397 -0.17578615]]. Action = [[-0.03491441  0.08873206  0.00673622  0.6616111 ]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, True, False, False]
State prediction error at timestep 778 is tensor(8.1432e-05, grad_fn=<MseLossBackward0>)
Current timestep = 779. State = [[-0.15615548 -0.17498209]]. Action = [[ 0.08709163 -0.09474128 -0.09121832 -0.4023556 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, True, False, False]
State prediction error at timestep 779 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.15615608 -0.1755795 ]]. Action = [[-0.04895693  0.08216328 -0.00221607  0.9652772 ]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, True, False, False]
State prediction error at timestep 780 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 781. State = [[-0.15601255 -0.17459284]]. Action = [[ 0.06183278  0.05144442 -0.07197244 -0.15820545]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, True, False, False]
State prediction error at timestep 781 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[-0.15450786 -0.17285854]]. Action = [[ 8.6759828e-02  3.4209341e-04 -1.3561942e-02 -8.2912052e-01]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, True, False, False]
State prediction error at timestep 782 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 783. State = [[-0.15233651 -0.17152655]]. Action = [[0.02349262 0.09720767 0.05216909 0.60562575]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, True, False, False]
State prediction error at timestep 783 is tensor(9.3385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 784. State = [[-0.15059249 -0.1682756 ]]. Action = [[-0.01537649  0.06094327  0.04482045 -0.6107245 ]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, True, False, False]
State prediction error at timestep 784 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 785. State = [[-0.14987925 -0.16478492]]. Action = [[-0.03990318 -0.05073342  0.07539333  0.51714706]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, True, False, False]
State prediction error at timestep 785 is tensor(7.8347e-05, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[-0.1497747  -0.16420682]]. Action = [[ 0.07185545 -0.02652919  0.0221075  -0.50876766]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, True, False, False]
State prediction error at timestep 786 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 787. State = [[-0.14919057 -0.16411404]]. Action = [[-0.08849465  0.0478716  -0.06399    -0.9161208 ]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, True, False, False]
State prediction error at timestep 787 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[-0.14945386 -0.163486  ]]. Action = [[-0.03210135 -0.03566895  0.06116862  0.6006068 ]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, True, False, False]
State prediction error at timestep 788 is tensor(7.6648e-05, grad_fn=<MseLossBackward0>)
Current timestep = 789. State = [[-0.14952908 -0.16363718]]. Action = [[-0.00995631 -0.06876872  0.03182619 -0.6937297 ]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, True, False, False]
State prediction error at timestep 789 is tensor(7.4921e-05, grad_fn=<MseLossBackward0>)
Current timestep = 790. State = [[-0.14952773 -0.16458392]]. Action = [[-0.09301614  0.01307678  0.07748289 -0.01931566]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, True, False, False]
State prediction error at timestep 790 is tensor(5.6450e-05, grad_fn=<MseLossBackward0>)
Current timestep = 791. State = [[-0.15024172 -0.16503462]]. Action = [[-0.05589917  0.0813979   0.03992382 -0.59353954]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, True, False, False]
State prediction error at timestep 791 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[-0.15160336 -0.16410239]]. Action = [[-0.0132968   0.00316855 -0.03268174  0.07975388]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, True, False, False]
State prediction error at timestep 792 is tensor(8.0578e-05, grad_fn=<MseLossBackward0>)
Current timestep = 793. State = [[-0.15229371 -0.16388446]]. Action = [[ 0.0555352  -0.0616537  -0.05350164 -0.8923323 ]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, True, False, False]
State prediction error at timestep 793 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 794. State = [[-0.15229106 -0.16414398]]. Action = [[-0.08642036  0.08517089 -0.04680058 -0.11232173]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, True, False, False]
State prediction error at timestep 794 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 795. State = [[-0.15288544 -0.16347504]]. Action = [[-0.00896518 -0.06834782 -0.00868009  0.82332087]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, True, False, False]
State prediction error at timestep 795 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 796. State = [[-0.15366106 -0.16402282]]. Action = [[-0.06909899  0.07020814 -0.01462384  0.79908395]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, True, False, False]
State prediction error at timestep 796 is tensor(9.7738e-05, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[-0.15475722 -0.16341004]]. Action = [[ 0.07956309  0.02980161 -0.07685687  0.51101315]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, True, False, False]
State prediction error at timestep 797 is tensor(9.0386e-05, grad_fn=<MseLossBackward0>)
Current timestep = 798. State = [[-0.15485719 -0.16236435]]. Action = [[-0.07899864 -0.0488947  -0.02782667  0.8984587 ]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, True, False, False]
State prediction error at timestep 798 is tensor(5.4540e-05, grad_fn=<MseLossBackward0>)
Current timestep = 799. State = [[-0.15570489 -0.16256692]]. Action = [[-0.04239943  0.04746404 -0.00998211  0.04397953]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, True, False, False]
State prediction error at timestep 799 is tensor(7.7163e-05, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[-0.15656975 -0.16195768]]. Action = [[ 0.05148893 -0.02087915  0.02520324  0.02920902]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, True, False, False]
State prediction error at timestep 800 is tensor(5.7480e-05, grad_fn=<MseLossBackward0>)
Current timestep = 801. State = [[-0.15655786 -0.16176547]]. Action = [[ 0.08305082  0.05906741  0.02147869 -0.6957206 ]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, True, False, False]
State prediction error at timestep 801 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 802. State = [[-0.1564669  -0.15997428]]. Action = [[ 0.05203275 -0.02170257  0.04785132 -0.84729475]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, True, False, False]
State prediction error at timestep 802 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 803. State = [[-0.15649143 -0.15935695]]. Action = [[ 0.00333605 -0.02616518 -0.01280674  0.0708077 ]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, True, False, False]
State prediction error at timestep 803 is tensor(4.5903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 804. State = [[-0.15646787 -0.15947525]]. Action = [[-0.06402116 -0.06716716 -0.00447641  0.8263713 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, True, False, False]
State prediction error at timestep 804 is tensor(5.2718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[-0.15659103 -0.160329  ]]. Action = [[ 0.05835535 -0.08295383  0.04909489  0.75930095]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, True, False, False]
State prediction error at timestep 805 is tensor(7.5676e-05, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[-0.15667291 -0.16254225]]. Action = [[-0.09562927  0.07801688 -0.06085197 -0.6751737 ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, True, False, False]
State prediction error at timestep 806 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 807. State = [[-0.15676832 -0.16241331]]. Action = [[-0.09611633 -0.03458873 -0.07041631  0.9200034 ]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, True, False, False]
State prediction error at timestep 807 is tensor(5.1268e-05, grad_fn=<MseLossBackward0>)
Current timestep = 808. State = [[-0.15825242 -0.16372621]]. Action = [[ 0.06154091  0.065176   -0.08029208  0.06381571]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, True, False, False]
State prediction error at timestep 808 is tensor(8.6636e-05, grad_fn=<MseLossBackward0>)
Current timestep = 809. State = [[-0.15830763 -0.16344519]]. Action = [[-0.05449406 -0.09025387  0.06882279  0.52723217]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, True, False, False]
State prediction error at timestep 809 is tensor(5.6722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 810. State = [[-0.15879145 -0.16478576]]. Action = [[ 0.05348482 -0.04412252  0.06151194  0.9380882 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, True, False, False]
State prediction error at timestep 810 is tensor(8.2073e-05, grad_fn=<MseLossBackward0>)
Current timestep = 811. State = [[-0.15899877 -0.16615601]]. Action = [[ 0.08565065 -0.03563952 -0.00762613 -0.431538  ]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, True, False, False]
State prediction error at timestep 811 is tensor(7.5745e-05, grad_fn=<MseLossBackward0>)
Current timestep = 812. State = [[-0.15906423 -0.16743487]]. Action = [[ 0.0289577  -0.0436162  -0.00192371 -0.17638159]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, True, False, False]
State prediction error at timestep 812 is tensor(4.5304e-05, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[-0.159139   -0.16937995]]. Action = [[-0.0846571   0.09414618  0.09624394 -0.7407964 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, True, False, False]
State prediction error at timestep 813 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 814. State = [[-0.15915006 -0.16892304]]. Action = [[ 0.0687044   0.08483864 -0.05508615  0.04255188]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, True, False, False]
State prediction error at timestep 814 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 815. State = [[-0.15910569 -0.16730866]]. Action = [[ 0.08413135  0.02202376  0.03553274 -0.31263703]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, True, False, False]
State prediction error at timestep 815 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.15819137 -0.1659046 ]]. Action = [[-0.00314357  0.06300781 -0.06894    -0.3243271 ]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, True, False, False]
State prediction error at timestep 816 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 817. State = [[-0.15754712 -0.16323532]]. Action = [[ 0.0193577   0.07891832  0.05723316 -0.8506558 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, True, False, False]
State prediction error at timestep 817 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 818. State = [[-0.15692674 -0.15895467]]. Action = [[-0.01557308  0.02247091 -0.05150329 -0.64975387]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, True, False, False]
State prediction error at timestep 818 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[-0.15687147 -0.15622826]]. Action = [[ 2.6345253e-04 -6.2165931e-03  5.6489579e-02 -6.0691422e-01]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, True, False, False]
State prediction error at timestep 819 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 820. State = [[-0.15692727 -0.15484132]]. Action = [[-0.02952814 -0.0308297   0.0341422   0.28820074]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, True, False, False]
State prediction error at timestep 820 is tensor(3.7113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 821. State = [[-0.15678415 -0.15477626]]. Action = [[ 0.07089565  0.02441031 -0.08964286 -0.5248958 ]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, True, False, False]
State prediction error at timestep 821 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 822. State = [[-0.1565167  -0.15390438]]. Action = [[ 0.0279687   0.025971   -0.08847188  0.47974896]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, True, False, False]
State prediction error at timestep 822 is tensor(7.8339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 823. State = [[-0.15582165 -0.15313295]]. Action = [[ 0.05319858 -0.06748343  0.07142044 -0.7675909 ]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, True, False, False]
State prediction error at timestep 823 is tensor(8.7579e-05, grad_fn=<MseLossBackward0>)
Current timestep = 824. State = [[-0.15459612 -0.15326813]]. Action = [[-0.09747856  0.00663371 -0.06650664 -0.19659019]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, True, False, False]
State prediction error at timestep 824 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 825. State = [[-0.15459576 -0.15321088]]. Action = [[0.02835996 0.03248226 0.07189061 0.79656696]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, True, False, False]
State prediction error at timestep 825 is tensor(7.8392e-05, grad_fn=<MseLossBackward0>)
Current timestep = 826. State = [[-0.15473777 -0.15309578]]. Action = [[-0.04689683  0.01569729 -0.06887443 -0.57049894]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, True, False, False]
State prediction error at timestep 826 is tensor(8.3474e-05, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[-0.15474293 -0.15244862]]. Action = [[ 0.09548654  0.00518724 -0.0648376  -0.19276774]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, True, False, False]
State prediction error at timestep 827 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 828. State = [[-0.15439333 -0.15181877]]. Action = [[ 0.03555902  0.02836467 -0.08014835  0.28397822]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, True, False, False]
State prediction error at timestep 828 is tensor(7.4110e-05, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[-0.15327996 -0.15069424]]. Action = [[-0.08089639 -0.03026687  0.0854459  -0.4693048 ]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, True, False, False]
State prediction error at timestep 829 is tensor(6.8527e-05, grad_fn=<MseLossBackward0>)
Current timestep = 830. State = [[-0.15320814 -0.1505763 ]]. Action = [[-0.05418494  0.05462594  0.02251081  0.20329106]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, True, False, False]
State prediction error at timestep 830 is tensor(6.8171e-05, grad_fn=<MseLossBackward0>)
Current timestep = 831. State = [[-0.15329704 -0.14994818]]. Action = [[ 0.01534611 -0.0237673   0.07587712  0.35548818]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, True, False, False]
State prediction error at timestep 831 is tensor(2.6639e-05, grad_fn=<MseLossBackward0>)
Current timestep = 832. State = [[-0.15332371 -0.14945722]]. Action = [[ 0.09281073  0.09619126  0.03332413 -0.11067277]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, True, False, False]
State prediction error at timestep 832 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 833. State = [[-0.15282679 -0.14637673]]. Action = [[-0.00287765  0.09531412  0.060215    0.18965745]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, True, False, False]
State prediction error at timestep 833 is tensor(7.1249e-05, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[-0.15218619 -0.14155044]]. Action = [[ 0.02534609  0.02139395  0.07322996 -0.4108078 ]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, True, False, False]
State prediction error at timestep 834 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 835. State = [[-0.15164898 -0.13775234]]. Action = [[ 0.02390347  0.09307218 -0.09137069 -0.8966385 ]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, True, False, False]
State prediction error at timestep 835 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 836. State = [[-0.1511145  -0.13309842]]. Action = [[-0.09161156  0.04626251 -0.08136183 -0.16842914]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, True, False, False]
State prediction error at timestep 836 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 837. State = [[-0.15139484 -0.12876967]]. Action = [[-0.03671535  0.08689021 -0.09795265 -0.02670252]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, True, False, False]
State prediction error at timestep 837 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[-0.15160102 -0.12388558]]. Action = [[-0.07262267  0.02764735  0.04346948 -0.459444  ]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, False, True, False]
State prediction error at timestep 838 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 839. State = [[-0.15192243 -0.12030894]]. Action = [[-0.05727171 -0.09199574 -0.07503922  0.8016708 ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, False, True, False]
State prediction error at timestep 839 is tensor(4.5837e-05, grad_fn=<MseLossBackward0>)
Current timestep = 840. State = [[-0.15334857 -0.12027118]]. Action = [[-0.05710337  0.02250509  0.05405737 -0.41833293]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, False, True, False]
State prediction error at timestep 840 is tensor(4.7975e-05, grad_fn=<MseLossBackward0>)
Current timestep = 841. State = [[-0.15496227 -0.12024105]]. Action = [[-0.02958755 -0.07787918 -0.08691109 -0.57681215]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, False, True, False]
State prediction error at timestep 841 is tensor(2.7003e-05, grad_fn=<MseLossBackward0>)
Current timestep = 842. State = [[-0.1562014  -0.12158626]]. Action = [[ 0.01766513 -0.01317617 -0.06645566  0.46475923]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, False, True, False]
State prediction error at timestep 842 is tensor(2.5707e-05, grad_fn=<MseLossBackward0>)
Current timestep = 843. State = [[-0.15696223 -0.12270699]]. Action = [[-0.01926967 -0.05143962  0.04671774 -0.8736826 ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, False, True, False]
State prediction error at timestep 843 is tensor(8.4356e-05, grad_fn=<MseLossBackward0>)
Current timestep = 844. State = [[-0.15774776 -0.12450758]]. Action = [[-0.05879248 -0.00929371  0.03525665  0.7483945 ]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, False, True, False]
State prediction error at timestep 844 is tensor(8.5377e-05, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[-0.15929836 -0.1259089 ]]. Action = [[-0.09613919  0.0838952   0.05832656 -0.7738752 ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, True, False, False]
State prediction error at timestep 845 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 846. State = [[-0.16088688 -0.12538311]]. Action = [[ 0.0231686  -0.01706095  0.04999468  0.63007593]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, True, False, False]
State prediction error at timestep 846 is tensor(4.5506e-05, grad_fn=<MseLossBackward0>)
Current timestep = 847. State = [[-0.16170108 -0.12549376]]. Action = [[-0.03482051  0.0570282   0.09876659 -0.16963261]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, True, False, False]
State prediction error at timestep 847 is tensor(7.9308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 848. State = [[-0.16285218 -0.12436324]]. Action = [[-0.04030312  0.06110499  0.04914445 -0.11921597]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, False, True, False]
State prediction error at timestep 848 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 849. State = [[-0.16479953 -0.12184889]]. Action = [[-0.04486033  0.00884068 -0.02596935 -0.3432778 ]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, False, True, False]
State prediction error at timestep 849 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[-0.16793315 -0.11974098]]. Action = [[ 0.00442228  0.09487201  0.00449367 -0.11867917]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, False, True, False]
State prediction error at timestep 850 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 851. State = [[-0.17000952 -0.11591751]]. Action = [[ 0.05755246 -0.07206474 -0.02586699 -0.23138618]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 852. State = [[-0.17065604 -0.11568637]]. Action = [[-0.09558834  0.01996903 -0.02775766 -0.6297286 ]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(2.2766e-05, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[-0.1722282  -0.11539713]]. Action = [[ 0.05757182  0.08589866  0.01600645 -0.25380647]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, False, True, False]
State prediction error at timestep 853 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 854. State = [[-0.17288525 -0.11234199]]. Action = [[-0.02906519  0.02940639 -0.02876961 -0.01548582]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, False, True, False]
State prediction error at timestep 854 is tensor(9.4465e-05, grad_fn=<MseLossBackward0>)
Current timestep = 855. State = [[-0.17399502 -0.10970657]]. Action = [[ 0.07165248 -0.05106641 -0.01746492  0.78435314]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, False, True, False]
State prediction error at timestep 855 is tensor(5.2845e-05, grad_fn=<MseLossBackward0>)
Current timestep = 856. State = [[-0.17403318 -0.10942684]]. Action = [[-0.08132935  0.01250464  0.02368245 -0.39359838]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, False, True, False]
State prediction error at timestep 856 is tensor(5.2037e-05, grad_fn=<MseLossBackward0>)
Current timestep = 857. State = [[-0.175003   -0.10914023]]. Action = [[-0.05622343  0.05548487  0.04753702  0.01283419]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(4.8991e-05, grad_fn=<MseLossBackward0>)
Current timestep = 858. State = [[-0.17618002 -0.10726767]]. Action = [[ 0.05381573  0.01951513  0.00661139 -0.61007386]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(5.9912e-05, grad_fn=<MseLossBackward0>)
Current timestep = 859. State = [[-0.17653367 -0.10572784]]. Action = [[-0.06153407  0.06098444  0.09002138 -0.30909896]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, False, True, False]
State prediction error at timestep 859 is tensor(8.4006e-05, grad_fn=<MseLossBackward0>)
Current timestep = 860. State = [[-0.17767632 -0.10275199]]. Action = [[ 0.06736577  0.05814344 -0.06986731 -0.5726736 ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, False, True, False]
State prediction error at timestep 860 is tensor(8.0907e-05, grad_fn=<MseLossBackward0>)
Current timestep = 861. State = [[-0.17796716 -0.09957775]]. Action = [[-0.02349506 -0.06302035 -0.07946439  0.7561383 ]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, False, True, False]
State prediction error at timestep 861 is tensor(5.3160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 862. State = [[-0.17873655 -0.09918292]]. Action = [[-0.03935855 -0.05837191 -0.07074926  0.22877753]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, False, True, False]
State prediction error at timestep 862 is tensor(4.2778e-06, grad_fn=<MseLossBackward0>)
Current timestep = 863. State = [[-0.18059385 -0.10005015]]. Action = [[-0.09808256 -0.00688243 -0.04008706  0.62826955]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, False, True, False]
State prediction error at timestep 863 is tensor(1.9919e-05, grad_fn=<MseLossBackward0>)
Current timestep = 864. State = [[-0.1838602  -0.10065005]]. Action = [[ 0.02694412  0.03487522  0.02296567 -0.07825166]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(6.2873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 865. State = [[-0.18512489 -0.10077351]]. Action = [[ 0.09617316 -0.05778207  0.00176929 -0.98242456]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, False, True, False]
State prediction error at timestep 865 is tensor(1.5554e-05, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[-0.18483715 -0.10123676]]. Action = [[ 0.03158415 -0.01548666  0.02292521  0.92061615]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, False, True, False]
State prediction error at timestep 866 is tensor(7.3977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 867. State = [[-0.18447843 -0.10135079]]. Action = [[ 0.06829774  0.07683792 -0.01674135  0.925823  ]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, False, True, False]
State prediction error at timestep 867 is tensor(4.4323e-05, grad_fn=<MseLossBackward0>)
Current timestep = 868. State = [[-0.18435904 -0.10101809]]. Action = [[-0.01164925 -0.0740785  -0.09472915  0.41240716]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, False, True, False]
State prediction error at timestep 868 is tensor(1.4775e-05, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[-0.1842068  -0.10148152]]. Action = [[-0.08509077  0.08025116  0.07952959  0.75035906]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, False, True, False]
State prediction error at timestep 869 is tensor(5.1974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 870. State = [[-0.18427092 -0.10097653]]. Action = [[ 0.00114354 -0.06801812 -0.08127124  0.05172682]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, False, True, False]
State prediction error at timestep 870 is tensor(2.0233e-05, grad_fn=<MseLossBackward0>)
Current timestep = 871. State = [[-0.1843156  -0.10132182]]. Action = [[-0.03150403 -0.04766917  0.08979373  0.4269458 ]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, False, True, False]
State prediction error at timestep 871 is tensor(2.9577e-05, grad_fn=<MseLossBackward0>)
Current timestep = 872. State = [[-0.18425256 -0.10299443]]. Action = [[ 0.04861098 -0.05087803 -0.0751303   0.7376566 ]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(7.5061e-05, grad_fn=<MseLossBackward0>)
Current timestep = 873. State = [[-0.18404698 -0.10464593]]. Action = [[-0.05108985  0.00851313  0.070499   -0.30514216]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(4.7471e-05, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[-0.18440817 -0.1056269 ]]. Action = [[-0.05615796 -0.09046692 -0.01671715  0.07698464]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, False, True, False]
State prediction error at timestep 874 is tensor(2.7696e-07, grad_fn=<MseLossBackward0>)
Current timestep = 875. State = [[-0.18547748 -0.1087497 ]]. Action = [[-0.06751604  0.07072007 -0.07466283  0.6866087 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, False, True, False]
State prediction error at timestep 875 is tensor(2.1951e-05, grad_fn=<MseLossBackward0>)
Current timestep = 876. State = [[-0.18684135 -0.10947582]]. Action = [[ 0.01419952  0.04410207 -0.0777612  -0.27240074]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(8.4618e-05, grad_fn=<MseLossBackward0>)
Current timestep = 877. State = [[-0.18733415 -0.10922151]]. Action = [[-3.2654703e-02 -3.3052266e-04 -8.8890165e-02 -7.0965767e-01]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(6.0923e-05, grad_fn=<MseLossBackward0>)
Current timestep = 878. State = [[-0.18800217 -0.10912039]]. Action = [[ 0.01270525 -0.05290706 -0.03219448  0.94757175]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(5.6290e-05, grad_fn=<MseLossBackward0>)
Current timestep = 879. State = [[-0.18842669 -0.1099148 ]]. Action = [[ 0.09389872  0.01879998 -0.07758854 -0.8863508 ]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, False, True, False]
State prediction error at timestep 879 is tensor(3.1710e-05, grad_fn=<MseLossBackward0>)
Current timestep = 880. State = [[-0.18845761 -0.10984396]]. Action = [[-0.07148112 -0.01169568  0.07038087 -0.7609506 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, False, True, False]
State prediction error at timestep 880 is tensor(6.1617e-05, grad_fn=<MseLossBackward0>)
Current timestep = 881. State = [[-0.18858603 -0.11019851]]. Action = [[ 0.05788947 -0.03532949  0.0028737  -0.3208208 ]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, False, True, False]
State prediction error at timestep 881 is tensor(4.9729e-05, grad_fn=<MseLossBackward0>)
Current timestep = 882. State = [[-0.18851642 -0.11099073]]. Action = [[ 0.00616362 -0.00803636 -0.062148    0.14151573]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(1.8318e-05, grad_fn=<MseLossBackward0>)
Current timestep = 883. State = [[-0.18858373 -0.11147399]]. Action = [[ 0.04371037 -0.01153342  0.07518785 -0.47167385]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, False, True, False]
State prediction error at timestep 883 is tensor(1.4631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[-0.18817648 -0.11176282]]. Action = [[0.05321338 0.03873564 0.06701959 0.6023319 ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, False, True, False]
State prediction error at timestep 884 is tensor(4.6119e-05, grad_fn=<MseLossBackward0>)
Current timestep = 885. State = [[-0.18815029 -0.11168339]]. Action = [[-0.05600587  0.02889796  0.02569032  0.6024847 ]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, False, True, False]
State prediction error at timestep 885 is tensor(3.4607e-05, grad_fn=<MseLossBackward0>)
Current timestep = 886. State = [[-0.18816207 -0.11152309]]. Action = [[-0.0381433   0.05224555 -0.09652968  0.22526908]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(4.2520e-05, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.18823428 -0.11042533]]. Action = [[ 0.06687189 -0.06968351  0.05892298  0.49321485]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, False, True, False]
State prediction error at timestep 887 is tensor(2.1030e-05, grad_fn=<MseLossBackward0>)
Current timestep = 888. State = [[-0.18811125 -0.11059972]]. Action = [[ 0.0279041  -0.04673195  0.06448    -0.40030348]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(2.5555e-05, grad_fn=<MseLossBackward0>)
Current timestep = 889. State = [[-0.18736283 -0.11147717]]. Action = [[-0.05590255  0.01283853 -0.0290909  -0.9101512 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, False, True, False]
State prediction error at timestep 889 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 890. State = [[-0.18683557 -0.11148059]]. Action = [[ 0.03108002  0.06237564 -0.0299003  -0.37870014]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, False, True, False]
State prediction error at timestep 890 is tensor(4.5246e-05, grad_fn=<MseLossBackward0>)
Current timestep = 891. State = [[-0.18682954 -0.11131468]]. Action = [[-0.06317003  0.00140654 -0.03258709 -0.64677495]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, False, True, False]
State prediction error at timestep 891 is tensor(1.9801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[-0.18693487 -0.1109794 ]]. Action = [[-0.05739669  0.01140926  0.05435706 -0.6695902 ]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, False, True, False]
State prediction error at timestep 892 is tensor(1.7766e-05, grad_fn=<MseLossBackward0>)
Current timestep = 893. State = [[-0.1870284  -0.11096586]]. Action = [[-0.07882828  0.03163017 -0.07953122  0.38674068]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, False, True, False]
State prediction error at timestep 893 is tensor(1.1138e-05, grad_fn=<MseLossBackward0>)
Current timestep = 894. State = [[-0.18798186 -0.10997705]]. Action = [[0.06815172 0.05648313 0.07080703 0.9822831 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(3.7651e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.18811889 -0.10810302]]. Action = [[-0.02673249  0.06169156  0.01435126  0.20346594]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(4.6901e-05, grad_fn=<MseLossBackward0>)
Current timestep = 896. State = [[-0.18851821 -0.10494144]]. Action = [[-0.04633263 -0.05803908 -0.07307966  0.40075493]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(1.1582e-05, grad_fn=<MseLossBackward0>)
Current timestep = 897. State = [[-0.18942972 -0.10448862]]. Action = [[ 0.06570887  0.08639262 -0.04870893  0.2146666 ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(5.9401e-05, grad_fn=<MseLossBackward0>)
Current timestep = 898. State = [[-0.18971309 -0.10204422]]. Action = [[-0.04436183  0.02262308  0.00567386 -0.5564841 ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, False, True, False]
State prediction error at timestep 898 is tensor(1.5944e-05, grad_fn=<MseLossBackward0>)
Current timestep = 899. State = [[-0.19018555 -0.09975809]]. Action = [[3.7211739e-02 8.5569568e-02 1.2711436e-04 5.5125678e-01]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(1.3459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 900. State = [[-0.19056183 -0.09590829]]. Action = [[-0.08056699  0.04789687  0.03933274 -0.8604626 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, False, True, False]
State prediction error at timestep 900 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 901. State = [[-0.25809926  0.00808589]]. Action = [[ 0.02024648  0.09595322  0.07606622 -0.16831112]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Current timestep = 902. State = [[-0.2574592   0.00717885]]. Action = [[-0.07364558  0.09761471 -0.08631022 -0.44928372]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(1.9294e-05, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[-0.25718316  0.00680249]]. Action = [[ 0.08290518  0.08697424  0.0084482  -0.00463122]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(3.5175e-05, grad_fn=<MseLossBackward0>)
Current timestep = 904. State = [[-0.25680304  0.00625902]]. Action = [[-0.00159758 -0.04265504  0.05027109 -0.6541669 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(2.6669e-05, grad_fn=<MseLossBackward0>)
