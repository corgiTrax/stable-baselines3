Current timestep = 0. State = [[-0.01003599  1.4279668  -0.5073682   0.37816858  0.00948947  0.07379487
   0.          0.        ]]. Action = [[0.21324694 0.9795289 ]]. Reward = [-1.4340123]
Curr episode timestep = 0
Scene graph at timestep 0 is [False, True, True, True, True, True, True, True, False, False]
State prediction error at timestep 0 is tensor(0.3411, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of 0
Current timestep = 1. State = [[-0.01507254  1.4358772  -0.50738     0.35153988  0.0131768   0.07375313
   0.          0.        ]]. Action = [[-0.8392989  -0.09151709]]. Reward = [0.38931507]
Curr episode timestep = 1
Scene graph at timestep 1 is [False, True, True, True, True, True, True, False, True, False]
State prediction error at timestep 1 is tensor(0.2693, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of 0
Current timestep = 2. State = [[-0.01999855  1.4436843  -0.4968813   0.34694323  0.01741473  0.08476616
   0.          0.        ]]. Action = [[ 0.5412488 -0.3681814]]. Reward = [-0.31727478]
Curr episode timestep = 2
Scene graph at timestep 2 is [False, True, True, True, True, True, True, True, False, True]
State prediction error at timestep 2 is tensor(0.2057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of 0
Current timestep = 3. State = [[-0.0250618   1.4514254  -0.50996715  0.34400362  0.02102486  0.07220919
   0.          0.        ]]. Action = [[0.694458  0.4086058]]. Reward = [-2.3096716]
Curr episode timestep = 3
Scene graph at timestep 3 is [False, True, True, True, True, True, True, True, False, True]
State prediction error at timestep 3 is tensor(0.1471, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of 0
Current timestep = 4. State = [[-0.03018465  1.4595016  -0.5157125   0.3588955   0.02441256  0.06776048
   0.          0.        ]]. Action = [[ 0.19856894 -0.34518683]]. Reward = [-2.6514814]
Curr episode timestep = 4
Scene graph at timestep 4 is [False, True, True, True, True, True, True, True, False, False]
State prediction error at timestep 4 is tensor(0.1094, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of 0
Current timestep = 5. State = [[-0.0353076   1.4669781  -0.5157221   0.33222073  0.02779968  0.06774884
   0.          0.        ]]. Action = [[-0.48751938  0.37142277]]. Reward = [0.3861813]
Curr episode timestep = 5
Scene graph at timestep 5 is [False, True, True, True, True, True, True, False, False, False]
State prediction error at timestep 5 is tensor(0.0612, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 0
Current timestep = 6. State = [[-0.04048681  1.4738466  -0.52276343  0.30517238  0.03259959  0.09600665
   0.          0.        ]]. Action = [[-0.8642677  -0.88990295]]. Reward = [-0.3920592]
Curr episode timestep = 6
Scene graph at timestep 6 is [False, True, True, True, True, True, True, False, True, False]
State prediction error at timestep 6 is tensor(0.0235, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 0
Current timestep = 7. State = [[-0.04566603  1.4801154  -0.5227785   0.2785023   0.03739812  0.09597951
   0.          0.        ]]. Action = [[-0.7313478   0.12852001]]. Reward = [0.17686926]
Curr episode timestep = 7
Scene graph at timestep 7 is [False, True, True, True, True, True, True, False, True, False]
State prediction error at timestep 7 is tensor(0.0115, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of 0
Current timestep = 8. State = [[-0.05085278  1.4870319  -0.5249625   0.3072284   0.04361516  0.1243525
   0.          0.        ]]. Action = [[ 0.71824884 -0.6218263 ]]. Reward = [-3.1982937]
Curr episode timestep = 8
Scene graph at timestep 8 is [False, True, True, True, True, True, True, True, False, True]
State prediction error at timestep 8 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of 0
Current timestep = 9. State = [[-0.05606585  1.493829   -0.5275292   0.3018998   0.04977305  0.12316911
   0.          0.        ]]. Action = [[ 0.62048745 -0.44107276]]. Reward = [-1.512055]
Curr episode timestep = 9
Scene graph at timestep 9 is [False, True, True, True, True, True, True, True, False, True]
State prediction error at timestep 9 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of 0
Current timestep = 10. State = [[-0.06121387  1.5000231  -0.51937133  0.27513608  0.05429552  0.09045767
   0.          0.        ]]. Action = [[-0.89852095  0.9645393 ]]. Reward = [0.8858354]
Curr episode timestep = 10
Scene graph at timestep 10 is [False, True, True, True, True, True, True, False, True, False]
State prediction error at timestep 10 is tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of 0
Current timestep = 11. State = [[-0.06629018  1.505632   -0.51036155  0.24918348  0.05699736  0.05404178
   0.          0.        ]]. Action = [[-0.5509269  0.8640044]]. Reward = [1.1022036]
Curr episode timestep = 11
Scene graph at timestep 11 is [False, True, True, True, True, True, True, False, True, False]
State prediction error at timestep 11 is tensor(0.0376, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of 0
Current timestep = 12. State = [[-0.07143851  1.5106411  -0.5193804   0.22244288  0.06150397  0.09014057
   0.          0.        ]]. Action = [[-0.07265693 -0.75516325]]. Reward = [-0.7037473]
Curr episode timestep = 12
Scene graph at timestep 12 is [False, True, True, True, True, True, True, False, False, False]
State prediction error at timestep 12 is tensor(0.0320, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of 0
Current timestep = 13. State = [[-0.07678775  1.5155694  -0.5408501   0.21877836  0.06736684  0.11726803
   0.          0.        ]]. Action = [[ 0.72341275 -0.79962856]]. Reward = [-3.2284822]
Curr episode timestep = 13
Scene graph at timestep 13 is [False, True, True, True, True, True, True, True, False, True]
State prediction error at timestep 13 is tensor(0.0418, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.08231354  1.5205828  -0.55968523  0.2224806   0.07441257  0.14092746
   0.          0.        ]]. Action = [[ 0.4033035 -0.6817219]]. Reward = [-3.3510804]
Curr episode timestep = 14
Scene graph at timestep 14 is [False, True, True, True, True, True, True, True, False, False]
State prediction error at timestep 14 is tensor(0.0291, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 0
Current timestep = 15. State = [[-0.08783951  1.5249966  -0.55970484  0.19580382  0.08145709  0.14090316
   0.          0.        ]]. Action = [[-0.04590839 -0.26846147]]. Reward = [-0.24423887]
Curr episode timestep = 15
Scene graph at timestep 15 is [False, True, True, True, True, True, True, False, False, False]
State prediction error at timestep 15 is tensor(0.0189, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 0
Current timestep = 16. State = [[-0.09345665  1.528799   -0.57113105  0.16846247  0.09079944  0.18686415
   0.          0.        ]]. Action = [[-0.2982549  -0.92774105]]. Reward = [-1.6241386]
Curr episode timestep = 16
Scene graph at timestep 16 is [False, True, True, True, True, True, True, False, False, False]
State prediction error at timestep 16 is tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.09918699  1.5331945  -0.5807185   0.19487526  0.09842733  0.15257129
   0.          0.        ]]. Action = [[0.6257986 0.7614999]]. Reward = [-3.212784]
Curr episode timestep = 17
Scene graph at timestep 17 is [False, True, True, True, True, True, True, True, False, True]
State prediction error at timestep 17 is tensor(0.0134, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 0
Current timestep = 18. State = [[-0.10487594  1.5370036  -0.57549196  0.16884036  0.1049854   0.13117318
   0.          0.        ]]. Action = [[-0.70804787  0.62042797]]. Reward = [0.18737222]
