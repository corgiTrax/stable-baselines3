Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of 0
Current timestep = 2. State = [[-0.33283383 -0.08530018]]. Action = [[ 0.0591953  -0.05022874  0.          0.01753402]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of 1
Current timestep = 3. State = [[-0.32827213 -0.08361286]]. Action = [[ 0.07941724  0.04838646  0.         -0.5082104 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.3254111  -0.08398443]]. Action = [[ 0.02240539 -0.05189413  0.          0.06062686]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of 0
Current timestep = 5. State = [[-0.32796562 -0.08313386]]. Action = [[-0.06501994  0.04297563  0.         -0.71768856]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 1
Current timestep = 6. State = [[-0.3347769  -0.08682413]]. Action = [[-0.09400186 -0.09525966  0.         -0.5530498 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 0
Current timestep = 7. State = [[-0.34229678 -0.0895338 ]]. Action = [[-0.08571552  0.01548761  0.          0.7501576 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.3429899  -0.09310177]]. Action = [[ 0.06542508 -0.06993444  0.          0.3592459 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of 1
Current timestep = 9. State = [[-0.33895582 -0.0981642 ]]. Action = [[ 0.05122799 -0.04897422  0.          0.96609044]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of 0
Current timestep = 10. State = [[-0.34124407 -0.09599863]]. Action = [[-0.09368446  0.09744806  0.          0.9562907 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.3479232  -0.08940001]]. Action = [[-0.06633136  0.08776695  0.         -0.7748937 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.3524371  -0.09065792]]. Action = [[-0.02044018 -0.08020763  0.         -0.9538968 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35138932 -0.09706468]]. Action = [[ 0.06876298 -0.08378222  0.          0.79918134]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.3489947  -0.10333291]]. Action = [[ 0.03643642 -0.07364122  0.          0.9914768 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 0
Current timestep = 15. State = [[-0.3490545  -0.10776089]]. Action = [[-0.00874167 -0.03399675  0.         -0.21531457]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 0
Current timestep = 16. State = [[-0.3514492  -0.11457369]]. Action = [[-0.03317284 -0.09522063  0.         -0.46883   ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.35020816 -0.11524183]]. Action = [[ 0.06369903  0.0782093   0.         -0.04920816]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.35222882 -0.11077806]]. Action = [[-0.07404876  0.06244356  0.         -0.8011607 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.35545054 -0.10875916]]. Action = [[-0.0137436   0.01367154  0.         -0.61683035]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.6885e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.35240525 -0.10705176]]. Action = [[0.08641436 0.02212337 0.         0.3029065 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 0
Current timestep = 21. State = [[-0.3546716  -0.10593522]]. Action = [[-0.09330896  0.00641755  0.          0.02459872]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.35532948 -0.10997096]]. Action = [[ 0.06886234 -0.09308708  0.         -0.5452224 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.34958392 -0.11709596]]. Action = [[ 0.08848224 -0.0906605   0.          0.19208848]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.34299785 -0.12415883]]. Action = [[ 0.07390421 -0.08430719  0.         -0.63372886]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.33689216 -0.13070002]]. Action = [[ 0.05996113 -0.06846595  0.         -0.713076  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.3328732  -0.13672088]]. Action = [[ 0.01979733 -0.05593113  0.         -0.5039198 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 0
Current timestep = 27. State = [[-0.3271394  -0.14266874]]. Action = [[ 0.07559907 -0.05292628  0.         -0.8413073 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.32688653 -0.14325005]]. Action = [[-0.07894908  0.06929497  0.          0.70221055]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 0
Current timestep = 29. State = [[-0.32479247 -0.14016713]]. Action = [[ 0.07824219  0.05332459  0.         -0.21865898]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.32019976 -0.13723694]]. Action = [[0.04019081 0.03705273 0.         0.9082135 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.3187132  -0.13385221]]. Action = [[-0.00428964  0.05057745  0.         -0.6988953 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.31412283 -0.13590431]]. Action = [[ 0.09212277 -0.07517277  0.         -0.03933841]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 0
Current timestep = 33. State = [[-0.311713   -0.13538525]]. Action = [[-0.02737613  0.06053653  0.         -0.45546395]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 0
Current timestep = 34. State = [[-0.31361347 -0.13128443]]. Action = [[-0.03207552  0.05022833  0.         -0.98407006]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 0
Current timestep = 35. State = [[-0.31201604 -0.13103063]]. Action = [[ 0.05047034 -0.03335588  0.          0.7136332 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.31380296 -0.1343639 ]]. Action = [[-0.07212551 -0.05330601  0.         -0.94122505]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.31647712 -0.14077842]]. Action = [[-0.01979241 -0.08855762  0.         -0.6881677 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.31528398 -0.14297844]]. Action = [[ 0.02971432  0.01984738  0.         -0.70324075]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of 0
Current timestep = 39. State = [[-0.31804198 -0.13951011]]. Action = [[-0.08491311  0.07140372  0.         -0.53491676]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.3177031  -0.14129922]]. Action = [[ 0.06528539 -0.08222056  0.         -0.5150507 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.31035402 -0.14593904]]. Action = [[ 0.0972101  -0.04252072  0.         -0.46656775]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of 0
Current timestep = 42. State = [[-0.30413622 -0.14271493]]. Action = [[ 0.04084075  0.09596565  0.         -0.7093684 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 0
Current timestep = 43. State = [[-0.30225393 -0.14206135]]. Action = [[-0.00776651 -0.04124233  0.          0.8767073 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
