Current timestep = 0. State = [[-0.25918037  0.00783255]]. Action = [[ 0.05920762 -0.07895482  0.10002279  0.6916728 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0421, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25791442  0.00598314]]. Action = [[-0.2317657   0.16157115 -0.17458358 -0.9083724 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.2595307   0.00851894]]. Action = [[ 0.10773206 -0.17334172  0.23817366 -0.82670146]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25959268  0.00535983]]. Action = [[-0.15484227 -0.21613888 -0.09073718 -0.9427044 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2616548  -0.00200784]]. Action = [[-0.2233034  -0.07971513 -0.0163092  -0.9672672 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.2673328  -0.00869154]]. Action = [[-0.15626399  0.22388703 -0.17905656 -0.8183722 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.27532074 -0.0077666 ]]. Action = [[-0.22812088 -0.23531502 -0.01779941 -0.9419929 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.2859551  -0.01352601]]. Action = [[ 0.24833357 -0.1589666   0.10213259 -0.4465652 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.2895753  -0.02048738]]. Action = [[-0.12122977 -0.00838391  0.22882777  0.6865624 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.29229343 -0.02543143]]. Action = [[0.11072823 0.22055018 0.17683983 0.39470816]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.2932535  -0.02328781]]. Action = [[-0.11419702 -0.11468327 -0.12569718 -0.8624731 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.29408148 -0.02429224]]. Action = [[ 0.09041986  0.00990984 -0.18701757 -0.26434743]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.293948   -0.02469259]]. Action = [[ 0.12472767 -0.05171153  0.24319816 -0.14950627]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.29319024 -0.02611662]]. Action = [[-0.15796839 -0.07620254 -0.11188005 -0.10306561]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.29394183 -0.02935768]]. Action = [[-0.19224341 -0.08820286  0.19591266 -0.14860094]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.29845005 -0.03469082]]. Action = [[-0.22864161 -0.14790897 -0.22791325 -0.09022743]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.3055738 -0.0416104]]. Action = [[ 0.02617988  0.04806307 -0.15877637 -0.6880208 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.31157833 -0.04520823]]. Action = [[ 0.17055812  0.15221435  0.24378222 -0.9490855 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.3124823  -0.04375634]]. Action = [[ 0.09076089 -0.07554707  0.183034    0.561579  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.31187275 -0.04373263]]. Action = [[ 0.19133323 -0.04339921 -0.21247418  0.7883444 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.30839983 -0.04456632]]. Action = [[ 0.19273728 -0.02031948 -0.20063125 -0.00328368]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.30091962 -0.04521232]]. Action = [[0.16801658 0.06893885 0.2392714  0.6962435 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.2930916  -0.04546121]]. Action = [[-0.1051776  -0.20458522  0.09344646  0.8427348 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.28949037 -0.04948248]]. Action = [[ 0.02031448  0.0501647  -0.02870201 -0.90057725]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.28811485 -0.05046096]]. Action = [[-0.14807087 -0.1380519  -0.01561715  0.39058697]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.28827587 -0.05586894]]. Action = [[-0.11495721 -0.1783371   0.16024816  0.669436  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.2908126  -0.06524388]]. Action = [[-0.18333921 -0.17333387 -0.03229426 -0.27636766]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.29579547 -0.0755233 ]]. Action = [[-0.11343452 -0.1331558  -0.17077908  0.68977726]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.30158877 -0.08629426]]. Action = [[-0.19216922  0.08357662  0.09986627  0.964852  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.30923083 -0.09192385]]. Action = [[-0.0200417  -0.06577495 -0.19739759 -0.17547047]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.31559572 -0.09638529]]. Action = [[-0.13351992  0.02786636  0.20248151  0.45465016]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.3227635  -0.09831889]]. Action = [[ 0.0022707  -0.16279654 -0.10234332 -0.96534926]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.32638922 -0.10233972]]. Action = [[0.14282465 0.20240408 0.20364672 0.6665162 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.32621488 -0.10111723]]. Action = [[-0.06496766 -0.16803393 -0.07111675 -0.50275576]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 33 of -1
Current timestep = 34. State = [[-0.32630506 -0.10359483]]. Action = [[-0.05478436 -0.18497433  0.01674879 -0.16609848]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.32693332 -0.10946741]]. Action = [[0.06566465 0.20662624 0.18471462 0.63378286]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.32666773 -0.10924035]]. Action = [[ 0.18104374 -0.16276024  0.02956581 -0.7844846 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.32455203 -0.11166179]]. Action = [[ 0.22922397  0.16983676 -0.09689625 -0.42982364]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.3200288  -0.11037587]]. Action = [[-0.04656447 -0.21394046 -0.11162537 -0.6890026 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.31772432 -0.1136768 ]]. Action = [[ 0.1332553  -0.00562209 -0.09258863  0.29326057]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.31396163 -0.11681053]]. Action = [[ 0.08507904 -0.1668768  -0.11733851  0.4711212 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.30882534 -0.1232792 ]]. Action = [[-0.05055261 -0.1378023   0.24588132 -0.24206036]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.30653703 -0.13155785]]. Action = [[-0.13249584 -0.00346251 -0.09201786 -0.6622387 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.30644247 -0.13673663]]. Action = [[ 0.0937613   0.15707806 -0.14957677 -0.78746337]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.30620342 -0.13593851]]. Action = [[-0.00851464  0.18554202  0.05785269 -0.8645772 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.30610335 -0.13113232]]. Action = [[-0.15032548  0.05500603 -0.24210866 -0.5057693 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.30731767 -0.12696706]]. Action = [[-0.15726775  0.14064395  0.15485585  0.21037924]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.31106487 -0.12038703]]. Action = [[-0.13518323  0.03717807 -0.06470242  0.6670588 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.31556004 -0.11545914]]. Action = [[ 0.14815581 -0.13646528 -0.19487692  0.5708196 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.31581038 -0.1155853 ]]. Action = [[-0.20045395 -0.04302089 -0.23933105 -0.7374215 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.3179002 -0.1168986]]. Action = [[-0.04895465 -0.01574722  0.17575705 -0.6391192 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.32007653 -0.11857089]]. Action = [[-0.23338018 -0.19344419  0.09887829  0.4663205 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.32588324 -0.12516706]]. Action = [[ 0.156071   -0.21297315 -0.2208812  -0.34488642]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, True, False, False]
State prediction error at timestep 52 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.32762116 -0.13335963]]. Action = [[0.15021831 0.14334297 0.19557351 0.77002954]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.3260418  -0.13502882]]. Action = [[ 0.05962288 -0.21511008  0.15192026 -0.09369534]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.32511687 -0.14100885]]. Action = [[-0.07252608  0.2374379   0.02057245 -0.04722828]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.32484713 -0.13989952]]. Action = [[0.2289766  0.14827526 0.22862428 0.5329628 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.32216582 -0.13588005]]. Action = [[-0.06055622 -0.12635809  0.09489554 -0.9179242 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.3216073  -0.13611197]]. Action = [[ 0.11050397 -0.18859303 -0.16253765  0.90548944]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.3184613  -0.14127757]]. Action = [[ 0.01062211 -0.03164735 -0.01418614  0.8953965 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 60. State = [[-0.3173241  -0.14411715]]. Action = [[0.20911402 0.06994343 0.02126294 0.21714735]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.31393877 -0.1449571 ]]. Action = [[-0.17111062 -0.12509644  0.09689891 -0.16433436]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.3137717  -0.14803521]]. Action = [[ 0.22786266 -0.14655414 -0.00216064  0.33290195]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.31075406 -0.15365234]]. Action = [[-0.12927815 -0.05553304 -0.1964897   0.8512988 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.30986843 -0.15952726]]. Action = [[-0.05350205 -0.05831617  0.20853364 -0.592907  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.30977094 -0.16500841]]. Action = [[ 0.13785696  0.01139507 -0.2361397  -0.6664119 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.3092503  -0.16751383]]. Action = [[-0.17192826 -0.07841994 -0.2317439  -0.83464843]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.30976933 -0.17148659]]. Action = [[-2.0910299e-01  9.8139048e-05 -2.1706605e-01 -4.2927939e-01]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.31282726 -0.17452352]]. Action = [[-0.13618912  0.23586461  0.12156364  0.8249073 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.31707186 -0.17145874]]. Action = [[ 0.19052237 -0.03223224  0.17912322  0.09560061]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.31720492 -0.1703316 ]]. Action = [[ 0.02320156  0.10812259 -0.12522475 -0.26997423]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.31687248 -0.16670643]]. Action = [[-0.22796032  0.1961624  -0.2369105   0.4242468 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.31874582 -0.159827  ]]. Action = [[ 0.18885702 -0.02292129 -0.12239096  0.57346654]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.3189468  -0.15539362]]. Action = [[-0.17294097 -0.05585067 -0.05825451  0.00096548]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.31998402 -0.15473177]]. Action = [[ 0.14602536 -0.02633111 -0.12688884  0.12532103]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.31962523 -0.15472446]]. Action = [[0.10014531 0.02359599 0.04097763 0.98514867]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.31843337 -0.15464723]]. Action = [[-0.13895638 -0.23757237 -0.04601356  0.5132458 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.3191318  -0.15883026]]. Action = [[ 0.114328    0.04098144 -0.09543246 -0.10401207]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.31857315 -0.16013783]]. Action = [[ 0.0968433  -0.05654331 -0.08872983  0.26834857]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.3166548  -0.16165413]]. Action = [[-0.2055647   0.12307125  0.22297332 -0.40854466]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.3172295  -0.16173369]]. Action = [[ 0.0941079  -0.21485493  0.23019376 -0.8284544 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.31768247 -0.1657244 ]]. Action = [[ 0.04565313  0.0101161   0.15562046 -0.5600144 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.31746402 -0.16773552]]. Action = [[ 0.05873448  0.10668021  0.23290342 -0.0450235 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.31673703 -0.16742721]]. Action = [[-0.1235451   0.04098943  0.24183637 -0.692052  ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.31676945 -0.16666012]]. Action = [[-0.22297403  0.17115843 -0.05179399 -0.7023228 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.31962395 -0.16246416]]. Action = [[ 0.03514737 -0.04577415  0.1586662  -0.5839659 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.3206581  -0.16064917]]. Action = [[-0.06957255  0.18298021 -0.1712119   0.09564853]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.32235038 -0.15457349]]. Action = [[-0.18432903  0.19158632 -0.22170003  0.12543666]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.3263194  -0.14558187]]. Action = [[ 0.03710461  0.00943786  0.20437795 -0.42754483]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.32765123 -0.14001884]]. Action = [[-0.03078429  0.15127844  0.08040807 -0.7245856 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.3284165  -0.13230011]]. Action = [[ 0.16545695 -0.06768967 -0.05162776 -0.94699234]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(8.1736e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.328641  -0.1286485]]. Action = [[-0.20403308 -0.11056492  0.07776424 -0.29476368]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 92. State = [[-0.32997188 -0.12998977]]. Action = [[-0.0353919   0.01381347  0.11304092  0.8720927 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 93. State = [[-0.3308971  -0.13064377]]. Action = [[ 0.06701314 -0.1327367   0.2179529   0.1153276 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.33137742 -0.13182577]]. Action = [[0.07586676 0.14627898 0.2098422  0.15790856]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.331291   -0.13165525]]. Action = [[-0.18476836 -0.05438009 -0.12267178  0.7984514 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.3327706  -0.13291687]]. Action = [[-0.21879855 -0.19050917 -0.0811623  -0.76338995]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.33727542 -0.13741523]]. Action = [[ 0.06350297  0.10760948 -0.17348292 -0.7009772 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 98. State = [[-0.33874044 -0.13833235]]. Action = [[-0.21506743  0.22225815 -0.21818723 -0.25491726]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.34375894 -0.13624872]]. Action = [[-0.10800447  0.16043329  0.16122329 -0.58187956]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.35085532 -0.13330376]]. Action = [[-0.18447264 -0.09077427 -0.03354086 -0.5352407 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.3580994 -0.1313928]]. Action = [[ 0.23369813 -0.15765771 -0.171738    0.56372666]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.35825694 -0.13260579]]. Action = [[-0.12962168  0.04173604  0.13711143 -0.8685036 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.3590095  -0.13298486]]. Action = [[-0.09547585 -0.11060575 -0.19425553 -0.05473489]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.36092955 -0.1346956 ]]. Action = [[ 0.23552483  0.05866072 -0.20603399 -0.8253536 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.36092263 -0.13485217]]. Action = [[ 0.08704925  0.09750023 -0.22047491 -0.8403174 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 106. State = [[-0.36023733 -0.13464299]]. Action = [[-0.17469424  0.22180134 -0.05642435 -0.10857552]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.36097467 -0.13321787]]. Action = [[ 0.00670424 -0.19837189 -0.22927819  0.15865946]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.36170277 -0.1332448 ]]. Action = [[-0.03275312  0.07545388  0.19304806  0.95632124]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.36256656 -0.13285947]]. Action = [[ 0.03291529 -0.05663045  0.10982201 -0.8540771 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.36302713 -0.13294704]]. Action = [[-0.16448729 -0.17872497 -0.19556417 -0.14674175]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
State prediction error at timestep 110 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.36558834 -0.1349755 ]]. Action = [[ 0.17695701  0.0067836   0.16681483 -0.93720114]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.36569056 -0.13531409]]. Action = [[-0.16027011 -0.02970575  0.05651328  0.06515741]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.36770445 -0.13717544]]. Action = [[ 0.24166548 -0.06623533 -0.21513477  0.4849676 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.36656216 -0.13786158]]. Action = [[ 0.10786444 -0.054802   -0.11932348  0.49073482]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.36356828 -0.13955775]]. Action = [[-0.11148539 -0.08974659 -0.21645151  0.92013466]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
State prediction error at timestep 115 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 116. State = [[-0.36293253 -0.14223257]]. Action = [[ 0.15652394  0.09846374 -0.24154554 -0.153548  ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.3608003  -0.14342079]]. Action = [[-0.15341908  0.15626365  0.03902516  0.10864937]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.36081585 -0.14326926]]. Action = [[-0.05490707  0.07965121  0.15508169 -0.19060391]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.36139718 -0.14271852]]. Action = [[ 0.2295933  -0.14005025 -0.12183614 -0.05437499]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.35988095 -0.14288235]]. Action = [[-0.00446206  0.19137919  0.11111745  0.834003  ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of -1
Current timestep = 121. State = [[-0.3584926  -0.14085586]]. Action = [[ 0.17717564 -0.21468648  0.16301793  0.9549097 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.35487315 -0.1408929 ]]. Action = [[ 0.05126023 -0.05579178 -0.21841785  0.85938966]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.35216483 -0.14171128]]. Action = [[-0.15169713 -0.03697845  0.11432886 -0.5465604 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.35223427 -0.14219946]]. Action = [[ 0.10385659  0.21600986 -0.15451074  0.51113474]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.35132253 -0.14060116]]. Action = [[ 0.18663418  0.2190409  -0.13399613 -0.9362661 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
State prediction error at timestep 125 is tensor(3.5749e-05, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.34880736 -0.13618805]]. Action = [[-0.24367559 -0.00511974  0.0846945   0.19199097]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.34951603 -0.13500181]]. Action = [[ 0.15555     0.14081115  0.22017473 -0.778982  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.34783435 -0.1315524 ]]. Action = [[0.03649011 0.11692378 0.11305425 0.23286009]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.34552482 -0.12622626]]. Action = [[-0.07638916  0.18088436  0.00289243  0.62318134]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, True, False, False]
State prediction error at timestep 129 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.34553236 -0.11991888]]. Action = [[ 0.18435588 -0.15539448 -0.14898601 -0.11219305]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.34330586 -0.11726535]]. Action = [[-0.18354961 -0.12182923 -0.04190844 -0.82244676]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(3.9412e-05, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.34315062 -0.11718149]]. Action = [[-0.06572397  0.03694442  0.2316851  -0.8604655 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(7.8352e-05, grad_fn=<MseLossBackward0>)
Current timestep = 133. State = [[-0.3434066  -0.11702836]]. Action = [[ 0.03311634 -0.14298132  0.19251025 -0.9449521 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(5.0706e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.34373793 -0.11797668]]. Action = [[ 0.10280907 -0.18770875  0.03767052 -0.7090995 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.34357294 -0.12041403]]. Action = [[-0.06305176  0.04951704 -0.00946744 -0.6247771 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.34377518 -0.12102433]]. Action = [[0.09441641 0.17019176 0.15398347 0.14393032]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.34348944 -0.12056308]]. Action = [[-0.00637306 -0.03658427 -0.07905161 -0.59066474]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.3433364  -0.12041302]]. Action = [[-0.03583458 -0.08774653 -0.12678652  0.7610513 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.34348357 -0.12051312]]. Action = [[-0.17195004  0.19879031 -0.19611284 -0.7778257 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(5.5314e-05, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.34499788 -0.11972087]]. Action = [[-0.1477606   0.21419513 -0.13310385 -0.6312529 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.3487808  -0.11655386]]. Action = [[ 0.07932001 -0.03250366  0.11264771 -0.55974644]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.3508254  -0.11455226]]. Action = [[-0.08588737 -0.08670726 -0.14835353 -0.5159358 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.35393277 -0.11389074]]. Action = [[-0.23702988 -0.10620302  0.01330385  0.21130228]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.35960037 -0.11443043]]. Action = [[-0.06116962 -0.11753535 -0.03336111  0.32925928]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.36324844 -0.11595808]]. Action = [[0.15252045 0.19453591 0.19738251 0.52231526]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.36339378 -0.11587376]]. Action = [[-0.19617993 -0.21452786  0.17255381  0.5489814 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.3654396  -0.11750787]]. Action = [[-0.09332776 -0.0749633  -0.17687869  0.10614419]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.3695067  -0.12030137]]. Action = [[-0.16489585 -0.12724489  0.10020557  0.76812315]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.3730519  -0.12511118]]. Action = [[-0.04824433 -0.1281284  -0.0938938   0.60132325]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.37554747 -0.13141374]]. Action = [[0.0594337  0.2311666  0.18209383 0.43972993]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.37660775 -0.13323078]]. Action = [[-0.228519    0.23662335  0.14672342  0.74520373]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.37758133 -0.13415277]]. Action = [[-0.15594791  0.19607913 -0.01454619 -0.82542515]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.37814572 -0.13456725]]. Action = [[ 0.10374996  0.15142953 -0.00196588  0.8471242 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.3782126  -0.13357599]]. Action = [[-0.18474133 -0.13302863 -0.20989856  0.7556877 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.37856454 -0.13232085]]. Action = [[ 0.01985991 -0.13424951  0.03880197  0.771528  ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.37924433 -0.13233885]]. Action = [[ 0.24296242  0.21081322  0.23035496 -0.9975031 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, True, False, False]
State prediction error at timestep 156 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.37704062 -0.12922713]]. Action = [[-0.18403225 -0.046207    0.09245563  0.5876515 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.37523046 -0.12689796]]. Action = [[ 0.09115025  0.16128299  0.04573581 -0.8766831 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.37261698 -0.12283831]]. Action = [[ 0.21723795  0.07836717 -0.00785315 -0.5134465 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.36921728 -0.11878636]]. Action = [[-0.05027497  0.07851502 -0.17886327 -0.8140869 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.3669103  -0.11459133]]. Action = [[-0.05629332  0.21161145  0.12268698 -0.79988664]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 162. State = [[-0.36692354 -0.1087833 ]]. Action = [[-0.21181488 -0.05234587 -0.24385871  0.5926838 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.37090135 -0.10524434]]. Action = [[ 0.08856761  0.16445324 -0.17513394  0.0132767 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.3724831 -0.0995505]]. Action = [[-0.10905677  0.10936075  0.04876274 -0.71453744]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.37458885 -0.09408208]]. Action = [[ 0.05455941  0.22269666 -0.21311887 -0.5635424 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.37507325 -0.08541685]]. Action = [[-0.19389158 -0.07252946 -0.19793825 -0.49218297]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.3761767  -0.07949866]]. Action = [[ 0.12635165  0.06053299  0.04369396 -0.14167649]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.37643978 -0.07444682]]. Action = [[-0.13207181  0.01665205 -0.01565692  0.5872649 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.37903282 -0.0695505 ]]. Action = [[ 0.01321977  0.22396433 -0.1586552  -0.8639763 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.38120425 -0.06116545]]. Action = [[ 0.21757269 -0.09331998 -0.05264419  0.89614916]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.37923658 -0.05723142]]. Action = [[-0.0756291   0.16506183  0.05655959  0.8801341 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.378565   -0.05154662]]. Action = [[ 0.00713909  0.13965577  0.12357807 -0.95927846]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 173. State = [[-0.37855062 -0.04396053]]. Action = [[ 0.09790772 -0.1984234  -0.11963192  0.62393606]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.3773378  -0.04202939]]. Action = [[ 0.02244025 -0.13626847  0.15748507 -0.965609  ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.37636617 -0.04237096]]. Action = [[-0.23090936 -0.0934526  -0.12505613 -0.92927223]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.3753015  -0.04230787]]. Action = [[ 0.10832059  0.158337    0.24448526 -0.72783273]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.373356   -0.04079869]]. Action = [[ 0.1668008  -0.03393078  0.18721426 -0.23913348]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.36941466 -0.03981141]]. Action = [[-0.1097488  -0.16188954  0.00234702 -0.6976472 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.3675513  -0.04062252]]. Action = [[-0.14911535 -0.01700355 -0.21868177  0.56340766]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.36823583 -0.04075945]]. Action = [[-0.21555853  0.1400894   0.00369066 -0.03005356]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 181. State = [[-0.37198386 -0.03988956]]. Action = [[-0.05775587 -0.16392663  0.03203288  0.03754449]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.37478393 -0.04033448]]. Action = [[-0.0786065   0.16124266 -0.09131391  0.03796029]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.37842166 -0.0393584 ]]. Action = [[-0.18547447 -0.07739186 -0.01218998 -0.7088379 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.38171655 -0.03848283]]. Action = [[-0.23223    -0.00234109  0.20269424  0.7466984 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.38428047 -0.03758626]]. Action = [[ 0.06418517  0.23273796 -0.10182093  0.82445705]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.3847024  -0.03361088]]. Action = [[-3.4344196e-04 -1.3912259e-01  4.1655093e-02  3.6405957e-01]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.38512298 -0.03348535]]. Action = [[ 0.12296593 -0.14183769  0.15623152  0.9907911 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.38479388 -0.03410225]]. Action = [[ 0.1312258  -0.14522657 -0.11420739 -0.3935334 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.38243452 -0.03644587]]. Action = [[ 0.13978869  0.20176387 -0.23206301  0.0802561 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.3785614  -0.03635781]]. Action = [[-0.01261732 -0.13146727  0.03004012  0.89326763]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.37639964 -0.03703564]]. Action = [[ 0.24297613 -0.07245995  0.05387831  0.5733578 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.36909288 -0.03856882]]. Action = [[ 0.09186679  0.0584875   0.0072459  -0.96225435]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.36201438 -0.03964388]]. Action = [[ 0.05269626 -0.06147406  0.04793885  0.6501477 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.35732684 -0.04066478]]. Action = [[ 0.16776913  0.1753462   0.19780487 -0.7740687 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.35238808 -0.04077569]]. Action = [[-0.11544378 -0.11155003 -0.19884723 -0.93683213]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.35082    -0.04124165]]. Action = [[-0.15380016 -0.1072717   0.18873262  0.66566706]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.35088703 -0.04267504]]. Action = [[ 0.22196001 -0.1567414  -0.1519425  -0.13280213]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.34811163 -0.04600183]]. Action = [[-0.04857622  0.21863955 -0.09864335 -0.7190028 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.34738111 -0.04574168]]. Action = [[-0.24321677  0.07156536 -0.22211377 -0.41592407]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.34996527 -0.04479282]]. Action = [[ 0.10301501 -0.18038344  0.07198787  0.3693285 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.35043555 -0.04604194]]. Action = [[-0.00824443 -0.20017473  0.19500616 -0.55257577]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.35033286 -0.05079274]]. Action = [[-0.11274686 -0.1919557  -0.19039808  0.2866639 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.35030544 -0.05674573]]. Action = [[-0.05851461  0.19987696  0.03985506  0.2510836 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.35078567 -0.05738968]]. Action = [[ 0.18445534  0.21453679 -0.20570357  0.85669184]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.35034662 -0.05528775]]. Action = [[-0.06687635 -0.02660716  0.00177833  0.02480006]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.35033137 -0.05456021]]. Action = [[-0.05761835  0.00340971 -0.01024476 -0.8163211 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.35078198 -0.05426846]]. Action = [[ 0.10806847  0.08081365 -0.22023278  0.7168721 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.35062665 -0.05261225]]. Action = [[ 0.02468643  0.16091365 -0.12657103 -0.95640594]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.35053962 -0.04900052]]. Action = [[-0.2116725  -0.03442934 -0.1980738  -0.9758649 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.35242793 -0.04769029]]. Action = [[-0.13208884 -0.0765204  -0.00875449 -0.43436718]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.35559824 -0.04720051]]. Action = [[ 0.06685817 -0.11692169 -0.07465598  0.4527372 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 212. State = [[-0.3559615  -0.04770675]]. Action = [[ 0.11220193 -0.23082666 -0.17850229 -0.46882546]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-0.35479158 -0.05360507]]. Action = [[ 0.17637268 -0.04370436  0.17922899 -0.09839845]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.35224906 -0.05633879]]. Action = [[ 0.1994453   0.03295803 -0.03109506 -0.44950557]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.34730366 -0.05823819]]. Action = [[-0.17262565  0.22639853  0.19696414 -0.84491307]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of 1
Current timestep = 216. State = [[-0.3474264  -0.05706138]]. Action = [[-0.1356438   0.14619869  0.11187857 -0.00731218]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.34992018 -0.05384854]]. Action = [[-0.19479641 -0.1978918  -0.11882564  0.01628721]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.35342193 -0.05348923]]. Action = [[-0.09672824  0.18474922  0.12590408 -0.29116976]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.35703212 -0.05184769]]. Action = [[-0.07612546  0.04168132 -0.11369438 -0.86011887]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 220. State = [[-0.3604242  -0.05026659]]. Action = [[ 0.03095806 -0.2293386   0.08887303  0.10866177]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.3622376  -0.05063695]]. Action = [[-0.19574353 -0.08111873  0.08535343 -0.7880626 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 221 of -1
Current timestep = 222. State = [[-0.36687753 -0.051788  ]]. Action = [[-0.12189625 -0.23304531 -0.10262141  0.7863736 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of -1
Current timestep = 223. State = [[-0.37260804 -0.05619254]]. Action = [[-0.1576595   0.21602982 -0.01528256 -0.6098259 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.37798473 -0.05593431]]. Action = [[-0.06315637  0.11935574 -0.15918387 -0.9068387 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.3828636  -0.05393057]]. Action = [[-0.1080049   0.11223271  0.21736282  0.01796901]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.38835588 -0.05095993]]. Action = [[-0.14337733 -0.12659189 -0.0419527  -0.06808567]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.39176723 -0.04952997]]. Action = [[ 0.07965815 -0.21588972  0.1802254  -0.35599244]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.39214176 -0.05129851]]. Action = [[-0.03303367 -0.07238631 -0.09062447 -0.9048399 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.39247957 -0.05258269]]. Action = [[-0.20175898  0.07809374 -0.2384597   0.11733878]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.39284906 -0.05385741]]. Action = [[ 0.16854745 -0.20041023 -0.09560287 -0.7111129 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.39152116 -0.05834299]]. Action = [[-0.01345795  0.0148136  -0.01710588 -0.18558586]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.390434   -0.06271707]]. Action = [[-0.22818102  0.1433028   0.12119919 -0.26796114]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 232 of -1
Current timestep = 233. State = [[-0.3898409  -0.06576912]]. Action = [[-0.08722556  0.20866442  0.21950239  0.5449903 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.389296   -0.06796739]]. Action = [[ 0.02614814 -0.13528134  0.09193376 -0.31530452]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.38814357 -0.07183649]]. Action = [[ 0.01831791 -0.1589092  -0.11372453 -0.827469  ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.3868603  -0.07784514]]. Action = [[-0.23146017 -0.21720855 -0.02082673 -0.32206023]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.386784   -0.08310988]]. Action = [[-0.18470077  0.16524947  0.03913307 -0.42943078]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.38776153 -0.08653171]]. Action = [[ 0.15728217  0.22507614 -0.20481586 -0.42408073]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.38623625 -0.08393177]]. Action = [[ 0.04033884  0.05813962 -0.03769304 -0.51564085]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 240. State = [[-0.38451993 -0.08013366]]. Action = [[ 0.06108168  0.13575464 -0.16058631  0.97073317]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 241. State = [[-0.38270676 -0.07642134]]. Action = [[ 0.01100728  0.00084594 -0.04197247  0.1398437 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.38126513 -0.07413348]]. Action = [[ 0.2353077   0.1282981  -0.15682076 -0.1811701 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.37841803 -0.07081354]]. Action = [[-0.0370734   0.12951005 -0.10406628 -0.590099  ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 244. State = [[-0.37706187 -0.06656129]]. Action = [[ 0.13557926  0.08520779  0.13065878 -0.6955003 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.3748916  -0.06153394]]. Action = [[-0.1731902   0.2302593  -0.20309351 -0.9399094 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.37345538 -0.05863464]]. Action = [[-0.1887141  -0.00261487 -0.05185217 -0.36481297]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.3725752  -0.05679199]]. Action = [[-0.13675481  0.1424936  -0.06342241  0.21676147]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 248. State = [[-0.37255704 -0.05361424]]. Action = [[ 0.01156968 -0.217474    0.20706373 -0.97520655]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.37270814 -0.05333791]]. Action = [[-0.16181505  0.21627721 -0.1477314  -0.49431646]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 250. State = [[-0.37574053 -0.05098701]]. Action = [[ 0.1025846  -0.18440317 -0.0846771  -0.4979514 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.37590706 -0.05106177]]. Action = [[-0.24008223 -0.10932484  0.21636283  0.8885467 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.37592033 -0.05110224]]. Action = [[-0.18970929 -0.14500973  0.14846545 -0.55875635]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.3758527  -0.05115321]]. Action = [[-0.19720544 -0.2414254   0.10764304 -0.3054967 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.37580985 -0.05116436]]. Action = [[-0.21310084 -0.06594916  0.21460652  0.6580486 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 255. State = [[-0.37568647 -0.05118479]]. Action = [[ 0.23459584  0.11935487 -0.15100494  0.59254503]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 256. State = [[-0.3734779  -0.05083331]]. Action = [[-0.22825468 -0.20072988  0.06554911 -0.5871984 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of 1
Current timestep = 257. State = [[-0.37206134 -0.05047415]]. Action = [[-0.15636484  0.20354736  0.18773091 -0.7093482 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.37256786 -0.04619399]]. Action = [[ 0.17325145 -0.14834017 -0.08927733 -0.79054576]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 0
Current timestep = 259. State = [[-0.37087885 -0.04506788]]. Action = [[ 0.02841777  0.03739947 -0.17600371  0.37797475]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of 0
Current timestep = 260. State = [[-0.36939505 -0.04370049]]. Action = [[-0.01128475  0.22245342  0.06156725  0.2565949 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.36897215 -0.03847065]]. Action = [[-0.00743996  0.06795964  0.0827626   0.02174187]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 262. State = [[-0.36826274 -0.03272137]]. Action = [[ 0.18671939 -0.17521831  0.04593435  0.6705829 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of 0
Current timestep = 263. State = [[-0.36321145 -0.03129775]]. Action = [[-0.2016205   0.08954185 -0.16984057 -0.05455464]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of 0
Current timestep = 264. State = [[-0.36297038 -0.02980204]]. Action = [[-0.2444007  -0.08405513  0.17163152 -0.85506666]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 265. State = [[-0.3664861  -0.02943694]]. Action = [[-0.03880121 -0.11457893  0.16035622  0.94731843]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.36944783 -0.02976481]]. Action = [[-0.17332798  0.16631004  0.1150364   0.71937406]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of 1
Current timestep = 267. State = [[-0.3762242  -0.02807532]]. Action = [[ 0.14486542  0.21017212 -0.21040699 -0.86165553]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.37883472 -0.02325491]]. Action = [[0.13067362 0.111862   0.08292735 0.13496733]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.3798869  -0.01640539]]. Action = [[-0.17943    -0.01620081 -0.1819497  -0.5798168 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.37992164 -0.01134657]]. Action = [[ 0.19150603 -0.13695808  0.15683359 -0.3108195 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 271. State = [[-0.37580037 -0.0112897 ]]. Action = [[-0.08184585  0.0103032   0.09544656 -0.08775163]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 272. State = [[-0.37486872 -0.01142093]]. Action = [[-0.07973877 -0.13128842 -0.03989342 -0.9730036 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.37434256 -0.01169752]]. Action = [[-0.02708539 -0.10106042  0.17727974  0.8337375 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.37400773 -0.01350645]]. Action = [[-0.06635654 -0.21266928  0.23364106 -0.5880073 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.37374032 -0.01818046]]. Action = [[-0.01631513  0.13903537  0.14057326 -0.70701516]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.37382296 -0.01847165]]. Action = [[-0.20400637  0.19780552  0.04817063 -0.77448815]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.3740029  -0.01856488]]. Action = [[ 0.122372    0.09914717  0.11805683 -0.8384349 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-0.37369552 -0.01829939]]. Action = [[ 0.20966086  0.14090312 -0.2252031   0.3201307 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.3708504  -0.01657878]]. Action = [[-0.10309984 -0.10694356 -0.15238746 -0.9763632 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 280. State = [[-0.3702452  -0.01681869]]. Action = [[ 0.10997134 -0.05432625 -0.24787396 -0.38875365]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.3689074  -0.01712829]]. Action = [[-0.11325166  0.13265917 -0.21178591  0.32380927]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.36872348 -0.01701405]]. Action = [[ 0.13133353 -0.16704525 -0.00739783  0.01488173]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.3669544  -0.01778196]]. Action = [[ 0.01269549  0.02287316 -0.1121238  -0.36977708]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 284. State = [[-0.36569092 -0.01835242]]. Action = [[-0.1878966  -0.23234852  0.12318334  0.66576195]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 285. State = [[-0.36564046 -0.02157473]]. Action = [[-0.16498983  0.19709632 -0.14857903  0.73708653]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 286. State = [[-0.36893657 -0.02070138]]. Action = [[ 0.12867135  0.09428459 -0.09803718 -0.08666605]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 287. State = [[-0.3692203  -0.01949846]]. Action = [[-0.0416764   0.2257427  -0.23493727  0.8675301 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.37014493 -0.01486939]]. Action = [[-0.03984678  0.10226494  0.23974678 -0.8339528 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 289. State = [[-0.37186095 -0.00943472]]. Action = [[ 0.04987031 -0.1694031   0.00788972  0.9235766 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.37207404 -0.00866321]]. Action = [[-0.07954472 -0.1820031  -0.06774512  0.30023193]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.37198624 -0.00968928]]. Action = [[ 0.026452    0.2472058   0.09282121 -0.3262763 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 0
Current timestep = 292. State = [[-0.37249726 -0.00775696]]. Action = [[ 0.0044626   0.22313002 -0.18972088  0.67487407]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 0
Current timestep = 293. State = [[-0.373798   -0.00192811]]. Action = [[ 0.20906675  0.09900171 -0.23612034 -0.00511456]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.37247553  0.00449633]]. Action = [[ 0.04880255  0.06285858  0.01796779 -0.15222484]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of 0
Current timestep = 295. State = [[-0.36996385  0.0102608 ]]. Action = [[-0.2305422   0.20817494 -0.20965075 -0.5276444 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.3681748   0.01381023]]. Action = [[-0.20546764 -0.04077001 -0.15818721 -0.03215134]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.36963212  0.0162992 ]]. Action = [[ 0.14914167  0.02475223 -0.20530646  0.3361299 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.36924258  0.01739823]]. Action = [[-0.0831905   0.1394029  -0.15394291 -0.7899557 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 0
Current timestep = 299. State = [[-0.37027305  0.02067667]]. Action = [[-0.00716746 -0.22982022 -0.19429593 -0.22846162]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of 0
Current timestep = 300. State = [[-0.37037155  0.02059994]]. Action = [[-0.14054424  0.15090239  0.01359612 -0.80792105]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 301. State = [[-0.3722144   0.02164429]]. Action = [[ 0.21639755 -0.21829212  0.2253679  -0.88936555]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 0
Current timestep = 302. State = [[-0.3703736   0.02001201]]. Action = [[ 0.13805473 -0.10190934 -0.23086117  0.31724513]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 0
Current timestep = 303. State = [[-0.36649466  0.0176783 ]]. Action = [[-0.14227273  0.0288465   0.21435308 -0.85777855]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.36638567  0.0164676 ]]. Action = [[-0.22248268  0.12378013 -0.03763208  0.5654104 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.36968622  0.01717905]]. Action = [[ 0.03601873 -0.16697432 -0.18545996 -0.58150333]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.37093514  0.01602014]]. Action = [[ 0.2425344   0.23606867 -0.21645746  0.54114413]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.3697314   0.01699235]]. Action = [[ 0.14768416 -0.02819161  0.02558222  0.33395815]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[-0.36474746  0.01749667]]. Action = [[-0.07987231  0.09732363 -0.10515839  0.2538488 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 309. State = [[-0.36288676  0.01912761]]. Action = [[ 0.12839112 -0.03355184  0.19721437 -0.7305679 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 310. State = [[-0.35878813  0.01983916]]. Action = [[-0.17256111  0.1938748   0.23283738  0.8647342 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 311. State = [[-0.36009684  0.02451939]]. Action = [[-0.09410331 -0.09972897  0.22452408 -0.37984538]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.36141443  0.02648725]]. Action = [[ 0.2075935   0.0954839  -0.06815688 -0.87933916]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.3597459   0.02888958]]. Action = [[-0.20976701 -0.00197256 -0.24800822 -0.62312126]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.36118075  0.03147153]]. Action = [[-0.19513796  0.15900809 -0.10971725  0.6766236 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.3653906   0.03701839]]. Action = [[ 0.20191506  0.1410695  -0.13113397 -0.03641826]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.36671415  0.0435176 ]]. Action = [[ 0.11486888  0.03973627 -0.24411334  0.28703237]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.36468917  0.04922672]]. Action = [[-0.04805475  0.04012433 -0.08018924  0.5360848 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.36519307  0.05389125]]. Action = [[-0.15846641  0.06148544 -0.19492623  0.35762763]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.3674719   0.05900401]]. Action = [[-0.06094038  0.07472277 -0.06824371 -0.5667157 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 320. State = [[-0.37057084  0.0643266 ]]. Action = [[-0.16768743  0.0116125  -0.06908664  0.25328934]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.37621295  0.0722221 ]]. Action = [[ 0.17394388  0.22228974 -0.14373277 -0.97233105]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.37733033  0.07742064]]. Action = [[ 0.19991314 -0.11643881  0.10344335  0.80200875]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.37321326  0.07942394]]. Action = [[-0.17723776 -0.10388644  0.03700855 -0.8155637 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.36972314  0.0801173 ]]. Action = [[ 0.18477422 -0.07925978 -0.10159439  0.49402976]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.36413035  0.08087252]]. Action = [[-0.0043027   0.19067395 -0.1761999   0.3862388 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 326. State = [[-0.3607728   0.08361238]]. Action = [[-0.23701778 -0.2223959  -0.18453196 -0.5689349 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.36207092  0.0832859 ]]. Action = [[-0.13470528  0.0486882   0.16859865  0.9563272 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.36536837  0.08279092]]. Action = [[ 0.09787029 -0.17208216 -0.22768898  0.58774686]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.3658218  0.0811345]]. Action = [[ 0.05235797  0.0632461  -0.03352192 -0.9429604 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.36585388  0.08051237]]. Action = [[-0.21497765 -0.2419645   0.06852543 -0.8676815 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 331. State = [[-0.36861914  0.0758662 ]]. Action = [[ 0.16142327 -0.14809269 -0.23217711 -0.732244  ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.36850372  0.06886908]]. Action = [[-0.08782411 -0.19862027 -0.14501578 -0.41310048]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 333. State = [[-0.36909443  0.06019199]]. Action = [[ 0.07358781 -0.04158951  0.0223982  -0.25488555]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of 0
Current timestep = 334. State = [[-0.36886197  0.05430811]]. Action = [[-0.07973519 -0.1245057  -0.19296613  0.8329834 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.36870682  0.04880546]]. Action = [[ 0.07111797 -0.19114186  0.18724519  0.7487775 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 336. State = [[-0.3673573  0.0413857]]. Action = [[ 0.20319796 -0.06959391 -0.13572401  0.48269844]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 0
Current timestep = 337. State = [[-0.362429    0.03540535]]. Action = [[-0.11488283  0.19788128  0.20100546 -0.2543319 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 0
Current timestep = 338. State = [[-0.36160734  0.0337667 ]]. Action = [[-0.23502897 -0.1975745   0.0923011   0.36539102]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.36375964  0.03073484]]. Action = [[-0.08966139  0.05203265 -0.12347367 -0.96256435]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 340. State = [[-0.36608684  0.02979778]]. Action = [[-0.16241956  0.18622386 -0.04931463 -0.48785484]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of 0
Current timestep = 341. State = [[-0.37015668  0.03118051]]. Action = [[ 0.09631586 -0.03774145 -0.11509846  0.85210204]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 0
Current timestep = 342. State = [[-0.37096262  0.0313245 ]]. Action = [[ 0.09806398 -0.03653309  0.01218298 -0.5219472 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 0
Current timestep = 343. State = [[-0.37089017  0.03131568]]. Action = [[-0.11242032  0.00791892  0.1882326  -0.84828407]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.37110475  0.03155069]]. Action = [[-0.11719656  0.22208148 -0.15327656 -0.2510891 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 0
Current timestep = 345. State = [[-0.37365514  0.03473977]]. Action = [[ 0.20679146  0.07630175 -0.1689797   0.20527935]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of 0
Current timestep = 346. State = [[-0.37398505  0.03808937]]. Action = [[-0.07656598  0.17835733 -0.04804644 -0.5939262 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 346 of 0
Current timestep = 347. State = [[-0.37549257  0.04345515]]. Action = [[-0.08368766  0.0089072  -0.11057255  0.17330933]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 348. State = [[-0.37736616  0.0470231 ]]. Action = [[ 0.09019873  0.08358321 -0.04866453 -0.90938616]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 349. State = [[-0.37940273  0.05143349]]. Action = [[ 0.22822088  0.08212698 -0.23145941  0.39309168]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of 0
Current timestep = 350. State = [[-0.376956    0.05543014]]. Action = [[ 0.16294014  0.18054008 -0.00755735  0.88051665]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 350 of 0
Current timestep = 351. State = [[-0.37241903  0.06124844]]. Action = [[ 0.11198282  0.24641106  0.01721677 -0.44337708]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of 0
Current timestep = 352. State = [[-0.3673046   0.06940626]]. Action = [[-0.01071984  0.11186904 -0.2398875   0.10411429]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of 0
Current timestep = 353. State = [[-0.36402082  0.07864043]]. Action = [[-0.10248879  0.00450045  0.07574779  0.92623353]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(8.7397e-05, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.36472613  0.08578154]]. Action = [[-0.2008303   0.07742167  0.13573709 -0.21798772]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(3.3060e-05, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.36785403  0.09172649]]. Action = [[-0.22715023 -0.12581809  0.02090484  0.9487946 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of 0
Current timestep = 356. State = [[-0.37477472  0.09464653]]. Action = [[0.21263409 0.22179359 0.02421758 0.00408244]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of 0
Current timestep = 357. State = [[-0.37608775  0.09952575]]. Action = [[0.17963177 0.06215006 0.17495191 0.83774054]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 0
Current timestep = 358. State = [[-0.37274924  0.10421319]]. Action = [[-0.03419253 -0.12300862 -0.02641509 -0.3400663 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.37098897  0.10517949]]. Action = [[-0.02698858  0.13560426  0.14606935 -0.19086337]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(6.7057e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 0
Current timestep = 360. State = [[-0.37127185  0.10788836]]. Action = [[-0.17100671  0.07696509 -0.13651735  0.27209032]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 361. State = [[-0.37400916  0.1126392 ]]. Action = [[-0.00525925 -0.14892603  0.03860959 -0.9160519 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.3750695  0.1135597]]. Action = [[-0.01038043 -0.23892719 -0.08243486 -0.46651018]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.37458566  0.11035644]]. Action = [[-0.05768274 -0.08092755 -0.06715582  0.4980073 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.375668    0.10617463]]. Action = [[ 0.00093693 -0.03590506 -0.10643104  0.7720587 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 365. State = [[-0.37687394  0.10227954]]. Action = [[ 0.00382739  0.05983001  0.1114755  -0.5043243 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 366. State = [[-0.3780635   0.10064483]]. Action = [[-0.04589701  0.06640613 -0.03937313  0.7353082 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.3797409   0.10049537]]. Action = [[-0.22881205  0.23283842 -0.18537268  0.29405212]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.38103643  0.09998619]]. Action = [[ 0.20727402 -0.16018587  0.14695066 -0.28027606]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.3797799   0.09753233]]. Action = [[-0.16537182  0.09584051  0.0457111   0.8469248 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(6.1111e-05, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.37922797  0.09550792]]. Action = [[ 0.05291706  0.11214632 -0.19030225  0.02822626]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 371. State = [[-0.37754604  0.0958855 ]]. Action = [[ 0.24128956  0.15393674  0.07993045 -0.785565  ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.3701867   0.09855042]]. Action = [[ 0.1483235   0.20263922 -0.12379728  0.16445088]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.36041823  0.10466529]]. Action = [[-0.10437968 -0.1978619   0.24651176 -0.2716099 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.3583917  0.1050633]]. Action = [[-0.20669632 -0.04841182 -0.20804618 -0.8113031 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.36028174  0.10458641]]. Action = [[ 0.03714344  0.21549612 -0.07183121  0.02653813]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(1.1581e-05, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.3614444   0.10649346]]. Action = [[-0.01959214 -0.150802    0.04535532  0.47630644]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(3.5311e-05, grad_fn=<MseLossBackward0>)
Current timestep = 377. State = [[-0.36165276  0.10662733]]. Action = [[-0.09357443  0.14336371 -0.09705642 -0.6835616 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.3638878   0.10862949]]. Action = [[-0.14267546 -0.23239653 -0.18556389 -0.7448272 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.36779845  0.1069246 ]]. Action = [[-0.0587007  -0.20523506  0.06809017  0.7070966 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(2.1837e-05, grad_fn=<MseLossBackward0>)
Current timestep = 380. State = [[-0.37147248  0.10182539]]. Action = [[0.17138922 0.15648115 0.20157135 0.49153304]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.3713619   0.10144344]]. Action = [[0.20774841 0.20668161 0.24142551 0.6059542 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.3674546   0.10353768]]. Action = [[-0.18654463 -0.07020423  0.09235114  0.7363572 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(3.0899e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.36794767  0.10387281]]. Action = [[ 0.06393433  0.06689367  0.19986096 -0.40103984]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 384. State = [[-0.36829102  0.10433186]]. Action = [[-0.13772152  0.03254449 -0.15952781 -0.07490146]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(8.1990e-05, grad_fn=<MseLossBackward0>)
Current timestep = 385. State = [[-0.3703445   0.10649601]]. Action = [[-0.21472056 -0.07376722 -0.03773022  0.876868  ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(1.6725e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.37550083  0.10802992]]. Action = [[ 0.19960058  0.17366582 -0.17646445  0.07366157]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.37651446  0.11066847]]. Action = [[-0.11280924 -0.02068442  0.22988665 -0.8626301 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.37861583  0.11331788]]. Action = [[-0.15645324  0.02600318  0.2121068   0.94120836]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(2.0291e-05, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.37968045  0.11500898]]. Action = [[ 0.20327121  0.10433811 -0.02196199  0.02536809]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.37748203  0.1168908 ]]. Action = [[-0.04306352 -0.02376065 -0.17987579 -0.731644  ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.376943    0.11791062]]. Action = [[ 0.06427401  0.09500888 -0.24394879 -0.8427959 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.37555522  0.12025156]]. Action = [[-0.13687328 -0.12595432  0.10708123  0.3591671 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(2.3573e-05, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.3760337   0.12052999]]. Action = [[ 0.04360026 -0.08723125  0.0246416  -0.9221486 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 394. State = [[-0.37600365  0.12011532]]. Action = [[-0.18822071 -0.06534618  0.1872854  -0.8670641 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.37610018  0.11990051]]. Action = [[-0.11499307  0.01166511 -0.13795187 -0.92084044]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.3774016   0.11967739]]. Action = [[-0.08606267 -0.05269603 -0.0710068  -0.8608456 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.38015506  0.11847779]]. Action = [[-0.20052743 -0.2169245  -0.22052647 -0.02486962]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 398. State = [[-0.38169408  0.11793526]]. Action = [[ 0.09373847  0.19624388 -0.17334418  0.5018089 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.38219926  0.1192296 ]]. Action = [[ 0.0061129  -0.15682325 -0.14421853  0.07752669]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.38218564  0.11901173]]. Action = [[-0.21777329  0.06571734  0.07556015 -0.4315526 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(9.6918e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.38204873  0.11864338]]. Action = [[ 0.1631009 -0.1711178  0.1761514 -0.7441857]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.38002735  0.11608991]]. Action = [[ 0.06426629  0.24494213  0.17700791 -0.34513485]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(8.3626e-05, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.3765736   0.11816606]]. Action = [[ 0.052874    0.23037153  0.1321804  -0.52564776]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 404. State = [[-0.37285206  0.12362997]]. Action = [[-0.17257884  0.00152299 -0.20896661  0.9738798 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(2.7748e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.37041754  0.12673162]]. Action = [[ 0.11128902  0.11954772  0.23931292 -0.7281851 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.36638528  0.13108645]]. Action = [[-0.10398811 -0.00880072  0.2034868   0.3616768 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(4.4625e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.3654803   0.13446374]]. Action = [[-0.16709998 -0.02571955 -0.18291289 -0.34202582]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, False, True]
State prediction error at timestep 407 is tensor(7.0098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.36852387  0.13730647]]. Action = [[-0.22557782  0.15921754 -0.08213937  0.8597324 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, False, True]
State prediction error at timestep 408 is tensor(4.9305e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.37449512  0.1423793 ]]. Action = [[-0.22407739  0.09158525 -0.14339587  0.26936007]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, False, True]
State prediction error at timestep 409 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.38437086  0.14956452]]. Action = [[ 0.05120352  0.16501221 -0.10644791  0.04415798]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, False, True]
State prediction error at timestep 410 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.3912224   0.15759255]]. Action = [[ 0.17700326  0.06544644 -0.12338048  0.29008436]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 412. State = [[-0.39174813  0.1645168 ]]. Action = [[ 0.2439676  -0.05081442  0.1341517  -0.55405897]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(3.9727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.38632664  0.16816373]]. Action = [[ 0.21080336 -0.19461253  0.06275296 -0.8247799 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.37831217  0.1680349 ]]. Action = [[-0.22779223 -0.02597994 -0.18644138 -0.0959512 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.37346283  0.16784623]]. Action = [[-0.1747186   0.19804257 -0.24255991 -0.6676765 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.37085682  0.16725087]]. Action = [[ 0.135939   -0.22666392 -0.08402053 -0.8382649 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.366348    0.16260792]]. Action = [[ 0.15865767  0.15636009  0.00833914 -0.9292297 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, False, True]
State prediction error at timestep 417 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.36064193  0.16359714]]. Action = [[-0.12032834 -0.22970442 -0.15329802 -0.8016429 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.35839847  0.15947403]]. Action = [[ 0.18826652  0.15101153 -0.03201047 -0.47394443]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.35406908  0.15886882]]. Action = [[-0.11848816 -0.07528839 -0.0585302  -0.52909714]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.35327417  0.15558314]]. Action = [[-0.05647418 -0.23228972  0.1343345  -0.74173254]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, False, True]
State prediction error at timestep 421 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.3521996   0.15045132]]. Action = [[ 0.19770288 -0.04713205  0.0522382   0.6567781 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.34908345  0.14466844]]. Action = [[ 0.00512803 -0.20729518 -0.19888441 -0.2766255 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, False, True]
State prediction error at timestep 423 is tensor(9.8707e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of -1
Current timestep = 424. State = [[-0.34600458  0.13625814]]. Action = [[ 0.22276431  0.07067186  0.01573852 -0.51329887]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, False, True]
State prediction error at timestep 424 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.33971593  0.13353822]]. Action = [[-0.17900567 -0.0091951  -0.06289788 -0.84700257]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.3381442   0.13202444]]. Action = [[-0.18257602  0.18562546 -0.20110433  0.8265753 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, False, True]
State prediction error at timestep 426 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.33988705  0.13398246]]. Action = [[-0.23679513 -0.02260472  0.15908068  0.05254924]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.34436116  0.1351744 ]]. Action = [[0.0688749  0.01756361 0.05998048 0.39167905]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, False, True]
State prediction error at timestep 428 is tensor(4.8031e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.34599638  0.1362151 ]]. Action = [[ 0.24006978  0.10564008 -0.13870886  0.08364344]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, False, True]
State prediction error at timestep 429 is tensor(5.6899e-05, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.34410474  0.13777086]]. Action = [[ 0.08994356 -0.06047666  0.1323018   0.22340691]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.34129283  0.13817394]]. Action = [[-0.1430612  -0.12811556  0.00380704 -0.70202506]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, False, True]
State prediction error at timestep 431 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of -1
Current timestep = 432. State = [[-0.3412556   0.13717665]]. Action = [[0.03758651 0.21329498 0.21638793 0.01407552]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.34154934  0.13873273]]. Action = [[ 0.20512056  0.23390615 -0.15696833 -0.21714973]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, False, True]
State prediction error at timestep 433 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.33695748  0.1440274 ]]. Action = [[ 0.10359454  0.16707855 -0.03982311 -0.1480006 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, False, True]
State prediction error at timestep 434 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 435. State = [[-0.3303728   0.15100834]]. Action = [[-0.21041372 -0.04509851 -0.19264653  0.7615864 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.33010417  0.15398322]]. Action = [[0.16729173 0.08300158 0.1807437  0.32029223]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of -1
Current timestep = 437. State = [[-0.3266662   0.15747313]]. Action = [[ 0.21570265  0.14755297 -0.23117334  0.1903882 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.31905895  0.16372791]]. Action = [[-0.15127261  0.23638728 -0.03225167 -0.91379803]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, False, True]
State prediction error at timestep 438 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 439. State = [[-0.31739962  0.17249218]]. Action = [[-0.21241663  0.19745779 -0.04415855  0.01414871]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, False, True]
State prediction error at timestep 439 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.32240915  0.18199153]]. Action = [[-0.17800306  0.07850179 -0.07864076  0.32903898]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.3293257   0.19227713]]. Action = [[ 0.06596068  0.22537684  0.1911748  -0.32140112]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, False, True]
State prediction error at timestep 441 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.33353245  0.20375162]]. Action = [[-0.17012054  0.11124745 -0.11266838 -0.02867919]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 443. State = [[-0.34025165  0.21489938]]. Action = [[-0.06002998 -0.03030106 -0.02266915  0.40476584]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.34493953  0.22070998]]. Action = [[ 0.05515784  0.04513454 -0.10600808 -0.32758534]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, False, True]
State prediction error at timestep 444 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.34700298  0.2245522 ]]. Action = [[-0.1910605   0.21113434  0.10503143  0.9022629 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 446. State = [[-0.35277948  0.2322975 ]]. Action = [[ 0.03657719 -0.12068653 -0.04577738 -0.47493118]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 447. State = [[-0.35539275  0.2353341 ]]. Action = [[0.04091734 0.12112322 0.14283514 0.68396485]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, False, True]
State prediction error at timestep 447 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 448. State = [[-0.35586968  0.23865755]]. Action = [[ 0.13063526  0.20397526  0.0722388  -0.16072768]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, False, True]
State prediction error at timestep 448 is tensor(6.6369e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.3519632   0.24548222]]. Action = [[0.18075758 0.08342716 0.05933625 0.6790161 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.34493598  0.2519971 ]]. Action = [[-0.09268385 -0.07221243 -0.21038273 -0.9257452 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 451. State = [[-0.34196168  0.2542766 ]]. Action = [[ 0.06313583 -0.06427276 -0.08305106 -0.28641218]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, False, True]
State prediction error at timestep 451 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 452. State = [[-0.3406306  0.2546776]]. Action = [[ 0.14136022  0.07659793 -0.19887705 -0.272623  ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.33682972  0.256798  ]]. Action = [[-0.2286401   0.19899607 -0.0491354   0.37091136]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, False, True]
State prediction error at timestep 453 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[-0.33932757  0.2617117 ]]. Action = [[-0.06039542 -0.22639182 -0.16297539  0.53453016]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.3404979  0.2624959]]. Action = [[ 0.13511664  0.05531159 -0.0954833   0.5395565 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, False, True]
State prediction error at timestep 455 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.34006962  0.2627694 ]]. Action = [[-0.04852724  0.06716865  0.05527484 -0.25823772]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, False, True]
State prediction error at timestep 456 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.34057522  0.26336193]]. Action = [[ 0.08278915  0.13286847 -0.18805502  0.8315139 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, False, True]
State prediction error at timestep 457 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.33856097  0.2663342 ]]. Action = [[-0.01520723  0.03813413 -0.13910827  0.7250453 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, False, True]
State prediction error at timestep 458 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 459. State = [[-0.33641955  0.27001294]]. Action = [[ 0.19377172  0.21216393 -0.16076232 -0.14401221]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, False, True]
State prediction error at timestep 459 is tensor(9.8340e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.33117604  0.2775414 ]]. Action = [[-0.14863051 -0.16453382  0.02647138  0.4304068 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.3311525   0.27879584]]. Action = [[-0.22488105 -0.08209457  0.01007229 -0.6412182 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, False, True]
State prediction error at timestep 461 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 462. State = [[-0.33429134  0.27920493]]. Action = [[-0.15313512 -0.23283386 -0.10515012  0.5701766 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, False, True]
State prediction error at timestep 462 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.34046137  0.2761667 ]]. Action = [[-0.10993135  0.10264128  0.19043738  0.2870841 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, False, True]
State prediction error at timestep 463 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.34614098  0.27747965]]. Action = [[ 0.18572205  0.10967475 -0.21462993 -0.396127  ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, False, True]
State prediction error at timestep 464 is tensor(3.6939e-05, grad_fn=<MseLossBackward0>)
Current timestep = 465. State = [[-0.34716976  0.27885577]]. Action = [[ 0.2357791  -0.02697398  0.19027859 -0.3034191 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.34312454  0.27938846]]. Action = [[ 0.2448057  -0.06636664 -0.08898193  0.8367584 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.33637607  0.2785057 ]]. Action = [[-0.08985534  0.10414174 -0.04707967 -0.09767663]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, False, True]
State prediction error at timestep 467 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 468. State = [[-0.33450434  0.2792063 ]]. Action = [[-0.02671559 -0.15433133  0.0328373  -0.8711498 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, False, True]
State prediction error at timestep 468 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.33296162  0.27755883]]. Action = [[ 0.1299426   0.11527559  0.21194404 -0.01531541]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.33053368  0.27815467]]. Action = [[ 0.17803109  0.11759233 -0.02614334 -0.39462233]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.32513258  0.28028965]]. Action = [[ 0.00042966 -0.1469612  -0.13003406 -0.34631574]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, False, True]
State prediction error at timestep 471 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.32060292  0.2792738 ]]. Action = [[ 0.16111621  0.02438965 -0.18247764  0.93408096]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, False, True]
State prediction error at timestep 472 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.31446505  0.27873027]]. Action = [[-0.19754119 -0.01933657 -0.07340404 -0.73165905]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, False, True]
State prediction error at timestep 473 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.31407118  0.2779983 ]]. Action = [[ 0.18830979 -0.02442068 -0.07063085  0.49870384]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, False, True]
State prediction error at timestep 474 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.31072015  0.27700225]]. Action = [[ 0.2198368   0.22282204 -0.02108628  0.9178139 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, False, True]
State prediction error at timestep 475 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 476. State = [[-0.3033608   0.28063095]]. Action = [[-0.0582761  -0.1984735  -0.22940998 -0.37975836]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.29930776  0.27975303]]. Action = [[-0.02786285 -0.04404679 -0.09009087 -0.60681206]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 478. State = [[-0.29778895  0.2775733 ]]. Action = [[ 0.03326833 -0.07098961 -0.15152533 -0.2813866 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, False, True]
State prediction error at timestep 478 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.29636234  0.27490813]]. Action = [[-0.01484492  0.00823525  0.09479374  0.1731534 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, False, True]
State prediction error at timestep 479 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.29535848  0.2729523 ]]. Action = [[-0.01211065  0.11612561 -0.12338348  0.48750293]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, False, True]
State prediction error at timestep 480 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.2957608   0.27327794]]. Action = [[-0.17130488  0.08454311 -0.09018195  0.93869925]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 1
Current timestep = 482. State = [[-0.29783246  0.2753607 ]]. Action = [[ 0.19867718  0.10486522 -0.07786968 -0.61465716]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, False, True]
State prediction error at timestep 482 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 483. State = [[-0.29541498  0.27688098]]. Action = [[ 0.23209995  0.22820336 -0.23079437 -0.89839625]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, False, True]
State prediction error at timestep 483 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 1
Current timestep = 484. State = [[-0.28798255  0.28228208]]. Action = [[-0.18080008 -0.0850133  -0.20387836  0.09779274]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, False, True]
State prediction error at timestep 484 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of 1
Current timestep = 485. State = [[-0.28472573  0.2856196 ]]. Action = [[-0.05611899 -0.003416   -0.15756804  0.9562602 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, False, True]
State prediction error at timestep 485 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of 1
Current timestep = 486. State = [[-0.28515303  0.28705925]]. Action = [[ 0.08811629 -0.05360352  0.10334519  0.4403553 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, False, True]
State prediction error at timestep 486 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of 1
Current timestep = 487. State = [[-0.2846913   0.28724524]]. Action = [[-0.21981052 -0.16052397 -0.00354801  0.9600575 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, False, True]
State prediction error at timestep 487 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of 1
Current timestep = 488. State = [[-0.28527647  0.28700897]]. Action = [[ 0.21475828 -0.11669579  0.01410416  0.6807488 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, False, True]
State prediction error at timestep 488 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 489. State = [[-0.2832648  0.2832271]]. Action = [[ 0.01597229 -0.1197879   0.11015984  0.57973933]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, False, True]
State prediction error at timestep 489 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of 1
Current timestep = 490. State = [[-0.28120184  0.27827302]]. Action = [[ 0.05800316  0.17008418 -0.14975953 -0.6912879 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, False, True]
State prediction error at timestep 490 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of 1
Current timestep = 491. State = [[-0.28044266  0.278306  ]]. Action = [[ 0.18952191  0.17133346 -0.08548494  0.8740823 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of 1
Current timestep = 492. State = [[-0.2764363   0.28085473]]. Action = [[-0.17518444 -0.20525613 -0.23101312  0.5046176 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, False, True]
State prediction error at timestep 492 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 493. State = [[-0.27605712  0.27995756]]. Action = [[-0.2059783  -0.12302521  0.06246325  0.5500207 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.27794358  0.2778088 ]]. Action = [[ 0.04330465 -0.09209129  0.0472413   0.21576369]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, False, True]
State prediction error at timestep 494 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of 1
Current timestep = 495. State = [[-0.27783704  0.27307257]]. Action = [[ 0.12980115  0.17703408  0.11381075 -0.96609086]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, False, True]
State prediction error at timestep 495 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.27767557  0.27276653]]. Action = [[-0.15155993 -0.03013361 -0.16444613  0.0344708 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.2781302   0.27292198]]. Action = [[-0.18572716 -0.15664482  0.20386046 -0.64581513]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.28229612  0.27092502]]. Action = [[0.11298206 0.18692386 0.00129357 0.2742777 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, False, True]
State prediction error at timestep 498 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 499. State = [[-0.28362405  0.27246562]]. Action = [[-0.04606618  0.21920311 -0.17662376  0.6519438 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(8.2532e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.28685138  0.27645108]]. Action = [[-0.2316737  -0.15127777 -0.03821009 -0.04992795]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, False, True]
State prediction error at timestep 500 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of 1
Current timestep = 501. State = [[-0.29228738  0.28020874]]. Action = [[-0.22841969  0.16341129  0.24409351  0.26443362]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.3013331  0.2874721]]. Action = [[-0.09765285 -0.12480742 -0.03260988  0.9645057 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, False, True]
State prediction error at timestep 502 is tensor(3.9525e-06, grad_fn=<MseLossBackward0>)
Current timestep = 503. State = [[-0.30939695  0.2906307 ]]. Action = [[ 0.00648549  0.11018524 -0.10086337 -0.08714086]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, False, True]
State prediction error at timestep 503 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of 1
Current timestep = 504. State = [[-0.31481063  0.2938171 ]]. Action = [[ 0.18680623  0.00863054 -0.07693599  0.89867127]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, False, True]
State prediction error at timestep 504 is tensor(6.1962e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of 1
Current timestep = 505. State = [[-0.31528872  0.29379433]]. Action = [[-0.05854267  0.16454     0.18703079  0.17788112]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, False, True]
State prediction error at timestep 505 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.31668946  0.29542398]]. Action = [[-0.1990131   0.19318828  0.21691024  0.8261695 ]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, False, True]
State prediction error at timestep 506 is tensor(8.8458e-06, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.32231066  0.30182272]]. Action = [[ 0.20651993  0.20632702 -0.18979186 -0.01424819]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, False, True]
State prediction error at timestep 507 is tensor(9.6427e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of 1
Current timestep = 508. State = [[-0.32198802  0.3094091 ]]. Action = [[ 0.23458841  0.0862819  -0.11013466  0.7374952 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, False, True]
State prediction error at timestep 508 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.31593508  0.3152145 ]]. Action = [[0.15765285 0.02476722 0.10154381 0.01576066]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, False, True]
State prediction error at timestep 509 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 510. State = [[-0.30908966  0.31942338]]. Action = [[0.17196432 0.19760352 0.18680438 0.17745316]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, False, True]
State prediction error at timestep 510 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.30379903  0.3227668 ]]. Action = [[ 0.2460692  -0.22065327 -0.1509833   0.16439378]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, False, True]
State prediction error at timestep 511 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of 1
Current timestep = 512. State = [[-0.29590756  0.32069495]]. Action = [[-0.22377922 -0.19592275 -0.2058008  -0.3714534 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, False, True]
State prediction error at timestep 512 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.29429632  0.31710392]]. Action = [[ 0.16483814 -0.0976125   0.07815382 -0.13208437]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, False, True]
State prediction error at timestep 513 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 514. State = [[-0.29102537  0.31081027]]. Action = [[ 0.00444132  0.12072062  0.09266454 -0.07209706]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, False, True]
State prediction error at timestep 514 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of 1
Current timestep = 515. State = [[-0.28906122  0.30609715]]. Action = [[-0.23758063 -0.16610126 -0.17611289 -0.34286094]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, False, True]
State prediction error at timestep 515 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.28936824  0.3035602 ]]. Action = [[-0.17455648  0.0464974   0.23907608  0.06382358]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, False, True]
State prediction error at timestep 516 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 517. State = [[-0.2931929   0.30259886]]. Action = [[-0.2146864  -0.24332987 -0.13125858  0.66759825]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(9.7339e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.30542478  0.29379463]]. Action = [[-0.17962995 -0.21201496  0.00788119  0.01280367]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, False, True]
State prediction error at timestep 518 is tensor(4.2367e-05, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.31375453  0.28571743]]. Action = [[ 0.2168693   0.14720926 -0.2059802  -0.19009036]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, False, True]
State prediction error at timestep 519 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of 1
Current timestep = 520. State = [[-0.31502032  0.28190953]]. Action = [[ 0.20310467  0.10221508  0.00954828 -0.34225762]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, False, True]
State prediction error at timestep 520 is tensor(8.6473e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of 1
Current timestep = 521. State = [[-0.31279427  0.2809622 ]]. Action = [[ 0.13357246  0.04746401 -0.01359327  0.226156  ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, False, True]
State prediction error at timestep 521 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 522. State = [[-0.30983928  0.28163922]]. Action = [[-0.16969937 -0.14267679  0.09771302  0.5701275 ]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, False, True]
State prediction error at timestep 522 is tensor(8.6301e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 522 of 1
Current timestep = 523. State = [[-0.30950797  0.28020975]]. Action = [[ 0.03148243 -0.17401658 -0.05581604 -0.4976653 ]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, False, True]
State prediction error at timestep 523 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 1
Current timestep = 524. State = [[-0.30789518  0.27632582]]. Action = [[0.1626901  0.13746062 0.03445143 0.9025986 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, False, True]
State prediction error at timestep 524 is tensor(2.9864e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of 1
Current timestep = 525. State = [[-0.30653656  0.27441058]]. Action = [[-0.16432403 -0.23289552  0.20154119  0.30199397]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, False, True]
State prediction error at timestep 525 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 526. State = [[-0.30548254  0.26959175]]. Action = [[ 0.20601833 -0.13726145  0.20734924 -0.5338587 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, False, True]
State prediction error at timestep 526 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 526 of 1
Current timestep = 527. State = [[-0.30220774  0.2624983 ]]. Action = [[ 0.22887099  0.17130983  0.14343107 -0.13682532]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, False, True]
State prediction error at timestep 527 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 1
Current timestep = 528. State = [[-0.29629627  0.26100564]]. Action = [[ 0.19132334 -0.07635304 -0.246517   -0.94338703]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, False, True]
State prediction error at timestep 528 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.2874408   0.25829205]]. Action = [[-0.02948627 -0.17562462  0.16989774  0.99222326]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, False, True]
State prediction error at timestep 529 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.28092247  0.25404334]]. Action = [[-0.1950886   0.0232234   0.08377579  0.5728557 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, False, True]
State prediction error at timestep 530 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of 1
Current timestep = 531. State = [[-0.2807561   0.25076872]]. Action = [[-0.15726805 -0.03847426  0.02122152  0.85882187]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, False, True]
State prediction error at timestep 531 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.28184134  0.24954242]]. Action = [[-0.10476127 -0.20624477  0.0400328   0.7341988 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, False, True]
State prediction error at timestep 532 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.28414905  0.24532644]]. Action = [[-0.08130816  0.11254475 -0.07647079 -0.77017385]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, False, True]
State prediction error at timestep 533 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of 1
Current timestep = 534. State = [[-0.2866885   0.24383374]]. Action = [[ 0.19078732 -0.22946693 -0.2360774   0.7524158 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, False, True]
State prediction error at timestep 534 is tensor(6.8676e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.28515658  0.23804773]]. Action = [[ 0.09182727 -0.0873923   0.24649805 -0.80125654]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, False, True]
State prediction error at timestep 535 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.28300536  0.2316873 ]]. Action = [[ 0.00500345 -0.1161873  -0.19243266 -0.29973847]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, False, True]
State prediction error at timestep 536 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 1
Current timestep = 537. State = [[-0.28125975  0.22520386]]. Action = [[ 0.20142949 -0.22771558 -0.13051042 -0.05437386]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, False, True]
State prediction error at timestep 537 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.2775392   0.21549287]]. Action = [[ 0.24164808 -0.17861652  0.12945965 -0.7618789 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, False, True]
State prediction error at timestep 538 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 539. State = [[-0.27041224  0.20572042]]. Action = [[ 0.19629231  0.20921618  0.09775034 -0.63675356]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, False, True]
State prediction error at timestep 539 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 1
Current timestep = 540. State = [[-0.26186866  0.20325139]]. Action = [[ 0.2163645   0.24304038  0.00118896 -0.6119452 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, False, True]
State prediction error at timestep 540 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 541. State = [[-0.2497577   0.20789672]]. Action = [[-0.23228079  0.03841767 -0.00337584 -0.46940494]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, False, True]
State prediction error at timestep 541 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.2454462   0.21214017]]. Action = [[-0.06510152  0.02504262  0.07683644  0.8914424 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, False, True]
State prediction error at timestep 542 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[-0.24577947  0.21287668]]. Action = [[-0.24490441  0.08353025  0.17370796  0.00448275]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, False, True]
State prediction error at timestep 543 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of 1
Current timestep = 544. State = [[-0.24811567  0.21557572]]. Action = [[ 0.07244411 -0.0297672  -0.03654957  0.7012007 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, False, True]
State prediction error at timestep 544 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 545. State = [[-0.24939963  0.2169572 ]]. Action = [[-0.05830406 -0.22072938 -0.09760454  0.86049986]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, False, True]
State prediction error at timestep 545 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of 1
Current timestep = 546. State = [[-0.24994618  0.21427378]]. Action = [[-0.23938386 -0.09807935  0.08332378  0.4331516 ]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, False, True]
State prediction error at timestep 546 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 546 of 1
Current timestep = 547. State = [[-0.25510085  0.20901713]]. Action = [[ 0.15878195 -0.20485774 -0.23083201 -0.50914365]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, False, True]
State prediction error at timestep 547 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.25507575  0.20187417]]. Action = [[ 0.05705974 -0.18321542  0.14532706 -0.8185844 ]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, False, True]
State prediction error at timestep 548 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[-0.25395045  0.19369917]]. Action = [[-0.08541048 -0.02257101  0.02789795  0.5227432 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, False, True]
State prediction error at timestep 549 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[-0.25476897  0.187963  ]]. Action = [[-0.17946261 -0.02434583 -0.15246902 -0.62962604]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, False, True]
State prediction error at timestep 550 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 551. State = [[-0.25832814  0.18374792]]. Action = [[-0.06149668 -0.24137945  0.13271582  0.45357716]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, False, True]
State prediction error at timestep 551 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.26748157  0.18071793]]. Action = [[-0.14404531 -0.07010645  0.17673856  0.72869563]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, False, True]
State prediction error at timestep 552 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.27686024  0.17738093]]. Action = [[-0.22732699  0.09158018 -0.23541187  0.43649578]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, False, True]
State prediction error at timestep 553 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 554. State = [[-0.2865966   0.17783314]]. Action = [[ 0.09744769 -0.22674581  0.0417105   0.29786885]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, False, True]
State prediction error at timestep 554 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.2895142   0.17238194]]. Action = [[ 0.02091894  0.22805533 -0.05463234 -0.23358494]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, False, True]
State prediction error at timestep 555 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 1
Current timestep = 556. State = [[-0.29452264  0.17473747]]. Action = [[-0.2062929   0.09348625  0.21635562 -0.851799  ]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, False, True]
State prediction error at timestep 556 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of 1
Current timestep = 557. State = [[-0.30053043  0.17939816]]. Action = [[ 0.01653719 -0.14396858 -0.05137233  0.55099726]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, False, True]
State prediction error at timestep 557 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.30269665  0.17843415]]. Action = [[-0.08400759 -0.03568606 -0.15850855  0.52338386]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, False, True]
State prediction error at timestep 558 is tensor(7.7348e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.3045689  0.1767475]]. Action = [[ 0.10643286 -0.07898426  0.16164458 -0.8629258 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, False, True]
State prediction error at timestep 559 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of -1
Current timestep = 560. State = [[-0.3041398   0.17271246]]. Action = [[ 0.18552262 -0.11186051 -0.14505544  0.81847227]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, False, True]
State prediction error at timestep 560 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.30193552  0.16702983]]. Action = [[-0.14076656  0.07288328 -0.16188216  0.14452446]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, False, True]
State prediction error at timestep 561 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 562. State = [[-0.30205512  0.16632181]]. Action = [[ 0.15501216 -0.09921902 -0.00861825  0.1876769 ]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, False, True]
State prediction error at timestep 562 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 562 of -1
Current timestep = 563. State = [[-0.30024803  0.16348527]]. Action = [[0.04795578 0.02580839 0.12250692 0.01800835]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, False, True]
State prediction error at timestep 563 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.29918954  0.16159481]]. Action = [[ 0.14340276  0.03272992 -0.15807982 -0.8390704 ]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, False, True]
State prediction error at timestep 564 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 565. State = [[-0.29697543  0.16021386]]. Action = [[ 0.09310606 -0.23309791 -0.0555858  -0.34731835]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, False, True]
State prediction error at timestep 565 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.29179776  0.1533217 ]]. Action = [[ 0.23945332  0.20093772  0.0223549  -0.06867594]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, False, True]
State prediction error at timestep 566 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of -1
Current timestep = 567. State = [[-0.2837691   0.15456697]]. Action = [[0.19869655 0.2486009  0.06880894 0.9001112 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, False, True]
State prediction error at timestep 567 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.27403465  0.16159762]]. Action = [[ 0.20995396  0.06042567 -0.22864854 -0.57957906]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, False, True]
State prediction error at timestep 568 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 568 of -1
Current timestep = 569. State = [[-0.2614535  0.1698695]]. Action = [[-0.1886948   0.22803855  0.15737307  0.3354323 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, False, True]
State prediction error at timestep 569 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.2571086   0.17918783]]. Action = [[-0.24530354 -0.17437893  0.01135221  0.9587002 ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, False, True]
State prediction error at timestep 570 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of -1
Current timestep = 571. State = [[-0.2585918   0.18036832]]. Action = [[ 0.02934799 -0.14367175 -0.08407137 -0.75547826]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, False, True]
State prediction error at timestep 571 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.2581096   0.17833833]]. Action = [[ 0.17963922  0.13750508 -0.2150806  -0.93184406]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, False, True]
State prediction error at timestep 572 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 573. State = [[-0.2574835  0.1785678]]. Action = [[-0.24819133 -0.12060118 -0.15251127 -0.47044408]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, False, True]
State prediction error at timestep 573 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.25818282  0.17812371]]. Action = [[-0.11491156  0.12513444  0.01457173 -0.7897704 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, False, True]
State prediction error at timestep 574 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.26131576  0.18173838]]. Action = [[ 0.01125497  0.21149832  0.03544641 -0.07302022]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, False, True]
State prediction error at timestep 575 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of -1
Current timestep = 576. State = [[-0.26609397  0.1881392 ]]. Action = [[-0.0039859   0.1277867   0.19119716  0.86053467]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, False, True]
State prediction error at timestep 576 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.27099746  0.19513257]]. Action = [[-0.09099203  0.09368813 -0.07631242  0.33928418]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, False, True]
State prediction error at timestep 577 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 578. State = [[-0.27644035  0.20199747]]. Action = [[-0.22997698 -0.08980879 -0.19754453  0.6158166 ]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, False, True]
State prediction error at timestep 578 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 579. State = [[-0.28168654  0.20663711]]. Action = [[-0.11124861  0.14794481  0.12432355 -0.5955333 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, False, True]
State prediction error at timestep 579 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 579 of -1
Current timestep = 580. State = [[-0.2877763   0.21289209]]. Action = [[ 0.14153954 -0.1789926   0.20365396  0.12225831]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, False, True]
State prediction error at timestep 580 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of -1
Current timestep = 581. State = [[-0.28868768  0.21185467]]. Action = [[-0.04777357  0.20262167 -0.12991133 -0.97426975]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, False, True]
State prediction error at timestep 581 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 582. State = [[-0.29206944  0.21584225]]. Action = [[-0.21360919  0.16157442  0.03765914 -0.54071224]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, False, True]
State prediction error at timestep 582 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 583. State = [[-0.29909256  0.22345845]]. Action = [[0.07006133 0.13912177 0.21541864 0.112939  ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.30466393  0.23016146]]. Action = [[ 0.04430693 -0.00890787  0.11578241  0.78100514]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, False, True]
State prediction error at timestep 584 is tensor(1.4706e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[-0.30681533  0.23337023]]. Action = [[-0.23062253  0.10977352 -0.18577477  0.82482696]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(5.8647e-05, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.31293285  0.23950407]]. Action = [[ 0.17027336 -0.20888859  0.1824337  -0.8320159 ]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, False, True]
State prediction error at timestep 586 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 587. State = [[-0.3114581   0.23834594]]. Action = [[ 0.15336597 -0.16692552 -0.22782974 -0.26408106]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, False, True]
State prediction error at timestep 587 is tensor(1.0613e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[-0.3065079   0.23178531]]. Action = [[-0.01706369 -0.2253578   0.13672721  0.85266626]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, False, True]
State prediction error at timestep 588 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.30328277  0.22333483]]. Action = [[-0.05807282  0.2221801   0.04542467 -0.31578845]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.30385786  0.22394727]]. Action = [[-0.21057124  0.17552328 -0.21648607 -0.73235404]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, False, True]
State prediction error at timestep 590 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of -1
Current timestep = 591. State = [[-0.3086746   0.22858804]]. Action = [[ 0.08804265 -0.02694622  0.15279251 -0.78406477]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.31016096  0.22984147]]. Action = [[ 0.0980652  -0.22485013  0.12782472 -0.8510222 ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.30808347  0.22629943]]. Action = [[-0.0784421  -0.05968185 -0.1347207   0.11785007]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, False, True]
State prediction error at timestep 593 is tensor(4.2182e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.30800578  0.22271368]]. Action = [[1.16740525e-01 1.64061069e-01 6.78449869e-04 9.16586637e-01]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, False, True]
State prediction error at timestep 594 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 594 of -1
Current timestep = 595. State = [[-0.30766466  0.22287245]]. Action = [[-0.10666932 -0.22574098 -0.15158103 -0.7416037 ]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, False, True]
State prediction error at timestep 595 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.30797213  0.2185849 ]]. Action = [[-0.18362425 -0.02461745 -0.18646656  0.8362714 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, False, True]
State prediction error at timestep 596 is tensor(7.0453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 597. State = [[-0.3116819   0.21460636]]. Action = [[-0.18151474 -0.22522247 -0.24231209 -0.1503287 ]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, False, True]
State prediction error at timestep 597 is tensor(1.7650e-05, grad_fn=<MseLossBackward0>)
Current timestep = 598. State = [[-0.31790072  0.20705134]]. Action = [[-0.21710661  0.2203334  -0.14274219 -0.26284003]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, False, True]
State prediction error at timestep 598 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.32793435  0.20908535]]. Action = [[-0.01232731  0.01515472  0.04841304 -0.51222605]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, False, True]
State prediction error at timestep 599 is tensor(5.5510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.33537704  0.20982094]]. Action = [[ 0.0684433  -0.13699035 -0.14882083 -0.8947855 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, False, True]
State prediction error at timestep 600 is tensor(5.8918e-05, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.33802944  0.2062452 ]]. Action = [[-0.22139226  0.06707922 -0.05008754 -0.738513  ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, False, True]
State prediction error at timestep 601 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 602. State = [[-0.34530303  0.20575462]]. Action = [[-0.24679917 -0.24263851 -0.01400711 -0.7152723 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, False, True]
State prediction error at timestep 602 is tensor(4.3610e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.35509723  0.20000216]]. Action = [[ 0.11639047  0.19530699 -0.06459987  0.22638261]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, False, True]
State prediction error at timestep 603 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.36032018  0.20097488]]. Action = [[ 0.16134071  0.16279066  0.17428386 -0.24555922]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, False, True]
State prediction error at timestep 604 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 605. State = [[-0.36095417  0.20355356]]. Action = [[ 0.06904128 -0.07933947 -0.09527932  0.8067527 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, False, True]
State prediction error at timestep 605 is tensor(9.9496e-05, grad_fn=<MseLossBackward0>)
Current timestep = 606. State = [[-0.36022153  0.20410688]]. Action = [[-0.04338419  0.18354607 -0.03895476  0.46788597]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, False, True]
State prediction error at timestep 606 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.360515    0.20735304]]. Action = [[ 0.11827999 -0.23467432  0.17037454 -0.00223196]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, False, True]
State prediction error at timestep 607 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.35666507  0.20538008]]. Action = [[ 0.08213866 -0.17156887  0.07324234 -0.24197978]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, False, True]
State prediction error at timestep 608 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.35277668  0.20008211]]. Action = [[-0.07514237  0.19519547  0.12927651  0.9433211 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, False, True]
State prediction error at timestep 609 is tensor(6.5719e-05, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.35326993  0.20079546]]. Action = [[-0.09052077  0.02574033 -0.07058915 -0.8932851 ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, False, True]
State prediction error at timestep 610 is tensor(4.4643e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.35479125  0.2022659 ]]. Action = [[-0.09907135  0.11981213  0.12196368 -0.5614201 ]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, False, True]
State prediction error at timestep 611 is tensor(1.1495e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.35853943  0.20588234]]. Action = [[ 0.08728448 -0.10820842  0.11119685  0.8900635 ]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, False, True]
State prediction error at timestep 612 is tensor(7.1032e-05, grad_fn=<MseLossBackward0>)
Current timestep = 613. State = [[-0.35820776  0.20586728]]. Action = [[0.16273743 0.21012536 0.1280672  0.12057245]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, False, True]
State prediction error at timestep 613 is tensor(8.6891e-05, grad_fn=<MseLossBackward0>)
Current timestep = 614. State = [[-0.3554669  0.2091716]]. Action = [[ 0.24700224 -0.12459657  0.06608632 -0.90053356]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, False, True]
State prediction error at timestep 614 is tensor(2.3719e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.3483104   0.20865595]]. Action = [[-0.08211574 -0.21723317  0.18046108 -0.82941425]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, False, True]
State prediction error at timestep 615 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.34358698  0.20373504]]. Action = [[ 0.18002135 -0.22734943  0.10616744  0.83099234]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, False, True]
State prediction error at timestep 616 is tensor(2.7560e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of -1
Current timestep = 617. State = [[-0.33568296  0.19479299]]. Action = [[0.1339306  0.21312058 0.15569746 0.44607663]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, False, True]
State prediction error at timestep 617 is tensor(1.8050e-05, grad_fn=<MseLossBackward0>)
Current timestep = 618. State = [[-0.32928732  0.1931781 ]]. Action = [[ 0.00253597 -0.2140452   0.01170409  0.28189623]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, False, True]
State prediction error at timestep 618 is tensor(3.5055e-05, grad_fn=<MseLossBackward0>)
Current timestep = 619. State = [[-0.32306105  0.18544857]]. Action = [[-0.17497233 -0.10408269 -0.12294078  0.93727803]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, False, True]
State prediction error at timestep 619 is tensor(6.9315e-05, grad_fn=<MseLossBackward0>)
Current timestep = 620. State = [[-0.3226165   0.18232018]]. Action = [[-0.04357648  0.00778404 -0.238441    0.4185381 ]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, False, True]
State prediction error at timestep 620 is tensor(1.8440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 621. State = [[-0.32327065  0.18025574]]. Action = [[-0.06189416 -0.14340958 -0.13254069 -0.5591755 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, False, True]
State prediction error at timestep 621 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.3243708   0.17477992]]. Action = [[-0.05739915 -0.19060978  0.22087342  0.19580781]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(5.9538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.32788187  0.16511743]]. Action = [[-0.11821039 -0.00499873  0.19050759  0.13368535]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, False, True]
State prediction error at timestep 623 is tensor(6.4613e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of 1
Current timestep = 624. State = [[-0.33167988  0.15791918]]. Action = [[ 0.23529118 -0.22412415 -0.10147664  0.35742986]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, False, True]
State prediction error at timestep 624 is tensor(4.2700e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.3299182  0.1485358]]. Action = [[0.08220634 0.21755591 0.04326123 0.7913966 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, False, True]
State prediction error at timestep 625 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 626. State = [[-0.3278963   0.14797471]]. Action = [[ 0.04072744  0.06577644  0.14576623 -0.1409629 ]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, False, True]
State prediction error at timestep 626 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.32708776  0.14863609]]. Action = [[-2.1731637e-01  3.6716461e-05  4.4957995e-03  8.5631251e-01]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.3292245   0.14984114]]. Action = [[-0.20785278 -0.13114662 -0.11169522  0.33350956]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, False, True]
State prediction error at timestep 628 is tensor(1.9515e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.33438277  0.14856853]]. Action = [[ 0.03409842  0.15350574 -0.02032159 -0.22777414]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, False, True]
State prediction error at timestep 629 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of 1
Current timestep = 630. State = [[-0.33632115  0.15072823]]. Action = [[-0.08969733 -0.22016597 -0.20884836  0.9759345 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, False, True]
State prediction error at timestep 630 is tensor(4.7233e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.33935952  0.14678812]]. Action = [[-0.09732205 -0.06618214  0.14621866  0.88477576]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, False, True]
State prediction error at timestep 631 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of 0
Current timestep = 632. State = [[-0.34461898  0.14226273]]. Action = [[-0.1972907   0.01683366 -0.12084728  0.46912384]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, False, True]
State prediction error at timestep 632 is tensor(5.5492e-05, grad_fn=<MseLossBackward0>)
Current timestep = 633. State = [[-0.3532203   0.13950385]]. Action = [[-0.02823368 -0.04201792 -0.10754827  0.46373677]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, False, True]
State prediction error at timestep 633 is tensor(2.0779e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of 0
Current timestep = 634. State = [[-0.35965586  0.1382721 ]]. Action = [[-0.14788401  0.04050627  0.09457317 -0.10584378]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(2.9007e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.36729914  0.14330573]]. Action = [[ 0.21909297  0.20870161  0.09899297 -0.80646324]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, False, True]
State prediction error at timestep 635 is tensor(9.3859e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.37230843  0.15226895]]. Action = [[ 0.10529763 -0.17196928 -0.22924504 -0.8209034 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, False, True]
State prediction error at timestep 636 is tensor(5.4081e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of -1
Current timestep = 637. State = [[-0.37391648  0.15377334]]. Action = [[-0.15478681  0.06053683 -0.01136129  0.8288232 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, False, True]
State prediction error at timestep 637 is tensor(5.9944e-05, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.37715703  0.15726388]]. Action = [[0.15274388 0.161163   0.02074188 0.75773835]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, False, True]
State prediction error at timestep 638 is tensor(1.3833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.37661687  0.1592284 ]]. Action = [[ 0.09453666  0.21408162  0.07656577 -0.923971  ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, False, True]
State prediction error at timestep 639 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.37247804  0.16110213]]. Action = [[-0.19343556 -0.20890033  0.17536175  0.555261  ]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, False, True]
State prediction error at timestep 640 is tensor(1.7645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[-0.36973786  0.16178843]]. Action = [[-0.19972457  0.14850932 -0.18844402 -0.08969587]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, False, True]
State prediction error at timestep 641 is tensor(4.7746e-05, grad_fn=<MseLossBackward0>)
Current timestep = 642. State = [[-0.37166566  0.166263  ]]. Action = [[ 0.22246653  0.23351061  0.23401177 -0.8973299 ]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, False, True]
State prediction error at timestep 642 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 643. State = [[-0.36792743  0.1745838 ]]. Action = [[ 0.18184525 -0.14370061 -0.11653915  0.8958566 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, False, True]
State prediction error at timestep 643 is tensor(4.3525e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of -1
Current timestep = 644. State = [[-0.3623935   0.17677498]]. Action = [[ 0.12098819 -0.00862998 -0.10736579  0.9623866 ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, False, True]
State prediction error at timestep 644 is tensor(1.4492e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[-0.355703    0.17818995]]. Action = [[-0.15348886 -0.11312455 -0.08579569  0.90989935]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, False, True]
State prediction error at timestep 645 is tensor(1.5767e-05, grad_fn=<MseLossBackward0>)
Current timestep = 646. State = [[-0.354175   0.1770126]]. Action = [[-0.23310895  0.04148385  0.17544663  0.9052427 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, False, True]
State prediction error at timestep 646 is tensor(8.5042e-05, grad_fn=<MseLossBackward0>)
Current timestep = 647. State = [[-0.35592297  0.17835546]]. Action = [[-0.11888587  0.04253879 -0.12618037 -0.23667479]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, False, True]
State prediction error at timestep 647 is tensor(3.7279e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.35878056  0.18108042]]. Action = [[-0.24502696  0.12037212  0.17368409  0.19607568]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, False, True]
State prediction error at timestep 648 is tensor(6.0695e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of -1
Current timestep = 649. State = [[-0.36701867  0.1873679 ]]. Action = [[-0.0481904   0.23344553  0.16568267 -0.9086268 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, False, True]
State prediction error at timestep 649 is tensor(7.5392e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.37495172  0.19489974]]. Action = [[ 0.08513916 -0.18833849  0.06150997 -0.182621  ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.37740698  0.19617762]]. Action = [[ 0.10217315  0.11521891  0.2240684  -0.84058374]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.37799335  0.19691777]]. Action = [[-0.23435001  0.0547975   0.04102695  0.8716867 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, False, True]
State prediction error at timestep 652 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 652 of -1
Current timestep = 653. State = [[-0.3779089   0.19700468]]. Action = [[ 0.18464524 -0.24436542  0.05449909  0.9820709 ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, False, True]
State prediction error at timestep 653 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.37305397  0.19310626]]. Action = [[ 0.03264967  0.15322912 -0.18413737  0.6974386 ]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, False, True]
State prediction error at timestep 654 is tensor(4.9968e-05, grad_fn=<MseLossBackward0>)
Current timestep = 655. State = [[-0.3714047   0.19384138]]. Action = [[0.14042729 0.1591388  0.0900532  0.1876725 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, False, True]
State prediction error at timestep 655 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of -1
Current timestep = 656. State = [[-0.36751333  0.19701551]]. Action = [[-0.1933255  -0.18340035 -0.15962693  0.13829386]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, False, False, True]
State prediction error at timestep 656 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.36748952  0.19639449]]. Action = [[ 0.0412333  -0.16380447 -0.15810937  0.03279781]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, False, True]
State prediction error at timestep 657 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 657 of -1
Current timestep = 658. State = [[-0.36678207  0.19254996]]. Action = [[ 0.08901691  0.10936606  0.15309799 -0.345379  ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, False, True]
State prediction error at timestep 658 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.36556804  0.1914391 ]]. Action = [[-0.2160853  -0.18369369 -0.05600117 -0.21247429]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, False, True]
State prediction error at timestep 659 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.3667501  0.1882578]]. Action = [[ 0.12480909 -0.19206621  0.20912766  0.46799588]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, False, True]
State prediction error at timestep 660 is tensor(1.9448e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of -1
Current timestep = 661. State = [[-0.36755866  0.18296784]]. Action = [[-0.18985155  0.15900439 -0.15767606  0.5577178 ]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, False, True]
State prediction error at timestep 661 is tensor(6.7337e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of -1
Current timestep = 662. State = [[-0.37171036  0.18496655]]. Action = [[ 0.23686051 -0.16952822  0.02848154  0.6659987 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, False, True]
State prediction error at timestep 662 is tensor(2.7555e-05, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.37294355  0.18516716]]. Action = [[-0.02514382  0.09628597 -0.06830084  0.8048277 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, False, True]
State prediction error at timestep 663 is tensor(5.3004e-05, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.3732503   0.18539223]]. Action = [[-0.00485204 -0.05550584  0.14685118 -0.8450349 ]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, False, True]
State prediction error at timestep 664 is tensor(2.2530e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of -1
Current timestep = 665. State = [[-0.3730742   0.18529034]]. Action = [[ 0.22762686  0.04296023 -0.16792108  0.9337406 ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, False, True]
State prediction error at timestep 665 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of -1
Current timestep = 666. State = [[-0.3705195   0.18576773]]. Action = [[-0.17978291 -0.20576604 -0.11702573  0.25836146]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, False, True]
State prediction error at timestep 666 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 667. State = [[-0.37098262  0.18018661]]. Action = [[0.13304463 0.09398341 0.06269208 0.38698077]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, False, True]
State prediction error at timestep 667 is tensor(2.1586e-05, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.37039122  0.18056105]]. Action = [[ 0.16663915 -0.18924141  0.06851816 -0.26727724]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, False, False, True]
State prediction error at timestep 668 is tensor(8.6077e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 668 of -1
Current timestep = 669. State = [[-0.36864117  0.17989777]]. Action = [[ 0.2127412  -0.01380038 -0.0333526  -0.900902  ]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, False, False, True]
State prediction error at timestep 669 is tensor(9.0947e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of -1
Current timestep = 670. State = [[-0.36280555  0.17641005]]. Action = [[0.10754851 0.03357649 0.00729445 0.19544625]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, False, True]
State prediction error at timestep 670 is tensor(1.9411e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of -1
Current timestep = 671. State = [[-0.35594183  0.17422037]]. Action = [[ 0.04318017  0.22789937 -0.22790569  0.39960408]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, False, True]
State prediction error at timestep 671 is tensor(3.5347e-05, grad_fn=<MseLossBackward0>)
Current timestep = 672. State = [[-0.347486    0.17354985]]. Action = [[ 0.13496196 -0.17100458  0.15046942 -0.02802914]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, False, False, True]
State prediction error at timestep 672 is tensor(1.3839e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 672 of -1
Current timestep = 673. State = [[-0.33829248  0.16906318]]. Action = [[-0.1156856  -0.18589994  0.11668724 -0.9193336 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, False, False, True]
State prediction error at timestep 673 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of -1
Current timestep = 674. State = [[-0.33378676  0.1622846 ]]. Action = [[ 0.09583104 -0.11503373  0.15032834 -0.529347  ]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, False, False, True]
State prediction error at timestep 674 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 674 of 1
Current timestep = 675. State = [[-0.3303403   0.15729855]]. Action = [[ 0.08162591  0.16750118 -0.06551562  0.4286635 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, False, False, True]
State prediction error at timestep 675 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 675 of 1
Current timestep = 676. State = [[-0.3248614   0.15394196]]. Action = [[ 0.14399868 -0.16462003 -0.22592233  0.4721943 ]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, False, False, True]
State prediction error at timestep 676 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 677. State = [[-0.31848234  0.14923565]]. Action = [[ 0.22976494 -0.03504305 -0.14623939 -0.17228979]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, False, False, True]
State prediction error at timestep 677 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 678. State = [[-0.308638    0.14520077]]. Action = [[ 0.15133852 -0.13289653 -0.17340118  0.26686788]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, False, False, True]
State prediction error at timestep 678 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.29894435  0.13895683]]. Action = [[-0.01268883 -0.05207938 -0.10874298 -0.8989241 ]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, False, False, True]
State prediction error at timestep 679 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.28935528  0.12971659]]. Action = [[-0.00790201  0.19998786 -0.22389188  0.15541553]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, False, False, True]
State prediction error at timestep 680 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 680 of 1
Current timestep = 681. State = [[-0.2795324   0.12721157]]. Action = [[-0.04508312 -0.22014548 -0.17419313 -0.8990734 ]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[-0.27556115  0.12015239]]. Action = [[-0.07796478 -0.13827622  0.12295777 -0.96527326]]. Reward = [0.]
Curr episode timestep = 682
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.27285329  0.11413571]]. Action = [[ 0.05194396 -0.18103924 -0.06425929  0.5482912 ]]. Reward = [0.]
Curr episode timestep = 683
Human Feedback received at timestep 683 of 1
Current timestep = 684. State = [[-0.27003506  0.10614868]]. Action = [[ 0.19943196  0.19091654 -0.10438374  0.51884294]]. Reward = [0.]
Curr episode timestep = 684
Current timestep = 685. State = [[-0.26596943  0.10462499]]. Action = [[-0.06623745  0.07083929 -0.22710426 -0.40057182]]. Reward = [0.]
Curr episode timestep = 685
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.2648891   0.10537998]]. Action = [[-0.18741477  0.10533851 -0.16883159 -0.48518848]]. Reward = [0.]
Curr episode timestep = 686
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.2674677   0.10894314]]. Action = [[-0.12943482  0.15287769 -0.2226891  -0.7384726 ]]. Reward = [0.]
Curr episode timestep = 687
Current timestep = 688. State = [[-0.2716655   0.11500572]]. Action = [[-0.04552327  0.22664252  0.19292927  0.13976932]]. Reward = [0.]
Curr episode timestep = 688
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[-0.27598545  0.12348109]]. Action = [[-0.14567304 -0.09722686  0.19086194 -0.9528819 ]]. Reward = [0.]
Curr episode timestep = 689
Current timestep = 690. State = [[-0.2808334   0.12730974]]. Action = [[ 0.0742054  -0.22763658  0.02713659  0.50182605]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, False, False, True]
State prediction error at timestep 690 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 691. State = [[-0.28258935  0.12509996]]. Action = [[ 0.09010026  0.05872536 -0.08775322  0.21981585]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, False, False, True]
State prediction error at timestep 691 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.28215456  0.12445886]]. Action = [[ 0.04453152  0.0027847   0.0771755  -0.6462777 ]]. Reward = [0.]
Curr episode timestep = 692
Current timestep = 693. State = [[-0.28201103  0.12410644]]. Action = [[-0.21543984 -0.21246551  0.1789307  -0.8315202 ]]. Reward = [0.]
Curr episode timestep = 693
Current timestep = 694. State = [[-0.28345302  0.11977882]]. Action = [[-0.18264146  0.00571531 -0.17265233  0.47119224]]. Reward = [0.]
Curr episode timestep = 694
Current timestep = 695. State = [[-0.28830767  0.11677565]]. Action = [[ 0.17755887  0.18769836 -0.22230005  0.23177719]]. Reward = [0.]
Curr episode timestep = 695
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.2897754   0.11857987]]. Action = [[-0.23389588  0.191086   -0.00429791 -0.35563147]]. Reward = [0.]
Curr episode timestep = 696
Current timestep = 697. State = [[-0.29487622  0.12505099]]. Action = [[0.10435969 0.14155358 0.14481008 0.32346094]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, False, False, True]
State prediction error at timestep 697 is tensor(7.6310e-05, grad_fn=<MseLossBackward0>)
Current timestep = 698. State = [[-0.298993    0.13145037]]. Action = [[-0.22945666 -0.2369817   0.20522672 -0.02633613]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, False, False, True]
State prediction error at timestep 698 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 699. State = [[-0.30472398  0.1313598 ]]. Action = [[ 0.18440798  0.03083989 -0.1632613   0.478518  ]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, False, False, True]
State prediction error at timestep 699 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 700. State = [[-0.30488256  0.13117598]]. Action = [[ 0.04838452 -0.08901939  0.05953723  0.15464878]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, False, False, True]
State prediction error at timestep 700 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of -1
Current timestep = 701. State = [[-0.3032794   0.12878424]]. Action = [[ 0.00257519 -0.23876476 -0.18262625  0.34713006]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, False, False, True]
State prediction error at timestep 701 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 701 of -1
Current timestep = 702. State = [[-0.3025073   0.12121978]]. Action = [[ 0.2343995   0.22704566 -0.06077127 -0.15944004]]. Reward = [0.]
Curr episode timestep = 702
Current timestep = 703. State = [[-0.29988465  0.12187899]]. Action = [[ 0.22004613 -0.01314023  0.13733369 -0.7596059 ]]. Reward = [0.]
Curr episode timestep = 703
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-0.29425877  0.12246698]]. Action = [[-0.23009844  0.18296635  0.13961416 -0.11537629]]. Reward = [0.]
Curr episode timestep = 704
Human Feedback received at timestep 704 of -1
Current timestep = 705. State = [[-0.2927618  0.1265212]]. Action = [[-0.01642559  0.00200072  0.22043002 -0.38780892]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, False, False, True]
State prediction error at timestep 705 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 706. State = [[-0.2926603   0.12918651]]. Action = [[0.12344909 0.09989634 0.05676767 0.5502229 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, False, False, True]
State prediction error at timestep 706 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[-0.2897443  0.1341906]]. Action = [[ 0.00784418  0.02603099  0.13384926 -0.34055686]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, False, False, True]
State prediction error at timestep 707 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 708. State = [[-0.2866943   0.13736121]]. Action = [[ 0.18820408 -0.12118231 -0.09205766  0.9208485 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, False, False, True]
State prediction error at timestep 708 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[-0.2824091  0.1370381]]. Action = [[-0.21294466  0.14707822  0.02930859 -0.24370718]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, False, False, True]
State prediction error at timestep 709 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.2835088   0.14033401]]. Action = [[ 0.17808384  0.02566117 -0.12642975 -0.59166044]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, False, False, True]
State prediction error at timestep 710 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of -1
Current timestep = 711. State = [[-0.2809178  0.1431205]]. Action = [[ 0.12399185  0.20643407 -0.1976456   0.9725069 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, False, False, True]
State prediction error at timestep 711 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 712. State = [[-0.27547014  0.15021154]]. Action = [[ 0.22320145  0.08481878  0.06242359 -0.62950075]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, False, False, True]
State prediction error at timestep 712 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 713. State = [[-0.2662224  0.1577844]]. Action = [[ 0.1270468   0.02064294 -0.14265068 -0.22746986]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, False, False, True]
State prediction error at timestep 713 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.2574036   0.16265687]]. Action = [[ 0.06395078 -0.15719935 -0.20078418  0.39648175]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, False, False, True]
State prediction error at timestep 714 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 715. State = [[-0.2494876   0.16167915]]. Action = [[ 0.11713639 -0.20452692 -0.14247061  0.11408257]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, False, False, True]
State prediction error at timestep 715 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of 1
Current timestep = 716. State = [[-0.24208254  0.15536824]]. Action = [[ 0.20649818  0.00430405  0.00676808 -0.04485214]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, False, False, True]
State prediction error at timestep 716 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.2325767   0.15181586]]. Action = [[-0.08732155  0.04156613 -0.00958359 -0.02168107]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, False, False, True]
State prediction error at timestep 717 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.22709335  0.15013109]]. Action = [[ 0.12052679 -0.22711203 -0.04599014 -0.27573633]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, False, False, True]
State prediction error at timestep 718 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.22165862  0.1428562 ]]. Action = [[-0.0864397  -0.20300888 -0.19455631 -0.47515237]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, False, False, True]
State prediction error at timestep 719 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 720. State = [[-0.21710804  0.13368732]]. Action = [[-0.0436812  -0.23495692 -0.04216026 -0.87299734]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, False, False, True]
State prediction error at timestep 720 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[-0.2157294   0.12145758]]. Action = [[ 0.16099572  0.1586625  -0.1405012   0.7031822 ]]. Reward = [0.]
Curr episode timestep = 721
Human Feedback received at timestep 721 of 1
Current timestep = 722. State = [[-0.2130232   0.11790142]]. Action = [[-0.0610607  -0.18055335 -0.05445956 -0.11530036]]. Reward = [0.]
Curr episode timestep = 722
Current timestep = 723. State = [[-0.21067919  0.11238001]]. Action = [[ 0.12622344  0.09158814  0.04817158 -0.07276338]]. Reward = [0.]
Curr episode timestep = 723
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.20830598  0.10991048]]. Action = [[-0.1743721  -0.15522964 -0.10594818  0.54306436]]. Reward = [0.]
Curr episode timestep = 724
Current timestep = 725. State = [[-0.20841682  0.10557795]]. Action = [[-0.09484613 -0.1853118   0.05820501 -0.8711165 ]]. Reward = [0.]
Curr episode timestep = 725
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[-0.20976265  0.09805373]]. Action = [[ 0.17242932 -0.05208255 -0.21930835  0.70233846]]. Reward = [0.]
Curr episode timestep = 726
Current timestep = 727. State = [[-0.20726699  0.09199669]]. Action = [[ 0.16204    -0.07039744 -0.11699195 -0.58662033]]. Reward = [0.]
Curr episode timestep = 727
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[-0.20358369  0.08533943]]. Action = [[-0.0260797   0.06531978  0.22764042  0.73920846]]. Reward = [0.]
Curr episode timestep = 728
Current timestep = 729. State = [[-0.20133463  0.08286631]]. Action = [[-0.13063248  0.2289092   0.10273343  0.8604207 ]]. Reward = [0.]
Curr episode timestep = 729
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.2019998   0.08592986]]. Action = [[-0.02255693 -0.10510832  0.13420165  0.71315265]]. Reward = [0.]
Curr episode timestep = 730
Current timestep = 731. State = [[-0.20250168  0.08632574]]. Action = [[-0.02173309  0.19119844  0.14202052 -0.39606738]]. Reward = [0.]
Curr episode timestep = 731
Human Feedback received at timestep 731 of 1
Current timestep = 732. State = [[-0.2053318   0.09406374]]. Action = [[ 0.1916835   0.17846078 -0.04362193 -0.8640107 ]]. Reward = [0.]
Curr episode timestep = 732
Human Feedback received at timestep 732 of 1
Current timestep = 733. State = [[-0.20275062  0.10010543]]. Action = [[-0.09039606 -0.24337636 -0.21937864  0.8194306 ]]. Reward = [0.]
Curr episode timestep = 733
Current timestep = 734. State = [[-0.20212366  0.0987479 ]]. Action = [[-0.01208051  0.10232949 -0.16330971 -0.91756874]]. Reward = [0.]
Curr episode timestep = 734
Current timestep = 735. State = [[-0.20192254  0.09934773]]. Action = [[ 0.15277705 -0.06272134 -0.20332357 -0.12088031]]. Reward = [0.]
Curr episode timestep = 735
Human Feedback received at timestep 735 of 1
Current timestep = 736. State = [[-0.19970381  0.09886197]]. Action = [[ 0.09183449 -0.15891215 -0.1186631  -0.6462658 ]]. Reward = [0.]
Curr episode timestep = 736
Current timestep = 737. State = [[-0.19584858  0.0943744 ]]. Action = [[ 0.2015965  -0.0985667  -0.17614342  0.8245716 ]]. Reward = [0.]
Curr episode timestep = 737
Current timestep = 738. State = [[-0.18955761  0.08907843]]. Action = [[-0.10917765  0.06850445 -0.0866268  -0.03439194]]. Reward = [0.]
Curr episode timestep = 738
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[-0.18589984  0.08770432]]. Action = [[ 0.09567446  0.20227057  0.14447466 -0.8694267 ]]. Reward = [0.]
Curr episode timestep = 739
Current timestep = 740. State = [[-0.18167946  0.09280748]]. Action = [[-0.04363918  0.23869148  0.16775826 -0.8963617 ]]. Reward = [0.]
Curr episode timestep = 740
Current timestep = 741. State = [[-0.17934892  0.10116526]]. Action = [[ 0.03897625  0.12629941 -0.00459297  0.36407435]]. Reward = [0.]
Curr episode timestep = 741
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.17691772  0.10883545]]. Action = [[-0.1334066  -0.17356212  0.01044691  0.12853754]]. Reward = [0.]
Curr episode timestep = 742
Current timestep = 743. State = [[-0.1778449   0.11083508]]. Action = [[-0.0687544   0.24604672  0.17632705  0.15306282]]. Reward = [0.]
Curr episode timestep = 743
Current timestep = 744. State = [[-0.17971     0.11683336]]. Action = [[-0.07443172 -0.21700606  0.09508491  0.9794831 ]]. Reward = [0.]
Curr episode timestep = 744
Human Feedback received at timestep 744 of 1
Current timestep = 745. State = [[-0.18099958  0.11713646]]. Action = [[ 0.08861741  0.19066367  0.09703532 -0.67294455]]. Reward = [0.]
Curr episode timestep = 745
Current timestep = 746. State = [[-0.18130998  0.12074542]]. Action = [[ 0.15624738  0.07007512  0.19169545 -0.8528059 ]]. Reward = [0.]
Curr episode timestep = 746
Current timestep = 747. State = [[-0.17868659  0.12457968]]. Action = [[ 0.2128169  -0.09163761  0.10133538 -0.18854123]]. Reward = [0.]
Curr episode timestep = 747
Current timestep = 748. State = [[-0.17291497  0.12504993]]. Action = [[-0.14486131 -0.12448932  0.03865463 -0.06660795]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, False, False, True]
State prediction error at timestep 748 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 749. State = [[-0.17146374  0.12311523]]. Action = [[-0.24445489  0.18966988 -0.11405812  0.07299721]]. Reward = [0.]
Curr episode timestep = 749
Current timestep = 750. State = [[-0.17392342  0.12680487]]. Action = [[ 0.19411609  0.21965447  0.02003175 -0.9496571 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, False, False, True]
State prediction error at timestep 750 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 751. State = [[-0.17402744  0.13326512]]. Action = [[ 0.13161731 -0.22581053 -0.13764058  0.46596408]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, False, False, True]
State prediction error at timestep 751 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 752. State = [[-0.1720015  0.1322862]]. Action = [[-0.21219368  0.06850174 -0.03211637 -0.7850598 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, False, False, True]
State prediction error at timestep 752 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 753. State = [[-0.17278585  0.13361408]]. Action = [[ 0.1388093  -0.15229559 -0.08536768 -0.49134517]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, False, False, True]
State prediction error at timestep 753 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-0.17102052  0.13104251]]. Action = [[ 0.16853285  0.2154141   0.09754723 -0.22295988]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, False, False, True]
State prediction error at timestep 754 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 755. State = [[-0.16749355  0.13499223]]. Action = [[-0.18748723  0.15542373 -0.0733512  -0.67984015]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, False, False, True]
State prediction error at timestep 755 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of -1
Current timestep = 756. State = [[-0.168097    0.14146546]]. Action = [[-0.09665738  0.18470982  0.00379381  0.22965801]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, False, False, True]
State prediction error at timestep 756 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 757. State = [[-0.1696133  0.1497166]]. Action = [[ 0.04278526  0.13493079 -0.0406685   0.47645545]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, False, False, True]
State prediction error at timestep 757 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 758. State = [[-0.17058265  0.1600116 ]]. Action = [[ 0.09119511  0.07832259 -0.10624853  0.53477335]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.16978358  0.16774416]]. Action = [[ 0.00963536  0.15113801 -0.18726854  0.494637  ]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, False, False, True]
State prediction error at timestep 759 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 760. State = [[-0.16790016  0.17743482]]. Action = [[ 0.06623271  0.04315132 -0.21558051 -0.3868456 ]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, False, False, True]
State prediction error at timestep 760 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.16544834  0.18476523]]. Action = [[0.07071346 0.19811773 0.12158251 0.74520516]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, False, False, True]
State prediction error at timestep 761 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[-0.16163118  0.19493088]]. Action = [[ 2.0902553e-01 -5.0786138e-04  6.8891108e-02 -9.5150864e-01]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, False, False, True]
State prediction error at timestep 762 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of 1
Current timestep = 763. State = [[-0.15500173  0.20085925]]. Action = [[ 0.10119748 -0.13970797  0.19092032 -0.93075854]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[-0.14880383  0.20047143]]. Action = [[ 0.24495775 -0.22626396  0.2352587  -0.73855674]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, False, False, True]
State prediction error at timestep 764 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.14029731  0.19451952]]. Action = [[ 0.18372974  0.24380156 -0.1943989  -0.32002842]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, False, False, True]
State prediction error at timestep 765 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 1
Current timestep = 766. State = [[-0.12780634  0.19746022]]. Action = [[ 0.03896785  0.06602246 -0.0731834  -0.08429152]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, False, False, True]
State prediction error at timestep 766 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.11734726  0.20146196]]. Action = [[-0.08068861 -0.22634824 -0.17789765  0.291093  ]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, False, False, True]
State prediction error at timestep 767 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of 1
Current timestep = 768. State = [[-0.11235952  0.19649012]]. Action = [[ 0.05045742 -0.14134075 -0.05498326 -0.7729551 ]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, False, False, True]
State prediction error at timestep 768 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[-0.10940854  0.1906602 ]]. Action = [[-0.20439823  0.07960686 -0.0230246   0.79950416]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, False, False, True]
State prediction error at timestep 769 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[-0.11011495  0.19027132]]. Action = [[ 0.14662111  0.19834387 -0.10020369  0.5925884 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, False, False, True]
State prediction error at timestep 770 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[-0.10932571  0.19373819]]. Action = [[ 0.22369125 -0.01457995  0.21332085 -0.3577988 ]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, False, False, True]
State prediction error at timestep 771 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[-0.10422     0.19622295]]. Action = [[-0.03127141 -0.06205112  0.01772645 -0.21735787]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, False, False, True]
State prediction error at timestep 772 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[-0.10118897  0.19613536]]. Action = [[ 0.03634164  0.2071844  -0.22138107 -0.72355306]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, False, False, True]
State prediction error at timestep 773 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 774. State = [[-0.09802847  0.20326449]]. Action = [[-0.2206104   0.24586177 -0.01144469 -0.6431016 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, False, False, True]
State prediction error at timestep 774 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[-0.09981093  0.2136413 ]]. Action = [[-0.10911638  0.24008107 -0.1580259  -0.2906924 ]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, False, False, True]
State prediction error at timestep 775 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of 1
Current timestep = 776. State = [[-0.10347073  0.22782251]]. Action = [[-0.10828578 -0.15317734  0.09973952 -0.22692907]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, False, False, True]
State prediction error at timestep 776 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 776 of 1
Current timestep = 777. State = [[-0.1069405   0.23216751]]. Action = [[-0.0664795   0.0423761   0.03839004  0.04234529]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, False, False, True]
State prediction error at timestep 777 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 778. State = [[-0.10937847  0.23523311]]. Action = [[ 0.16876185 -0.10877229  0.21662772  0.03325343]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, False, False, True]
State prediction error at timestep 778 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 779. State = [[-0.10881288  0.23408608]]. Action = [[ 0.15973777  0.11925682 -0.09916803  0.90566444]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, False, False, True]
State prediction error at timestep 779 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.10712082  0.23522833]]. Action = [[ 0.12235808 -0.1519517  -0.08761817 -0.38791883]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, False, False, True]
State prediction error at timestep 780 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 780 of -1
Current timestep = 781. State = [[-0.10302924  0.23125611]]. Action = [[-0.1329329  -0.03955801 -0.15272115 -0.57757634]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, False, False, True]
State prediction error at timestep 781 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[-0.10228557  0.2296887 ]]. Action = [[-0.18863112 -0.04255396  0.20927918  0.04724634]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, False, False, True]
State prediction error at timestep 782 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 0
Current timestep = 783. State = [[-0.10381352  0.22719793]]. Action = [[ 0.07229844 -0.23157619 -0.24194951  0.944648  ]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, False, False, True]
State prediction error at timestep 783 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 784. State = [[-0.1036839  0.2196571]]. Action = [[-0.13677968 -0.24720201 -0.10086823  0.18255556]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, False, False, True]
State prediction error at timestep 784 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 785. State = [[-0.10610779  0.2074912 ]]. Action = [[-0.1650872  -0.22973612 -0.13818686 -0.5438977 ]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, False, False, True]
State prediction error at timestep 785 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[-0.11094137  0.1933038 ]]. Action = [[ 0.07127017 -0.10943717  0.04814523 -0.16516131]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 0
Current timestep = 787. State = [[-0.11379151  0.18221724]]. Action = [[0.0531832  0.05616614 0.06034461 0.80557656]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 0
Current timestep = 788. State = [[-0.11468466  0.17831266]]. Action = [[ 0.04238662  0.03300232 -0.14114162  0.6411996 ]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, False, False, True]
State prediction error at timestep 788 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 789. State = [[-0.11381886  0.17716424]]. Action = [[-0.13618872 -0.03751709  0.13154775  0.91101384]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, False, False, True]
State prediction error at timestep 789 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of 0
Current timestep = 790. State = [[-0.1147973   0.17544304]]. Action = [[0.05827862 0.01749992 0.24105796 0.87030435]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, False, False, True]
State prediction error at timestep 790 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 0
Current timestep = 791. State = [[-0.11481591  0.17544079]]. Action = [[-0.22481436  0.22792786  0.17497385  0.7855041 ]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, False, False, True]
State prediction error at timestep 791 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[-0.11850961  0.18120037]]. Action = [[0.00643241 0.03656206 0.16321313 0.12329781]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, False, False, True]
State prediction error at timestep 792 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 0
Current timestep = 793. State = [[-0.12121701  0.18599476]]. Action = [[-0.02766788  0.11043361 -0.0907924   0.54597425]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, False, False, True]
State prediction error at timestep 793 is tensor(5.8728e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 793 of 0
Current timestep = 794. State = [[-0.12431794  0.19167784]]. Action = [[ 0.05408046  0.2275796  -0.16835529 -0.6554574 ]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, False, False, True]
State prediction error at timestep 794 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 795. State = [[-0.12696661  0.19917782]]. Action = [[ 0.18080088 -0.20093426 -0.19330241 -0.9365473 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, False, False, True]
State prediction error at timestep 795 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 796. State = [[-0.12541378  0.19877888]]. Action = [[ 0.10868299  0.1587219  -0.16870554 -0.5930651 ]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, False, False, True]
State prediction error at timestep 796 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 796 of -1
Current timestep = 797. State = [[-0.12332454  0.2025116 ]]. Action = [[-0.09212862 -0.08165923 -0.10744131 -0.6127659 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, False, False, True]
State prediction error at timestep 797 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 797 of -1
Current timestep = 798. State = [[-0.12352193  0.20232913]]. Action = [[-0.09556895 -0.14302316  0.21382731  0.4685279 ]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 799. State = [[-0.12417402  0.19959444]]. Action = [[ 0.20477164 -0.22618018 -0.23294161 -0.89085567]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[-0.12146883  0.19260526]]. Action = [[-0.1139026   0.12196749 -0.0809454   0.20261407]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, False, False, True]
State prediction error at timestep 800 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[-0.12059577  0.19126937]]. Action = [[ 0.08300963  0.10684139 -0.07583776  0.36001801]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, False, False, True]
State prediction error at timestep 801 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 802. State = [[-0.11981497  0.19195415]]. Action = [[ 0.01521426 -0.07060465 -0.21942787 -0.01473767]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, False, False, True]
State prediction error at timestep 802 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of -1
Current timestep = 803. State = [[-0.11941013  0.19114351]]. Action = [[-0.19984223  0.06011477 -0.1250256   0.14484406]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, False, False, True]
State prediction error at timestep 803 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of -1
Current timestep = 804. State = [[-0.1209574   0.19276156]]. Action = [[-0.1403754   0.06423762 -0.22775981  0.81356823]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, False, False, True]
State prediction error at timestep 804 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of -1
Current timestep = 805. State = [[-0.12412932  0.1955247 ]]. Action = [[-0.21494024 -0.0854975  -0.11240859  0.7613976 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, False, False, True]
State prediction error at timestep 805 is tensor(7.9453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[-0.12936532  0.19523019]]. Action = [[ 0.00393188 -0.24284345  0.11815023  0.04675269]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, False, False, True]
State prediction error at timestep 806 is tensor(9.4160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 807. State = [[-0.13485704  0.1877061 ]]. Action = [[ 0.22001469  0.14417216 -0.22704126 -0.5545608 ]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, False, False, True]
State prediction error at timestep 807 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of -1
Current timestep = 808. State = [[-0.13458225  0.18712914]]. Action = [[ 0.09637272  0.10952708  0.0545812  -0.05524045]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, False, False, True]
State prediction error at timestep 808 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of -1
Current timestep = 809. State = [[-0.13381672  0.1887613 ]]. Action = [[-0.24555653  0.03592861 -0.15463592  0.07158101]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, False, False, True]
State prediction error at timestep 809 is tensor(4.9857e-05, grad_fn=<MseLossBackward0>)
Current timestep = 810. State = [[-0.13624911  0.19173054]]. Action = [[ 0.21346962 -0.00383314  0.17678785  0.9488901 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, False, False, True]
State prediction error at timestep 810 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 811. State = [[-0.13559124  0.19169031]]. Action = [[-0.18190756 -0.2375652  -0.17920077 -0.52464086]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, False, False, True]
State prediction error at timestep 811 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 812. State = [[-0.13714246  0.18690099]]. Action = [[-0.10424654  0.05020663 -0.09069341  0.45950925]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, False, False, True]
State prediction error at timestep 812 is tensor(3.3716e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[-0.14004229  0.1850643 ]]. Action = [[ 0.08783859  0.06842989  0.06405878 -0.6125995 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, False, False, True]
State prediction error at timestep 813 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of -1
Current timestep = 814. State = [[-0.14074427  0.18651259]]. Action = [[-0.141969    0.24398929 -0.23869456 -0.4626912 ]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, False, False, True]
State prediction error at timestep 814 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 815. State = [[-0.14435674  0.19295913]]. Action = [[ 0.22423285 -0.22097035 -0.0192983   0.20241141]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, False, False, True]
State prediction error at timestep 815 is tensor(6.9164e-05, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.14333756  0.19197021]]. Action = [[ 0.11965257  0.24763727  0.12201512 -0.1424241 ]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, False, False, True]
State prediction error at timestep 816 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of -1
Current timestep = 817. State = [[-0.14106072  0.19734092]]. Action = [[-0.06689128  0.14766508 -0.08620209 -0.9647755 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of -1
Current timestep = 818. State = [[-0.14091295  0.2048901 ]]. Action = [[ 0.12052733  0.22368228 -0.11982892 -0.34315944]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, False, False, True]
State prediction error at timestep 818 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[-0.13861771  0.21474588]]. Action = [[-0.09479329 -0.24782683  0.02445436  0.6092937 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, False, False, True]
State prediction error at timestep 819 is tensor(1.5298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[-0.13796306  0.21486709]]. Action = [[0.17396182 0.00294045 0.07540077 0.86845016]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, False, False, True]
State prediction error at timestep 820 is tensor(7.9298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of -1
Current timestep = 821. State = [[-0.1349344   0.21498647]]. Action = [[-0.14510946  0.20783779 -0.19134037 -0.83603334]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, False, False, True]
State prediction error at timestep 821 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 822. State = [[-0.13537185  0.22022122]]. Action = [[-0.10165593 -0.17441113  0.00596216 -0.85231334]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, False, False, True]
State prediction error at timestep 822 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 823. State = [[-0.13588725  0.21958308]]. Action = [[-0.15271015 -0.08927783 -0.14087668 -0.21034837]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, False, False, True]
State prediction error at timestep 823 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[-0.13855544  0.21710216]]. Action = [[-0.16049336 -0.15801488  0.22162321  0.01228476]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, False, False, True]
State prediction error at timestep 824 is tensor(5.3818e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of 1
Current timestep = 825. State = [[-0.14387462  0.2112207 ]]. Action = [[-0.17741492  0.23120415  0.06548935 -0.04024231]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, False, False, True]
State prediction error at timestep 825 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 826. State = [[-0.1514506  0.2145421]]. Action = [[ 0.21615732 -0.03364855 -0.18202488  0.63679314]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, False, False, True]
State prediction error at timestep 826 is tensor(7.7226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[-0.15379453  0.21465772]]. Action = [[ 0.21318886 -0.22402605 -0.16204979  0.18128705]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, False, False, True]
State prediction error at timestep 827 is tensor(2.6168e-05, grad_fn=<MseLossBackward0>)
Current timestep = 828. State = [[-0.15139553  0.20847242]]. Action = [[ 0.08203235 -0.12995896 -0.02768502  0.44744503]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, False, False, True]
State prediction error at timestep 828 is tensor(1.2977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[-0.14714415  0.20163462]]. Action = [[ 0.11034098  0.10267991 -0.13377637 -0.9925661 ]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, False, False, True]
State prediction error at timestep 829 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 830. State = [[-0.143697    0.19880588]]. Action = [[-0.06075731 -0.00573261  0.19978791 -0.05196667]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, False, False, True]
State prediction error at timestep 830 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 831. State = [[-0.14309222  0.197688  ]]. Action = [[-0.01851778  0.01139039  0.11805049  0.19987941]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, False, False, True]
State prediction error at timestep 831 is tensor(3.6471e-05, grad_fn=<MseLossBackward0>)
Current timestep = 832. State = [[-0.1430775   0.19666572]]. Action = [[-0.20880592 -0.22510041  0.21440798 -0.5850265 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, False, False, True]
State prediction error at timestep 832 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of 1
Current timestep = 833. State = [[-0.1449882   0.19127831]]. Action = [[ 0.06256947 -0.11882403 -0.09171107  0.87842226]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, False, False, True]
State prediction error at timestep 833 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[-0.14453274  0.1848687 ]]. Action = [[-0.22022064 -0.04418653 -0.1317174   0.43523908]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, False, False, True]
State prediction error at timestep 834 is tensor(6.1504e-06, grad_fn=<MseLossBackward0>)
Current timestep = 835. State = [[-0.14824422  0.17939107]]. Action = [[-0.02070872 -0.03248128  0.19316936  0.10013545]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, False, False, True]
State prediction error at timestep 835 is tensor(3.1247e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[-0.1518966   0.17454453]]. Action = [[0.08941376 0.01175991 0.19819564 0.84022784]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, False, False, True]
State prediction error at timestep 836 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 837. State = [[-0.1533147  0.1715265]]. Action = [[-0.17082533 -0.05805358 -0.22525378 -0.9426898 ]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, False, False, True]
State prediction error at timestep 837 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 1
Current timestep = 838. State = [[-0.15674768  0.16880344]]. Action = [[ 0.07999006  0.1226739   0.23882985 -0.38797444]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, False, False, True]
State prediction error at timestep 838 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 839. State = [[-0.15735057  0.17008945]]. Action = [[ 0.22746909  0.18902051  0.19547111 -0.6334923 ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, False, False, True]
State prediction error at timestep 839 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[-0.15489571  0.17567441]]. Action = [[ 0.14612675  0.1513721   0.0554938  -0.10761768]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, False, False, True]
State prediction error at timestep 840 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 841. State = [[-0.15072744  0.18330866]]. Action = [[-0.19081791  0.15682429 -0.0005482  -0.41211164]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, False, False, True]
State prediction error at timestep 841 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of 1
Current timestep = 842. State = [[-0.15132296  0.19222255]]. Action = [[-0.1435385   0.13337475 -0.05443934  0.11236048]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, False, False, True]
State prediction error at timestep 842 is tensor(5.4931e-05, grad_fn=<MseLossBackward0>)
Current timestep = 843. State = [[-0.15589292  0.2018163 ]]. Action = [[-0.01942527  0.02892271  0.10634169  0.4824549 ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, False, False, True]
State prediction error at timestep 843 is tensor(5.8426e-05, grad_fn=<MseLossBackward0>)
Current timestep = 844. State = [[-0.15938297  0.20720477]]. Action = [[ 0.12303194 -0.16425203 -0.2230126   0.20943356]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, False, False, True]
State prediction error at timestep 844 is tensor(3.6061e-06, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[-0.15898421  0.2068145 ]]. Action = [[-0.12946332 -0.01095773 -0.12894465 -0.58787787]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, False, False, True]
State prediction error at timestep 845 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 846. State = [[-0.15959418  0.20683803]]. Action = [[-0.14724056 -0.1074845   0.02524832  0.78292465]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, False, False, True]
State prediction error at timestep 846 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 1
Current timestep = 847. State = [[-0.16287023  0.20407869]]. Action = [[-0.09599677  0.21915972  0.11664623  0.589484  ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, False, False, True]
State prediction error at timestep 847 is tensor(4.5440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 848. State = [[-0.16760874  0.20882027]]. Action = [[ 0.00255767 -0.2261243  -0.08922687 -0.49170923]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, False, False, True]
State prediction error at timestep 848 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of 1
Current timestep = 849. State = [[-0.17049667  0.20539734]]. Action = [[ 0.09228954 -0.1597839  -0.03509097 -0.8886222 ]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, False, False, True]
State prediction error at timestep 849 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[-0.17188287  0.19797048]]. Action = [[-0.15217553 -0.21047007 -0.16735283 -0.48736697]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, False, False, True]
State prediction error at timestep 850 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 851. State = [[-0.17396034  0.18978441]]. Action = [[0.15691751 0.19859618 0.23892081 0.83989215]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, False, False, True]
State prediction error at timestep 851 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 0
Current timestep = 852. State = [[-0.17380004  0.18940315]]. Action = [[-0.16031365  0.18215555  0.16592938 -0.63092136]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, False, False, True]
State prediction error at timestep 852 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[-0.17657717  0.19424963]]. Action = [[-0.15510589 -0.15274175 -0.17117348  0.02705181]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, False, False, True]
State prediction error at timestep 853 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 854. State = [[-0.18074465  0.19284831]]. Action = [[ 0.06000847 -0.05324277 -0.19948716  0.61744606]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, False, False, True]
State prediction error at timestep 854 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 854 of 0
Current timestep = 855. State = [[-0.1828162   0.19068794]]. Action = [[ 0.02389219  0.14424688 -0.12538943  0.79872584]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, False, False, True]
State prediction error at timestep 855 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 856. State = [[-0.18393037  0.19248375]]. Action = [[-0.0513825  -0.02737632 -0.13949144 -0.876501  ]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, False, False, True]
State prediction error at timestep 856 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of 0
Current timestep = 857. State = [[-0.18452068  0.19344577]]. Action = [[ 0.1412015   0.07279009 -0.22485918 -0.34816027]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, False, False, True]
State prediction error at timestep 857 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 858. State = [[-0.18447389  0.19396143]]. Action = [[-0.09129807 -0.13128515 -0.23795383 -0.3635075 ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, False, False, True]
State prediction error at timestep 858 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of -1
Current timestep = 859. State = [[-0.18466865  0.19270651]]. Action = [[ 0.12536508 -0.13469389  0.01613605  0.79618263]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, False, False, True]
State prediction error at timestep 859 is tensor(5.9871e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of -1
Current timestep = 860. State = [[-0.1817686   0.18770969]]. Action = [[ 0.17243445 -0.22582279 -0.06260601 -0.1885258 ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, False, False, True]
State prediction error at timestep 860 is tensor(1.9902e-05, grad_fn=<MseLossBackward0>)
Current timestep = 861. State = [[-0.17855303  0.17813338]]. Action = [[ 0.08614212  0.06189725 -0.07280454  0.07023084]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, False, False, True]
State prediction error at timestep 861 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 862. State = [[-0.17531478  0.1734256 ]]. Action = [[-0.1995752   0.22714496 -0.1403572  -0.68251354]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, False, False, True]
State prediction error at timestep 862 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 863. State = [[-0.17632133  0.17601536]]. Action = [[ 0.174258   -0.2404102  -0.17707504 -0.15368396]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, False, False, True]
State prediction error at timestep 863 is tensor(1.7862e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of -1
Current timestep = 864. State = [[-0.17391355  0.17208949]]. Action = [[-0.07812747  0.07331219  0.22141725 -0.24456555]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, False, False, True]
State prediction error at timestep 864 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 865. State = [[-0.17395936  0.1719522 ]]. Action = [[-0.12183058 -0.02551533  0.0254806  -0.03779972]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, False, False, True]
State prediction error at timestep 865 is tensor(3.1912e-06, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[-0.17467025  0.17122637]]. Action = [[-0.09704396 -0.03123641  0.06968564  0.17697287]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, False, False, True]
State prediction error at timestep 866 is tensor(3.1620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of 1
Current timestep = 867. State = [[-0.17684813  0.16966724]]. Action = [[-0.18717982 -0.19885603  0.01338097  0.5492873 ]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, False, False, True]
State prediction error at timestep 867 is tensor(1.2285e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of 1
Current timestep = 868. State = [[-0.181301    0.16396205]]. Action = [[-0.04548657 -0.03962815 -0.22157325 -0.1326065 ]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, False, False, True]
State prediction error at timestep 868 is tensor(7.9241e-05, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[-0.1855773   0.15949059]]. Action = [[ 0.18564478  0.2255165  -0.15906422 -0.67575234]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, False, False, True]
State prediction error at timestep 869 is tensor(9.0565e-05, grad_fn=<MseLossBackward0>)
Current timestep = 870. State = [[-0.18650134  0.16239652]]. Action = [[0.21187991 0.15721235 0.2147128  0.984642  ]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, False, False, True]
State prediction error at timestep 870 is tensor(6.7751e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of 1
Current timestep = 871. State = [[-0.18328357  0.16852303]]. Action = [[-0.05702679  0.18555897 -0.09192547  0.29459083]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, False, False, True]
State prediction error at timestep 871 is tensor(2.6957e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 871 of -1
Current timestep = 872. State = [[-0.1818488   0.17713462]]. Action = [[ 0.06558999  0.08206216 -0.18663678  0.65798724]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, False, False, True]
State prediction error at timestep 872 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 873. State = [[-0.1810477   0.18338032]]. Action = [[-0.01508231 -0.17796157  0.1712029  -0.81007576]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, False, False, True]
State prediction error at timestep 873 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[-0.18076439  0.18334174]]. Action = [[-0.03433108  0.24359447 -0.18649001  0.63945293]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, False, False, True]
State prediction error at timestep 874 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of -1
Current timestep = 875. State = [[-0.18084584  0.189376  ]]. Action = [[ 0.12756252 -0.1429215   0.0954355  -0.8644528 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, False, False, True]
State prediction error at timestep 875 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 876. State = [[-0.17881887  0.18863255]]. Action = [[-0.13689514 -0.18150446 -0.2320832  -0.85422003]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, False, False, True]
State prediction error at timestep 876 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.17761634  0.18339156]]. Action = [[ 0.21687329 -0.17863204  0.21301466  0.59400666]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, False, False, True]
State prediction error at timestep 877 is tensor(7.6830e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of 1
Current timestep = 878. State = [[-0.17402153  0.17678794]]. Action = [[-0.08738634 -0.23740739  0.1017012   0.5989369 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, False, False, True]
State prediction error at timestep 878 is tensor(5.6792e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.1721101  0.1650705]]. Action = [[-0.23049918 -0.01706423  0.22265965 -0.73808324]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of 1
Current timestep = 880. State = [[-0.17469063  0.15761697]]. Action = [[-0.05126251 -0.05183333 -0.16107894  0.9713584 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 881. State = [[-0.17705196  0.15298627]]. Action = [[ 0.09524632 -0.00708857  0.03844088 -0.6163912 ]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, False, False, True]
State prediction error at timestep 881 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of 1
Current timestep = 882. State = [[-0.17795752  0.15026481]]. Action = [[-0.05208874  0.13323134 -0.07226458  0.25610495]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, False, False, True]
State prediction error at timestep 882 is tensor(1.8176e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 1
Current timestep = 883. State = [[-0.17936286  0.1520737 ]]. Action = [[ 0.15517765  0.23021924 -0.2180846   0.5983752 ]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, False, False, True]
State prediction error at timestep 883 is tensor(7.3448e-05, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[-0.17842196  0.15869072]]. Action = [[-0.05355728  0.13089162  0.24083185 -0.7695178 ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, False, False, True]
State prediction error at timestep 884 is tensor(6.5055e-05, grad_fn=<MseLossBackward0>)
Current timestep = 885. State = [[-0.1784916   0.16574997]]. Action = [[ 0.04846901  0.13133436 -0.04900008  0.5286937 ]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, False, False, True]
State prediction error at timestep 885 is tensor(6.4669e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 885 of 1
Current timestep = 886. State = [[-0.1785306  0.1730642]]. Action = [[ 0.1019547  -0.19221617 -0.18188335  0.9533247 ]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, False, False, True]
State prediction error at timestep 886 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.17710175  0.17247139]]. Action = [[ 0.11252052  0.05012521 -0.16805246  0.49845076]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, False, False, True]
State prediction error at timestep 887 is tensor(6.0043e-06, grad_fn=<MseLossBackward0>)
Current timestep = 888. State = [[-0.17436655  0.1729777 ]]. Action = [[ 0.00223932 -0.17179947 -0.20695725  0.8344374 ]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, False, False, True]
State prediction error at timestep 888 is tensor(9.3855e-05, grad_fn=<MseLossBackward0>)
Current timestep = 889. State = [[-0.17167495  0.16862719]]. Action = [[ 0.24023211 -0.22539994  0.19609988  0.4418589 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, False, False, True]
State prediction error at timestep 889 is tensor(9.6220e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 889 of 1
Current timestep = 890. State = [[-0.16533649  0.1595789 ]]. Action = [[-0.0389909   0.08304483  0.08423325 -0.3469975 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, False, False, True]
State prediction error at timestep 890 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 891. State = [[-0.16038401  0.15698919]]. Action = [[-0.19172765  0.17709836  0.21522021 -0.8135785 ]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, False, False, True]
State prediction error at timestep 891 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[-0.16071779  0.15997694]]. Action = [[-0.17943451 -0.10203966 -0.18481071  0.40375388]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, False, False, True]
State prediction error at timestep 892 is tensor(6.9686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of 1
Current timestep = 893. State = [[-0.16275145  0.16031829]]. Action = [[-0.0011376  -0.16687039  0.16035223 -0.16352469]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, False, False, True]
State prediction error at timestep 893 is tensor(1.7347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of 1
Current timestep = 894. State = [[-0.16371857  0.15664499]]. Action = [[ 0.1288212   0.13424358 -0.07902884 -0.8246264 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, False, False, True]
State prediction error at timestep 894 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 1
Current timestep = 895. State = [[-0.16372754  0.15649119]]. Action = [[ 0.22695488 -0.16960444 -0.22742856  0.57592916]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, False, False, True]
State prediction error at timestep 895 is tensor(1.8987e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of 1
Current timestep = 896. State = [[-0.15974224  0.151408  ]]. Action = [[-0.19844602 -0.07658553  0.05060598  0.33534646]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, False, False, True]
State prediction error at timestep 896 is tensor(3.4003e-05, grad_fn=<MseLossBackward0>)
Current timestep = 897. State = [[-0.15895806  0.14815256]]. Action = [[-0.13393356  0.03958464  0.12229961  0.3732015 ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, False, False, True]
State prediction error at timestep 897 is tensor(6.1865e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.16120468  0.14638686]]. Action = [[-0.19119205  0.1505791   0.17530179  0.4698336 ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, False, False, True]
State prediction error at timestep 898 is tensor(1.4148e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.1664561   0.14990205]]. Action = [[-0.23600581  0.22547802  0.01231888 -0.3144213 ]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, False, False, True]
State prediction error at timestep 899 is tensor(1.9561e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of 1
Current timestep = 900. State = [[-0.17512499  0.16001101]]. Action = [[ 0.00795457  0.19718009 -0.1986012  -0.90657914]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, False, False, True]
State prediction error at timestep 900 is tensor(2.8728e-05, grad_fn=<MseLossBackward0>)
Current timestep = 901. State = [[-0.2577782   0.00804603]]. Action = [[-0.17553073 -0.10333776  0.16894233  0.24454379]]. Reward = [0.]
Curr episode timestep = 901
Human Feedback received at timestep 901 of 1
Current timestep = 902. State = [[-0.25639683  0.010118  ]]. Action = [[-0.15158324 -0.14438568 -0.17095956  0.8681382 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 903. State = [[-0.25277838  0.02873755]]. Action = [[ 0.21719408  0.21628734 -0.0796113   0.11220276]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 904. State = [[-0.24544735  0.07427508]]. Action = [[ 0.17026904  0.00151232  0.12561712 -0.80888104]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 905. State = [[-0.23435518  0.11109226]]. Action = [[-0.09020726 -0.19631064  0.1224947  -0.5004464 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 906. State = [[-0.22382495  0.13004506]]. Action = [[-0.0351913  -0.22344199 -0.12066931 -0.76015437]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 906 is [True, False, False, False, False, True]
State prediction error at timestep 906 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 907. State = [[-0.21815792  0.1357354 ]]. Action = [[-0.18779156  0.17196465 -0.2173376  -0.30758083]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 907 is [True, False, False, False, False, True]
State prediction error at timestep 907 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 908. State = [[-0.22052436  0.14509124]]. Action = [[-0.04783632  0.23078233  0.12192792  0.01984942]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 908 is [True, False, False, False, False, True]
State prediction error at timestep 908 is tensor(2.5345e-05, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.22465217  0.15664583]]. Action = [[-0.11570936 -0.22157104  0.16396391 -0.83098394]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 909 is [True, False, False, False, False, True]
State prediction error at timestep 909 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 910. State = [[-0.22679053  0.1586471 ]]. Action = [[ 0.16441649 -0.17704175  0.10620618  0.7599628 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 910 is [True, False, False, False, False, True]
State prediction error at timestep 910 is tensor(9.0558e-06, grad_fn=<MseLossBackward0>)
Current timestep = 911. State = [[-0.22526322  0.15484646]]. Action = [[-0.13573672 -0.0861252  -0.23243769 -0.1082201 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 911 is [True, False, False, False, False, True]
State prediction error at timestep 911 is tensor(8.6960e-06, grad_fn=<MseLossBackward0>)
Current timestep = 912. State = [[-0.2248804  0.1514148]]. Action = [[-0.22975346  0.22364348  0.07714686 -0.7836296 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 912 is [True, False, False, False, False, True]
State prediction error at timestep 912 is tensor(5.8825e-05, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[-0.22878674  0.15590942]]. Action = [[ 0.14263844 -0.15649332 -0.21970466 -0.30744672]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 913 is [True, False, False, False, False, True]
State prediction error at timestep 913 is tensor(2.6313e-06, grad_fn=<MseLossBackward0>)
Current timestep = 914. State = [[-0.22885808  0.15358452]]. Action = [[0.13327146 0.17683178 0.20757723 0.713449  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 914 is [True, False, False, False, False, True]
State prediction error at timestep 914 is tensor(1.5854e-05, grad_fn=<MseLossBackward0>)
Current timestep = 915. State = [[-0.22890429  0.15548672]]. Action = [[-0.20942898  0.08304408 -0.23541558 -0.9540079 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 915 is [True, False, False, False, False, True]
State prediction error at timestep 915 is tensor(2.5240e-05, grad_fn=<MseLossBackward0>)
Current timestep = 916. State = [[-0.2325126   0.16096003]]. Action = [[-0.14700127  0.18534452 -0.13172038  0.10693812]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 916 is [True, False, False, False, False, True]
State prediction error at timestep 916 is tensor(8.4331e-06, grad_fn=<MseLossBackward0>)
Current timestep = 917. State = [[-0.23814198  0.1699215 ]]. Action = [[ 0.22759825  0.19944012 -0.18024033  0.7479068 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 917 is [True, False, False, False, False, True]
State prediction error at timestep 917 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 918. State = [[-0.23954491  0.17821363]]. Action = [[ 0.23752576  0.10188287 -0.07875097  0.9046984 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 918 is [True, False, False, False, False, True]
State prediction error at timestep 918 is tensor(6.7718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 919. State = [[-0.23543823  0.18529513]]. Action = [[-0.0980666  -0.10053796 -0.07379913  0.85666823]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 919 is [True, False, False, False, False, True]
State prediction error at timestep 919 is tensor(4.0851e-06, grad_fn=<MseLossBackward0>)
Current timestep = 920. State = [[-0.23377165  0.18697484]]. Action = [[-0.04693094 -0.21724163  0.04457924  0.6338997 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 920 is [True, False, False, False, False, True]
State prediction error at timestep 920 is tensor(2.4170e-05, grad_fn=<MseLossBackward0>)
Current timestep = 921. State = [[-0.23299249  0.18396792]]. Action = [[-0.17462283  0.07843167 -0.16498873 -0.86920327]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 921 is [True, False, False, False, False, True]
State prediction error at timestep 921 is tensor(8.3171e-05, grad_fn=<MseLossBackward0>)
Current timestep = 922. State = [[-0.23459668  0.18461998]]. Action = [[ 0.05025342 -0.180599   -0.24066083 -0.95960915]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 922 is [True, False, False, False, False, True]
State prediction error at timestep 922 is tensor(2.8088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 923. State = [[-0.23410259  0.18086462]]. Action = [[-0.22990337  0.05723014  0.14375198 -0.21357948]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 923 is [True, False, False, False, False, True]
State prediction error at timestep 923 is tensor(4.1440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 924. State = [[-0.2378183   0.18125536]]. Action = [[-0.20020254  0.05276364  0.01862207 -0.591808  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 924 is [True, False, False, False, False, True]
State prediction error at timestep 924 is tensor(4.8610e-05, grad_fn=<MseLossBackward0>)
Current timestep = 925. State = [[-0.24665397  0.18352635]]. Action = [[-0.18332407  0.15702638 -0.23757806  0.84847474]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 925 is [True, False, False, False, False, True]
State prediction error at timestep 925 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 926. State = [[-0.25590557  0.18850723]]. Action = [[ 0.18356371 -0.20842172 -0.07436121 -0.7831986 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 926 is [True, False, False, False, False, True]
State prediction error at timestep 926 is tensor(2.5602e-05, grad_fn=<MseLossBackward0>)
Current timestep = 927. State = [[-0.25721368  0.18581952]]. Action = [[-0.21579064  0.07399121  0.19457391  0.45137894]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 927 is [True, False, False, False, False, True]
State prediction error at timestep 927 is tensor(7.0812e-06, grad_fn=<MseLossBackward0>)
Current timestep = 928. State = [[-0.26187688  0.18680885]]. Action = [[-0.09857398 -0.1787635  -0.1452048  -0.01387501]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 928 is [True, False, False, False, False, True]
State prediction error at timestep 928 is tensor(5.2441e-05, grad_fn=<MseLossBackward0>)
Current timestep = 929. State = [[-0.26653016  0.18275592]]. Action = [[-0.03968616 -0.04318887 -0.08682111  0.8533428 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 929 is [True, False, False, False, False, True]
State prediction error at timestep 929 is tensor(8.1247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[-0.27169824  0.17833455]]. Action = [[-0.00765529 -0.15003237  0.17361364 -0.7704532 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 930 is [True, False, False, False, False, True]
State prediction error at timestep 930 is tensor(7.2140e-05, grad_fn=<MseLossBackward0>)
Current timestep = 931. State = [[-0.2747499   0.17145848]]. Action = [[ 0.08725676 -0.05752689 -0.11167777  0.19009757]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 931 is [True, False, False, False, False, True]
State prediction error at timestep 931 is tensor(1.9479e-05, grad_fn=<MseLossBackward0>)
Current timestep = 932. State = [[-0.27571774  0.16481304]]. Action = [[ 0.09352291 -0.00201449 -0.08236489 -0.5693161 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 932 is [True, False, False, False, False, True]
State prediction error at timestep 932 is tensor(6.1136e-05, grad_fn=<MseLossBackward0>)
Current timestep = 933. State = [[-0.27553213  0.16096215]]. Action = [[-0.10445237  0.22922581  0.14310342  0.16162479]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 933 is [True, False, False, False, False, True]
State prediction error at timestep 933 is tensor(9.8060e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of -1
Current timestep = 934. State = [[-0.27805886  0.16535136]]. Action = [[-0.11284624  0.15682179 -0.21703106 -0.18705976]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 934 is [True, False, False, False, False, True]
State prediction error at timestep 934 is tensor(1.8430e-05, grad_fn=<MseLossBackward0>)
Current timestep = 935. State = [[-0.28257653  0.17222643]]. Action = [[-0.04638821 -0.19781648 -0.12932548  0.48006225]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 935 is [True, False, False, False, False, True]
State prediction error at timestep 935 is tensor(3.2520e-05, grad_fn=<MseLossBackward0>)
Current timestep = 936. State = [[-0.2850329   0.17090012]]. Action = [[ 0.21156234 -0.10783049  0.1927855  -0.27000284]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 936 is [True, False, False, False, False, True]
State prediction error at timestep 936 is tensor(1.3294e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of -1
Current timestep = 937. State = [[-0.28279707  0.16696121]]. Action = [[ 0.1783554  -0.07997742  0.17870772  0.10569441]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 937 is [True, False, False, False, False, True]
State prediction error at timestep 937 is tensor(4.7750e-08, grad_fn=<MseLossBackward0>)
Current timestep = 938. State = [[-0.278174    0.16095972]]. Action = [[ 0.23065376 -0.09099475  0.07172853 -0.03523266]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 938 is [True, False, False, False, False, True]
State prediction error at timestep 938 is tensor(3.8207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of -1
Current timestep = 939. State = [[-0.27093387  0.15495247]]. Action = [[ 0.111292    0.10787264 -0.18183833  0.20712471]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 939 is [True, False, False, False, False, True]
State prediction error at timestep 939 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of -1
Current timestep = 940. State = [[-0.2643869   0.15403593]]. Action = [[ 0.13046688 -0.18545865  0.0872941  -0.2769909 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 940 is [True, False, False, False, False, True]
State prediction error at timestep 940 is tensor(2.9275e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 940 of -1
Current timestep = 941. State = [[-0.25564453  0.14765827]]. Action = [[ 0.09979379 -0.19595468 -0.04004236  0.7395034 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 941 is [True, False, False, False, False, True]
State prediction error at timestep 941 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 942. State = [[-0.24743469  0.13861513]]. Action = [[-0.08750546 -0.11052622 -0.22086442 -0.7014029 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 942 is [True, False, False, False, False, True]
State prediction error at timestep 942 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 943. State = [[-0.24424414  0.13047269]]. Action = [[-0.23600356 -0.04232624 -0.16957481 -0.40316284]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 943 is [True, False, False, False, False, True]
State prediction error at timestep 943 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of -1
Current timestep = 944. State = [[-0.24511747  0.12604481]]. Action = [[-0.07123438  0.14143056  0.10252374  0.4215337 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 944 is [True, False, False, False, False, True]
State prediction error at timestep 944 is tensor(1.0239e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of -1
Current timestep = 945. State = [[-0.24746194  0.12671602]]. Action = [[-0.03710406  0.05160743 -0.09684779  0.01836395]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 945 is [True, False, False, False, False, True]
State prediction error at timestep 945 is tensor(1.8462e-05, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[-0.24958532  0.12802185]]. Action = [[-0.14338507 -0.18925092  0.0960778   0.85992694]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 946 is [True, False, False, False, False, True]
State prediction error at timestep 946 is tensor(1.9481e-05, grad_fn=<MseLossBackward0>)
Current timestep = 947. State = [[-0.25330937  0.12434645]]. Action = [[-0.2428548  -0.10167292  0.21625394 -0.3936547 ]]. Reward = [0.]
Curr episode timestep = 45
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.2603216   0.11985643]]. Action = [[-0.19668761 -0.16360505  0.09409672 -0.03271759]]. Reward = [0.]
Curr episode timestep = 46
Human Feedback received at timestep 948 of -1
Current timestep = 949. State = [[-0.26906925  0.11199302]]. Action = [[ 0.14102596 -0.16372694  0.23594046 -0.8069348 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 950. State = [[-0.27406517  0.10261879]]. Action = [[-0.08968285 -0.19222662 -0.18073665 -0.06124544]]. Reward = [0.]
Curr episode timestep = 48
Human Feedback received at timestep 950 of -1
Current timestep = 951. State = [[-0.27949232  0.09237259]]. Action = [[-0.21116282  0.0685358  -0.06655908 -0.03201389]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 952. State = [[-0.28722152  0.08722612]]. Action = [[-0.18296178 -0.03556563  0.23066586  0.64433146]]. Reward = [0.]
Curr episode timestep = 50
Human Feedback received at timestep 952 of -1
Current timestep = 953. State = [[-0.295689    0.08301539]]. Action = [[-0.1938474  -0.09587538 -0.14692292  0.40212643]]. Reward = [0.]
Curr episode timestep = 51
Human Feedback received at timestep 953 of -1
Current timestep = 954. State = [[-0.3061331   0.07781138]]. Action = [[ 0.0110386  -0.1608442  -0.13413592  0.5854771 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 955. State = [[-0.31321022  0.07023089]]. Action = [[ 0.09226406 -0.05102029 -0.08754969  0.373788  ]]. Reward = [0.]
Curr episode timestep = 53
Human Feedback received at timestep 955 of -1
Current timestep = 956. State = [[-0.318865    0.06339838]]. Action = [[-0.22573908  0.09519047  0.10896125  0.85051084]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 957. State = [[-0.32678524  0.06225634]]. Action = [[-0.19244741  0.2211318   0.02066919  0.94576335]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 958. State = [[-0.33493525  0.06660378]]. Action = [[ 0.1452803  -0.08407593 -0.02110337 -0.28345335]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 959. State = [[-0.33733737  0.06688605]]. Action = [[ 0.15611732 -0.11109656 -0.03105262  0.60112286]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 960. State = [[-0.33633545  0.06392823]]. Action = [[ 0.0306544  -0.21058059  0.04747781 -0.3631252 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 961. State = [[-0.33446303  0.05870118]]. Action = [[-0.12608379  0.21613616 -0.13383074  0.05522907]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 962. State = [[-0.33562672  0.05957847]]. Action = [[ 0.08947831  0.17939079 -0.1503846  -0.6123531 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 963. State = [[-0.3363485   0.06277458]]. Action = [[ 0.20530576 -0.02835591  0.20550716 -0.9159465 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 964. State = [[-0.33430907  0.06359923]]. Action = [[-0.21118404 -0.20881926 -0.08526573  0.4734782 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 965. State = [[-0.33403265  0.06167718]]. Action = [[ 0.09269956  0.10005668 -0.19842291  0.61585593]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 966. State = [[-0.3340298   0.06212143]]. Action = [[ 0.01524577  0.24271584  0.00436839 -0.38848603]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 967. State = [[-0.3347647  0.0663193]]. Action = [[ 0.05699399 -0.16757928 -0.1973568   0.07070637]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 968. State = [[-0.33337948  0.06640656]]. Action = [[ 0.12868327  0.01759842 -0.21262506  0.49608088]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 969. State = [[-0.33090246  0.06673391]]. Action = [[-0.04869388 -0.19241858 -0.03215629 -0.7628046 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 970. State = [[-0.3285393   0.06321667]]. Action = [[-0.0024537  -0.07560745  0.08805296 -0.16916156]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 971. State = [[-0.32684332  0.05903513]]. Action = [[ 0.11322901  0.1415155   0.16180325 -0.35300744]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 972. State = [[-0.3252796   0.05930179]]. Action = [[ 0.03125441  0.03075182 -0.22897191  0.13323402]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 973. State = [[-0.32376638  0.05989621]]. Action = [[ 0.0341489  -0.19942023  0.05290633 -0.43921924]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 974. State = [[-0.32073447  0.05614839]]. Action = [[-0.09769946  0.20015061 -0.177149    0.9335413 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 975. State = [[-0.32104793  0.05757209]]. Action = [[-0.24133432  0.19290838  0.0812909   0.7065022 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 976. State = [[-0.32472226  0.06315683]]. Action = [[-0.21865632 -0.08663815 -0.18557288  0.2335738 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 977. State = [[-0.33028862  0.06617731]]. Action = [[ 0.01935387  0.23409373 -0.10387897 -0.82963765]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 978. State = [[-0.33447948  0.07336646]]. Action = [[ 0.18480158  0.08166721 -0.1130012  -0.5429953 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 979. State = [[-0.33618823  0.07822723]]. Action = [[-0.10600467 -0.21563844 -0.11283499 -0.21134448]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 980. State = [[-0.33643287  0.07732464]]. Action = [[ 0.07402942 -0.21938984  0.23658848  0.9124515 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 981. State = [[-0.33529806  0.07308055]]. Action = [[-0.18795075  0.13808581 -0.24066943 -0.7371025 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 982. State = [[-0.3372351   0.07381468]]. Action = [[-0.19242068  0.15513387 -0.22507724 -0.70314693]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 983. State = [[-0.34332457  0.07791571]]. Action = [[-0.02821139 -0.18997276  0.03857237 -0.80081546]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 984. State = [[-0.3483484   0.07450318]]. Action = [[-0.03661056 -0.2328868  -0.18519829  0.6266329 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 985. State = [[-0.3532712   0.06759556]]. Action = [[ 0.1885851   0.06595191 -0.03019406 -0.71205735]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 986. State = [[-0.3543865   0.06429794]]. Action = [[-0.00099176  0.22305179  0.24398053 -0.4074378 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 987. State = [[-0.35539627  0.06681401]]. Action = [[ 0.04098716  0.22310948  0.17986768 -0.07081699]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 988. State = [[-0.35626587  0.0722595 ]]. Action = [[ 0.20095819 -0.2163288  -0.02346618  0.14492393]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 989. State = [[-0.35343534  0.07194242]]. Action = [[-0.12769863 -0.04796755 -0.16557212  0.26255977]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 990. State = [[-0.352641    0.07153071]]. Action = [[-0.15128824  0.05697557  0.11069694 -0.51257074]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 991. State = [[-0.35399267  0.07225695]]. Action = [[-0.17473555 -0.06069173 -0.09570256  0.61583066]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 992. State = [[-0.3575733   0.07235914]]. Action = [[-0.23226334  0.21797839  0.01910144  0.5724158 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 993. State = [[-0.36489046  0.07673711]]. Action = [[ 0.1833401  -0.07700279  0.1113494   0.7400763 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 994. State = [[-0.36705312  0.0782775 ]]. Action = [[0.04550484 0.1676293  0.10011625 0.46727884]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 995. State = [[-0.3696409   0.08276875]]. Action = [[-0.08382094  0.24618042  0.23019934 -0.8327081 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 996. State = [[-0.37386167  0.09107861]]. Action = [[-0.20352286  0.18309021  0.0122799  -0.18296295]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 997. State = [[-0.38013536  0.10104825]]. Action = [[ 0.05390859  0.11372936  0.05149934 -0.6365766 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 998. State = [[-0.38421497  0.10986377]]. Action = [[ 0.23369318 -0.23335685 -0.00241321  0.54452085]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 999. State = [[-0.3818197   0.11073185]]. Action = [[ 0.12591344 -0.07462049 -0.15824415 -0.39550287]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1000. State = [[-0.37794122  0.11010117]]. Action = [[ 0.1389702   0.14243868 -0.04514652 -0.46199888]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1001. State = [[-0.37344757  0.11147905]]. Action = [[ 0.20679563 -0.18720502  0.01912132  0.2968837 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1002. State = [[-0.36580858  0.10928948]]. Action = [[-0.00970274  0.19235903  0.19748059  0.3726107 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1003. State = [[-0.36128148  0.11129393]]. Action = [[ 0.03101552  0.15360227 -0.14040422  0.20312762]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1004. State = [[-0.35670155  0.11617093]]. Action = [[ 0.15527236  0.10075951 -0.03604749  0.8160366 ]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1005. State = [[-0.35037807  0.12046576]]. Action = [[ 0.10324755 -0.22511317  0.15914589 -0.3459556 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1006. State = [[-0.34245682  0.11992175]]. Action = [[ 0.23738986 -0.20951511  0.03111333  0.70355487]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1007. State = [[-0.33301237  0.11434335]]. Action = [[ 0.19529939 -0.20212705 -0.16690822  0.00442302]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1008. State = [[-0.322153    0.10614922]]. Action = [[0.03731996 0.07474124 0.20332205 0.5995891 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1009. State = [[-0.31276265  0.10138119]]. Action = [[ 0.02189201 -0.23548882 -0.09313723  0.54421806]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1010. State = [[-0.30595347  0.09361206]]. Action = [[ 0.11946568  0.00310972  0.20987046 -0.04134822]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1011. State = [[-0.29892597  0.08821159]]. Action = [[-0.11431268  0.03590405 -0.23872913  0.45150042]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1012. State = [[-0.29637805  0.08645106]]. Action = [[ 0.14829862  0.04655686  0.10102603 -0.42202985]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1013. State = [[-0.29211172  0.08601772]]. Action = [[0.19365454 0.01183313 0.14486992 0.32915437]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1014. State = [[-0.28575885  0.0856929 ]]. Action = [[-0.06730419 -0.0067274   0.04934376  0.7944732 ]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1015. State = [[-0.28268972  0.08518442]]. Action = [[-0.15938567 -0.1457414   0.20187095 -0.68105406]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1016. State = [[-0.28198603  0.08244133]]. Action = [[-0.00344141 -0.06422636  0.06178394 -0.22413969]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1017. State = [[-0.2812437   0.07962041]]. Action = [[ 0.07865801  0.2424643  -0.04399812 -0.5878445 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1018. State = [[-0.28192008  0.08175045]]. Action = [[0.04185578 0.19491667 0.22139806 0.94355524]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1019. State = [[-0.2812329   0.08677682]]. Action = [[ 0.01243478 -0.0306828  -0.2364243  -0.33434653]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1020. State = [[-0.28001648  0.08939701]]. Action = [[-0.12868032  0.2245298  -0.21653439 -0.8106664 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1021. State = [[-0.28185076  0.09717054]]. Action = [[-0.08263052  0.00282454 -0.02309641  0.16009152]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1022. State = [[-0.28493807  0.1024321 ]]. Action = [[-0.07502434  0.13784254 -0.18170938 -0.24502337]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1023. State = [[-0.28942707  0.11004271]]. Action = [[-0.21401733  0.15442675  0.01167676  0.8266896 ]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1024. State = [[-0.29534242  0.11914538]]. Action = [[-0.22839263  0.1620329  -0.21397771  0.46838796]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1025. State = [[-0.30370057  0.130488  ]]. Action = [[ 0.04390728  0.06172484  0.12364683 -0.64156294]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1025 is [True, False, False, False, False, True]
State prediction error at timestep 1025 is tensor(2.7866e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1026. State = [[-0.3103363   0.13982935]]. Action = [[-0.20401292  0.14320189  0.07669657 -0.19473243]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1026 is [True, False, False, False, False, True]
State prediction error at timestep 1026 is tensor(4.1360e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1027. State = [[-0.31757295  0.14905122]]. Action = [[-0.1050871   0.0199174   0.19496721 -0.07181305]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1027 is [True, False, False, False, False, True]
State prediction error at timestep 1027 is tensor(9.3568e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1028. State = [[-0.32484412  0.15586402]]. Action = [[-0.0586073  -0.13121447 -0.18061253  0.92917943]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1028 is [True, False, False, False, False, True]
State prediction error at timestep 1028 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1029. State = [[-0.3302301   0.15715893]]. Action = [[-0.0758726   0.1126653  -0.1413345   0.80665994]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1029 is [True, False, False, False, False, True]
State prediction error at timestep 1029 is tensor(1.4499e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1030. State = [[-0.33557898  0.16074331]]. Action = [[ 0.1358549   0.00256747 -0.02760069 -0.30939668]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1030 is [True, False, False, False, False, True]
State prediction error at timestep 1030 is tensor(5.0230e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1031. State = [[-0.33678907  0.1619221 ]]. Action = [[ 0.02558768 -0.19102369 -0.14862767 -0.8888106 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1031 is [True, False, False, False, False, True]
State prediction error at timestep 1031 is tensor(7.1381e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1032. State = [[-0.33583078  0.15843222]]. Action = [[ 0.20481566 -0.17167233 -0.075435   -0.629628  ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1032 is [True, False, False, False, False, True]
State prediction error at timestep 1032 is tensor(2.3622e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1032 of -1
Current timestep = 1033. State = [[-0.33168188  0.15084136]]. Action = [[0.18341702 0.11079064 0.02782679 0.5908437 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1033 is [True, False, False, False, False, True]
State prediction error at timestep 1033 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1034. State = [[-0.3270289   0.14903282]]. Action = [[-0.10929564 -0.05427574  0.07689691  0.00580645]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1034 is [True, False, False, False, False, True]
State prediction error at timestep 1034 is tensor(1.8460e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of -1
Current timestep = 1035. State = [[-0.3252585   0.14780173]]. Action = [[-0.12384152  0.08948588  0.10801199  0.8628597 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1035 is [True, False, False, False, False, True]
State prediction error at timestep 1035 is tensor(2.7727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1035 of -1
Current timestep = 1036. State = [[-0.32596436  0.14826997]]. Action = [[-0.00129965 -0.1823216  -0.14111726  0.5781772 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1036 is [True, False, False, False, False, True]
State prediction error at timestep 1036 is tensor(4.3581e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1037. State = [[-0.3261794   0.14474832]]. Action = [[ 0.20800322  0.0677985  -0.09137586  0.7627909 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1037 is [True, False, False, False, False, True]
State prediction error at timestep 1037 is tensor(3.0711e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1038. State = [[-0.32403985  0.14237672]]. Action = [[-0.13925354 -0.23203392 -0.19890885 -0.21154875]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1038 is [True, False, False, False, False, True]
State prediction error at timestep 1038 is tensor(1.0223e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of -1
Current timestep = 1039. State = [[-0.32296124  0.13632001]]. Action = [[ 0.12738776 -0.02557233  0.19011986 -0.6491128 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1039 is [True, False, False, False, False, True]
State prediction error at timestep 1039 is tensor(6.7686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of -1
Current timestep = 1040. State = [[-0.32108214  0.13105077]]. Action = [[-0.05894771 -0.08233294  0.14075232  0.9896648 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1040 is [True, False, False, False, False, True]
State prediction error at timestep 1040 is tensor(7.0156e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of -1
Current timestep = 1041. State = [[-0.3193719   0.12547405]]. Action = [[ 0.10265201 -0.2122662  -0.1893918   0.312086  ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1041 is [True, False, False, False, False, True]
State prediction error at timestep 1041 is tensor(7.8354e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1042. State = [[-0.3162371   0.11611807]]. Action = [[-0.1276757   0.02936831 -0.14485367 -0.43629122]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1043. State = [[-0.3173085   0.11303485]]. Action = [[-0.24692397  0.06165218  0.01872033  0.8557787 ]]. Reward = [0.]
Curr episode timestep = 141
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[-0.32247436  0.11219614]]. Action = [[-0.1512306  -0.16759703 -0.20455839  0.07420504]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1045. State = [[-0.32823122  0.10694369]]. Action = [[-0.12678936 -0.22184968  0.15080613  0.6159713 ]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1046. State = [[-0.33453438  0.09869348]]. Action = [[ 0.20415446  0.16472208 -0.14828889  0.60020614]]. Reward = [0.]
Curr episode timestep = 144
Human Feedback received at timestep 1046 of -1
Current timestep = 1047. State = [[-0.33652216  0.09616828]]. Action = [[-0.08356106  0.17180657  0.07946274 -0.21943378]]. Reward = [0.]
Curr episode timestep = 145
Human Feedback received at timestep 1047 of -1
Current timestep = 1048. State = [[-0.33896616  0.09836021]]. Action = [[-0.21581872 -0.18556052 -0.18917061 -0.91377014]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1049. State = [[-0.34481737  0.09590417]]. Action = [[-0.13913785 -0.21526486  0.07764453 -0.03959411]]. Reward = [0.]
Curr episode timestep = 147
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.351466    0.08901557]]. Action = [[-0.18214515 -0.03143348 -0.18510704  0.04023874]]. Reward = [0.]
Curr episode timestep = 148
Human Feedback received at timestep 1050 of -1
Current timestep = 1051. State = [[-0.3589573   0.08458751]]. Action = [[-0.00289837  0.21841282  0.03260019  0.7214149 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1052. State = [[-0.36697435  0.08675336]]. Action = [[0.2151882  0.19436201 0.03958574 0.7885432 ]]. Reward = [0.]
Curr episode timestep = 150
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[-0.3684007   0.09034252]]. Action = [[ 0.15156293 -0.18653378  0.18500501  0.7476928 ]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1054. State = [[-0.3662528  0.0898003]]. Action = [[0.07773119 0.05103123 0.08728498 0.6174395 ]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1055. State = [[-0.3637085   0.08943447]]. Action = [[ 0.1518698  -0.19611512  0.2151961   0.6542572 ]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1056. State = [[-0.35877633  0.08586986]]. Action = [[ 0.16597033 -0.00825046 -0.16566718  0.3520707 ]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1057. State = [[-0.35189098  0.08165766]]. Action = [[ 0.06159389 -0.1290539   0.00599051 -0.39988732]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1058. State = [[-0.3454023   0.07576707]]. Action = [[-0.14309977 -0.14089298 -0.04359543  0.05925   ]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1059. State = [[-0.34275642  0.0694992 ]]. Action = [[ 0.05259666 -0.10362886 -0.05806607 -0.7131652 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1060. State = [[-0.34015408  0.06218949]]. Action = [[ 0.0141983   0.03802323 -0.23351742  0.25640297]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1061. State = [[-0.33885983  0.05900958]]. Action = [[-0.07462253  0.10483477 -0.24798067  0.8088467 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1062. State = [[-0.33874565  0.05948788]]. Action = [[-0.24490196  0.04720929 -0.01046199  0.03014112]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1063. State = [[-0.34269404  0.06143668]]. Action = [[-0.03755938  0.23873663 -0.24121071 -0.05675048]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1064. State = [[-0.34677988  0.06697195]]. Action = [[ 0.0351516  -0.06887473  0.09400842 -0.02554679]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1065. State = [[-0.3487738   0.06923816]]. Action = [[-0.03178824 -0.07106233  0.08709604 -0.10502291]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1066. State = [[-0.35055542  0.07003918]]. Action = [[-0.1914755   0.20495698  0.04225498  0.4018346 ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1067. State = [[-0.35510355  0.07481366]]. Action = [[-0.00265136  0.03749931 -0.19691126  0.17203128]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1068. State = [[-0.35903785  0.07925846]]. Action = [[-0.21958047  0.05099359 -0.15360925 -0.57204354]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1069. State = [[-0.3652738  0.0833347]]. Action = [[ 0.20365515 -0.24236675  0.1701293  -0.6312042 ]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1070. State = [[-0.36612293  0.08037914]]. Action = [[ 0.0580281   0.05448303 -0.16919023 -0.4720825 ]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1071. State = [[-0.36598957  0.07963697]]. Action = [[-0.1933709   0.21272057 -0.02200204 -0.63193184]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1072. State = [[-0.3695667   0.08390167]]. Action = [[ 0.01404315 -0.03194723 -0.22262727  0.75291395]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1073. State = [[-0.37153324  0.08610692]]. Action = [[-0.19660974 -0.15567818 -0.12306744  0.2448206 ]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1074. State = [[-0.37588847  0.08437704]]. Action = [[-0.14845805 -0.06174177  0.14856282  0.5282073 ]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1075. State = [[-0.38104048  0.08294196]]. Action = [[ 0.08026767  0.15336737 -0.12750131 -0.7843657 ]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1076. State = [[-0.38377357  0.08589305]]. Action = [[ 0.20862514 -0.00574289 -0.15357108 -0.1973049 ]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1077. State = [[-0.38471898  0.08857859]]. Action = [[-0.22734311 -0.08654766  0.22544238 -0.5574061 ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1078. State = [[-0.3855263   0.08990459]]. Action = [[ 0.12885243 -0.23882258 -0.15914589 -0.24773288]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1079. State = [[-0.38458353  0.08770869]]. Action = [[ 0.19524223 -0.18243842 -0.15730481  0.8826772 ]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1080. State = [[-0.381526    0.08442468]]. Action = [[ 0.12283391  0.14696938 -0.06713203  0.2932055 ]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1081. State = [[-0.37734845  0.08405894]]. Action = [[ 0.21533722 -0.0472568   0.2264882  -0.61482203]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1082. State = [[-0.37089095  0.08026306]]. Action = [[-0.17142798  0.20299742  0.07008642  0.26318765]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1083. State = [[-0.36568975  0.07775354]]. Action = [[ 0.19937682 -0.22715154 -0.17876993  0.8854475 ]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1084. State = [[-0.35956714  0.07350708]]. Action = [[ 0.00550348  0.0225046  -0.14257422  0.558882  ]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1085. State = [[-0.35443458  0.06889596]]. Action = [[ 0.03473431  0.1363607  -0.24054837 -0.42546177]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1086. State = [[-0.34814173  0.06648996]]. Action = [[0.07069138 0.11082494 0.11638659 0.74332094]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1087. State = [[-0.34107682  0.06459893]]. Action = [[ 0.12913543 -0.22223707  0.13238046  0.90179133]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1088. State = [[-0.33255687  0.05841066]]. Action = [[ 0.12973195 -0.08026275 -0.16620122  0.835011  ]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1089. State = [[-0.3235786   0.05201923]]. Action = [[-0.21476117  0.16319728 -0.21568717 -0.8875042 ]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1090. State = [[-0.32180828  0.05178415]]. Action = [[0.17102593 0.00512582 0.19255799 0.06555223]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1091. State = [[-0.31827602  0.05162143]]. Action = [[-0.08567344 -0.1779104   0.22620144  0.0202738 ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1092. State = [[-0.31656754  0.04847644]]. Action = [[ 0.20553708 -0.07644513  0.01312479  0.5044441 ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1093. State = [[-0.31175414  0.04347438]]. Action = [[-0.06275541 -0.12187764  0.06470484 -0.8285341 ]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1094. State = [[-0.3083551   0.03809544]]. Action = [[-0.00649984  0.12418324  0.14474848 -0.9731459 ]]. Reward = [0.]
Curr episode timestep = 192
Current timestep = 1095. State = [[-0.3075585   0.03672365]]. Action = [[-0.23520993 -0.00144182 -0.05848286 -0.43498218]]. Reward = [0.]
Curr episode timestep = 193
Current timestep = 1096. State = [[-0.3089485   0.03666603]]. Action = [[0.09839106 0.05915153 0.05425924 0.17568183]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 1097. State = [[-0.30922204  0.03737601]]. Action = [[-0.18506469  0.21902853  0.24243993 -0.7222631 ]]. Reward = [0.]
Curr episode timestep = 195
Current timestep = 1098. State = [[-0.31233874  0.04327492]]. Action = [[0.15748951 0.11289445 0.21903798 0.6475878 ]]. Reward = [0.]
Curr episode timestep = 196
Current timestep = 1099. State = [[-0.3137238   0.04909274]]. Action = [[-0.08283804  0.17367008 -0.18597448  0.5285387 ]]. Reward = [0.]
Curr episode timestep = 197
Current timestep = 1100. State = [[-0.31632888  0.05624489]]. Action = [[-0.00736228 -0.18694459  0.09832034  0.1914798 ]]. Reward = [0.]
Curr episode timestep = 198
Current timestep = 1101. State = [[-0.31722152  0.05760374]]. Action = [[-0.23830251 -0.00725979  0.06859899  0.48539233]]. Reward = [0.]
Curr episode timestep = 199
Current timestep = 1102. State = [[-0.3204106   0.05881376]]. Action = [[-0.01325585 -0.01022626 -0.13103297 -0.57765543]]. Reward = [0.]
Curr episode timestep = 200
Current timestep = 1103. State = [[-0.3227352   0.05984562]]. Action = [[-0.08410589  0.00552711 -0.19479798 -0.78579587]]. Reward = [0.]
Curr episode timestep = 201
Current timestep = 1104. State = [[-0.32530382  0.06097756]]. Action = [[ 0.23928362 -0.13463666 -0.2442316   0.6931988 ]]. Reward = [0.]
Curr episode timestep = 202
Current timestep = 1105. State = [[-0.32602787  0.06110584]]. Action = [[-0.18398765 -0.21555817  0.23742521 -0.9915418 ]]. Reward = [0.]
Curr episode timestep = 203
Current timestep = 1106. State = [[-0.32706735  0.05756022]]. Action = [[ 0.09530869  0.06254575 -0.15887341 -0.8710747 ]]. Reward = [0.]
Curr episode timestep = 204
Current timestep = 1107. State = [[-0.3276974  0.0557888]]. Action = [[-0.1595479   0.02522233  0.11480939  0.8763907 ]]. Reward = [0.]
Curr episode timestep = 205
Current timestep = 1108. State = [[-0.32978576  0.05552106]]. Action = [[-0.20480494  0.20798212 -0.0048866  -0.81973046]]. Reward = [0.]
Curr episode timestep = 206
Current timestep = 1109. State = [[-0.33574668  0.05813342]]. Action = [[-0.11233369 -0.07384792 -0.04545945  0.66152525]]. Reward = [0.]
Curr episode timestep = 207
Current timestep = 1110. State = [[-0.3430563   0.05901229]]. Action = [[-0.13052581  0.20478964 -0.20674317  0.72235143]]. Reward = [0.]
Curr episode timestep = 208
Current timestep = 1111. State = [[-0.35140812  0.06265082]]. Action = [[ 0.10823423  0.05767965 -0.15558107 -0.3705535 ]]. Reward = [0.]
Curr episode timestep = 209
Current timestep = 1112. State = [[-0.35502923  0.06678847]]. Action = [[ 0.2241888  -0.18091114  0.07694164 -0.32320786]]. Reward = [0.]
Curr episode timestep = 210
Current timestep = 1113. State = [[-0.35613564  0.06888049]]. Action = [[-0.06451878  0.24143845 -0.00600101 -0.3788104 ]]. Reward = [0.]
Curr episode timestep = 211
Current timestep = 1114. State = [[-0.3575481   0.07126248]]. Action = [[ 0.07596028 -0.08687563  0.03836486 -0.18823975]]. Reward = [0.]
Curr episode timestep = 212
Current timestep = 1115. State = [[-0.3581171   0.07288164]]. Action = [[-0.08864941  0.2366896   0.12088957  0.06117475]]. Reward = [0.]
Curr episode timestep = 213
Current timestep = 1116. State = [[-0.35921088  0.07607811]]. Action = [[-0.13289566  0.1205951  -0.11912808 -0.57647055]]. Reward = [0.]
Curr episode timestep = 214
Current timestep = 1117. State = [[-0.3615157   0.08000526]]. Action = [[ 0.01587409 -0.03779835  0.23339903  0.77108955]]. Reward = [0.]
Curr episode timestep = 215
Current timestep = 1118. State = [[-0.36403686  0.08374925]]. Action = [[-0.1736847  -0.09150995  0.1684019   0.10511863]]. Reward = [0.]
Curr episode timestep = 216
Current timestep = 1119. State = [[-0.36683735  0.08591025]]. Action = [[ 0.06010076 -0.211401    0.23134947 -0.08323902]]. Reward = [0.]
Curr episode timestep = 217
Current timestep = 1120. State = [[-0.3684067   0.08459224]]. Action = [[-0.22257675  0.16027224  0.10553959 -0.5618403 ]]. Reward = [0.]
Curr episode timestep = 218
Current timestep = 1121. State = [[-0.3720704   0.08548824]]. Action = [[-0.09811935  0.0774768  -0.17377706 -0.61304134]]. Reward = [0.]
Curr episode timestep = 219
Current timestep = 1122. State = [[-0.3756503   0.08769679]]. Action = [[-0.05810022  0.03850898  0.2244165  -0.79645365]]. Reward = [0.]
Curr episode timestep = 220
Current timestep = 1123. State = [[-0.3795664   0.09058021]]. Action = [[-0.17407477 -0.18832116 -0.03892338  0.1933527 ]]. Reward = [0.]
Curr episode timestep = 221
Current timestep = 1124. State = [[-0.3827821   0.09288567]]. Action = [[-0.20122166 -0.0639483  -0.12359038 -0.45136964]]. Reward = [0.]
Curr episode timestep = 222
Current timestep = 1125. State = [[-0.38565537  0.09523103]]. Action = [[ 0.07336295 -0.09566605 -0.10689336 -0.2436248 ]]. Reward = [0.]
Curr episode timestep = 223
Current timestep = 1126. State = [[-0.38709155  0.09578619]]. Action = [[ 0.03814891 -0.13397591 -0.15338041  0.03252602]]. Reward = [0.]
Curr episode timestep = 224
Current timestep = 1127. State = [[-0.38730922  0.09467179]]. Action = [[ 0.24756855 -0.03904267 -0.02421127 -0.94117576]]. Reward = [0.]
Curr episode timestep = 225
Current timestep = 1128. State = [[-0.3854148   0.09375956]]. Action = [[ 0.14295381 -0.19446485  0.1376358   0.11604178]]. Reward = [0.]
Curr episode timestep = 226
Current timestep = 1129. State = [[-0.38286963  0.09078099]]. Action = [[ 0.05278754  0.03795227 -0.16998719  0.32939982]]. Reward = [0.]
Curr episode timestep = 227
Current timestep = 1130. State = [[-0.38038173  0.088248  ]]. Action = [[-0.05825549 -0.19339933  0.13313699 -0.3858798 ]]. Reward = [0.]
Curr episode timestep = 228
Current timestep = 1131. State = [[-0.379592    0.08478358]]. Action = [[-0.04959628  0.14633411 -0.22258735 -0.52697325]]. Reward = [0.]
Curr episode timestep = 229
Current timestep = 1132. State = [[-0.37959695  0.08424406]]. Action = [[-0.03455222  0.06765282  0.12028795  0.37486148]]. Reward = [0.]
Curr episode timestep = 230
Current timestep = 1133. State = [[-0.37962368  0.08423681]]. Action = [[-0.06425464 -0.02334253  0.08350122  0.63224936]]. Reward = [0.]
Curr episode timestep = 231
Current timestep = 1134. State = [[-0.37958986  0.08425704]]. Action = [[ 0.20573664 -0.03341912  0.17236724 -0.9191421 ]]. Reward = [0.]
Curr episode timestep = 232
Current timestep = 1135. State = [[-0.37878865  0.08417722]]. Action = [[ 0.0753018  -0.19004072  0.10561624 -0.00639188]]. Reward = [0.]
Curr episode timestep = 233
Current timestep = 1136. State = [[-0.3774358   0.08191387]]. Action = [[ 0.20939341 -0.08778733  0.05975646  0.33785748]]. Reward = [0.]
Curr episode timestep = 234
Current timestep = 1137. State = [[-0.37371686  0.0796579 ]]. Action = [[-0.04821941  0.16834003  0.08568695  0.07785654]]. Reward = [0.]
Curr episode timestep = 235
Current timestep = 1138. State = [[-0.37091643  0.07933721]]. Action = [[-0.10384466 -0.0850005   0.04629645 -0.6477973 ]]. Reward = [0.]
Curr episode timestep = 236
Current timestep = 1139. State = [[-0.37036085  0.07744341]]. Action = [[-0.09950349 -0.12408909 -0.1122756   0.54338026]]. Reward = [0.]
Curr episode timestep = 237
Current timestep = 1140. State = [[-0.37026712  0.07424862]]. Action = [[-0.2109983  -0.17152146 -0.10194775 -0.11485398]]. Reward = [0.]
Curr episode timestep = 238
Current timestep = 1141. State = [[-0.37214506  0.06879763]]. Action = [[-0.00653557 -0.05886585  0.17697221 -0.31275183]]. Reward = [0.]
Curr episode timestep = 239
Current timestep = 1142. State = [[-0.37344405  0.06395318]]. Action = [[ 0.06237298 -0.12389228  0.10860059  0.37025762]]. Reward = [0.]
Curr episode timestep = 240
Current timestep = 1143. State = [[-0.37346286  0.05983011]]. Action = [[ 0.15677083 -0.22184971  0.07885149  0.07546175]]. Reward = [0.]
Curr episode timestep = 241
Current timestep = 1144. State = [[-0.37329915  0.0541435 ]]. Action = [[ 0.03337327  0.19679749 -0.105692    0.97109354]]. Reward = [0.]
Curr episode timestep = 242
Current timestep = 1145. State = [[-0.3724896   0.05252017]]. Action = [[-0.09701005 -0.06066346 -0.14994277  0.77649   ]]. Reward = [0.]
Curr episode timestep = 243
Current timestep = 1146. State = [[-0.37256283  0.05079752]]. Action = [[-0.1620029   0.11495209 -0.0346092  -0.30419087]]. Reward = [0.]
Curr episode timestep = 244
Current timestep = 1147. State = [[-0.37321183  0.05022575]]. Action = [[ 0.15573657 -0.22282529 -0.21993563 -0.32967448]]. Reward = [0.]
Curr episode timestep = 245
Current timestep = 1148. State = [[-0.37311363  0.04774187]]. Action = [[ 0.19774574 -0.22033662 -0.21845897  0.2940737 ]]. Reward = [0.]
Curr episode timestep = 246
Current timestep = 1149. State = [[-0.3720475   0.04273197]]. Action = [[ 0.07640308 -0.2490764   0.05011153  0.66510844]]. Reward = [0.]
Curr episode timestep = 247
Current timestep = 1150. State = [[-0.37049884  0.0372654 ]]. Action = [[0.0286358  0.15854508 0.11558396 0.8620503 ]]. Reward = [0.]
Curr episode timestep = 248
Current timestep = 1151. State = [[-0.36806977  0.03465132]]. Action = [[ 0.18556714 -0.23111732 -0.09882224 -0.5193978 ]]. Reward = [0.]
Curr episode timestep = 249
Current timestep = 1152. State = [[-0.36057734  0.02557779]]. Action = [[0.17956504 0.21564549 0.23997092 0.77835107]]. Reward = [0.]
Curr episode timestep = 250
Current timestep = 1153. State = [[-0.35334784  0.02329746]]. Action = [[-0.12920557  0.08899587  0.09792292  0.8654009 ]]. Reward = [0.]
Curr episode timestep = 251
Current timestep = 1154. State = [[-0.34886354  0.02217493]]. Action = [[ 0.24535125  0.13204312 -0.08669296 -0.755141  ]]. Reward = [0.]
Curr episode timestep = 252
Current timestep = 1155. State = [[-0.3418386   0.02072546]]. Action = [[ 0.17685717  0.03382319 -0.20569734 -0.48525608]]. Reward = [0.]
Curr episode timestep = 253
Current timestep = 1156. State = [[-0.33460853  0.02222918]]. Action = [[ 0.22396731 -0.19924532  0.01293081 -0.5267767 ]]. Reward = [0.]
Curr episode timestep = 254
Current timestep = 1157. State = [[-0.3271301   0.01910578]]. Action = [[ 0.18523508  0.11816007  0.04378614 -0.3810526 ]]. Reward = [0.]
Curr episode timestep = 255
Current timestep = 1158. State = [[-0.31602487  0.01523412]]. Action = [[ 0.04873514 -0.22999182  0.17663676  0.18168461]]. Reward = [0.]
Curr episode timestep = 256
Current timestep = 1159. State = [[-0.30758384  0.01592313]]. Action = [[-0.0152133   0.21957216 -0.03148673 -0.7967941 ]]. Reward = [0.]
Curr episode timestep = 257
Current timestep = 1160. State = [[-0.3012523   0.01500599]]. Action = [[-0.16214766  0.00219929  0.1850195   0.6435807 ]]. Reward = [0.]
Curr episode timestep = 258
Current timestep = 1161. State = [[-0.29794502  0.01387702]]. Action = [[ 0.10449848  0.01314855  0.05526438 -0.05877167]]. Reward = [0.]
Curr episode timestep = 259
Current timestep = 1162. State = [[-0.29285222  0.01267738]]. Action = [[ 0.15396482  0.09428561 -0.22815575 -0.04112828]]. Reward = [0.]
Curr episode timestep = 260
Current timestep = 1163. State = [[-0.28704405  0.01395204]]. Action = [[ 0.14051723  0.01218539 -0.19128445 -0.5729993 ]]. Reward = [0.]
Curr episode timestep = 261
Current timestep = 1164. State = [[-0.28082696  0.01439979]]. Action = [[-0.06953418 -0.22929932  0.01766089 -0.32644296]]. Reward = [0.]
Curr episode timestep = 262
Current timestep = 1165. State = [[-0.2784176   0.01141148]]. Action = [[-0.11192323  0.00809252 -0.19839351 -0.46188337]]. Reward = [0.]
Curr episode timestep = 263
Current timestep = 1166. State = [[-0.27647847  0.00798248]]. Action = [[ 0.13619974 -0.19591647  0.02979207 -0.67550707]]. Reward = [0.]
Curr episode timestep = 264
Current timestep = 1167. State = [[-0.27415568  0.00398822]]. Action = [[ 0.20425388  0.16759932 -0.03127566 -0.48804188]]. Reward = [0.]
Curr episode timestep = 265
Current timestep = 1168. State = [[-0.26840204  0.00130697]]. Action = [[ 0.05243456  0.23145652  0.18735567 -0.16810119]]. Reward = [0.]
Curr episode timestep = 266
Current timestep = 1169. State = [[-0.26245177  0.00383356]]. Action = [[ 0.23802921 -0.08877009  0.2472862   0.64924026]]. Reward = [0.]
Curr episode timestep = 267
Current timestep = 1170. State = [[-0.2529872   0.00222306]]. Action = [[-0.15275681 -0.11925177 -0.18445037  0.19083679]]. Reward = [0.]
Curr episode timestep = 268
Current timestep = 1171. State = [[-2.4816689e-01  3.4003810e-05]]. Action = [[ 0.04114288 -0.18397751  0.2001062  -0.970835  ]]. Reward = [0.]
Curr episode timestep = 269
Current timestep = 1172. State = [[-0.24435507 -0.0053885 ]]. Action = [[-0.20138022  0.18168154  0.19037864 -0.6947211 ]]. Reward = [0.]
Curr episode timestep = 270
Current timestep = 1173. State = [[-0.24367276 -0.01009569]]. Action = [[-0.24228308  0.01720145  0.12720212 -0.67739165]]. Reward = [0.]
Curr episode timestep = 271
Current timestep = 1174. State = [[-0.24597885 -0.01286123]]. Action = [[ 0.20273346  0.01977402 -0.06346327  0.10391712]]. Reward = [0.]
Curr episode timestep = 272
Current timestep = 1175. State = [[-0.24472381 -0.0137281 ]]. Action = [[ 0.01006669 -0.22193487 -0.15741798  0.01120484]]. Reward = [0.]
Curr episode timestep = 273
Current timestep = 1176. State = [[-0.24402447 -0.01578561]]. Action = [[ 0.06157079 -0.11964911  0.12401417 -0.8918601 ]]. Reward = [0.]
Curr episode timestep = 274
Current timestep = 1177. State = [[-0.24362466 -0.01929955]]. Action = [[-0.11450352 -0.03635919  0.08086669 -0.30864626]]. Reward = [0.]
Curr episode timestep = 275
Current timestep = 1178. State = [[-0.24397439 -0.0218839 ]]. Action = [[-0.00652686 -0.12944981 -0.15630034 -0.17077887]]. Reward = [0.]
Curr episode timestep = 276
Current timestep = 1179. State = [[-0.24450341 -0.02267889]]. Action = [[-0.23179512 -0.21993037  0.05886084 -0.32724422]]. Reward = [0.]
Curr episode timestep = 277
Current timestep = 1180. State = [[-0.24825716 -0.02655126]]. Action = [[ 0.08244956 -0.02459182  0.1287176  -0.8028266 ]]. Reward = [0.]
Curr episode timestep = 278
Current timestep = 1181. State = [[-0.25113434 -0.031402  ]]. Action = [[-0.12651348  0.16947007 -0.09780049 -0.7188347 ]]. Reward = [0.]
Curr episode timestep = 279
Current timestep = 1182. State = [[-0.25495845 -0.03402141]]. Action = [[ 0.13592577 -0.23510085  0.00447515 -0.08539128]]. Reward = [0.]
Curr episode timestep = 280
Current timestep = 1183. State = [[-0.25527117 -0.0388972 ]]. Action = [[-0.17327641  0.0172666  -0.13437903 -0.9833841 ]]. Reward = [0.]
Curr episode timestep = 281
Current timestep = 1184. State = [[-0.25805342 -0.0382429 ]]. Action = [[-0.04917064 -0.16076884  0.10036609  0.24030471]]. Reward = [0.]
Curr episode timestep = 282
Current timestep = 1185. State = [[-0.2613644  -0.04268774]]. Action = [[-0.19616863  0.17503262 -0.23191467 -0.44529903]]. Reward = [0.]
Curr episode timestep = 283
Current timestep = 1186. State = [[-0.26603124 -0.04146635]]. Action = [[-0.14609101  0.00807217 -0.11558309  0.49791813]]. Reward = [0.]
Curr episode timestep = 284
Current timestep = 1187. State = [[-0.27294296 -0.04411303]]. Action = [[0.22295964 0.16273159 0.18880767 0.9636904 ]]. Reward = [0.]
Curr episode timestep = 285
Current timestep = 1188. State = [[-0.27427134 -0.04132992]]. Action = [[-0.04546243  0.08841348 -0.18110554 -0.05771983]]. Reward = [0.]
Curr episode timestep = 286
Current timestep = 1189. State = [[-0.2758037  -0.03856957]]. Action = [[ 0.20824003  0.2072072  -0.12554526  0.5714735 ]]. Reward = [0.]
Curr episode timestep = 287
Current timestep = 1190. State = [[-0.27336606 -0.03748593]]. Action = [[ 0.23573321 -0.08344644 -0.02754799 -0.71871364]]. Reward = [0.]
Curr episode timestep = 288
Current timestep = 1191. State = [[-0.26810208 -0.03590292]]. Action = [[ 0.13260272  0.06674281 -0.04259259 -0.27290773]]. Reward = [0.]
Curr episode timestep = 289
Current timestep = 1192. State = [[-0.26247415 -0.03606608]]. Action = [[ 0.17167175 -0.09016931  0.2242916   0.62055635]]. Reward = [0.]
Curr episode timestep = 290
Current timestep = 1193. State = [[-0.25569573 -0.03658228]]. Action = [[-0.19764534 -0.05053529 -0.08223915 -0.367342  ]]. Reward = [0.]
Curr episode timestep = 291
Current timestep = 1194. State = [[-0.25295755 -0.03610704]]. Action = [[ 0.15430427 -0.12660226 -0.03690618 -0.1631282 ]]. Reward = [0.]
Curr episode timestep = 292
Current timestep = 1195. State = [[-0.24975537 -0.03709905]]. Action = [[-0.09987071  0.23485649  0.1505273   0.93504405]]. Reward = [0.]
Curr episode timestep = 293
Current timestep = 1196. State = [[-0.24768679 -0.03797686]]. Action = [[-0.02944873  0.09363127  0.02869087  0.03447509]]. Reward = [0.]
Curr episode timestep = 294
Current timestep = 1197. State = [[-0.24581602 -0.03804019]]. Action = [[ 0.24189425 -0.13277851 -0.11319819 -0.89283437]]. Reward = [0.]
Curr episode timestep = 295
Current timestep = 1198. State = [[-0.24126786 -0.03867213]]. Action = [[ 0.06722599 -0.2485387   0.17719284  0.7958772 ]]. Reward = [0.]
Curr episode timestep = 296
Current timestep = 1199. State = [[-0.23714055 -0.04073781]]. Action = [[ 0.02475026 -0.13954367  0.05313748 -0.3273177 ]]. Reward = [0.]
Curr episode timestep = 297
Current timestep = 1200. State = [[-0.23411596 -0.04544613]]. Action = [[ 0.16691276  0.10369891  0.04335913 -0.67519945]]. Reward = [0.]
Curr episode timestep = 298
Current timestep = 1201. State = [[-0.22921716 -0.05079596]]. Action = [[-0.22264878  0.07720792  0.23681307 -0.0625512 ]]. Reward = [0.]
Curr episode timestep = 299
Current timestep = 1202. State = [[-0.22759666 -0.05574489]]. Action = [[-0.10046843 -0.00244787  0.1750865   0.02683806]]. Reward = [0.]
Curr episode timestep = 300
Current timestep = 1203. State = [[-0.22782797 -0.05789195]]. Action = [[ 0.0033755  -0.17895487 -0.04568064  0.01663733]]. Reward = [0.]
Curr episode timestep = 301
Current timestep = 1204. State = [[-0.22868682 -0.06126127]]. Action = [[-0.10921794  0.06499639  0.2327134  -0.92313874]]. Reward = [0.]
Curr episode timestep = 302
Current timestep = 1205. State = [[-0.22982034 -0.06016044]]. Action = [[-0.04250354 -0.14592734  0.18395191 -0.95380723]]. Reward = [0.]
Curr episode timestep = 303
Current timestep = 1206. State = [[-0.23193863 -0.0613855 ]]. Action = [[-0.22249837 -0.02004784 -0.22311683 -0.6612798 ]]. Reward = [0.]
Curr episode timestep = 304
Current timestep = 1207. State = [[-0.23608656 -0.06138219]]. Action = [[ 0.01636243 -0.20618753 -0.04676835  0.38354123]]. Reward = [0.]
Curr episode timestep = 305
Current timestep = 1208. State = [[-0.2379835 -0.0642287]]. Action = [[-0.01469253 -0.06195475  0.0287562   0.55235016]]. Reward = [0.]
Curr episode timestep = 306
Current timestep = 1209. State = [[-0.23999397 -0.0652415 ]]. Action = [[ 0.10714173 -0.14244428  0.20292234  0.9370556 ]]. Reward = [0.]
Curr episode timestep = 307
Current timestep = 1210. State = [[-0.24012053 -0.0699003 ]]. Action = [[ 0.0938676  -0.15397614 -0.21092126 -0.08088005]]. Reward = [0.]
Curr episode timestep = 308
Current timestep = 1211. State = [[-0.23895599 -0.07478467]]. Action = [[-0.10604648  0.0953548  -0.12787989 -0.16546631]]. Reward = [0.]
Curr episode timestep = 309
Current timestep = 1212. State = [[-0.23973586 -0.07724678]]. Action = [[-0.10136479  0.1250493  -0.22247298 -0.86687094]]. Reward = [0.]
Curr episode timestep = 310
Current timestep = 1213. State = [[-0.24121498 -0.07641389]]. Action = [[-0.10937774  0.03763711  0.1996184   0.936532  ]]. Reward = [0.]
Curr episode timestep = 311
Current timestep = 1214. State = [[-0.24539164 -0.07538838]]. Action = [[-0.08911824 -0.06935142  0.21693671 -0.6534465 ]]. Reward = [0.]
Curr episode timestep = 312
Current timestep = 1215. State = [[-0.24878538 -0.07859229]]. Action = [[ 0.0367623   0.16556883 -0.05338006 -0.6364914 ]]. Reward = [0.]
Curr episode timestep = 313
Current timestep = 1216. State = [[-0.25216255 -0.07655434]]. Action = [[-0.18140426 -0.24238016  0.1296956  -0.7136718 ]]. Reward = [0.]
Curr episode timestep = 314
Current timestep = 1217. State = [[-0.25567028 -0.07656591]]. Action = [[ 0.12059578  0.13665944 -0.01311539 -0.24901217]]. Reward = [0.]
Curr episode timestep = 315
Current timestep = 1218. State = [[-0.25893787 -0.077178  ]]. Action = [[ 0.04508424 -0.14483954 -0.05312051  0.3722309 ]]. Reward = [0.]
Curr episode timestep = 316
Current timestep = 1219. State = [[-0.26012853 -0.0764779 ]]. Action = [[ 0.1938344  -0.0068745  -0.12292877  0.206043  ]]. Reward = [0.]
Curr episode timestep = 317
Current timestep = 1220. State = [[-0.2591551  -0.07895644]]. Action = [[ 0.15824512 -0.20380193 -0.02562037 -0.5747641 ]]. Reward = [0.]
Curr episode timestep = 318
Current timestep = 1221. State = [[-0.25374255 -0.08374003]]. Action = [[ 0.23584181 -0.10787559 -0.02161776  0.32783663]]. Reward = [0.]
Curr episode timestep = 319
Current timestep = 1222. State = [[-0.2475505  -0.08882823]]. Action = [[-0.23060662  0.02778304 -0.04539515 -0.9549399 ]]. Reward = [0.]
Curr episode timestep = 320
Current timestep = 1223. State = [[-0.2457938  -0.09364158]]. Action = [[-0.1386725   0.1347481  -0.20454739  0.3363706 ]]. Reward = [0.]
Curr episode timestep = 321
Current timestep = 1224. State = [[-0.24592556 -0.09389126]]. Action = [[ 0.18654984 -0.12495875  0.13769281  0.1550498 ]]. Reward = [0.]
Curr episode timestep = 322
Current timestep = 1225. State = [[-0.24475196 -0.09573841]]. Action = [[-0.03475252  0.2312321  -0.04465136  0.61000013]]. Reward = [0.]
Curr episode timestep = 323
Current timestep = 1226. State = [[-0.2440603  -0.09830485]]. Action = [[-0.18873261 -0.21986426  0.07761222 -0.32524943]]. Reward = [0.]
Curr episode timestep = 324
Current timestep = 1227. State = [[-0.2454775  -0.10088602]]. Action = [[-0.04869482  0.11160797 -0.13200207 -0.87830156]]. Reward = [0.]
Curr episode timestep = 325
Current timestep = 1228. State = [[-0.24715264 -0.09871313]]. Action = [[-0.2150205  -0.00830826  0.18699503 -0.9779688 ]]. Reward = [0.]
Curr episode timestep = 326
Current timestep = 1229. State = [[-0.2509474  -0.09993754]]. Action = [[-0.19948836  0.09402725 -0.16735958 -0.04690355]]. Reward = [0.]
Curr episode timestep = 327
Current timestep = 1230. State = [[-0.25764903 -0.09728412]]. Action = [[-0.17763829  0.23367512  0.0341433   0.04411232]]. Reward = [0.]
Curr episode timestep = 328
Current timestep = 1231. State = [[-0.2657593  -0.09226506]]. Action = [[0.0654296  0.00663948 0.06244361 0.9206357 ]]. Reward = [0.]
Curr episode timestep = 329
Current timestep = 1232. State = [[-0.2696647  -0.08487058]]. Action = [[-0.12774207  0.02690119 -0.16120285 -0.9419967 ]]. Reward = [0.]
Curr episode timestep = 330
Current timestep = 1233. State = [[-0.2745714  -0.07638305]]. Action = [[ 0.00257039  0.02369004 -0.20164877 -0.96866786]]. Reward = [0.]
Curr episode timestep = 331
Current timestep = 1234. State = [[-0.27902067 -0.06931526]]. Action = [[ 0.19249535 -0.13964085  0.17491472  0.34925508]]. Reward = [0.]
Curr episode timestep = 332
Current timestep = 1235. State = [[-0.28047794 -0.06376623]]. Action = [[-0.2259619   0.16844094  0.07720411 -0.37505144]]. Reward = [0.]
Curr episode timestep = 333
Current timestep = 1236. State = [[-0.2844588  -0.06020238]]. Action = [[-0.03346354 -0.00201641 -0.22023559 -0.97580636]]. Reward = [0.]
Curr episode timestep = 334
Current timestep = 1237. State = [[-0.28814182 -0.057183  ]]. Action = [[ 0.13471967 -0.04436797 -0.16842976 -0.5127977 ]]. Reward = [0.]
Curr episode timestep = 335
Current timestep = 1238. State = [[-0.28900635 -0.05392425]]. Action = [[ 0.04428154 -0.15187486 -0.19673613  0.01169431]]. Reward = [0.]
Curr episode timestep = 336
Current timestep = 1239. State = [[-0.2892934  -0.05316623]]. Action = [[ 0.21888545 -0.03539282  0.04644105 -0.9864731 ]]. Reward = [0.]
Curr episode timestep = 337
Current timestep = 1240. State = [[-0.28663728 -0.05210206]]. Action = [[-0.11383376 -0.2035548   0.1697573  -0.186687  ]]. Reward = [0.]
Curr episode timestep = 338
Current timestep = 1241. State = [[-0.28658485 -0.05284695]]. Action = [[ 0.09219792  0.1142945  -0.2014211   0.8376988 ]]. Reward = [0.]
Curr episode timestep = 339
Current timestep = 1242. State = [[-0.28561065 -0.05225667]]. Action = [[-0.23108518  0.05952916 -0.04661064  0.4762255 ]]. Reward = [0.]
Curr episode timestep = 340
Current timestep = 1243. State = [[-0.28671902 -0.05273529]]. Action = [[-0.21694663  0.11960322  0.24769199  0.8156488 ]]. Reward = [0.]
Curr episode timestep = 341
Current timestep = 1244. State = [[-0.29192802 -0.05385718]]. Action = [[-0.23823373  0.19981676  0.06076649 -0.8475804 ]]. Reward = [0.]
Curr episode timestep = 342
Current timestep = 1245. State = [[-0.30142775 -0.05365192]]. Action = [[ 0.08892375  0.05349481 -0.21011883  0.8869972 ]]. Reward = [0.]
Curr episode timestep = 343
Current timestep = 1246. State = [[-0.30598196 -0.05083877]]. Action = [[-1.5794344e-01  6.1381340e-02  7.8681111e-04 -8.5335004e-01]]. Reward = [0.]
Curr episode timestep = 344
Current timestep = 1247. State = [[-0.3114021  -0.04696938]]. Action = [[-0.05328566 -0.05060995  0.1702447   0.25229025]]. Reward = [0.]
Curr episode timestep = 345
Current timestep = 1248. State = [[-0.31684983 -0.04306253]]. Action = [[-0.1273714   0.15683547  0.01091287  0.8341677 ]]. Reward = [0.]
Curr episode timestep = 346
Current timestep = 1249. State = [[-0.32499576 -0.03934447]]. Action = [[-0.1044834   0.0477463   0.12654474 -0.40325367]]. Reward = [0.]
Curr episode timestep = 347
Current timestep = 1250. State = [[-0.33425727 -0.03500127]]. Action = [[-0.24324949  0.14305711  0.12379262 -0.93950135]]. Reward = [0.]
Curr episode timestep = 348
Current timestep = 1251. State = [[-0.3447536  -0.03115991]]. Action = [[ 0.01658031 -0.22660382  0.21859139 -0.59710616]]. Reward = [0.]
Curr episode timestep = 349
Current timestep = 1252. State = [[-0.35324633 -0.02681576]]. Action = [[ 0.104628    0.08912265  0.20033857 -0.891786  ]]. Reward = [0.]
Curr episode timestep = 350
Current timestep = 1253. State = [[-0.35768598 -0.0210237 ]]. Action = [[-0.18475874  0.01971829 -0.23130126 -0.2032566 ]]. Reward = [0.]
Curr episode timestep = 351
Current timestep = 1254. State = [[-0.3633558  -0.01650696]]. Action = [[ 0.03047585 -0.21058676 -0.01458344 -0.15496719]]. Reward = [0.]
Curr episode timestep = 352
Current timestep = 1255. State = [[-0.3675193  -0.01316126]]. Action = [[ 0.01794463 -0.09085116 -0.03761229  0.6779945 ]]. Reward = [0.]
Curr episode timestep = 353
Current timestep = 1256. State = [[-0.37047708 -0.01071889]]. Action = [[-0.12103146 -0.20891711 -0.1070956  -0.56363696]]. Reward = [0.]
Curr episode timestep = 354
Current timestep = 1257. State = [[-0.37521017 -0.00983985]]. Action = [[0.09508175 0.09196055 0.19910026 0.48831296]]. Reward = [0.]
Curr episode timestep = 355
Current timestep = 1258. State = [[-0.37705672 -0.00728112]]. Action = [[-0.16540487 -0.10147034 -0.15095928 -0.20605683]]. Reward = [0.]
Curr episode timestep = 356
Current timestep = 1259. State = [[-0.38035    -0.00490336]]. Action = [[ 0.12488067  0.19384474 -0.14575158  0.5547328 ]]. Reward = [0.]
Curr episode timestep = 357
Current timestep = 1260. State = [[-0.3808992  -0.00125757]]. Action = [[-0.24375874 -0.04695559 -0.08666563 -0.15106279]]. Reward = [0.]
Curr episode timestep = 358
Current timestep = 1261. State = [[-0.38145924  0.00169003]]. Action = [[ 0.07719058  0.23003948  0.14744163 -0.74968314]]. Reward = [0.]
Curr episode timestep = 359
Current timestep = 1262. State = [[-0.38181543  0.00457377]]. Action = [[-0.11564291 -0.00028469  0.00779545  0.10959363]]. Reward = [0.]
Curr episode timestep = 360
Current timestep = 1263. State = [[-0.3823258   0.00635638]]. Action = [[-0.17456774 -0.10204974 -0.10571353  0.68262434]]. Reward = [0.]
Curr episode timestep = 361
Current timestep = 1264. State = [[-0.38292098  0.00762029]]. Action = [[-0.23902676  0.17585883 -0.06998737 -0.7360661 ]]. Reward = [0.]
Curr episode timestep = 362
Current timestep = 1265. State = [[-0.3833441   0.00842946]]. Action = [[-0.04081158  0.2134009  -0.1269802  -0.36499435]]. Reward = [0.]
Curr episode timestep = 363
Current timestep = 1266. State = [[-0.38386413  0.00940318]]. Action = [[-0.16471799  0.23278338  0.19856387 -0.29452235]]. Reward = [0.]
Curr episode timestep = 364
Current timestep = 1267. State = [[-0.38433942  0.01029359]]. Action = [[ 0.02396584 -0.22941452 -0.17392369  0.53317904]]. Reward = [0.]
Curr episode timestep = 365
Current timestep = 1268. State = [[-0.38445663  0.01053633]]. Action = [[ 0.07620445  0.02329063 -0.16982448 -0.3173839 ]]. Reward = [0.]
Curr episode timestep = 366
Current timestep = 1269. State = [[-0.38433337  0.01069422]]. Action = [[ 0.08764294  0.19512385 -0.0549667   0.629727  ]]. Reward = [0.]
Curr episode timestep = 367
Current timestep = 1270. State = [[-0.3832135   0.01183158]]. Action = [[ 0.05418816  0.04212567 -0.03633386  0.25863206]]. Reward = [0.]
Curr episode timestep = 368
Current timestep = 1271. State = [[-0.38173738  0.01323402]]. Action = [[0.2387434  0.18902302 0.20230308 0.5448673 ]]. Reward = [0.]
Curr episode timestep = 369
Current timestep = 1272. State = [[-0.37695765  0.01749127]]. Action = [[-0.08548309  0.00577584 -0.04745726 -0.2727729 ]]. Reward = [0.]
Curr episode timestep = 370
Current timestep = 1273. State = [[-0.3754282   0.01906564]]. Action = [[ 0.20970723  0.02621558  0.04638955 -0.7955927 ]]. Reward = [0.]
Curr episode timestep = 371
Current timestep = 1274. State = [[-0.36973587  0.02367988]]. Action = [[ 0.19670379 -0.06792697 -0.17822376 -0.49500775]]. Reward = [0.]
Curr episode timestep = 372
Current timestep = 1275. State = [[-0.36352983  0.02918344]]. Action = [[ 0.05590597 -0.05514094  0.02384329  0.03299856]]. Reward = [0.]
Curr episode timestep = 373
Current timestep = 1276. State = [[-0.35928324  0.03400169]]. Action = [[-0.22140776 -0.12347355 -0.19448794 -0.7233411 ]]. Reward = [0.]
Curr episode timestep = 374
Current timestep = 1277. State = [[-0.35964108  0.03434379]]. Action = [[ 0.07979405  0.20223546  0.22134078 -0.9387449 ]]. Reward = [0.]
Curr episode timestep = 375
Current timestep = 1278. State = [[-0.35854498  0.03552189]]. Action = [[ 0.22533995  0.2319997   0.22304827 -0.19057894]]. Reward = [0.]
Curr episode timestep = 376
Current timestep = 1279. State = [[-0.35286412  0.04029123]]. Action = [[-0.09161893 -0.10949361 -0.24548608  0.6797184 ]]. Reward = [0.]
Curr episode timestep = 377
Current timestep = 1280. State = [[-0.3507962   0.04233804]]. Action = [[ 0.20426983  0.04608181  0.19659197 -0.39754844]]. Reward = [0.]
Curr episode timestep = 378
Current timestep = 1281. State = [[-0.3452994   0.04683107]]. Action = [[ 0.06606594 -0.15765463 -0.13649505 -0.813602  ]]. Reward = [0.]
Curr episode timestep = 379
Current timestep = 1282. State = [[-0.34198776  0.04969437]]. Action = [[ 0.1458745   0.19138178 -0.09955649  0.37758374]]. Reward = [0.]
Curr episode timestep = 380
Current timestep = 1283. State = [[-0.3356959   0.05357707]]. Action = [[ 0.03459215 -0.24258658  0.03947362 -0.4900452 ]]. Reward = [0.]
Curr episode timestep = 381
Current timestep = 1284. State = [[-0.33130193  0.05488639]]. Action = [[-0.06621149 -0.14609204  0.19819427  0.83261013]]. Reward = [0.]
Curr episode timestep = 382
Current timestep = 1285. State = [[-0.32814345  0.04591132]]. Action = [[ 0.24749982  0.03358889 -0.20344247  0.22160947]]. Reward = [0.]
Curr episode timestep = 383
Current timestep = 1286. State = [[-0.3199044   0.03776257]]. Action = [[ 0.0851894   0.23110273 -0.10223585  0.9069705 ]]. Reward = [0.]
Curr episode timestep = 384
Current timestep = 1287. State = [[-0.3142084   0.03229012]]. Action = [[-0.00134481 -0.05995172  0.0889138  -0.5674211 ]]. Reward = [0.]
Curr episode timestep = 385
Current timestep = 1288. State = [[-0.30868417  0.02703766]]. Action = [[ 0.12394303 -0.22015262 -0.22019234 -0.95815665]]. Reward = [0.]
Curr episode timestep = 386
Current timestep = 1289. State = [[-0.30093026  0.01947019]]. Action = [[-0.20955275 -0.18186621  0.00951186 -0.54581743]]. Reward = [0.]
Curr episode timestep = 387
Current timestep = 1290. State = [[-0.29834285  0.01259314]]. Action = [[-0.14563724  0.20224738 -0.03168523  0.59006774]]. Reward = [0.]
Curr episode timestep = 388
Current timestep = 1291. State = [[-0.29695415  0.00824991]]. Action = [[ 0.03426707 -0.12602997 -0.10890162  0.12022126]]. Reward = [0.]
Curr episode timestep = 389
Current timestep = 1292. State = [[-0.29657102  0.0046478 ]]. Action = [[-0.20170507  0.078078    0.02928406  0.3735746 ]]. Reward = [0.]
Curr episode timestep = 390
Current timestep = 1293. State = [[-0.29861617  0.00133008]]. Action = [[-0.14748414  0.2363692   0.24258322  0.70521   ]]. Reward = [0.]
Curr episode timestep = 391
Current timestep = 1294. State = [[-0.3014746   0.00387108]]. Action = [[ 0.10567558 -0.07963011  0.0279811  -0.5880937 ]]. Reward = [0.]
Curr episode timestep = 392
Current timestep = 1295. State = [[-0.30389056 -0.00135328]]. Action = [[-0.19465236  0.02545014 -0.11594196 -0.20656204]]. Reward = [0.]
Curr episode timestep = 393
Current timestep = 1296. State = [[-0.3066639   0.00091489]]. Action = [[ 0.02926061 -0.14361976 -0.19906674  0.34729362]]. Reward = [0.]
Curr episode timestep = 394
Current timestep = 1297. State = [[-0.30902362  0.00252912]]. Action = [[ 0.1635786  -0.06295082  0.13064504  0.5090091 ]]. Reward = [0.]
Curr episode timestep = 395
Current timestep = 1298. State = [[-0.30950245  0.00384196]]. Action = [[-0.14453867 -0.16501673  0.04098025 -0.19724822]]. Reward = [0.]
Curr episode timestep = 396
Current timestep = 1299. State = [[-0.31056082  0.00103786]]. Action = [[ 0.10606995  0.04170376 -0.05225909 -0.24429864]]. Reward = [0.]
Curr episode timestep = 397
Current timestep = 1300. State = [[-0.3108392   0.00225286]]. Action = [[-0.12607118 -0.19719845 -0.064325   -0.06436074]]. Reward = [0.]
Curr episode timestep = 398
Current timestep = 1301. State = [[-0.31180772  0.00235696]]. Action = [[ 0.07527882 -0.17118742  0.18959981  0.8238025 ]]. Reward = [0.]
Curr episode timestep = 399
Current timestep = 1302. State = [[-0.3122377  -0.00073956]]. Action = [[-0.04169643 -0.05586821  0.02582675 -0.6568056 ]]. Reward = [0.]
Curr episode timestep = 400
Current timestep = 1303. State = [[-0.3147462  -0.00111157]]. Action = [[-0.20273885 -0.05273652  0.09168047  0.8512956 ]]. Reward = [0.]
Curr episode timestep = 401
Current timestep = 1304. State = [[-0.31901947 -0.00200542]]. Action = [[-0.02874988  0.13633516 -0.12689897 -0.7455786 ]]. Reward = [0.]
Curr episode timestep = 402
Current timestep = 1305. State = [[-0.32194215 -0.00222401]]. Action = [[ 0.20329016  0.04177374  0.03723472 -0.0186857 ]]. Reward = [0.]
Curr episode timestep = 403
Current timestep = 1306. State = [[-0.32212234 -0.00211193]]. Action = [[-0.11158219 -0.21476327 -0.20822144 -0.15636408]]. Reward = [0.]
Curr episode timestep = 404
Current timestep = 1307. State = [[-0.32304934 -0.00219407]]. Action = [[-0.23626076  0.04310232 -0.00576539  0.96379817]]. Reward = [0.]
Curr episode timestep = 405
Current timestep = 1308. State = [[-0.328394   -0.00355662]]. Action = [[-0.22718436  0.04415545 -0.10772051  0.0753572 ]]. Reward = [0.]
Curr episode timestep = 406
Current timestep = 1309. State = [[-0.33597082 -0.00409386]]. Action = [[-0.0355792  -0.08665234  0.16947755 -0.81217444]]. Reward = [0.]
Curr episode timestep = 407
Current timestep = 1310. State = [[-0.3452376  -0.00381269]]. Action = [[ 0.03168112  0.08248121 -0.12047848  0.54249644]]. Reward = [0.]
Curr episode timestep = 408
Current timestep = 1311. State = [[-0.35117385 -0.00162299]]. Action = [[-0.20268819  0.05817029 -0.16030277 -0.5219571 ]]. Reward = [0.]
Curr episode timestep = 409
Current timestep = 1312. State = [[-3.5996768e-01 -2.4009276e-04]]. Action = [[-0.19574396 -0.04319759 -0.16541062  0.00945199]]. Reward = [0.]
Curr episode timestep = 410
Current timestep = 1313. State = [[-0.36942708  0.00098254]]. Action = [[ 0.22563648  0.01509428  0.11801562 -0.926312  ]]. Reward = [0.]
Curr episode timestep = 411
Current timestep = 1314. State = [[-0.3750513   0.00502845]]. Action = [[-0.21962729 -0.14260188 -0.09545168  0.9318547 ]]. Reward = [0.]
Curr episode timestep = 412
Current timestep = 1315. State = [[-0.38223827  0.00663975]]. Action = [[-0.06586513 -0.21553367  0.24375299 -0.08913571]]. Reward = [0.]
Curr episode timestep = 413
Current timestep = 1316. State = [[-0.38991064  0.0085697 ]]. Action = [[-0.04907957  0.13714576  0.17344993  0.5119996 ]]. Reward = [0.]
Curr episode timestep = 414
Current timestep = 1317. State = [[-0.39380968  0.01332203]]. Action = [[ 0.23147798  0.06510606 -0.16359976  0.11192751]]. Reward = [0.]
Curr episode timestep = 415
Current timestep = 1318. State = [[-0.3952528   0.01865519]]. Action = [[ 0.10367441 -0.1306771   0.08865705  0.73838985]]. Reward = [0.]
Curr episode timestep = 416
Current timestep = 1319. State = [[-0.39647815  0.02215318]]. Action = [[ 0.23934126 -0.24092236 -0.17233135 -0.6456768 ]]. Reward = [0.]
Curr episode timestep = 417
Current timestep = 1320. State = [[-0.39612457  0.02342989]]. Action = [[ 0.03998721 -0.22285423  0.03087386  0.6482142 ]]. Reward = [0.]
Curr episode timestep = 418
Current timestep = 1321. State = [[-0.3958185   0.02321966]]. Action = [[ 0.13690981  0.21586138 -0.17456423 -0.4313646 ]]. Reward = [0.]
Curr episode timestep = 419
Current timestep = 1322. State = [[-0.3933322   0.02518893]]. Action = [[-0.11561494 -0.14289837 -0.12828466  0.9541464 ]]. Reward = [0.]
Curr episode timestep = 420
Current timestep = 1323. State = [[-0.3916451   0.02671551]]. Action = [[ 0.22608906 -0.11386596 -0.22248237  0.3476956 ]]. Reward = [0.]
Curr episode timestep = 421
Current timestep = 1324. State = [[-0.38828433  0.02910928]]. Action = [[ 0.16638657  0.11454743 -0.14397971  0.22892392]]. Reward = [0.]
Curr episode timestep = 422
Current timestep = 1325. State = [[-0.38206545  0.03364597]]. Action = [[ 0.07882023  0.00877401  0.20932657 -0.6576152 ]]. Reward = [0.]
Curr episode timestep = 423
Current timestep = 1326. State = [[-0.37453276  0.03894578]]. Action = [[-0.11292456 -0.0632408  -0.22553946 -0.96292955]]. Reward = [0.]
Curr episode timestep = 424
Current timestep = 1327. State = [[-0.37060782  0.04159738]]. Action = [[-0.19528091  0.01321766  0.04359069  0.01187301]]. Reward = [0.]
Curr episode timestep = 425
Current timestep = 1328. State = [[-0.36797115  0.04320295]]. Action = [[-0.12576234  0.19714999 -0.18210323  0.73786783]]. Reward = [0.]
Curr episode timestep = 426
Current timestep = 1329. State = [[-0.36758396  0.04348763]]. Action = [[-0.06824347  0.02883437 -0.18236478 -0.4083339 ]]. Reward = [0.]
Curr episode timestep = 427
Current timestep = 1330. State = [[-0.36758143  0.04350383]]. Action = [[ 0.09276679 -0.01257996 -0.03355514  0.6861383 ]]. Reward = [0.]
Curr episode timestep = 428
Current timestep = 1331. State = [[-0.36715642  0.04401259]]. Action = [[-0.06670573  0.08367532  0.20543563 -0.65879923]]. Reward = [0.]
Curr episode timestep = 429
Current timestep = 1332. State = [[-0.3670489   0.04416543]]. Action = [[0.2090737  0.16274172 0.2233926  0.06916583]]. Reward = [0.]
Curr episode timestep = 430
Current timestep = 1333. State = [[-0.36290577  0.04730503]]. Action = [[-0.15736304 -0.07867774  0.10821995 -0.9360346 ]]. Reward = [0.]
Curr episode timestep = 431
Current timestep = 1334. State = [[-0.36218303  0.04815127]]. Action = [[-0.14790286  0.13472846  0.22536707 -0.61818594]]. Reward = [0.]
Curr episode timestep = 432
Current timestep = 1335. State = [[-0.362363    0.04818301]]. Action = [[0.09550443 0.04337448 0.22512546 0.0447253 ]]. Reward = [0.]
Curr episode timestep = 433
Current timestep = 1336. State = [[-0.3624269   0.04820403]]. Action = [[ 0.06038091 -0.21036413  0.01597703  0.8915849 ]]. Reward = [0.]
Curr episode timestep = 434
Current timestep = 1337. State = [[-0.36235794  0.04830942]]. Action = [[0.13443309 0.1739941  0.10153684 0.71991825]]. Reward = [0.]
Curr episode timestep = 435
Current timestep = 1338. State = [[-0.3592436   0.05075088]]. Action = [[ 0.07675558  0.11552507 -0.09022802 -0.18249905]]. Reward = [0.]
Curr episode timestep = 436
Current timestep = 1339. State = [[-0.35483378  0.05468395]]. Action = [[-0.2464044   0.19252297  0.17295456  0.9111904 ]]. Reward = [0.]
Curr episode timestep = 437
Current timestep = 1340. State = [[-0.35489327  0.05526606]]. Action = [[ 0.10424107  0.15309578 -0.14866309  0.2902404 ]]. Reward = [0.]
Curr episode timestep = 438
Current timestep = 1341. State = [[-0.3530263   0.05768632]]. Action = [[ 0.12005112  0.03078535 -0.16399337  0.3293705 ]]. Reward = [0.]
Curr episode timestep = 439
Current timestep = 1342. State = [[-0.35003445  0.06120999]]. Action = [[-0.07905328 -0.23586345 -0.15975375 -0.3970698 ]]. Reward = [0.]
Curr episode timestep = 440
Current timestep = 1343. State = [[-0.34971285  0.06152945]]. Action = [[-0.10249728  0.17175996 -0.11233415 -0.49260545]]. Reward = [0.]
Curr episode timestep = 441
Current timestep = 1344. State = [[-0.34978732  0.06146906]]. Action = [[ 0.05078119 -0.05273435 -0.13776809 -0.1787296 ]]. Reward = [0.]
Curr episode timestep = 442
Current timestep = 1345. State = [[-0.34984708  0.06141859]]. Action = [[-0.19678836 -0.06604335  0.04662603  0.3692516 ]]. Reward = [0.]
Curr episode timestep = 443
Current timestep = 1346. State = [[-0.35187435  0.05963364]]. Action = [[-0.1472378  -0.1600321   0.00524333 -0.27414262]]. Reward = [0.]
Curr episode timestep = 444
Current timestep = 1347. State = [[-0.35584804  0.05650085]]. Action = [[ 0.03881902 -0.14645205  0.06620157 -0.4898094 ]]. Reward = [0.]
Curr episode timestep = 445
Current timestep = 1348. State = [[-0.35802308  0.054373  ]]. Action = [[ 0.2374773  -0.00902557 -0.08396962 -0.43198365]]. Reward = [0.]
Curr episode timestep = 446
Current timestep = 1349. State = [[-0.35802656  0.05428164]]. Action = [[-0.1283934  -0.22263977  0.17686778  0.3929839 ]]. Reward = [0.]
Curr episode timestep = 447
Current timestep = 1350. State = [[-0.35847092  0.05364206]]. Action = [[ 0.12458795  0.22701645  0.15964687 -0.29287744]]. Reward = [0.]
Curr episode timestep = 448
Current timestep = 1351. State = [[-0.35835713  0.05377293]]. Action = [[-0.11216694  0.24428326  0.15251315 -0.71411186]]. Reward = [0.]
Curr episode timestep = 449
Current timestep = 1352. State = [[-0.35835713  0.05377293]]. Action = [[ 0.10496172 -0.13373801  0.24797523 -0.00961274]]. Reward = [0.]
Curr episode timestep = 450
Current timestep = 1353. State = [[-0.35821354  0.05392683]]. Action = [[-0.05289948  0.18217438 -0.21024701 -0.09705305]]. Reward = [0.]
Curr episode timestep = 451
Current timestep = 1354. State = [[-0.35819438  0.05402046]]. Action = [[-0.12043598 -0.02396853 -0.10050601  0.96074057]]. Reward = [0.]
Curr episode timestep = 452
Current timestep = 1355. State = [[-0.3582848   0.05389741]]. Action = [[-0.15529977 -0.17130136 -0.1997326   0.37281835]]. Reward = [0.]
Curr episode timestep = 453
Current timestep = 1356. State = [[-0.36072728  0.05190151]]. Action = [[ 0.02839929 -0.23997818  0.23688743  0.9016969 ]]. Reward = [0.]
Curr episode timestep = 454
Current timestep = 1357. State = [[-0.36340764  0.04908138]]. Action = [[ 0.1375184  -0.01130044 -0.14198048  0.03592503]]. Reward = [0.]
Curr episode timestep = 455
Current timestep = 1358. State = [[-0.36392757  0.04857795]]. Action = [[-0.16756363  0.04816785 -0.15287337 -0.02786064]]. Reward = [0.]
Curr episode timestep = 456
Current timestep = 1359. State = [[-0.3665744   0.04582801]]. Action = [[-0.10677665  0.16570759 -0.06492238 -0.5934874 ]]. Reward = [0.]
Curr episode timestep = 457
Current timestep = 1360. State = [[-0.369194    0.04358133]]. Action = [[-0.06519648 -0.00322258  0.22505578 -0.7567576 ]]. Reward = [0.]
Curr episode timestep = 458
Current timestep = 1361. State = [[-0.3720655   0.04123178]]. Action = [[ 0.18150473  0.13674471  0.00828037 -0.08447337]]. Reward = [0.]
Curr episode timestep = 459
Current timestep = 1362. State = [[-0.37178606  0.04144683]]. Action = [[ 0.16167283  0.21484295 -0.16742475  0.28556728]]. Reward = [0.]
Curr episode timestep = 460
Current timestep = 1363. State = [[-0.36904088  0.04388973]]. Action = [[ 0.0082849  -0.05319709 -0.16008371  0.5010903 ]]. Reward = [0.]
Curr episode timestep = 461
Current timestep = 1364. State = [[-0.366927    0.04582366]]. Action = [[ 0.23672318  0.04686522 -0.04617806 -0.7998105 ]]. Reward = [0.]
Curr episode timestep = 462
Current timestep = 1365. State = [[-0.36140284  0.05041901]]. Action = [[0.21748844 0.18031329 0.15333974 0.5885118 ]]. Reward = [0.]
Curr episode timestep = 463
Current timestep = 1366. State = [[-0.351613    0.05875942]]. Action = [[-0.09283552  0.24812496 -0.22897652 -0.52623075]]. Reward = [0.]
Curr episode timestep = 464
Current timestep = 1367. State = [[-0.3453576   0.06332372]]. Action = [[-0.22642124 -0.01057656  0.24236044  0.72447324]]. Reward = [0.]
Curr episode timestep = 465
Current timestep = 1368. State = [[-0.34560466  0.06276298]]. Action = [[ 0.19440848 -0.04469824 -0.22648734 -0.15503693]]. Reward = [0.]
Curr episode timestep = 466
Current timestep = 1369. State = [[-0.34271604  0.06449076]]. Action = [[-0.20855525 -0.01976562 -0.22022477 -0.35953903]]. Reward = [0.]
Curr episode timestep = 467
Current timestep = 1370. State = [[-0.3434434   0.06245914]]. Action = [[-0.03842621 -0.19029373 -0.08592774  0.8298671 ]]. Reward = [0.]
Curr episode timestep = 468
Current timestep = 1371. State = [[-0.34545442  0.06236513]]. Action = [[-0.08342892 -0.14827795  0.15823042  0.82754064]]. Reward = [0.]
Curr episode timestep = 469
Current timestep = 1372. State = [[-0.3467445   0.05800778]]. Action = [[ 0.03742778 -0.11354399 -0.14422704 -0.804586  ]]. Reward = [0.]
Curr episode timestep = 470
Current timestep = 1373. State = [[-0.34964553  0.05683576]]. Action = [[0.18907762 0.11816087 0.18751621 0.6242087 ]]. Reward = [0.]
Curr episode timestep = 471
Current timestep = 1374. State = [[-0.34919035  0.05829854]]. Action = [[ 0.01385468  0.09736496 -0.04563889  0.42660117]]. Reward = [0.]
Curr episode timestep = 472
Current timestep = 1375. State = [[-0.34797737  0.0592343 ]]. Action = [[-0.05973166 -0.21706986 -0.13629979 -0.36519623]]. Reward = [0.]
Curr episode timestep = 473
Current timestep = 1376. State = [[-0.34883377  0.05712949]]. Action = [[-0.01050574  0.11936158 -0.20943305 -0.3289659 ]]. Reward = [0.]
Curr episode timestep = 474
Current timestep = 1377. State = [[-0.34915265  0.05748055]]. Action = [[-0.08171463  0.12022245  0.23179486 -0.4917264 ]]. Reward = [0.]
Curr episode timestep = 475
Current timestep = 1378. State = [[-0.34921393  0.05752464]]. Action = [[ 0.16944054 -0.09053147  0.18320823  0.1310687 ]]. Reward = [0.]
Curr episode timestep = 476
Current timestep = 1379. State = [[-0.34785244  0.05881742]]. Action = [[ 0.17684063 -0.2238917  -0.09359193  0.9824228 ]]. Reward = [0.]
Curr episode timestep = 477
Current timestep = 1380. State = [[-0.34415865  0.05643274]]. Action = [[ 0.08335912 -0.12446663  0.16142264  0.51310086]]. Reward = [0.]
Curr episode timestep = 478
Current timestep = 1381. State = [[-0.34069696  0.05443669]]. Action = [[0.04331905 0.07166404 0.1428971  0.7923405 ]]. Reward = [0.]
Curr episode timestep = 479
Current timestep = 1382. State = [[-0.33682823  0.04876251]]. Action = [[-0.22869009 -0.12452647  0.16140172 -0.6499818 ]]. Reward = [0.]
Curr episode timestep = 480
Current timestep = 1383. State = [[-0.33680153  0.04468791]]. Action = [[ 0.24712637  0.19919413  0.1602427  -0.03212863]]. Reward = [0.]
Curr episode timestep = 481
Current timestep = 1384. State = [[-0.3322504   0.04132737]]. Action = [[-0.04656933  0.10531133  0.1891157   0.01749575]]. Reward = [0.]
Curr episode timestep = 482
Current timestep = 1385. State = [[-0.32656246  0.03884842]]. Action = [[ 0.15464091 -0.23703411  0.04041338  0.45070612]]. Reward = [0.]
Curr episode timestep = 483
Current timestep = 1386. State = [[-0.32137132  0.03342474]]. Action = [[-0.18787916 -0.07881247  0.07788771  0.23314953]]. Reward = [0.]
Curr episode timestep = 484
Current timestep = 1387. State = [[-0.31878832  0.02789147]]. Action = [[0.21561521 0.21174908 0.23668426 0.24918127]]. Reward = [0.]
Curr episode timestep = 485
Current timestep = 1388. State = [[-0.31326053  0.02335929]]. Action = [[ 0.09324229  0.20479673 -0.0539045  -0.07280689]]. Reward = [0.]
Curr episode timestep = 486
Current timestep = 1389. State = [[-0.30582854  0.02020885]]. Action = [[-0.23278308  0.21120155 -0.20263065  0.77289057]]. Reward = [0.]
Curr episode timestep = 487
Current timestep = 1390. State = [[-0.30170375  0.01805764]]. Action = [[ 0.0521864   0.23456633  0.19742954 -0.5666595 ]]. Reward = [0.]
Curr episode timestep = 488
Current timestep = 1391. State = [[-0.29722866  0.01905941]]. Action = [[ 0.23075491 -0.12867309  0.08508995  0.48232877]]. Reward = [0.]
Curr episode timestep = 489
Current timestep = 1392. State = [[-0.29040664  0.01870801]]. Action = [[-0.09396169 -0.16606878  0.04125553  0.12722671]]. Reward = [0.]
Curr episode timestep = 490
Current timestep = 1393. State = [[-0.2867473   0.01845881]]. Action = [[-0.14481512  0.18283004 -0.10213272 -0.47384882]]. Reward = [0.]
Curr episode timestep = 491
Current timestep = 1394. State = [[-0.2859457   0.01695849]]. Action = [[ 0.03901362  0.01220834  0.12410855 -0.49581355]]. Reward = [0.]
Curr episode timestep = 492
Current timestep = 1395. State = [[-0.2853203   0.01877213]]. Action = [[-0.13418706 -0.12942475  0.19027144 -0.7445792 ]]. Reward = [0.]
Curr episode timestep = 493
Current timestep = 1396. State = [[-0.28690428  0.01744035]]. Action = [[ 0.0556196   0.1837633   0.09184498 -0.90508586]]. Reward = [0.]
Curr episode timestep = 494
Current timestep = 1397. State = [[-0.28641245  0.01935404]]. Action = [[-0.18818904  0.19988573  0.04989031  0.90161   ]]. Reward = [0.]
Curr episode timestep = 495
Current timestep = 1398. State = [[-0.2878531   0.02140419]]. Action = [[-0.12482175 -0.22806188  0.04108     0.913867  ]]. Reward = [0.]
Curr episode timestep = 496
Current timestep = 1399. State = [[-0.29234427  0.0218539 ]]. Action = [[-0.21634145 -0.01127809  0.17318463 -0.39173073]]. Reward = [0.]
Curr episode timestep = 497
Current timestep = 1400. State = [[-0.29910797  0.02401816]]. Action = [[-0.03846547 -0.09396809 -0.14305608 -0.45461953]]. Reward = [0.]
Curr episode timestep = 498
Current timestep = 1401. State = [[-0.3054866   0.02721407]]. Action = [[ 0.06425148 -0.02108556 -0.1530554  -0.7440452 ]]. Reward = [0.]
Curr episode timestep = 499
Current timestep = 1402. State = [[-0.31016     0.03101703]]. Action = [[ 0.18708056 -0.13524482  0.24726975  0.18293929]]. Reward = [0.]
Curr episode timestep = 500
Current timestep = 1403. State = [[-0.31213146  0.03438333]]. Action = [[0.14697152 0.1721201  0.19302696 0.9552431 ]]. Reward = [0.]
Curr episode timestep = 501
Current timestep = 1404. State = [[-0.31014457  0.03680911]]. Action = [[0.18967372 0.03756469 0.21932527 0.6441889 ]]. Reward = [0.]
Curr episode timestep = 502
Current timestep = 1405. State = [[-0.30489004  0.04012984]]. Action = [[-0.03621444  0.21482086  0.01338217 -0.16575176]]. Reward = [0.]
Curr episode timestep = 503
Current timestep = 1406. State = [[-0.30182642  0.03867603]]. Action = [[-0.24192436  0.11717302  0.04316968  0.17200458]]. Reward = [0.]
Curr episode timestep = 504
Current timestep = 1407. State = [[-0.30236256  0.03944431]]. Action = [[-0.13835749 -0.03250596  0.11030236  0.598614  ]]. Reward = [0.]
Curr episode timestep = 505
Current timestep = 1408. State = [[-0.30431205  0.04131677]]. Action = [[-0.24750683  0.15538508 -0.21362005  0.01954007]]. Reward = [0.]
Curr episode timestep = 506
Current timestep = 1409. State = [[-0.3099908   0.04276567]]. Action = [[ 0.08785108  0.14208496  0.04865795 -0.95630956]]. Reward = [0.]
Curr episode timestep = 507
Current timestep = 1410. State = [[-0.3134305   0.04641914]]. Action = [[ 0.17689854 -0.21672761  0.10303658 -0.05740857]]. Reward = [0.]
Curr episode timestep = 508
Current timestep = 1411. State = [[-0.3150857  0.0498836]]. Action = [[-0.17132941  0.10904443 -0.05857985  0.12997723]]. Reward = [0.]
Curr episode timestep = 509
Current timestep = 1412. State = [[-0.31736013  0.05170736]]. Action = [[-0.16798808 -0.11168547 -0.19885916 -0.8741502 ]]. Reward = [0.]
Curr episode timestep = 510
Current timestep = 1413. State = [[-0.32339054  0.05189851]]. Action = [[-0.18033288  0.23509836 -0.19584659  0.46825314]]. Reward = [0.]
Curr episode timestep = 511
Current timestep = 1414. State = [[-0.33037934  0.0523786 ]]. Action = [[ 0.23006636 -0.21855423 -0.07032949 -0.2550462 ]]. Reward = [0.]
Curr episode timestep = 512
Current timestep = 1415. State = [[-0.33314925  0.0553625 ]]. Action = [[-0.11850032 -0.02593088 -0.20947196 -0.87396675]]. Reward = [0.]
Curr episode timestep = 513
Current timestep = 1416. State = [[-0.33686388  0.05657687]]. Action = [[-0.07978979  0.05197322  0.03591222  0.68572783]]. Reward = [0.]
Curr episode timestep = 514
Current timestep = 1417. State = [[-0.3410086   0.05768867]]. Action = [[-0.06398755 -0.20695014 -0.17858085  0.5416434 ]]. Reward = [0.]
Curr episode timestep = 515
Current timestep = 1418. State = [[-0.34647384  0.05826814]]. Action = [[-0.18458064  0.04019573 -0.13553338 -0.8029364 ]]. Reward = [0.]
Curr episode timestep = 516
Current timestep = 1419. State = [[-0.35350597  0.05783505]]. Action = [[ 0.09278032  0.22892943  0.05557963 -0.49420702]]. Reward = [0.]
Curr episode timestep = 517
Current timestep = 1420. State = [[-0.35776743  0.05939784]]. Action = [[ 0.11299703  0.14383858 -0.01709527  0.31479883]]. Reward = [0.]
Curr episode timestep = 518
Current timestep = 1421. State = [[-0.3585527   0.06107308]]. Action = [[0.13316572 0.04453114 0.14528576 0.8757262 ]]. Reward = [0.]
Curr episode timestep = 519
Current timestep = 1422. State = [[-0.357421    0.06210547]]. Action = [[-0.18668054 -0.21422909 -0.04376441 -0.73197085]]. Reward = [0.]
Curr episode timestep = 520
Current timestep = 1423. State = [[-0.35855296  0.06136129]]. Action = [[ 0.07901993  0.04667383 -0.17485783  0.8384204 ]]. Reward = [0.]
Curr episode timestep = 521
Current timestep = 1424. State = [[-0.35848543  0.0613776 ]]. Action = [[ 0.15327221 -0.24099371  0.18944079 -0.763503  ]]. Reward = [0.]
Curr episode timestep = 522
Current timestep = 1425. State = [[-0.35834777  0.06153392]]. Action = [[ 0.03291535 -0.10120997 -0.00787656  0.6287538 ]]. Reward = [0.]
Curr episode timestep = 523
Current timestep = 1426. State = [[-0.35827184  0.06159586]]. Action = [[-0.16487207  0.20770732 -0.24423634  0.5026258 ]]. Reward = [0.]
Curr episode timestep = 524
Current timestep = 1427. State = [[-0.35826916  0.0616024 ]]. Action = [[ 0.03073588  0.02653211 -0.11527145  0.10198069]]. Reward = [0.]
Curr episode timestep = 525
Current timestep = 1428. State = [[-0.358195    0.06168343]]. Action = [[ 0.19781923 -0.03877969 -0.03612636  0.3560202 ]]. Reward = [0.]
Curr episode timestep = 526
Current timestep = 1429. State = [[-0.3564665  0.0635648]]. Action = [[ 0.01758835 -0.16872807 -0.05726433 -0.10636765]]. Reward = [0.]
Curr episode timestep = 527
Current timestep = 1430. State = [[-0.355251    0.06491108]]. Action = [[ 0.21034643 -0.22097552 -0.09077962  0.3646549 ]]. Reward = [0.]
Curr episode timestep = 528
Current timestep = 1431. State = [[-0.35197505  0.06738763]]. Action = [[-0.18752848  0.24778196 -0.19704358 -0.29932487]]. Reward = [0.]
Curr episode timestep = 529
Current timestep = 1432. State = [[-0.35208052  0.06779918]]. Action = [[-0.22158167 -0.22052242 -0.21090545  0.61825895]]. Reward = [0.]
Curr episode timestep = 530
Current timestep = 1433. State = [[-0.35424384  0.06666519]]. Action = [[-0.11845505  0.0082649  -0.00187068 -0.6427801 ]]. Reward = [0.]
Curr episode timestep = 531
Current timestep = 1434. State = [[-0.35855177  0.06378028]]. Action = [[-0.1850975   0.08622038  0.12574673 -0.5896524 ]]. Reward = [0.]
Curr episode timestep = 532
Current timestep = 1435. State = [[-0.36483362  0.05980439]]. Action = [[-0.19401237 -0.20802476 -0.23719196 -0.49294686]]. Reward = [0.]
Curr episode timestep = 533
Current timestep = 1436. State = [[-0.37476966  0.05434255]]. Action = [[ 0.13023534  0.09951043 -0.22029856 -0.5125256 ]]. Reward = [0.]
Curr episode timestep = 534
Current timestep = 1437. State = [[-0.3782895   0.05476074]]. Action = [[-0.00747144  0.22626203  0.21335268 -0.72535455]]. Reward = [0.]
Curr episode timestep = 535
Current timestep = 1438. State = [[-0.37960446  0.05672248]]. Action = [[0.00596917 0.14870754 0.04799384 0.36488414]]. Reward = [0.]
Curr episode timestep = 536
Current timestep = 1439. State = [[-0.38030124  0.05850004]]. Action = [[ 0.06686541  0.14713782  0.12714452 -0.08384883]]. Reward = [0.]
Curr episode timestep = 537
Current timestep = 1440. State = [[-0.3799861   0.05913176]]. Action = [[-0.13973103 -0.06100658 -0.11661988 -0.53288275]]. Reward = [0.]
Curr episode timestep = 538
Current timestep = 1441. State = [[-0.3792967   0.05931426]]. Action = [[-0.15553117  0.08858109  0.24020973 -0.10570753]]. Reward = [0.]
Curr episode timestep = 539
Current timestep = 1442. State = [[-0.37876117  0.05947223]]. Action = [[ 0.03890499 -0.14681283 -0.07798995 -0.85118175]]. Reward = [0.]
Curr episode timestep = 540
Current timestep = 1443. State = [[-0.37845123  0.05974593]]. Action = [[ 0.18487293  0.1279741  -0.0723179   0.55473554]]. Reward = [0.]
Curr episode timestep = 541
Current timestep = 1444. State = [[-0.37573048  0.0629261 ]]. Action = [[ 0.22730419 -0.1919392  -0.14388567 -0.5015919 ]]. Reward = [0.]
Curr episode timestep = 542
Current timestep = 1445. State = [[-0.37113425  0.06773312]]. Action = [[ 0.07088962 -0.06982532 -0.2250827  -0.33862782]]. Reward = [0.]
Curr episode timestep = 543
Current timestep = 1446. State = [[-0.36741188  0.07192262]]. Action = [[ 0.22972924 -0.23435704 -0.0059799   0.56287503]]. Reward = [0.]
Curr episode timestep = 544
Current timestep = 1447. State = [[-0.36314145  0.07657556]]. Action = [[-0.22493291  0.00330144 -0.1694939   0.48433852]]. Reward = [0.]
Curr episode timestep = 545
Current timestep = 1448. State = [[-0.36293325  0.07807715]]. Action = [[ 0.1437757   0.01626351 -0.0676309  -0.36143452]]. Reward = [0.]
Curr episode timestep = 546
Current timestep = 1449. State = [[-0.3614063   0.08078334]]. Action = [[0.08800679 0.21912718 0.01126286 0.73971105]]. Reward = [0.]
Curr episode timestep = 547
Current timestep = 1450. State = [[-0.35801485  0.08557083]]. Action = [[-0.04789484 -0.20573238 -0.1660551  -0.54646987]]. Reward = [0.]
Curr episode timestep = 548
Current timestep = 1451. State = [[-0.3572738   0.08718652]]. Action = [[ 0.07001454 -0.14741793 -0.22565559 -0.23757195]]. Reward = [0.]
Curr episode timestep = 549
Current timestep = 1452. State = [[-0.35662502  0.08809743]]. Action = [[ 0.08446902  0.16502982  0.0969319  -0.79095876]]. Reward = [0.]
Curr episode timestep = 550
Current timestep = 1453. State = [[-0.3540951   0.09081568]]. Action = [[ 0.08960956 -0.20911688 -0.01348525 -0.83786833]]. Reward = [0.]
Curr episode timestep = 551
Current timestep = 1454. State = [[-0.35220322  0.09221933]]. Action = [[-0.17111243  0.1942583   0.21791893 -0.34089357]]. Reward = [0.]
Curr episode timestep = 552
Current timestep = 1455. State = [[-0.35089034  0.08909114]]. Action = [[ 0.16996533  0.13036036 -0.06254105  0.72140396]]. Reward = [0.]
Curr episode timestep = 553
Current timestep = 1456. State = [[-0.34570235  0.08956027]]. Action = [[ 0.01718771 -0.16723295 -0.1106759   0.6282029 ]]. Reward = [0.]
Curr episode timestep = 554
Current timestep = 1457. State = [[-0.34295145  0.08716714]]. Action = [[-0.04513504  0.14270997  0.1507299   0.9298216 ]]. Reward = [0.]
Curr episode timestep = 555
Current timestep = 1458. State = [[-0.33876207  0.08319456]]. Action = [[ 0.01331925  0.1595667   0.17269063 -0.13108253]]. Reward = [0.]
Curr episode timestep = 556
Current timestep = 1459. State = [[-0.33494624  0.08318222]]. Action = [[ 0.15866226  0.06459653 -0.22561857 -0.63242507]]. Reward = [0.]
Curr episode timestep = 557
Current timestep = 1460. State = [[-0.32899642  0.07914983]]. Action = [[ 0.23343432 -0.19638278  0.08677375  0.5298761 ]]. Reward = [0.]
Curr episode timestep = 558
Current timestep = 1461. State = [[-0.31771448  0.07476151]]. Action = [[-0.20137176  0.04347518 -0.04701874  0.33789814]]. Reward = [0.]
Curr episode timestep = 559
Current timestep = 1462. State = [[-0.31375855  0.07031976]]. Action = [[-0.16711533  0.22269493  0.10949713  0.7554666 ]]. Reward = [0.]
Curr episode timestep = 560
Current timestep = 1463. State = [[-0.31111962  0.06498906]]. Action = [[ 0.22168419 -0.17396715  0.21457878 -0.20273626]]. Reward = [0.]
Curr episode timestep = 561
Current timestep = 1464. State = [[-0.30616394  0.06308117]]. Action = [[-0.17137712  0.1172561   0.12831098 -0.45947218]]. Reward = [0.]
Curr episode timestep = 562
Current timestep = 1465. State = [[-0.30452415  0.05846633]]. Action = [[-0.10659489 -0.00651899 -0.14408569 -0.41230524]]. Reward = [0.]
Curr episode timestep = 563
Current timestep = 1466. State = [[-0.30600595  0.05767366]]. Action = [[-0.16689156 -0.18087839  0.1702733  -0.06234854]]. Reward = [0.]
Curr episode timestep = 564
Current timestep = 1467. State = [[-0.30834344  0.05503901]]. Action = [[-0.08144599 -0.24238193 -0.06339461  0.7021899 ]]. Reward = [0.]
Curr episode timestep = 565
Current timestep = 1468. State = [[-0.3129281   0.05352823]]. Action = [[-0.03474522 -0.22911873 -0.13375305 -0.5708342 ]]. Reward = [0.]
Curr episode timestep = 566
Current timestep = 1469. State = [[-0.31734553  0.05289455]]. Action = [[ 0.04693612  0.18415618 -0.02085859  0.17770803]]. Reward = [0.]
Curr episode timestep = 567
Current timestep = 1470. State = [[-0.31929472  0.05312377]]. Action = [[-0.17573008  0.23121837 -0.05803809 -0.7257774 ]]. Reward = [0.]
Curr episode timestep = 568
Current timestep = 1471. State = [[-0.32261482  0.05280678]]. Action = [[ 0.17999819  0.11211497 -0.00382224  0.08903456]]. Reward = [0.]
Curr episode timestep = 569
Current timestep = 1472. State = [[-0.32318348  0.05388438]]. Action = [[-0.12655614 -0.13920943  0.06598333  0.081774  ]]. Reward = [0.]
Curr episode timestep = 570
Current timestep = 1473. State = [[-0.32561108  0.0534688 ]]. Action = [[-0.20538445 -0.18161952 -0.16372941  0.22093177]]. Reward = [0.]
Curr episode timestep = 571
Current timestep = 1474. State = [[-0.33161983  0.05140161]]. Action = [[ 0.12736368 -0.14151683 -0.19374625 -0.5372854 ]]. Reward = [0.]
Curr episode timestep = 572
Current timestep = 1475. State = [[-0.33552516  0.05111635]]. Action = [[-0.0909085  -0.05749679 -0.20626113 -0.38660347]]. Reward = [0.]
Curr episode timestep = 573
Current timestep = 1476. State = [[-0.33966133  0.05044984]]. Action = [[-0.03424138 -0.18116157 -0.07287495  0.8703153 ]]. Reward = [0.]
Curr episode timestep = 574
Current timestep = 1477. State = [[-0.3444284   0.04993456]]. Action = [[-0.07619837  0.00446042 -0.2238449   0.30383706]]. Reward = [0.]
Curr episode timestep = 575
Current timestep = 1478. State = [[-0.34878546  0.05020975]]. Action = [[ 0.07739267 -0.09719035 -0.16391993  0.31940114]]. Reward = [0.]
Curr episode timestep = 576
Current timestep = 1479. State = [[-0.3518955   0.05151786]]. Action = [[ 0.07908437  0.15090543 -0.05137122 -0.923269  ]]. Reward = [0.]
Curr episode timestep = 577
Current timestep = 1480. State = [[-0.35282764  0.05286922]]. Action = [[ 0.08377081 -0.06566623 -0.03832808  0.45598304]]. Reward = [0.]
Curr episode timestep = 578
Current timestep = 1481. State = [[-0.3530536   0.05304286]]. Action = [[ 0.0769586   0.04039475 -0.02490962  0.64048696]]. Reward = [0.]
Curr episode timestep = 579
Current timestep = 1482. State = [[-0.35265866  0.0535542 ]]. Action = [[-0.23747818  0.1581249   0.00800607 -0.9814301 ]]. Reward = [0.]
Curr episode timestep = 580
Current timestep = 1483. State = [[-0.35318127  0.05324584]]. Action = [[ 0.15491334  0.24704617 -0.16067564 -0.80900455]]. Reward = [0.]
Curr episode timestep = 581
Current timestep = 1484. State = [[-0.35221586  0.05425699]]. Action = [[-0.22483414 -0.13823593  0.19838268 -0.38102734]]. Reward = [0.]
Curr episode timestep = 582
Current timestep = 1485. State = [[-0.3535222   0.05367068]]. Action = [[ 0.22481331 -0.07401609  0.12396842  0.38151824]]. Reward = [0.]
Curr episode timestep = 583
Current timestep = 1486. State = [[-0.35322034  0.05393712]]. Action = [[ 0.08601737 -0.00438063  0.03878373  0.5750029 ]]. Reward = [0.]
Curr episode timestep = 584
Current timestep = 1487. State = [[-0.35190105  0.05546393]]. Action = [[ 0.0354149  -0.01124194 -0.06380257  0.55205965]]. Reward = [0.]
Curr episode timestep = 585
Current timestep = 1488. State = [[-0.35051587  0.05693817]]. Action = [[ 0.16329402 -0.07255995 -0.16825286  0.8521235 ]]. Reward = [0.]
Curr episode timestep = 586
Current timestep = 1489. State = [[-0.34738043  0.05967939]]. Action = [[0.02192619 0.08340555 0.06331307 0.8541539 ]]. Reward = [0.]
Curr episode timestep = 587
Current timestep = 1490. State = [[-0.34485042  0.06244566]]. Action = [[-0.16204564  0.07644245 -0.07843892  0.12480068]]. Reward = [0.]
Curr episode timestep = 588
Current timestep = 1491. State = [[-0.34479278  0.06257726]]. Action = [[-0.02322368  0.0259693   0.19554716  0.22173095]]. Reward = [0.]
Curr episode timestep = 589
Current timestep = 1492. State = [[-0.34491384  0.06251907]]. Action = [[-0.21822932  0.23979798 -0.07476178  0.30210018]]. Reward = [0.]
Curr episode timestep = 590
Current timestep = 1493. State = [[-0.346158    0.06225854]]. Action = [[ 0.1759904   0.02012318 -0.07302791  0.26567125]]. Reward = [0.]
Curr episode timestep = 591
Current timestep = 1494. State = [[-0.3454494   0.06270813]]. Action = [[ 0.22458494  0.15337878  0.03566599 -0.08298862]]. Reward = [0.]
Curr episode timestep = 592
Current timestep = 1495. State = [[-0.34104827  0.06660707]]. Action = [[ 0.17589214 -0.16730072 -0.16522694 -0.8736624 ]]. Reward = [0.]
Curr episode timestep = 593
Current timestep = 1496. State = [[-0.33546683  0.07205643]]. Action = [[-0.20283552  0.03871715  0.0715785  -0.46458846]]. Reward = [0.]
Curr episode timestep = 594
Current timestep = 1497. State = [[-0.3345758   0.07330911]]. Action = [[ 0.13183093 -0.09456375  0.08555537  0.2195034 ]]. Reward = [0.]
Curr episode timestep = 595
Current timestep = 1498. State = [[-0.3327487   0.07169856]]. Action = [[ 0.17625621 -0.18806823  0.01799437 -0.22336912]]. Reward = [0.]
Curr episode timestep = 596
Current timestep = 1499. State = [[-0.32599825  0.06963857]]. Action = [[ 0.19752795 -0.14784177  0.00632983  0.6331682 ]]. Reward = [0.]
Curr episode timestep = 597
Current timestep = 1500. State = [[-0.32053924  0.06684286]]. Action = [[-0.17678256 -0.21071705  0.06221795  0.90398693]]. Reward = [0.]
Curr episode timestep = 598
Current timestep = 1501. State = [[-0.3169802   0.05988985]]. Action = [[ 0.20478481 -0.05330619 -0.20082793  0.04544342]]. Reward = [0.]
Curr episode timestep = 599
Current timestep = 1502. State = [[-0.31232557  0.05231879]]. Action = [[ 0.22036153  0.21955875 -0.0481723  -0.65677005]]. Reward = [0.]
Curr episode timestep = 600
Current timestep = 1503. State = [[-0.30427557  0.04246223]]. Action = [[-0.1609021   0.12201273  0.04163006  0.240296  ]]. Reward = [0.]
Curr episode timestep = 601
Current timestep = 1504. State = [[-0.29561093  0.03499578]]. Action = [[ 0.11156863 -0.24343076 -0.19855028 -0.4219669 ]]. Reward = [0.]
Curr episode timestep = 602
Current timestep = 1505. State = [[-0.28874624  0.02935679]]. Action = [[ 0.03809348  0.16424423 -0.20718211 -0.82121783]]. Reward = [0.]
Curr episode timestep = 603
Current timestep = 1506. State = [[-0.28128156  0.02106329]]. Action = [[ 0.2441982   0.18126488 -0.16118519 -0.93057233]]. Reward = [0.]
Curr episode timestep = 604
Current timestep = 1507. State = [[-0.2708835  0.0145787]]. Action = [[-0.13421199  0.21320415 -0.21864307  0.58966017]]. Reward = [0.]
Curr episode timestep = 605
Current timestep = 1508. State = [[-0.2646547   0.00626032]]. Action = [[0.1602464  0.02453116 0.00204766 0.29155016]]. Reward = [0.]
Curr episode timestep = 606
Current timestep = 1509. State = [[-0.25778484 -0.0018411 ]]. Action = [[-0.18890901 -0.0970393  -0.22826648 -0.25610363]]. Reward = [0.]
Curr episode timestep = 607
Current timestep = 1510. State = [[-0.2546862  -0.00906853]]. Action = [[-0.03232461  0.1569216   0.20335835  0.12512648]]. Reward = [0.]
Curr episode timestep = 608
Current timestep = 1511. State = [[-0.2527339 -0.0114606]]. Action = [[-0.1189083  -0.19548959 -0.17159578  0.39511037]]. Reward = [0.]
Curr episode timestep = 609
Current timestep = 1512. State = [[-0.25343165 -0.01317144]]. Action = [[ 0.01589957 -0.06721525  0.13721633 -0.80915886]]. Reward = [0.]
Curr episode timestep = 610
Current timestep = 1513. State = [[-0.2541651  -0.01335089]]. Action = [[ 0.13810903  0.23450977  0.04590029 -0.30669904]]. Reward = [0.]
Curr episode timestep = 611
Current timestep = 1514. State = [[-0.2522022  -0.01164157]]. Action = [[-0.11843613  0.19670886  0.14035946  0.15913594]]. Reward = [0.]
Curr episode timestep = 612
Current timestep = 1515. State = [[-0.25251737 -0.01031477]]. Action = [[-0.11005792 -0.13913988 -0.2062346   0.40607536]]. Reward = [0.]
Curr episode timestep = 613
Current timestep = 1516. State = [[-0.25460437 -0.01090575]]. Action = [[ 0.10548657  0.12508768 -0.19506374 -0.40297365]]. Reward = [0.]
Curr episode timestep = 614
Current timestep = 1517. State = [[-0.25430965 -0.0085901 ]]. Action = [[-0.14916874  0.15185314 -0.06446624  0.29988623]]. Reward = [0.]
Curr episode timestep = 615
Current timestep = 1518. State = [[-0.2559251  -0.00490757]]. Action = [[-0.18950525  0.10319    -0.16048045  0.22058666]]. Reward = [0.]
Curr episode timestep = 616
Current timestep = 1519. State = [[-2.6029164e-01  4.5359911e-07]]. Action = [[-0.00562751  0.06472385 -0.2381739  -0.73743254]]. Reward = [0.]
Curr episode timestep = 617
Current timestep = 1520. State = [[-0.26377353  0.00684361]]. Action = [[-0.0693121   0.13103864  0.05642474  0.17616308]]. Reward = [0.]
Curr episode timestep = 618
Current timestep = 1521. State = [[-0.2680917   0.01541446]]. Action = [[-0.22267185  0.004397    0.05396038 -0.71980613]]. Reward = [0.]
Curr episode timestep = 619
Current timestep = 1522. State = [[-0.27567998  0.02331703]]. Action = [[ 0.21742785  0.02867481 -0.09558186 -0.07201219]]. Reward = [0.]
Curr episode timestep = 620
Current timestep = 1523. State = [[-0.2787521   0.03100294]]. Action = [[ 0.13851881  0.20530918 -0.07140708 -0.20595908]]. Reward = [0.]
Curr episode timestep = 621
Current timestep = 1524. State = [[-0.2788673   0.03663964]]. Action = [[ 0.05202302  0.040245   -0.16423878 -0.18418813]]. Reward = [0.]
Curr episode timestep = 622
Current timestep = 1525. State = [[-0.2775438  0.0391481]]. Action = [[-0.16237107 -0.10464031 -0.13038428  0.9790907 ]]. Reward = [0.]
Curr episode timestep = 623
Current timestep = 1526. State = [[-0.2778694   0.03974243]]. Action = [[ 0.07534945  0.15027225  0.13413155 -0.8101898 ]]. Reward = [0.]
Curr episode timestep = 624
Current timestep = 1527. State = [[-0.27779427  0.04005934]]. Action = [[ 0.1737284  -0.03670518  0.02083078  0.96336365]]. Reward = [0.]
Curr episode timestep = 625
Current timestep = 1528. State = [[-0.2754615   0.04195238]]. Action = [[ 0.12030834 -0.18084031 -0.04562995  0.22835183]]. Reward = [0.]
Curr episode timestep = 626
Current timestep = 1529. State = [[-0.27239904  0.0399619 ]]. Action = [[-0.06705077  0.04914111 -0.1870159   0.5525272 ]]. Reward = [0.]
Curr episode timestep = 627
Current timestep = 1530. State = [[-0.2699222   0.03575737]]. Action = [[0.24453312 0.18477127 0.1217652  0.07990885]]. Reward = [0.]
Curr episode timestep = 628
Current timestep = 1531. State = [[-0.26299816  0.0377796 ]]. Action = [[ 0.00617427 -0.13665983 -0.10522254  0.7339206 ]]. Reward = [0.]
Curr episode timestep = 629
Current timestep = 1532. State = [[-0.25863832  0.03622585]]. Action = [[-0.17847922 -0.00930443 -0.20531997  0.82252705]]. Reward = [0.]
Curr episode timestep = 630
Current timestep = 1533. State = [[-0.25803164  0.03517503]]. Action = [[-0.1613977  -0.11669725  0.19805789 -0.9668844 ]]. Reward = [0.]
Curr episode timestep = 631
Current timestep = 1534. State = [[-0.25986084  0.03192406]]. Action = [[ 0.12000903  0.03367361 -0.16489431  0.67565155]]. Reward = [0.]
Curr episode timestep = 632
Current timestep = 1535. State = [[-0.25918722  0.031204  ]]. Action = [[ 0.14169598 -0.04772608  0.06496727 -0.03182292]]. Reward = [0.]
Curr episode timestep = 633
Current timestep = 1536. State = [[-0.2569456   0.02938028]]. Action = [[ 0.21858227  0.06003988 -0.14294589 -0.30018544]]. Reward = [0.]
Curr episode timestep = 634
Current timestep = 1537. State = [[-0.25133973  0.02940609]]. Action = [[ 0.04317582  0.12608653 -0.03201461 -0.46440887]]. Reward = [0.]
Curr episode timestep = 635
Current timestep = 1538. State = [[-0.24611217  0.03196188]]. Action = [[-0.17758292  0.09593511  0.0474537  -0.89919233]]. Reward = [0.]
Curr episode timestep = 636
Current timestep = 1539. State = [[-0.24572133  0.0343945 ]]. Action = [[-0.17861606 -0.1001894  -0.01670839  0.794981  ]]. Reward = [0.]
Curr episode timestep = 637
Current timestep = 1540. State = [[-0.24741542  0.03491732]]. Action = [[ 0.21878675  0.07128266 -0.04404451 -0.24005657]]. Reward = [0.]
Curr episode timestep = 638
Current timestep = 1541. State = [[-0.24690495  0.03597816]]. Action = [[-0.09000298 -0.21920256  0.04086047  0.9648683 ]]. Reward = [0.]
Curr episode timestep = 639
Current timestep = 1542. State = [[-0.24692148  0.03328721]]. Action = [[-0.18658668  0.07749513 -0.03606701  0.21551335]]. Reward = [0.]
Curr episode timestep = 640
Current timestep = 1543. State = [[-0.2494289   0.03297011]]. Action = [[-0.10350725  0.21084028  0.10815322  0.21266639]]. Reward = [0.]
Curr episode timestep = 641
Current timestep = 1544. State = [[-0.25281784  0.03640773]]. Action = [[ 0.1014297  -0.18078244  0.02351078 -0.42025745]]. Reward = [0.]
Curr episode timestep = 642
Current timestep = 1545. State = [[-0.25486243  0.03529543]]. Action = [[-0.02707961  0.03916544 -0.22178252  0.6008525 ]]. Reward = [0.]
Curr episode timestep = 643
Current timestep = 1546. State = [[-0.25613096  0.03593028]]. Action = [[-0.12476027  0.13306454  0.16406125  0.06633127]]. Reward = [0.]
Curr episode timestep = 644
Current timestep = 1547. State = [[-0.25907102  0.03887595]]. Action = [[-0.19465588  0.20513976  0.24706131  0.36474335]]. Reward = [0.]
Curr episode timestep = 645
Current timestep = 1548. State = [[-0.26365834  0.04545265]]. Action = [[ 0.23275548 -0.19540976  0.10961947  0.48373103]]. Reward = [0.]
Curr episode timestep = 646
Current timestep = 1549. State = [[-0.26523274  0.04591985]]. Action = [[-0.24679305  0.02802286 -0.15789855  0.31688356]]. Reward = [0.]
Curr episode timestep = 647
Current timestep = 1550. State = [[-0.26864526  0.04672968]]. Action = [[-0.05768652  0.01026782  0.18044567 -0.2775327 ]]. Reward = [0.]
Curr episode timestep = 648
Current timestep = 1551. State = [[-0.27254423  0.04703174]]. Action = [[ 0.11018142 -0.14983183 -0.24753727  0.6008549 ]]. Reward = [0.]
Curr episode timestep = 649
Current timestep = 1552. State = [[-0.2748319   0.04430165]]. Action = [[ 0.09570566 -0.00965804 -0.13849455 -0.9028893 ]]. Reward = [0.]
Curr episode timestep = 650
Current timestep = 1553. State = [[-0.27477235  0.04370876]]. Action = [[0.20342147 0.0963617  0.17342937 0.20964682]]. Reward = [0.]
Curr episode timestep = 651
Current timestep = 1554. State = [[-0.2713864   0.04514807]]. Action = [[ 0.11708716 -0.14988732 -0.06689259 -0.84539944]]. Reward = [0.]
Curr episode timestep = 652
Current timestep = 1555. State = [[-0.26746687  0.04261195]]. Action = [[ 0.12979034  0.22023848 -0.09658954 -0.7032002 ]]. Reward = [0.]
Curr episode timestep = 653
Current timestep = 1556. State = [[-0.26040098  0.04412926]]. Action = [[ 0.18786454 -0.18789496  0.11707956 -0.47743446]]. Reward = [0.]
Curr episode timestep = 654
Current timestep = 1557. State = [[-0.25296655  0.0415073 ]]. Action = [[-0.09396663  0.24419379  0.07852215  0.7129991 ]]. Reward = [0.]
Curr episode timestep = 655
Current timestep = 1558. State = [[-0.24712028  0.04473054]]. Action = [[ 0.15488982 -0.02777562 -0.06584141 -0.48593247]]. Reward = [0.]
Curr episode timestep = 656
Current timestep = 1559. State = [[-0.24243975  0.04764012]]. Action = [[-0.19076191  0.103221   -0.23309223  0.61978734]]. Reward = [0.]
Curr episode timestep = 657
Current timestep = 1560. State = [[-0.24161793  0.05087285]]. Action = [[ 0.07090312 -0.13657902  0.01077199  0.74522305]]. Reward = [0.]
Curr episode timestep = 658
Current timestep = 1561. State = [[-0.24048778  0.04990247]]. Action = [[-0.04570572 -0.23614295 -0.20540474  0.10212338]]. Reward = [0.]
Curr episode timestep = 659
Current timestep = 1562. State = [[-0.23919258  0.04460371]]. Action = [[-0.20944569  0.05362388 -0.16242021  0.54170275]]. Reward = [0.]
Curr episode timestep = 660
Current timestep = 1563. State = [[-0.24078691  0.04240074]]. Action = [[-0.0765278   0.03989723  0.06526473 -0.16694629]]. Reward = [0.]
Curr episode timestep = 661
Current timestep = 1564. State = [[-0.24433544  0.04023313]]. Action = [[-0.12128009 -0.19099873  0.22697157  0.22691762]]. Reward = [0.]
Curr episode timestep = 662
Current timestep = 1565. State = [[-0.25041014  0.03470362]]. Action = [[-0.03233942 -0.0883889   0.19710281 -0.7711725 ]]. Reward = [0.]
Curr episode timestep = 663
Current timestep = 1566. State = [[-0.2553045   0.02999242]]. Action = [[-0.20103481  0.12190479 -0.06870458 -0.9006954 ]]. Reward = [0.]
Curr episode timestep = 664
Current timestep = 1567. State = [[-0.2613487   0.02887896]]. Action = [[-0.05821501 -0.03553966  0.19795275 -0.8915063 ]]. Reward = [0.]
Curr episode timestep = 665
Current timestep = 1568. State = [[-0.26652852  0.02808845]]. Action = [[ 0.20167649 -0.02287696  0.03142604 -0.46970272]]. Reward = [0.]
Curr episode timestep = 666
Current timestep = 1569. State = [[-0.26880726  0.02660265]]. Action = [[ 0.1883935   0.18822408 -0.19708188  0.7898636 ]]. Reward = [0.]
Curr episode timestep = 667
Current timestep = 1570. State = [[-0.2658666   0.02889311]]. Action = [[-0.14918569  0.02376264  0.0987643   0.21727896]]. Reward = [0.]
Curr episode timestep = 668
Current timestep = 1571. State = [[-0.2659476   0.03030716]]. Action = [[ 0.22329563  0.06229326 -0.17831391  0.41226375]]. Reward = [0.]
Curr episode timestep = 669
Current timestep = 1572. State = [[-0.26228613  0.03469429]]. Action = [[-0.0824621   0.09052595 -0.18064553 -0.3962078 ]]. Reward = [0.]
Curr episode timestep = 670
Current timestep = 1573. State = [[-0.2605617   0.03875776]]. Action = [[ 0.24267888 -0.23549385 -0.24322355 -0.53379774]]. Reward = [0.]
Curr episode timestep = 671
Current timestep = 1574. State = [[-0.25700167  0.03707773]]. Action = [[-1.6769350e-02 -1.2175739e-04 -1.1931825e-01 -1.3838941e-01]]. Reward = [0.]
Curr episode timestep = 672
Current timestep = 1575. State = [[-0.25393036  0.03668305]]. Action = [[-0.11853056  0.21841305 -0.05786893  0.90360856]]. Reward = [0.]
Curr episode timestep = 673
Current timestep = 1576. State = [[-0.25335062  0.04083613]]. Action = [[-0.22075441  0.12109578 -0.21684001 -0.42540634]]. Reward = [0.]
Curr episode timestep = 674
Current timestep = 1577. State = [[-0.25620902  0.04635406]]. Action = [[-0.06959555 -0.03200307  0.10416991 -0.561643  ]]. Reward = [0.]
Curr episode timestep = 675
Current timestep = 1578. State = [[-0.26054808  0.04932698]]. Action = [[-0.23939389 -0.13432486  0.02385843  0.82118666]]. Reward = [0.]
Curr episode timestep = 676
Current timestep = 1579. State = [[-0.26723808  0.0488895 ]]. Action = [[ 0.15605766  0.13209859 -0.0767512  -0.15129006]]. Reward = [0.]
Curr episode timestep = 677
Current timestep = 1580. State = [[-0.2693313   0.05158338]]. Action = [[-0.13302901 -0.04802507  0.05634648  0.74261475]]. Reward = [0.]
Curr episode timestep = 678
Current timestep = 1581. State = [[-0.27206025  0.05278514]]. Action = [[-0.20519802  0.20154536  0.09881845  0.8179649 ]]. Reward = [0.]
Curr episode timestep = 679
Current timestep = 1582. State = [[-0.27862445  0.05793132]]. Action = [[ 0.0509764  -0.1871599  -0.0480832  -0.24395454]]. Reward = [0.]
Curr episode timestep = 680
Current timestep = 1583. State = [[-0.28457037  0.05673938]]. Action = [[ 0.07542619 -0.22651745  0.21987629  0.19507062]]. Reward = [0.]
Curr episode timestep = 681
Current timestep = 1584. State = [[-0.2870059   0.05883638]]. Action = [[-0.059609   -0.22065513 -0.12872982 -0.5536752 ]]. Reward = [0.]
Curr episode timestep = 682
Current timestep = 1585. State = [[-0.28914565  0.06010553]]. Action = [[ 0.18280375 -0.19664577  0.19070101 -0.38610673]]. Reward = [0.]
Curr episode timestep = 683
Current timestep = 1586. State = [[-0.2903066   0.05972062]]. Action = [[ 0.06822583  0.11953202 -0.23274358 -0.90797085]]. Reward = [0.]
Curr episode timestep = 684
Current timestep = 1587. State = [[-0.28998992  0.05971136]]. Action = [[-0.01267582  0.06567925  0.11113757  0.382365  ]]. Reward = [0.]
Curr episode timestep = 685
Current timestep = 1588. State = [[-0.28995064  0.05997733]]. Action = [[ 0.05409664 -0.1456134  -0.1558633  -0.00852138]]. Reward = [0.]
Curr episode timestep = 686
Current timestep = 1589. State = [[-0.28929958  0.06050532]]. Action = [[-0.0035602  -0.21618892 -0.13433434  0.6471329 ]]. Reward = [0.]
Curr episode timestep = 687
Current timestep = 1590. State = [[-0.28948268  0.05990137]]. Action = [[-0.17440356  0.22179925  0.20774245  0.90454984]]. Reward = [0.]
Curr episode timestep = 688
Current timestep = 1591. State = [[-0.29091015  0.05692958]]. Action = [[ 0.00949055 -0.13950722 -0.16502751 -0.1347996 ]]. Reward = [0.]
Curr episode timestep = 689
Current timestep = 1592. State = [[-0.2912221   0.05648357]]. Action = [[ 0.19119221  0.21855277 -0.2419623  -0.45233738]]. Reward = [0.]
Curr episode timestep = 690
Current timestep = 1593. State = [[-0.28906786  0.05826826]]. Action = [[0.12116477 0.17722988 0.12155211 0.29878378]]. Reward = [0.]
Curr episode timestep = 691
Current timestep = 1594. State = [[-0.2857066   0.05750237]]. Action = [[ 0.16714531 -0.02583864 -0.1520243   0.8439555 ]]. Reward = [0.]
Curr episode timestep = 692
Current timestep = 1595. State = [[-0.28006813  0.05713803]]. Action = [[-0.02570999 -0.20891687 -0.21292122  0.9381769 ]]. Reward = [0.]
Curr episode timestep = 693
Current timestep = 1596. State = [[-0.27641582  0.05350069]]. Action = [[-0.03839648  0.11017489  0.18429741  0.14987898]]. Reward = [0.]
Curr episode timestep = 694
Current timestep = 1597. State = [[-0.2730347   0.04712421]]. Action = [[-0.22628267 -0.15351062  0.22994393  0.48612142]]. Reward = [0.]
Curr episode timestep = 695
Current timestep = 1598. State = [[-0.2734048   0.04586204]]. Action = [[0.22666535 0.10798705 0.05841136 0.01229548]]. Reward = [0.]
Curr episode timestep = 696
Current timestep = 1599. State = [[-0.27248546  0.04387096]]. Action = [[-0.17787786 -0.21689795  0.1268647  -0.48175657]]. Reward = [0.]
Curr episode timestep = 697
Current timestep = 1600. State = [[-0.27309102  0.04388752]]. Action = [[ 0.20150048 -0.23094505 -0.1908686  -0.25947618]]. Reward = [0.]
Curr episode timestep = 698
Current timestep = 1601. State = [[-0.2717907   0.04141297]]. Action = [[-0.20278378  0.17607874 -0.16766624 -0.32133645]]. Reward = [0.]
Curr episode timestep = 699
Current timestep = 1602. State = [[-0.27295253  0.04125572]]. Action = [[-0.24129288 -0.16657247  0.21498922  0.90458786]]. Reward = [0.]
Curr episode timestep = 700
Current timestep = 1603. State = [[-0.27771318  0.03711228]]. Action = [[ 0.01848793  0.05408299 -0.07964474  0.70280457]]. Reward = [0.]
Curr episode timestep = 701
Current timestep = 1604. State = [[-0.28034842  0.03732507]]. Action = [[-0.17359821  0.15398121  0.10455179  0.23018146]]. Reward = [0.]
Curr episode timestep = 702
Current timestep = 1605. State = [[-0.28391632  0.03825385]]. Action = [[0.07453898 0.08049375 0.22573715 0.25248408]]. Reward = [0.]
Curr episode timestep = 703
Current timestep = 1606. State = [[-0.28625286  0.04098862]]. Action = [[ 0.18272689  0.05571473 -0.05897847 -0.61709726]]. Reward = [0.]
Curr episode timestep = 704
Current timestep = 1607. State = [[-0.28644526  0.04219189]]. Action = [[-0.17483637  0.01966611 -0.07651161  0.36829233]]. Reward = [0.]
Curr episode timestep = 705
Current timestep = 1608. State = [[-0.28723386  0.04253824]]. Action = [[ 0.13421136  0.20986259 -0.1670255   0.33253622]]. Reward = [0.]
Curr episode timestep = 706
Current timestep = 1609. State = [[-0.28718054  0.04242754]]. Action = [[-0.11645561  0.01389343 -0.07865471  0.23052168]]. Reward = [0.]
Curr episode timestep = 707
Current timestep = 1610. State = [[-0.28733742  0.04303367]]. Action = [[ 0.21865839  0.07272476  0.23613575 -0.7121161 ]]. Reward = [0.]
Curr episode timestep = 708
Current timestep = 1611. State = [[-0.28578928  0.04477589]]. Action = [[-0.07732576 -0.22581074 -0.12371543  0.6621871 ]]. Reward = [0.]
Curr episode timestep = 709
Current timestep = 1612. State = [[-0.2858226   0.04470175]]. Action = [[-0.14543356  0.14006579  0.10087872  0.97011554]]. Reward = [0.]
Curr episode timestep = 710
Current timestep = 1613. State = [[-0.28632188  0.04511109]]. Action = [[ 0.06515345  0.08922571 -0.04762116  0.33407593]]. Reward = [0.]
Curr episode timestep = 711
Current timestep = 1614. State = [[-0.28642547  0.04532245]]. Action = [[ 0.00442669  0.10615528 -0.16050777  0.9016665 ]]. Reward = [0.]
Curr episode timestep = 712
Current timestep = 1615. State = [[-0.28627622  0.045531  ]]. Action = [[ 0.2345779   0.2424534   0.01852924 -0.45092434]]. Reward = [0.]
Curr episode timestep = 713
Current timestep = 1616. State = [[-0.28310668  0.04886612]]. Action = [[ 0.10681289  0.01328769  0.01710394 -0.5158015 ]]. Reward = [0.]
Curr episode timestep = 714
Current timestep = 1617. State = [[-0.2791135   0.05149333]]. Action = [[-0.13367146 -0.11328045 -0.05261752 -0.1001876 ]]. Reward = [0.]
Curr episode timestep = 715
Current timestep = 1618. State = [[-0.27791956  0.05164088]]. Action = [[ 0.18034428  0.17537749 -0.1944055  -0.05437452]]. Reward = [0.]
Curr episode timestep = 716
Current timestep = 1619. State = [[-0.2741596   0.05483104]]. Action = [[-0.13769357 -0.1748849  -0.23591089 -0.28587085]]. Reward = [0.]
Curr episode timestep = 717
Current timestep = 1620. State = [[-0.27346557  0.05332888]]. Action = [[ 0.08658141 -0.01530491 -0.00463547 -0.606365  ]]. Reward = [0.]
Curr episode timestep = 718
Current timestep = 1621. State = [[-0.2719522   0.05198882]]. Action = [[ 0.1733625  -0.04089886 -0.19613357  0.48249996]]. Reward = [0.]
Curr episode timestep = 719
Current timestep = 1622. State = [[-0.2682497  0.0507257]]. Action = [[-0.03387381  0.23097134 -0.11570576 -0.29558617]]. Reward = [0.]
Curr episode timestep = 720
Current timestep = 1623. State = [[-0.26465547  0.05498547]]. Action = [[-0.20874634  0.22581822 -0.07894841 -0.7188186 ]]. Reward = [0.]
Curr episode timestep = 721
Current timestep = 1624. State = [[-0.26468763  0.06193746]]. Action = [[-0.05803856 -0.24116543  0.24892163 -0.2969792 ]]. Reward = [0.]
Curr episode timestep = 722
Current timestep = 1625. State = [[-0.26685426  0.06138381]]. Action = [[-0.24074079 -0.06724755 -0.22842237 -0.84061503]]. Reward = [0.]
Curr episode timestep = 723
Current timestep = 1626. State = [[-0.27245256  0.05942639]]. Action = [[-0.01237325 -0.20218912  0.07318974  0.45741832]]. Reward = [0.]
Curr episode timestep = 724
Current timestep = 1627. State = [[-0.27693462  0.05430659]]. Action = [[ 4.66167927e-04 -1.03861526e-01 -6.98566437e-04  7.47590899e-01]]. Reward = [0.]
Curr episode timestep = 725
Current timestep = 1628. State = [[-0.28206393  0.04608843]]. Action = [[ 0.06544319  0.17014337 -0.1830384  -0.5032403 ]]. Reward = [0.]
Curr episode timestep = 726
Current timestep = 1629. State = [[-0.28259972  0.04646916]]. Action = [[ 0.23122042 -0.18191627  0.02432564 -0.0096612 ]]. Reward = [0.]
Curr episode timestep = 727
Current timestep = 1630. State = [[-0.28041303  0.04862243]]. Action = [[-0.13872778 -0.01310879 -0.10943651 -0.7798999 ]]. Reward = [0.]
Curr episode timestep = 728
Current timestep = 1631. State = [[-0.2807805   0.04875918]]. Action = [[-0.19151168 -0.22942181  0.07970244 -0.7204684 ]]. Reward = [0.]
Curr episode timestep = 729
Current timestep = 1632. State = [[-0.28314996  0.04868837]]. Action = [[ 0.09728867  0.22124392 -0.11765751  0.33181357]]. Reward = [0.]
Curr episode timestep = 730
Current timestep = 1633. State = [[-0.2837115   0.04946722]]. Action = [[ 0.1283642  -0.16870649 -0.22608258  0.02704144]]. Reward = [0.]
Curr episode timestep = 731
Current timestep = 1634. State = [[-0.28357348  0.04947096]]. Action = [[-0.2409574  -0.23595075 -0.13086851 -0.39251125]]. Reward = [0.]
Curr episode timestep = 732
Current timestep = 1635. State = [[-0.28574717  0.04886748]]. Action = [[ 0.20569104 -0.13904487  0.13744062  0.75555444]]. Reward = [0.]
Curr episode timestep = 733
Current timestep = 1636. State = [[-0.28654912  0.04871514]]. Action = [[ 0.19419837  0.06594491 -0.1914725  -0.12944877]]. Reward = [0.]
Curr episode timestep = 734
Current timestep = 1637. State = [[-0.2827822   0.04631681]]. Action = [[ 0.19353375 -0.03346586  0.11451823 -0.9090704 ]]. Reward = [0.]
Curr episode timestep = 735
Current timestep = 1638. State = [[-0.27607226  0.04564865]]. Action = [[ 0.16796297 -0.15576418 -0.24660188  0.4723016 ]]. Reward = [0.]
Curr episode timestep = 736
Current timestep = 1639. State = [[-0.2700852   0.04045864]]. Action = [[-2.0011358e-01 -2.3533763e-01  5.4505467e-04 -8.5219246e-01]]. Reward = [0.]
Curr episode timestep = 737
Current timestep = 1640. State = [[-0.26659098  0.03277292]]. Action = [[-0.00252174  0.0010159  -0.18491524 -0.37913156]]. Reward = [0.]
Curr episode timestep = 738
Current timestep = 1641. State = [[-0.2636643   0.02780056]]. Action = [[-0.19654793 -0.11755686  0.22068787 -0.48239017]]. Reward = [0.]
Curr episode timestep = 739
Current timestep = 1642. State = [[-0.26535848  0.02238773]]. Action = [[0.2171629  0.10793367 0.12887385 0.66903925]]. Reward = [0.]
Curr episode timestep = 740
Current timestep = 1643. State = [[-0.26332787  0.01932898]]. Action = [[-0.17352255 -0.11843704 -0.21003105 -0.1203571 ]]. Reward = [0.]
Curr episode timestep = 741
Current timestep = 1644. State = [[-0.26493073  0.01760315]]. Action = [[-0.24458522  0.19460851 -0.2251743   0.2852713 ]]. Reward = [0.]
Curr episode timestep = 742
Current timestep = 1645. State = [[-0.26857984  0.01864298]]. Action = [[-0.03811771  0.2480888   0.09363943  0.37782705]]. Reward = [0.]
Curr episode timestep = 743
Current timestep = 1646. State = [[-0.27134454  0.02154908]]. Action = [[ 0.03322554 -0.20394097 -0.11486593  0.85265124]]. Reward = [0.]
Curr episode timestep = 744
Current timestep = 1647. State = [[-0.2729782   0.02446931]]. Action = [[ 0.24340296  0.2336666  -0.02845819  0.44154108]]. Reward = [0.]
Curr episode timestep = 745
Current timestep = 1648. State = [[-0.27246997  0.02335293]]. Action = [[-0.21692713 -0.1374459  -0.17492765 -0.00424594]]. Reward = [0.]
Curr episode timestep = 746
Current timestep = 1649. State = [[-0.27356556  0.02386488]]. Action = [[ 0.21070153 -0.11799629  0.20397562 -0.7114413 ]]. Reward = [0.]
Curr episode timestep = 747
Current timestep = 1650. State = [[-0.2729      0.02500967]]. Action = [[2.0602286e-01 7.7366829e-04 5.6165159e-02 8.0284178e-01]]. Reward = [0.]
Curr episode timestep = 748
Current timestep = 1651. State = [[-0.27003592  0.02234968]]. Action = [[-0.19813298  0.08427912 -0.03332071  0.30049062]]. Reward = [0.]
Curr episode timestep = 749
Current timestep = 1652. State = [[-0.26953143  0.02202624]]. Action = [[-0.11248308 -0.07119749 -0.03770417 -0.9644954 ]]. Reward = [0.]
Curr episode timestep = 750
Current timestep = 1653. State = [[-0.2703906   0.02239508]]. Action = [[-0.13791835 -0.03391345 -0.2188121  -0.6196496 ]]. Reward = [0.]
Curr episode timestep = 751
Current timestep = 1654. State = [[-0.27250963  0.02319271]]. Action = [[-0.10460746 -0.21147646 -0.17728823  0.25063825]]. Reward = [0.]
Curr episode timestep = 752
Current timestep = 1655. State = [[-0.27701774  0.02099061]]. Action = [[-0.09176084 -0.10962133 -0.1714669  -0.1976757 ]]. Reward = [0.]
Curr episode timestep = 753
Current timestep = 1656. State = [[-0.28173086  0.02016478]]. Action = [[0.18616846 0.07402104 0.12412295 0.03441119]]. Reward = [0.]
Curr episode timestep = 754
Current timestep = 1657. State = [[-0.28276578  0.02088394]]. Action = [[-0.08370334  0.01959565 -0.09055457  0.19931412]]. Reward = [0.]
Curr episode timestep = 755
Current timestep = 1658. State = [[-0.2838626   0.02087426]]. Action = [[-0.21165961 -0.01259634  0.17171806 -0.5567987 ]]. Reward = [0.]
Curr episode timestep = 756
Current timestep = 1659. State = [[-0.28830191  0.02008323]]. Action = [[-0.18089576 -0.24072635 -0.1704103   0.5916796 ]]. Reward = [0.]
Curr episode timestep = 757
Current timestep = 1660. State = [[-0.29565862  0.01976102]]. Action = [[ 0.05977681  0.15635246  0.17929858 -0.6043028 ]]. Reward = [0.]
Curr episode timestep = 758
Current timestep = 1661. State = [[-0.3008763   0.02141535]]. Action = [[-0.16961429 -0.01327854  0.14612919 -0.70860815]]. Reward = [0.]
Curr episode timestep = 759
Current timestep = 1662. State = [[-0.30905184  0.02233042]]. Action = [[ 0.04138601 -0.12722252 -0.10996644  0.73969066]]. Reward = [0.]
Curr episode timestep = 760
Current timestep = 1663. State = [[-0.3142144   0.02440462]]. Action = [[ 0.03602451 -0.11588484  0.11770669  0.51648784]]. Reward = [0.]
Curr episode timestep = 761
Current timestep = 1664. State = [[-0.31776208  0.02633312]]. Action = [[-0.05913636 -0.21837616  0.1923143  -0.7065173 ]]. Reward = [0.]
Curr episode timestep = 762
Current timestep = 1665. State = [[-0.32162333  0.02678243]]. Action = [[-0.2237029  -0.03890225  0.03389934 -0.68709165]]. Reward = [0.]
Curr episode timestep = 763
Current timestep = 1666. State = [[-0.32883868  0.02611226]]. Action = [[ 0.22497553  0.10273027 -0.01351383  0.7886138 ]]. Reward = [0.]
Curr episode timestep = 764
Current timestep = 1667. State = [[-0.33098203  0.0288655 ]]. Action = [[-0.17188402  0.07307604  0.1348938  -0.6602654 ]]. Reward = [0.]
Curr episode timestep = 765
Current timestep = 1668. State = [[-0.3352506  0.029601 ]]. Action = [[-0.13475809 -0.20856574  0.20725304 -0.8617276 ]]. Reward = [0.]
Curr episode timestep = 766
Current timestep = 1669. State = [[-0.34260914  0.0280619 ]]. Action = [[-0.18103266  0.13178477  0.01873481  0.3767047 ]]. Reward = [0.]
Curr episode timestep = 767
Current timestep = 1670. State = [[-0.35240453  0.02605695]]. Action = [[ 0.12248278 -0.19796403  0.08089212  0.717327  ]]. Reward = [0.]
Curr episode timestep = 768
Current timestep = 1671. State = [[-0.35714716  0.02714917]]. Action = [[ 0.07620096  0.14985421 -0.138905   -0.69412416]]. Reward = [0.]
Curr episode timestep = 769
Current timestep = 1672. State = [[-0.3584376   0.02974433]]. Action = [[-0.03447634 -0.18481225 -0.13680841  0.35649943]]. Reward = [0.]
Curr episode timestep = 770
Current timestep = 1673. State = [[-0.3607567   0.03104783]]. Action = [[-0.22150949  0.20222199 -0.2128741   0.80196095]]. Reward = [0.]
Curr episode timestep = 771
Current timestep = 1674. State = [[-0.36648872  0.03066481]]. Action = [[-0.17270169 -0.2059349   0.06432834  0.17102015]]. Reward = [0.]
Curr episode timestep = 772
Current timestep = 1675. State = [[-0.37643176  0.02775542]]. Action = [[ 0.03025979  0.12300274  0.17325997 -0.01420552]]. Reward = [0.]
Curr episode timestep = 773
Current timestep = 1676. State = [[-0.38358903  0.027339  ]]. Action = [[ 0.09390694  0.15628871 -0.15094998 -0.39005172]]. Reward = [0.]
Curr episode timestep = 774
Current timestep = 1677. State = [[-0.38535413  0.02978103]]. Action = [[-0.126244    0.14539447 -0.19785222 -0.66312164]]. Reward = [0.]
Curr episode timestep = 775
Current timestep = 1678. State = [[-0.38669866  0.03188691]]. Action = [[-0.22070116 -0.09303923  0.19108528 -0.8261448 ]]. Reward = [0.]
Curr episode timestep = 776
Current timestep = 1679. State = [[-0.3879999   0.03390948]]. Action = [[-0.19643207 -0.02817014  0.11168194  0.42571807]]. Reward = [0.]
Curr episode timestep = 777
Current timestep = 1680. State = [[-0.3891991   0.03553108]]. Action = [[ 0.19312328  0.10061395  0.2192486  -0.56812096]]. Reward = [0.]
Curr episode timestep = 778
Current timestep = 1681. State = [[-0.38849798  0.03689981]]. Action = [[-0.20815712  0.02935186  0.21033445  0.33673894]]. Reward = [0.]
Curr episode timestep = 779
Current timestep = 1682. State = [[-0.3881191   0.03725789]]. Action = [[-0.02313299 -0.09087132 -0.18336166  0.07258546]]. Reward = [0.]
Curr episode timestep = 780
Current timestep = 1683. State = [[-0.38799375  0.0373627 ]]. Action = [[-0.20018531  0.07022005  0.21650669  0.9427285 ]]. Reward = [0.]
Curr episode timestep = 781
Current timestep = 1684. State = [[-0.38764235  0.03770971]]. Action = [[ 0.13871527  0.07788101 -0.09944692  0.25809455]]. Reward = [0.]
Curr episode timestep = 782
Current timestep = 1685. State = [[-0.3850351   0.04031891]]. Action = [[ 0.24377602 -0.16879451  0.06616205 -0.9116362 ]]. Reward = [0.]
Curr episode timestep = 783
Current timestep = 1686. State = [[-0.3793467   0.04524451]]. Action = [[ 0.15957278 -0.22288729  0.01516756 -0.7875406 ]]. Reward = [0.]
Curr episode timestep = 784
Current timestep = 1687. State = [[-0.3739689   0.04926259]]. Action = [[-0.19398196  0.0532214   0.22638965  0.19880438]]. Reward = [0.]
Curr episode timestep = 785
Current timestep = 1688. State = [[-0.3699296   0.05328622]]. Action = [[ 0.00147697  0.07500297 -0.02920868 -0.45288873]]. Reward = [0.]
Curr episode timestep = 786
Current timestep = 1689. State = [[-0.3673567   0.05612985]]. Action = [[0.06799212 0.24385178 0.18840319 0.6292151 ]]. Reward = [0.]
Curr episode timestep = 787
Current timestep = 1690. State = [[-0.36303124  0.06051623]]. Action = [[-0.06203929  0.0305357  -0.2055259   0.50972176]]. Reward = [0.]
Curr episode timestep = 788
Current timestep = 1691. State = [[-0.36135626  0.06265751]]. Action = [[ 0.06121755 -0.19337672 -0.11272109  0.5118277 ]]. Reward = [0.]
Curr episode timestep = 789
Current timestep = 1692. State = [[-0.3603141   0.06359866]]. Action = [[-0.09043953  0.02640986 -0.14275819  0.64045036]]. Reward = [0.]
Curr episode timestep = 790
Current timestep = 1693. State = [[-0.36029443  0.06352317]]. Action = [[-0.00858714  0.04457268 -0.19932625  0.27058876]]. Reward = [0.]
Curr episode timestep = 791
Current timestep = 1694. State = [[-0.36032614  0.06344053]]. Action = [[-0.11174861  0.04011685 -0.07554701  0.20510483]]. Reward = [0.]
Curr episode timestep = 792
Current timestep = 1695. State = [[-0.3602594   0.06348298]]. Action = [[ 0.20930368  0.2234138  -0.2029358   0.6310165 ]]. Reward = [0.]
Curr episode timestep = 793
Current timestep = 1696. State = [[-0.3581608   0.06570575]]. Action = [[-0.1541091   0.22160089  0.24866658 -0.8492943 ]]. Reward = [0.]
Curr episode timestep = 794
Current timestep = 1697. State = [[-0.35766995  0.06616902]]. Action = [[-0.05405578  0.15990615 -0.04615022  0.8839513 ]]. Reward = [0.]
Curr episode timestep = 795
Current timestep = 1698. State = [[-0.35750648  0.0663439 ]]. Action = [[-0.06829926  0.15962964 -0.10650975  0.32807362]]. Reward = [0.]
Curr episode timestep = 796
Current timestep = 1699. State = [[-0.3575107   0.06643712]]. Action = [[-0.05439059  0.24322045 -0.0449404  -0.6633951 ]]. Reward = [0.]
Curr episode timestep = 797
Current timestep = 1700. State = [[-0.35774142  0.06695163]]. Action = [[-0.23425278  0.19456702 -0.10308085  0.0759511 ]]. Reward = [0.]
Curr episode timestep = 798
Current timestep = 1701. State = [[-0.35877433  0.06803738]]. Action = [[ 0.15345389 -0.13873465  0.17806876  0.56983566]]. Reward = [0.]
Curr episode timestep = 799
Current timestep = 1702. State = [[-0.35880795  0.06829184]]. Action = [[-0.02314113 -0.17933083  0.20758498 -0.04872131]]. Reward = [0.]
Curr episode timestep = 800
Current timestep = 1703. State = [[-0.35921746  0.06791458]]. Action = [[-0.11604041 -0.21547651 -0.10250369 -0.63381046]]. Reward = [0.]
Curr episode timestep = 801
Current timestep = 1704. State = [[-0.3616413   0.06551316]]. Action = [[ 0.20004442  0.23696983 -0.20960358 -0.10283756]]. Reward = [0.]
Curr episode timestep = 802
Current timestep = 1705. State = [[-0.36119258  0.06594577]]. Action = [[-0.03404067  0.24773276 -0.20875424 -0.058819  ]]. Reward = [0.]
Curr episode timestep = 803
Current timestep = 1706. State = [[-0.3605107   0.06650308]]. Action = [[-0.02867997 -0.00473262 -0.11653566  0.97030854]]. Reward = [0.]
Curr episode timestep = 804
Current timestep = 1707. State = [[-0.36004418  0.06665805]]. Action = [[0.16966552 0.08906868 0.04262918 0.8560357 ]]. Reward = [0.]
Curr episode timestep = 805
Current timestep = 1708. State = [[-0.3567546   0.06973726]]. Action = [[0.05267364 0.24749479 0.1624639  0.37709498]]. Reward = [0.]
Curr episode timestep = 806
Current timestep = 1709. State = [[-0.3525051   0.07487328]]. Action = [[ 0.16980892 -0.00198245 -0.14829329  0.77170825]]. Reward = [0.]
Curr episode timestep = 807
Current timestep = 1710. State = [[-0.34670132  0.08156532]]. Action = [[-0.02135246  0.09014151  0.11026895  0.44752657]]. Reward = [0.]
Curr episode timestep = 808
Current timestep = 1711. State = [[-0.34232238  0.08728276]]. Action = [[0.06767583 0.14922863 0.24853897 0.73752356]]. Reward = [0.]
Curr episode timestep = 809
Current timestep = 1712. State = [[-0.33816966  0.09266241]]. Action = [[ 0.20979226 -0.23847678 -0.15787067  0.27703297]]. Reward = [0.]
Curr episode timestep = 810
Current timestep = 1713. State = [[-0.33231485  0.09958653]]. Action = [[0.14141971 0.2092855  0.01506394 0.20093727]]. Reward = [0.]
Curr episode timestep = 811
Current timestep = 1714. State = [[-0.3249582   0.10701699]]. Action = [[ 0.12442756  0.00164229 -0.1173078   0.15856099]]. Reward = [0.]
Curr episode timestep = 812
Current timestep = 1715. State = [[-0.31863487  0.10729063]]. Action = [[-0.11089888 -0.00782874  0.12143397  0.36618233]]. Reward = [0.]
Curr episode timestep = 813
Current timestep = 1716. State = [[-0.31249574  0.10304946]]. Action = [[-0.1527193  -0.0034824  -0.08014429 -0.11345565]]. Reward = [0.]
Curr episode timestep = 814
Current timestep = 1717. State = [[-0.31028226  0.09989141]]. Action = [[-0.13340232 -0.2054314   0.14124936 -0.5548081 ]]. Reward = [0.]
Curr episode timestep = 815
Current timestep = 1718. State = [[-0.31113595  0.09840098]]. Action = [[-0.00438805 -0.08380961 -0.06740612 -0.09083235]]. Reward = [0.]
Curr episode timestep = 816
Current timestep = 1719. State = [[-0.31273344  0.09810778]]. Action = [[ 0.15850532 -0.1503826   0.10223463 -0.61732787]]. Reward = [0.]
Curr episode timestep = 817
Current timestep = 1720. State = [[-0.31260693  0.09472203]]. Action = [[ 0.13068163  0.19034314  0.17876798 -0.27487147]]. Reward = [0.]
Curr episode timestep = 818
Current timestep = 1721. State = [[-0.3091122   0.09433245]]. Action = [[ 0.12421134 -0.20179233 -0.08111422 -0.41614735]]. Reward = [0.]
Curr episode timestep = 819
Current timestep = 1722. State = [[-0.30534345  0.08993469]]. Action = [[ 0.04252088  0.07417744 -0.05738783  0.61996853]]. Reward = [0.]
Curr episode timestep = 820
Current timestep = 1723. State = [[-0.29860887  0.08601744]]. Action = [[ 0.1712743   0.04286385  0.10061702 -0.36916327]]. Reward = [0.]
Curr episode timestep = 821
Current timestep = 1724. State = [[-0.29112402  0.08286112]]. Action = [[-0.11840326  0.16926986  0.19741696  0.32916546]]. Reward = [0.]
Curr episode timestep = 822
Current timestep = 1725. State = [[-0.28600422  0.07627403]]. Action = [[-0.04143395  0.07453308 -0.1773909  -0.692641  ]]. Reward = [0.]
Curr episode timestep = 823
Current timestep = 1726. State = [[-0.28308654  0.07184565]]. Action = [[ 0.22547191  0.1681602   0.16101551 -0.68288606]]. Reward = [0.]
Curr episode timestep = 824
Current timestep = 1727. State = [[-0.27689055  0.06817304]]. Action = [[-0.236529   -0.21946383  0.13777077  0.42991364]]. Reward = [0.]
Curr episode timestep = 825
Current timestep = 1728. State = [[-0.2746739   0.06363206]]. Action = [[ 0.12761813  0.13695994 -0.13386682 -0.0702976 ]]. Reward = [0.]
Curr episode timestep = 826
Current timestep = 1729. State = [[-0.27243477  0.06191295]]. Action = [[ 0.0729661  -0.01714812 -0.07663459 -0.71167964]]. Reward = [0.]
Curr episode timestep = 827
Current timestep = 1730. State = [[-0.26970792  0.05812729]]. Action = [[-0.22374958  0.0641306   0.20812008  0.03766584]]. Reward = [0.]
Curr episode timestep = 828
Current timestep = 1731. State = [[-0.2697768  0.0568387]]. Action = [[-0.19345365  0.13605928  0.09761763 -0.5212466 ]]. Reward = [0.]
Curr episode timestep = 829
Current timestep = 1732. State = [[-0.273052    0.05750213]]. Action = [[ 0.0210529  -0.14990616 -0.07921998  0.41634917]]. Reward = [0.]
Curr episode timestep = 830
Current timestep = 1733. State = [[-0.2751368   0.05596715]]. Action = [[ 0.12110838  0.03126588 -0.10882792  0.19223368]]. Reward = [0.]
Curr episode timestep = 831
Current timestep = 1734. State = [[-0.2755466   0.05690214]]. Action = [[-0.20302136 -0.11769846 -0.00225714 -0.7959908 ]]. Reward = [0.]
Curr episode timestep = 832
Current timestep = 1735. State = [[-0.27750278  0.0582717 ]]. Action = [[ 0.07715082 -0.03461289 -0.09939303  0.65661883]]. Reward = [0.]
Curr episode timestep = 833
Current timestep = 1736. State = [[-0.2790639   0.06075611]]. Action = [[-0.09428521  0.10856259  0.2448614  -0.2716158 ]]. Reward = [0.]
Curr episode timestep = 834
Current timestep = 1737. State = [[-0.2812742   0.06399377]]. Action = [[0.00104958 0.12236285 0.22836095 0.02625418]]. Reward = [0.]
Curr episode timestep = 835
Current timestep = 1738. State = [[-0.28307864  0.06728268]]. Action = [[ 0.09416097  0.06864125 -0.09721759  0.64038265]]. Reward = [0.]
Curr episode timestep = 836
Current timestep = 1739. State = [[-0.28426358  0.06923091]]. Action = [[-0.07009381  0.18718603  0.17960578  0.7888893 ]]. Reward = [0.]
Curr episode timestep = 837
Current timestep = 1740. State = [[-0.2851996   0.07110436]]. Action = [[ 0.04645467  0.21701118 -0.0346714   0.14164758]]. Reward = [0.]
Curr episode timestep = 838
Current timestep = 1741. State = [[-0.28503308  0.07644156]]. Action = [[-0.10916668  0.08199224 -0.18716279  0.7978251 ]]. Reward = [0.]
Curr episode timestep = 839
Current timestep = 1742. State = [[-0.28731626  0.08380075]]. Action = [[ 0.02990872 -0.2148772   0.16986465  0.40545595]]. Reward = [0.]
Curr episode timestep = 840
Current timestep = 1743. State = [[-0.2887467   0.08469437]]. Action = [[ 0.03348666  0.19684568 -0.02377912  0.02505219]]. Reward = [0.]
Curr episode timestep = 841
Current timestep = 1744. State = [[-0.2892288   0.08916753]]. Action = [[-0.17324297 -0.13483308  0.24231857  0.21876347]]. Reward = [0.]
Curr episode timestep = 842
Current timestep = 1745. State = [[-0.29113886  0.08930912]]. Action = [[-1.2207222e-01  1.5670553e-01  2.8976798e-04 -3.6779833e-01]]. Reward = [0.]
Curr episode timestep = 843
Current timestep = 1746. State = [[-0.29549763  0.09419576]]. Action = [[-0.10379431  0.21846855  0.16812098  0.13165772]]. Reward = [0.]
Curr episode timestep = 844
Current timestep = 1747. State = [[-0.30080205  0.10206722]]. Action = [[-0.05016945  0.23668423  0.22912455 -0.27993834]]. Reward = [0.]
Curr episode timestep = 845
Current timestep = 1748. State = [[-0.30768532  0.11267354]]. Action = [[ 0.07138011 -0.07880157  0.16454482  0.75683236]]. Reward = [0.]
Curr episode timestep = 846
Current timestep = 1749. State = [[-0.31130514  0.11811884]]. Action = [[-0.09564824  0.22801027  0.12920535 -0.41321146]]. Reward = [0.]
Curr episode timestep = 847
Current timestep = 1750. State = [[-0.3154954   0.12597236]]. Action = [[ 0.0861465   0.10009202 -0.18595211  0.37246442]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 1750 is [True, False, False, False, False, True]
State prediction error at timestep 1750 is tensor(3.1600e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1751. State = [[-0.31757632  0.13449688]]. Action = [[ 0.11662164 -0.17683566 -0.24650702 -0.29591435]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 1751 is [True, False, False, False, False, True]
State prediction error at timestep 1751 is tensor(3.2844e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1752. State = [[-0.3161951   0.13688967]]. Action = [[0.10470271 0.1523248  0.16417176 0.11161137]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 1752 is [True, False, False, False, False, True]
State prediction error at timestep 1752 is tensor(1.9451e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1753. State = [[-0.31342456  0.14203098]]. Action = [[ 0.1795688   0.17948607  0.09501553 -0.88589853]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 1753 is [True, False, False, False, False, True]
State prediction error at timestep 1753 is tensor(3.1762e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1754. State = [[-0.30721986  0.1493223 ]]. Action = [[ 0.15581605  0.01230443 -0.22892465 -0.20796382]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 1754 is [True, False, False, False, False, True]
State prediction error at timestep 1754 is tensor(3.8947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1755. State = [[-0.29949686  0.15375239]]. Action = [[ 0.22654822  0.00209492 -0.05259228 -0.9040802 ]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 1755 is [True, False, False, False, False, True]
State prediction error at timestep 1755 is tensor(4.2833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1756. State = [[-0.28885132  0.15623   ]]. Action = [[ 0.19920856  0.16481891 -0.00492412 -0.36246097]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 1756 is [True, False, False, False, False, True]
State prediction error at timestep 1756 is tensor(3.4517e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1757. State = [[-0.27753067  0.16303508]]. Action = [[ 0.21758789 -0.20837192 -0.10307997 -0.17268848]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 1757 is [True, False, False, False, False, True]
State prediction error at timestep 1757 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1758. State = [[-0.26280063  0.16158997]]. Action = [[ 0.05426523  0.02299571  0.06407797 -0.86623126]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 1758 is [True, False, False, False, False, True]
State prediction error at timestep 1758 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1759. State = [[-0.25131166  0.16135581]]. Action = [[-0.01659442  0.12645257 -0.08944628 -0.6899908 ]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 1759 is [True, False, False, False, False, True]
State prediction error at timestep 1759 is tensor(4.9914e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1759 of 1
Current timestep = 1760. State = [[-0.24422196  0.16456151]]. Action = [[ 0.05843061  0.00531447  0.24581334 -0.656064  ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 1760 is [True, False, False, False, False, True]
State prediction error at timestep 1760 is tensor(2.0838e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1761. State = [[-0.23948638  0.16643889]]. Action = [[-0.23318277 -0.08868843 -0.01478225  0.88737595]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 1761 is [True, False, False, False, False, True]
State prediction error at timestep 1761 is tensor(1.4170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1762. State = [[-0.23989882  0.1667301 ]]. Action = [[-0.19449784  0.19739783  0.24550325  0.368268  ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 1762 is [True, False, False, False, False, True]
State prediction error at timestep 1762 is tensor(9.1051e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1762 of 1
Current timestep = 1763. State = [[-0.24459474  0.17204607]]. Action = [[ 0.17816693  0.03826469 -0.13273107  0.31221724]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 1763 is [True, False, False, False, False, True]
State prediction error at timestep 1763 is tensor(5.8149e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1763 of 1
Current timestep = 1764. State = [[-0.24503776  0.17566153]]. Action = [[ 0.22857839  0.0031316  -0.1935125  -0.8145006 ]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 1764 is [True, False, False, False, False, True]
State prediction error at timestep 1764 is tensor(2.8117e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1765. State = [[-0.24027942  0.17880024]]. Action = [[ 0.00644684  0.14013931 -0.13517411 -0.123501  ]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 1765 is [True, False, False, False, False, True]
State prediction error at timestep 1765 is tensor(4.6388e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1766. State = [[-0.23674637  0.18422295]]. Action = [[ 0.12977159  0.15224478 -0.15544531  0.6335188 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 1766 is [True, False, False, False, False, True]
State prediction error at timestep 1766 is tensor(3.7690e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1766 of 1
Current timestep = 1767. State = [[-0.23141536  0.19212802]]. Action = [[ 0.051999   -0.05222131  0.20891547  0.4444579 ]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 1767 is [True, False, False, False, False, True]
State prediction error at timestep 1767 is tensor(6.9307e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1767 of 1
Current timestep = 1768. State = [[-0.22625363  0.19578032]]. Action = [[ 0.22851685 -0.20643263 -0.0349108   0.09443498]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 1768 is [True, False, False, False, False, True]
State prediction error at timestep 1768 is tensor(4.6878e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1769. State = [[-0.22024693  0.19076551]]. Action = [[ 0.04308665  0.2020852  -0.10960945  0.00367486]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 1769 is [True, False, False, False, False, True]
State prediction error at timestep 1769 is tensor(3.4778e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1769 of 1
Current timestep = 1770. State = [[-0.21247424  0.19353561]]. Action = [[ 0.14994326  0.03554904  0.23555732 -0.742635  ]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 1770 is [True, False, False, False, False, True]
State prediction error at timestep 1770 is tensor(3.6512e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1771. State = [[-0.20390351  0.19698328]]. Action = [[ 0.04888415 -0.05075975  0.14310375 -0.24147898]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 1771 is [True, False, False, False, False, True]
State prediction error at timestep 1771 is tensor(4.9101e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1771 of 1
Current timestep = 1772. State = [[-0.19593915  0.19839461]]. Action = [[-0.21324894 -0.09481928 -0.16726446  0.31588125]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 1772 is [True, False, False, False, False, True]
State prediction error at timestep 1772 is tensor(3.3150e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1773. State = [[-0.19508354  0.19628213]]. Action = [[-0.13059446  0.06017643  0.0385884  -0.94633585]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 1773 is [True, False, False, False, False, True]
State prediction error at timestep 1773 is tensor(2.8792e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1773 of 1
Current timestep = 1774. State = [[-0.1971512   0.19614676]]. Action = [[-0.24195     0.14647827 -0.07175979  0.12259424]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 1774 is [True, False, False, False, False, True]
State prediction error at timestep 1774 is tensor(5.8303e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1775. State = [[-0.20167634  0.20043322]]. Action = [[ 0.06663939  0.16901457 -0.03183821  0.4316367 ]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 1775 is [True, False, False, False, False, True]
State prediction error at timestep 1775 is tensor(7.7942e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1775 of 1
Current timestep = 1776. State = [[-0.20647219  0.2086235 ]]. Action = [[ 0.21894187  0.21812499 -0.01025023  0.11297798]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 1776 is [True, False, False, False, False, True]
State prediction error at timestep 1776 is tensor(6.7631e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1776 of 1
Current timestep = 1777. State = [[-0.20688468  0.21963668]]. Action = [[-0.06211877 -0.08421357  0.03183705 -0.6678828 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 1777 is [True, False, False, False, False, True]
State prediction error at timestep 1777 is tensor(3.1799e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1778. State = [[-0.20589896  0.2247062 ]]. Action = [[-0.10966131  0.204683   -0.07897317  0.32638776]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 1778 is [True, False, False, False, False, True]
State prediction error at timestep 1778 is tensor(2.6950e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1778 of 1
Current timestep = 1779. State = [[-0.20850459  0.23246516]]. Action = [[-0.13878913 -0.20629734 -0.22812808  0.9080448 ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 1779 is [True, False, False, False, False, True]
State prediction error at timestep 1779 is tensor(5.3705e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1780. State = [[-0.21165702  0.23197235]]. Action = [[ 0.109743   -0.23642845  0.06540406  0.35852766]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 1780 is [True, False, False, False, False, True]
State prediction error at timestep 1780 is tensor(1.1403e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1780 of 1
Current timestep = 1781. State = [[-0.21202943  0.22624154]]. Action = [[ 0.06782967  0.21125677 -0.19390506 -0.73957163]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 1781 is [True, False, False, False, False, True]
State prediction error at timestep 1781 is tensor(7.8495e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1781 of 1
Current timestep = 1782. State = [[-0.21093947  0.22620264]]. Action = [[ 0.180561   -0.1866063   0.14278826  0.94809556]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 1782 is [True, False, False, False, False, True]
State prediction error at timestep 1782 is tensor(2.1332e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1783. State = [[-0.20607871  0.22199874]]. Action = [[-0.00385229  0.00182104  0.17564005 -0.05178738]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 1783 is [True, False, False, False, False, True]
State prediction error at timestep 1783 is tensor(1.0897e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1784. State = [[-0.20309511  0.22002906]]. Action = [[ 0.08703968  0.20142972 -0.0515877   0.44557214]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 1784 is [True, False, False, False, False, True]
State prediction error at timestep 1784 is tensor(2.1353e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1784 of 1
Current timestep = 1785. State = [[-0.2001664   0.22443923]]. Action = [[ 0.09089375  0.08228087 -0.21483453 -0.43998194]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 1785 is [True, False, False, False, False, True]
State prediction error at timestep 1785 is tensor(1.3546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1785 of 1
Current timestep = 1786. State = [[-0.19653414  0.2301166 ]]. Action = [[ 0.08393314  0.24262196 -0.19764158  0.05050385]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 1786 is [True, False, False, False, False, True]
State prediction error at timestep 1786 is tensor(2.0495e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1787. State = [[-0.19180286  0.23949762]]. Action = [[-0.22279869 -0.23742336 -0.09207059 -0.98718905]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 1787 is [True, False, False, False, False, True]
State prediction error at timestep 1787 is tensor(8.4746e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1788. State = [[-0.1920598   0.23828639]]. Action = [[-0.19014099 -0.08993882 -0.19110951 -0.25589812]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 1788 is [True, False, False, False, False, True]
State prediction error at timestep 1788 is tensor(1.3685e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1789. State = [[-0.19476391  0.23637174]]. Action = [[-0.1071769  -0.11632408  0.2220065   0.37638712]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 1789 is [True, False, False, False, False, True]
State prediction error at timestep 1789 is tensor(6.6231e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1790. State = [[-0.19916949  0.23073637]]. Action = [[ 0.20888379  0.02869475 -0.13102065 -0.69290215]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 1790 is [True, False, False, False, False, True]
State prediction error at timestep 1790 is tensor(8.9028e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1790 of 1
Current timestep = 1791. State = [[-0.19814493  0.22779919]]. Action = [[ 0.01733032 -0.19990925  0.03439811  0.8281653 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 1791 is [True, False, False, False, False, True]
State prediction error at timestep 1791 is tensor(2.8325e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1791 of 1
Current timestep = 1792. State = [[-0.19652747  0.22237709]]. Action = [[ 0.13581467  0.19858801  0.22187704 -0.17115676]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 1792 is [True, False, False, False, False, True]
State prediction error at timestep 1792 is tensor(3.3841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1793. State = [[-0.19480243  0.22247608]]. Action = [[-0.07000506 -0.14676033 -0.0470846   0.8336539 ]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 1793 is [True, False, False, False, False, True]
State prediction error at timestep 1793 is tensor(1.8906e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1794. State = [[-0.19350769  0.21981493]]. Action = [[-0.2185965  -0.09083651 -0.08561417  0.30905366]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 1794 is [True, False, False, False, False, True]
State prediction error at timestep 1794 is tensor(4.4530e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1794 of 1
Current timestep = 1795. State = [[-0.19720034  0.21474959]]. Action = [[-0.22039582 -0.11451939  0.19711137  0.21545136]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 1795 is [True, False, False, False, False, True]
State prediction error at timestep 1795 is tensor(1.4073e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1795 of 1
Current timestep = 1796. State = [[-0.20411612  0.20943308]]. Action = [[-0.06492317  0.21669436 -0.10973153 -0.9757603 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 1796 is [True, False, False, False, False, True]
State prediction error at timestep 1796 is tensor(2.9268e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1797. State = [[-0.20979656  0.21290714]]. Action = [[ 0.16103837  0.10094473 -0.20912287  0.3840506 ]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 1797 is [True, False, False, False, False, True]
State prediction error at timestep 1797 is tensor(1.4997e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1797 of 1
Current timestep = 1798. State = [[-0.21212186  0.21730922]]. Action = [[-0.07354295 -0.16432808 -0.13765562  0.8726939 ]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 1798 is [True, False, False, False, False, True]
State prediction error at timestep 1798 is tensor(5.1726e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1798 of 1
Current timestep = 1799. State = [[-0.21399747  0.21551788]]. Action = [[ 0.23736042  0.00825387  0.08646619 -0.3618083 ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 1799 is [True, False, False, False, False, True]
State prediction error at timestep 1799 is tensor(9.1038e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1800. State = [[-0.21239232  0.21205892]]. Action = [[-0.24296695 -0.07497475  0.14737019 -0.27249086]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 1800 is [True, False, False, False, False, True]
State prediction error at timestep 1800 is tensor(1.8132e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1800 of -1
Current timestep = 1801. State = [[-0.21400526  0.2099256 ]]. Action = [[-0.00033377 -0.09031817 -0.11046216  0.1305927 ]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 1801 is [True, False, False, False, False, True]
State prediction error at timestep 1801 is tensor(8.3386e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1802. State = [[-0.21546899  0.20621555]]. Action = [[0.13850272 0.15314794 0.22092116 0.6054821 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 1802 is [True, False, False, False, False, True]
State prediction error at timestep 1802 is tensor(9.0953e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1803. State = [[-0.25778812  0.00805448]]. Action = [[-0.03737277  0.12576121  0.17693377  0.03074944]]. Reward = [0.]
Curr episode timestep = 901
Human Feedback received at timestep 1803 of -1
Current timestep = 1804. State = [[-0.2568993   0.00922955]]. Action = [[-0.03457995  0.11075959  0.10572639 -0.03979254]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1805. State = [[-0.25246614  0.02645233]]. Action = [[ 0.14236295  0.23467994  0.06503773 -0.9207492 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1806. State = [[-0.24493417  0.06832685]]. Action = [[-0.1900682   0.08809569 -0.09038949  0.8558891 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1807. State = [[-0.24363206  0.12666881]]. Action = [[0.07387188 0.09820968 0.16007444 0.9488723 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1807 is [True, False, False, False, False, True]
State prediction error at timestep 1807 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 1808. State = [[-0.23841427  0.18003827]]. Action = [[-0.16600601  0.21664232 -0.10484752 -0.6253914 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1808 is [True, False, False, False, False, True]
State prediction error at timestep 1808 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 1809. State = [[-0.23741813  0.21593344]]. Action = [[-0.15710986  0.06813833  0.17117989 -0.10953355]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1809 is [True, False, False, False, False, True]
State prediction error at timestep 1809 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1810. State = [[-0.23974243  0.2383629 ]]. Action = [[ 0.18053076  0.07724124 -0.15572728  0.16292286]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1810 is [True, False, False, False, False, True]
State prediction error at timestep 1810 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1811. State = [[-0.23848875  0.2538723 ]]. Action = [[ 0.04691023  0.19824794 -0.14392182 -0.85998666]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1811 is [True, False, False, False, False, True]
State prediction error at timestep 1811 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1812. State = [[-0.23689783  0.26704988]]. Action = [[-0.17672507 -0.13279516 -0.00143082 -0.4508881 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1812 is [True, False, False, False, False, True]
State prediction error at timestep 1812 is tensor(6.7324e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1813. State = [[-0.23745391  0.27463144]]. Action = [[-0.22339481 -0.21805069 -0.16417949  0.86046004]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1813 is [True, False, False, False, False, True]
State prediction error at timestep 1813 is tensor(5.5812e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1814. State = [[-0.24036218  0.27550912]]. Action = [[-0.04490709  0.01249477  0.15190351  0.90406775]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1814 is [True, False, False, False, False, True]
State prediction error at timestep 1814 is tensor(3.6000e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1815. State = [[-0.24339989  0.2767675 ]]. Action = [[-0.2303028  -0.04531004  0.19743389  0.6977105 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1815 is [True, False, False, False, False, True]
State prediction error at timestep 1815 is tensor(2.1380e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1816. State = [[-0.2493216  0.2774619]]. Action = [[0.03972435 0.13154948 0.08175308 0.34840024]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1816 is [True, False, False, False, False, True]
State prediction error at timestep 1816 is tensor(6.1112e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1817. State = [[-0.25391153  0.28064275]]. Action = [[-0.08129421  0.2091015  -0.15681806  0.715343  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1817 is [True, False, False, False, False, True]
State prediction error at timestep 1817 is tensor(4.2891e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1818. State = [[-0.26050577  0.28702262]]. Action = [[ 0.1296249  -0.21853052 -0.09469658 -0.47594333]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1818 is [True, False, False, False, False, True]
State prediction error at timestep 1818 is tensor(3.3379e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1819. State = [[-0.25996727  0.28478137]]. Action = [[-0.03893721 -0.19814585 -0.04100901 -0.27660537]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1819 is [True, False, False, False, False, True]
State prediction error at timestep 1819 is tensor(5.2019e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1819 of -1
Current timestep = 1820. State = [[-0.25826997  0.2798876 ]]. Action = [[-0.23985356 -0.0397352  -0.21674244  0.13696098]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1820 is [True, False, False, False, False, True]
State prediction error at timestep 1820 is tensor(1.8284e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1821. State = [[-0.2630829  0.2771753]]. Action = [[-0.16458432  0.16916907 -0.14808005  0.47512627]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1821 is [True, False, False, False, False, True]
State prediction error at timestep 1821 is tensor(4.3700e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1821 of -1
Current timestep = 1822. State = [[-0.27182135  0.28003293]]. Action = [[-0.09374908  0.17733487  0.17870891  0.65920496]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1822 is [True, False, False, False, False, True]
State prediction error at timestep 1822 is tensor(3.5459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1823. State = [[-0.2811616   0.28689313]]. Action = [[ 0.02308172 -0.1914088   0.03501159 -0.06742913]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1823 is [True, False, False, False, False, True]
State prediction error at timestep 1823 is tensor(4.3397e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1823 of -1
Current timestep = 1824. State = [[-0.28399587  0.28570774]]. Action = [[ 0.00389504 -0.15083714 -0.10859114  0.01855779]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1824 is [True, False, False, False, False, True]
State prediction error at timestep 1824 is tensor(6.0918e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1824 of -1
Current timestep = 1825. State = [[-0.28504598  0.28100705]]. Action = [[-0.13445663  0.18133491  0.12790847 -0.62218213]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1825 is [True, False, False, False, False, True]
State prediction error at timestep 1825 is tensor(1.0939e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1825 of -1
Current timestep = 1826. State = [[-0.2907121   0.28374958]]. Action = [[-0.12775493  0.23692417 -0.15897992 -0.82613415]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1826 is [True, False, False, False, False, True]
State prediction error at timestep 1826 is tensor(2.0371e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1827. State = [[-0.29872018  0.29137984]]. Action = [[ 0.22003141 -0.0143394   0.1985138  -0.40431494]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1827 is [True, False, False, False, False, True]
State prediction error at timestep 1827 is tensor(7.6104e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1827 of -1
Current timestep = 1828. State = [[-0.3005078  0.2936226]]. Action = [[-0.05330688  0.13202888 -0.16632046  0.59891415]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1828 is [True, False, False, False, False, True]
State prediction error at timestep 1828 is tensor(5.0030e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1828 of -1
Current timestep = 1829. State = [[-0.3033269   0.29692096]]. Action = [[ 0.13411954  0.00390485  0.17623293 -0.11414444]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1829 is [True, False, False, False, False, True]
State prediction error at timestep 1829 is tensor(9.1221e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1830. State = [[-0.3021848   0.29884773]]. Action = [[-0.04579489  0.24487168  0.19263864  0.8001417 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1830 is [True, False, False, False, False, True]
State prediction error at timestep 1830 is tensor(4.2030e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1831. State = [[-0.303867    0.30562922]]. Action = [[ 0.12719625 -0.05503039  0.08286166  0.3750615 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1831 is [True, False, False, False, False, True]
State prediction error at timestep 1831 is tensor(5.6409e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1831 of -1
Current timestep = 1832. State = [[-0.3014523   0.30800423]]. Action = [[ 0.24145114 -0.23467678 -0.22200643  0.5681182 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1832 is [True, False, False, False, False, True]
State prediction error at timestep 1832 is tensor(1.3290e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1832 of -1
Current timestep = 1833. State = [[-0.29311818  0.30323824]]. Action = [[ 0.12732852  0.02423805 -0.22161312  0.86339235]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1833 is [True, False, False, False, False, True]
State prediction error at timestep 1833 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1834. State = [[-0.28468126  0.30148903]]. Action = [[ 0.01728323  0.16240597 -0.18823195 -0.7767112 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1834 is [True, False, False, False, False, True]
State prediction error at timestep 1834 is tensor(4.5247e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1834 of -1
Current timestep = 1835. State = [[-0.27883232  0.3043182 ]]. Action = [[ 0.14024654 -0.23484895 -0.17595358 -0.52421486]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1835 is [True, False, False, False, False, True]
State prediction error at timestep 1835 is tensor(7.5583e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1835 of -1
Current timestep = 1836. State = [[-0.27008578  0.3009462 ]]. Action = [[-0.2233482   0.2103517   0.23397452  0.5818914 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1836 is [True, False, False, False, False, True]
State prediction error at timestep 1836 is tensor(7.7694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1836 of -1
Current timestep = 1837. State = [[-0.2664446   0.29831296]]. Action = [[-0.00692248  0.1336466   0.04222509  0.71960425]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1837 is [True, False, False, False, False, True]
State prediction error at timestep 1837 is tensor(1.8042e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1838. State = [[-0.26633692  0.29971084]]. Action = [[-0.17595948  0.16892165 -0.13739462 -0.96092564]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1838 is [True, False, False, False, False, True]
State prediction error at timestep 1838 is tensor(2.6386e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1839. State = [[-0.27015194  0.304348  ]]. Action = [[ 0.01606688 -0.18401337  0.11008143  0.89878416]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1839 is [True, False, False, False, False, True]
State prediction error at timestep 1839 is tensor(4.1778e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1839 of -1
Current timestep = 1840. State = [[-0.26955    0.3036458]]. Action = [[ 0.23098278 -0.00746378 -0.1796826  -0.8038756 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1840 is [True, False, False, False, False, True]
State prediction error at timestep 1840 is tensor(4.6768e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1840 of -1
Current timestep = 1841. State = [[-0.265893    0.30211058]]. Action = [[-0.17108287  0.05011162 -0.14531013 -0.07473642]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1841 is [True, False, False, False, False, True]
State prediction error at timestep 1841 is tensor(3.0435e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1842. State = [[-0.26686105  0.3029262 ]]. Action = [[ 0.00624752 -0.03677601  0.18534979 -0.7724944 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1842 is [True, False, False, False, False, True]
State prediction error at timestep 1842 is tensor(5.6861e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1842 of -1
Current timestep = 1843. State = [[-0.26690757  0.30285436]]. Action = [[-0.07606071  0.22434404 -0.02662238  0.4838338 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1843 is [True, False, False, False, False, True]
State prediction error at timestep 1843 is tensor(1.2230e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1843 of -1
Current timestep = 1844. State = [[-0.26656982  0.30253837]]. Action = [[ 0.20483649 -0.08584371 -0.0901641  -0.03216642]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1844 is [True, False, False, False, False, True]
State prediction error at timestep 1844 is tensor(6.5632e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1845. State = [[-0.26225397  0.30001488]]. Action = [[-0.19488236  0.17452526  0.11775279  0.01613569]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1845 is [True, False, False, False, False, True]
State prediction error at timestep 1845 is tensor(2.9876e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1846. State = [[-0.26417208  0.30339146]]. Action = [[-0.19710472 -0.21140756  0.24641222 -0.23973334]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1846 is [True, False, False, False, False, True]
State prediction error at timestep 1846 is tensor(1.3939e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1847. State = [[-0.26515728  0.30322772]]. Action = [[0.20849317 0.16609049 0.02406907 0.40066433]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1847 is [True, False, False, False, False, True]
State prediction error at timestep 1847 is tensor(3.3636e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1847 of -1
Current timestep = 1848. State = [[-0.2647768  0.3037299]]. Action = [[ 0.09309882 -0.08281335 -0.22445096 -0.83318096]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1848 is [True, False, False, False, False, True]
State prediction error at timestep 1848 is tensor(3.4866e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1849. State = [[-0.2623586  0.3021736]]. Action = [[ 0.0137682  -0.05152941  0.14151677  0.4335692 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1849 is [True, False, False, False, False, True]
State prediction error at timestep 1849 is tensor(2.2449e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1849 of -1
Current timestep = 1850. State = [[-0.26044372  0.30011666]]. Action = [[ 0.18391281  0.21343744 -0.19935971 -0.739881  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1850 is [True, False, False, False, False, True]
State prediction error at timestep 1850 is tensor(1.3719e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1850 of -1
Current timestep = 1851. State = [[-0.25927678  0.29898688]]. Action = [[ 0.12651041  0.00131878 -0.01544864  0.43183994]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1851 is [True, False, False, False, False, True]
State prediction error at timestep 1851 is tensor(1.1263e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1852. State = [[-0.25541908  0.297831  ]]. Action = [[ 0.22417936 -0.23068672 -0.18852277  0.882962  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1852 is [True, False, False, False, False, True]
State prediction error at timestep 1852 is tensor(1.5590e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1853. State = [[-0.24664946  0.29071778]]. Action = [[-0.17447875  0.20043856 -0.22611548  0.01473522]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1853 is [True, False, False, False, False, True]
State prediction error at timestep 1853 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1853 of -1
Current timestep = 1854. State = [[-0.24633966  0.2925587 ]]. Action = [[-0.13438904  0.13092789 -0.11058487 -0.40131557]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1854 is [True, False, False, False, False, True]
State prediction error at timestep 1854 is tensor(8.1173e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1855. State = [[-0.24858423  0.29699066]]. Action = [[ 0.23551142 -0.06350043  0.19951594  0.4489596 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1855 is [True, False, False, False, False, True]
State prediction error at timestep 1855 is tensor(6.9659e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1855 of -1
Current timestep = 1856. State = [[-0.24445048  0.29877123]]. Action = [[-0.24158567  0.24893302 -0.20744543  0.0211153 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1856 is [True, False, False, False, False, True]
State prediction error at timestep 1856 is tensor(2.2124e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1856 of -1
Current timestep = 1857. State = [[-0.24896848  0.3051765 ]]. Action = [[ 0.04085195 -0.2097963   0.10002118 -0.4932217 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1857 is [True, False, False, False, False, True]
State prediction error at timestep 1857 is tensor(4.5241e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1858. State = [[-0.24879156  0.304119  ]]. Action = [[ 0.00505537 -0.22403622  0.1500575   0.76947665]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1858 is [True, False, False, False, False, True]
State prediction error at timestep 1858 is tensor(5.9353e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1859. State = [[-0.24631967  0.29925132]]. Action = [[0.20126772 0.20393157 0.17283857 0.11854231]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1859 is [True, False, False, False, False, True]
State prediction error at timestep 1859 is tensor(1.0788e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1859 of -1
Current timestep = 1860. State = [[-0.2451289   0.29600304]]. Action = [[-0.11712025  0.05772614 -0.20315644  0.49075055]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1860 is [True, False, False, False, False, True]
State prediction error at timestep 1860 is tensor(1.2723e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1860 of -1
Current timestep = 1861. State = [[-0.24633822  0.29646242]]. Action = [[-0.01314087  0.02128571  0.08873552 -0.3733567 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1861 is [True, False, False, False, False, True]
State prediction error at timestep 1861 is tensor(5.0179e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1862. State = [[-0.2473433   0.29730248]]. Action = [[ 0.22868982  0.13820484 -0.03622662 -0.15925956]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1862 is [True, False, False, False, False, True]
State prediction error at timestep 1862 is tensor(2.4418e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1863. State = [[-0.24732444  0.29787892]]. Action = [[-0.14627254 -0.01963994  0.10367015 -0.14930797]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1863 is [True, False, False, False, False, True]
State prediction error at timestep 1863 is tensor(5.3338e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1863 of -1
Current timestep = 1864. State = [[-0.24846798  0.2987736 ]]. Action = [[-0.09259519 -0.20090488  0.07930192  0.76980245]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1864 is [True, False, False, False, False, True]
State prediction error at timestep 1864 is tensor(2.9116e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1865. State = [[-0.24806012  0.29684067]]. Action = [[ 0.18106511 -0.01450187  0.1247279  -0.25224304]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1865 is [True, False, False, False, False, True]
State prediction error at timestep 1865 is tensor(6.1432e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1866. State = [[-0.24622704  0.29384476]]. Action = [[-0.19801322 -0.03350377  0.05938527 -0.06046635]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1866 is [True, False, False, False, False, True]
State prediction error at timestep 1866 is tensor(2.2540e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1866 of -1
Current timestep = 1867. State = [[-0.24782531  0.29340857]]. Action = [[-0.1408697   0.18286493  0.04637274 -0.9436336 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1867 is [True, False, False, False, False, True]
State prediction error at timestep 1867 is tensor(3.3519e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1867 of -1
Current timestep = 1868. State = [[-0.25257677  0.29772356]]. Action = [[-0.22036773 -0.11052801 -0.03352204  0.83856153]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1868 is [True, False, False, False, False, True]
State prediction error at timestep 1868 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1869. State = [[-0.26053795  0.29948556]]. Action = [[-0.21967866 -0.00780088  0.23521638 -0.26552343]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1869 is [True, False, False, False, False, True]
State prediction error at timestep 1869 is tensor(2.3575e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1870. State = [[-0.27016783  0.30046493]]. Action = [[0.21549049 0.08857155 0.00363758 0.8870534 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1870 is [True, False, False, False, False, True]
State prediction error at timestep 1870 is tensor(5.4370e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1870 of -1
Current timestep = 1871. State = [[-0.27301866  0.30135176]]. Action = [[-0.21322127 -0.05490433 -0.22735983  0.48388827]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1871 is [True, False, False, False, False, True]
State prediction error at timestep 1871 is tensor(3.1405e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1871 of -1
Current timestep = 1872. State = [[-0.2792577   0.30208704]]. Action = [[-0.16038635 -0.01283798 -0.21960369 -0.8535321 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1872 is [True, False, False, False, False, True]
State prediction error at timestep 1872 is tensor(3.3050e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1873. State = [[-0.28604198  0.30169746]]. Action = [[ 0.2278423  -0.22639458  0.23745993  0.95935464]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1873 is [True, False, False, False, False, True]
State prediction error at timestep 1873 is tensor(3.3815e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1874. State = [[-0.28417844  0.29487866]]. Action = [[-0.19187242 -0.09161264 -0.09088761 -0.3711778 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1874 is [True, False, False, False, False, True]
State prediction error at timestep 1874 is tensor(5.9452e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1874 of -1
Current timestep = 1875. State = [[-0.28730047  0.2885865 ]]. Action = [[ 0.1477837  -0.07955688 -0.20492941  0.31538272]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1875 is [True, False, False, False, False, True]
State prediction error at timestep 1875 is tensor(6.6409e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1875 of -1
Current timestep = 1876. State = [[-0.28648067  0.2817667 ]]. Action = [[0.0953179  0.1876083  0.11863488 0.749269  ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1876 is [True, False, False, False, False, True]
State prediction error at timestep 1876 is tensor(2.7679e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1877. State = [[-0.2859461  0.2821806]]. Action = [[ 0.13761052  0.1695022   0.22170496 -0.552416  ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1877 is [True, False, False, False, False, True]
State prediction error at timestep 1877 is tensor(2.0980e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1878. State = [[-0.2851142  0.2849449]]. Action = [[-0.07257897  0.2263315  -0.0628112  -0.22845596]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1878 is [True, False, False, False, False, True]
State prediction error at timestep 1878 is tensor(5.7635e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1878 of -1
Current timestep = 1879. State = [[-0.28739882  0.29167455]]. Action = [[ 0.06132597 -0.09115705 -0.18183656  0.3893311 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1879 is [True, False, False, False, False, True]
State prediction error at timestep 1879 is tensor(1.6288e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1879 of -1
Current timestep = 1880. State = [[-0.2868313   0.29259422]]. Action = [[-0.19326913 -0.20325807 -0.09663168 -0.9113324 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1880 is [True, False, False, False, False, True]
State prediction error at timestep 1880 is tensor(1.0846e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1881. State = [[-0.28648788  0.29103038]]. Action = [[ 0.13009483 -0.1365185   0.05005854  0.6694187 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1881 is [True, False, False, False, False, True]
State prediction error at timestep 1881 is tensor(3.0756e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1881 of -1
Current timestep = 1882. State = [[-0.28404772  0.28623432]]. Action = [[-0.19321424 -0.1970462   0.07602939  0.40696597]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1882 is [True, False, False, False, False, True]
State prediction error at timestep 1882 is tensor(6.0214e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1882 of -1
Current timestep = 1883. State = [[-0.28532872  0.28003618]]. Action = [[ 0.08160424  0.2094819   0.20123091 -0.00385076]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1883 is [True, False, False, False, False, True]
State prediction error at timestep 1883 is tensor(2.9745e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1883 of -1
Current timestep = 1884. State = [[-0.28643167  0.28042498]]. Action = [[-0.14051369  0.1393862   0.12113217 -0.31180543]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1884 is [True, False, False, False, False, True]
State prediction error at timestep 1884 is tensor(1.6943e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1885. State = [[-0.2903819   0.28443617]]. Action = [[-0.1947371  -0.2091547   0.14009589 -0.6046358 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1885 is [True, False, False, False, False, True]
State prediction error at timestep 1885 is tensor(1.3426e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1885 of -1
Current timestep = 1886. State = [[-0.29487452  0.28302088]]. Action = [[ 0.18212748 -0.15179928  0.1475043   0.30555987]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1886 is [True, False, False, False, False, True]
State prediction error at timestep 1886 is tensor(1.5570e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1886 of -1
Current timestep = 1887. State = [[-0.29332146  0.2781637 ]]. Action = [[0.10406494 0.128362   0.0740087  0.26273537]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1887 is [True, False, False, False, False, True]
State prediction error at timestep 1887 is tensor(1.5836e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1887 of -1
Current timestep = 1888. State = [[-0.29243782  0.2764991 ]]. Action = [[-0.05965871 -0.09630868  0.07975656 -0.69486314]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1888 is [True, False, False, False, False, True]
State prediction error at timestep 1888 is tensor(4.3819e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1889. State = [[-0.29124942  0.2741986 ]]. Action = [[0.13919586 0.02555093 0.12162021 0.74908113]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1889 is [True, False, False, False, False, True]
State prediction error at timestep 1889 is tensor(2.8796e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1890. State = [[-0.2892841   0.27184966]]. Action = [[ 0.07491195 -0.10104404  0.06337756  0.5819714 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1890 is [True, False, False, False, False, True]
State prediction error at timestep 1890 is tensor(3.2458e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1890 of -1
Current timestep = 1891. State = [[-0.28606528  0.26761174]]. Action = [[ 0.19086355 -0.14874788  0.21503246 -0.13154149]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1891 is [True, False, False, False, False, True]
State prediction error at timestep 1891 is tensor(2.4140e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1891 of -1
Current timestep = 1892. State = [[-0.27935925  0.26021174]]. Action = [[ 0.08575776 -0.11102813  0.02841395 -0.7043876 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1892 is [True, False, False, False, False, True]
State prediction error at timestep 1892 is tensor(4.0823e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1893. State = [[-0.2731961   0.25174406]]. Action = [[-0.14366919 -0.20465939  0.07571378  0.30131948]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1893 is [True, False, False, False, False, True]
State prediction error at timestep 1893 is tensor(4.7171e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1893 of -1
Current timestep = 1894. State = [[-0.26972136  0.2430185 ]]. Action = [[-0.09542575 -0.13690975 -0.1590441  -0.398265  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1894 is [True, False, False, False, False, True]
State prediction error at timestep 1894 is tensor(6.4981e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1894 of -1
Current timestep = 1895. State = [[-0.26986113  0.23461369]]. Action = [[ 0.06522793  0.10837805 -0.22205672 -0.17313635]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1895 is [True, False, False, False, False, True]
State prediction error at timestep 1895 is tensor(8.7522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1895 of 1
Current timestep = 1896. State = [[-0.26978642  0.23210701]]. Action = [[-0.08841169 -0.0680017  -0.20377034 -0.7577473 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1896 is [True, False, False, False, False, True]
State prediction error at timestep 1896 is tensor(8.2337e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1897. State = [[-0.27057323  0.22874816]]. Action = [[-0.05238932 -0.02592519  0.13400263 -0.16043341]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1897 is [True, False, False, False, False, True]
State prediction error at timestep 1897 is tensor(1.7238e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1898. State = [[-0.272213    0.22621366]]. Action = [[-0.12695922 -0.02104661  0.17393231  0.83724594]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1898 is [True, False, False, False, False, True]
State prediction error at timestep 1898 is tensor(1.0973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1898 of 1
Current timestep = 1899. State = [[-0.2792123  0.2223129]]. Action = [[ 0.00983262 -0.19571336 -0.12918335 -0.6834122 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1899 is [True, False, False, False, False, True]
State prediction error at timestep 1899 is tensor(1.8316e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1900. State = [[-0.2809807  0.21628  ]]. Action = [[-0.09445626  0.0281994   0.17718819 -0.5893179 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1900 is [True, False, False, False, False, True]
State prediction error at timestep 1900 is tensor(5.0981e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1901. State = [[-0.28345674  0.21363512]]. Action = [[ 0.09953749 -0.05313702  0.19442832  0.42695093]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1901 is [True, False, False, False, False, True]
State prediction error at timestep 1901 is tensor(5.1957e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1901 of 1
Current timestep = 1902. State = [[-0.28416783  0.20996109]]. Action = [[-0.0631156  -0.21183243 -0.12756068 -0.5707867 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1902 is [True, False, False, False, False, True]
State prediction error at timestep 1902 is tensor(6.7194e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1902 of 1
Current timestep = 1903. State = [[-0.28652725  0.20077828]]. Action = [[-0.1998106   0.01804355 -0.18581395  0.3208449 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1903 is [True, False, False, False, False, True]
State prediction error at timestep 1903 is tensor(2.7096e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1903 of 1
Current timestep = 1904. State = [[-0.29252312  0.19631954]]. Action = [[ 5.9420824e-02 -9.1942072e-02  1.7687678e-04 -3.6822361e-01]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1904 is [True, False, False, False, False, True]
State prediction error at timestep 1904 is tensor(6.3941e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1905. State = [[-0.29452515  0.19156879]]. Action = [[-0.24198823 -0.09503943  0.20588404  0.7692621 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1905 is [True, False, False, False, False, True]
State prediction error at timestep 1905 is tensor(2.1951e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1906. State = [[-0.30026835  0.1870173 ]]. Action = [[-0.12707408  0.19484055  0.22825933  0.65210223]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1906 is [True, False, False, False, False, True]
State prediction error at timestep 1906 is tensor(1.1622e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1906 of 1
Current timestep = 1907. State = [[-0.31037733  0.1879467 ]]. Action = [[-0.11343117 -0.13052393  0.07132614  0.9248531 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1907 is [True, False, False, False, False, True]
State prediction error at timestep 1907 is tensor(6.9507e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1908. State = [[-0.31842557  0.1858161 ]]. Action = [[-0.01467     0.12902844 -0.1891875   0.5283263 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1908 is [True, False, False, False, False, True]
State prediction error at timestep 1908 is tensor(2.9925e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1908 of 0
Current timestep = 1909. State = [[-0.3245516   0.18691535]]. Action = [[ 0.22888082  0.04648486  0.0377757  -0.59428173]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1909 is [True, False, False, False, False, True]
State prediction error at timestep 1909 is tensor(5.5195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1909 of 0
Current timestep = 1910. State = [[-0.32372126  0.18757747]]. Action = [[ 0.02195233 -0.24014804 -0.21481     0.5770345 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1910 is [True, False, False, False, False, True]
State prediction error at timestep 1910 is tensor(8.6126e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1911. State = [[-0.32140166  0.182772  ]]. Action = [[-0.02698317  0.04349238  0.2054489   0.433084  ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1911 is [True, False, False, False, False, True]
State prediction error at timestep 1911 is tensor(1.7243e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1912. State = [[-0.3211001   0.18036625]]. Action = [[ 0.12234932 -0.22136016 -0.18811326 -0.33866072]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1912 is [True, False, False, False, False, True]
State prediction error at timestep 1912 is tensor(6.6407e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1912 of -1
Current timestep = 1913. State = [[-0.31802648  0.17282307]]. Action = [[-0.18339121  0.09862185  0.02595773  0.57149005]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1913 is [True, False, False, False, False, True]
State prediction error at timestep 1913 is tensor(1.9613e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1913 of -1
Current timestep = 1914. State = [[-0.31987354  0.17116705]]. Action = [[-0.1361952   0.05082774  0.16892526  0.90465665]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1914 is [True, False, False, False, False, True]
State prediction error at timestep 1914 is tensor(1.4210e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1914 of -1
Current timestep = 1915. State = [[-0.32523286  0.17197935]]. Action = [[-0.14003894  0.1463986   0.10050833  0.9894209 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1915 is [True, False, False, False, False, True]
State prediction error at timestep 1915 is tensor(3.6315e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1916. State = [[-0.33159524  0.17591485]]. Action = [[ 0.15061802 -0.0954774   0.14826733  0.34415317]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1916 is [True, False, False, False, False, True]
State prediction error at timestep 1916 is tensor(1.4516e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1916 of -1
Current timestep = 1917. State = [[-0.3317001   0.17567976]]. Action = [[-0.10359366 -0.07253978  0.17272073 -0.11193436]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1917 is [True, False, False, False, False, True]
State prediction error at timestep 1917 is tensor(1.0924e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1917 of -1
Current timestep = 1918. State = [[-0.33308014  0.17425074]]. Action = [[ 0.12731773 -0.04163484 -0.21072976  0.53655124]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1918 is [True, False, False, False, False, True]
State prediction error at timestep 1918 is tensor(1.4582e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1918 of -1
Current timestep = 1919. State = [[-0.3324071   0.17225419]]. Action = [[-0.2055821  -0.13676515 -0.00542882  0.32906604]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1919 is [True, False, False, False, False, True]
State prediction error at timestep 1919 is tensor(1.7756e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1920. State = [[-0.33474424  0.16817957]]. Action = [[-0.02989623  0.06766471 -0.1940672  -0.07232255]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1920 is [True, False, False, False, False, True]
State prediction error at timestep 1920 is tensor(1.0385e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1920 of -1
Current timestep = 1921. State = [[-0.33878794  0.16590437]]. Action = [[-0.1872611  -0.18469267 -0.17554092  0.9305191 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1921 is [True, False, False, False, False, True]
State prediction error at timestep 1921 is tensor(2.3913e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1921 of -1
Current timestep = 1922. State = [[-0.34595075  0.15984365]]. Action = [[0.19077763 0.21567306 0.163567   0.4989822 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1922 is [True, False, False, False, False, True]
State prediction error at timestep 1922 is tensor(3.3149e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1923. State = [[-0.34866714  0.16018961]]. Action = [[-0.17972964  0.12094754 -0.20766094 -0.15182805]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1923 is [True, False, False, False, False, True]
State prediction error at timestep 1923 is tensor(9.4550e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1923 of -1
Current timestep = 1924. State = [[-0.3537991   0.16533484]]. Action = [[-0.07930808  0.07694775 -0.10223779  0.923378  ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1924 is [True, False, False, False, False, True]
State prediction error at timestep 1924 is tensor(3.5344e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1924 of -1
Current timestep = 1925. State = [[-0.35943782  0.17055736]]. Action = [[ 0.14084056  0.22854668 -0.11881861  0.9355254 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1925 is [True, False, False, False, False, True]
State prediction error at timestep 1925 is tensor(9.9838e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1925 of -1
Current timestep = 1926. State = [[-0.36262807  0.17680044]]. Action = [[ 0.05129269 -0.07663457 -0.20793144  0.12834835]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1926 is [True, False, False, False, False, True]
State prediction error at timestep 1926 is tensor(1.9978e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1927. State = [[-0.3622044   0.17912287]]. Action = [[-0.05675037 -0.06336051  0.06170174 -0.9091404 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1927 is [True, False, False, False, False, True]
State prediction error at timestep 1927 is tensor(3.8111e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1927 of -1
Current timestep = 1928. State = [[-0.36219427  0.17931524]]. Action = [[0.20642143 0.00576937 0.09522039 0.5430367 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1928 is [True, False, False, False, False, True]
State prediction error at timestep 1928 is tensor(1.3312e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1928 of -1
Current timestep = 1929. State = [[-0.35960796  0.17881477]]. Action = [[ 0.10885882 -0.21278666 -0.2148484  -0.9268527 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1929 is [True, False, False, False, False, True]
State prediction error at timestep 1929 is tensor(1.0805e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1930. State = [[-0.35455486  0.17424127]]. Action = [[-0.08793432 -0.01422627  0.13676295  0.50268364]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1930 is [True, False, False, False, False, True]
State prediction error at timestep 1930 is tensor(1.1692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1930 of -1
Current timestep = 1931. State = [[-0.35188836  0.17172568]]. Action = [[ 0.18296063 -0.05436964 -0.04188323  0.19359708]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1931 is [True, False, False, False, False, True]
State prediction error at timestep 1931 is tensor(4.5765e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1931 of -1
Current timestep = 1932. State = [[-0.3466091   0.16762018]]. Action = [[ 0.22776163 -0.12902136  0.0724138   0.6128583 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1932 is [True, False, False, False, False, True]
State prediction error at timestep 1932 is tensor(7.6465e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1932 of -1
Current timestep = 1933. State = [[-0.33685672  0.15973432]]. Action = [[ 0.12797165 -0.16961695  0.11048239  0.33264923]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1933 is [True, False, False, False, False, True]
State prediction error at timestep 1933 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1934. State = [[-0.32776704  0.1501891 ]]. Action = [[0.20673269 0.14171886 0.1143485  0.12742841]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1934 is [True, False, False, False, False, True]
State prediction error at timestep 1934 is tensor(6.8099e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1934 of -1
Current timestep = 1935. State = [[-0.3187765   0.14786805]]. Action = [[-0.19067976  0.14288437  0.10684845  0.4046942 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1935 is [True, False, False, False, False, True]
State prediction error at timestep 1935 is tensor(4.2694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1935 of -1
Current timestep = 1936. State = [[-0.31752166  0.1494596 ]]. Action = [[ 0.07931226 -0.17289172 -0.21791953 -0.69471306]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1936 is [True, False, False, False, False, True]
State prediction error at timestep 1936 is tensor(9.2520e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1937. State = [[-0.31509662  0.14758141]]. Action = [[ 0.16456097 -0.05521031 -0.18838158 -0.80940354]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1937 is [True, False, False, False, False, True]
State prediction error at timestep 1937 is tensor(8.9401e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1937 of 1
Current timestep = 1938. State = [[-0.30950937  0.14364444]]. Action = [[ 0.03892452 -0.10862048 -0.20196189  0.26139677]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1938 is [True, False, False, False, False, True]
State prediction error at timestep 1938 is tensor(3.8737e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1938 of 1
Current timestep = 1939. State = [[-0.30423275  0.13872534]]. Action = [[-0.00949976  0.03084418  0.15322387  0.8252585 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1939 is [True, False, False, False, False, True]
State prediction error at timestep 1939 is tensor(4.9938e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1940. State = [[-0.3019472   0.13643758]]. Action = [[ 0.11217335  0.15091017  0.15956223 -0.5473619 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1940 is [True, False, False, False, False, True]
State prediction error at timestep 1940 is tensor(1.7284e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1941. State = [[-0.29808703  0.13806212]]. Action = [[ 0.14332771  0.04719698  0.01787162 -0.61445063]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1941 is [True, False, False, False, False, True]
State prediction error at timestep 1941 is tensor(4.2865e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1941 of 1
Current timestep = 1942. State = [[-0.29186445  0.14144787]]. Action = [[-0.0278219   0.21096325 -0.11342576  0.6883137 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1942 is [True, False, False, False, False, True]
State prediction error at timestep 1942 is tensor(9.7745e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1942 of 1
Current timestep = 1943. State = [[-0.28737226  0.14867626]]. Action = [[-0.11527166  0.08670285  0.11866111  0.7171445 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1943 is [True, False, False, False, False, True]
State prediction error at timestep 1943 is tensor(2.5536e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1943 of 1
Current timestep = 1944. State = [[-0.2878441   0.15539597]]. Action = [[ 0.14464     0.16115925 -0.15606688  0.49233449]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1944 is [True, False, False, False, False, True]
State prediction error at timestep 1944 is tensor(3.6790e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1945. State = [[-0.28594697  0.16277197]]. Action = [[ 0.0723542   0.02175063 -0.20803292 -0.15895724]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1945 is [True, False, False, False, False, True]
State prediction error at timestep 1945 is tensor(2.9846e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1945 of 1
Current timestep = 1946. State = [[-0.28167343  0.16921684]]. Action = [[-0.04181106  0.18269193 -0.00796637  0.82683873]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1946 is [True, False, False, False, False, True]
State prediction error at timestep 1946 is tensor(2.5026e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1946 of 1
Current timestep = 1947. State = [[-0.28007016  0.17757922]]. Action = [[-0.20651804 -0.24705547  0.0350436  -0.68337345]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1947 is [True, False, False, False, False, True]
State prediction error at timestep 1947 is tensor(3.1286e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1947 of 1
Current timestep = 1948. State = [[-0.28092986  0.17819564]]. Action = [[-0.06700857 -0.07412104 -0.0745768  -0.00113672]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1948 is [True, False, False, False, False, True]
State prediction error at timestep 1948 is tensor(5.5586e-08, grad_fn=<MseLossBackward0>)
Current timestep = 1949. State = [[-0.28186545  0.17702885]]. Action = [[ 0.00620931  0.12692678 -0.0197676   0.9146087 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1949 is [True, False, False, False, False, True]
State prediction error at timestep 1949 is tensor(1.3510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1949 of 1
Current timestep = 1950. State = [[-0.2837012   0.17887029]]. Action = [[ 0.1012736  -0.23565486  0.12952507 -0.33224165]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1950 is [True, False, False, False, False, True]
State prediction error at timestep 1950 is tensor(1.7559e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1950 of 1
Current timestep = 1951. State = [[-0.28209695  0.17480128]]. Action = [[-0.0602593   0.20220828  0.14270693 -0.5819258 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1951 is [True, False, False, False, False, True]
State prediction error at timestep 1951 is tensor(1.2217e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1952. State = [[-0.28343722  0.17670326]]. Action = [[ 0.04517612  0.08137819  0.11225447 -0.21326572]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1952 is [True, False, False, False, False, True]
State prediction error at timestep 1952 is tensor(1.4192e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1952 of 1
Current timestep = 1953. State = [[-0.28445786  0.17874932]]. Action = [[ 0.24120367 -0.11135691 -0.0262987   0.8088857 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1953 is [True, False, False, False, False, True]
State prediction error at timestep 1953 is tensor(1.0435e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1953 of 1
Current timestep = 1954. State = [[-0.28112045  0.17759733]]. Action = [[-0.24303634  0.01320398 -0.13700509 -0.97116256]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1954 is [True, False, False, False, False, True]
State prediction error at timestep 1954 is tensor(1.0396e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1955. State = [[-0.28184995  0.17832637]]. Action = [[-0.09878737 -0.01619607 -0.04515716  0.34672475]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1955 is [True, False, False, False, False, True]
State prediction error at timestep 1955 is tensor(1.5200e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1955 of 1
Current timestep = 1956. State = [[-0.2831963   0.17924017]]. Action = [[ 0.06717202  0.20927936 -0.04850434  0.23640275]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1956 is [True, False, False, False, False, True]
State prediction error at timestep 1956 is tensor(2.8969e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1956 of 1
Current timestep = 1957. State = [[-0.28563845  0.18287562]]. Action = [[ 0.13552302 -0.01522678  0.05248851 -0.6409775 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1957 is [True, False, False, False, False, True]
State prediction error at timestep 1957 is tensor(1.8734e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1958. State = [[-0.28452042  0.18447241]]. Action = [[ 0.12309098  0.13269663  0.24503314 -0.40654218]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1958 is [True, False, False, False, False, True]
State prediction error at timestep 1958 is tensor(4.7480e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1958 of 1
Current timestep = 1959. State = [[-0.28138193  0.18806806]]. Action = [[0.16432825 0.07080755 0.10829929 0.83631873]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1959 is [True, False, False, False, False, True]
State prediction error at timestep 1959 is tensor(2.1663e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1959 of 1
Current timestep = 1960. State = [[-0.2763308   0.19187486]]. Action = [[-0.14684954  0.02979249 -0.18639292  0.01672781]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1960 is [True, False, False, False, False, True]
State prediction error at timestep 1960 is tensor(5.6497e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1961. State = [[-0.2754965   0.19641107]]. Action = [[-0.22709158  0.1249398  -0.23458438  0.2815051 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1961 is [True, False, False, False, False, True]
State prediction error at timestep 1961 is tensor(3.2416e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1961 of 1
Current timestep = 1962. State = [[-0.28036457  0.20291154]]. Action = [[ 0.02375978 -0.24852325 -0.1184977  -0.01146036]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1962 is [True, False, False, False, False, True]
State prediction error at timestep 1962 is tensor(1.9948e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1962 of 1
Current timestep = 1963. State = [[-0.2801602  0.2010528]]. Action = [[ 0.16867948 -0.15039489  0.12309951 -0.10980636]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1963 is [True, False, False, False, False, True]
State prediction error at timestep 1963 is tensor(2.0863e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1964. State = [[-0.277101    0.19653505]]. Action = [[ 0.21431077  0.19650602 -0.2055168  -0.9164356 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1964 is [True, False, False, False, False, True]
State prediction error at timestep 1964 is tensor(2.0996e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1964 of 1
Current timestep = 1965. State = [[-0.27282384  0.19817355]]. Action = [[-0.08534265  0.06011227  0.1745019   0.76096594]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1965 is [True, False, False, False, False, True]
State prediction error at timestep 1965 is tensor(2.2506e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1965 of 1
Current timestep = 1966. State = [[-0.27021876  0.20076866]]. Action = [[ 0.10767385 -0.13639438  0.08372393  0.31233263]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1966 is [True, False, False, False, False, True]
State prediction error at timestep 1966 is tensor(8.7429e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1967. State = [[-0.26638773  0.19967271]]. Action = [[-0.1093403   0.17114726  0.0203808  -0.28155875]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1967 is [True, False, False, False, False, True]
State prediction error at timestep 1967 is tensor(6.2333e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1967 of 1
Current timestep = 1968. State = [[-0.2673771   0.20263949]]. Action = [[-0.17112558  0.20145416  0.06906721  0.6803006 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1968 is [True, False, False, False, False, True]
State prediction error at timestep 1968 is tensor(1.1334e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1968 of 1
Current timestep = 1969. State = [[-0.2723026   0.20918971]]. Action = [[ 0.11778706 -0.04645401 -0.0692458  -0.23556614]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1969 is [True, False, False, False, False, True]
State prediction error at timestep 1969 is tensor(4.6681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1970. State = [[-0.2720656   0.21146835]]. Action = [[-0.21215388 -0.12613119 -0.03958218 -0.73645484]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1970 is [True, False, False, False, False, True]
State prediction error at timestep 1970 is tensor(1.3836e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1971. State = [[-0.2730143   0.21190038]]. Action = [[ 0.14485425 -0.20632118 -0.02915505  0.4959556 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1971 is [True, False, False, False, False, True]
State prediction error at timestep 1971 is tensor(8.5712e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1971 of 1
Current timestep = 1972. State = [[-0.27061912  0.20687036]]. Action = [[ 0.01277363  0.04980561 -0.2262099  -0.5965204 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1972 is [True, False, False, False, False, True]
State prediction error at timestep 1972 is tensor(1.0321e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1973. State = [[-0.27016488  0.20562355]]. Action = [[-0.20342422 -0.07466304 -0.10734603  0.10120142]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1973 is [True, False, False, False, False, True]
State prediction error at timestep 1973 is tensor(2.2994e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1973 of 1
Current timestep = 1974. State = [[-0.2718353   0.20408314]]. Action = [[-0.04531346  0.17595392 -0.24762808  0.98324096]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1974 is [True, False, False, False, False, True]
State prediction error at timestep 1974 is tensor(9.7145e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1974 of 1
Current timestep = 1975. State = [[-0.2744685  0.2065758]]. Action = [[-0.08583638 -0.18990767  0.07786289  0.65593266]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1975 is [True, False, False, False, False, True]
State prediction error at timestep 1975 is tensor(8.8697e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1975 of 1
Current timestep = 1976. State = [[-0.276038    0.20294665]]. Action = [[ 0.08352822 -0.2209793   0.05018991 -0.00283593]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1976 is [True, False, False, False, False, True]
State prediction error at timestep 1976 is tensor(3.3113e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1977. State = [[-0.2758645   0.19558024]]. Action = [[ 0.18695113  0.16423619 -0.15476577 -0.32585883]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1977 is [True, False, False, False, False, True]
State prediction error at timestep 1977 is tensor(3.6454e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1977 of 1
Current timestep = 1978. State = [[-0.27523676  0.19440362]]. Action = [[-0.12793612  0.11944622 -0.24293976 -0.6867488 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1978 is [True, False, False, False, False, True]
State prediction error at timestep 1978 is tensor(1.9743e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1978 of 1
Current timestep = 1979. State = [[-0.27642643  0.19611864]]. Action = [[ 0.08594996 -0.11067522  0.0612334   0.22313404]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1979 is [True, False, False, False, False, True]
State prediction error at timestep 1979 is tensor(8.6828e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1980. State = [[-0.27580944  0.1956146 ]]. Action = [[ 0.05445457  0.23726138 -0.20970652  0.8844187 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1980 is [True, False, False, False, False, True]
State prediction error at timestep 1980 is tensor(1.1876e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1980 of 1
Current timestep = 1981. State = [[-0.27630162  0.1984461 ]]. Action = [[-0.21125746 -0.17272556  0.12867677  0.1522001 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1981 is [True, False, False, False, False, True]
State prediction error at timestep 1981 is tensor(2.7516e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1982. State = [[-0.2772748   0.19820179]]. Action = [[-0.12032457 -0.2131093   0.21378767  0.6660048 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1982 is [True, False, False, False, False, True]
State prediction error at timestep 1982 is tensor(1.3694e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1983. State = [[-0.28082168  0.19285746]]. Action = [[-0.04327747 -0.04801875  0.20995128  0.8941059 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1983 is [True, False, False, False, False, True]
State prediction error at timestep 1983 is tensor(4.0921e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1983 of 1
Current timestep = 1984. State = [[-0.28419653  0.1876953 ]]. Action = [[-0.00760108 -0.14934973  0.12951055  0.3074081 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1984 is [True, False, False, False, False, True]
State prediction error at timestep 1984 is tensor(1.7387e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1984 of 1
Current timestep = 1985. State = [[-0.2871846   0.18017258]]. Action = [[0.1035029  0.22726321 0.23952073 0.18330467]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1985 is [True, False, False, False, False, True]
State prediction error at timestep 1985 is tensor(2.1438e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1986. State = [[-0.28749412  0.18032241]]. Action = [[-0.01716542 -0.21912779 -0.2153881  -0.2909634 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1986 is [True, False, False, False, False, True]
State prediction error at timestep 1986 is tensor(1.1026e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1986 of -1
Current timestep = 1987. State = [[-0.28742355  0.17739171]]. Action = [[-0.1902032   0.1754564   0.14111039  0.13730907]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1987 is [True, False, False, False, False, True]
State prediction error at timestep 1987 is tensor(4.4713e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1988. State = [[-0.29138005  0.18012385]]. Action = [[ 0.09384719  0.22670239  0.24599555 -0.00369924]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1988 is [True, False, False, False, False, True]
State prediction error at timestep 1988 is tensor(1.7939e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1988 of -1
Current timestep = 1989. State = [[-0.2945005   0.18509005]]. Action = [[-0.12107155 -0.03587177  0.17780083  0.63370967]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1989 is [True, False, False, False, False, True]
State prediction error at timestep 1989 is tensor(1.6819e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1989 of -1
Current timestep = 1990. State = [[-0.29802766  0.18947344]]. Action = [[-0.02385309  0.16316715 -0.06055179  0.8061893 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1990 is [True, False, False, False, False, True]
State prediction error at timestep 1990 is tensor(1.8259e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1991. State = [[-0.30250868  0.19507252]]. Action = [[ 0.21972805  0.18647307 -0.07439241  0.7947916 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1991 is [True, False, False, False, False, True]
State prediction error at timestep 1991 is tensor(1.0308e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1991 of 0
Current timestep = 1992. State = [[-0.30222696  0.20193173]]. Action = [[-0.06243026  0.13692325  0.07004172 -0.02785987]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1992 is [True, False, False, False, False, True]
State prediction error at timestep 1992 is tensor(2.9041e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1993. State = [[-0.30356827  0.20957999]]. Action = [[0.1205349  0.04357636 0.09763917 0.58135176]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1993 is [True, False, False, False, False, True]
State prediction error at timestep 1993 is tensor(2.3244e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1994. State = [[-0.30126593  0.21549278]]. Action = [[0.15102655 0.18231148 0.01151264 0.78344107]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1994 is [True, False, False, False, False, True]
State prediction error at timestep 1994 is tensor(1.8781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1994 of -1
Current timestep = 1995. State = [[-0.29602927  0.2232568 ]]. Action = [[ 0.03889459 -0.10477006 -0.21811011  0.61876416]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1995 is [True, False, False, False, False, True]
State prediction error at timestep 1995 is tensor(5.9335e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1995 of -1
Current timestep = 1996. State = [[-0.2902624   0.22751072]]. Action = [[-0.16000047  0.04997575  0.19348007  0.90857506]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1996 is [True, False, False, False, False, True]
State prediction error at timestep 1996 is tensor(3.1056e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1996 of -1
Current timestep = 1997. State = [[-0.29121858  0.23059726]]. Action = [[ 0.22196716  0.132855   -0.13923521  0.87363434]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1997 is [True, False, False, False, False, True]
State prediction error at timestep 1997 is tensor(2.0103e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1997 of -1
Current timestep = 1998. State = [[-0.2885129   0.23428154]]. Action = [[-0.1202025   0.05060181  0.12675178  0.7040639 ]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 1999. State = [[-0.2883874   0.23923665]]. Action = [[-0.00492424  0.1610725   0.19384855 -0.52915263]]. Reward = [0.]
Curr episode timestep = 195
Human Feedback received at timestep 1999 of -1
Current timestep = 2000. State = [[-0.2887932  0.2470099]]. Action = [[-0.04161106  0.17966926 -0.20984302  0.97587395]]. Reward = [0.]
Curr episode timestep = 196
Human Feedback received at timestep 2000 of -1
Current timestep = 2001. State = [[-0.29116514  0.26159775]]. Action = [[-0.22412992  0.04646274  0.19964305 -0.5544347 ]]. Reward = [0.]
Curr episode timestep = 197
Human Feedback received at timestep 2001 of -1
Current timestep = 2002. State = [[-0.2967177   0.26803902]]. Action = [[-0.20776318  0.11920464  0.20025724 -0.11473447]]. Reward = [0.]
Curr episode timestep = 198
Human Feedback received at timestep 2002 of -1
Current timestep = 2003. State = [[-0.30448353  0.2756648 ]]. Action = [[ 0.12925261 -0.14606428 -0.11120163  0.1898799 ]]. Reward = [0.]
Curr episode timestep = 199
Current timestep = 2004. State = [[-0.30583677  0.27727208]]. Action = [[-0.24451734  0.01911283 -0.07263345 -0.9273433 ]]. Reward = [0.]
Curr episode timestep = 200
Current timestep = 2005. State = [[-0.31150296  0.28079057]]. Action = [[-0.20535715  0.1023787  -0.23579152  0.51987004]]. Reward = [0.]
Curr episode timestep = 201
Current timestep = 2006. State = [[-0.31895077  0.28553304]]. Action = [[-0.12662663 -0.13935335 -0.19991116 -0.91748756]]. Reward = [0.]
Curr episode timestep = 202
Current timestep = 2007. State = [[-0.3261712  0.2866156]]. Action = [[ 0.10376912  0.19677919 -0.22016872  0.39929688]]. Reward = [0.]
Curr episode timestep = 203
Current timestep = 2008. State = [[-0.33081803  0.29052854]]. Action = [[-2.3472759e-01  1.3358474e-01  4.1612983e-04  7.3517585e-01]]. Reward = [0.]
Curr episode timestep = 204
Current timestep = 2009. State = [[-0.33872867  0.29774076]]. Action = [[ 0.19368005  0.1443635   0.068295   -0.55384034]]. Reward = [0.]
Curr episode timestep = 205
Current timestep = 2010. State = [[-0.34221455  0.30245808]]. Action = [[ 0.1147995  -0.07229561  0.19704786  0.87462175]]. Reward = [0.]
Curr episode timestep = 206
Current timestep = 2011. State = [[-0.34160402  0.3038546 ]]. Action = [[-0.23420084  0.22938555 -0.23652382  0.762691  ]]. Reward = [0.]
Curr episode timestep = 207
Current timestep = 2012. State = [[-0.34106487  0.30400115]]. Action = [[-0.18525995 -0.19905302  0.06913415  0.2901641 ]]. Reward = [0.]
Curr episode timestep = 208
Current timestep = 2013. State = [[-0.34085295  0.30259687]]. Action = [[ 0.22906733 -0.07749836 -0.12784323  0.35533988]]. Reward = [0.]
Curr episode timestep = 209
Current timestep = 2014. State = [[-0.3376192  0.2992364]]. Action = [[-0.10617709  0.24345079  0.1104126   0.7146871 ]]. Reward = [0.]
Curr episode timestep = 210
Current timestep = 2015. State = [[-0.33583957  0.2968497 ]]. Action = [[0.1514419  0.13085929 0.10998869 0.44552743]]. Reward = [0.]
Curr episode timestep = 211
Current timestep = 2016. State = [[-0.334455    0.29680526]]. Action = [[-0.21350735  0.12401414 -0.20787562  0.2147708 ]]. Reward = [0.]
Curr episode timestep = 212
Current timestep = 2017. State = [[-0.33660355  0.299208  ]]. Action = [[ 0.22102681 -0.23939651 -0.16865084 -0.7582106 ]]. Reward = [0.]
Curr episode timestep = 213
Current timestep = 2018. State = [[-0.3320877  0.2956381]]. Action = [[ 0.1777139   0.04731601 -0.16972274  0.3411975 ]]. Reward = [0.]
Curr episode timestep = 214
Current timestep = 2019. State = [[-0.32808843  0.29348084]]. Action = [[-0.18305728 -0.19790651 -0.12337925  0.73244333]]. Reward = [0.]
Curr episode timestep = 215
Current timestep = 2020. State = [[-0.32569316  0.29027152]]. Action = [[ 0.24200928 -0.0724391   0.12976569 -0.53679127]]. Reward = [0.]
Curr episode timestep = 216
Current timestep = 2021. State = [[-0.3198967   0.28424144]]. Action = [[0.2191602  0.13791591 0.21688628 0.8259101 ]]. Reward = [0.]
Curr episode timestep = 217
Current timestep = 2022. State = [[-0.3135163   0.28275743]]. Action = [[-0.15758885  0.0650467   0.0714041   0.8188536 ]]. Reward = [0.]
Curr episode timestep = 218
Current timestep = 2023. State = [[-0.31175852  0.28364083]]. Action = [[ 0.12252516 -0.2118408   0.04064795  0.4453026 ]]. Reward = [0.]
Curr episode timestep = 219
Current timestep = 2024. State = [[-0.30749303  0.2801828 ]]. Action = [[-0.16623661  0.16147876 -0.08569238  0.5218836 ]]. Reward = [0.]
Curr episode timestep = 220
Current timestep = 2025. State = [[-0.30824888  0.2809503 ]]. Action = [[-8.3134651e-02 -1.4931227e-01  3.4907460e-04  7.2894311e-01]]. Reward = [0.]
Curr episode timestep = 221
Current timestep = 2026. State = [[-0.30827886  0.27959684]]. Action = [[ 0.041857   -0.21758564 -0.17315121  0.07923603]]. Reward = [0.]
Curr episode timestep = 222
Current timestep = 2027. State = [[-0.30699313  0.27390185]]. Action = [[-0.23820019  0.07154083 -0.19791393  0.18277788]]. Reward = [0.]
Curr episode timestep = 223
Current timestep = 2028. State = [[-0.3103466   0.27299833]]. Action = [[-0.09275994  0.15641749  0.03938931 -0.31689966]]. Reward = [0.]
Curr episode timestep = 224
Current timestep = 2029. State = [[-0.31448203  0.27537504]]. Action = [[ 0.18124735 -0.19083492 -0.22870843 -0.14125967]]. Reward = [0.]
Curr episode timestep = 225
Current timestep = 2030. State = [[-0.3131354   0.27162802]]. Action = [[ 0.1657272   0.01118642  0.01593018 -0.50073594]]. Reward = [0.]
Curr episode timestep = 226
Current timestep = 2031. State = [[-0.31085134  0.26894182]]. Action = [[-0.11379322 -0.06324503  0.13458735  0.13349032]]. Reward = [0.]
Curr episode timestep = 227
Current timestep = 2032. State = [[-0.3101455   0.26752055]]. Action = [[ 0.09509906  0.01701498 -0.01832324 -0.7917989 ]]. Reward = [0.]
Curr episode timestep = 228
Current timestep = 2033. State = [[-0.309331    0.26604834]]. Action = [[ 0.02911103 -0.08395216 -0.20085336  0.07196534]]. Reward = [0.]
Curr episode timestep = 229
Current timestep = 2034. State = [[-0.30717456  0.26319575]]. Action = [[ 0.07023519  0.05376866  0.14430681 -0.5573948 ]]. Reward = [0.]
Curr episode timestep = 230
Current timestep = 2035. State = [[-0.30618286  0.26205018]]. Action = [[-0.03804672  0.17369539 -0.12640588 -0.8515714 ]]. Reward = [0.]
Curr episode timestep = 231
Current timestep = 2036. State = [[-0.30670747  0.26348826]]. Action = [[0.23045236 0.22235376 0.00184378 0.8392854 ]]. Reward = [0.]
Curr episode timestep = 232
Current timestep = 2037. State = [[-0.30347976  0.2675736 ]]. Action = [[ 0.20357436 -0.07300821  0.07587168  0.50520444]]. Reward = [0.]
Curr episode timestep = 233
Current timestep = 2038. State = [[-0.29595163  0.27040413]]. Action = [[0.16777727 0.10934031 0.17962626 0.6041248 ]]. Reward = [0.]
Curr episode timestep = 234
Current timestep = 2039. State = [[-0.2879111   0.27500752]]. Action = [[ 0.08439174  0.22620064  0.21432704 -0.08717799]]. Reward = [0.]
Curr episode timestep = 235
Current timestep = 2040. State = [[-0.2804678   0.28314075]]. Action = [[-0.03709655 -0.0729447  -0.06459993  0.8461875 ]]. Reward = [0.]
Curr episode timestep = 236
Current timestep = 2041. State = [[-0.27559096  0.28839794]]. Action = [[-0.20289524  0.12770486 -0.07550356  0.87715256]]. Reward = [0.]
Curr episode timestep = 237
Current timestep = 2042. State = [[-0.27564853  0.2941603 ]]. Action = [[ 0.10261351 -0.05658139 -0.22481908  0.4100802 ]]. Reward = [0.]
Curr episode timestep = 238
Current timestep = 2043. State = [[-0.2735254  0.2969869]]. Action = [[0.00509295 0.00369295 0.2278027  0.55002165]]. Reward = [0.]
Curr episode timestep = 239
Current timestep = 2044. State = [[-0.27216893  0.29808295]]. Action = [[-0.20556046  0.00512797 -0.15200825 -0.5366204 ]]. Reward = [0.]
Curr episode timestep = 240
Current timestep = 2045. State = [[-0.27448288  0.3001061 ]]. Action = [[ 0.06357414  0.2101796  -0.00386405  0.76895404]]. Reward = [0.]
Curr episode timestep = 241
Current timestep = 2046. State = [[-0.2769334   0.30460414]]. Action = [[0.22054285 0.19548291 0.06507128 0.9019575 ]]. Reward = [0.]
Curr episode timestep = 242
Current timestep = 2047. State = [[-0.27473173  0.31099975]]. Action = [[ 0.13888353  0.12810811  0.2438057  -0.7294391 ]]. Reward = [0.]
Curr episode timestep = 243
Current timestep = 2048. State = [[-0.26873738  0.31910062]]. Action = [[ 0.15843028 -0.1320933   0.07644597 -0.23883581]]. Reward = [0.]
Curr episode timestep = 244
Current timestep = 2049. State = [[-0.2610626  0.3224906]]. Action = [[0.02831233 0.06007832 0.24605614 0.54705465]]. Reward = [0.]
Curr episode timestep = 245
Current timestep = 2050. State = [[-0.25532416  0.32613108]]. Action = [[ 0.1826793   0.08977693 -0.15179943  0.8355043 ]]. Reward = [0.]
Curr episode timestep = 246
Current timestep = 2051. State = [[-0.25187615  0.32788303]]. Action = [[ 0.0291625  -0.16751976 -0.04881158  0.5140636 ]]. Reward = [0.]
Curr episode timestep = 247
Current timestep = 2052. State = [[-0.24778505  0.3250067 ]]. Action = [[ 0.00996312 -0.16083352 -0.20903759 -0.22307909]]. Reward = [0.]
Curr episode timestep = 248
Current timestep = 2053. State = [[-0.2441123   0.31998584]]. Action = [[ 0.05212331 -0.08289415  0.11013803 -0.39088404]]. Reward = [0.]
Curr episode timestep = 249
Current timestep = 2054. State = [[-0.24168582  0.31686214]]. Action = [[-0.11258377 -0.18558386 -0.07836738 -0.8242653 ]]. Reward = [0.]
Curr episode timestep = 250
Current timestep = 2055. State = [[-0.23920165  0.31231004]]. Action = [[-0.13447234  0.09445539  0.09573704  0.22417045]]. Reward = [0.]
Curr episode timestep = 251
Current timestep = 2056. State = [[-0.23803689  0.30924255]]. Action = [[-0.18770134  0.23584116 -0.19051546  0.03477919]]. Reward = [0.]
Curr episode timestep = 252
Current timestep = 2057. State = [[-0.23732977  0.30685154]]. Action = [[-0.14230506  0.23115513 -0.2173896   0.6460962 ]]. Reward = [0.]
Curr episode timestep = 253
Current timestep = 2058. State = [[-0.23719679  0.30547827]]. Action = [[-0.21954562 -0.13346069  0.01790881 -0.38110256]]. Reward = [0.]
Curr episode timestep = 254
Current timestep = 2059. State = [[-0.23883343  0.30283487]]. Action = [[ 0.05400753 -0.04374658 -0.22521698 -0.894561  ]]. Reward = [0.]
Curr episode timestep = 255
Current timestep = 2060. State = [[-0.23963857  0.29958838]]. Action = [[ 0.10733041  0.19411445 -0.17249453  0.29826844]]. Reward = [0.]
Curr episode timestep = 256
Current timestep = 2061. State = [[-0.24013288  0.3001313 ]]. Action = [[ 0.22122863 -0.03279439 -0.06153096  0.7241131 ]]. Reward = [0.]
Curr episode timestep = 257
Current timestep = 2062. State = [[-0.23824981  0.29826677]]. Action = [[-0.20239504  0.05794734  0.11431593 -0.48135638]]. Reward = [0.]
Curr episode timestep = 258
Current timestep = 2063. State = [[-0.23949231  0.2997492 ]]. Action = [[ 0.00906384 -0.08669469 -0.13789394 -0.41509044]]. Reward = [0.]
Curr episode timestep = 259
Current timestep = 2064. State = [[-0.23913601  0.29918215]]. Action = [[ 0.15636289 -0.12621267  0.1630733   0.35791516]]. Reward = [0.]
Curr episode timestep = 260
Current timestep = 2065. State = [[-0.23557977  0.29517543]]. Action = [[-0.03001645 -0.17772876  0.08047283  0.8646947 ]]. Reward = [0.]
Curr episode timestep = 261
Current timestep = 2066. State = [[-0.23219474  0.2890871 ]]. Action = [[ 0.17479566 -0.06711859  0.03919464 -0.80494267]]. Reward = [0.]
Curr episode timestep = 262
Current timestep = 2067. State = [[-0.22736537  0.28238583]]. Action = [[ 0.20411879 -0.12052713  0.12166959  0.06219351]]. Reward = [0.]
Curr episode timestep = 263
Current timestep = 2068. State = [[-0.21974954  0.2728424 ]]. Action = [[-0.02078593 -0.2019041  -0.06113993 -0.65616864]]. Reward = [0.]
Curr episode timestep = 264
Current timestep = 2069. State = [[-0.21419166  0.2642906 ]]. Action = [[-0.10993469  0.15499747  0.20016444 -0.37751484]]. Reward = [0.]
Curr episode timestep = 265
Current timestep = 2070. State = [[-0.2134103   0.26286557]]. Action = [[-0.1294116   0.12767139 -0.15396617 -0.04264498]]. Reward = [0.]
Curr episode timestep = 266
Current timestep = 2071. State = [[-0.21567938  0.26458722]]. Action = [[-0.24247889 -0.12610722 -0.00309707  0.3578744 ]]. Reward = [0.]
Curr episode timestep = 267
Current timestep = 2072. State = [[-0.22019559  0.26348868]]. Action = [[-0.13438372 -0.17190804 -0.01778874 -0.79167616]]. Reward = [0.]
Curr episode timestep = 268
Current timestep = 2073. State = [[-0.22635597  0.25869435]]. Action = [[ 0.10060763  0.22038344 -0.21230505 -0.79320496]]. Reward = [0.]
Curr episode timestep = 269
Current timestep = 2074. State = [[-0.2299954   0.26002365]]. Action = [[-0.18479428 -0.01837483  0.11196119 -0.43088412]]. Reward = [0.]
Curr episode timestep = 270
Current timestep = 2075. State = [[-0.23473307  0.26221535]]. Action = [[ 0.08192572  0.1477536  -0.19207025 -0.628961  ]]. Reward = [0.]
Curr episode timestep = 271
Current timestep = 2076. State = [[-0.23751667  0.26587373]]. Action = [[ 0.00153726  0.08838975 -0.08171666 -0.18527615]]. Reward = [0.]
Curr episode timestep = 272
Current timestep = 2077. State = [[-0.24073716  0.26933485]]. Action = [[-0.1908066  -0.24719307  0.16321924  0.27935743]]. Reward = [0.]
Curr episode timestep = 273
Current timestep = 2078. State = [[-0.24367061  0.2682794 ]]. Action = [[-0.17468676  0.07795492 -0.13950281  0.9874761 ]]. Reward = [0.]
Curr episode timestep = 274
Current timestep = 2079. State = [[-0.2504504  0.2687043]]. Action = [[-0.03919373 -0.19343968 -0.0550231   0.01790535]]. Reward = [0.]
Curr episode timestep = 275
Current timestep = 2080. State = [[-0.25553054  0.26424137]]. Action = [[ 0.16639441  0.08851868 -0.03544027 -0.3196565 ]]. Reward = [0.]
Curr episode timestep = 276
Current timestep = 2081. State = [[-0.2569735   0.26243663]]. Action = [[ 0.14703137  0.11412215  0.20776504 -0.13146174]]. Reward = [0.]
Curr episode timestep = 277
Current timestep = 2082. State = [[-0.25672     0.26244143]]. Action = [[ 0.08276501  0.03292263 -0.09623203  0.09665966]]. Reward = [0.]
Curr episode timestep = 278
Current timestep = 2083. State = [[-0.25610235  0.26225048]]. Action = [[ 0.02419752 -0.20102164  0.04698172 -0.3788827 ]]. Reward = [0.]
Curr episode timestep = 279
Current timestep = 2084. State = [[-0.2530801   0.25877443]]. Action = [[ 0.04153785 -0.24070325  0.08322144 -0.71458966]]. Reward = [0.]
Curr episode timestep = 280
Current timestep = 2085. State = [[-0.2487824   0.25067848]]. Action = [[-0.07447633 -0.10226661 -0.05049352  0.13985467]]. Reward = [0.]
Curr episode timestep = 281
Current timestep = 2086. State = [[-0.24801497  0.24418584]]. Action = [[-0.05635427  0.04388535  0.14510363  0.4925604 ]]. Reward = [0.]
Curr episode timestep = 282
Current timestep = 2087. State = [[-0.24799633  0.24081336]]. Action = [[ 0.19201523 -0.11088873 -0.18844467  0.3758974 ]]. Reward = [0.]
Curr episode timestep = 283
Current timestep = 2088. State = [[-0.24451864  0.23579644]]. Action = [[0.19006732 0.04532099 0.09084523 0.74697614]]. Reward = [0.]
Curr episode timestep = 284
Current timestep = 2089. State = [[-0.239474    0.23136882]]. Action = [[-0.00602295 -0.11794646 -0.11546901 -0.6604003 ]]. Reward = [0.]
Curr episode timestep = 285
Current timestep = 2090. State = [[-0.23475216  0.22632909]]. Action = [[-0.24614805 -0.16574593 -0.14622098  0.09493589]]. Reward = [0.]
Curr episode timestep = 286
Current timestep = 2091. State = [[-0.23474659  0.22048512]]. Action = [[ 0.21595299  0.13168526 -0.00391755  0.05808699]]. Reward = [0.]
Curr episode timestep = 287
Current timestep = 2092. State = [[-0.2336195   0.21923587]]. Action = [[-0.1226632   0.10000345 -0.10992023  0.22260773]]. Reward = [0.]
Curr episode timestep = 288
Current timestep = 2093. State = [[-0.23471352  0.22112451]]. Action = [[-0.19285695  0.19795483 -0.10955501  0.91224086]]. Reward = [0.]
Curr episode timestep = 289
Current timestep = 2094. State = [[-0.23859432  0.22616266]]. Action = [[0.16202894 0.01536167 0.14329922 0.01043034]]. Reward = [0.]
Curr episode timestep = 290
Current timestep = 2095. State = [[-0.23983853  0.22827113]]. Action = [[ 0.02512062 -0.05241378  0.19380671  0.08852351]]. Reward = [0.]
Curr episode timestep = 291
Current timestep = 2096. State = [[-0.23931007  0.22771528]]. Action = [[ 0.22690359 -0.22115605 -0.07739916 -0.42263925]]. Reward = [0.]
Curr episode timestep = 292
Current timestep = 2097. State = [[-0.23480853  0.22356157]]. Action = [[-0.05763148  0.17917356  0.24303555  0.32042813]]. Reward = [0.]
Curr episode timestep = 293
Current timestep = 2098. State = [[-0.23391105  0.2246206 ]]. Action = [[-0.22725895  0.18149179 -0.22740044 -0.4058246 ]]. Reward = [0.]
Curr episode timestep = 294
Current timestep = 2099. State = [[-0.23719718  0.22989225]]. Action = [[-0.05458987 -0.02759357  0.00883591  0.8041953 ]]. Reward = [0.]
Curr episode timestep = 295
Current timestep = 2100. State = [[-0.23973732  0.23311289]]. Action = [[-0.0893434  -0.15947317 -0.19725916  0.63434327]]. Reward = [0.]
Curr episode timestep = 296
Current timestep = 2101. State = [[-0.24082495  0.23246248]]. Action = [[ 0.17455739 -0.17819698  0.15956736  0.7626512 ]]. Reward = [0.]
Curr episode timestep = 297
Current timestep = 2102. State = [[-0.23829363  0.22802056]]. Action = [[-0.06534228 -0.01222676 -0.11417985 -0.9456782 ]]. Reward = [0.]
Curr episode timestep = 298
Current timestep = 2103. State = [[-0.23799835  0.22616887]]. Action = [[-0.16251673  0.20243463  0.05716488 -0.38961518]]. Reward = [0.]
Curr episode timestep = 299
Current timestep = 2104. State = [[-0.24117236  0.22889395]]. Action = [[-0.19562179 -0.20627323  0.22121203  0.9241862 ]]. Reward = [0.]
Curr episode timestep = 300
Current timestep = 2105. State = [[-0.24614538  0.22560328]]. Action = [[ 0.04645842 -0.16885349 -0.13191739 -0.8336026 ]]. Reward = [0.]
Curr episode timestep = 301
Current timestep = 2106. State = [[-0.25026712  0.21808435]]. Action = [[-0.183521   -0.12328404  0.06929812  0.6503407 ]]. Reward = [0.]
Curr episode timestep = 302
Current timestep = 2107. State = [[-0.25648656  0.2112194 ]]. Action = [[-0.16570006  0.19754308  0.01003587  0.3367647 ]]. Reward = [0.]
Curr episode timestep = 303
Current timestep = 2108. State = [[-0.2644543   0.21171308]]. Action = [[-0.0531912   0.06026769  0.16997606 -0.25453043]]. Reward = [0.]
Curr episode timestep = 304
Current timestep = 2109. State = [[-0.2726589   0.21377443]]. Action = [[-0.11799121  0.12607804 -0.22351815  0.7405646 ]]. Reward = [0.]
Curr episode timestep = 305
Current timestep = 2110. State = [[-0.27988887  0.2178999 ]]. Action = [[ 0.14845568 -0.20443699  0.09727117 -0.28136647]]. Reward = [0.]
Curr episode timestep = 306
Current timestep = 2111. State = [[-0.28236097  0.21526998]]. Action = [[-0.08480075  0.15368718 -0.02349594  0.49277377]]. Reward = [0.]
Curr episode timestep = 307
Current timestep = 2112. State = [[-0.28498766  0.21729913]]. Action = [[-0.01350497 -0.01618385 -0.21547744  0.66482794]]. Reward = [0.]
Curr episode timestep = 308
Current timestep = 2113. State = [[-0.28680068  0.21845542]]. Action = [[0.04909265 0.02614945 0.19517064 0.6617335 ]]. Reward = [0.]
Curr episode timestep = 309
Current timestep = 2114. State = [[-0.28752944  0.21931589]]. Action = [[ 0.08608189  0.11393222 -0.15213943 -0.09002721]]. Reward = [0.]
Curr episode timestep = 310
Current timestep = 2115. State = [[-0.28839672  0.22067605]]. Action = [[-0.17604189 -0.00656286  0.01726225  0.57399416]]. Reward = [0.]
Curr episode timestep = 311
Current timestep = 2116. State = [[-0.29105833  0.22381335]]. Action = [[ 0.12719432  0.21017453  0.18388182 -0.8095735 ]]. Reward = [0.]
Curr episode timestep = 312
Current timestep = 2117. State = [[-0.2928895  0.2278612]]. Action = [[-0.20971656 -0.18436493 -0.18340343  0.9340687 ]]. Reward = [0.]
Curr episode timestep = 313
Current timestep = 2118. State = [[-0.29589427  0.22927344]]. Action = [[-0.03893429  0.05441689 -0.20728259  0.54455066]]. Reward = [0.]
Curr episode timestep = 314
Current timestep = 2119. State = [[-0.2983694   0.23121202]]. Action = [[-0.02369037 -0.16996756 -0.11049369  0.8562143 ]]. Reward = [0.]
Curr episode timestep = 315
Current timestep = 2120. State = [[-0.2994504   0.22891544]]. Action = [[ 0.17144102 -0.18773997  0.09955323 -0.03352642]]. Reward = [0.]
Curr episode timestep = 316
Current timestep = 2121. State = [[-0.2969846   0.22231294]]. Action = [[ 0.09622136 -0.1557122  -0.16723551  0.47178257]]. Reward = [0.]
Curr episode timestep = 317
Current timestep = 2122. State = [[-0.29297096  0.21439187]]. Action = [[-0.16115056 -0.18402253 -0.21955307 -0.6867723 ]]. Reward = [0.]
Curr episode timestep = 318
Current timestep = 2123. State = [[-0.29431498  0.2053836 ]]. Action = [[-0.22380994  0.19021595 -0.10954261  0.72025895]]. Reward = [0.]
Curr episode timestep = 319
Current timestep = 2124. State = [[-0.30081654  0.20383142]]. Action = [[-0.21040928  0.14830673 -0.21303895  0.98988605]]. Reward = [0.]
Curr episode timestep = 320
Current timestep = 2125. State = [[-0.3101203   0.20674618]]. Action = [[ 0.13350058  0.19363093 -0.22073446  0.9818783 ]]. Reward = [0.]
Curr episode timestep = 321
Current timestep = 2126. State = [[-0.31433702  0.21180473]]. Action = [[ 0.03496635 -0.18463555 -0.06556571  0.19780374]]. Reward = [0.]
Curr episode timestep = 322
Current timestep = 2127. State = [[-0.31556877  0.21133797]]. Action = [[-0.00851052  0.09239435 -0.09869742 -0.56384796]]. Reward = [0.]
Curr episode timestep = 323
Current timestep = 2128. State = [[-0.3165685   0.21261545]]. Action = [[ 0.0322457   0.02230838  0.22252467 -0.7367836 ]]. Reward = [0.]
Curr episode timestep = 324
Current timestep = 2129. State = [[-0.3168298  0.2133632]]. Action = [[0.043982   0.02514121 0.19398898 0.6307514 ]]. Reward = [0.]
Curr episode timestep = 325
Current timestep = 2130. State = [[-0.31679276  0.21393146]]. Action = [[ 0.17748946  0.1816476  -0.22673409  0.2354939 ]]. Reward = [0.]
Curr episode timestep = 326
Current timestep = 2131. State = [[-0.31497002  0.2169364 ]]. Action = [[ 0.07116762 -0.01505671 -0.21959218  0.80945945]]. Reward = [0.]
Curr episode timestep = 327
Current timestep = 2132. State = [[-0.31282914  0.21918267]]. Action = [[ 0.04819259  0.05228195  0.0800854  -0.8883474 ]]. Reward = [0.]
Curr episode timestep = 328
Current timestep = 2133. State = [[-0.31078345  0.221257  ]]. Action = [[-0.08049378  0.15111196 -0.16586703 -0.52779186]]. Reward = [0.]
Curr episode timestep = 329
Current timestep = 2134. State = [[-0.31073773  0.22592275]]. Action = [[ 0.20476955 -0.16561095 -0.24002416 -0.8377071 ]]. Reward = [0.]
Curr episode timestep = 330
Current timestep = 2135. State = [[-0.30791107  0.22582205]]. Action = [[-0.18822351  0.11723515 -0.22711667 -0.09985888]]. Reward = [0.]
Curr episode timestep = 331
Current timestep = 2136. State = [[-0.3087182   0.22786964]]. Action = [[-0.01867791 -0.20770067 -0.14547068 -0.11155975]]. Reward = [0.]
Curr episode timestep = 332
Current timestep = 2137. State = [[-0.30777186  0.2270817 ]]. Action = [[ 0.11849698  0.14524338 -0.17469761  0.02266634]]. Reward = [0.]
Curr episode timestep = 333
Current timestep = 2138. State = [[-0.30724365  0.22754505]]. Action = [[-0.1518928   0.17652306 -0.06887513  0.8745687 ]]. Reward = [0.]
Curr episode timestep = 334
Current timestep = 2139. State = [[-0.3097609   0.23281951]]. Action = [[-0.07365708  0.1551525   0.21777835 -0.8151308 ]]. Reward = [0.]
Curr episode timestep = 335
Current timestep = 2140. State = [[-0.31300133  0.23974961]]. Action = [[ 0.19374597  0.11102861 -0.14385273 -0.2504027 ]]. Reward = [0.]
Curr episode timestep = 336
Current timestep = 2141. State = [[-0.31214604  0.2462608 ]]. Action = [[-0.07469532  0.07755184 -0.20200737 -0.2013464 ]]. Reward = [0.]
Curr episode timestep = 337
Current timestep = 2142. State = [[-0.3111882   0.25399688]]. Action = [[0.18753299 0.06448534 0.08528346 0.23482656]]. Reward = [0.]
Curr episode timestep = 338
Current timestep = 2143. State = [[-0.30798623  0.25906453]]. Action = [[0.19751716 0.03186283 0.15409547 0.67126036]]. Reward = [0.]
Curr episode timestep = 339
Current timestep = 2144. State = [[-0.30191058  0.26407042]]. Action = [[-0.19768755  0.21132767 -0.09832227  0.95586705]]. Reward = [0.]
Curr episode timestep = 340
Current timestep = 2145. State = [[-0.30084148  0.27187344]]. Action = [[ 0.138408   -0.08786303  0.13869375  0.9057133 ]]. Reward = [0.]
Curr episode timestep = 341
Current timestep = 2146. State = [[-0.29722136  0.2753324 ]]. Action = [[-0.12954652 -0.22551206  0.06332737 -0.72405607]]. Reward = [0.]
Curr episode timestep = 342
Current timestep = 2147. State = [[-0.29618412  0.27437654]]. Action = [[ 1.3348502e-01 -5.1304698e-05  1.5818399e-01  9.8734379e-01]]. Reward = [0.]
Curr episode timestep = 343
Current timestep = 2148. State = [[-0.29366794  0.27258486]]. Action = [[ 0.07265353 -0.08326986  0.09105283 -0.17387456]]. Reward = [0.]
Curr episode timestep = 344
Current timestep = 2149. State = [[-0.29027528  0.2695456 ]]. Action = [[-0.2327822  -0.24296625  0.12035835  0.39780104]]. Reward = [0.]
Curr episode timestep = 345
Current timestep = 2150. State = [[-0.29028785  0.26409763]]. Action = [[-0.06405669 -0.18701002  0.23700106 -0.5784758 ]]. Reward = [0.]
Curr episode timestep = 346
Current timestep = 2151. State = [[-0.29251167  0.2554316 ]]. Action = [[-0.15561917  0.03740934 -0.1827725  -0.8081362 ]]. Reward = [0.]
Curr episode timestep = 347
Current timestep = 2152. State = [[-0.29653966  0.25035262]]. Action = [[ 0.06878406 -0.15025142  0.06912881 -0.21339196]]. Reward = [0.]
Curr episode timestep = 348
Current timestep = 2153. State = [[-0.2990055   0.24352312]]. Action = [[-0.18648654  0.24106371 -0.21675156  0.6552744 ]]. Reward = [0.]
Curr episode timestep = 349
Current timestep = 2154. State = [[-0.30283877  0.24513265]]. Action = [[ 0.15763748 -0.23945309 -0.1746797  -0.10829872]]. Reward = [0.]
Curr episode timestep = 350
Current timestep = 2155. State = [[-0.30238554  0.24070416]]. Action = [[ 0.08542201  0.09293202  0.12982187 -0.94752693]]. Reward = [0.]
Curr episode timestep = 351
Current timestep = 2156. State = [[-0.3013819   0.23992202]]. Action = [[0.03720313 0.14221212 0.13562757 0.83481956]]. Reward = [0.]
Curr episode timestep = 352
Current timestep = 2157. State = [[-0.30163857  0.24071963]]. Action = [[-0.20951922 -0.17981309 -0.03382725  0.24167407]]. Reward = [0.]
Curr episode timestep = 353
Current timestep = 2158. State = [[-0.30225968  0.23934218]]. Action = [[ 0.07468745 -0.20066576 -0.23363595 -0.43386465]]. Reward = [0.]
Curr episode timestep = 354
Current timestep = 2159. State = [[-0.30140737  0.23371744]]. Action = [[-0.12641795 -0.062682    0.11895409 -0.6527118 ]]. Reward = [0.]
Curr episode timestep = 355
Current timestep = 2160. State = [[-0.30372968  0.22723629]]. Action = [[-0.0236831  -0.16478567  0.04486299 -0.5670085 ]]. Reward = [0.]
Curr episode timestep = 356
Current timestep = 2161. State = [[-0.3060666   0.21886934]]. Action = [[ 0.07570097 -0.07096019  0.10539639 -0.04722822]]. Reward = [0.]
Curr episode timestep = 357
Current timestep = 2162. State = [[-0.30665448  0.21067058]]. Action = [[ 0.24419326 -0.23831308 -0.15087563 -0.86973375]]. Reward = [0.]
Curr episode timestep = 358
Current timestep = 2163. State = [[-0.30169043  0.19938147]]. Action = [[ 0.17949784 -0.0162591   0.08995461 -0.22282326]]. Reward = [0.]
Curr episode timestep = 359
Current timestep = 2164. State = [[-0.29434854  0.19235463]]. Action = [[-0.2190371  -0.01383847 -0.16055407  0.6571889 ]]. Reward = [0.]
Curr episode timestep = 360
Current timestep = 2165. State = [[-0.29226482  0.18818061]]. Action = [[ 0.12812772 -0.13175234 -0.22132897  0.9771223 ]]. Reward = [0.]
Curr episode timestep = 361
Current timestep = 2166. State = [[-0.29013932  0.18098707]]. Action = [[-0.18663545  0.05618167 -0.17028642 -0.7451398 ]]. Reward = [0.]
Curr episode timestep = 362
Current timestep = 2167. State = [[-0.292664    0.17708334]]. Action = [[ 0.01150879 -0.17037484 -0.23846725  0.84528255]]. Reward = [0.]
Curr episode timestep = 363
Current timestep = 2168. State = [[-0.29439965  0.17124948]]. Action = [[ 0.23899853  0.23204297  0.07657471 -0.5578565 ]]. Reward = [0.]
Curr episode timestep = 364
Current timestep = 2169. State = [[-0.29294455  0.17086853]]. Action = [[ 0.01292604 -0.19466673 -0.11812007 -0.7235978 ]]. Reward = [0.]
Curr episode timestep = 365
Current timestep = 2170. State = [[-0.29012895  0.166802  ]]. Action = [[-0.17192574  0.07388985 -0.2281961  -0.49009424]]. Reward = [0.]
Curr episode timestep = 366
Current timestep = 2171. State = [[-0.29081708  0.16640021]]. Action = [[ 0.15363634  0.2081874   0.14435211 -0.4500625 ]]. Reward = [0.]
Curr episode timestep = 367
Current timestep = 2172. State = [[-0.2904723   0.16860983]]. Action = [[ 0.12867564  0.01890105 -0.01510787 -0.03449452]]. Reward = [0.]
Curr episode timestep = 368
Current timestep = 2173. State = [[-0.28865403  0.17060867]]. Action = [[ 0.03944021  0.06558087 -0.22435579 -0.06008196]]. Reward = [0.]
Curr episode timestep = 369
Current timestep = 2174. State = [[-0.28613368  0.17384677]]. Action = [[-0.06475559  0.0763379  -0.02452642 -0.438282  ]]. Reward = [0.]
Curr episode timestep = 370
Current timestep = 2175. State = [[-0.28482008  0.17810021]]. Action = [[-0.11014801  0.21118796 -0.17923602  0.8121791 ]]. Reward = [0.]
Curr episode timestep = 371
Current timestep = 2176. State = [[-0.2862665   0.18497288]]. Action = [[ 0.02040434 -0.0018253   0.08394036 -0.65765053]]. Reward = [0.]
Curr episode timestep = 372
Current timestep = 2177. State = [[-0.2868118   0.18984213]]. Action = [[ 0.19622117 -0.11391795 -0.21323182  0.89993525]]. Reward = [0.]
Curr episode timestep = 373
Current timestep = 2178. State = [[-0.28415012  0.19077797]]. Action = [[ 0.00584358 -0.01842597 -0.23626459  0.7282418 ]]. Reward = [0.]
Curr episode timestep = 374
Current timestep = 2179. State = [[-0.28139806  0.19074506]]. Action = [[ 0.16275954 -0.05562033 -0.05837665  0.158705  ]]. Reward = [0.]
Curr episode timestep = 375
Current timestep = 2180. State = [[-0.27730975  0.1887245 ]]. Action = [[-0.07359757 -0.18244389  0.12653184  0.76254725]]. Reward = [0.]
Curr episode timestep = 376
Current timestep = 2181. State = [[-0.27367383  0.18412201]]. Action = [[ 0.0771395  -0.18882278 -0.08743405  0.80861473]]. Reward = [0.]
Curr episode timestep = 377
Current timestep = 2182. State = [[-0.26895082  0.17744084]]. Action = [[ 0.11710927 -0.03456251 -0.13451505  0.5358472 ]]. Reward = [0.]
Curr episode timestep = 378
Current timestep = 2183. State = [[-0.26391742  0.17133988]]. Action = [[ 0.11055177  0.15710992  0.20872426 -0.21626282]]. Reward = [0.]
Curr episode timestep = 379
Current timestep = 2184. State = [[-0.25951067  0.1701236 ]]. Action = [[ 0.24605167  0.04781261  0.2108655  -0.7533823 ]]. Reward = [0.]
Curr episode timestep = 380
Current timestep = 2185. State = [[-0.25225437  0.17037565]]. Action = [[ 0.19734466  0.13651246  0.17987704 -0.8313432 ]]. Reward = [0.]
Curr episode timestep = 381
Current timestep = 2186. State = [[-0.2421713   0.17502418]]. Action = [[-0.01108447  0.17773187  0.01153228 -0.25922656]]. Reward = [0.]
Curr episode timestep = 382
Current timestep = 2187. State = [[-0.23496307  0.18288752]]. Action = [[-0.15100184  0.00459129 -0.12866634 -0.6414464 ]]. Reward = [0.]
Curr episode timestep = 383
Current timestep = 2188. State = [[-0.23309633  0.18779892]]. Action = [[-0.07403369 -0.07183361 -0.0975115  -0.67812884]]. Reward = [0.]
Curr episode timestep = 384
Current timestep = 2189. State = [[-0.2328334   0.18947569]]. Action = [[ 0.21387744  0.1976634  -0.14586876  0.8904476 ]]. Reward = [0.]
Curr episode timestep = 385
Current timestep = 2190. State = [[-0.22992043  0.19454303]]. Action = [[-0.19020976  0.23864216  0.16579732  0.4531381 ]]. Reward = [0.]
Curr episode timestep = 386
Current timestep = 2191. State = [[-0.2298006   0.20411566]]. Action = [[-0.03530885 -0.08339098 -0.15605567  0.08372188]]. Reward = [0.]
Curr episode timestep = 387
Current timestep = 2192. State = [[-0.23078963  0.20877096]]. Action = [[ 0.09338069  0.24668682 -0.21991478 -0.7242546 ]]. Reward = [0.]
Curr episode timestep = 388
Current timestep = 2193. State = [[-0.23070566  0.21672381]]. Action = [[0.00331905 0.14567906 0.16959119 0.86848307]]. Reward = [0.]
Curr episode timestep = 389
Current timestep = 2194. State = [[-0.22998281  0.22622593]]. Action = [[ 0.12251756 -0.09041548 -0.14014058 -0.6453381 ]]. Reward = [0.]
Curr episode timestep = 390
Current timestep = 2195. State = [[-0.22751707  0.23060828]]. Action = [[-0.01819971  0.13234895  0.12917256 -0.14498699]]. Reward = [0.]
Curr episode timestep = 391
Current timestep = 2196. State = [[-0.22485773  0.23620969]]. Action = [[ 0.20026332  0.1666019   0.23548481 -0.4765247 ]]. Reward = [0.]
Curr episode timestep = 392
Current timestep = 2197. State = [[-0.22053926  0.243827  ]]. Action = [[-0.0745769   0.2243472  -0.1452046   0.08085465]]. Reward = [0.]
Curr episode timestep = 393
Current timestep = 2198. State = [[-0.21756142  0.2554478 ]]. Action = [[ 0.19820881  0.17695457 -0.11821315  0.48581672]]. Reward = [0.]
Curr episode timestep = 394
Current timestep = 2199. State = [[-0.21248558  0.26779786]]. Action = [[-0.1526488   0.18072003 -0.17524283 -0.6728178 ]]. Reward = [0.]
Curr episode timestep = 395
Current timestep = 2200. State = [[-0.21076798  0.2798548 ]]. Action = [[ 0.22136706 -0.00085913  0.17476672 -0.34766465]]. Reward = [0.]
Curr episode timestep = 396
Current timestep = 2201. State = [[-0.20583422  0.28771883]]. Action = [[ 0.15576506  0.07149577 -0.01800065 -0.9391656 ]]. Reward = [0.]
Curr episode timestep = 397
Current timestep = 2202. State = [[-0.20001538  0.29423353]]. Action = [[-0.10214701 -0.1784004  -0.12302008 -0.43612325]]. Reward = [0.]
Curr episode timestep = 398
Current timestep = 2203. State = [[-0.19589178  0.29537153]]. Action = [[ 0.24225897  0.0594767   0.19473282 -0.8335627 ]]. Reward = [0.]
Curr episode timestep = 399
Current timestep = 2204. State = [[-0.19047351  0.29685485]]. Action = [[-0.17715468  0.08062252 -0.0465567   0.7722361 ]]. Reward = [0.]
Curr episode timestep = 400
Current timestep = 2205. State = [[-0.18861866  0.29982352]]. Action = [[ 0.13621575 -0.18121615  0.14102486 -0.8473462 ]]. Reward = [0.]
Curr episode timestep = 401
Current timestep = 2206. State = [[-0.18483965  0.2969852 ]]. Action = [[ 1.2572223e-01  5.9750676e-04 -2.3101868e-01 -7.0961076e-01]]. Reward = [0.]
Curr episode timestep = 402
Current timestep = 2207. State = [[-0.17946415  0.29528514]]. Action = [[-0.22455508 -0.14135723  0.23020011  0.20486486]]. Reward = [0.]
Curr episode timestep = 403
Current timestep = 2208. State = [[-0.17812437  0.2930267 ]]. Action = [[-0.05157338 -0.07827541 -0.19792552 -0.96066344]]. Reward = [0.]
Curr episode timestep = 404
Current timestep = 2209. State = [[-0.17774636  0.29023275]]. Action = [[-0.04967749  0.16781032  0.10619313  0.26671505]]. Reward = [0.]
Curr episode timestep = 405
Current timestep = 2210. State = [[-0.17940263  0.2921462 ]]. Action = [[-0.14475659  0.23677188  0.07714009 -0.2892145 ]]. Reward = [0.]
Curr episode timestep = 406
Current timestep = 2211. State = [[-0.1838827   0.29776573]]. Action = [[ 0.13549453  0.00515783 -0.08000621  0.93269265]]. Reward = [0.]
Curr episode timestep = 407
Current timestep = 2212. State = [[-0.18566434  0.30058601]]. Action = [[-0.11621457  0.03084293 -0.02044052  0.7227633 ]]. Reward = [0.]
Curr episode timestep = 408
Current timestep = 2213. State = [[-0.18806258  0.30343094]]. Action = [[ 0.07821724  0.23753339 -0.12396799  0.79150975]]. Reward = [0.]
Curr episode timestep = 409
Current timestep = 2214. State = [[-0.18992338  0.3055129 ]]. Action = [[-0.19348678 -0.09948072 -0.18601577  0.9049535 ]]. Reward = [0.]
Curr episode timestep = 410
Current timestep = 2215. State = [[-0.19231668  0.30649942]]. Action = [[ 0.07198453 -0.20401058 -0.00366335  0.67828846]]. Reward = [0.]
Curr episode timestep = 411
Current timestep = 2216. State = [[-0.19234897  0.30213392]]. Action = [[-0.15315923 -0.01873614  0.07890558 -0.1962375 ]]. Reward = [0.]
Curr episode timestep = 412
Current timestep = 2217. State = [[-0.19484855  0.29864383]]. Action = [[ 0.01773542 -0.1191439   0.10472149  0.5811672 ]]. Reward = [0.]
Curr episode timestep = 413
Current timestep = 2218. State = [[-0.19644625  0.2928173 ]]. Action = [[ 0.15706289 -0.16410257  0.12822935  0.32761073]]. Reward = [0.]
Curr episode timestep = 414
Current timestep = 2219. State = [[-0.19457833  0.28599727]]. Action = [[ 0.11583543  0.12563467  0.2036919  -0.42220342]]. Reward = [0.]
Curr episode timestep = 415
Current timestep = 2220. State = [[-0.19276433  0.2838222 ]]. Action = [[ 0.05243409 -0.23342413 -0.06826094  0.9913492 ]]. Reward = [0.]
Curr episode timestep = 416
Current timestep = 2221. State = [[-0.18891145  0.27796352]]. Action = [[-0.06052172  0.08441833  0.23473042 -0.7762052 ]]. Reward = [0.]
Curr episode timestep = 417
Current timestep = 2222. State = [[-0.18752001  0.27575254]]. Action = [[ 0.14105707 -0.12847523 -0.07843255 -0.44951105]]. Reward = [0.]
Curr episode timestep = 418
Current timestep = 2223. State = [[-0.18439244  0.27096853]]. Action = [[-0.1262891  -0.18488559 -0.23220019 -0.8411078 ]]. Reward = [0.]
Curr episode timestep = 419
Current timestep = 2224. State = [[-0.1832649  0.2631207]]. Action = [[-0.22644964 -0.03865419 -0.18755095 -0.12657678]]. Reward = [0.]
Curr episode timestep = 420
Current timestep = 2225. State = [[-0.18756299  0.25734404]]. Action = [[-0.05319816  0.22408032 -0.16949415  0.13484025]]. Reward = [0.]
Curr episode timestep = 421
Current timestep = 2226. State = [[-0.19200975  0.25966084]]. Action = [[0.08100307 0.22024786 0.14383554 0.87080264]]. Reward = [0.]
Curr episode timestep = 422
Current timestep = 2227. State = [[-0.19513984  0.2646403 ]]. Action = [[ 0.20669675  0.00919512 -0.18625328  0.26548195]]. Reward = [0.]
Curr episode timestep = 423
Current timestep = 2228. State = [[-0.19452408  0.26668453]]. Action = [[ 0.23550564  0.15680674 -0.15385012 -0.2228759 ]]. Reward = [0.]
Curr episode timestep = 424
Current timestep = 2229. State = [[-0.19089314  0.27192253]]. Action = [[-0.17707829  0.23192102  0.12142974  0.32546186]]. Reward = [0.]
Curr episode timestep = 425
Current timestep = 2230. State = [[-0.18988799  0.28246507]]. Action = [[ 0.22144243  0.20843163 -0.11506271  0.88144505]]. Reward = [0.]
Curr episode timestep = 426
Current timestep = 2231. State = [[-0.18611158  0.2922434 ]]. Action = [[ 0.21604592 -0.21604784 -0.22792184  0.28250265]]. Reward = [0.]
Curr episode timestep = 427
Current timestep = 2232. State = [[-0.17897291  0.2938558 ]]. Action = [[-0.21012904 -0.07387763 -0.20576686 -0.37283456]]. Reward = [0.]
Curr episode timestep = 428
Current timestep = 2233. State = [[-0.17646421  0.29464012]]. Action = [[ 0.06544083 -0.13976508  0.02192548  0.43320525]]. Reward = [0.]
Curr episode timestep = 429
Current timestep = 2234. State = [[-0.17386429  0.29189974]]. Action = [[ 0.04593933  0.18749684  0.10871297 -0.6921806 ]]. Reward = [0.]
Curr episode timestep = 430
Current timestep = 2235. State = [[-0.17233533  0.2937663 ]]. Action = [[ 0.01154873 -0.11369821  0.07137865  0.24968314]]. Reward = [0.]
Curr episode timestep = 431
Current timestep = 2236. State = [[-0.17047527  0.29216382]]. Action = [[-0.05213717 -0.22657387  0.10814345 -0.48666036]]. Reward = [0.]
Curr episode timestep = 432
Current timestep = 2237. State = [[-0.16699626  0.28684264]]. Action = [[ 0.04249394 -0.17890158 -0.07239896  0.7878258 ]]. Reward = [0.]
Curr episode timestep = 433
Current timestep = 2238. State = [[-0.16355063  0.28068054]]. Action = [[-0.16552179  0.15680063  0.09430665 -0.07888901]]. Reward = [0.]
Curr episode timestep = 434
Current timestep = 2239. State = [[-0.16385573  0.28023094]]. Action = [[ 0.16408518 -0.19536287  0.10608223 -0.52385515]]. Reward = [0.]
Curr episode timestep = 435
Current timestep = 2240. State = [[-0.16158627  0.27497283]]. Action = [[ 0.07567078  0.23562622  0.1620974  -0.1283266 ]]. Reward = [0.]
Curr episode timestep = 436
Current timestep = 2241. State = [[-0.16112716  0.2749521 ]]. Action = [[-0.21653743 -0.21757114  0.1519258  -0.5006333 ]]. Reward = [0.]
Curr episode timestep = 437
Current timestep = 2242. State = [[-0.16184458  0.2710775 ]]. Action = [[ 0.10878283 -0.22676536 -0.16162671 -0.37750947]]. Reward = [0.]
Curr episode timestep = 438
Current timestep = 2243. State = [[-0.16096172  0.26243904]]. Action = [[-0.03702752 -0.22558619 -0.03636271  0.83154225]]. Reward = [0.]
Curr episode timestep = 439
Current timestep = 2244. State = [[-0.1600976   0.25211295]]. Action = [[-0.08341566  0.24594617 -0.08069578  0.8068582 ]]. Reward = [0.]
Curr episode timestep = 440
Current timestep = 2245. State = [[-0.16235428  0.25114822]]. Action = [[-0.24617112 -0.01834916  0.22406524 -0.00637394]]. Reward = [0.]
Curr episode timestep = 441
Current timestep = 2246. State = [[-0.16691068  0.25072497]]. Action = [[ 0.01401386 -0.19920027 -0.0045366  -0.25813842]]. Reward = [0.]
Curr episode timestep = 442
Current timestep = 2247. State = [[-0.16954325  0.24508525]]. Action = [[ 0.04159653  0.10346434 -0.1990827   0.6568191 ]]. Reward = [0.]
Curr episode timestep = 443
Current timestep = 2248. State = [[-0.17140281  0.24380378]]. Action = [[-0.14324975  0.09618631 -0.17937389 -0.22148728]]. Reward = [0.]
Curr episode timestep = 444
Current timestep = 2249. State = [[-0.17467505  0.24590935]]. Action = [[ 0.24541104  0.04469359 -0.1746605   0.6748928 ]]. Reward = [0.]
Curr episode timestep = 445
Current timestep = 2250. State = [[-0.17463367  0.24630967]]. Action = [[-0.04936911 -0.18375334  0.0160512   0.7185495 ]]. Reward = [0.]
Curr episode timestep = 446
Current timestep = 2251. State = [[-0.1738819   0.24372634]]. Action = [[-0.07873181 -0.0152062   0.19693851  0.7121093 ]]. Reward = [0.]
Curr episode timestep = 447
Current timestep = 2252. State = [[-0.17477667  0.24148375]]. Action = [[-0.00618646 -0.01385577  0.23347288 -0.9265541 ]]. Reward = [0.]
Curr episode timestep = 448
Current timestep = 2253. State = [[-0.17517501  0.23894525]]. Action = [[ 0.08728972 -0.19012667  0.10387447  0.52899814]]. Reward = [0.]
Curr episode timestep = 449
Current timestep = 2254. State = [[-0.17462793  0.23260449]]. Action = [[-0.14638424 -0.11800715  0.02907023 -0.81715775]]. Reward = [0.]
Curr episode timestep = 450
Current timestep = 2255. State = [[-0.1769026   0.22631621]]. Action = [[-0.19283848  0.10839856 -0.00268643  0.89172006]]. Reward = [0.]
Curr episode timestep = 451
Current timestep = 2256. State = [[-0.18136942  0.22514038]]. Action = [[ 0.2297194  -0.02431673 -0.24013513 -0.9439707 ]]. Reward = [0.]
Curr episode timestep = 452
Current timestep = 2257. State = [[-0.18149874  0.22342673]]. Action = [[-0.08218974  0.2222467   0.06886262  0.65732074]]. Reward = [0.]
Curr episode timestep = 453
Current timestep = 2258. State = [[-0.18349208  0.22690254]]. Action = [[ 0.16179967 -0.19067557 -0.01775262  0.1833148 ]]. Reward = [0.]
Curr episode timestep = 454
Current timestep = 2259. State = [[-0.18157566  0.22445548]]. Action = [[ 0.2381506  -0.19576272 -0.00100449 -0.60715324]]. Reward = [0.]
Curr episode timestep = 455
Current timestep = 2260. State = [[-0.17627145  0.21801707]]. Action = [[0.15940166 0.09233981 0.20603067 0.6984415 ]]. Reward = [0.]
Curr episode timestep = 456
Current timestep = 2261. State = [[-0.1715848   0.21519111]]. Action = [[ 0.02362865  0.23482937 -0.09070706 -0.30246   ]]. Reward = [0.]
Curr episode timestep = 457
Current timestep = 2262. State = [[-0.16756889  0.21966784]]. Action = [[-0.18350632  0.19655421  0.11064133 -0.5066548 ]]. Reward = [0.]
Curr episode timestep = 458
Current timestep = 2263. State = [[-0.16753636  0.22670431]]. Action = [[-0.07793835 -0.11730944  0.17071423 -0.6312208 ]]. Reward = [0.]
Curr episode timestep = 459
Current timestep = 2264. State = [[-0.1690519   0.22900285]]. Action = [[-0.020211    0.19995496  0.02791163 -0.18396777]]. Reward = [0.]
Curr episode timestep = 460
Current timestep = 2265. State = [[-0.17128412  0.23442183]]. Action = [[ 0.09172308  0.07578391  0.18039191 -0.8560975 ]]. Reward = [0.]
Curr episode timestep = 461
Current timestep = 2266. State = [[-0.17130278  0.23961186]]. Action = [[ 0.21829447  0.1677452  -0.04370308  0.41452694]]. Reward = [0.]
Curr episode timestep = 462
Current timestep = 2267. State = [[-0.16773038  0.24739979]]. Action = [[-0.12788244  0.15684995  0.14698547  0.3973739 ]]. Reward = [0.]
Curr episode timestep = 463
Current timestep = 2268. State = [[-0.1660174   0.25775892]]. Action = [[-0.18903044  0.1731529  -0.07490939  0.49922228]]. Reward = [0.]
Curr episode timestep = 464
Current timestep = 2269. State = [[-0.16841839  0.2685965 ]]. Action = [[ 0.06513792  0.04143071  0.10675657 -0.39473665]]. Reward = [0.]
Curr episode timestep = 465
Current timestep = 2270. State = [[-0.16942376  0.27610347]]. Action = [[ 0.06177425 -0.17631969 -0.0327149  -0.5868789 ]]. Reward = [0.]
Curr episode timestep = 466
Current timestep = 2271. State = [[-0.16888836  0.27623057]]. Action = [[-0.16942048 -0.17566586 -0.07246113  0.82605934]]. Reward = [0.]
Curr episode timestep = 467
Current timestep = 2272. State = [[-0.16896896  0.27364337]]. Action = [[ 0.17729527 -0.23187563  0.03349495  0.8472624 ]]. Reward = [0.]
Curr episode timestep = 468
Current timestep = 2273. State = [[-0.16656695  0.2666088 ]]. Action = [[ 0.1970399  -0.0286331   0.05668536  0.38983   ]]. Reward = [0.]
Curr episode timestep = 469
Current timestep = 2274. State = [[-0.16195314  0.26024777]]. Action = [[-0.11650339  0.06453103 -0.14727026  0.4129312 ]]. Reward = [0.]
Curr episode timestep = 470
Current timestep = 2275. State = [[-0.16086403  0.25892156]]. Action = [[ 0.06793696  0.22204727  0.15404397 -0.35385692]]. Reward = [0.]
Curr episode timestep = 471
Current timestep = 2276. State = [[-0.16043378  0.2618616 ]]. Action = [[ 0.18954492 -0.00676847  0.12861979  0.17119145]]. Reward = [0.]
Curr episode timestep = 472
Current timestep = 2277. State = [[-0.1568062   0.26336765]]. Action = [[ 0.20590782  0.10075793 -0.05735978 -0.8879875 ]]. Reward = [0.]
Curr episode timestep = 473
Current timestep = 2278. State = [[-0.1513483   0.26615834]]. Action = [[-0.23353712 -0.16546567 -0.04886098  0.00634146]]. Reward = [0.]
Curr episode timestep = 474
Current timestep = 2279. State = [[-0.14862488  0.2649721 ]]. Action = [[ 0.08916569 -0.16198333 -0.14196281  0.45697522]]. Reward = [0.]
Curr episode timestep = 475
Current timestep = 2280. State = [[-0.14569001  0.26084334]]. Action = [[-0.08638218  0.01162788 -0.20168489 -0.2845527 ]]. Reward = [0.]
Curr episode timestep = 476
Current timestep = 2281. State = [[-0.14421695  0.25891617]]. Action = [[0.21223935 0.05753344 0.01412901 0.03774714]]. Reward = [0.]
Curr episode timestep = 477
Current timestep = 2282. State = [[-0.14145553  0.2572434 ]]. Action = [[-0.06985706 -0.16928238 -0.0570181  -0.95844245]]. Reward = [0.]
Curr episode timestep = 478
Current timestep = 2283. State = [[-0.13908339  0.25363812]]. Action = [[-0.20322251  0.19036204 -0.20138498 -0.20187604]]. Reward = [0.]
Curr episode timestep = 479
Current timestep = 2284. State = [[-0.14084674  0.25584438]]. Action = [[-0.23268715 -0.01135242  0.01412737  0.81832063]]. Reward = [0.]
Curr episode timestep = 480
Current timestep = 2285. State = [[-0.14419442  0.25838116]]. Action = [[ 0.10801178  0.24080011  0.21239978 -0.3054886 ]]. Reward = [0.]
Curr episode timestep = 481
Current timestep = 2286. State = [[-0.14781432  0.26387104]]. Action = [[-0.01682779 -0.20849413  0.05093026 -0.44781148]]. Reward = [0.]
Curr episode timestep = 482
Current timestep = 2287. State = [[-0.14802578  0.26362216]]. Action = [[-0.08716448 -0.08993345 -0.00202103  0.96838343]]. Reward = [0.]
Curr episode timestep = 483
Current timestep = 2288. State = [[-0.14869878  0.2612491 ]]. Action = [[-0.21143213 -0.24451327 -0.0249591  -0.6303998 ]]. Reward = [0.]
Curr episode timestep = 484
Current timestep = 2289. State = [[-0.15152428  0.255046  ]]. Action = [[-0.04784906  0.09499076  0.21200901 -0.90907544]]. Reward = [0.]
Curr episode timestep = 485
Current timestep = 2290. State = [[-0.15761799  0.25250807]]. Action = [[ 0.02564043  0.1453754  -0.23641345 -0.6830366 ]]. Reward = [0.]
Curr episode timestep = 486
Current timestep = 2291. State = [[-0.16200384  0.25450698]]. Action = [[-0.15427943  0.09426922 -0.13377364 -0.50010014]]. Reward = [0.]
Curr episode timestep = 487
Current timestep = 2292. State = [[-0.1675894   0.25843367]]. Action = [[ 0.24299943  0.1897865  -0.08430335  0.8718914 ]]. Reward = [0.]
Curr episode timestep = 488
Current timestep = 2293. State = [[-0.1704337   0.26320893]]. Action = [[-0.23669305 -0.13367756 -0.15218213  0.7209351 ]]. Reward = [0.]
Curr episode timestep = 489
Current timestep = 2294. State = [[-0.1738524   0.26458412]]. Action = [[ 0.05540752 -0.02390772  0.02003706 -0.2822951 ]]. Reward = [0.]
Curr episode timestep = 490
Current timestep = 2295. State = [[-0.17546853  0.26546547]]. Action = [[-0.17029919  0.20025897  0.16468576  0.4764248 ]]. Reward = [0.]
Curr episode timestep = 491
Current timestep = 2296. State = [[-0.1797661   0.27089497]]. Action = [[-0.04223435  0.07987854 -0.03553885  0.6377878 ]]. Reward = [0.]
Curr episode timestep = 492
Current timestep = 2297. State = [[-0.18439102  0.2766856 ]]. Action = [[0.18893892 0.01184827 0.23196548 0.21703458]]. Reward = [0.]
Curr episode timestep = 493
Current timestep = 2298. State = [[-0.18536039  0.27875185]]. Action = [[ 0.06905127  0.13874501 -0.0806358   0.898695  ]]. Reward = [0.]
Curr episode timestep = 494
Current timestep = 2299. State = [[-0.18527405  0.28268483]]. Action = [[ 0.00997162 -0.09207878  0.1285212   0.52765846]]. Reward = [0.]
Curr episode timestep = 495
Current timestep = 2300. State = [[-0.18500665  0.28323498]]. Action = [[ 0.13604993  0.11417001 -0.12413625 -0.5599204 ]]. Reward = [0.]
Curr episode timestep = 496
Current timestep = 2301. State = [[-0.18334696  0.2857102 ]]. Action = [[-0.09869194 -0.15898849 -0.23729275  0.5535362 ]]. Reward = [0.]
Curr episode timestep = 497
Current timestep = 2302. State = [[-0.18264517  0.284898  ]]. Action = [[ 0.10637587  0.06589282 -0.21942827 -0.87602705]]. Reward = [0.]
Curr episode timestep = 498
Current timestep = 2303. State = [[-0.18168199  0.28536972]]. Action = [[ 0.15144151  0.14830327 -0.11578901  0.54987526]]. Reward = [0.]
Curr episode timestep = 499
Current timestep = 2304. State = [[-0.17861883  0.28970093]]. Action = [[ 0.15468693  0.18296939  0.02753901 -0.0842011 ]]. Reward = [0.]
Curr episode timestep = 500
Current timestep = 2305. State = [[-0.17360419  0.2968837 ]]. Action = [[-0.13768424  0.06853613 -0.08049196  0.599277  ]]. Reward = [0.]
Curr episode timestep = 501
Current timestep = 2306. State = [[-0.17080267  0.30361998]]. Action = [[ 0.2423347  -0.19010693 -0.01511718 -0.1365189 ]]. Reward = [0.]
Curr episode timestep = 502
Current timestep = 2307. State = [[-0.16522504  0.30259684]]. Action = [[-0.10559931 -0.08859238  0.10568389 -0.77583927]]. Reward = [0.]
Curr episode timestep = 503
Current timestep = 2308. State = [[-0.16150564  0.30011177]]. Action = [[ 0.16092473 -0.07997483  0.21357226  0.19151592]]. Reward = [0.]
Curr episode timestep = 504
Current timestep = 2309. State = [[-0.15698446  0.2959604 ]]. Action = [[-0.06096904 -0.14025085 -0.10661206 -0.24530184]]. Reward = [0.]
Curr episode timestep = 505
Current timestep = 2310. State = [[-0.15348226  0.2912066 ]]. Action = [[-0.16470985  0.23347718 -0.02889141 -0.21963477]]. Reward = [0.]
Curr episode timestep = 506
Current timestep = 2311. State = [[-0.15441936  0.29329512]]. Action = [[ 0.12981054 -0.05464728 -0.22975853  0.2635547 ]]. Reward = [0.]
Curr episode timestep = 507
Current timestep = 2312. State = [[-0.15362388  0.29321107]]. Action = [[ 0.00095341  0.21942845 -0.20953703 -0.35005486]]. Reward = [0.]
Curr episode timestep = 508
Current timestep = 2313. State = [[-0.1537501  0.2987379]]. Action = [[-0.08464351  0.13498282  0.05534905 -0.6718503 ]]. Reward = [0.]
Curr episode timestep = 509
Current timestep = 2314. State = [[-0.15382975  0.30565375]]. Action = [[ 0.2024365  -0.22725046  0.14729667 -0.3542025 ]]. Reward = [0.]
Curr episode timestep = 510
Current timestep = 2315. State = [[-0.1507383  0.3039116]]. Action = [[ 0.23627162 -0.07149303  0.15542102  0.8699571 ]]. Reward = [0.]
Curr episode timestep = 511
Current timestep = 2316. State = [[-0.14411138  0.30028912]]. Action = [[ 0.05973104 -0.05917609 -0.08336316 -0.8544992 ]]. Reward = [0.]
Curr episode timestep = 512
Current timestep = 2317. State = [[-0.13686134  0.2968574 ]]. Action = [[ 0.00447276 -0.1753201   0.1166229   0.60845804]]. Reward = [0.]
Curr episode timestep = 513
Current timestep = 2318. State = [[-0.13071798  0.29073605]]. Action = [[ 0.2037074  -0.0032206   0.13016522  0.5061667 ]]. Reward = [0.]
Curr episode timestep = 514
Current timestep = 2319. State = [[-0.12353161  0.2857787 ]]. Action = [[-0.09853935 -0.22642137  0.24154145 -0.10001385]]. Reward = [0.]
Curr episode timestep = 515
Current timestep = 2320. State = [[-0.11798519  0.27848968]]. Action = [[ 0.06703579  0.23075491  0.05389282 -0.03439105]]. Reward = [0.]
Curr episode timestep = 516
Current timestep = 2321. State = [[-0.11555976  0.27947876]]. Action = [[-0.15457675  0.23731667 -0.01773226 -0.09939301]]. Reward = [0.]
Curr episode timestep = 517
Current timestep = 2322. State = [[-0.11613682  0.28693295]]. Action = [[-0.1442284   0.0932965   0.06812873  0.21117723]]. Reward = [0.]
Curr episode timestep = 518
Current timestep = 2323. State = [[-0.11855398  0.29397616]]. Action = [[ 0.16051745  0.18268144 -0.1565766  -0.37533128]]. Reward = [0.]
Curr episode timestep = 519
Current timestep = 2324. State = [[-0.11876509  0.30186978]]. Action = [[-0.1081447   0.00239819  0.15276837 -0.14341259]]. Reward = [0.]
Curr episode timestep = 520
Current timestep = 2325. State = [[-0.11934968  0.30834746]]. Action = [[-0.13975492  0.04148844 -0.07369351 -0.98667806]]. Reward = [0.]
Curr episode timestep = 521
Current timestep = 2326. State = [[-0.12225575  0.31287178]]. Action = [[-0.18796654 -0.09376994  0.14891702 -0.32363963]]. Reward = [0.]
Curr episode timestep = 522
Current timestep = 2327. State = [[-0.12643243  0.3153317 ]]. Action = [[-0.21255036  0.23556507 -0.12483291  0.3807609 ]]. Reward = [0.]
Curr episode timestep = 523
Current timestep = 2328. State = [[-0.12888795  0.3166395 ]]. Action = [[-0.07501532 -0.23826616 -0.13860543  0.30356216]]. Reward = [0.]
Curr episode timestep = 524
Current timestep = 2329. State = [[-0.13092636  0.3118562 ]]. Action = [[-0.15357901  0.16733441 -0.08645573  0.38622308]]. Reward = [0.]
Curr episode timestep = 525
Current timestep = 2330. State = [[-0.13321742  0.30817327]]. Action = [[0.06870151 0.03052908 0.21239871 0.38045025]]. Reward = [0.]
Curr episode timestep = 526
Current timestep = 2331. State = [[-0.13404821  0.30661866]]. Action = [[ 0.10945839  0.1220361  -0.08250809  0.08453238]]. Reward = [0.]
Curr episode timestep = 527
Current timestep = 2332. State = [[-0.13433985  0.3058749 ]]. Action = [[-0.05946341 -0.14191961  0.15436935 -0.5368691 ]]. Reward = [0.]
Curr episode timestep = 528
Current timestep = 2333. State = [[-0.13474186  0.3024902 ]]. Action = [[-0.191338   -0.1245936  -0.01913477 -0.8022337 ]]. Reward = [0.]
Curr episode timestep = 529
Current timestep = 2334. State = [[-0.13706578  0.2973653 ]]. Action = [[ 0.21276057 -0.03474186 -0.06340379  0.980366  ]]. Reward = [0.]
Curr episode timestep = 530
Current timestep = 2335. State = [[-0.1374748   0.29188615]]. Action = [[ 0.18118829 -0.07523894 -0.13875452  0.6824415 ]]. Reward = [0.]
Curr episode timestep = 531
Current timestep = 2336. State = [[-0.13455452  0.28621578]]. Action = [[-0.04521501 -0.1681788   0.07125142 -0.07491517]]. Reward = [0.]
Curr episode timestep = 532
Current timestep = 2337. State = [[-0.1321943  0.2789487]]. Action = [[-0.1246451  -0.15757197 -0.07933891  0.10469663]]. Reward = [0.]
Curr episode timestep = 533
Current timestep = 2338. State = [[-0.13289535  0.27071986]]. Action = [[-0.2026642   0.09797877  0.18383667  0.14630091]]. Reward = [0.]
Curr episode timestep = 534
Current timestep = 2339. State = [[-0.13678429  0.26815268]]. Action = [[ 0.23668182 -0.20528549 -0.14166534 -0.5472678 ]]. Reward = [0.]
Curr episode timestep = 535
Current timestep = 2340. State = [[-0.13626271  0.2604483 ]]. Action = [[-0.16221283 -0.10687925 -0.12795025 -0.9183278 ]]. Reward = [0.]
Curr episode timestep = 536
Current timestep = 2341. State = [[-0.13787599  0.25251254]]. Action = [[ 0.24691337 -0.22907321 -0.15373452 -0.34515435]]. Reward = [0.]
Curr episode timestep = 537
Current timestep = 2342. State = [[-0.13621494  0.24107963]]. Action = [[ 0.14397478 -0.15310538 -0.03986739 -0.2355634 ]]. Reward = [0.]
Curr episode timestep = 538
Current timestep = 2343. State = [[-0.13138169  0.23081872]]. Action = [[ 0.16478726  0.17447317  0.14484903 -0.9271169 ]]. Reward = [0.]
Curr episode timestep = 539
Current timestep = 2344. State = [[-0.12630142  0.22615877]]. Action = [[-0.15363401 -0.16639312  0.1359492   0.9783721 ]]. Reward = [0.]
Curr episode timestep = 540
Current timestep = 2345. State = [[-0.12350434  0.22138079]]. Action = [[0.10610494 0.20175731 0.16761854 0.29600978]]. Reward = [0.]
Curr episode timestep = 541
Current timestep = 2346. State = [[-0.12213825  0.22134155]]. Action = [[ 0.00880215 -0.21017747 -0.08205388  0.01032388]]. Reward = [0.]
Curr episode timestep = 542
Current timestep = 2347. State = [[-0.11986453  0.21732338]]. Action = [[ 0.18135276  0.01919124 -0.0735741  -0.03019094]]. Reward = [0.]
Curr episode timestep = 543
Current timestep = 2348. State = [[-0.11619777  0.21364553]]. Action = [[ 0.11228564 -0.2125406  -0.21965931  0.26102042]]. Reward = [0.]
Curr episode timestep = 544
Current timestep = 2349. State = [[-0.11058857  0.20552929]]. Action = [[-0.22202386 -0.01932144 -0.02372104  0.8814013 ]]. Reward = [0.]
Curr episode timestep = 545
Current timestep = 2350. State = [[-0.10872586  0.20179272]]. Action = [[ 0.1932992   0.15293223  0.0728032  -0.59782064]]. Reward = [0.]
Curr episode timestep = 546
Current timestep = 2351. State = [[-0.10722003  0.20207366]]. Action = [[-0.06321377  0.16509092 -0.02215517 -0.76431614]]. Reward = [0.]
Curr episode timestep = 547
Current timestep = 2352. State = [[-0.10701445  0.20515656]]. Action = [[-0.21649471 -0.21515994  0.05683321 -0.129754  ]]. Reward = [0.]
Curr episode timestep = 548
Current timestep = 2353. State = [[-0.10828668  0.20378046]]. Action = [[-0.08549501  0.18569976  0.19954175 -0.7652077 ]]. Reward = [0.]
Curr episode timestep = 549
Current timestep = 2354. State = [[-0.11061314  0.20730904]]. Action = [[ 0.1344235  -0.01621114  0.23378956 -0.79332083]]. Reward = [0.]
Curr episode timestep = 550
Current timestep = 2355. State = [[-0.11106183  0.20887257]]. Action = [[ 0.10904866  0.21889237 -0.13609095  0.727401  ]]. Reward = [0.]
Curr episode timestep = 551
Current timestep = 2356. State = [[-0.11026839  0.21386084]]. Action = [[ 0.21410295  0.0512861  -0.17399876 -0.09212726]]. Reward = [0.]
Curr episode timestep = 552
Current timestep = 2357. State = [[-0.10674842  0.21901411]]. Action = [[-0.11498398  0.21819362  0.2018336  -0.27742934]]. Reward = [0.]
Curr episode timestep = 553
Current timestep = 2358. State = [[-0.10472052  0.22907856]]. Action = [[0.20668504 0.23457038 0.12537482 0.74949837]]. Reward = [0.]
Curr episode timestep = 554
Current timestep = 2359. State = [[-0.10044205  0.24088505]]. Action = [[-0.00716642  0.16896752  0.08273047 -0.6623076 ]]. Reward = [0.]
Curr episode timestep = 555
Current timestep = 2360. State = [[-0.09653631  0.25288093]]. Action = [[ 0.2287234  -0.18302236 -0.23955248  0.35606694]]. Reward = [0.]
Curr episode timestep = 556
Current timestep = 2361. State = [[-0.09054039  0.25577137]]. Action = [[-0.22629139  0.04999536  0.1515804   0.833338  ]]. Reward = [0.]
Curr episode timestep = 557
Current timestep = 2362. State = [[-0.08946186  0.25922155]]. Action = [[-0.08524223  0.04479456  0.15787151  0.3557794 ]]. Reward = [0.]
Curr episode timestep = 558
Current timestep = 2363. State = [[-0.0907326  0.2622302]]. Action = [[-0.16573058  0.14947927  0.07117265 -0.39734006]]. Reward = [0.]
Curr episode timestep = 559
Current timestep = 2364. State = [[-0.09424258  0.2677933 ]]. Action = [[ 0.11310416 -0.07581747  0.03632826  0.87415195]]. Reward = [0.]
Curr episode timestep = 560
Current timestep = 2365. State = [[-0.09465553  0.26902124]]. Action = [[ 0.00617784 -0.09428915 -0.06565818 -0.7745756 ]]. Reward = [0.]
Curr episode timestep = 561
Current timestep = 2366. State = [[-0.09436759  0.26867065]]. Action = [[-0.05291608  0.18456876 -0.03609778 -0.53660125]]. Reward = [0.]
Curr episode timestep = 562
Current timestep = 2367. State = [[-0.09624064  0.27302116]]. Action = [[-0.22570372  0.13532808  0.2008295   0.77462626]]. Reward = [0.]
Curr episode timestep = 563
Current timestep = 2368. State = [[-0.10033337  0.27921462]]. Action = [[-0.0009447  -0.18944232 -0.15783678 -0.8008856 ]]. Reward = [0.]
Curr episode timestep = 564
Current timestep = 2369. State = [[-0.10187818  0.27930123]]. Action = [[ 0.10563344 -0.23777546  0.1642614   0.90283847]]. Reward = [0.]
Curr episode timestep = 565
Current timestep = 2370. State = [[-0.10120521  0.27353388]]. Action = [[-0.06653142 -0.18069701  0.24352217  0.51796985]]. Reward = [0.]
Curr episode timestep = 566
Current timestep = 2371. State = [[-0.10172487  0.26486707]]. Action = [[-0.16492374 -0.10212952  0.23993793  0.52495   ]]. Reward = [0.]
Curr episode timestep = 567
Current timestep = 2372. State = [[-0.10401769  0.25765985]]. Action = [[ 0.11656708 -0.21747771  0.22012478  0.35842025]]. Reward = [0.]
Curr episode timestep = 568
Current timestep = 2373. State = [[-0.10524245  0.24608143]]. Action = [[-0.24537195 -0.16246782  0.07362014  0.9763446 ]]. Reward = [0.]
Curr episode timestep = 569
Current timestep = 2374. State = [[-0.11005418  0.23506127]]. Action = [[-0.17502451  0.13457662  0.08563322 -0.20936835]]. Reward = [0.]
Curr episode timestep = 570
Current timestep = 2375. State = [[-0.1164128   0.23234993]]. Action = [[ 0.12173715  0.22293457 -0.24418929  0.8806324 ]]. Reward = [0.]
Curr episode timestep = 571
Current timestep = 2376. State = [[-0.12037854  0.23578274]]. Action = [[ 0.10308608 -0.04347855 -0.18976118  0.31357598]]. Reward = [0.]
Curr episode timestep = 572
Current timestep = 2377. State = [[-0.12061989  0.2365891 ]]. Action = [[ 0.23010468 -0.01160592  0.11424857 -0.20405406]]. Reward = [0.]
Curr episode timestep = 573
Current timestep = 2378. State = [[-0.11858858  0.23531969]]. Action = [[-0.0730124  -0.15393163 -0.15872356  0.66961956]]. Reward = [0.]
Curr episode timestep = 574
Current timestep = 2379. State = [[-0.11663164  0.23131143]]. Action = [[-0.06160183 -0.23825787 -0.19673494  0.62079775]]. Reward = [0.]
Curr episode timestep = 575
Current timestep = 2380. State = [[-0.11581188  0.2240325 ]]. Action = [[-0.2138559   0.15216881 -0.01891619 -0.7052839 ]]. Reward = [0.]
Curr episode timestep = 576
Current timestep = 2381. State = [[-0.11912068  0.22269057]]. Action = [[-0.23672602 -0.17463978  0.07267284 -0.8695274 ]]. Reward = [0.]
Curr episode timestep = 577
Current timestep = 2382. State = [[-0.12507571  0.21707973]]. Action = [[-0.00218502 -0.18865255  0.21135485  0.44206762]]. Reward = [0.]
Curr episode timestep = 578
Current timestep = 2383. State = [[-0.13043196  0.20878673]]. Action = [[ 0.02452672  0.21138537  0.18077391 -0.87889665]]. Reward = [0.]
Curr episode timestep = 579
Current timestep = 2384. State = [[-0.135184    0.20817912]]. Action = [[ 0.10036737 -0.19920179  0.03677434 -0.8375492 ]]. Reward = [0.]
Curr episode timestep = 580
Current timestep = 2385. State = [[-0.13585225  0.2035624 ]]. Action = [[-0.07415676 -0.04776564 -0.24657458  0.815783  ]]. Reward = [0.]
Curr episode timestep = 581
Current timestep = 2386. State = [[-0.13672873  0.19926897]]. Action = [[ 0.10750481 -0.03132634  0.01788718  0.03196561]]. Reward = [0.]
Curr episode timestep = 582
Current timestep = 2387. State = [[-0.13660517  0.1955039 ]]. Action = [[-0.00093135 -0.1640893  -0.14627813  0.82754993]]. Reward = [0.]
Curr episode timestep = 583
Current timestep = 2388. State = [[-0.1363158   0.18866068]]. Action = [[ 0.16689691 -0.15563422  0.21030226  0.9091133 ]]. Reward = [0.]
Curr episode timestep = 584
Current timestep = 2389. State = [[-0.13366398  0.18020132]]. Action = [[ 0.08648792  0.15910923  0.17484134 -0.5706858 ]]. Reward = [0.]
Curr episode timestep = 585
Current timestep = 2390. State = [[-0.13154979  0.17726919]]. Action = [[ 0.11571467 -0.1459104  -0.2266147   0.7872493 ]]. Reward = [0.]
Curr episode timestep = 586
Current timestep = 2391. State = [[-0.12773265  0.17151347]]. Action = [[-0.21731222 -0.12419528  0.01128632  0.09403038]]. Reward = [0.]
Curr episode timestep = 587
Current timestep = 2392. State = [[-0.12681954  0.16521798]]. Action = [[-0.04110724 -0.18579209 -0.23449081  0.11304605]]. Reward = [0.]
Curr episode timestep = 588
Current timestep = 2393. State = [[-0.12683967  0.15696524]]. Action = [[ 0.11479712 -0.14717962 -0.20542356 -0.30476975]]. Reward = [0.]
Curr episode timestep = 589
Current timestep = 2394. State = [[-0.12485947  0.14830662]]. Action = [[-0.08871077 -0.21334518 -0.0017986   0.798445  ]]. Reward = [0.]
Curr episode timestep = 590
Current timestep = 2395. State = [[-0.12402652  0.13580458]]. Action = [[ 0.22061595 -0.16940168 -0.11130631 -0.4799081 ]]. Reward = [0.]
Curr episode timestep = 591
Current timestep = 2396. State = [[-0.12188931  0.12434176]]. Action = [[-0.19518705  0.14453775 -0.10531545 -0.65688837]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 2396 is [True, False, False, False, True, False]
State prediction error at timestep 2396 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2397. State = [[-0.12223087  0.12027402]]. Action = [[ 0.02509058  0.11732706  0.24470222 -0.06683856]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 2397 is [True, False, False, False, True, False]
State prediction error at timestep 2397 is tensor(4.6660e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2398. State = [[-0.12249482  0.12023738]]. Action = [[-0.048206   -0.13055192  0.15643829 -0.22821665]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 2398 is [True, False, False, False, True, False]
State prediction error at timestep 2398 is tensor(1.7871e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2399. State = [[-0.12365119  0.11780577]]. Action = [[ 0.16296     0.12189606 -0.01282676  0.24066913]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 2399 is [True, False, False, False, True, False]
State prediction error at timestep 2399 is tensor(4.0205e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2400. State = [[-0.12342536  0.11760516]]. Action = [[ 0.0361028  -0.12715013  0.14011592  0.3747661 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 2400 is [True, False, False, False, True, False]
State prediction error at timestep 2400 is tensor(2.6117e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2401. State = [[-0.1221466   0.11449337]]. Action = [[-0.04183507 -0.22038077  0.12637976  0.08495331]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 2401 is [True, False, False, False, True, False]
State prediction error at timestep 2401 is tensor(3.7937e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2402. State = [[-0.1213601   0.10641433]]. Action = [[-0.06136306 -0.23932067 -0.21859442  0.8690195 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 2402 is [True, False, False, False, True, False]
State prediction error at timestep 2402 is tensor(5.1473e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2403. State = [[-0.12141241  0.09574769]]. Action = [[-0.19987582  0.23185602 -0.16452816 -0.5130424 ]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 2403 is [True, False, False, False, True, False]
State prediction error at timestep 2403 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2403 of -1
Current timestep = 2404. State = [[-0.1229921   0.09460578]]. Action = [[ 0.16373098 -0.1646798   0.24255002 -0.53555787]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 2404 is [True, False, False, False, True, False]
State prediction error at timestep 2404 is tensor(8.1724e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2405. State = [[-0.12149639  0.09067377]]. Action = [[-0.11124925 -0.02318995  0.14917055  0.00930285]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 2405 is [True, False, False, False, True, False]
State prediction error at timestep 2405 is tensor(4.2703e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2406. State = [[-0.12226793  0.08824902]]. Action = [[0.02045909 0.21562433 0.2128486  0.6787641 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 2406 is [True, False, False, False, True, False]
State prediction error at timestep 2406 is tensor(1.4763e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2407. State = [[-0.12322731  0.09077767]]. Action = [[-0.13489665 -0.02192922 -0.11887628 -0.1507073 ]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 2407 is [True, False, False, False, True, False]
State prediction error at timestep 2407 is tensor(5.8674e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2408. State = [[-0.12507874  0.09180156]]. Action = [[-0.06114353 -0.0288226  -0.17829596 -0.4617954 ]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 2408 is [True, False, False, False, True, False]
State prediction error at timestep 2408 is tensor(5.9322e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2408 of 1
Current timestep = 2409. State = [[-0.1289336   0.09090216]]. Action = [[-0.21458486 -0.06417972 -0.2233191  -0.33867323]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 2409 is [True, False, False, False, True, False]
State prediction error at timestep 2409 is tensor(3.7446e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2410. State = [[-0.13712123  0.08777543]]. Action = [[ 0.00783497 -0.1500552   0.0938839  -0.2984193 ]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 2410 is [True, False, False, False, True, False]
State prediction error at timestep 2410 is tensor(4.9043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2410 of 1
Current timestep = 2411. State = [[-0.14356631  0.08266881]]. Action = [[ 0.09020537  0.07035685  0.21108657 -0.77076626]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 2411 is [True, False, False, False, True, False]
State prediction error at timestep 2411 is tensor(8.5348e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2411 of 1
Current timestep = 2412. State = [[-0.14461952  0.08240151]]. Action = [[-0.02230738  0.22588533  0.04528189 -0.00507879]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 2412 is [True, False, False, False, True, False]
State prediction error at timestep 2412 is tensor(1.0747e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2413. State = [[-0.14633217  0.08781609]]. Action = [[-0.10955127 -0.03176597 -0.06145996  0.27362585]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 2413 is [True, False, False, False, True, False]
State prediction error at timestep 2413 is tensor(3.8004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2413 of 1
Current timestep = 2414. State = [[-0.14980046  0.09015132]]. Action = [[-0.09211248 -0.21010888  0.04401326 -0.0267992 ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 2414 is [True, False, False, False, True, False]
State prediction error at timestep 2414 is tensor(2.9535e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2414 of -1
Current timestep = 2415. State = [[-0.15529732  0.08542823]]. Action = [[-0.18261728  0.13436425 -0.07318139  0.26866102]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 2415 is [True, False, False, False, True, False]
State prediction error at timestep 2415 is tensor(9.2964e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2416. State = [[-0.16390026  0.08566375]]. Action = [[-0.14798442 -0.13346547  0.22869632 -0.07102329]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 2416 is [True, False, False, False, True, False]
State prediction error at timestep 2416 is tensor(3.7924e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2417. State = [[-0.17213973  0.08222029]]. Action = [[-0.0276594  -0.24479377  0.19329828 -0.63853264]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 2417 is [True, False, False, False, True, False]
State prediction error at timestep 2417 is tensor(3.7111e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2417 of -1
Current timestep = 2418. State = [[-0.17995332  0.07176684]]. Action = [[-0.01680167 -0.2291903  -0.11207607  0.71099687]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 2418 is [True, False, False, False, True, False]
State prediction error at timestep 2418 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2418 of -1
Current timestep = 2419. State = [[-0.18569842  0.06009709]]. Action = [[-0.1267464   0.20055404 -0.11976182  0.23874617]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 2419 is [True, False, False, False, True, False]
State prediction error at timestep 2419 is tensor(8.6484e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2419 of -1
Current timestep = 2420. State = [[-0.1915187   0.05795104]]. Action = [[-0.15292792  0.09147209 -0.04983394 -0.09489095]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 2420 is [True, False, False, False, True, False]
State prediction error at timestep 2420 is tensor(2.1331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2420 of -1
Current timestep = 2421. State = [[-0.19879965  0.06021663]]. Action = [[-0.19762138  0.02300334  0.07496977 -0.66047883]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 2421 is [True, False, False, False, True, False]
State prediction error at timestep 2421 is tensor(5.1444e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2421 of -1
Current timestep = 2422. State = [[-0.20883326  0.06335562]]. Action = [[-0.06715037  0.1577026  -0.01995288  0.80112386]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 2422 is [True, False, False, False, True, False]
State prediction error at timestep 2422 is tensor(3.3718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2423. State = [[-0.21760696  0.06905214]]. Action = [[ 0.18730041 -0.07724127  0.2094959   0.84177864]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 2423 is [True, False, False, False, True, False]
State prediction error at timestep 2423 is tensor(5.1707e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2423 of -1
Current timestep = 2424. State = [[-0.21984838  0.07069296]]. Action = [[0.1956214  0.20347095 0.0857611  0.13535762]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 2424 is [True, False, False, False, True, False]
State prediction error at timestep 2424 is tensor(4.5920e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2424 of -1
Current timestep = 2425. State = [[-0.21773098  0.07517681]]. Action = [[-0.00916968 -0.24040641  0.20234615 -0.3658427 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 2425 is [True, False, False, False, True, False]
State prediction error at timestep 2425 is tensor(6.0978e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2425 of -1
Current timestep = 2426. State = [[-0.21647434  0.07240789]]. Action = [[-0.08938912 -0.14128958 -0.03887169 -0.5632175 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 2426 is [True, False, False, False, True, False]
State prediction error at timestep 2426 is tensor(9.0593e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2426 of -1
Current timestep = 2427. State = [[-0.21790376  0.06668513]]. Action = [[-0.13384104  0.0228202   0.11089781  0.10999906]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 2427 is [True, False, False, False, True, False]
State prediction error at timestep 2427 is tensor(1.6589e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2427 of -1
Current timestep = 2428. State = [[-0.22068359  0.06331243]]. Action = [[-0.03043605  0.17923129  0.03519577 -0.7652288 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 2428 is [True, False, False, False, True, False]
State prediction error at timestep 2428 is tensor(2.2281e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2428 of -1
Current timestep = 2429. State = [[-0.22306408  0.066212  ]]. Action = [[-0.24074534 -0.22280978  0.23630995 -0.80879784]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 2429 is [True, False, False, False, True, False]
State prediction error at timestep 2429 is tensor(1.5281e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2430. State = [[-0.2305989   0.06191377]]. Action = [[-0.04146963 -0.1116637  -0.20250884  0.75472975]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 2430 is [True, False, False, False, True, False]
State prediction error at timestep 2430 is tensor(3.8637e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2431. State = [[-0.23723914  0.05485721]]. Action = [[-0.15528691 -0.1895875   0.19011527  0.10626245]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 2431 is [True, False, False, False, True, False]
State prediction error at timestep 2431 is tensor(4.1098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2432. State = [[-0.2458556   0.04541938]]. Action = [[ 0.22919813  0.20491552 -0.20990536 -0.5436736 ]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 2432 is [True, False, False, False, True, False]
State prediction error at timestep 2432 is tensor(8.2839e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2433. State = [[-0.24518771  0.04625954]]. Action = [[-0.19891766  0.07703704  0.15627414 -0.20952833]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 2433 is [True, False, False, False, True, False]
State prediction error at timestep 2433 is tensor(2.3726e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2433 of -1
Current timestep = 2434. State = [[-0.24749844  0.04995658]]. Action = [[ 0.20751852 -0.0291452  -0.03288653  0.75245285]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 2434 is [True, False, False, False, True, False]
State prediction error at timestep 2434 is tensor(1.5223e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2434 of -1
Current timestep = 2435. State = [[-0.24765848  0.05139465]]. Action = [[-0.03266034  0.1682126  -0.05369139 -0.37290275]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 2435 is [True, False, False, False, True, False]
State prediction error at timestep 2435 is tensor(1.2725e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2435 of -1
Current timestep = 2436. State = [[-0.24836762  0.05582223]]. Action = [[-0.24475208  0.22162306  0.21539962  0.53846824]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 2436 is [True, False, False, False, True, False]
State prediction error at timestep 2436 is tensor(2.5418e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2436 of -1
Current timestep = 2437. State = [[-0.25326258  0.06535587]]. Action = [[ 0.11616775  0.03352818 -0.13385391 -0.0255717 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 2437 is [True, False, False, False, True, False]
State prediction error at timestep 2437 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2438. State = [[-0.2558451   0.07193191]]. Action = [[ 0.05028298  0.24198711 -0.15386532  0.1011833 ]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 2438 is [True, False, False, False, True, False]
State prediction error at timestep 2438 is tensor(5.9129e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2439. State = [[-0.25672418  0.08294779]]. Action = [[-0.1354483  -0.13970912 -0.11570892  0.57975507]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 2439 is [True, False, False, False, True, False]
State prediction error at timestep 2439 is tensor(4.3178e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2439 of -1
Current timestep = 2440. State = [[-0.2592656   0.08582165]]. Action = [[ 0.04033503 -0.21088535  0.15887839  0.32698274]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 2440 is [True, False, False, False, True, False]
State prediction error at timestep 2440 is tensor(1.1432e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2440 of -1
Current timestep = 2441. State = [[-0.26113272  0.08141787]]. Action = [[ 0.18745548  0.22292864 -0.24688597 -0.51250994]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 2441 is [True, False, False, False, True, False]
State prediction error at timestep 2441 is tensor(1.4774e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2441 of -1
Current timestep = 2442. State = [[-0.25777724  0.08374058]]. Action = [[ 0.21689358 -0.20532447 -0.12180763 -0.9257417 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 2442 is [True, False, False, False, True, False]
State prediction error at timestep 2442 is tensor(1.2186e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2442 of -1
Current timestep = 2443. State = [[-0.25172627  0.08054624]]. Action = [[ 0.16418096 -0.18842652  0.11833692 -0.24943697]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 2443 is [True, False, False, False, True, False]
State prediction error at timestep 2443 is tensor(1.4071e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2444. State = [[-0.2454346   0.07309388]]. Action = [[-0.21565805 -0.11334088 -0.2096256   0.9142461 ]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 2444 is [True, False, False, False, True, False]
State prediction error at timestep 2444 is tensor(4.7592e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2445. State = [[-0.24242151  0.0634931 ]]. Action = [[ 0.20042092 -0.2159993  -0.1845484   0.5689392 ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 2445 is [True, False, False, False, True, False]
State prediction error at timestep 2445 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2445 of -1
Current timestep = 2446. State = [[-0.23819953  0.05400473]]. Action = [[-0.06816339  0.19338018  0.0568555   0.70872927]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 2446 is [True, False, False, False, True, False]
State prediction error at timestep 2446 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2446 of -1
Current timestep = 2447. State = [[-0.23489422  0.05243293]]. Action = [[ 0.19601879 -0.01959898 -0.19546145  0.6404588 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 2447 is [True, False, False, False, True, False]
State prediction error at timestep 2447 is tensor(2.7298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2447 of -1
Current timestep = 2448. State = [[-0.23049869  0.05337262]]. Action = [[-0.168239    0.11596522 -0.0313998  -0.18513358]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 2448 is [True, False, False, False, True, False]
State prediction error at timestep 2448 is tensor(3.5247e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2449. State = [[-0.23063229  0.0548997 ]]. Action = [[-0.10479593 -0.20031247  0.06137761  0.42959738]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 2449 is [True, False, False, False, True, False]
State prediction error at timestep 2449 is tensor(5.3982e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2449 of -1
Current timestep = 2450. State = [[-0.23258646  0.050844  ]]. Action = [[-0.19840878  0.14189008  0.14121151  0.9254695 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 2450 is [True, False, False, False, True, False]
State prediction error at timestep 2450 is tensor(1.4849e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2450 of -1
Current timestep = 2451. State = [[-0.23588139  0.05193756]]. Action = [[0.06223887 0.14767146 0.00874811 0.29973125]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 2451 is [True, False, False, False, True, False]
State prediction error at timestep 2451 is tensor(4.8580e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2451 of -1
Current timestep = 2452. State = [[-0.23730233  0.05683288]]. Action = [[ 0.08874857 -0.16804717 -0.17577475  0.03244054]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 2452 is [True, False, False, False, True, False]
State prediction error at timestep 2452 is tensor(2.9074e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2453. State = [[-0.2373334   0.05582376]]. Action = [[ 0.01408738 -0.14231995  0.09032974  0.63803804]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 2453 is [True, False, False, False, True, False]
State prediction error at timestep 2453 is tensor(1.1673e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2453 of -1
Current timestep = 2454. State = [[-0.2377449   0.05100508]]. Action = [[-0.05554627  0.04133439  0.21183646  0.9421847 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 2454 is [True, False, False, False, True, False]
State prediction error at timestep 2454 is tensor(1.8901e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2454 of -1
Current timestep = 2455. State = [[-0.23852995  0.04744703]]. Action = [[-0.09056079 -0.13873503 -0.20647307 -0.04217154]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 2455 is [True, False, False, False, True, False]
State prediction error at timestep 2455 is tensor(6.5689e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2455 of -1
Current timestep = 2456. State = [[-0.24034426  0.04289432]]. Action = [[-0.0676485   0.106507   -0.22958831 -0.12530088]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 2456 is [True, False, False, False, True, False]
State prediction error at timestep 2456 is tensor(2.2565e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2456 of -1
Current timestep = 2457. State = [[-0.24165829  0.0440147 ]]. Action = [[ 0.21887231  0.2195391  -0.1896542  -0.00704288]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 2457 is [True, False, False, False, True, False]
State prediction error at timestep 2457 is tensor(9.3833e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2458. State = [[-0.23984317  0.04995708]]. Action = [[ 0.09658659 -0.16022484  0.05630687  0.873338  ]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 2458 is [True, False, False, False, True, False]
State prediction error at timestep 2458 is tensor(2.5582e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2459. State = [[-0.23790833  0.0503013 ]]. Action = [[ 0.07406488  0.20054984  0.17630762 -0.16524881]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 2459 is [True, False, False, False, True, False]
State prediction error at timestep 2459 is tensor(3.0558e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2459 of -1
Current timestep = 2460. State = [[-0.23450248  0.05412907]]. Action = [[-0.20675768 -0.22158414 -0.03263769  0.24484468]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 2460 is [True, False, False, False, True, False]
State prediction error at timestep 2460 is tensor(1.8339e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2460 of -1
Current timestep = 2461. State = [[-0.23548211  0.05092013]]. Action = [[-0.14931895  0.01610374  0.03953674 -0.11965346]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 2461 is [True, False, False, False, True, False]
State prediction error at timestep 2461 is tensor(5.9213e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2461 of -1
Current timestep = 2462. State = [[-0.23955275  0.04805091]]. Action = [[0.10669878 0.01315537 0.13794172 0.23197615]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 2462 is [True, False, False, False, True, False]
State prediction error at timestep 2462 is tensor(1.6408e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2462 of -1
Current timestep = 2463. State = [[-0.23982565  0.04832731]]. Action = [[-0.14816023 -0.07676539  0.01933217 -0.796025  ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 2463 is [True, False, False, False, True, False]
State prediction error at timestep 2463 is tensor(1.9729e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2464. State = [[-0.2424057   0.04756937]]. Action = [[-0.02801648 -0.01324736 -0.05214171  0.5414655 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 2464 is [True, False, False, False, True, False]
State prediction error at timestep 2464 is tensor(1.1714e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2465. State = [[-0.24444383  0.0450572 ]]. Action = [[ 0.11935923 -0.00706398 -0.05977911 -0.84792906]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 2465 is [True, False, False, False, True, False]
State prediction error at timestep 2465 is tensor(1.3144e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2465 of -1
Current timestep = 2466. State = [[-0.24330017  0.04282439]]. Action = [[-0.0177249  -0.20497634 -0.20538987  0.3730632 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 2466 is [True, False, False, False, True, False]
State prediction error at timestep 2466 is tensor(3.9515e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2466 of -1
Current timestep = 2467. State = [[-0.2429205   0.03755105]]. Action = [[-0.23104554  0.23044872  0.15664232 -0.5805667 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 2467 is [True, False, False, False, True, False]
State prediction error at timestep 2467 is tensor(3.8163e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2467 of -1
Current timestep = 2468. State = [[-0.24691194  0.03996544]]. Action = [[ 0.00252748 -0.01971516 -0.23660989 -0.05760324]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 2468 is [True, False, False, False, True, False]
State prediction error at timestep 2468 is tensor(5.8405e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2468 of -1
Current timestep = 2469. State = [[-0.2511909   0.04101589]]. Action = [[ 8.9257956e-05 -5.3063542e-02  2.2794086e-01  9.0321326e-01]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 2469 is [True, False, False, False, True, False]
State prediction error at timestep 2469 is tensor(1.9924e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2470. State = [[-0.2550118   0.03906657]]. Action = [[-0.16509652 -0.14455555 -0.24149942 -0.12877917]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 2470 is [True, False, False, False, True, False]
State prediction error at timestep 2470 is tensor(1.3601e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2471. State = [[-0.26137665  0.03282386]]. Action = [[-0.02667022 -0.05067846 -0.13599968 -0.63712764]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 2471 is [True, False, False, False, True, False]
State prediction error at timestep 2471 is tensor(5.5914e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2472. State = [[-0.26517364  0.02879584]]. Action = [[ 0.09302738  0.19592625  0.02192989 -0.7715748 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 2472 is [True, False, False, False, True, False]
State prediction error at timestep 2472 is tensor(4.1033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2473. State = [[-0.26569507  0.0326077 ]]. Action = [[-0.12995066  0.17147842  0.13974184  0.0956409 ]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 2473 is [True, False, False, False, True, False]
State prediction error at timestep 2473 is tensor(2.1020e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2473 of -1
Current timestep = 2474. State = [[-0.26938704  0.04010799]]. Action = [[-0.17480184 -0.15748443 -0.21804309 -0.16718823]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 2474 is [True, False, False, False, True, False]
State prediction error at timestep 2474 is tensor(6.6804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2474 of -1
Current timestep = 2475. State = [[-0.27649015  0.03893062]]. Action = [[-0.18457793  0.24328387 -0.08718845  0.71942496]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 2475 is [True, False, False, False, True, False]
State prediction error at timestep 2475 is tensor(2.1677e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2475 of -1
Current timestep = 2476. State = [[-0.2846838   0.04532686]]. Action = [[ 0.19452333  0.14689219 -0.19295247 -0.77271134]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 2476 is [True, False, False, False, True, False]
State prediction error at timestep 2476 is tensor(7.0947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2477. State = [[-0.28848466  0.05509951]]. Action = [[-0.1924379   0.06870863 -0.03433286 -0.57632643]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 2477 is [True, False, False, False, True, False]
State prediction error at timestep 2477 is tensor(3.8630e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2478. State = [[-0.29345956  0.06460018]]. Action = [[ 0.15544909  0.13815579 -0.22437339 -0.61536145]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 2478 is [True, False, False, False, True, False]
State prediction error at timestep 2478 is tensor(6.4774e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2478 of -1
Current timestep = 2479. State = [[-0.29627225  0.07206076]]. Action = [[-0.16677909 -0.0175402  -0.19066803  0.5518787 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 2479 is [True, False, False, False, True, False]
State prediction error at timestep 2479 is tensor(1.3785e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2479 of -1
Current timestep = 2480. State = [[-0.29991156  0.07820644]]. Action = [[-0.12080513 -0.108417    0.2414932   0.4494002 ]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 2480 is [True, False, False, False, True, False]
State prediction error at timestep 2480 is tensor(2.0743e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2480 of -1
Current timestep = 2481. State = [[-0.3050038   0.08412624]]. Action = [[-0.0444773  -0.09448196  0.0499948  -0.25841516]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 2481 is [True, False, False, False, True, False]
State prediction error at timestep 2481 is tensor(5.6144e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2482. State = [[-0.31001985  0.08994234]]. Action = [[-0.0223397   0.13064575  0.04432207  0.85985446]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 2482 is [True, False, False, False, True, False]
State prediction error at timestep 2482 is tensor(1.0247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2483. State = [[-0.3139208   0.09574128]]. Action = [[ 0.04925534  0.2398082  -0.21699198 -0.570189  ]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 2483 is [True, False, False, False, True, False]
State prediction error at timestep 2483 is tensor(2.3627e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2484. State = [[-0.31656983  0.10005195]]. Action = [[ 0.08681041  0.10992992  0.0909805  -0.82149345]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 2484 is [True, False, False, False, True, False]
State prediction error at timestep 2484 is tensor(1.2270e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2484 of -1
Current timestep = 2485. State = [[-0.3167086   0.10174917]]. Action = [[-0.24240918  0.12955362 -0.02174439  0.18999124]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 2485 is [True, False, False, False, True, False]
State prediction error at timestep 2485 is tensor(2.5146e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2485 of -1
Current timestep = 2486. State = [[-0.31826583  0.10350822]]. Action = [[ 0.0630936  -0.09161794  0.05551046  0.81122756]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 2486 is [True, False, False, False, True, False]
State prediction error at timestep 2486 is tensor(8.2100e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2486 of -1
Current timestep = 2487. State = [[-0.31991473  0.10602465]]. Action = [[-0.02561006 -0.2125079   0.15938878  0.40630984]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 2487 is [True, False, False, False, True, False]
State prediction error at timestep 2487 is tensor(7.7187e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2487 of -1
Current timestep = 2488. State = [[-0.32201776  0.10821085]]. Action = [[-0.12929721 -0.01897231 -0.16635272 -0.50416714]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 2488 is [True, False, False, False, True, False]
State prediction error at timestep 2488 is tensor(1.3340e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2488 of -1
Current timestep = 2489. State = [[-0.32698947  0.10905781]]. Action = [[-0.20982751  0.17008716 -0.15665163 -0.11450124]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 2489 is [True, False, False, False, True, False]
State prediction error at timestep 2489 is tensor(9.4483e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2490. State = [[-0.3346076   0.10989565]]. Action = [[ 0.10281149 -0.07048109  0.19633031  0.7995658 ]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 2490 is [True, False, False, False, True, False]
State prediction error at timestep 2490 is tensor(1.3618e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2491. State = [[-0.33812654  0.11458297]]. Action = [[ 0.1865713   0.10520244 -0.13783474 -0.59024787]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 2491 is [True, False, False, False, True, False]
State prediction error at timestep 2491 is tensor(3.5514e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2491 of -1
Current timestep = 2492. State = [[-0.33818752  0.1174414 ]]. Action = [[ 0.03639591 -0.23938969 -0.21669629 -0.06945777]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 2492 is [True, False, False, False, True, False]
State prediction error at timestep 2492 is tensor(3.7927e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2492 of -1
Current timestep = 2493. State = [[-0.338181   0.1173013]]. Action = [[ 0.04761779  0.03874877 -0.17338279  0.5223403 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 2493 is [True, False, False, False, True, False]
State prediction error at timestep 2493 is tensor(1.2450e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2493 of -1
Current timestep = 2494. State = [[-0.33786744  0.11776312]]. Action = [[-0.05788517  0.19325227  0.11207545  0.8199465 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 2494 is [True, False, False, False, True, False]
State prediction error at timestep 2494 is tensor(1.2511e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2494 of -1
Current timestep = 2495. State = [[-0.3376873   0.11806942]]. Action = [[-0.06033859 -0.07976487 -0.07353501 -0.0542658 ]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 2495 is [True, False, False, False, True, False]
State prediction error at timestep 2495 is tensor(3.3920e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2495 of -1
Current timestep = 2496. State = [[-0.33801278  0.11784356]]. Action = [[-0.21026331 -0.04690385  0.03833359 -0.37038124]]. Reward = [0.]
Curr episode timestep = 692
Current timestep = 2497. State = [[-0.3410327   0.11650654]]. Action = [[-0.12222056 -0.04413523 -0.02935036  0.92413425]]. Reward = [0.]
Curr episode timestep = 693
Current timestep = 2498. State = [[-0.34487048  0.11609729]]. Action = [[ 0.22959688 -0.14095755 -0.02156506  0.6687025 ]]. Reward = [0.]
Curr episode timestep = 694
Human Feedback received at timestep 2498 of -1
Current timestep = 2499. State = [[-0.34604076  0.11762692]]. Action = [[ 0.14645359  0.09054363 -0.09951344 -0.08285105]]. Reward = [0.]
Curr episode timestep = 695
Human Feedback received at timestep 2499 of -1
Current timestep = 2500. State = [[-0.34499654  0.11877014]]. Action = [[-0.15391904  0.14705265  0.22339275  0.54758716]]. Reward = [0.]
Curr episode timestep = 696
Current timestep = 2501. State = [[-0.345176    0.11889181]]. Action = [[-0.01718329  0.11239806  0.02255616  0.2985245 ]]. Reward = [0.]
Curr episode timestep = 697
Current timestep = 2502. State = [[-0.34536928  0.11897371]]. Action = [[-0.2144835   0.03460616  0.24905398 -0.0126726 ]]. Reward = [0.]
Curr episode timestep = 698
Human Feedback received at timestep 2502 of -1
Current timestep = 2503. State = [[-0.34870595  0.12079042]]. Action = [[ 0.22998536  0.14101556  0.07758692 -0.6942197 ]]. Reward = [0.]
Curr episode timestep = 699
Current timestep = 2504. State = [[-0.34774944  0.1229039 ]]. Action = [[0.0626705  0.12984115 0.11805516 0.30881894]]. Reward = [0.]
Curr episode timestep = 700
Current timestep = 2505. State = [[-0.34560722  0.1258502 ]]. Action = [[ 0.23844564  0.22807366  0.04761264 -0.8677937 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 2505 is [True, False, False, False, False, True]
State prediction error at timestep 2505 is tensor(2.1179e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2506. State = [[-0.33927318  0.13261199]]. Action = [[ 0.0306825   0.2221578  -0.04330939  0.82010484]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 2506 is [True, False, False, False, False, True]
State prediction error at timestep 2506 is tensor(3.4724e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2507. State = [[-0.3332034   0.14222777]]. Action = [[-0.22083287  0.13778004 -0.19515279  0.0199759 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 2507 is [True, False, False, False, False, True]
State prediction error at timestep 2507 is tensor(6.2443e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2508. State = [[-0.33358392  0.15197088]]. Action = [[-0.15086485  0.15320829 -0.05523193  0.6254027 ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 2508 is [True, False, False, False, False, True]
State prediction error at timestep 2508 is tensor(2.0226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2509. State = [[-0.3385647   0.16198416]]. Action = [[-0.23073742  0.18670568 -0.19483073 -0.5711738 ]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 2509 is [True, False, False, False, False, True]
State prediction error at timestep 2509 is tensor(8.3547e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2510. State = [[-0.34616435  0.17318416]]. Action = [[-0.19072537  0.11647576 -0.08496995  0.912318  ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 2510 is [True, False, False, False, False, True]
State prediction error at timestep 2510 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2511. State = [[-0.35641417  0.18562151]]. Action = [[-0.12581316  0.13994819  0.14899272  0.12353706]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 2511 is [True, False, False, False, False, True]
State prediction error at timestep 2511 is tensor(8.9586e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2511 of -1
Current timestep = 2512. State = [[-0.36767507  0.19824946]]. Action = [[ 0.01012826  0.07299864  0.08700365 -0.954183  ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 2512 is [True, False, False, False, False, True]
State prediction error at timestep 2512 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 2513. State = [[-0.37450004  0.2055515 ]]. Action = [[ 0.14512676 -0.11739494  0.24228328  0.46454644]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 2513 is [True, False, False, False, False, True]
State prediction error at timestep 2513 is tensor(3.6702e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2514. State = [[-0.3765012   0.20729394]]. Action = [[-0.1509754   0.05072212  0.22459877  0.6667483 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 2514 is [True, False, False, False, False, True]
State prediction error at timestep 2514 is tensor(1.5198e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2515. State = [[-0.3787797  0.2088593]]. Action = [[-0.11668611 -0.03975949  0.05767447  0.15170515]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 2515 is [True, False, False, False, False, True]
State prediction error at timestep 2515 is tensor(4.3197e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2515 of -1
Current timestep = 2516. State = [[-0.38246566  0.21010076]]. Action = [[-0.1074926  -0.05370758 -0.15239504  0.85773194]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 2516 is [True, False, False, False, False, True]
State prediction error at timestep 2516 is tensor(2.7078e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2516 of -1
Current timestep = 2517. State = [[-0.3875326   0.21050031]]. Action = [[-0.11637054 -0.17637786  0.20608449  0.7732954 ]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 2517 is [True, False, False, False, False, True]
State prediction error at timestep 2517 is tensor(1.4122e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2517 of -1
Current timestep = 2518. State = [[-0.3913271   0.21036287]]. Action = [[-0.1480338  -0.19548197 -0.08487032 -0.80532527]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 2518 is [True, False, False, False, False, True]
State prediction error at timestep 2518 is tensor(1.7540e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2518 of -1
Current timestep = 2519. State = [[-0.3926151   0.20900375]]. Action = [[ 0.20083746 -0.22772375 -0.01884229 -0.16937923]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 2519 is [True, False, False, False, False, True]
State prediction error at timestep 2519 is tensor(3.6188e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2520. State = [[-0.39247438  0.20725976]]. Action = [[-0.10216695  0.15057987  0.11191654 -0.08721244]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 2520 is [True, False, False, False, False, True]
State prediction error at timestep 2520 is tensor(1.2934e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2521. State = [[-0.39254236  0.20750825]]. Action = [[-0.09933138  0.1598686  -0.09280665  0.5598068 ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 2521 is [True, False, False, False, False, True]
State prediction error at timestep 2521 is tensor(7.5434e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2521 of -1
Current timestep = 2522. State = [[-0.3929951   0.20722494]]. Action = [[ 0.06564593 -0.17705607  0.13548985 -0.6157322 ]]. Reward = [0.]
Curr episode timestep = 718
Human Feedback received at timestep 2522 of -1
Current timestep = 2523. State = [[-0.39328805  0.20710632]]. Action = [[-0.04824963 -0.20032915 -0.24139269  0.10237229]]. Reward = [0.]
Curr episode timestep = 719
Current timestep = 2524. State = [[-0.39355248  0.20695345]]. Action = [[-0.10745987 -0.2189051   0.02806759  0.9475398 ]]. Reward = [0.]
Curr episode timestep = 720
Current timestep = 2525. State = [[-0.3936475   0.20690536]]. Action = [[ 0.03868043  0.02627772  0.24216056 -0.74942803]]. Reward = [0.]
Curr episode timestep = 721
Human Feedback received at timestep 2525 of -1
Current timestep = 2526. State = [[-0.39373207  0.20690551]]. Action = [[ 0.07857224  0.1611206  -0.07847185  0.12786078]]. Reward = [0.]
Curr episode timestep = 722
Human Feedback received at timestep 2526 of -1
Current timestep = 2527. State = [[-0.3930848   0.20777887]]. Action = [[ 0.01028296 -0.04168679  0.0429998   0.77780604]]. Reward = [0.]
Curr episode timestep = 723
Human Feedback received at timestep 2527 of -1
Current timestep = 2528. State = [[-0.39185643  0.20628878]]. Action = [[-0.02592587  0.16922015 -0.22799669  0.44942558]]. Reward = [0.]
Curr episode timestep = 724
Current timestep = 2529. State = [[-0.39170882  0.20700474]]. Action = [[-0.1620248  -0.23360169 -0.17308114  0.37802887]]. Reward = [0.]
Curr episode timestep = 725
Current timestep = 2530. State = [[-0.39140636  0.20762946]]. Action = [[ 0.1272651  -0.1891883  -0.08407974 -0.494995  ]]. Reward = [0.]
Curr episode timestep = 726
Human Feedback received at timestep 2530 of -1
Current timestep = 2531. State = [[-0.39069247  0.2073892 ]]. Action = [[0.04836985 0.22407687 0.22910863 0.43990183]]. Reward = [0.]
Curr episode timestep = 727
Human Feedback received at timestep 2531 of -1
Current timestep = 2532. State = [[-0.38799897  0.20741838]]. Action = [[-4.1471422e-04  1.9389391e-02 -2.3463549e-01 -7.4888396e-01]]. Reward = [0.]
Curr episode timestep = 728
Human Feedback received at timestep 2532 of -1
Current timestep = 2533. State = [[-0.3850788   0.20601647]]. Action = [[-0.14590856 -0.10885903 -0.23426513 -0.40702105]]. Reward = [0.]
Curr episode timestep = 729
Current timestep = 2534. State = [[-0.38222072  0.2051649 ]]. Action = [[-0.1345653   0.19804996 -0.06556743 -0.6931225 ]]. Reward = [0.]
Curr episode timestep = 730
Current timestep = 2535. State = [[-0.37916943  0.2029859 ]]. Action = [[-0.17555617  0.08218241 -0.13940766 -0.02891546]]. Reward = [0.]
Curr episode timestep = 731
Human Feedback received at timestep 2535 of -1
Current timestep = 2536. State = [[-0.37802222  0.2024783 ]]. Action = [[ 0.00902289 -0.05368999 -0.22432302  0.7263727 ]]. Reward = [0.]
Curr episode timestep = 732
Human Feedback received at timestep 2536 of -1
Current timestep = 2537. State = [[-0.37740132  0.20180637]]. Action = [[ 0.05552381  0.17192024 -0.11102501 -0.60660976]]. Reward = [0.]
Curr episode timestep = 733
Current timestep = 2538. State = [[-0.37452593  0.20179568]]. Action = [[ 0.03625345  0.2028768  -0.17160067  0.00955153]]. Reward = [0.]
Curr episode timestep = 734
Current timestep = 2539. State = [[-0.37080112  0.20522122]]. Action = [[-0.0372338  -0.20407973  0.17952931  0.75864863]]. Reward = [0.]
Curr episode timestep = 735
Human Feedback received at timestep 2539 of -1
Current timestep = 2540. State = [[-0.3684825   0.20348184]]. Action = [[ 0.03881204 -0.00337942 -0.05414224  0.15874934]]. Reward = [0.]
Curr episode timestep = 736
Current timestep = 2541. State = [[-0.36633363  0.20210938]]. Action = [[ 0.18117326  0.07962632 -0.0565014  -0.58548445]]. Reward = [0.]
Curr episode timestep = 737
Current timestep = 2542. State = [[-0.36235687  0.2027528 ]]. Action = [[-0.12521203  0.00664085 -0.0337314   0.59639776]]. Reward = [0.]
Curr episode timestep = 738
Human Feedback received at timestep 2542 of -1
Current timestep = 2543. State = [[-0.36074463  0.20429835]]. Action = [[-0.06619599  0.22113478  0.10489696  0.26159048]]. Reward = [0.]
Curr episode timestep = 739
Current timestep = 2544. State = [[-0.36092365  0.20924951]]. Action = [[-0.01629119 -0.15428646 -0.12352268 -0.3953635 ]]. Reward = [0.]
Curr episode timestep = 740
Current timestep = 2545. State = [[-0.36062667  0.20975715]]. Action = [[ 0.1555019   0.11441261 -0.11034504  0.93162537]]. Reward = [0.]
Curr episode timestep = 741
Human Feedback received at timestep 2545 of -1
Current timestep = 2546. State = [[-0.3586826   0.21258411]]. Action = [[ 0.04557213  0.21534884 -0.18000107  0.665581  ]]. Reward = [0.]
Curr episode timestep = 742
Current timestep = 2547. State = [[-0.35665202  0.21969129]]. Action = [[ 0.07629427  0.00977021 -0.23955955 -0.1779669 ]]. Reward = [0.]
Curr episode timestep = 743
Current timestep = 2548. State = [[-0.35273474  0.22475846]]. Action = [[ 0.23787177 -0.22334261  0.23060739  0.4753027 ]]. Reward = [0.]
Curr episode timestep = 744
Current timestep = 2549. State = [[-0.3464684   0.22165866]]. Action = [[ 0.10412562 -0.0005278   0.17078817  0.44804096]]. Reward = [0.]
Curr episode timestep = 745
Current timestep = 2550. State = [[-0.33813196  0.22016564]]. Action = [[ 0.14101952 -0.24032104  0.05065519  0.92034054]]. Reward = [0.]
Curr episode timestep = 746
Current timestep = 2551. State = [[-0.33051434  0.213576  ]]. Action = [[-0.0982026   0.13641194  0.21804604 -0.527828  ]]. Reward = [0.]
Curr episode timestep = 747
Current timestep = 2552. State = [[-0.32450894  0.21216246]]. Action = [[ 0.1995107  -0.19195339  0.11742789  0.03311503]]. Reward = [0.]
Curr episode timestep = 748
Current timestep = 2553. State = [[-0.3183568   0.20682783]]. Action = [[-0.07321805 -0.14739993  0.17947584  0.45788097]]. Reward = [0.]
Curr episode timestep = 749
Current timestep = 2554. State = [[-0.31379685  0.20115921]]. Action = [[-0.06842721  0.23031741 -0.2103404   0.28520966]]. Reward = [0.]
Curr episode timestep = 750
Current timestep = 2555. State = [[-0.31239364  0.20302169]]. Action = [[ 0.10830212  0.22968912 -0.22470234 -0.07834995]]. Reward = [0.]
Curr episode timestep = 751
Current timestep = 2556. State = [[-0.31004304  0.20855056]]. Action = [[-0.22793372  0.10839987 -0.01867269 -0.713956  ]]. Reward = [0.]
Curr episode timestep = 752
Current timestep = 2557. State = [[-0.31160218  0.21547955]]. Action = [[-0.17250057  0.14380687 -0.10807061 -0.6323943 ]]. Reward = [0.]
Curr episode timestep = 753
Current timestep = 2558. State = [[-0.31625512  0.2235134 ]]. Action = [[-0.11348818  0.04694489 -0.10157552  0.03880358]]. Reward = [0.]
Curr episode timestep = 754
Current timestep = 2559. State = [[-0.32198343  0.23030162]]. Action = [[0.1853571  0.15984875 0.0414933  0.9893515 ]]. Reward = [0.]
Curr episode timestep = 755
Current timestep = 2560. State = [[-0.3246361   0.23795481]]. Action = [[ 0.12631494 -0.1540307  -0.24246264 -0.49375546]]. Reward = [0.]
Curr episode timestep = 756
Current timestep = 2561. State = [[-0.32353944  0.23921439]]. Action = [[ 0.14633596  0.09955776 -0.09167378  0.7106433 ]]. Reward = [0.]
Curr episode timestep = 757
Current timestep = 2562. State = [[-0.31904003  0.24214417]]. Action = [[ 0.12292957 -0.20869634 -0.03573126 -0.22493243]]. Reward = [0.]
Curr episode timestep = 758
Current timestep = 2563. State = [[-0.31228513  0.23880929]]. Action = [[ 0.13502088 -0.11300683 -0.16346094  0.10548532]]. Reward = [0.]
Curr episode timestep = 759
Current timestep = 2564. State = [[-0.30511546  0.23476823]]. Action = [[ 0.11767977  0.16392422 -0.21590652  0.75882196]]. Reward = [0.]
Curr episode timestep = 760
Current timestep = 2565. State = [[-0.29672614  0.23519091]]. Action = [[ 0.04790655 -0.20345412  0.12070262 -0.60141927]]. Reward = [0.]
Curr episode timestep = 761
Current timestep = 2566. State = [[-0.2906046   0.23190524]]. Action = [[-0.13702895  0.21672341 -0.16523568 -0.00747949]]. Reward = [0.]
Curr episode timestep = 762
Current timestep = 2567. State = [[-0.28862774  0.23459916]]. Action = [[-0.22399813  0.05003297  0.1839337   0.93616796]]. Reward = [0.]
Curr episode timestep = 763
Current timestep = 2568. State = [[-0.29161885  0.23757978]]. Action = [[-0.12112214 -0.12349808  0.08110395  0.7379172 ]]. Reward = [0.]
Curr episode timestep = 764
Current timestep = 2569. State = [[-0.29485857  0.23811485]]. Action = [[-0.02801649  0.20027304  0.16805536 -0.10575664]]. Reward = [0.]
Curr episode timestep = 765
Current timestep = 2570. State = [[-0.2989253   0.24269676]]. Action = [[ 0.10378665  0.10689205  0.22935909 -0.4445734 ]]. Reward = [0.]
Curr episode timestep = 766
Current timestep = 2571. State = [[-0.30044004  0.24641429]]. Action = [[ 0.03318152 -0.21772453  0.12894261  0.47427917]]. Reward = [0.]
Curr episode timestep = 767
Current timestep = 2572. State = [[-0.2998564   0.24581668]]. Action = [[0.11659214 0.13793567 0.19373542 0.19659758]]. Reward = [0.]
Curr episode timestep = 768
Current timestep = 2573. State = [[-0.29832047  0.24651405]]. Action = [[ 0.01194927 -0.21309732  0.12533826  0.15154481]]. Reward = [0.]
Curr episode timestep = 769
Current timestep = 2574. State = [[-0.2959437   0.24348313]]. Action = [[ 0.02325469  0.05009949  0.08179331 -0.0048151 ]]. Reward = [0.]
Curr episode timestep = 770
Current timestep = 2575. State = [[-0.29451835  0.24199471]]. Action = [[ 0.23584801 -0.09042534  0.14701366  0.5802293 ]]. Reward = [0.]
Curr episode timestep = 771
Current timestep = 2576. State = [[-0.28964406  0.23808575]]. Action = [[-0.23133275 -0.15081218  0.14269337  0.49425936]]. Reward = [0.]
Curr episode timestep = 772
Current timestep = 2577. State = [[-0.2882235   0.23428173]]. Action = [[ 0.16185862  0.17675596 -0.12011012  0.70026195]]. Reward = [0.]
Curr episode timestep = 773
Current timestep = 2578. State = [[-0.28655994  0.23431033]]. Action = [[ 0.0923309  -0.01903178 -0.05556518 -0.41030145]]. Reward = [0.]
Curr episode timestep = 774
Current timestep = 2579. State = [[-0.28373817  0.23307092]]. Action = [[-0.23317164 -0.23452196  0.23169264  0.76835084]]. Reward = [0.]
Curr episode timestep = 775
Current timestep = 2580. State = [[-0.2838092   0.22850275]]. Action = [[ 0.21469384 -0.12334573  0.0984779   0.07647443]]. Reward = [0.]
Curr episode timestep = 776
Current timestep = 2581. State = [[-0.28199354  0.22333126]]. Action = [[-0.21325615  0.16552705 -0.0238097   0.37562788]]. Reward = [0.]
Curr episode timestep = 777
Current timestep = 2582. State = [[-0.28258818  0.22312   ]]. Action = [[-0.04643749  0.08045962  0.07650673  0.4073434 ]]. Reward = [0.]
Curr episode timestep = 778
Current timestep = 2583. State = [[-0.28412333  0.22503853]]. Action = [[ 0.02881134  0.16307709 -0.23551673  0.224787  ]]. Reward = [0.]
Curr episode timestep = 779
Current timestep = 2584. State = [[-0.28566647  0.22838795]]. Action = [[-0.18989345 -0.17604484  0.21574709  0.33957684]]. Reward = [0.]
Curr episode timestep = 780
Current timestep = 2585. State = [[-0.2877117   0.22782484]]. Action = [[ 0.1925993  -0.21379395 -0.19229244 -0.74031866]]. Reward = [0.]
Curr episode timestep = 781
Current timestep = 2586. State = [[-0.2871041   0.22312967]]. Action = [[-0.0071716   0.19863611 -0.03737178  0.06188655]]. Reward = [0.]
Curr episode timestep = 782
Current timestep = 2587. State = [[-0.28735876  0.22377063]]. Action = [[ 0.03773493  0.23518789 -0.21221381 -0.7606746 ]]. Reward = [0.]
Curr episode timestep = 783
Current timestep = 2588. State = [[-0.28766373  0.2294236 ]]. Action = [[-0.03599086  0.2088621   0.02413189 -0.94686556]]. Reward = [0.]
Curr episode timestep = 784
Current timestep = 2589. State = [[-0.28804436  0.2384312 ]]. Action = [[-0.08134991  0.16622087  0.0480099  -0.79137236]]. Reward = [0.]
Curr episode timestep = 785
Current timestep = 2590. State = [[-0.28915158  0.24814494]]. Action = [[-0.04027817 -0.03978279 -0.2329582   0.79478884]]. Reward = [0.]
Curr episode timestep = 786
Current timestep = 2591. State = [[-0.29167792  0.2540619 ]]. Action = [[ 0.05753267  0.10490733 -0.11034739  0.6036053 ]]. Reward = [0.]
Curr episode timestep = 787
Current timestep = 2592. State = [[-0.29366767  0.26056248]]. Action = [[-0.21113953  0.15400088  0.0889672   0.5785296 ]]. Reward = [0.]
Curr episode timestep = 788
Current timestep = 2593. State = [[-0.29774627  0.26856863]]. Action = [[-0.15409762  0.2070204  -0.12338954 -0.47578728]]. Reward = [0.]
Curr episode timestep = 789
Current timestep = 2594. State = [[-0.3034726  0.2789933]]. Action = [[ 0.12935212 -0.2415794  -0.08977859  0.29446602]]. Reward = [0.]
Curr episode timestep = 790
Current timestep = 2595. State = [[-0.3055942  0.2808893]]. Action = [[-0.15157747 -0.0314647  -0.22579196  0.58047783]]. Reward = [0.]
Curr episode timestep = 791
Current timestep = 2596. State = [[-0.30814427  0.2819131 ]]. Action = [[-0.1880051   0.02653462  0.1955421  -0.17545354]]. Reward = [0.]
Curr episode timestep = 792
Current timestep = 2597. State = [[-0.3145626  0.28443  ]]. Action = [[ 0.05227888  0.22512603 -0.1450606  -0.5968472 ]]. Reward = [0.]
Curr episode timestep = 793
Current timestep = 2598. State = [[-0.31971422  0.28990242]]. Action = [[-0.0996646   0.18942276 -0.14309649 -0.82053506]]. Reward = [0.]
Curr episode timestep = 794
Current timestep = 2599. State = [[-0.3254078   0.29759407]]. Action = [[ 0.23106349  0.10438898 -0.15508053 -0.2825178 ]]. Reward = [0.]
Curr episode timestep = 795
Current timestep = 2600. State = [[-0.326041    0.30519256]]. Action = [[ 0.23616666  0.21671396 -0.02971327  0.6726341 ]]. Reward = [0.]
Curr episode timestep = 796
Current timestep = 2601. State = [[-0.32283202  0.31429735]]. Action = [[ 0.02148828 -0.23760863  0.01823553 -0.7187385 ]]. Reward = [0.]
Curr episode timestep = 797
Current timestep = 2602. State = [[-0.31808367  0.3150365 ]]. Action = [[ 0.2266711   0.22600186  0.12043622 -0.33603156]]. Reward = [0.]
Curr episode timestep = 798
Current timestep = 2603. State = [[-0.31490183  0.31593356]]. Action = [[-0.21204379 -0.01639476 -0.22232711  0.2535423 ]]. Reward = [0.]
Curr episode timestep = 799
Current timestep = 2604. State = [[-0.31604287  0.31702918]]. Action = [[-0.24477084 -0.02024974 -0.05682394  0.88791597]]. Reward = [0.]
Curr episode timestep = 800
Current timestep = 2605. State = [[-0.32012475  0.3185946 ]]. Action = [[-0.04317762  0.20784795  0.11574557  0.54477954]]. Reward = [0.]
Curr episode timestep = 801
Current timestep = 2606. State = [[-0.32292226  0.3198316 ]]. Action = [[-0.15237874  0.22595221 -0.1838135  -0.4968322 ]]. Reward = [0.]
Curr episode timestep = 802
Current timestep = 2607. State = [[-0.32472903  0.32059643]]. Action = [[0.17017823 0.23374236 0.17713293 0.8587438 ]]. Reward = [0.]
Curr episode timestep = 803
Current timestep = 2608. State = [[-0.32601705  0.32083756]]. Action = [[0.10844386 0.19947684 0.2274645  0.09435523]]. Reward = [0.]
Curr episode timestep = 804
Current timestep = 2609. State = [[-0.32682857  0.32073388]]. Action = [[ 0.1240997  -0.1357937   0.05795884 -0.6373774 ]]. Reward = [0.]
Curr episode timestep = 805
Current timestep = 2610. State = [[-0.32602376  0.31688216]]. Action = [[-0.18944563  0.11844325 -0.13665734 -0.6506857 ]]. Reward = [0.]
Curr episode timestep = 806
Current timestep = 2611. State = [[-0.3254085  0.3147784]]. Action = [[ 0.04868922 -0.06094283  0.11431015 -0.40051568]]. Reward = [0.]
Curr episode timestep = 807
Current timestep = 2612. State = [[-0.3240181   0.31199217]]. Action = [[ 0.04839528 -0.02762149 -0.07120144  0.03319669]]. Reward = [0.]
Curr episode timestep = 808
Current timestep = 2613. State = [[-0.3220697   0.30936033]]. Action = [[ 0.00489455  0.03930563 -0.18193741  0.36996496]]. Reward = [0.]
Curr episode timestep = 809
Current timestep = 2614. State = [[-0.32081333  0.30812111]]. Action = [[-0.04060771  0.22601283  0.05707824  0.39692247]]. Reward = [0.]
Curr episode timestep = 810
Current timestep = 2615. State = [[-0.32001334  0.30704919]]. Action = [[ 0.14085302 -0.20756671 -0.13588315  0.09509063]]. Reward = [0.]
Curr episode timestep = 811
Current timestep = 2616. State = [[-0.3163186   0.30242565]]. Action = [[ 0.23683673  0.06810114 -0.20590295 -0.45536047]]. Reward = [0.]
Curr episode timestep = 812
Current timestep = 2617. State = [[-0.30965942  0.29907742]]. Action = [[ 0.16214842 -0.01627281 -0.19494797 -0.8310091 ]]. Reward = [0.]
Curr episode timestep = 813
Current timestep = 2618. State = [[-0.30224422  0.29649726]]. Action = [[-0.18994494  0.21641213 -0.06510697  0.41570938]]. Reward = [0.]
Curr episode timestep = 814
Current timestep = 2619. State = [[-0.30032313  0.29994303]]. Action = [[ 0.15916869  0.23247683 -0.22364196 -0.5477675 ]]. Reward = [0.]
Curr episode timestep = 815
Current timestep = 2620. State = [[-0.29743177  0.30752277]]. Action = [[ 0.13338238 -0.04499325 -0.12214032  0.04682338]]. Reward = [0.]
Curr episode timestep = 816
Current timestep = 2621. State = [[-0.2923971   0.31114796]]. Action = [[ 0.21615863  0.12464917  0.08663547 -0.7004628 ]]. Reward = [0.]
Curr episode timestep = 817
Current timestep = 2622. State = [[-0.2872534   0.31354082]]. Action = [[-0.07944322 -0.21317059 -0.18211553  0.22947395]]. Reward = [0.]
Curr episode timestep = 818
Current timestep = 2623. State = [[-0.284826    0.31099963]]. Action = [[0.14577258 0.117677   0.13428119 0.7757852 ]]. Reward = [0.]
Curr episode timestep = 819
Current timestep = 2624. State = [[-0.2836986  0.3097036]]. Action = [[ 0.03239116  0.10102552  0.15468344 -0.977437  ]]. Reward = [0.]
Curr episode timestep = 820
Current timestep = 2625. State = [[-0.28325108  0.30913046]]. Action = [[0.14697832 0.19391668 0.00219616 0.14459038]]. Reward = [0.]
Curr episode timestep = 821
Current timestep = 2626. State = [[-0.28316355  0.30862132]]. Action = [[-0.18736483 -0.05613078 -0.14900376 -0.82222456]]. Reward = [0.]
Curr episode timestep = 822
Current timestep = 2627. State = [[-0.28430226  0.30765742]]. Action = [[-0.20647287 -0.11846144 -0.1423131   0.8838401 ]]. Reward = [0.]
Curr episode timestep = 823
Current timestep = 2628. State = [[-0.28819337  0.30511263]]. Action = [[ 0.24053583  0.23052633  0.10279891 -0.7672821 ]]. Reward = [0.]
Curr episode timestep = 824
Current timestep = 2629. State = [[-0.2908875   0.30309275]]. Action = [[0.17114192 0.04198772 0.09173119 0.47065115]]. Reward = [0.]
Curr episode timestep = 825
Current timestep = 2630. State = [[-0.29059654  0.30133983]]. Action = [[ 0.22734189 -0.1572326   0.2357158  -0.6492471 ]]. Reward = [0.]
Curr episode timestep = 826
Current timestep = 2631. State = [[-0.28703755  0.29685187]]. Action = [[-0.00374387  0.20390594 -0.06152783  0.01720309]]. Reward = [0.]
Curr episode timestep = 827
Current timestep = 2632. State = [[-0.2852509   0.29690933]]. Action = [[ 0.20716286 -0.13997021 -0.16154245 -0.39715493]]. Reward = [0.]
Curr episode timestep = 828
Current timestep = 2633. State = [[-0.28025457  0.29342568]]. Action = [[ 0.07917854 -0.05226412 -0.1822624   0.8617153 ]]. Reward = [0.]
Curr episode timestep = 829
Current timestep = 2634. State = [[-0.2748354  0.2900682]]. Action = [[-0.11699943  0.0315865   0.14909261 -0.6258461 ]]. Reward = [0.]
Curr episode timestep = 830
Current timestep = 2635. State = [[-0.2722247  0.2895177]]. Action = [[0.19908261 0.1496957  0.06169352 0.8864844 ]]. Reward = [0.]
Curr episode timestep = 831
Current timestep = 2636. State = [[-0.26774484  0.29186746]]. Action = [[-0.03957282 -0.22900525  0.11083701 -0.93230426]]. Reward = [0.]
Curr episode timestep = 832
Current timestep = 2637. State = [[-0.26433903  0.2885524 ]]. Action = [[ 0.17328236  0.01057237  0.04031655 -0.5671487 ]]. Reward = [0.]
Curr episode timestep = 833
Current timestep = 2638. State = [[-0.25902665  0.2859041 ]]. Action = [[ 0.20391923 -0.15346465  0.14021325 -0.25595224]]. Reward = [0.]
Curr episode timestep = 834
Current timestep = 2639. State = [[-0.25239363  0.28039962]]. Action = [[-0.108224    0.06062534 -0.24820445  0.8831972 ]]. Reward = [0.]
Curr episode timestep = 835
Current timestep = 2640. State = [[-0.24753772  0.2779647 ]]. Action = [[ 0.12668705 -0.10269013  0.04880199  0.7592292 ]]. Reward = [0.]
Curr episode timestep = 836
Current timestep = 2641. State = [[-0.24205306  0.27339464]]. Action = [[ 0.07365093 -0.22625707  0.2447792   0.48247743]]. Reward = [0.]
Curr episode timestep = 837
Current timestep = 2642. State = [[-0.23643608  0.26626167]]. Action = [[ 0.0921686  -0.22727054  0.13055354  0.9219283 ]]. Reward = [0.]
Curr episode timestep = 838
Current timestep = 2643. State = [[-0.22996785  0.2562032 ]]. Action = [[ 0.14955294  0.00435165 -0.15226229  0.32182705]]. Reward = [0.]
Curr episode timestep = 839
Current timestep = 2644. State = [[-0.22208415  0.2479721 ]]. Action = [[ 0.21239269 -0.23192091  0.11114711 -0.97026926]]. Reward = [0.]
Curr episode timestep = 840
Current timestep = 2645. State = [[-0.2137697   0.23756887]]. Action = [[-0.24046929 -0.12853658 -0.23993567  0.83907247]]. Reward = [0.]
Curr episode timestep = 841
Current timestep = 2646. State = [[-0.20885052  0.22871588]]. Action = [[-0.08770457  0.10108986  0.06507877  0.8116709 ]]. Reward = [0.]
Curr episode timestep = 842
Current timestep = 2647. State = [[-0.20822619  0.22654289]]. Action = [[0.14676142 0.00485906 0.04877889 0.8684318 ]]. Reward = [0.]
Curr episode timestep = 843
Current timestep = 2648. State = [[-0.2060997   0.22366959]]. Action = [[ 0.19507915 -0.01163828 -0.21526903 -0.26808167]]. Reward = [0.]
Curr episode timestep = 844
Current timestep = 2649. State = [[-0.20204882  0.22146374]]. Action = [[-0.13589221  0.19844368  0.24118459 -0.96831685]]. Reward = [0.]
Curr episode timestep = 845
Current timestep = 2650. State = [[-0.20084693  0.2248535 ]]. Action = [[-0.0107902  -0.06914678 -0.16592564  0.06695855]]. Reward = [0.]
Curr episode timestep = 846
Current timestep = 2651. State = [[-0.20047954  0.22541073]]. Action = [[ 0.10549304  0.0075444  -0.10759158 -0.3417369 ]]. Reward = [0.]
Curr episode timestep = 847
Current timestep = 2652. State = [[-0.19907725  0.22611693]]. Action = [[ 0.09023196  0.08596617 -0.06144315  0.87403643]]. Reward = [0.]
Curr episode timestep = 848
Current timestep = 2653. State = [[-0.19601256  0.22800751]]. Action = [[ 0.08407074 -0.2024238   0.24555692 -0.5757424 ]]. Reward = [0.]
Curr episode timestep = 849
Current timestep = 2654. State = [[-0.19158497  0.22492592]]. Action = [[-0.03041124  0.19758153  0.0135738  -0.9352993 ]]. Reward = [0.]
Curr episode timestep = 850
Current timestep = 2655. State = [[-0.18874007  0.22792488]]. Action = [[0.0634166  0.10414174 0.00155589 0.8574772 ]]. Reward = [0.]
Curr episode timestep = 851
Current timestep = 2656. State = [[-0.1852812   0.23244922]]. Action = [[ 0.1133039   0.01273316 -0.23137748 -0.81080425]]. Reward = [0.]
Curr episode timestep = 852
Current timestep = 2657. State = [[-0.18170913  0.23528886]]. Action = [[-0.00182508  0.00492358  0.042431   -0.48011327]]. Reward = [0.]
Curr episode timestep = 853
Current timestep = 2658. State = [[-0.17841096  0.23787035]]. Action = [[ 0.12700865  0.04390407 -0.07167192  0.5603626 ]]. Reward = [0.]
Curr episode timestep = 854
Current timestep = 2659. State = [[-0.17391877  0.2408728 ]]. Action = [[-0.18771033  0.07217637 -0.1176993  -0.9385344 ]]. Reward = [0.]
Curr episode timestep = 855
Current timestep = 2660. State = [[-0.17245789  0.24438928]]. Action = [[-0.09305161 -0.04763217  0.15952173 -0.6986171 ]]. Reward = [0.]
Curr episode timestep = 856
Current timestep = 2661. State = [[-0.17319925  0.24568069]]. Action = [[-0.0125612  -0.08197382  0.04777601 -0.6212728 ]]. Reward = [0.]
Curr episode timestep = 857
Current timestep = 2662. State = [[-0.1731302   0.24549702]]. Action = [[-0.00724493  0.21941215  0.10250163  0.25890172]]. Reward = [0.]
Curr episode timestep = 858
Current timestep = 2663. State = [[-0.17437154  0.24910936]]. Action = [[ 0.06791407 -0.10830128  0.22004402  0.5482367 ]]. Reward = [0.]
Curr episode timestep = 859
Current timestep = 2664. State = [[-0.17410272  0.24887057]]. Action = [[-0.14539482 -0.08554207  0.185083    0.3882246 ]]. Reward = [0.]
Curr episode timestep = 860
Current timestep = 2665. State = [[-0.17467919  0.24751492]]. Action = [[-0.00425445 -0.13074248 -0.08025357 -0.2531947 ]]. Reward = [0.]
Curr episode timestep = 861
Current timestep = 2666. State = [[-0.1747271   0.24455617]]. Action = [[-0.03676683 -0.06889117 -0.23933324 -0.32044268]]. Reward = [0.]
Curr episode timestep = 862
Current timestep = 2667. State = [[-0.1753833   0.24109563]]. Action = [[-0.13381846 -0.03398278 -0.0251871  -0.3226347 ]]. Reward = [0.]
Curr episode timestep = 863
Current timestep = 2668. State = [[-0.1783995   0.23809576]]. Action = [[-0.20302115  0.24919093  0.18081951 -0.06386524]]. Reward = [0.]
Curr episode timestep = 864
Current timestep = 2669. State = [[-0.18424563  0.24148087]]. Action = [[ 0.13118446 -0.1728644  -0.00307555  0.08947885]]. Reward = [0.]
Curr episode timestep = 865
Current timestep = 2670. State = [[-0.18735021  0.23980181]]. Action = [[ 0.02855024  0.09742168 -0.19374211 -0.835899  ]]. Reward = [0.]
Curr episode timestep = 866
Current timestep = 2671. State = [[-0.18800966  0.24057361]]. Action = [[0.23144013 0.21412325 0.22176361 0.47711062]]. Reward = [0.]
Curr episode timestep = 867
Current timestep = 2672. State = [[-0.1864322   0.24477604]]. Action = [[-0.1615359  -0.06210807  0.20089507  0.19968104]]. Reward = [0.]
Curr episode timestep = 868
Current timestep = 2673. State = [[-0.18735775  0.24636757]]. Action = [[ 0.11311972 -0.09700853 -0.16327849  0.47635365]]. Reward = [0.]
Curr episode timestep = 869
Current timestep = 2674. State = [[-0.18645234  0.24574581]]. Action = [[ 0.10130543  0.12133497  0.16310918 -0.12604272]]. Reward = [0.]
Curr episode timestep = 870
Current timestep = 2675. State = [[-0.18535279  0.24705447]]. Action = [[-0.12960553 -0.07909426  0.19715825  0.5085728 ]]. Reward = [0.]
Curr episode timestep = 871
Current timestep = 2676. State = [[-0.18538454  0.24715135]]. Action = [[ 0.05625027 -0.11279833 -0.01948634  0.30899215]]. Reward = [0.]
Curr episode timestep = 872
Current timestep = 2677. State = [[-0.18410842  0.24417013]]. Action = [[-0.20498905 -0.20785682 -0.19374552  0.21527445]]. Reward = [0.]
Curr episode timestep = 873
Current timestep = 2678. State = [[-0.18618712  0.23840073]]. Action = [[-0.14844646 -0.07031059  0.23785377 -0.29835117]]. Reward = [0.]
Curr episode timestep = 874
Current timestep = 2679. State = [[-0.19059184  0.2314868 ]]. Action = [[ 0.18455082 -0.23913753 -0.21988358 -0.7747329 ]]. Reward = [0.]
Curr episode timestep = 875
Current timestep = 2680. State = [[-0.1911909   0.22193883]]. Action = [[ 0.00791109 -0.12123986  0.19663253 -0.5453951 ]]. Reward = [0.]
Curr episode timestep = 876
Current timestep = 2681. State = [[-0.1915635   0.21276619]]. Action = [[-0.23609099  0.21092185  0.16413337 -0.69020027]]. Reward = [0.]
Curr episode timestep = 877
Current timestep = 2682. State = [[-0.19451082  0.21213692]]. Action = [[ 0.02043045  0.01748499 -0.13273126  0.44352853]]. Reward = [0.]
Curr episode timestep = 878
Current timestep = 2683. State = [[-0.19638453  0.21145438]]. Action = [[ 0.07844943 -0.07804397  0.16922343 -0.9146454 ]]. Reward = [0.]
Curr episode timestep = 879
Current timestep = 2684. State = [[-0.19650878  0.20872727]]. Action = [[ 0.15990245 -0.19370602  0.00702375  0.44970357]]. Reward = [0.]
Curr episode timestep = 880
Current timestep = 2685. State = [[-0.19449656  0.20230883]]. Action = [[-0.21346352 -0.14376812  0.19786322  0.22541547]]. Reward = [0.]
Curr episode timestep = 881
Current timestep = 2686. State = [[-0.19576572  0.1961733 ]]. Action = [[-0.20332478  0.05050272 -0.00159186  0.9072938 ]]. Reward = [0.]
Curr episode timestep = 882
Current timestep = 2687. State = [[-0.20018187  0.19223905]]. Action = [[ 0.21478191 -0.16943789 -0.17916977 -0.11731261]]. Reward = [0.]
Curr episode timestep = 883
Current timestep = 2688. State = [[-0.20242958  0.18558078]]. Action = [[-0.09370309  0.12997738  0.07755291 -0.99061537]]. Reward = [0.]
Curr episode timestep = 884
Current timestep = 2689. State = [[-0.20375656  0.1838198 ]]. Action = [[-0.07024086 -0.12384731 -0.04975116  0.74596906]]. Reward = [0.]
Curr episode timestep = 885
Current timestep = 2690. State = [[-0.20646982  0.18034934]]. Action = [[-0.15420237  0.11276048 -0.0149008   0.01951087]]. Reward = [0.]
Curr episode timestep = 886
Current timestep = 2691. State = [[-0.21020886  0.18126138]]. Action = [[-0.1958983   0.24433273  0.04794517 -0.74594754]]. Reward = [0.]
Curr episode timestep = 887
Current timestep = 2692. State = [[-0.21656883  0.18765482]]. Action = [[ 0.06649366 -0.11060368 -0.10023651  0.32123947]]. Reward = [0.]
Curr episode timestep = 888
Current timestep = 2693. State = [[-0.22053748  0.18893562]]. Action = [[-0.20824835 -0.23459493  0.0202786  -0.42238855]]. Reward = [0.]
Curr episode timestep = 889
Current timestep = 2694. State = [[-0.22677346  0.18347007]]. Action = [[-0.1736556   0.01834455  0.11515081 -0.28604436]]. Reward = [0.]
Curr episode timestep = 890
Current timestep = 2695. State = [[-0.23690878  0.17998725]]. Action = [[-0.02298938 -0.03982893  0.08961773  0.4742478 ]]. Reward = [0.]
Curr episode timestep = 891
Current timestep = 2696. State = [[-0.24417463  0.1780123 ]]. Action = [[ 0.05046242  0.1423252   0.11167359 -0.18389797]]. Reward = [0.]
Curr episode timestep = 892
Current timestep = 2697. State = [[-0.24867024  0.1792521 ]]. Action = [[ 0.12054017  0.20929635 -0.06848891  0.15657663]]. Reward = [0.]
Curr episode timestep = 893
Current timestep = 2698. State = [[-0.25128245  0.18438342]]. Action = [[-0.16295908  0.07823592  0.23884195 -0.24416012]]. Reward = [0.]
Curr episode timestep = 894
Current timestep = 2699. State = [[-0.25519577  0.18937285]]. Action = [[-0.03120153 -0.19983487 -0.17353389 -0.13407749]]. Reward = [0.]
Curr episode timestep = 895
Current timestep = 2700. State = [[-0.25789538  0.18885858]]. Action = [[ 0.16530877  0.0133096  -0.13767424  0.77100277]]. Reward = [0.]
Curr episode timestep = 896
Current timestep = 2701. State = [[-0.25742546  0.18821207]]. Action = [[0.23937184 0.02403304 0.07653442 0.07073331]]. Reward = [0.]
Curr episode timestep = 897
Current timestep = 2702. State = [[-0.25385705  0.18826698]]. Action = [[ 0.05504936  0.18561018 -0.13807464 -0.6365475 ]]. Reward = [0.]
Curr episode timestep = 898
Current timestep = 2703. State = [[-0.24984245  0.19330539]]. Action = [[ 0.18277538  0.2269659  -0.09578198 -0.0552361 ]]. Reward = [0.]
Curr episode timestep = 899
Current timestep = 2704. State = [[-0.24545375  0.2016194 ]]. Action = [[-0.23007417  0.1583533   0.12278283  0.9386759 ]]. Reward = [0.]
Curr episode timestep = 900
Current timestep = 2705. State = [[-0.2577764   0.00798515]]. Action = [[-0.22541642  0.24179757 -0.05392267  0.84398365]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 2705 is [True, False, False, False, True, False]
State prediction error at timestep 2705 is tensor(0.0186, grad_fn=<MseLossBackward0>)
Current timestep = 2706. State = [[-0.2591175   0.00947855]]. Action = [[-0.22886154 -0.08744499  0.21385849  0.22912037]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 2706 is [True, False, False, False, True, False]
State prediction error at timestep 2706 is tensor(6.6218e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2707. State = [[-0.2694057   0.02715766]]. Action = [[ 0.15215349 -0.18906009  0.1952318  -0.50202864]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 2707 is [True, False, False, False, True, False]
State prediction error at timestep 2707 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 2708. State = [[-0.2731399   0.08482493]]. Action = [[ 0.16632628  0.07041451 -0.22274125  0.785934  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2708 is [True, False, False, False, True, False]
State prediction error at timestep 2708 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 2709. State = [[-0.27094305  0.14145233]]. Action = [[-0.01043801 -0.01710336 -0.23873453 -0.4862476 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2710. State = [[-0.26581317  0.17520131]]. Action = [[ 0.05208641  0.23483822 -0.1524749   0.73598194]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2711. State = [[-0.2618565   0.20050249]]. Action = [[-0.08933821  0.2042442   0.02209598  0.48343766]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2712. State = [[-0.26096854  0.22090773]]. Action = [[ 0.22334611  0.03932917 -0.11581495  0.5511117 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2713. State = [[-0.25608405  0.23339365]]. Action = [[0.23683196 0.05143437 0.18349588 0.72931886]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2714. State = [[-0.2472825   0.24240132]]. Action = [[-0.16532901 -0.1515418   0.02373731  0.62631273]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2715. State = [[-0.24298362  0.24678479]]. Action = [[-0.10333881  0.12611574 -0.0648395   0.66473114]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2716. State = [[-0.24304816  0.25287205]]. Action = [[-0.13572928  0.12000614 -0.20285045 -0.6671591 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2717. State = [[-0.24678479  0.25996462]]. Action = [[-0.08362114  0.15356418 -0.00134388 -0.00377196]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2718. State = [[-0.25258842  0.26786286]]. Action = [[ 0.11089039 -0.19608375  0.11514914  0.06289959]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2719. State = [[-0.25307778  0.26836276]]. Action = [[ 0.00281143  0.18579435 -0.10055506  0.57862234]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2720. State = [[-0.2538597   0.27252036]]. Action = [[ 0.2292785   0.21126515 -0.02364278  0.44978237]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2721. State = [[-0.25050914  0.27987635]]. Action = [[-0.2031952   0.16849643 -0.15839267  0.4773612 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2722. State = [[-0.25172332  0.28994024]]. Action = [[-0.2101331  -0.04033782 -0.0925326  -0.82164603]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2723. State = [[-0.25565073  0.2973846 ]]. Action = [[ 0.01533037  0.21562311 -0.18950863  0.5658498 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2724. State = [[-0.26029792  0.3069656 ]]. Action = [[-0.1852895   0.21843177 -0.24606442  0.728045  ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2725. State = [[-0.2666935  0.3183009]]. Action = [[-0.08177729 -0.11264512  0.07165834  0.5045259 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2726. State = [[-0.27211362  0.3241547 ]]. Action = [[-0.03169712 -0.15251032  0.13172656  0.09009945]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2727. State = [[-0.2748599  0.3246335]]. Action = [[ 0.2436704   0.15900871  0.04624391 -0.27623695]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2728. State = [[-0.27652836  0.32485467]]. Action = [[-0.17775995  0.11872953 -0.20548609  0.5298451 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2729. State = [[-0.2773834   0.32511157]]. Action = [[0.00942665 0.22621053 0.01314935 0.68839765]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2730. State = [[-0.27781868  0.3250441 ]]. Action = [[-0.1999183   0.21213287 -0.12828612  0.22031033]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2731. State = [[-0.2778853  0.3249142]]. Action = [[-0.01714846  0.06862438  0.03129894  0.2218765 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2732. State = [[-0.27806756  0.32505134]]. Action = [[ 0.07737586  0.16382378 -0.1566001  -0.9420974 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2733. State = [[-0.27817994  0.3250992 ]]. Action = [[-0.08761168  0.15777421  0.02811825 -0.23825026]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2734. State = [[-0.2781815   0.32501894]]. Action = [[-0.10628247  0.2009393   0.02719444 -0.78700006]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2735. State = [[-0.27818197  0.32499227]]. Action = [[ 0.2169078   0.10844058 -0.2114724  -0.8299908 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2736. State = [[-0.27809232  0.32471406]]. Action = [[ 0.04928309 -0.19195417 -0.05811846  0.19151998]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2737. State = [[-0.275596    0.32017344]]. Action = [[-0.08085057 -0.21706082  0.18023539  0.69345427]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2738. State = [[-0.27351138  0.31386852]]. Action = [[ 0.05188009 -0.19819483  0.08489522  0.15278411]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2739. State = [[-0.27029657  0.30480257]]. Action = [[-0.12818514  0.24181363  0.22391543 -0.7895101 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2740. State = [[-0.26837194  0.29942438]]. Action = [[ 0.0188227  -0.02519931 -0.16679576  0.48440814]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2741. State = [[-0.2667303  0.2950121]]. Action = [[-0.10304147 -0.07474017  0.08158311 -0.19147164]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2742. State = [[-0.26658335  0.2910263 ]]. Action = [[ 0.08433488 -0.07373857  0.01111642 -0.23518217]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2743. State = [[-0.26528117  0.28672323]]. Action = [[-0.11026889  0.17116386 -0.15421572  0.255849  ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2744. State = [[-0.26815304  0.2886447 ]]. Action = [[-0.03673568  0.18232548 -0.23740612  0.00243556]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2745. State = [[-0.27268752  0.29314625]]. Action = [[-0.08343592 -0.06502688 -0.0589551  -0.26027673]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2746. State = [[-0.2756893  0.2957577]]. Action = [[ 0.15964562  0.19790477 -0.16959825 -0.30886757]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2747. State = [[-0.27800795  0.29894143]]. Action = [[ 0.09699115 -0.14113349  0.15946203 -0.9322108 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2748. State = [[-0.2768971   0.29797137]]. Action = [[-0.17921422  0.15879893  0.05516821  0.9404862 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2749. State = [[-0.28003594  0.30127755]]. Action = [[ 0.2380732  -0.03508784 -0.15643273  0.8091681 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2750. State = [[-0.27872032  0.30154452]]. Action = [[-0.11088216  0.20769402 -0.02090196  0.13498223]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2751. State = [[-0.28086367  0.3059868 ]]. Action = [[-0.14785685  0.04248539  0.10707527  0.55417967]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2752. State = [[-0.28524932  0.3106706 ]]. Action = [[-0.11078048  0.1699523  -0.2186514   0.72257984]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2753. State = [[-0.28813088  0.3135254 ]]. Action = [[-0.13007438 -0.10208546  0.23966205  0.60195386]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2754. State = [[-0.29004458  0.31492138]]. Action = [[-0.1138989  -0.19082007 -0.15531264  0.8676572 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2755. State = [[-0.29122353  0.31276155]]. Action = [[ 0.20172918 -0.16429922 -0.23927003 -0.48655516]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2756. State = [[-0.28805378  0.3069609 ]]. Action = [[0.15716046 0.1439907  0.11209363 0.56518316]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2757. State = [[-0.28626326  0.30306837]]. Action = [[-0.22976583  0.15738118  0.01529521 -0.8569879 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2758. State = [[-0.28490117  0.30015254]]. Action = [[-0.03198098  0.20033962 -0.06046855 -0.53666574]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2759. State = [[-0.2840621  0.298302 ]]. Action = [[-0.01047866 -0.10853231 -0.22121975 -0.666258  ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2760. State = [[-0.28242472  0.29477307]]. Action = [[ 0.17376578 -0.19697444  0.19331336  0.9290285 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2761. State = [[-0.27757463  0.2873261 ]]. Action = [[ 0.18445146 -0.12759611 -0.18159135  0.46484697]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2762. State = [[-0.2711638  0.2784112]]. Action = [[ 0.2391974   0.02335504 -0.16168095 -0.21384513]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2763. State = [[-0.264927  0.271791]]. Action = [[ 0.12618643 -0.00894661  0.23944849  0.77275085]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2764. State = [[-0.25872466  0.26712182]]. Action = [[-0.12672068  0.23310709 -0.16140015  0.4664712 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2765. State = [[-0.25794768  0.2693989 ]]. Action = [[-0.09676775 -0.15604368 -0.16501871  0.8099164 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2766. State = [[-0.25708178  0.26834887]]. Action = [[ 0.19261366 -0.0584368  -0.04363658  0.36878204]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2767. State = [[-0.25381586  0.2652065 ]]. Action = [[ 0.04941431  0.03852871 -0.07237887  0.71711564]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2768. State = [[-0.2515317   0.26368096]]. Action = [[-0.07473554 -0.14426272 -0.18900806 -0.80060107]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2769. State = [[-0.24930708  0.26059407]]. Action = [[0.24164575 0.18372744 0.2148388  0.8572154 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2770. State = [[-0.24611805  0.26072526]]. Action = [[-0.09644277 -0.20082419  0.21298218  0.21011734]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2771. State = [[-0.24373437  0.2568261 ]]. Action = [[-0.15373905 -0.1900701  -0.15442716 -0.449669  ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2772. State = [[-0.24164818  0.25161126]]. Action = [[ 0.04139408 -0.05594611 -0.1708437   0.08296502]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2773. State = [[-0.23969644  0.24709651]]. Action = [[ 0.10981759  0.04689497 -0.00966792  0.46226978]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2774. State = [[-0.23853937  0.24414496]]. Action = [[-0.17092143  0.05846408  0.2401579   0.7977948 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2775. State = [[-0.23951206  0.24479945]]. Action = [[ 0.22237685  0.15029675 -0.04130621 -0.72911525]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2776. State = [[-0.23914665  0.2454591 ]]. Action = [[ 0.24233425 -0.00363441  0.22848842 -0.9188682 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2777. State = [[-0.23365764  0.24626784]]. Action = [[ 0.21775305  0.19256556  0.14747572 -0.9135995 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2778. State = [[-0.22475979  0.25196487]]. Action = [[ 0.09115398  0.23231304 -0.08227587  0.7045239 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2779. State = [[-0.21611613  0.25978187]]. Action = [[ 0.20469359 -0.07888775  0.12140885 -0.6113796 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2780. State = [[-0.20528282  0.2636247 ]]. Action = [[-0.21766916 -0.18915467  0.01990375  0.691623  ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2781. State = [[-0.20026366  0.26376656]]. Action = [[ 0.04720512 -0.00718239 -0.20010108  0.8561444 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2782. State = [[-0.19696231  0.26227486]]. Action = [[ 0.05048612 -0.22313955 -0.23894532 -0.17970443]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2783. State = [[-0.19379233  0.2565813 ]]. Action = [[-0.11184908  0.1609016  -0.06415293 -0.62745345]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2784. State = [[-0.19447587  0.2568797 ]]. Action = [[-0.16035691 -0.09176326 -0.19525121  0.48679614]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2785. State = [[-0.19453603  0.25616825]]. Action = [[ 0.21858698 -0.24318676 -0.05590096 -0.64694107]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2786. State = [[-0.19067965  0.24903211]]. Action = [[ 0.070813    0.03333133 -0.00156783  0.60350657]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2787. State = [[-0.18850943  0.24451864]]. Action = [[ 0.19199732  0.14623201  0.17733192 -0.75166863]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2788. State = [[-0.18563284  0.24400352]]. Action = [[ 0.00818521 -0.01853204 -0.02256228 -0.30479908]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2789. State = [[-0.18305543  0.24374963]]. Action = [[0.16647816 0.11699694 0.22639745 0.11763954]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2790. State = [[-0.17783417  0.24599819]]. Action = [[ 0.18444207  0.17294174  0.08130908 -0.36318433]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2791. State = [[-0.17089438  0.2505871 ]]. Action = [[-0.01582493 -0.04428072  0.13228011  0.9268153 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2792. State = [[-0.16410635  0.2537316 ]]. Action = [[-0.16827491 -0.04888259  0.07869238  0.37355137]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2793. State = [[-0.1616445   0.25554982]]. Action = [[0.06618267 0.10895213 0.23967427 0.04896915]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2794. State = [[-0.15974957  0.2578515 ]]. Action = [[ 0.13280672 -0.0727639   0.21476412 -0.77310556]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2795. State = [[-0.15742676  0.25785103]]. Action = [[-0.17252062  0.14639717  0.0833002  -0.08045018]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2796. State = [[-0.15989546  0.26180974]]. Action = [[-0.18474913  0.196473    0.23090705 -0.18235171]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2797. State = [[-0.16500203  0.26883298]]. Action = [[ 0.19802362 -0.21039473 -0.13049375  0.6417271 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2798. State = [[-0.16367757  0.26758024]]. Action = [[ 0.22113341 -0.16883288 -0.10088566 -0.876744  ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2799. State = [[-0.15733749  0.262419  ]]. Action = [[-0.21203801 -0.2337436  -0.09449008 -0.8873344 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2800. State = [[-0.154684   0.2563081]]. Action = [[ 0.15345594  0.12974018 -0.19732769 -0.9591821 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2801. State = [[-0.15275739  0.2546358 ]]. Action = [[-0.1356324   0.17854673 -0.11163366  0.68499196]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2802. State = [[-0.15423378  0.2577566 ]]. Action = [[-0.1839331   0.20072708  0.2416203   0.70953226]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2803. State = [[-0.15926032  0.26489395]]. Action = [[ 0.02525377  0.10638821 -0.18991889 -0.9161868 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2804. State = [[-0.16297783  0.27132136]]. Action = [[-0.20967461 -0.15969291 -0.08963658  0.45459652]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2805. State = [[-0.16531943  0.2733709 ]]. Action = [[ 0.23419279 -0.169412    0.06283918  0.3151965 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2806. State = [[-0.1632142  0.2692337]]. Action = [[ 0.17120373 -0.21078204  0.18444496  0.42929816]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2807. State = [[-0.15828665  0.26063266]]. Action = [[ 0.10531741 -0.00702581  0.10582072  0.9014311 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2808. State = [[-0.15469141  0.2553791 ]]. Action = [[ 0.2212305   0.22903597 -0.02946997 -0.9102584 ]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 2809. State = [[-0.14928272  0.2562516 ]]. Action = [[ 0.08975348  0.05278617 -0.22351988  0.86330783]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 2810. State = [[-0.14196503  0.25893405]]. Action = [[ 0.15711755 -0.04320076 -0.19558075  0.51050997]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 2811. State = [[-0.1328156   0.25934917]]. Action = [[ 0.21257633 -0.22435664  0.08281317 -0.44301534]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 2812. State = [[-0.12117105  0.253625  ]]. Action = [[-0.11669049 -0.04377249 -0.16076738 -0.7115155 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 2813. State = [[-0.11484069  0.24994421]]. Action = [[-0.15186946 -0.1783136   0.06612438 -0.8841944 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 2814. State = [[-0.11187342  0.24414803]]. Action = [[0.194848   0.20924956 0.23218116 0.550405  ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 2815. State = [[-0.10871489  0.24470313]]. Action = [[ 0.11458075  0.04269281 -0.22465803 -0.9264608 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 2816. State = [[-0.10488392  0.24571852]]. Action = [[-0.21838075 -0.13787346  0.21128917  0.5302081 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 2817. State = [[-0.10430966  0.24405104]]. Action = [[ 0.06521311 -0.13392006  0.0233767   0.69926405]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 2818. State = [[-0.10189371  0.23903017]]. Action = [[-0.06384116 -0.09172942 -0.15057763 -0.00477725]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 2819. State = [[-0.1003955   0.23446837]]. Action = [[-0.20626454 -0.09521919 -0.0530476   0.8128979 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 2820. State = [[-0.10128768  0.22949608]]. Action = [[-0.05126603 -0.23691145 -0.20169754  0.63703465]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 2821. State = [[-0.10148124  0.22087069]]. Action = [[ 0.22746652 -0.05567515 -0.05436647  0.77102876]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 2822. State = [[-0.0982756  0.2126856]]. Action = [[ 0.17476767 -0.22567324  0.15393203 -0.3157609 ]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 2823. State = [[-0.09298628  0.2019136 ]]. Action = [[ 0.2225436  -0.15485886  0.13396859  0.64537144]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 2824. State = [[-0.08619506  0.18923265]]. Action = [[-0.06921145  0.13192746 -0.13993944  0.8263701 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 2825. State = [[-0.08375222  0.1846469 ]]. Action = [[-0.23654068 -0.15346138  0.17877263  0.5560913 ]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 2826. State = [[-0.08325137  0.18074876]]. Action = [[-0.13336678  0.2455742   0.22375888 -0.44949043]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 2827. State = [[-0.08568709  0.18419948]]. Action = [[-0.0977736  -0.02914569  0.15073067  0.46929228]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 2828. State = [[-0.08809007  0.18652762]]. Action = [[2.3204088e-04 1.9261771e-01 6.2823921e-02 9.5935631e-01]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 2829. State = [[-0.09075336  0.19165988]]. Action = [[ 0.00147223 -0.08738807 -0.01748419  0.879549  ]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 2830. State = [[-0.09198263  0.19339009]]. Action = [[ 0.23733437 -0.20075645  0.12333554 -0.50524306]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 2831. State = [[-0.08931425  0.18802969]]. Action = [[ 0.04258612 -0.13361132  0.22282127  0.56374097]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 2832. State = [[-0.08657183  0.18122128]]. Action = [[-0.10794443 -0.19380929  0.00240517  0.79837906]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 2833. State = [[-0.08462717  0.17228577]]. Action = [[ 0.13682362  0.05775258 -0.13986133  0.4259596 ]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 2834. State = [[-0.08292839  0.16826972]]. Action = [[-0.22742437  0.03077587  0.22297043 -0.8009467 ]]. Reward = [0.]
Curr episode timestep = 128
