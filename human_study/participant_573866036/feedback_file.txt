Current timestep = 0. State = [[-0.2564025   0.00608253]]. Action = [[ 0.02368274 -0.03158908  0.04000988  0.6916094 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0425, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25547945  0.00524854]]. Action = [[0.07372499 0.08091003 0.06755862 0.56340873]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0385, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25390762  0.00572045]]. Action = [[ 0.09737016 -0.09297638 -0.07287936  0.7404891 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25153178  0.00462704]]. Action = [[-0.04974121 -0.08747933  0.00550723 -0.9429156 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2510493   0.00190999]]. Action = [[-0.03769147  0.00626449 -0.02686653  0.9408524 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0197, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-2.5106198e-01  2.4119712e-04]]. Action = [[ 0.02641737 -0.01754613 -0.08054889 -0.7854333 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.2510491  -0.00114361]]. Action = [[-0.08957391 -0.00628562  0.08312204  0.7870674 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.2512078  -0.00253992]]. Action = [[-0.04435879 -0.05475331  0.03464062  0.82437897]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.25141555 -0.00449395]]. Action = [[-0.01611264  0.03257713  0.0861259  -0.36156017]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.25164315 -0.00502838]]. Action = [[-0.08462408  0.04811496  0.07207004 -0.9843382 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25268036 -0.00421369]]. Action = [[ 0.03853542  0.0197238   0.02015682 -0.12025291]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.2528317  -0.00388783]]. Action = [[ 0.08501042 -0.09208029  0.08364656  0.7485459 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.25292423 -0.00527951]]. Action = [[-0.09679342  0.05885877 -0.00306623  0.8257861 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.2533352  -0.00521554]]. Action = [[-0.07299456  0.06940851  0.05572364 -0.41459477]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25500312 -0.00302864]]. Action = [[ 0.07165212  0.08835893 -0.05514818  0.8676299 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-2.5593975e-01 -2.1034057e-05]]. Action = [[ 0.053576   -0.09649984  0.07312839 -0.39368862]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 15 of 0
Current timestep = 16. State = [[-0.25611752 -0.00046489]]. Action = [[-0.0406949  -0.08104657  0.07009485 -0.4706372 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25618598 -0.00268296]]. Action = [[ 0.07060084 -0.0949138  -0.00958148  0.18072224]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25620717 -0.00584249]]. Action = [[ 0.09145413  0.07848649  0.07934453 -0.706121  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.2552909 -0.0059472]]. Action = [[ 0.07052446  0.01761989  0.09339029 -0.0718261 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25363722 -0.00598999]]. Action = [[ 0.00099459 -0.0065393   0.04827451 -0.11342198]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25229156 -0.00610539]]. Action = [[ 0.04089346 -0.06382716 -0.0707231  -0.1677342 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.2503476  -0.00738566]]. Action = [[ 0.0254649  -0.05748566  0.05541738  0.8027015 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.24846435 -0.0092393 ]]. Action = [[0.08809901 0.00702752 0.07238204 0.60874903]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.24468443 -0.01037459]]. Action = [[ 0.05887366  0.02135273  0.04541904 -0.98601806]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.24032317 -0.01066217]]. Action = [[-0.00364453 -0.0721388  -0.0494316  -0.1690833 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.23752107 -0.0128875 ]]. Action = [[-0.00041866 -0.07629608  0.00760721 -0.374941  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.23614162 -0.01603754]]. Action = [[-0.08048634 -0.09671838  0.03165085  0.780396  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.23625407 -0.02133505]]. Action = [[ 0.01423435 -0.08384335  0.01528054  0.66773033]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.23623806 -0.02658443]]. Action = [[-0.01755849 -0.06032366  0.05319563 -0.4737484 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.23619425 -0.03171383]]. Action = [[ 0.04326249 -0.0293446  -0.07975073  0.4065231 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.23630428 -0.03583904]]. Action = [[-0.03809157 -0.04059422 -0.02364015  0.6688216 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.23685423 -0.03920376]]. Action = [[-0.09111493  0.08112175  0.00205884  0.7439232 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.2371541 -0.0399099]]. Action = [[-0.00251977  0.00487077  0.04927177  0.66211224]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.23727411 -0.04010987]]. Action = [[ 0.09164625 -0.0282016  -0.08333428 -0.9672185 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(6.8118e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.23725545 -0.04078051]]. Action = [[-0.04790992 -0.08900211 -0.04074979 -0.1454004 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.2375069  -0.04334303]]. Action = [[ 0.05998496 -0.04432043 -0.01687158 -0.3774196 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.23721826 -0.04585042]]. Action = [[ 0.07814849 -0.01315196  0.04994868  0.48304558]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[-0.2359758  -0.04763969]]. Action = [[ 0.08641595 -0.07669525 -0.01741728 -0.8576818 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.23257719 -0.05108619]]. Action = [[ 0.04413236 -0.037605    0.06266084 -0.34430987]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.22909094 -0.05418857]]. Action = [[-0.05781493  0.06855134  0.04141713 -0.67414874]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.22831905 -0.05444536]]. Action = [[ 0.01996918  0.00141309 -0.0072497  -0.53090614]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.2279985  -0.05439502]]. Action = [[0.0029261  0.04931536 0.02786575 0.51705754]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.22812134 -0.05393057]]. Action = [[-0.04603806  0.08302612  0.03231547 -0.18452084]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.22842824 -0.05205672]]. Action = [[ 0.03239501  0.08370119  0.05270164 -0.9586711 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.22847432 -0.04882159]]. Action = [[-0.02239989 -0.03152101  0.05743859 -0.38650912]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.22847721 -0.04786598]]. Action = [[ 0.07307107  0.06660328 -0.09084126 -0.5927844 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.22811379 -0.04568481]]. Action = [[-0.0827631  -0.07712163  0.08379365  0.4487785 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.2280103  -0.04585219]]. Action = [[-0.05113462 -0.0114614  -0.04811282 -0.9357211 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.22811612 -0.04600538]]. Action = [[-0.07782613 -0.0716811   0.0204661   0.73161817]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.22851138 -0.0477213 ]]. Action = [[-0.04758373  0.0349941   0.09867216 -0.76020825]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.22955996 -0.04809324]]. Action = [[ 0.04599867 -0.09000621 -0.01607119  0.44575775]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.22964185 -0.04997655]]. Action = [[-0.06122241 -0.09034715 -0.01230082 -0.9292656 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.23056163 -0.05360516]]. Action = [[ 0.09509934 -0.04147248 -0.08455084  0.21058726]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.23070236 -0.05687662]]. Action = [[ 0.08410258 -0.0781249  -0.06227633 -0.60420936]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.23023939 -0.06098214]]. Action = [[-0.02421635 -0.00150475 -0.04757326 -0.23503786]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 56. State = [[-0.23016804 -0.06343196]]. Action = [[-0.0484665  -0.06758299  0.04723997  0.94091153]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.23041847 -0.06660661]]. Action = [[-0.02833903  0.06474603  0.0247014   0.17671132]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.23052298 -0.06704459]]. Action = [[ 0.01155861 -0.04519799 -0.03433853  0.34610772]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.23079033 -0.06829532]]. Action = [[-0.0136938   0.0642314  -0.09605365  0.23430514]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.23083408 -0.06819176]]. Action = [[ 0.01282342  0.03234345 -0.08581664 -0.41421342]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of 1
Current timestep = 61. State = [[-0.23077962 -0.06806801]]. Action = [[ 0.06949558 -0.08340137  0.05231187 -0.9033266 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.23065561 -0.06831229]]. Action = [[ 0.08218641  0.02478439 -0.03439009  0.80600464]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.22991046 -0.0683561 ]]. Action = [[-0.00219434 -0.00807105  0.08976979 -0.47349072]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.22946054 -0.06834222]]. Action = [[-0.02445753  0.00415771  0.05918361 -0.8086491 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.22947486 -0.06839903]]. Action = [[-0.07959328  0.02752436 -0.05839446 -0.10263115]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.2295935  -0.06839386]]. Action = [[ 0.01366212 -0.01770295  0.04578752 -0.03124833]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.229626   -0.06835444]]. Action = [[-0.01018746  0.06238896 -0.0565612  -0.40734982]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.22973636 -0.06794365]]. Action = [[-0.08790284 -0.09499072  0.05792583  0.08422661]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.22986628 -0.06868345]]. Action = [[ 0.05653361 -0.03705107  0.02852339 -0.7330037 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.22988196 -0.06974644]]. Action = [[-0.04965436  0.06700642 -0.09275784  0.93227017]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.2302016  -0.06974199]]. Action = [[-0.07306692 -0.07470496 -0.03710189 -0.65440786]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.23156619 -0.0712163 ]]. Action = [[-0.09718272 -0.04017216  0.08112747  0.6544769 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.23448937 -0.07416209]]. Action = [[-0.05562267  0.07786911  0.09037671 -0.8225611 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.2368129  -0.07440764]]. Action = [[-0.04484085 -0.033525    0.0672146  -0.8052683 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.23874556 -0.07556088]]. Action = [[ 0.09149524 -0.00186561 -0.08575984 -0.41083753]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.23881899 -0.07598966]]. Action = [[ 0.04699858 -0.07434404  0.09543174  0.9072323 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.23896076 -0.07784014]]. Action = [[0.01451807 0.01256545 0.04493255 0.8785324 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.2390532  -0.07832911]]. Action = [[-0.01921105  0.09194369 -0.09171207  0.64367104]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.23916337 -0.07745401]]. Action = [[-0.02263974  0.04571251 -0.07830194 -0.3288015 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.23937248 -0.07579692]]. Action = [[ 0.06900208  0.08764393  0.01100986 -0.5893953 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.23960678 -0.07229961]]. Action = [[-0.03460737  0.09226639 -0.04415184  0.30422807]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.24024855 -0.06788114]]. Action = [[-0.08350886  0.06547368 -0.09910922 -0.5102139 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 83. State = [[-0.24115911 -0.06301113]]. Action = [[ 0.09902979  0.07141454  0.06735945 -0.24908727]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of -1
Current timestep = 84. State = [[-0.24190572 -0.05777074]]. Action = [[ 0.07899459  0.04702286 -0.02637269 -0.8223478 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(1.7391e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.24163254 -0.05370695]]. Action = [[ 0.02646197  0.05945208  0.06793142 -0.7165371 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(9.6139e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-0.24144751 -0.04992097]]. Action = [[-0.05290171 -0.04110939 -0.03364468  0.509045  ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.24169771 -0.04882067]]. Action = [[ 0.03570736  0.06752905  0.00639921 -0.70324266]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.24178824 -0.04645636]]. Action = [[ 0.02970388 -0.04765482  0.09385849 -0.31628096]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.24124882 -0.04619902]]. Action = [[-0.0297275   0.00961603  0.09070642  0.8017483 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.2411871  -0.04584055]]. Action = [[-0.03687427 -0.05962385  0.0383332   0.07429028]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 0
Current timestep = 91. State = [[-0.24123389 -0.04575586]]. Action = [[-0.09814952  0.060196    0.09133749  0.8448632 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 0
Current timestep = 92. State = [[-0.24149844 -0.04509858]]. Action = [[-0.00561868  0.05904079 -0.04101745 -0.24828249]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 93. State = [[-0.24204703 -0.04291891]]. Action = [[-0.05310282 -0.09342004  0.07362228  0.08335209]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.24291757 -0.0431806 ]]. Action = [[-0.05939964  0.09313212  0.09732366 -0.9518559 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(6.4453e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 0
Current timestep = 95. State = [[-0.24420832 -0.04133245]]. Action = [[ 0.0552176   0.07549181 -0.09029517  0.29653382]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.24490316 -0.03818004]]. Action = [[-0.0277103   0.06999221  0.04080952 -0.59761685]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 0
Current timestep = 97. State = [[-0.24582635 -0.03409477]]. Action = [[ 0.05375227 -0.02980691  0.03934833  0.54955626]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 98. State = [[-0.24609593 -0.03304781]]. Action = [[ 0.01377952 -0.02654416 -0.03141072 -0.7356528 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.24614407 -0.03285763]]. Action = [[-0.00045364  0.01683389  0.07834627  0.19235492]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.24611789 -0.03273828]]. Action = [[ 0.0606479  -0.07877408  0.08095697  0.6173786 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.2458953  -0.03306743]]. Action = [[ 0.05661681  0.0739037  -0.00292922  0.98148704]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(1.6554e-05, grad_fn=<MseLossBackward0>)
Current timestep = 102. State = [[-0.24524912 -0.03273507]]. Action = [[ 0.06425451  0.07520816 -0.06356368 -0.30079758]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.24379562 -0.03119276]]. Action = [[ 0.02057342 -0.03622054 -0.02107528  0.30072463]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.24268214 -0.03125091]]. Action = [[-0.09099969 -0.07657985  0.00572039  0.661271  ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.24264106 -0.03155321]]. Action = [[ 0.05201855 -0.03701657  0.02320542 -0.2522154 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 106. State = [[-0.24191901 -0.03255785]]. Action = [[ 0.0816957  -0.07959095  0.09143532  0.93750703]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(2.7538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.24048208 -0.03464121]]. Action = [[-0.04717424  0.05779452  0.05116846 -0.8691097 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.23986925 -0.03445794]]. Action = [[ 0.05541266  0.0013356  -0.08252113 -0.27059996]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.23807018 -0.03416191]]. Action = [[ 0.08762241 -0.04973077 -0.00856013 -0.5313992 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.2353212  -0.03481843]]. Action = [[ 0.03068934 -0.01132266 -0.07441843 -0.44708443]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.23270547 -0.03560289]]. Action = [[-0.01326241 -0.00536048  0.07906885  0.05634356]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.23140942 -0.03619245]]. Action = [[ 0.01381215  0.08023828 -0.04340462  0.86484337]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(1.8456e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.23100223 -0.03577726]]. Action = [[-0.0143059  -0.03046106 -0.09217739 -0.55639935]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(8.2075e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.23090385 -0.03574776]]. Action = [[-0.08840714 -0.02769101 -0.03824766  0.20514345]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 115. State = [[-0.23101448 -0.03612235]]. Action = [[-0.07402918  0.06582739 -0.06986503 -0.6823303 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.23119381 -0.03582801]]. Action = [[ 0.09122402 -0.08653876 -0.01318885  0.92656755]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(4.3506e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.23100252 -0.03649522]]. Action = [[-0.07445179  0.04362822 -0.05225192  0.11753857]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.23106699 -0.03645137]]. Action = [[ 0.01753715 -0.01313792  0.03202093 -0.41346788]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(8.2932e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.23106204 -0.03646606]]. Action = [[ 1.6100705e-04  4.2140432e-02 -3.3215240e-02  4.9699199e-01]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.23111004 -0.03616222]]. Action = [[0.00303476 0.00313028 0.09756614 0.5765834 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 121. State = [[-0.23112439 -0.03589797]]. Action = [[-0.08767329  0.03131757 -0.03917062 -0.22639644]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.23139395 -0.03512669]]. Action = [[-0.05523361 -0.05552901  0.05262    -0.31913698]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.23205334 -0.03552787]]. Action = [[ 0.0097039  -0.05743076 -0.02723589  0.2959746 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.23252603 -0.03697711]]. Action = [[-0.08670314  0.02169514 -0.07963152 -0.08676815]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.23434776 -0.03744913]]. Action = [[ 0.04952947 -0.03690742 -0.08017526  0.30320096]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.23508312 -0.03847193]]. Action = [[-0.08146612  0.03774261 -0.01903374  0.9005556 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(1.9546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.23650393 -0.03872548]]. Action = [[-0.07672624 -0.07811783  0.0661528  -0.435755  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(4.7469e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.23888886 -0.04083419]]. Action = [[-0.04972979  0.05839906 -0.0489159   0.8858484 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(3.5931e-05, grad_fn=<MseLossBackward0>)
Current timestep = 129. State = [[-0.24142452 -0.04107387]]. Action = [[0.00039095 0.0015582  0.016485   0.2876742 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.24301738 -0.04112972]]. Action = [[-0.00900183  0.07936304 -0.06703345  0.89627004]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(4.2711e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.24504769 -0.03793054]]. Action = [[ 0.07603646  0.05164845 -0.055668    0.6789956 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of -1
Current timestep = 132. State = [[-0.24555957 -0.03560993]]. Action = [[-0.09902187  0.09632199 -0.05066197  0.79110384]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.24672519 -0.03145264]]. Action = [[ 0.09092464 -0.04223774  0.07474983 -0.2000494 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.2471808  -0.03026502]]. Action = [[ 0.01971022 -0.08290683 -0.06915452 -0.06764489]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.24724412 -0.03115936]]. Action = [[ 0.02508345 -0.02430993  0.08030715  0.08886313]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.24738063 -0.03222573]]. Action = [[-0.08633819  0.02946759 -0.03125618 -0.92311954]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of -1
Current timestep = 137. State = [[-0.2475236  -0.03236567]]. Action = [[-0.0801868   0.04436121  0.04405708 -0.62250835]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.24819803 -0.03151819]]. Action = [[ 0.03372078  0.04833961  0.02883773 -0.30609334]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.24874055 -0.02977585]]. Action = [[-0.07082771  0.04410175  0.03639144  0.4517789 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.2498843  -0.02715127]]. Action = [[ 0.04250569 -0.01859139 -0.05251081 -0.8884658 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.25032422 -0.02615673]]. Action = [[-0.04344404 -0.02784972  0.06886273 -0.15732849]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.2507921 -0.0262709]]. Action = [[ 0.06975403  0.04260721 -0.09476949 -0.56879973]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.25110275 -0.02556866]]. Action = [[-0.05470049 -0.05155058 -0.09354033  0.9495094 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(6.7803e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of -1
Current timestep = 144. State = [[-0.25122336 -0.02611892]]. Action = [[ 0.01584059 -0.08980006 -0.01168098  0.7408607 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.25169265 -0.02871495]]. Action = [[-0.06845291 -0.09467544 -0.06816292 -0.01514465]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.25303757 -0.03338623]]. Action = [[-0.05113986 -0.09411868 -0.02918345 -0.19526547]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.2549503 -0.0385842]]. Action = [[ 0.01496725  0.07557752  0.03069205 -0.296399  ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 148. State = [[-0.25563845 -0.03976616]]. Action = [[ 0.04425976  0.09077529 -0.01192878 -0.0216307 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.25595    -0.03827367]]. Action = [[-0.05051895  0.06658227  0.02538905 -0.54447836]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.25672686 -0.03569445]]. Action = [[-0.08463947 -0.00595868 -0.03322954  0.70399034]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.25834355 -0.03418906]]. Action = [[-0.09227561 -0.0876912   0.09056974 -0.12979281]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.26210284 -0.03555863]]. Action = [[ 0.05926491 -0.0129073  -0.05203084 -0.96656555]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(6.7453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.2640704  -0.03705149]]. Action = [[-0.01768428 -0.06490393 -0.02046204 -0.03619975]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.2655512  -0.03975461]]. Action = [[ 0.08865235 -0.08577478  0.0407924   0.46818602]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.26587063 -0.04275028]]. Action = [[0.0900181  0.0838623  0.07460887 0.34820557]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.26566672 -0.04263008]]. Action = [[ 0.09045548  0.02075467  0.09472115 -0.73415864]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(9.7678e-05, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.26458356 -0.0425213 ]]. Action = [[-0.0674395  -0.01375124  0.01063476  0.8958168 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.26469153 -0.0423139 ]]. Action = [[-0.08764177  0.08822756 -0.02043726 -0.37554282]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.26513904 -0.04079879]]. Action = [[ 0.00486221 -0.09650593 -0.08611491 -0.17934406]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.26533544 -0.04130771]]. Action = [[-0.07203478 -0.02599762  0.00771145  0.58331513]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.2659923  -0.04206456]]. Action = [[-0.00343817  0.07692569 -0.04307142  0.43983006]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.2664902  -0.04140224]]. Action = [[ 0.08698275  0.05717159 -0.04948457  0.21522057]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.26646405 -0.04011111]]. Action = [[-0.00300054 -0.04307848 -0.09792537  0.6210313 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.26644742 -0.04020308]]. Action = [[-0.05077509 -0.09467465 -0.03282897  0.692924  ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 165. State = [[-0.2666066  -0.04154767]]. Action = [[-0.04027135 -0.02797683 -0.03495842 -0.6438954 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.26710755 -0.04346354]]. Action = [[ 0.05218448 -0.08773239  0.08396108 -0.0799982 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.2675451  -0.04652036]]. Action = [[-0.0704532   0.07922617 -0.09325077 -0.6412106 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.26833564 -0.04753765]]. Action = [[-0.07604013 -0.08479531 -0.08842052  0.2641312 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.27038565 -0.05018808]]. Action = [[-0.00111827  0.02178168 -0.01298127 -0.91189075]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(3.2482e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.27175403 -0.05128855]]. Action = [[-0.0120685  -0.01023101  0.06561535 -0.87483776]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(5.8991e-05, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.27277032 -0.05214396]]. Action = [[-0.06075171 -0.09667744  0.01865219  0.7888262 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.27468684 -0.05476536]]. Action = [[-0.05588323  0.0695704  -0.07482056  0.4265952 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.2769953 -0.0552583]]. Action = [[ 0.06912797 -0.04198003 -0.07204099  0.03714204]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 173 of -1
Current timestep = 174. State = [[-0.27764344 -0.0563331 ]]. Action = [[0.05260224 0.01698194 0.00165784 0.8998667 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.27747545 -0.05638672]]. Action = [[ 0.0574354   0.01590177 -0.08928385  0.81794393]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.27734002 -0.05632697]]. Action = [[ 0.07973575  0.0727784   0.02210708 -0.7735575 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.27724972 -0.05550088]]. Action = [[-0.09539894  0.01136041  0.01557453  0.13080156]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.27736664 -0.05458339]]. Action = [[-0.02802439  0.03763414  0.03529146 -0.72106177]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.2777104  -0.05280123]]. Action = [[-0.04545596  0.0935251  -0.02802254  0.1701647 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of -1
Current timestep = 180. State = [[-0.2785247  -0.04932331]]. Action = [[-0.03521036 -0.09570809  0.0125161   0.49592173]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.27896565 -0.04897884]]. Action = [[-0.0364347  -0.03325926 -0.09169696  0.17562568]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.27923754 -0.04952748]]. Action = [[-0.01146157  0.03015486  0.05888674 -0.67953247]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-0.280259   -0.04927235]]. Action = [[-0.00130632 -0.00660361  0.03954699  0.10222161]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.28076178 -0.04917039]]. Action = [[ 0.02307197  0.05641455 -0.00589438 -0.34338766]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.2811359  -0.04810231]]. Action = [[ 0.03590443  0.01224725 -0.07328874  0.25311756]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.28145215 -0.04692997]]. Action = [[-0.02976307  0.06332972  0.01744089 -0.0378806 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.28194857 -0.04454681]]. Action = [[0.09695566 0.08430011 0.0431493  0.2093575 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.28249505 -0.04128674]]. Action = [[-0.09705155 -0.04606693 -0.01743537  0.41881084]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.28289393 -0.0398255 ]]. Action = [[ 0.0159325   0.06729735  0.02214074 -0.5620304 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.28352094 -0.03734097]]. Action = [[ 0.06557371  0.00295494  0.08124217 -0.7603117 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.28380737 -0.0357861 ]]. Action = [[-0.01516052  0.09069759  0.05418391  0.386047  ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.28412354 -0.03254681]]. Action = [[-0.03824276 -0.02473396  0.02430568 -0.702807  ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.2843901  -0.03112566]]. Action = [[ 0.08571775 -0.07015721  0.03157853  0.29143476]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.28375527 -0.03094793]]. Action = [[-0.06262799  0.02982261  0.0189753  -0.6009169 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 195. State = [[-0.28365254 -0.03098677]]. Action = [[ 0.06645308 -0.08325948  0.0890291   0.82923067]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.28271297 -0.03155216]]. Action = [[0.02981281 0.08457556 0.09175938 0.6826439 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.28222707 -0.031135  ]]. Action = [[ 0.04441475 -0.02095909  0.02147631 -0.73803896]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.28124765 -0.03073478]]. Action = [[ 0.07337644 -0.01835258  0.05955007 -0.8677291 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.27978015 -0.03035252]]. Action = [[ 0.05409356 -0.01314445  0.07448835  0.08311045]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.2777573  -0.03030219]]. Action = [[-0.06072076  0.05828137 -0.0490729   0.42047858]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.27778614 -0.03013621]]. Action = [[-0.08030429 -0.08085527 -0.04527163  0.1671592 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.27790132 -0.03106458]]. Action = [[-0.05410128 -0.08338903  0.0360055  -0.23316413]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.27803817 -0.03352842]]. Action = [[-0.03019358 -0.03546703 -0.0248777  -0.63829577]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.27851373 -0.03602528]]. Action = [[0.002326   0.09233337 0.02440228 0.7584245 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 205. State = [[-0.27857712 -0.035824  ]]. Action = [[-0.01479942  0.05647271 -0.05340871 -0.96221524]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.27881375 -0.03484166]]. Action = [[0.04837868 0.03694119 0.01516747 0.9779844 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.27885994 -0.03405058]]. Action = [[ 0.0823283  -0.09722549  0.08960392  0.68676186]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.27863207 -0.03460021]]. Action = [[ 0.01276796 -0.04254345  0.04585578  0.3694644 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.27795917 -0.03557778]]. Action = [[-0.06414683 -0.00905444  0.0245163   0.8041146 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.2780104  -0.03615376]]. Action = [[-0.03601399  0.03958768 -0.0127859  -0.44680798]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.27798435 -0.03624514]]. Action = [[ 0.06794385 -0.07633924 -0.07003562 -0.46165657]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 212. State = [[-0.27783647 -0.03737502]]. Action = [[-0.02412016  0.08314439 -0.01611035 -0.80599046]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(7.9623e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-0.27778217 -0.03723603]]. Action = [[ 0.09687803 -0.03023418 -0.02934795  0.69906807]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.2774159  -0.03725412]]. Action = [[ 0.02199814 -0.00380903  0.01178835 -0.93277484]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.27693716 -0.03733722]]. Action = [[ 0.01835177 -0.06129902  0.05987222 -0.8392233 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(5.4746e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of 1
Current timestep = 216. State = [[-0.27585527 -0.03858085]]. Action = [[-0.00799596  0.0140236  -0.06067974 -0.37106705]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.27504826 -0.03929897]]. Action = [[-0.05218976 -0.09392498  0.05133391  0.30474246]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.2746104  -0.04239798]]. Action = [[-0.07512321 -0.08008256 -0.01734515  0.0559808 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.2746678  -0.04663009]]. Action = [[ 0.08974657 -0.07375397 -0.01073901 -0.8877019 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(4.5193e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.2740238  -0.05132485]]. Action = [[-0.05893596 -0.05667825  0.07378364 -0.67311585]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.27343887 -0.05602648]]. Action = [[ 0.0944285  -0.00839675 -0.02182207 -0.35977554]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.27261093 -0.05940158]]. Action = [[ 0.05623161 -0.09429162  0.09831405 -0.831641  ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.2715705  -0.06376885]]. Action = [[-0.08482718 -0.02412695  0.00896718 -0.8273586 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.27091086 -0.06714422]]. Action = [[ 0.02488428  0.06028562 -0.0758331   0.2960322 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.27078184 -0.06775667]]. Action = [[-0.05671647 -0.08647323 -0.01312451  0.33300543]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.2709208  -0.07030158]]. Action = [[0.01500201 0.09141529 0.00568223 0.06668735]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.27102345 -0.07010899]]. Action = [[ 0.09247141  0.04886056 -0.09400101  0.01668715]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.27080017 -0.06955988]]. Action = [[ 0.09663395  0.03482044 -0.03184743  0.1135422 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.26978642 -0.06834628]]. Action = [[ 0.09055071  0.03102637 -0.08395292 -0.9742387 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.26647997 -0.06669788]]. Action = [[-0.08272994 -0.09367584  0.06005193  0.23279333]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.2652488  -0.06684043]]. Action = [[-0.04245367 -0.01015482  0.00425811  0.09568059]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.26522532 -0.06695046]]. Action = [[0.00839107 0.00772049 0.0758527  0.71962106]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.26530698 -0.06717002]]. Action = [[-0.07326318 -0.08992526  0.03643528  0.5985037 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.26545066 -0.06990653]]. Action = [[-0.04467133  0.09836843  0.05964667  0.10705101]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.26557276 -0.06972539]]. Action = [[-0.07838625 -0.08248983 -0.02670193  0.68803287]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.26625296 -0.0710309 ]]. Action = [[-0.07055669  0.00442509  0.08739976 -0.91950107]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.26763892 -0.0720797 ]]. Action = [[-0.0848431   0.08820008 -0.05037571  0.7674837 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.27065268 -0.07075253]]. Action = [[ 0.0894044  -0.05280111  0.02131339 -0.06082082]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 239. State = [[-0.270867   -0.07123075]]. Action = [[ 0.03596344 -0.00991239  0.0148432  -0.1577425 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of 1
Current timestep = 240. State = [[-0.27091762 -0.07178551]]. Action = [[ 0.06134445 -0.07864545  0.05253424  0.87586594]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 241. State = [[-0.27084938 -0.07334156]]. Action = [[ 0.0690613  -0.01736135 -0.01408662  0.9796257 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.27087027 -0.07441599]]. Action = [[ 0.00329421 -0.04067105  0.01481558  0.19614208]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.2707975  -0.07593813]]. Action = [[ 0.03138735  0.08132797 -0.04201351  0.34388196]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 244. State = [[-0.27061152 -0.07584485]]. Action = [[ 0.05111199 -0.08111592  0.02846766 -0.85279655]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.26943636 -0.07706139]]. Action = [[ 0.0713462  -0.02948458  0.02409111  0.8061583 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.26656574 -0.07861734]]. Action = [[-0.03372997 -0.03708632 -0.06380452 -0.9278    ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.26477343 -0.08114616]]. Action = [[-0.08327825 -0.07888769  0.01119836  0.07363594]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 248. State = [[-0.26448816 -0.08454965]]. Action = [[ 0.08998764  0.02462409 -0.05025394  0.54006696]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.26378176 -0.08597147]]. Action = [[ 0.02001198 -0.08458311 -0.06445482 -0.23503453]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 250. State = [[-0.26314265 -0.08954377]]. Action = [[ 0.03692896 -0.03379846  0.0019315   0.11293125]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.26231757 -0.09289124]]. Action = [[-0.07043714  0.08651251 -0.07916774  0.88255036]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.2622103  -0.09286096]]. Action = [[ 0.05534089  0.0065339  -0.05787911 -0.12526232]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 1
Current timestep = 253. State = [[-0.26204193 -0.09284851]]. Action = [[-0.08298416 -0.09180602  0.02690173 -0.20707333]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of 1
Current timestep = 254. State = [[-0.2619873  -0.09469243]]. Action = [[-0.06228584 -0.03148667 -0.02835593  0.04040253]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 255. State = [[-0.26219228 -0.09683029]]. Action = [[-0.03298105 -0.0114972  -0.0617693   0.17357755]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 256. State = [[-0.2625498  -0.09818165]]. Action = [[-0.08002986  0.01626166  0.06397212  0.768772  ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of 1
Current timestep = 257. State = [[-0.26386872 -0.09900223]]. Action = [[-0.02437991  0.09213243 -0.08422265  0.64324546]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.26487413 -0.09845183]]. Action = [[ 0.08042135 -0.09351831  0.00921496 -0.68132496]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 259. State = [[-0.26485464 -0.09929176]]. Action = [[ 0.0117357  -0.06373553  0.09630742 -0.8330287 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of 1
Current timestep = 260. State = [[-0.26522753 -0.10142808]]. Action = [[ 0.07515571  0.09599213 -0.08468217 -0.670117  ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 1
Current timestep = 261. State = [[-0.26518077 -0.1013714 ]]. Action = [[ 0.02353986 -0.07045524  0.06120477  0.02000964]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of 1
Current timestep = 262. State = [[-0.2650646  -0.10154801]]. Action = [[ 0.08357095 -0.01975931  0.05498622 -0.6154092 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.2643867  -0.10222013]]. Action = [[ 0.08063509 -0.07877795 -0.04570768 -0.8986986 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.26188764 -0.10477378]]. Action = [[ 0.06563509  0.03089576  0.02285782 -0.82253534]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of 1
Current timestep = 265. State = [[-0.25866392 -0.10600448]]. Action = [[-0.07090048 -0.01978336  0.09399145 -0.38308764]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.25783512 -0.10660584]]. Action = [[0.05587976 0.00493036 0.04690041 0.2708484 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 267. State = [[-0.25608036 -0.10731841]]. Action = [[ 0.08516929 -0.07311481  0.07382355  0.8499094 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.25242954 -0.10942494]]. Action = [[ 0.03985219 -0.03487365 -0.05716323  0.5307009 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.24853268 -0.11195525]]. Action = [[ 0.02902383 -0.03695762 -0.00222357 -0.4882233 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of 1
Current timestep = 270. State = [[-0.2444892  -0.11490797]]. Action = [[-0.00438338  0.00477417 -0.09188949 -0.4136095 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.24213444 -0.11618975]]. Action = [[ 0.01348524 -0.04228414 -0.02239648 -0.2697233 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 272. State = [[-0.24084297 -0.11787487]]. Action = [[ 0.06627659 -0.03126758  0.06011754 -0.31797355]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.23938973 -0.11989547]]. Action = [[ 0.01349334  0.0726667  -0.0305922  -0.7616183 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.23827018 -0.12001888]]. Action = [[ 0.0321046   0.06267353  0.09179234 -0.9006738 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.23729008 -0.11925029]]. Action = [[ 0.05820432  0.03084595 -0.07270779  0.8362963 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.2353336  -0.11805405]]. Action = [[ 0.04700161  0.0410137  -0.03473019  0.43285918]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.23268004 -0.11574589]]. Action = [[-0.09470724 -0.06259596 -0.00948402  0.36799192]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.23250826 -0.11545962]]. Action = [[ 0.0537992   0.06833837 -0.03445621 -0.7459904 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.2322239  -0.11411871]]. Action = [[ 0.05477335  0.07586504  0.03669513 -0.3948201 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.23091751 -0.11139541]]. Action = [[ 0.07622767 -0.01524536 -0.09125216 -0.9198731 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 281. State = [[-0.2282058  -0.11023334]]. Action = [[ 0.04585791 -0.01498856 -0.01712547 -0.6144121 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.22502737 -0.11026158]]. Action = [[-0.02874564  0.0007522  -0.07465789 -0.5417665 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 282 of 1
Current timestep = 283. State = [[-0.22281358 -0.11035921]]. Action = [[-0.05098102  0.00664012  0.01523907  0.6907232 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.2221581  -0.11011829]]. Action = [[-0.07613146  0.01437473  0.00310831 -0.5403579 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.22233391 -0.10957407]]. Action = [[-0.06092883  0.0513057  -0.08875506 -0.9164564 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 286. State = [[-0.22257997 -0.10802503]]. Action = [[ 0.04300768  0.03004704 -0.05324024 -0.04396743]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of 1
Current timestep = 287. State = [[-0.22282563 -0.10622562]]. Action = [[-0.05410006  0.04249538  0.0705632   0.09221077]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 1
Current timestep = 288. State = [[-0.22295444 -0.10388409]]. Action = [[ 0.05919636  0.07415045  0.05067841 -0.544896  ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[-0.22323965 -0.10016475]]. Action = [[-0.0257739  -0.02072467 -0.02594428  0.09814644]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.22338092 -0.09879763]]. Action = [[-0.00672855 -0.03703165 -0.06609683 -0.34864777]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.2235692  -0.09893161]]. Action = [[-0.07401051 -0.05201033 -0.04485643 -0.27552003]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.22401546 -0.10001031]]. Action = [[-0.04006675  0.06226573 -0.07004245 -0.7226186 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.22462955 -0.09942537]]. Action = [[ 0.02166156 -0.02977428 -0.03980242 -0.83209604]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[-0.22486848 -0.09949212]]. Action = [[ 0.04038536  0.09762075 -0.06522185 -0.53033495]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 295. State = [[-0.2251309  -0.09752271]]. Action = [[-0.0107456  -0.00983156 -0.05339029  0.28849256]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 295 of 1
Current timestep = 296. State = [[-0.22516684 -0.0964037 ]]. Action = [[ 0.0444633   0.01309402  0.08832135 -0.42266315]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.225261   -0.09538704]]. Action = [[0.05216723 0.00727532 0.07919217 0.81056905]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.22518617 -0.09453893]]. Action = [[ 0.09821384 -0.03553481  0.0309579  -0.7511691 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[-0.22378007 -0.094751  ]]. Action = [[-0.03771896  0.01511014 -0.080189   -0.01398015]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 300. State = [[-0.22313878 -0.09461832]]. Action = [[ 0.0486427   0.06538583  0.07166976 -0.3319683 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.22236611 -0.09275884]]. Action = [[-0.03405847  0.08506458 -0.07793061  0.8715488 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.22225605 -0.0889653 ]]. Action = [[ 0.05428315 -0.03162219  0.00279906  0.05235171]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.22133633 -0.08777902]]. Action = [[ 0.03712464  0.06757238  0.00839897 -0.93665785]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.21994573 -0.08551122]]. Action = [[-0.04689418 -0.08570813  0.03792084  0.6752882 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 1
Current timestep = 305. State = [[-0.21968822 -0.08565207]]. Action = [[ 0.01664428 -0.02864572  0.05733373  0.985458  ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.21979553 -0.08564983]]. Action = [[ 0.02926678  0.066682   -0.09753629 -0.7791736 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.21859539 -0.08539076]]. Action = [[-0.02174888 -0.04755795  0.07079772 -0.3112408 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 308. State = [[-0.21857087 -0.08554785]]. Action = [[-0.05882829 -0.08175685 -0.02118061  0.54771936]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of 1
Current timestep = 309. State = [[-0.21871483 -0.0873016 ]]. Action = [[-0.05673708 -0.04640626 -0.0520891  -0.4288975 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of 1
Current timestep = 310. State = [[-0.21930751 -0.08956453]]. Action = [[ 0.03070312  0.06330662 -0.08885919 -0.7013729 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of 1
Current timestep = 311. State = [[-0.21933047 -0.0895735 ]]. Action = [[ 0.09615923  0.0054882  -0.05340994  0.49734712]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.21906279 -0.08947016]]. Action = [[-0.06733367  0.05102033  0.03247458  0.60996747]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.21904628 -0.08870596]]. Action = [[ 0.05308677  0.08804452 -0.09696454  0.15859497]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.2189737  -0.08584188]]. Action = [[0.09894072 0.07240091 0.04552675 0.9499309 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.21670966 -0.08262151]]. Action = [[-0.07978655 -0.02847736  0.00941694 -0.6124791 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.21654652 -0.08099191]]. Action = [[ 0.07639997 -0.02529027  0.03187769  0.4264021 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.21528088 -0.08075826]]. Action = [[-0.03077317 -0.01056824  0.076163   -0.57694995]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.21492827 -0.08090858]]. Action = [[-0.02993669 -0.06049088  0.02433877  0.8956518 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.2149033  -0.08140479]]. Action = [[ 0.02322918 -0.0826642  -0.03148085  0.7644725 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 320. State = [[-0.21396472 -0.08397841]]. Action = [[ 0.07636406 -0.04800702  0.09664226 -0.6501223 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.21182115 -0.08611677]]. Action = [[ 0.03503711 -0.06213027 -0.02501637  0.6332449 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.20952563 -0.08863163]]. Action = [[ 0.04905858  0.09632636 -0.09573107  0.6111406 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.20767912 -0.08864005]]. Action = [[-0.0092468   0.00398874 -0.08505796 -0.2503022 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 324. State = [[-0.20712985 -0.08855401]]. Action = [[-0.03814517  0.02972338 -0.05569159 -0.61934865]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.2071168  -0.08838979]]. Action = [[-0.00908078 -0.04939878 -0.00727411 -0.36671937]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.20719309 -0.08847219]]. Action = [[ 0.0708536  -0.0224966  -0.00689851 -0.47891355]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.20676434 -0.08868743]]. Action = [[-0.05657544  0.07883973  0.06534889 -0.18219924]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.2067753 -0.0882699]]. Action = [[ 0.0846022  -0.03043906 -0.0090768  -0.12342852]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.2059769  -0.08824676]]. Action = [[-0.09379761  0.03931753 -0.01630285 -0.07007992]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.20599239 -0.08792771]]. Action = [[-0.02359568  0.01508039 -0.06519066  0.785274  ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.20613065 -0.08754002]]. Action = [[-0.07545499 -0.07354747  0.04216775 -0.85984033]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.20632932 -0.08801276]]. Action = [[-0.03093407  0.0276208   0.03018735 -0.46891987]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.20642036 -0.08798515]]. Action = [[-0.04906806  0.09111407 -0.05771675  0.65338945]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of 1
Current timestep = 334. State = [[-0.20715977 -0.08648601]]. Action = [[-0.07622305 -0.08654021 -0.03347807  0.6866542 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.20887679 -0.08734792]]. Action = [[-0.0597727  -0.01584928  0.08346439 -0.86186874]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.21221165 -0.08861868]]. Action = [[ 0.0165231  -0.06743208 -0.07331304 -0.8268671 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.21294276 -0.08994579]]. Action = [[-0.07683061  0.04741091 -0.07460107  0.90924954]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.21444789 -0.0900072 ]]. Action = [[-0.01229858 -0.04041525 -0.01059654 -0.29239142]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.21572663 -0.09107802]]. Action = [[ 0.09753013 -0.02656107  0.08342863 -0.7619098 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 340. State = [[-0.21585418 -0.09191177]]. Action = [[-0.07976364  0.03215168  0.03543008 -0.26578158]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.21636522 -0.09222974]]. Action = [[ 0.01740875  0.0602458  -0.04784474 -0.5297434 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.21651246 -0.0915399 ]]. Action = [[ 3.9920308e-02  3.5535283e-02 -5.3533912e-04  8.3272481e-01]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.21661419 -0.09039209]]. Action = [[-0.05713395 -0.02284537  0.08944022 -0.57334715]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.21675721 -0.09021374]]. Action = [[-0.084735   -0.05745883  0.09408052  0.46692586]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 345. State = [[-0.21829279 -0.0914147 ]]. Action = [[ 0.04066955 -0.05682216  0.0813916   0.3981111 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.21903633 -0.09356614]]. Action = [[ 0.06315436 -0.01799323 -0.00915708 -0.96000856]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 346 of -1
Current timestep = 347. State = [[-0.21939746 -0.09497382]]. Action = [[-0.01945347  0.00785051  0.04834986  0.6414397 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 348. State = [[-0.21951823 -0.09576136]]. Action = [[ 0.08563805  0.08628429 -0.08591797 -0.64610666]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 349. State = [[-0.21959147 -0.09464462]]. Action = [[ 0.01279135  0.04239834 -0.04979969 -0.10009891]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of -1
Current timestep = 350. State = [[-0.21969822 -0.09264543]]. Action = [[ 0.08035042  0.07878452  0.04284059 -0.0566293 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 350 of -1
Current timestep = 351. State = [[-0.21936084 -0.0895605 ]]. Action = [[-0.05281805 -0.09761437 -0.09262837 -0.8677173 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.21935688 -0.08979377]]. Action = [[-0.09151099  0.07719476  0.00727707  0.81805885]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 353. State = [[-0.21957846 -0.08823919]]. Action = [[-0.00183977  0.02208145 -0.0288894  -0.18056834]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.2198213  -0.08701392]]. Action = [[ 0.05471637 -0.09051339  0.07894931  0.5268612 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.2198409  -0.08737908]]. Action = [[ 0.02636864 -0.06853688 -0.05743751  0.9830651 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.21978407 -0.08897968]]. Action = [[ 0.05577221  0.06042985 -0.0293711  -0.52869207]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 357. State = [[-0.21922018 -0.0891454 ]]. Action = [[-0.07787333 -0.0832326   0.05649025  0.1388979 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 358. State = [[-0.21932262 -0.09048116]]. Action = [[-7.8197971e-02 -9.2541143e-02 -5.0352514e-04  7.0068192e-01]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of -1
Current timestep = 359. State = [[-0.21990038 -0.09418533]]. Action = [[-0.00871831  0.00487851 -0.03191553 -0.50681764]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.22045916 -0.09587162]]. Action = [[-0.05963672 -0.0059853  -0.0814555  -0.85319495]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.22110929 -0.09730921]]. Action = [[ 9.872019e-06 -6.904678e-02  6.569073e-02  7.393266e-01]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.22229074 -0.10022996]]. Action = [[-0.08835971 -0.02937094  0.03682844  0.21624362]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.22420333 -0.10370427]]. Action = [[-0.04821992 -0.01369815  0.09001114  0.19856286]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.226204   -0.10631124]]. Action = [[ 0.03830197 -0.02992128  0.08807053  0.06077135]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.22674286 -0.10786217]]. Action = [[-0.07261694  0.08096268 -0.00848003  0.7209064 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 366. State = [[-0.2277867  -0.10765798]]. Action = [[ 0.0850196  -0.05217143 -0.03822732  0.82482994]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.22780327 -0.10819358]]. Action = [[-0.05030779 -0.07258983  0.06172799 -0.07347703]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.22845757 -0.11013963]]. Action = [[-0.07733417 -0.07483277 -0.0351634  -0.92594266]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.23078999 -0.11403254]]. Action = [[-0.03558894 -0.08321676  0.08484321  0.595515  ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.23378101 -0.11909864]]. Action = [[0.0473258  0.05680308 0.06837671 0.61289644]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.23530045 -0.12054945]]. Action = [[ 0.03887291 -0.0785163  -0.05870236  0.9058651 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.23536095 -0.12273143]]. Action = [[-0.0391515   0.0696841  -0.0071433   0.85590696]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.2362297 -0.1229882]]. Action = [[-0.04263553 -0.08213273  0.06115314  0.43751287]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.23779294 -0.12493981]]. Action = [[ 0.02593639 -0.08571479 -0.06931914 -0.33834112]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.2386469 -0.1284461]]. Action = [[-0.05003091  0.03258268 -0.0938447  -0.7840106 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, True, False, False]
State prediction error at timestep 375 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.24019738 -0.12976119]]. Action = [[-0.01615225 -0.08290879  0.05349625 -0.5587957 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, True, False, False]
State prediction error at timestep 376 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of -1
Current timestep = 377. State = [[-0.24183    -0.13314031]]. Action = [[0.02830311 0.01768098 0.07787798 0.5284319 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, True, False, False]
State prediction error at timestep 377 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 378. State = [[-0.24257067 -0.13486682]]. Action = [[-0.03841996  0.00283772 -0.01821276  0.79959345]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, True, False, False]
State prediction error at timestep 378 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.2438108 -0.1361818]]. Action = [[-0.0394979   0.04255805  0.05323911  0.6162007 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, True, False, False]
State prediction error at timestep 379 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.24525593 -0.13618706]]. Action = [[ 0.00376277 -0.07425756 -0.05396514 -0.84710205]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, True, False, False]
State prediction error at timestep 380 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.24629918 -0.13769363]]. Action = [[-0.06115905 -0.0320664  -0.06707205 -0.165802  ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, True, False, False]
State prediction error at timestep 381 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 382. State = [[-0.24812412 -0.13930976]]. Action = [[-0.05623568  0.0192145   0.08457976 -0.19224018]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, True, False, False]
State prediction error at timestep 382 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.24997285 -0.14018306]]. Action = [[-0.03382871 -0.02610483  0.06023253 -0.11398751]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, True, False, False]
State prediction error at timestep 383 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.25260016 -0.14123279]]. Action = [[ 0.07287336 -0.05670049  0.01496311 -0.7049433 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, True, False, False]
State prediction error at timestep 384 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.25310707 -0.1429957 ]]. Action = [[-0.06463923  0.09280809 -0.0842431   0.8780006 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, True, False, False]
State prediction error at timestep 385 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 386. State = [[-0.2542788  -0.14306496]]. Action = [[ 0.05580712 -0.07169154 -0.08993104  0.2741089 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, True, False, False]
State prediction error at timestep 386 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.25459117 -0.14424115]]. Action = [[-0.03644212 -0.06391867 -0.03073996 -0.76086384]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, True, False, False]
State prediction error at timestep 387 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.2551461  -0.14688262]]. Action = [[ 0.04217715 -0.09219577 -0.02034513 -0.2858119 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, True, False, False]
State prediction error at timestep 388 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.25484404 -0.15112017]]. Action = [[-0.078813   -0.05689968 -0.03851223  0.38931966]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, True, False, False]
State prediction error at timestep 389 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.25567028 -0.15554188]]. Action = [[ 0.01920185  0.04948146 -0.09346274  0.79405236]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, True, False, False]
State prediction error at timestep 390 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.25583562 -0.15663947]]. Action = [[ 0.01176856 -0.00166498  0.0790166  -0.8548146 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, True, False, False]
State prediction error at timestep 391 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 392. State = [[-0.2558876  -0.15707962]]. Action = [[ 0.03429271  0.04868085  0.04927128 -0.6796362 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, True, False, False]
State prediction error at timestep 392 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.25585243 -0.15693189]]. Action = [[ 0.05493229  0.0237985  -0.04910148 -0.71691877]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, True, False, False]
State prediction error at timestep 393 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.25572982 -0.15646578]]. Action = [[ 0.02963457  0.04730608 -0.0349731   0.62348557]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, True, False, False]
State prediction error at timestep 394 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.25517026 -0.15502325]]. Action = [[-0.0915101   0.07782888  0.09662712  0.43701065]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, True, False, False]
State prediction error at timestep 395 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 396. State = [[-0.25511858 -0.15292808]]. Action = [[-0.01519421  0.08417118 -0.04966431  0.86969805]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, True, False, False]
State prediction error at timestep 396 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.25504822 -0.14937048]]. Action = [[-0.0583318  -0.03732091  0.07797188  0.7892132 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, True, False, False]
State prediction error at timestep 397 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.25599834 -0.14834753]]. Action = [[ 0.03178556 -0.03591869  0.04329782  0.46398365]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, True, False, False]
State prediction error at timestep 398 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.2561875 -0.148277 ]]. Action = [[-0.02314316  0.02612885 -0.07799985 -0.07065958]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, True, False, False]
State prediction error at timestep 399 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 400. State = [[-0.25651574 -0.14761016]]. Action = [[0.07646651 0.09939902 0.02204968 0.52313066]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, True, False, False]
State prediction error at timestep 400 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 401. State = [[-0.2562895  -0.14426334]]. Action = [[-0.00856107  0.07211452 -0.01070026  0.4320891 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, True, False, False]
State prediction error at timestep 401 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.25628743 -0.14073735]]. Action = [[-0.0611637  -0.04641164  0.09784179  0.8474164 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, True, False, False]
State prediction error at timestep 402 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.25647733 -0.13952877]]. Action = [[-0.01912569 -0.00469939 -0.05716724 -0.39820302]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, True, False, False]
State prediction error at timestep 403 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.2568378  -0.13901022]]. Action = [[ 0.06420337 -0.05887937 -0.07393621  0.01135314]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, True, False, False]
State prediction error at timestep 404 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 405. State = [[-0.25702068 -0.13914302]]. Action = [[-0.03863332 -0.01721356  0.00786307  0.88344765]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, True, False, False]
State prediction error at timestep 405 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.2571793  -0.13952653]]. Action = [[ 0.03906953 -0.01172967  0.00165649 -0.54204637]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, True, False, False]
State prediction error at timestep 406 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.25717717 -0.1396189 ]]. Action = [[ 0.02354268  0.00709237 -0.02906401 -0.8686301 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, True, False, False]
State prediction error at timestep 407 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.2572086  -0.13979933]]. Action = [[-0.07929029 -0.07845806 -0.08075388  0.5103774 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, True, False, False]
State prediction error at timestep 408 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.2575284 -0.1419171]]. Action = [[ 0.04178385 -0.01180676  0.0083584  -0.2657932 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, True, False, False]
State prediction error at timestep 409 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.25754306 -0.14306413]]. Action = [[ 0.06593242  0.00073512 -0.07560053 -0.6169278 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, True, False, False]
State prediction error at timestep 410 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.25718158 -0.14381061]]. Action = [[-0.03905271 -0.09311041  0.02888305 -0.9409031 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, True, False, False]
State prediction error at timestep 411 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.2568027  -0.14664851]]. Action = [[-0.05321096 -0.00075691  0.02717686 -0.5635947 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, True, False, False]
State prediction error at timestep 412 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.25684062 -0.14829023]]. Action = [[ 0.09762592  0.09131982 -0.00503177 -0.9138124 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, True, False, False]
State prediction error at timestep 413 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.25649133 -0.14780425]]. Action = [[ 0.04052377  0.08777503 -0.07676749  0.7552048 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, True, False, False]
State prediction error at timestep 414 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.2558519  -0.14537907]]. Action = [[-0.03325041 -0.03281067 -0.04416398 -0.18057203]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, True, False, False]
State prediction error at timestep 415 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.25578094 -0.14537604]]. Action = [[ 0.07160769 -0.04770624  0.05416394  0.64375174]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, True, False, False]
State prediction error at timestep 416 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.25517732 -0.1455953 ]]. Action = [[-0.06687698 -0.05844107 -0.06249746  0.8476341 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, True, False, False]
State prediction error at timestep 417 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.25476715 -0.14690307]]. Action = [[ 0.06879807  0.00673654  0.02668972 -0.38309044]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, True, False, False]
State prediction error at timestep 418 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.25383788 -0.14699595]]. Action = [[-0.01606327  0.004402    0.03224409 -0.34850824]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, True, False, False]
State prediction error at timestep 419 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.2531674  -0.14707565]]. Action = [[-0.0898127   0.008821    0.09286316 -0.58267653]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, True, False, False]
State prediction error at timestep 420 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.2532541  -0.14740397]]. Action = [[-0.06441883 -0.012357    0.05430043  0.88236725]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, True, False, False]
State prediction error at timestep 421 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 421 of -1
Current timestep = 422. State = [[-0.2543793  -0.14830327]]. Action = [[-0.08970233 -0.09264626  0.00252875  0.21517909]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, True, False, False]
State prediction error at timestep 422 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.2566551  -0.15182103]]. Action = [[-0.04259956 -0.04524449 -0.0332907   0.8535539 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, True, False, False]
State prediction error at timestep 423 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of -1
Current timestep = 424. State = [[-0.25909224 -0.15545605]]. Action = [[-0.0520066   0.02172716 -0.00812172  0.6757541 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, True, False, False]
State prediction error at timestep 424 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.2611675  -0.15769556]]. Action = [[ 0.02940641  0.09629176 -0.07341245  0.81388545]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, True, False, False]
State prediction error at timestep 425 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of -1
Current timestep = 426. State = [[-0.2618117  -0.15664916]]. Action = [[-0.05682272  0.02252167 -0.06348528 -0.71397406]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, True, False, False]
State prediction error at timestep 426 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.26277193 -0.15619077]]. Action = [[-0.08597063 -0.09777933 -0.05931235 -0.04101425]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, True, False, False]
State prediction error at timestep 427 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.26564935 -0.15754767]]. Action = [[-0.0104299   0.00624418 -0.02125866 -0.99672645]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, True, False, False]
State prediction error at timestep 428 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.26869774 -0.1578914 ]]. Action = [[-0.06846146  0.0743954  -0.08570275  0.7924454 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, True, False, False]
State prediction error at timestep 429 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.27349794 -0.15628527]]. Action = [[-0.04550408  0.08348148  0.02898707  0.7010906 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, True, False, False]
State prediction error at timestep 430 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.2782593 -0.1533182]]. Action = [[-0.07730119 -0.07122524  0.07479446 -0.35203147]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, True, False, False]
State prediction error at timestep 431 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of -1
Current timestep = 432. State = [[-0.28282958 -0.15259936]]. Action = [[-0.02661659  0.07990218  0.02004075 -0.7426384 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, True, False, False]
State prediction error at timestep 432 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.28622922 -0.1512762 ]]. Action = [[-3.4694374e-04  8.7776236e-02 -7.0983127e-02  8.2872212e-01]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, True, False, False]
State prediction error at timestep 433 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.28725806 -0.14820355]]. Action = [[ 0.06508828  0.04517572  0.04556293 -0.74675053]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, True, False, False]
State prediction error at timestep 434 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 435. State = [[-0.28728148 -0.14532842]]. Action = [[-0.04720899 -0.08438589 -0.03438473 -0.09229732]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, True, False, False]
State prediction error at timestep 435 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.28733346 -0.14521433]]. Action = [[-0.07950035  0.05868819 -0.09433084 -0.84499025]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, True, False, False]
State prediction error at timestep 436 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of -1
Current timestep = 437. State = [[-0.28830418 -0.14400159]]. Action = [[-0.01467379  0.0688436   0.07291739  0.8748269 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, True, False, False]
State prediction error at timestep 437 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.28962088 -0.14008729]]. Action = [[ 0.08298445  0.075944   -0.01648436  0.3732288 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, True, False, False]
State prediction error at timestep 438 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.28961802 -0.13654166]]. Action = [[0.01260839 0.07534719 0.08059237 0.48052037]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, True, False, False]
State prediction error at timestep 439 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.2897895  -0.13210875]]. Action = [[-0.06642772 -0.04197332 -0.08038983  0.9050833 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, True, False, False]
State prediction error at timestep 440 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 441. State = [[-0.29015926 -0.13075718]]. Action = [[-0.06925927  0.00139247  0.09608958 -0.8899838 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, True, False, False]
State prediction error at timestep 441 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 441 of -1
Current timestep = 442. State = [[-0.29117504 -0.13010575]]. Action = [[ 0.02554894  0.06719971 -0.00932993  0.26547837]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, True, False, False]
State prediction error at timestep 442 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.29195446 -0.12746681]]. Action = [[-0.07692629  0.08604132  0.04105226  0.29761064]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, True, False, False]
State prediction error at timestep 443 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 444. State = [[-0.29418164 -0.12320179]]. Action = [[ 0.0213658   0.09221443 -0.04885458  0.484174  ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 445. State = [[-0.2965625  -0.11804108]]. Action = [[ 0.05339909 -0.05270028 -0.07619837  0.6197084 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.29778025 -0.11625434]]. Action = [[ 0.07568965 -0.01220496 -0.06157818 -0.55448574]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.29759672 -0.11578104]]. Action = [[-0.0653633   0.0783454   0.08554778  0.89409435]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.2984893  -0.11349846]]. Action = [[ 0.06121416 -0.08621938 -0.09685274 -0.64087695]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 449. State = [[-0.29856396 -0.11356039]]. Action = [[-0.04272811 -0.02237193  0.0389433   0.6441355 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 450. State = [[-0.29933542 -0.11385778]]. Action = [[ 0.06561885  0.06920586 -0.04633348  0.24289072]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of -1
Current timestep = 451. State = [[-0.29929677 -0.11333204]]. Action = [[-0.0433543  -0.07445595 -0.06913181 -0.7856771 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.29956257 -0.11380199]]. Action = [[ 0.08671881 -0.02478214  0.08237969 -0.7838485 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 453. State = [[-0.2988231  -0.11398216]]. Action = [[-0.00697968  0.08277646  0.05541211 -0.9243256 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.2986954  -0.11395796]]. Action = [[-0.04496355 -0.09678542 -0.0981553   0.1391139 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[-0.29908437 -0.11510243]]. Action = [[-0.09488786 -0.08605885  0.0501936   0.5115447 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.300911   -0.11811347]]. Action = [[ 0.04364493 -0.09411883  0.02610438  0.93483067]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.3015876  -0.12206213]]. Action = [[-0.03484283  0.03862073 -0.00263964  0.73690367]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 458. State = [[-0.3023155  -0.12294116]]. Action = [[ 0.01315336  0.08264697  0.05027581 -0.6423313 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.3026104  -0.12236751]]. Action = [[ 0.03731968  0.08992185 -0.04964188  0.28263652]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.3026067  -0.12034907]]. Action = [[-0.03580943  0.06058333  0.05925319  0.03092408]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.3028924  -0.11777348]]. Action = [[ 3.4809537e-02 -4.0054321e-04  6.6327676e-03 -4.2856276e-01]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 462. State = [[-0.302971   -0.11631798]]. Action = [[-0.09197826 -0.02814486  0.09111279 -0.4111694 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.30391574 -0.11611193]]. Action = [[ 0.00256109 -0.07591815  0.06118786 -0.8593691 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.30451065 -0.11648279]]. Action = [[ 0.06060546  0.0794002   0.06712449 -0.6819025 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.30432427 -0.11606009]]. Action = [[ 0.01179849  0.00742742 -0.03929676  0.9145757 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.3042704  -0.11564687]]. Action = [[-0.0594007   0.06750473 -0.04319017 -0.9830991 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.3043719  -0.11330768]]. Action = [[ 0.08626264  0.07298527  0.01951742 -0.7140183 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[-0.30448678 -0.10970111]]. Action = [[-0.01925935  0.0486378   0.00342906 -0.8250598 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.30475983 -0.1065606 ]]. Action = [[ 0.09602373 -0.06207625 -0.07255562 -0.714503  ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 470. State = [[-0.30415824 -0.10560673]]. Action = [[ 0.08424141  0.00587673 -0.07321163 -0.26955116]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.30287516 -0.10502527]]. Action = [[-0.08914234 -0.01230638 -0.02312431  0.7654532 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[-0.3027426 -0.1049957]]. Action = [[ 0.05007949 -0.01508667 -0.06394768 -0.67579156]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 473. State = [[-0.30234843 -0.10498201]]. Action = [[-0.08798551  0.04540891  0.04987509 -0.19735152]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.30248427 -0.10445895]]. Action = [[-0.02553444  0.06399625 -0.04021091 -0.4802029 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.30272096 -0.10224571]]. Action = [[-0.08170478  0.06190678 -0.08740638  0.30180955]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.30358803 -0.09888963]]. Action = [[ 0.02366445  0.07484198 -0.08571056 -0.3450495 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.30410287 -0.09489803]]. Action = [[-0.07305692  0.09131867 -0.04402177  0.32207572]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.30527464 -0.08960416]]. Action = [[-0.0055763  -0.03975102 -0.00163583  0.14995492]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.3063954  -0.08726899]]. Action = [[ 0.09211399 -0.01430057  0.03685714 -0.06330401]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.30648968 -0.08663403]]. Action = [[0.02641255 0.05951565 0.08067096 0.5117724 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.3064684  -0.08473446]]. Action = [[-0.00239786  0.04149651 -0.05713701  0.55484104]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.306617  -0.0828284]]. Action = [[ 0.08483202 -0.05998424 -0.0786375   0.642287  ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.30545035 -0.08282493]]. Action = [[ 0.04638126 -0.09379979  0.06776816  0.5907974 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.3031246  -0.08410382]]. Action = [[ 0.07898133 -0.07893163  0.07344503 -0.8274601 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 485. State = [[-0.30094063 -0.08424827]]. Action = [[ 0.02760413 -0.05174959  0.08642397 -0.6339511 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.29848552 -0.08677521]]. Action = [[ 0.0391222   0.05846048  0.05675138 -0.5961751 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.29589108 -0.08684438]]. Action = [[-0.07829283  0.04659332 -0.07187687  0.79691744]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[-0.2960445 -0.0847957]]. Action = [[-0.05328012  0.08804717  0.08722918 -0.94111574]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 489. State = [[-0.29668272 -0.08270814]]. Action = [[-0.02515738  0.09528191  0.08357092 -0.7202889 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[-0.29740155 -0.07876995]]. Action = [[-0.07915999 -0.08646066  0.02789853 -0.851377  ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.2984489  -0.07547818]]. Action = [[ 0.05975785 -0.03716907  0.07993499  0.9215238 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.29905096 -0.07376848]]. Action = [[-0.0508378   0.06160109 -0.09112477  0.0076139 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[-0.29936472 -0.07177231]]. Action = [[-0.04906655  0.07501649  0.03421003 -0.84919566]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.30025005 -0.06934212]]. Action = [[0.05825279 0.05018175 0.05219003 0.32599127]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[-0.3009243  -0.06644604]]. Action = [[-0.04015876  0.06235144  0.084248    0.53577757]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.30164582 -0.06292304]]. Action = [[ 0.09555096  0.06951056 -0.04608388 -0.31694275]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.30235404 -0.05944644]]. Action = [[-0.07207023 -0.05669204  0.08249963 -0.59341913]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.30299574 -0.05716903]]. Action = [[ 0.08528393 -0.01959219  0.05796558 -0.8962876 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.30322573 -0.05617009]]. Action = [[ 0.00754721 -0.07185981  0.05420939  0.20327473]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.3030387  -0.05623347]]. Action = [[ 0.00937204  0.01757944 -0.0261976  -0.93349326]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.30294585 -0.05623692]]. Action = [[-0.02267791  0.04706257  0.09236867  0.30567443]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.3028932  -0.05618348]]. Action = [[ 0.07985368  0.04004989 -0.07309318 -0.38350976]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.30180246 -0.05608446]]. Action = [[ 0.00109655 -0.03808236 -0.01021523  0.8445705 ]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 504. State = [[-0.30023512 -0.05723808]]. Action = [[-0.08899235  0.0902139   0.08552986 -0.75861394]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.30054903 -0.05488245]]. Action = [[-0.02954407 -0.07595496  0.02568687 -0.74531204]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 505 of -1
Current timestep = 506. State = [[-0.3008668  -0.05398738]]. Action = [[-0.00475792 -0.05405118 -0.09545728  0.88928866]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.30120775 -0.05432062]]. Action = [[-0.03567751  0.03897741 -0.06659324 -0.35682762]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.3014224  -0.05442422]]. Action = [[-0.04382609 -0.05687196  0.08439706 -0.95363563]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.30210418 -0.05559587]]. Action = [[-0.08777242 -0.01139678  0.0791605  -0.6977894 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 510. State = [[-0.30388194 -0.05713473]]. Action = [[ 0.00835206  0.02634265 -0.09131993  0.30224466]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.30520213 -0.05791621]]. Action = [[-0.00187697  0.01964281  0.09708474 -0.8711548 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.30571762 -0.05816177]]. Action = [[ 0.08551974  0.07365825  0.02847419 -0.22096515]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.30598912 -0.05746085]]. Action = [[ 0.00905132  0.04667716  0.03910359 -0.14279735]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.3062164  -0.05608609]]. Action = [[0.0501029  0.03997118 0.01208761 0.71720815]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.30641615 -0.05466075]]. Action = [[-0.04022678  0.05900981  0.0697547   0.8690531 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.30698967 -0.05236433]]. Action = [[ 0.09327243 -0.0888631   0.03361266  0.5753405 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 517. State = [[-0.3070402  -0.05083499]]. Action = [[-0.05655187  0.0353092   0.07238884  0.21530342]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.30714467 -0.04966781]]. Action = [[-0.06263198  0.01968183  0.07253235  0.6229789 ]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.3074775  -0.04844621]]. Action = [[-0.00197416 -0.08048853  0.06607205 -0.11721075]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 520. State = [[-0.30762815 -0.04835244]]. Action = [[ 0.09678035 -0.01832835 -0.01069462  0.6656289 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of -1
Current timestep = 521. State = [[-0.30756164 -0.04835347]]. Action = [[ 0.03603692  0.06768418  0.0221739  -0.02032858]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of -1
Current timestep = 522. State = [[-0.3070532  -0.04826466]]. Action = [[ 0.02188478  0.07974852 -0.05785265 -0.36648965]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.30610192 -0.04670922]]. Action = [[ 0.0690105  -0.02011187  0.00446822 -0.3938383 ]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.3042701  -0.04533937]]. Action = [[ 0.04961012  0.08092845  0.06703282 -0.49587262]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.30140135 -0.0460921 ]]. Action = [[ 0.07282104 -0.09089103 -0.07187023  0.9141021 ]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 526. State = [[-0.2970062  -0.04768002]]. Action = [[-0.06249657 -0.09876384 -0.0112173  -0.60421836]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 526 of -1
Current timestep = 527. State = [[-0.2941958  -0.05082108]]. Action = [[-0.07701226 -0.03269901  0.08694772 -0.5911417 ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of -1
Current timestep = 528. State = [[-0.2948135  -0.05072184]]. Action = [[ 0.09760898 -0.04533196  0.01350839  0.20056033]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.2935085  -0.05411399]]. Action = [[0.073721   0.04172707 0.0526308  0.56420386]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.2904196  -0.05640643]]. Action = [[ 0.05390883  0.05765618 -0.01040737  0.47219205]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(9.2861e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.2875179  -0.05701426]]. Action = [[ 0.08708172 -0.05970553  0.03262716 -0.550261  ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.28367683 -0.05790593]]. Action = [[ 0.03042754  0.03170519  0.08167917 -0.34564543]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(5.1161e-05, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.28002617 -0.05821813]]. Action = [[ 0.06332152  0.05509313 -0.07183299 -0.46320784]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(9.4560e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of 1
Current timestep = 534. State = [[-0.27652326 -0.0577409 ]]. Action = [[-0.05475853  0.04220396  0.06884251  0.67152977]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(2.8731e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.27493927 -0.0560344 ]]. Action = [[ 0.07241137  0.08778565 -0.02542361  0.9055004 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(1.2961e-06, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.27300486 -0.05330713]]. Action = [[ 0.0978395   0.00770888 -0.04456825  0.7307389 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(2.1612e-05, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.26947844 -0.05178734]]. Action = [[0.02915611 0.00352921 0.02595828 0.53824854]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(1.5457e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.2661236  -0.05053633]]. Action = [[ 0.06525178 -0.05256644 -0.03064671 -0.6059016 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(3.4930e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of 1
Current timestep = 539. State = [[-0.2633018  -0.05032637]]. Action = [[-0.01438393  0.03550436 -0.03433032 -0.8623954 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(3.9477e-05, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.26153493 -0.04975955]]. Action = [[-0.04566889 -0.07082461 -0.08440805  0.17860794]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(1.3277e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 540 of 1
Current timestep = 541. State = [[-0.26110348 -0.05024293]]. Action = [[ 0.08693013 -0.08141378 -0.07294399 -0.43754488]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(3.8807e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 541 of 1
Current timestep = 542. State = [[-0.25909784 -0.05224212]]. Action = [[-0.08962483 -0.014397   -0.0965701  -0.31304705]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(3.4021e-05, grad_fn=<MseLossBackward0>)
Current timestep = 543. State = [[-0.25817567 -0.05353064]]. Action = [[-0.02007627  0.01242526 -0.02243355  0.5052719 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(4.1952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.25780696 -0.05398883]]. Action = [[-0.02592669 -0.02512152 -0.05199788 -0.2815951 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(3.4124e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of 1
Current timestep = 545. State = [[-0.2576192  -0.05499186]]. Action = [[ 0.05431538 -0.07099923  0.00711495  0.19300365]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(1.8213e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of 1
Current timestep = 546. State = [[-0.256974   -0.05771843]]. Action = [[ 0.08763038 -0.03002407 -0.05380836 -0.7663362 ]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(8.3588e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 546 of 1
Current timestep = 547. State = [[-0.2556632  -0.05999471]]. Action = [[ 0.04361971 -0.08208938 -0.00363375  0.480821  ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.25408    -0.06332091]]. Action = [[ 0.04067745 -0.02803271 -0.08742495 -0.7079297 ]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[-0.2517743  -0.06643792]]. Action = [[-0.03751728 -0.03321106  0.01537131 -0.6452907 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(5.4276e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[-0.25133762 -0.06936064]]. Action = [[-0.08212171 -0.08947765 -0.05859855  0.23577464]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(6.4798e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.2511661  -0.07381232]]. Action = [[-0.01627503 -0.06771934 -0.06890963 -0.02126205]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(4.0310e-05, grad_fn=<MseLossBackward0>)
Current timestep = 552. State = [[-0.25088117 -0.07848908]]. Action = [[-0.05550973 -0.0914944  -0.07969815  0.29063487]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.250541   -0.08356502]]. Action = [[ 0.00898019  0.05745334 -0.02552144  0.9396293 ]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of 1
Current timestep = 554. State = [[-0.250333   -0.08508451]]. Action = [[-0.01309    -0.06977382  0.0767811   0.8375659 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.2501419  -0.08788525]]. Action = [[-0.08163007 -0.03656013  0.00388125  0.47476554]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 1
Current timestep = 556. State = [[-0.2512201  -0.09126212]]. Action = [[-0.0474002  -0.08615414 -0.04707079 -0.29339564]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(7.6324e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of 1
Current timestep = 557. State = [[-0.25328922 -0.09556966]]. Action = [[ 0.08981795 -0.03481     0.02398159  0.554814  ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.25333735 -0.09880813]]. Action = [[-0.0931619   0.03774203 -0.06488965  0.52886367]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(2.6928e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of 1
Current timestep = 559. State = [[-0.25501513 -0.10014988]]. Action = [[-0.03996889 -0.05957381 -0.03586294  0.64247   ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.25738004 -0.1027369 ]]. Action = [[-0.01011816 -0.06005792 -0.0925002  -0.2777629 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(2.1447e-05, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.26013353 -0.10660025]]. Action = [[ 0.08502796 -0.03851253  0.08737511  0.47885013]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(6.2398e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of 1
Current timestep = 562. State = [[-0.26025575 -0.10934536]]. Action = [[-0.08664517 -0.04558402 -0.04880156 -0.26603293]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(4.4052e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 562 of 1
Current timestep = 563. State = [[-0.2616895  -0.11258697]]. Action = [[-0.04780278 -0.02384555 -0.07125823  0.5166404 ]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(5.9188e-05, grad_fn=<MseLossBackward0>)
Current timestep = 564. State = [[-0.2629018  -0.11532692]]. Action = [[ 0.08272126 -0.07055885  0.02383727 -0.36468506]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(6.0083e-07, grad_fn=<MseLossBackward0>)
Current timestep = 565. State = [[-0.26269802 -0.11832343]]. Action = [[0.08964188 0.08489505 0.04053711 0.73351765]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(4.4035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 565 of 1
Current timestep = 566. State = [[-0.2624633 -0.1184997]]. Action = [[-0.06510764 -0.05663028 -0.09659233  0.08758879]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(1.9849e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of 1
Current timestep = 567. State = [[-0.26184472 -0.11978666]]. Action = [[ 0.00087734 -0.08587755 -0.01619841 -0.07211936]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(2.9575e-06, grad_fn=<MseLossBackward0>)
Current timestep = 568. State = [[-0.26143336 -0.12309745]]. Action = [[0.06334651 0.02052376 0.09610809 0.2820078 ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(3.2443e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 568 of 1
Current timestep = 569. State = [[-0.2608296  -0.12487134]]. Action = [[ 0.04265512 -0.04164533  0.0694254   0.21550953]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(1.0539e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 569 of 1
Current timestep = 570. State = [[-0.26032922 -0.12706518]]. Action = [[-0.06380091 -0.04963171 -0.05450621 -0.740188  ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, True, False, False]
State prediction error at timestep 570 is tensor(1.6206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.26010412 -0.13003026]]. Action = [[-0.03189386 -0.03873008  0.07515196  0.08852077]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, True, False, False]
State prediction error at timestep 571 is tensor(4.3886e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.26005164 -0.13306557]]. Action = [[ 0.08451476 -0.04483265  0.00814986  0.15239501]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, True, False, False]
State prediction error at timestep 572 is tensor(6.6504e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of 1
Current timestep = 573. State = [[-0.25988144 -0.13604462]]. Action = [[-0.05357808 -0.0350055  -0.00949681 -0.77243036]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, True, False, False]
State prediction error at timestep 573 is tensor(6.5803e-05, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.25986132 -0.13905065]]. Action = [[-0.07122524 -0.05749454  0.00371059 -0.7107743 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, True, False, False]
State prediction error at timestep 574 is tensor(3.7598e-05, grad_fn=<MseLossBackward0>)
Current timestep = 575. State = [[-0.26055926 -0.14225158]]. Action = [[0.05546226 0.09464762 0.04744852 0.01533878]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, True, False, False]
State prediction error at timestep 575 is tensor(5.3202e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of 1
Current timestep = 576. State = [[-0.26054174 -0.14244837]]. Action = [[-0.0163271  -0.09121658 -0.01745109  0.94173014]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, True, False, False]
State prediction error at timestep 576 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.26061717 -0.1444977 ]]. Action = [[-0.02793653 -0.07503766 -0.00559764 -0.29493034]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, True, False, False]
State prediction error at timestep 577 is tensor(2.1137e-05, grad_fn=<MseLossBackward0>)
Current timestep = 578. State = [[-0.2607508 -0.1474626]]. Action = [[ 0.02957635 -0.00586756  0.00741981  0.94675756]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, True, False, False]
State prediction error at timestep 578 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 579. State = [[-0.26084957 -0.14973958]]. Action = [[-0.02914643 -0.07058705  0.0648048  -0.0668605 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, True, False, False]
State prediction error at timestep 579 is tensor(7.0662e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 579 of 0
Current timestep = 580. State = [[-0.26124576 -0.15292865]]. Action = [[0.05744689 0.0767448  0.00222536 0.42811608]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, True, False, False]
State prediction error at timestep 580 is tensor(2.9489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 0
Current timestep = 581. State = [[-0.2612649  -0.15322822]]. Action = [[-0.07921996 -0.04389234 -0.05337421 -0.25145411]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, True, False, False]
State prediction error at timestep 581 is tensor(7.5987e-06, grad_fn=<MseLossBackward0>)
Current timestep = 582. State = [[-0.2617012  -0.15372805]]. Action = [[-0.02192888  0.04086211 -0.09116285 -0.80404097]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, True, False, False]
State prediction error at timestep 582 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 583. State = [[-0.2625207  -0.15361403]]. Action = [[-0.00594515  0.08888755  0.05969471  0.8240316 ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, True, False, False]
State prediction error at timestep 583 is tensor(2.5993e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of 0
Current timestep = 584. State = [[-0.26319674 -0.15240216]]. Action = [[ 0.0960558   0.06352163 -0.05298638 -0.39139396]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, True, False, False]
State prediction error at timestep 584 is tensor(7.7858e-05, grad_fn=<MseLossBackward0>)
Current timestep = 585. State = [[-0.26268944 -0.15033022]]. Action = [[ 0.09682731 -0.03035412 -0.0911652  -0.13203883]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, True, False, False]
State prediction error at timestep 585 is tensor(4.7841e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[-0.26181912 -0.14915475]]. Action = [[ 0.06305473  0.0369175  -0.05952105 -0.07563365]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, True, False, False]
State prediction error at timestep 586 is tensor(6.9198e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.2601145  -0.14772944]]. Action = [[-0.07933915  0.0484981   0.0874645   0.20292759]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, True, False, False]
State prediction error at timestep 587 is tensor(2.2034e-05, grad_fn=<MseLossBackward0>)
Current timestep = 588. State = [[-0.25985005 -0.14600803]]. Action = [[ 0.06525449 -0.03619323  0.00043178  0.31008673]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, True, False, False]
State prediction error at timestep 588 is tensor(5.7015e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.2592839  -0.14542782]]. Action = [[-0.02042557 -0.03835139  0.01528881 -0.7365095 ]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, True, False, False]
State prediction error at timestep 589 is tensor(5.6944e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.25924027 -0.14549264]]. Action = [[-0.05692046  0.03810606  0.09224234 -0.47126138]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, True, False, False]
State prediction error at timestep 590 is tensor(1.1204e-05, grad_fn=<MseLossBackward0>)
Current timestep = 591. State = [[-0.25921738 -0.14552958]]. Action = [[ 0.08167148 -0.0725708  -0.07557771  0.1531794 ]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, True, False, False]
State prediction error at timestep 591 is tensor(5.7391e-05, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.2591355 -0.1457263]]. Action = [[ 0.05821054 -0.09271973  0.01158869 -0.38030386]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, True, False, False]
State prediction error at timestep 592 is tensor(5.8327e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.25774935 -0.14801113]]. Action = [[-0.03760448 -0.00764479  0.00707626  0.25319326]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, True, False, False]
State prediction error at timestep 593 is tensor(2.3708e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.2576365  -0.14927402]]. Action = [[ 0.05048151 -0.09421509 -0.03594915  0.18925726]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, True, False, False]
State prediction error at timestep 594 is tensor(3.3850e-05, grad_fn=<MseLossBackward0>)
Current timestep = 595. State = [[-0.25629684 -0.15301558]]. Action = [[-0.02677338 -0.05544356  0.07609924 -0.7668148 ]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, True, False, False]
State prediction error at timestep 595 is tensor(2.0503e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.25574002 -0.15650947]]. Action = [[ 0.0146018   0.06405868  0.08986986 -0.8288665 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, True, False, False]
State prediction error at timestep 596 is tensor(9.0780e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of 1
Current timestep = 597. State = [[-0.25498307 -0.15739393]]. Action = [[ 0.09572455 -0.07731964  0.08873477  0.2738229 ]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, True, False, False]
State prediction error at timestep 597 is tensor(4.6016e-05, grad_fn=<MseLossBackward0>)
Current timestep = 598. State = [[-0.25293645 -0.15983385]]. Action = [[-0.03631128 -0.03946638  0.03581437  0.28742933]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, True, False, False]
State prediction error at timestep 598 is tensor(4.9687e-05, grad_fn=<MseLossBackward0>)
Current timestep = 599. State = [[-0.25200957 -0.16222519]]. Action = [[-0.08787441 -0.07480643 -0.08592144 -0.9616601 ]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, True, False, False]
State prediction error at timestep 599 is tensor(7.6739e-05, grad_fn=<MseLossBackward0>)
Current timestep = 600. State = [[-0.2521203  -0.16582066]]. Action = [[ 0.06993843  0.01403041 -0.07837921 -0.87420243]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, True, False, False]
State prediction error at timestep 600 is tensor(5.2745e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.25222927 -0.1678045 ]]. Action = [[ 0.02952553 -0.07008544 -0.03630762 -0.8403172 ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, True, False, False]
State prediction error at timestep 601 is tensor(8.3709e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 601 of 1
Current timestep = 602. State = [[-0.25117967 -0.1709832 ]]. Action = [[-0.04911628  0.06822445 -0.09405292 -0.24227518]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, True, False, False]
State prediction error at timestep 602 is tensor(2.3025e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of 1
Current timestep = 603. State = [[-0.25105777 -0.17108637]]. Action = [[-0.07787421 -0.03266825 -0.08717435 -0.5054105 ]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, True, False, False]
State prediction error at timestep 603 is tensor(1.6432e-05, grad_fn=<MseLossBackward0>)
Current timestep = 604. State = [[-0.25127748 -0.17214902]]. Action = [[ 0.05796636 -0.02370276  0.09052425 -0.09591079]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, True, False, False]
State prediction error at timestep 604 is tensor(1.0812e-05, grad_fn=<MseLossBackward0>)
Current timestep = 605. State = [[-0.25142348 -0.17313239]]. Action = [[-0.02768189  0.0244082   0.05293632  0.9749489 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, True, False, False]
State prediction error at timestep 605 is tensor(9.3876e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of 1
Current timestep = 606. State = [[-0.25144908 -0.17337331]]. Action = [[-0.0058394  -0.00846668 -0.08194025 -0.78696626]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, True, False, False]
State prediction error at timestep 606 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of 1
Current timestep = 607. State = [[-0.2513756 -0.173549 ]]. Action = [[-0.06321235  0.06612451  0.07934103 -0.7892898 ]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, True, False, False]
State prediction error at timestep 607 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of 1
Current timestep = 608. State = [[-0.25151974 -0.17348804]]. Action = [[-0.07933845 -0.08284909  0.01331183  0.25475144]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, True, False, False]
State prediction error at timestep 608 is tensor(4.1677e-05, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.25284252 -0.17505734]]. Action = [[ 0.04632006 -0.07989693  0.06902749 -0.75861233]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, True, False, False]
State prediction error at timestep 609 is tensor(6.2635e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 1
Current timestep = 610. State = [[-0.25359502 -0.17799771]]. Action = [[ 0.03413291 -0.08701842 -0.0705953   0.00471807]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, True, False, False]
State prediction error at timestep 610 is tensor(1.1337e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of 1
Current timestep = 611. State = [[-0.25376257 -0.18210448]]. Action = [[ 0.09530912  0.05733917 -0.0580317  -0.8981169 ]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, True, False, False]
State prediction error at timestep 611 is tensor(7.7669e-05, grad_fn=<MseLossBackward0>)
Current timestep = 612. State = [[-0.2535274  -0.18296684]]. Action = [[-6.5504208e-02 -4.2182207e-04 -6.0860664e-03  6.0141921e-01]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, True, False, False]
State prediction error at timestep 612 is tensor(6.3799e-05, grad_fn=<MseLossBackward0>)
Current timestep = 613. State = [[-0.25351316 -0.1835852 ]]. Action = [[-0.05722279 -0.04026232 -0.08495338 -0.5862053 ]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, True, False, False]
State prediction error at timestep 613 is tensor(5.3499e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of -1
Current timestep = 614. State = [[-0.25365376 -0.18489392]]. Action = [[-0.07401918 -0.00821207  0.02197238  0.8698468 ]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, True, False, False]
State prediction error at timestep 614 is tensor(9.5291e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.25478038 -0.18654312]]. Action = [[ 0.05193066 -0.05281587  0.03962482  0.54167056]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, True, False, False]
State prediction error at timestep 615 is tensor(8.1411e-05, grad_fn=<MseLossBackward0>)
Current timestep = 616. State = [[-0.25500146 -0.18990374]]. Action = [[-0.0326838  -0.04692046  0.0609094  -0.18372655]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, True, False, False]
State prediction error at timestep 616 is tensor(3.0503e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of -1
Current timestep = 617. State = [[-0.25545767 -0.19265306]]. Action = [[-0.07087652  0.07879598 -0.0419869   0.578876  ]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, True, False, False]
State prediction error at timestep 617 is tensor(2.2389e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.2565518  -0.19277419]]. Action = [[ 0.04757788 -0.06001249  0.08714678 -0.07596654]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, True, False, False]
State prediction error at timestep 618 is tensor(9.8124e-06, grad_fn=<MseLossBackward0>)
Current timestep = 619. State = [[-0.25678596 -0.19358957]]. Action = [[ 0.03936093  0.07268574  0.09530833 -0.6631332 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, True, False, False]
State prediction error at timestep 619 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of -1
Current timestep = 620. State = [[-0.2566627  -0.19322668]]. Action = [[-0.02370206  0.06624935 -0.03694066  0.70343065]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, True, False, False]
State prediction error at timestep 620 is tensor(5.4952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 621. State = [[-0.2566333  -0.19210064]]. Action = [[ 0.01425396 -0.09935362 -0.08323322 -0.02149171]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, True, False, False]
State prediction error at timestep 621 is tensor(1.4545e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.25680432 -0.19255891]]. Action = [[-0.05780643  0.08079112  0.02811315 -0.62189955]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, True, False, False]
State prediction error at timestep 622 is tensor(9.9591e-05, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.25740254 -0.19207737]]. Action = [[-0.05897969 -0.08564863  0.08024535 -0.31566024]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, True, False, False]
State prediction error at timestep 623 is tensor(5.6176e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.25883   -0.1933218]]. Action = [[-0.01012775 -0.08342892 -0.04067057  0.667866  ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, True, False, False]
State prediction error at timestep 624 is tensor(7.0816e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of -1
Current timestep = 625. State = [[-0.2604882 -0.195963 ]]. Action = [[-0.07280369 -0.03337868 -0.08680453  0.62933326]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, True, False, False]
State prediction error at timestep 625 is tensor(3.4048e-05, grad_fn=<MseLossBackward0>)
Current timestep = 626. State = [[-0.26341188 -0.19876026]]. Action = [[-0.01285782  0.01465897  0.06866116  0.6126299 ]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, True, False, False]
State prediction error at timestep 626 is tensor(2.8235e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.2655481  -0.20050174]]. Action = [[ 0.04568496 -0.05721226 -0.0756397  -0.95752   ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, True, False, False]
State prediction error at timestep 627 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.2663144  -0.20241877]]. Action = [[ 0.03909267 -0.06658019 -0.0550125   0.85963106]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, True, False, False]
State prediction error at timestep 628 is tensor(5.7671e-05, grad_fn=<MseLossBackward0>)
Current timestep = 629. State = [[-0.26667538 -0.20498839]]. Action = [[-0.08176665  0.05566587  0.05953973 -0.35859752]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, True, False, False]
State prediction error at timestep 629 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 630. State = [[-0.26779875 -0.20580094]]. Action = [[-0.05106115 -0.05168626 -0.06762305  0.7201979 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, True, False, False]
State prediction error at timestep 630 is tensor(3.1858e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[-0.26982468 -0.20778717]]. Action = [[ 0.01755778  0.03457744 -0.05384321 -0.6128071 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, True, False, False]
State prediction error at timestep 631 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 632. State = [[-0.27064845 -0.20787641]]. Action = [[0.03228302 0.00556333 0.05344992 0.7615479 ]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, True, False, False]
State prediction error at timestep 632 is tensor(3.7588e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.2707769  -0.20784159]]. Action = [[ 0.04212461 -0.01541694  0.07545937  0.3906983 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, True, False, False]
State prediction error at timestep 633 is tensor(1.8690e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.27073577 -0.20789391]]. Action = [[-0.02470771  0.03848713 -0.09703758  0.8991047 ]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, True, False, False]
State prediction error at timestep 634 is tensor(2.1574e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.270754  -0.2078359]]. Action = [[ 0.01999728  0.00824702 -0.01009895 -0.9302053 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, True, False, False]
State prediction error at timestep 635 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.270754  -0.2078359]]. Action = [[ 0.09624589 -0.04745208  0.03188258  0.94268477]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, True, False, False]
State prediction error at timestep 636 is tensor(4.9814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of -1
Current timestep = 637. State = [[-0.27058396 -0.20791633]]. Action = [[ 0.04671652 -0.07997781 -0.02292027  0.6550622 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, True, False, False]
State prediction error at timestep 637 is tensor(4.0008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[-0.26981226 -0.20915264]]. Action = [[-0.02758513  0.00320314  0.07950086 -0.5377875 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, True, False, False]
State prediction error at timestep 638 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.2692163  -0.21007185]]. Action = [[ 0.05729612  0.00832258 -0.02107619  0.06415319]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, True, False, False]
State prediction error at timestep 639 is tensor(4.6653e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.26814047 -0.20968494]]. Action = [[ 0.04604734 -0.04698014 -0.08308103 -0.708995  ]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, True, False, False]
State prediction error at timestep 640 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[-0.2664897  -0.21079329]]. Action = [[-0.03948031  0.0613453   0.03571602  0.2033931 ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, True, False, False]
State prediction error at timestep 641 is tensor(2.8051e-05, grad_fn=<MseLossBackward0>)
Current timestep = 642. State = [[-0.26641038 -0.21102667]]. Action = [[ 0.05044355 -0.04471269 -0.00997461  0.69173384]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, True, False, False]
State prediction error at timestep 642 is tensor(5.6319e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[-0.2655884  -0.21201108]]. Action = [[ 0.03584061  0.02930436 -0.07901141  0.71863306]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, True, False, False]
State prediction error at timestep 643 is tensor(4.2232e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[-0.26442125 -0.21232873]]. Action = [[-0.07436576 -0.07268678  0.01718185  0.7181976 ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, True, False, False]
State prediction error at timestep 644 is tensor(2.5471e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of 1
Current timestep = 645. State = [[-0.26438388 -0.21359742]]. Action = [[ 0.02843196  0.09088082 -0.07413711 -0.4837705 ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, True, False, False]
State prediction error at timestep 645 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 646. State = [[-0.2643273  -0.21332042]]. Action = [[-0.01919784 -0.02626134 -0.0335934  -0.04387671]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, True, False, False]
State prediction error at timestep 646 is tensor(3.9606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 647. State = [[-0.26434064 -0.21340175]]. Action = [[-0.02007223 -0.07300774 -0.05159776  0.24331653]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, True, False, False]
State prediction error at timestep 647 is tensor(2.5807e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of 1
Current timestep = 648. State = [[-0.26452124 -0.21444677]]. Action = [[-0.0665139   0.04811729 -0.05565966  0.71700263]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, True, False, False]
State prediction error at timestep 648 is tensor(2.7868e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of 1
Current timestep = 649. State = [[-0.26458278 -0.21467836]]. Action = [[-0.098661   -0.06975681  0.02684043 -0.96102977]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, True, False, False]
State prediction error at timestep 649 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of 1
Current timestep = 650. State = [[-0.26641485 -0.21699856]]. Action = [[ 0.08055491 -0.07190211 -0.07584082  0.78803396]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, True, False, False]
State prediction error at timestep 650 is tensor(3.2873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.26678973 -0.21994536]]. Action = [[ 0.05339734 -0.03811009 -0.03174189  0.12693429]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, True, False, False]
State prediction error at timestep 651 is tensor(1.5922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[-0.2669973  -0.22254384]]. Action = [[ 0.07888014  0.03253295 -0.02579871  0.84616303]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, True, False, False]
State prediction error at timestep 652 is tensor(3.8706e-05, grad_fn=<MseLossBackward0>)
Current timestep = 653. State = [[-0.26624718 -0.22337124]]. Action = [[ 0.04244805 -0.0818648  -0.06677276 -0.4520589 ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, True, False, False]
State prediction error at timestep 653 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 1
Current timestep = 654. State = [[-0.26531446 -0.22566283]]. Action = [[ 0.00287781 -0.01779459 -0.07127789 -0.28498816]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, True, False, False]
State prediction error at timestep 654 is tensor(6.9723e-05, grad_fn=<MseLossBackward0>)
Current timestep = 655. State = [[-0.2643748  -0.22755904]]. Action = [[ 0.08937543  0.03677889 -0.07796077  0.54513633]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, True, False, False]
State prediction error at timestep 655 is tensor(4.5098e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of 1
Current timestep = 656. State = [[-0.2613491  -0.22758012]]. Action = [[0.04580421 0.05298042 0.06855298 0.8356365 ]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, True, False, False]
State prediction error at timestep 656 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.25867316 -0.22712746]]. Action = [[-0.02111092 -0.02970366  0.03241656  0.4187584 ]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, True, False, False]
State prediction error at timestep 657 is tensor(5.8972e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 657 of 1
Current timestep = 658. State = [[-0.25662133 -0.2275851 ]]. Action = [[0.04169971 0.07040823 0.02547932 0.48765683]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, True, False, False]
State prediction error at timestep 658 is tensor(5.7681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.25374508 -0.22697882]]. Action = [[0.06653178 0.02865051 0.05479563 0.12963116]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, True, False, False]
State prediction error at timestep 659 is tensor(3.2162e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of 1
Current timestep = 660. State = [[-0.24897912 -0.22639246]]. Action = [[ 0.02116885  0.02288387 -0.04985983  0.30812812]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, True, False, False]
State prediction error at timestep 660 is tensor(8.6032e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 1
Current timestep = 661. State = [[-0.24585651 -0.22587578]]. Action = [[ 0.04457469 -0.05636458  0.04768438 -0.6500613 ]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, True, False, False]
State prediction error at timestep 661 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 662. State = [[-0.24339522 -0.22620407]]. Action = [[ 0.0826957  -0.03843825  0.0881798  -0.06009442]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, True, False, False]
State prediction error at timestep 662 is tensor(1.0958e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of 1
Current timestep = 663. State = [[-0.23948301 -0.22715013]]. Action = [[ 0.01201303 -0.00783192 -0.0765175   0.03481352]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, True, False, False]
State prediction error at timestep 663 is tensor(1.5771e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of 1
Current timestep = 664. State = [[-0.23612022 -0.22770818]]. Action = [[-0.07909338  0.0859478   0.07909801 -0.60331774]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, True, False, False]
State prediction error at timestep 664 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 665. State = [[-0.23601179 -0.22696392]]. Action = [[-0.07378657 -0.07106623 -0.02073    -0.38630843]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, True, False, False]
State prediction error at timestep 665 is tensor(1.6008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.23616117 -0.22734132]]. Action = [[0.07863189 0.04591996 0.05375537 0.0472331 ]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, True, False, False]
State prediction error at timestep 666 is tensor(3.8428e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of 1
Current timestep = 667. State = [[-0.23612162 -0.22699785]]. Action = [[ 0.087584    0.0111926  -0.02668843  0.13893044]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, True, False, False]
State prediction error at timestep 667 is tensor(3.1546e-05, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.2343436  -0.22669731]]. Action = [[ 0.05853618 -0.06703885  0.09241388 -0.35881174]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, True, False, False]
State prediction error at timestep 668 is tensor(3.2532e-05, grad_fn=<MseLossBackward0>)
Current timestep = 669. State = [[-0.23178208 -0.2273976 ]]. Action = [[ 0.02169231 -0.08696847  0.02708118 -0.33167696]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, True, False, False]
State prediction error at timestep 669 is tensor(9.6332e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of 1
Current timestep = 670. State = [[-0.22987472 -0.22921443]]. Action = [[ 0.04969344  0.04661173 -0.08978492 -0.45066428]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, True, False, False]
State prediction error at timestep 670 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.22682829 -0.22976203]]. Action = [[-0.02441625 -0.07795377 -0.03214192 -0.30332494]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, True, False, False]
State prediction error at timestep 671 is tensor(1.4541e-05, grad_fn=<MseLossBackward0>)
Current timestep = 672. State = [[-0.22460157 -0.23135307]]. Action = [[ 0.0886547   0.05488393 -0.02834056  0.18489945]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, True, False, False]
State prediction error at timestep 672 is tensor(3.8920e-05, grad_fn=<MseLossBackward0>)
Current timestep = 673. State = [[-0.22146839 -0.23170778]]. Action = [[ 0.06143064  0.02866288 -0.0505713   0.41178143]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, True, False, False]
State prediction error at timestep 673 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.21842116 -0.23168308]]. Action = [[-0.03102949 -0.06105245  0.03239498  0.07722855]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, True, False, False]
State prediction error at timestep 674 is tensor(3.3866e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 674 of 1
Current timestep = 675. State = [[-0.2164365  -0.23221536]]. Action = [[-0.06733157 -0.09179964  0.07155753  0.34120464]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, True, False, False]
State prediction error at timestep 675 is tensor(8.8431e-05, grad_fn=<MseLossBackward0>)
Current timestep = 676. State = [[-0.21644852 -0.23466976]]. Action = [[ 0.05383899  0.01743698 -0.08529521 -0.9033921 ]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, True, False, False]
State prediction error at timestep 676 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 676 of 1
Current timestep = 677. State = [[-0.21549322 -0.23547396]]. Action = [[-0.03615354 -0.03945755 -0.09535504 -0.69632846]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, True, False, False]
State prediction error at timestep 677 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 677 of 1
Current timestep = 678. State = [[-0.21525708 -0.23689058]]. Action = [[ 0.03172376 -0.02129535 -0.01394795 -0.5752346 ]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, True, False, False]
State prediction error at timestep 678 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.21468571 -0.23823069]]. Action = [[ 0.07622304  0.09544721  0.07903624 -0.55276775]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, True, False, False]
State prediction error at timestep 679 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 680. State = [[-0.21307819 -0.2377634 ]]. Action = [[ 0.07019312  0.00833534 -0.09231942 -0.8140755 ]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, True, False, False]
State prediction error at timestep 680 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 680 of 1
Current timestep = 681. State = [[-0.21031891 -0.23773678]]. Action = [[-0.08672779 -0.09372736  0.03725434 -0.47687954]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, True, False, False]
State prediction error at timestep 681 is tensor(4.2361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.21033032 -0.2389923 ]]. Action = [[-0.07292483  0.00367371 -0.00662974 -0.21521103]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [True, False, False, True, False, False]
State prediction error at timestep 682 is tensor(1.8514e-05, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[-0.21054433 -0.23986688]]. Action = [[-0.00179948  0.00433671 -0.07755058  0.57726836]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [True, False, False, True, False, False]
State prediction error at timestep 683 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of 1
Current timestep = 684. State = [[-0.21075463 -0.24059053]]. Action = [[-0.06722398 -0.08506428 -0.0592283  -0.7195376 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [True, False, False, True, False, False]
State prediction error at timestep 684 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of 1
Current timestep = 685. State = [[-0.21147785 -0.24390711]]. Action = [[ 0.00390996 -0.07231317  0.07795352  0.26731586]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [True, False, False, True, False, False]
State prediction error at timestep 685 is tensor(5.2173e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.21241453 -0.24827011]]. Action = [[-0.01230658 -0.08853536  0.033117   -0.40735608]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [True, False, False, True, False, False]
State prediction error at timestep 686 is tensor(3.9121e-06, grad_fn=<MseLossBackward0>)
Current timestep = 687. State = [[-0.21347441 -0.25311235]]. Action = [[ 0.04257571 -0.04116328  0.09434632 -0.73546046]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [True, False, False, True, False, False]
State prediction error at timestep 687 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 687 of 1
Current timestep = 688. State = [[-0.21413973 -0.25681606]]. Action = [[ 0.06058586 -0.07342814  0.02950711 -0.1573509 ]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [True, False, False, True, False, False]
State prediction error at timestep 688 is tensor(4.7985e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[-0.21458836 -0.26048848]]. Action = [[-0.0232335   0.08460047 -0.00141443 -0.08958662]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [True, False, False, True, False, False]
State prediction error at timestep 689 is tensor(1.9918e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 689 of 1
Current timestep = 690. State = [[-0.2146477 -0.2607972]]. Action = [[ 0.01293547  0.06137993 -0.02537017  0.81595063]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, True, False, False]
State prediction error at timestep 690 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 691. State = [[-0.21457528 -0.26017147]]. Action = [[ 0.04346664  0.02971656 -0.04752849  0.295542  ]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, True, False, False]
State prediction error at timestep 691 is tensor(7.3096e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.21416052 -0.25923923]]. Action = [[-0.03860345  0.06043876 -0.09674588 -0.4060669 ]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [True, False, False, True, False, False]
State prediction error at timestep 692 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 692 of 1
Current timestep = 693. State = [[-0.21384516 -0.25786272]]. Action = [[-0.08897787 -0.02509899  0.03677095 -0.44275033]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [True, False, False, True, False, False]
State prediction error at timestep 693 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 694. State = [[-0.21376972 -0.2579152 ]]. Action = [[-0.01310399  0.01617254 -0.00288728  0.3832661 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [True, False, False, True, False, False]
State prediction error at timestep 694 is tensor(5.7899e-05, grad_fn=<MseLossBackward0>)
Current timestep = 695. State = [[-0.21382332 -0.25789934]]. Action = [[-0.09442664 -0.06343317 -0.08292703  0.9516859 ]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [True, False, False, True, False, False]
State prediction error at timestep 695 is tensor(5.1082e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.21540613 -0.25895047]]. Action = [[ 0.0126976  -0.0586817  -0.02112461 -0.77116066]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [True, False, False, True, False, False]
State prediction error at timestep 696 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 696 of 1
Current timestep = 697. State = [[-0.21622168 -0.26071233]]. Action = [[ 0.06064541 -0.01864193 -0.04358605 -0.6547661 ]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, True, False, False]
State prediction error at timestep 697 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 698. State = [[-0.21641412 -0.26153848]]. Action = [[ 0.06592643 -0.07011898  0.08345724  0.33644438]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, True, False, False]
State prediction error at timestep 698 is tensor(3.4338e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[-0.21675429 -0.26349267]]. Action = [[-0.08830738 -0.05209362  0.02303229 -0.52377415]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, True, False, False]
State prediction error at timestep 699 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 699 of 1
Current timestep = 700. State = [[-0.21742754 -0.2667441 ]]. Action = [[ 0.06874884 -0.09333958 -0.04517537  0.7285819 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, True, False, False]
State prediction error at timestep 700 is tensor(7.6574e-05, grad_fn=<MseLossBackward0>)
Current timestep = 701. State = [[-0.21814507 -0.27067554]]. Action = [[ 0.03558641  0.02947626 -0.08913109  0.50350964]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, True, False, False]
State prediction error at timestep 701 is tensor(4.4832e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 701 of 1
Current timestep = 702. State = [[-0.21778733 -0.271766  ]]. Action = [[ 0.09105887  0.0516161   0.00749121 -0.1634984 ]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [True, False, False, True, False, False]
State prediction error at timestep 702 is tensor(6.4083e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 702 of 1
Current timestep = 703. State = [[-0.21590172 -0.271776  ]]. Action = [[0.08119481 0.06409682 0.01570318 0.8170128 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [True, False, False, True, False, False]
State prediction error at timestep 703 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.21250615 -0.2703745 ]]. Action = [[-0.0074525   0.08507671  0.01978619  0.6761315 ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [True, False, False, True, False, False]
State prediction error at timestep 704 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of 1
Current timestep = 705. State = [[-0.20954941 -0.2678692 ]]. Action = [[ 0.07178474  0.0369956   0.02290837 -0.38260293]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, True, False, False]
State prediction error at timestep 705 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of 1
Current timestep = 706. State = [[-0.20591742 -0.26542336]]. Action = [[ 0.00487576 -0.07494159  0.04170125  0.50184214]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, True, False, False]
State prediction error at timestep 706 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 707. State = [[-0.20373343 -0.2658896 ]]. Action = [[ 0.06500459 -0.02898167  0.02636454  0.84619296]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, True, False, False]
State prediction error at timestep 707 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 708. State = [[-0.20085238 -0.26660925]]. Action = [[-0.06557876  0.07142388 -0.05314832 -0.01244003]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, True, False, False]
State prediction error at timestep 708 is tensor(1.1338e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of 1
Current timestep = 709. State = [[-0.20032753 -0.2657172 ]]. Action = [[ 0.02868747 -0.07695265  0.00878163  0.44719923]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, True, False, False]
State prediction error at timestep 709 is tensor(7.6158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[-0.20040591 -0.2663335 ]]. Action = [[ 0.03412522 -0.08692517  0.02562276  0.92461085]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, True, False, False]
State prediction error at timestep 710 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 711. State = [[-0.19896728 -0.2689871 ]]. Action = [[-0.00943145  0.0597363   0.00984665 -0.65209115]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, True, False, False]
State prediction error at timestep 711 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 712. State = [[-0.19826406 -0.26890728]]. Action = [[ 0.03918431 -0.0962391  -0.06983373  0.843858  ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, True, False, False]
State prediction error at timestep 712 is tensor(9.0673e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of 1
Current timestep = 713. State = [[-0.19599308 -0.27032453]]. Action = [[ 0.08121187 -0.00574948  0.03845673 -0.4614088 ]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, True, False, False]
State prediction error at timestep 713 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 714. State = [[-0.19271472 -0.27157038]]. Action = [[-0.05951039  0.05760897 -0.0816612   0.7832632 ]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, True, False, False]
State prediction error at timestep 714 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.1908961 -0.2712968]]. Action = [[ 0.03128608  0.08283015 -0.03806549 -0.2802335 ]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, True, False, False]
State prediction error at timestep 715 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of 1
Current timestep = 716. State = [[-0.18956053 -0.2702916 ]]. Action = [[ 0.04450589 -0.06814098  0.05197094 -0.4642133 ]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, True, False, False]
State prediction error at timestep 716 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.18752745 -0.27076352]]. Action = [[-0.02011559  0.05979503 -0.0614013   0.42023492]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, True, False, False]
State prediction error at timestep 717 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 718. State = [[-0.18667458 -0.26995918]]. Action = [[-0.02929913  0.06795011 -0.08449826  0.63841534]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, True, False, False]
State prediction error at timestep 718 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.18662941 -0.26848814]]. Action = [[ 0.05895377 -0.08762705  0.03861659  0.0577606 ]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, True, False, False]
State prediction error at timestep 719 is tensor(2.0996e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.18593085 -0.26872933]]. Action = [[-0.02017196  0.01657557  0.05531699 -0.78962326]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, True, False, False]
State prediction error at timestep 720 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[-0.18578734 -0.26872632]]. Action = [[-0.07727377 -0.04275463  0.07245029 -0.21322018]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [True, False, False, True, False, False]
State prediction error at timestep 721 is tensor(4.5261e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 721 of 1
Current timestep = 722. State = [[-0.18611503 -0.26994723]]. Action = [[-0.08867391 -0.09382422  0.09417687 -0.16964287]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [True, False, False, True, False, False]
State prediction error at timestep 722 is tensor(2.4313e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.1873065  -0.27345222]]. Action = [[ 0.03632051  0.06433056  0.00611661 -0.21945906]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [True, False, False, True, False, False]
State prediction error at timestep 723 is tensor(5.4892e-05, grad_fn=<MseLossBackward0>)
Current timestep = 724. State = [[-0.18769085 -0.273707  ]]. Action = [[ 0.0862115  -0.06240436  0.07193837 -0.6394169 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [True, False, False, True, False, False]
State prediction error at timestep 724 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 725. State = [[-0.18768744 -0.27424458]]. Action = [[ 0.07458466 -0.06725681 -0.04435602  0.9400778 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [True, False, False, True, False, False]
State prediction error at timestep 725 is tensor(5.9204e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[-0.18686518 -0.27565438]]. Action = [[ 0.00906187  0.08586626 -0.00971261  0.32140648]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [True, False, False, True, False, False]
State prediction error at timestep 726 is tensor(2.4842e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 726 of 1
Current timestep = 727. State = [[-0.18608852 -0.27551076]]. Action = [[-0.0718995  -0.05989619  0.04625504 -0.9328536 ]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [True, False, False, True, False, False]
State prediction error at timestep 727 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 728. State = [[-0.18643403 -0.27614278]]. Action = [[0.03274154 0.07363675 0.0402661  0.69443655]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [True, False, False, True, False, False]
State prediction error at timestep 728 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.18649386 -0.27586517]]. Action = [[-0.07405512 -0.0409322  -0.00687145 -0.08293939]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [True, False, False, True, False, False]
State prediction error at timestep 729 is tensor(3.3332e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.18665896 -0.27623966]]. Action = [[-0.09438833 -0.00045668  0.07756541 -0.33275193]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [True, False, False, True, False, False]
State prediction error at timestep 730 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 731. State = [[-0.1873569  -0.27718735]]. Action = [[-0.09231146 -0.01525246 -0.0953169   0.5047599 ]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [True, False, False, True, False, False]
State prediction error at timestep 731 is tensor(5.2930e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of 1
Current timestep = 732. State = [[-0.18982175 -0.2788635 ]]. Action = [[ 0.01604833 -0.06981236 -0.01047931  0.56068635]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [True, False, False, True, False, False]
State prediction error at timestep 732 is tensor(2.8589e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of 1
Current timestep = 733. State = [[-0.19109769 -0.28131375]]. Action = [[ 0.05049521  0.07896171 -0.02373486  0.11674428]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [True, False, False, True, False, False]
State prediction error at timestep 733 is tensor(6.3008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.19107267 -0.28093162]]. Action = [[ 0.02571393  0.06595422 -0.09442058 -0.01129317]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [True, False, False, True, False, False]
State prediction error at timestep 734 is tensor(5.8066e-05, grad_fn=<MseLossBackward0>)
Current timestep = 735. State = [[-0.19079784 -0.27985892]]. Action = [[ 0.0823283   0.07547116 -0.02653571  0.11165571]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [True, False, False, True, False, False]
State prediction error at timestep 735 is tensor(5.1697e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 735 of 1
Current timestep = 736. State = [[-0.18994059 -0.27681747]]. Action = [[ 0.01320491  0.08826045 -0.01963115 -0.16869783]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [True, False, False, True, False, False]
State prediction error at timestep 736 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.18900247 -0.27292934]]. Action = [[-0.05178598  0.02539099 -0.02342059  0.81506443]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [True, False, False, True, False, False]
State prediction error at timestep 737 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[-0.18795928 -0.26967835]]. Action = [[ 0.07842172  0.03354866 -0.01950478 -0.205145  ]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [True, False, False, True, False, False]
State prediction error at timestep 738 is tensor(9.7424e-05, grad_fn=<MseLossBackward0>)
Current timestep = 739. State = [[-0.18708493 -0.26658618]]. Action = [[ 0.05201925  0.04448236  0.01082115 -0.6791025 ]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [True, False, False, True, False, False]
State prediction error at timestep 739 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 739 of 1
Current timestep = 740. State = [[-0.18587287 -0.26304638]]. Action = [[-0.01250203  0.02657197 -0.05563112  0.43377876]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [True, False, False, True, False, False]
State prediction error at timestep 740 is tensor(8.5603e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.18532209 -0.2607226 ]]. Action = [[ 0.03465981 -0.0713595  -0.08631303 -0.10898757]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [True, False, False, True, False, False]
State prediction error at timestep 741 is tensor(1.6444e-05, grad_fn=<MseLossBackward0>)
Current timestep = 742. State = [[-0.18484546 -0.26045382]]. Action = [[ 0.07143434  0.08327322  0.0354254  -0.8314553 ]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [True, False, False, True, False, False]
State prediction error at timestep 742 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of 1
Current timestep = 743. State = [[-0.18251778 -0.2580052 ]]. Action = [[0.07647754 0.05093377 0.08803809 0.2116791 ]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [True, False, False, True, False, False]
State prediction error at timestep 743 is tensor(3.5687e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 743 of 1
Current timestep = 744. State = [[-0.17968719 -0.2550246 ]]. Action = [[ 0.03694082 -0.06301716  0.04692691 -0.5538516 ]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [True, False, False, True, False, False]
State prediction error at timestep 744 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of 1
Current timestep = 745. State = [[-0.17716528 -0.25430676]]. Action = [[ 0.09330981  0.0650887   0.06821344 -0.17486095]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [True, False, False, True, False, False]
State prediction error at timestep 745 is tensor(5.6372e-05, grad_fn=<MseLossBackward0>)
Current timestep = 746. State = [[-0.17348298 -0.25259873]]. Action = [[-0.08498371 -0.04467234 -0.02345686 -0.813655  ]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [True, False, False, True, False, False]
State prediction error at timestep 746 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 746 of 1
Current timestep = 747. State = [[-0.1719188  -0.25247186]]. Action = [[ 0.00173422  0.0393913  -0.07361065  0.7808974 ]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [True, False, False, True, False, False]
State prediction error at timestep 747 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of 1
Current timestep = 748. State = [[-0.17105342 -0.25160828]]. Action = [[-0.06320292  0.08998687 -0.08823849  0.10271418]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, True, False, False]
State prediction error at timestep 748 is tensor(1.5854e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of 1
Current timestep = 749. State = [[-0.170684  -0.2493543]]. Action = [[ 0.06557348 -0.03329585 -0.00110705 -0.94472086]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [True, False, False, True, False, False]
State prediction error at timestep 749 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 750. State = [[-0.17041361 -0.24843411]]. Action = [[-0.05624938  0.0209859   0.07499445 -0.8606006 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, True, False, False]
State prediction error at timestep 750 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.1703784  -0.24779887]]. Action = [[ 0.0182832  -0.05356829 -0.00273461 -0.36491477]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, True, False, False]
State prediction error at timestep 751 is tensor(5.5616e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of 1
Current timestep = 752. State = [[-0.17038096 -0.2478097 ]]. Action = [[-0.02588324  0.08760422 -0.04750756 -0.70027804]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, True, False, False]
State prediction error at timestep 752 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 753. State = [[-0.17017585 -0.24652798]]. Action = [[-0.01418408  0.03094777  0.03076566  0.17008281]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, True, False, False]
State prediction error at timestep 753 is tensor(2.5875e-05, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-0.16993412 -0.2452303 ]]. Action = [[ 0.05957764 -0.04553095  0.00565799  0.87554455]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, True, False, False]
State prediction error at timestep 754 is tensor(7.7541e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of 1
Current timestep = 755. State = [[-0.16982204 -0.24490796]]. Action = [[ 0.07464582  0.06480505 -0.08971276 -0.78083026]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, True, False, False]
State prediction error at timestep 755 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of 1
Current timestep = 756. State = [[-0.16859744 -0.24319744]]. Action = [[-0.02453811  0.04799371 -0.04648361  0.24096441]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, True, False, False]
State prediction error at timestep 756 is tensor(4.6617e-05, grad_fn=<MseLossBackward0>)
Current timestep = 757. State = [[-0.16796699 -0.24058613]]. Action = [[ 0.09336964 -0.09560651  0.01502453  0.89191556]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, True, False, False]
State prediction error at timestep 757 is tensor(7.2266e-05, grad_fn=<MseLossBackward0>)
Current timestep = 758. State = [[-0.1661652  -0.24092825]]. Action = [[ 0.07700438 -0.01999454  0.03252097  0.72530854]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, True, False, False]
State prediction error at timestep 758 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.1624184  -0.24139352]]. Action = [[ 0.06503601  0.02202374 -0.02841956  0.5390744 ]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, True, False, False]
State prediction error at timestep 759 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of 1
Current timestep = 760. State = [[-0.15732948 -0.2412572 ]]. Action = [[ 0.08995888  0.01902572 -0.01917414 -0.4478376 ]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, True, False, False]
State prediction error at timestep 760 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.15152751 -0.24052736]]. Action = [[ 0.08495916  0.03919705 -0.0596634   0.80749893]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, True, False, False]
State prediction error at timestep 761 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[-0.1463687  -0.23944156]]. Action = [[-0.09400578  0.06645439 -0.07156114 -0.8814614 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, True, False, False]
State prediction error at timestep 762 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of 1
Current timestep = 763. State = [[-0.14400688 -0.23776244]]. Action = [[-0.02002823 -0.09861881 -0.08699458  0.7204884 ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, True, False, False]
State prediction error at timestep 763 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[-0.1439938  -0.23794466]]. Action = [[-0.02387134 -0.05900713 -0.08085543 -0.00823849]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, True, False, False]
State prediction error at timestep 764 is tensor(2.8760e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 764 of 1
Current timestep = 765. State = [[-0.1442069  -0.23937443]]. Action = [[-0.01750528 -0.04448619  0.00904526  0.03789759]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, True, False, False]
State prediction error at timestep 765 is tensor(1.5113e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 1
Current timestep = 766. State = [[-0.14458315 -0.24111381]]. Action = [[-0.05598479  0.03676968  0.07885323 -0.02595258]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, True, False, False]
State prediction error at timestep 766 is tensor(2.1644e-05, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.14490089 -0.24202062]]. Action = [[-0.0715242  -0.08930725 -0.06651539 -0.00972664]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, True, False, False]
State prediction error at timestep 767 is tensor(1.4742e-05, grad_fn=<MseLossBackward0>)
Current timestep = 768. State = [[-0.14615309 -0.24525335]]. Action = [[-0.09238794  0.06255784 -0.06183011  0.66026866]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, True, False, False]
State prediction error at timestep 768 is tensor(9.5685e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 1
Current timestep = 769. State = [[-0.1484516  -0.24638216]]. Action = [[ 0.08618202 -0.08346949  0.03957682  0.37063622]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, True, False, False]
State prediction error at timestep 769 is tensor(2.0783e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 769 of 1
Current timestep = 770. State = [[-0.14893259 -0.24801631]]. Action = [[ 0.0759558  -0.08810737 -0.06998182  0.9237951 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, True, False, False]
State prediction error at timestep 770 is tensor(4.9522e-05, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[-0.14931907 -0.25109062]]. Action = [[-0.05411797 -0.03784934  0.00400796  0.66002965]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, True, False, False]
State prediction error at timestep 771 is tensor(6.7090e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.14986691 -0.2541704 ]]. Action = [[ 0.02381985  0.07744809 -0.0749151   0.4516058 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, True, False, False]
State prediction error at timestep 772 is tensor(5.0072e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 1
Current timestep = 773. State = [[-0.1496922  -0.25387907]]. Action = [[ 0.03958174 -0.08083196 -0.05654979 -0.3013438 ]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, True, False, False]
State prediction error at timestep 773 is tensor(2.1394e-05, grad_fn=<MseLossBackward0>)
Current timestep = 774. State = [[-0.1500177  -0.25555903]]. Action = [[-0.04387122 -0.0706179  -0.04787707  0.8759463 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, True, False, False]
State prediction error at timestep 774 is tensor(5.7573e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 774 of 1
Current timestep = 775. State = [[-0.15055294 -0.2588683 ]]. Action = [[-0.02564721 -0.07265756  0.05651376  0.8921515 ]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, True, False, False]
State prediction error at timestep 775 is tensor(7.7745e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of 1
Current timestep = 776. State = [[-0.15120895 -0.2632249 ]]. Action = [[ 0.09284502 -0.06476963  0.04348668 -0.3335756 ]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, True, False, False]
State prediction error at timestep 776 is tensor(2.6804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 776 of 1
Current timestep = 777. State = [[-0.15124798 -0.26638803]]. Action = [[0.0329446  0.08334682 0.05488082 0.48657942]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, True, False, False]
State prediction error at timestep 777 is tensor(1.8433e-05, grad_fn=<MseLossBackward0>)
Current timestep = 778. State = [[-0.15008448 -0.26666176]]. Action = [[-0.03111321  0.09034098  0.00461544  0.6344845 ]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, True, False, False]
State prediction error at timestep 778 is tensor(5.1206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 779. State = [[-0.14966083 -0.26544175]]. Action = [[ 0.08706308 -0.09433477 -0.09228083 -0.4243579 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, True, False, False]
State prediction error at timestep 779 is tensor(8.3953e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[-0.148588   -0.26593477]]. Action = [[-0.04540095  0.08472569 -0.00517947  0.9602759 ]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, True, False, False]
State prediction error at timestep 780 is tensor(9.4198e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 780 of 1
Current timestep = 781. State = [[-0.14804098 -0.26489615]]. Action = [[ 0.06265638  0.05691595 -0.0745534  -0.18896449]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, True, False, False]
State prediction error at timestep 781 is tensor(5.0229e-05, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[-0.1461883  -0.26353443]]. Action = [[ 0.08669875  0.00717677 -0.01748135 -0.83324254]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, True, False, False]
State prediction error at timestep 782 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[-0.14199455 -0.26209977]]. Action = [[0.02565039 0.09773285 0.05016101 0.57407045]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, True, False, False]
State prediction error at timestep 783 is tensor(8.7742e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 783 of 1
Current timestep = 784. State = [[-0.13850431 -0.2592781 ]]. Action = [[-0.0123245   0.06582043  0.04206026 -0.62403214]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, True, False, False]
State prediction error at timestep 784 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of 1
Current timestep = 785. State = [[-0.13633277 -0.2555731 ]]. Action = [[-0.03684513 -0.04624948  0.0742128   0.48107302]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, True, False, False]
State prediction error at timestep 785 is tensor(4.8543e-05, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[-0.13611518 -0.25466383]]. Action = [[ 0.07194544 -0.02069827  0.01751356 -0.52676636]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, True, False, False]
State prediction error at timestep 786 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[-0.13536766 -0.253873  ]]. Action = [[-0.08719149  0.05343235 -0.0680793  -0.9163114 ]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, True, False, False]
State prediction error at timestep 787 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[-0.13531205 -0.25306582]]. Action = [[-0.02926795 -0.03034604  0.05858799  0.5660417 ]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, True, False, False]
State prediction error at timestep 788 is tensor(2.9833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 789. State = [[-0.13530822 -0.25298616]]. Action = [[-0.0074158  -0.06589545  0.02737414 -0.70125115]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, True, False, False]
State prediction error at timestep 789 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 790. State = [[-0.13544653 -0.2537571 ]]. Action = [[-0.09213338  0.01970486  0.07623071 -0.0523448 ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, True, False, False]
State prediction error at timestep 790 is tensor(4.1056e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.13618474 -0.25460696]]. Action = [[-0.05318573  0.08401773  0.03624623 -0.6029847 ]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, True, False, False]
State prediction error at timestep 791 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 791 of 1
Current timestep = 792. State = [[-0.13734034 -0.25382704]]. Action = [[-0.01051119  0.00997007 -0.03789448  0.04784226]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, True, False, False]
State prediction error at timestep 792 is tensor(1.4538e-05, grad_fn=<MseLossBackward0>)
Current timestep = 793. State = [[-0.13790062 -0.2536269 ]]. Action = [[ 0.05642729 -0.05781804 -0.0578883  -0.8905025 ]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, True, False, False]
State prediction error at timestep 793 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 794. State = [[-0.13783856 -0.25354752]]. Action = [[-0.08499479  0.0872742  -0.05129908 -0.1351161 ]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, True, False, False]
State prediction error at timestep 794 is tensor(4.0747e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[-0.13866115 -0.2527004 ]]. Action = [[-0.00595218 -0.0649593  -0.01328108  0.8044025 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, True, False, False]
State prediction error at timestep 795 is tensor(5.7262e-05, grad_fn=<MseLossBackward0>)
Current timestep = 796. State = [[-0.13935237 -0.25307178]]. Action = [[-0.06656155  0.07397903 -0.01913672  0.7786788 ]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, True, False, False]
State prediction error at timestep 796 is tensor(8.8948e-05, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[-0.14039193 -0.25227797]]. Action = [[ 0.07993949  0.03647111 -0.07949115  0.48116732]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, True, False, False]
State prediction error at timestep 797 is tensor(6.3507e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 797 of 1
Current timestep = 798. State = [[-0.14027229 -0.2508729 ]]. Action = [[-0.07681063 -0.0435397  -0.03209634  0.88508034]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, True, False, False]
State prediction error at timestep 798 is tensor(5.1530e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.14100234 -0.25080332]]. Action = [[-0.03855133  0.05313589 -0.0139793   0.01803339]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, True, False, False]
State prediction error at timestep 799 is tensor(2.0089e-05, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[-0.14180827 -0.24974397]]. Action = [[ 0.05339744 -0.01402487  0.02223547  0.00194335]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, True, False, False]
State prediction error at timestep 800 is tensor(1.7740e-05, grad_fn=<MseLossBackward0>)
Current timestep = 801. State = [[-0.14172971 -0.24926914]]. Action = [[ 0.08347965  0.06381311  0.01839521 -0.6968678 ]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, True, False, False]
State prediction error at timestep 801 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.14136633 -0.24690183]]. Action = [[ 0.05413909 -0.01513041  0.04597475 -0.8450518 ]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, True, False, False]
State prediction error at timestep 802 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of 1
Current timestep = 803. State = [[-0.14102489 -0.24531361]]. Action = [[ 0.00776903 -0.02004134 -0.01662141  0.03305876]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, True, False, False]
State prediction error at timestep 803 is tensor(1.3588e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of 1
Current timestep = 804. State = [[-0.14096369 -0.2449421 ]]. Action = [[-0.05987923 -0.06402627 -0.00807973  0.7994311 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, True, False, False]
State prediction error at timestep 804 is tensor(4.2119e-05, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[-0.14099757 -0.24551877]]. Action = [[ 0.06026042 -0.08148541  0.04747748  0.7238668 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, True, False, False]
State prediction error at timestep 805 is tensor(4.0797e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[-0.14112031 -0.24712019]]. Action = [[-0.09463221  0.08045214 -0.06390241 -0.6851432 ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, True, False, False]
State prediction error at timestep 806 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of 1
Current timestep = 807. State = [[-0.14128065 -0.24739751]]. Action = [[-0.095176   -0.03060645 -0.07300088  0.9011824 ]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, True, False, False]
State prediction error at timestep 807 is tensor(3.5609e-05, grad_fn=<MseLossBackward0>)
Current timestep = 808. State = [[-0.1429067  -0.24908924]]. Action = [[ 0.06326925  0.06814004 -0.08225654  0.00626624]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, True, False, False]
State prediction error at timestep 808 is tensor(1.7317e-05, grad_fn=<MseLossBackward0>)
Current timestep = 809. State = [[-0.14284152 -0.24840796]]. Action = [[-0.04915577 -0.08985517  0.06846017  0.46699715]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, True, False, False]
State prediction error at timestep 809 is tensor(1.9812e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 809 of 1
Current timestep = 810. State = [[-0.14339489 -0.24941288]]. Action = [[ 0.05583494 -0.04172951  0.06095494  0.9211483 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, True, False, False]
State prediction error at timestep 810 is tensor(5.2216e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of 1
Current timestep = 811. State = [[-0.14355852 -0.25028428]]. Action = [[ 0.08592596 -0.03320008 -0.01042046 -0.4661615 ]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, True, False, False]
State prediction error at timestep 811 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of 1
Current timestep = 812. State = [[-0.14360158 -0.25089014]]. Action = [[ 0.03300653 -0.04156629 -0.0044693  -0.22795564]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, True, False, False]
State prediction error at timestep 812 is tensor(2.9189e-05, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[-0.1433944  -0.25214577]]. Action = [[-0.08167677  0.09472168  0.0964535  -0.75200146]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, True, False, False]
State prediction error at timestep 813 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of 1
Current timestep = 814. State = [[-0.1433534  -0.25167087]]. Action = [[ 0.0701498   0.08612848 -0.0576859  -0.01764667]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, True, False, False]
State prediction error at timestep 814 is tensor(3.6690e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of 1
Current timestep = 815. State = [[-0.14295115 -0.24954927]]. Action = [[ 0.08461624  0.02549825  0.03465416 -0.35426247]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, True, False, False]
State prediction error at timestep 815 is tensor(9.3744e-05, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.14180295 -0.24775252]]. Action = [[ 0.00240023  0.06553153 -0.07106985 -0.36465967]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, True, False, False]
State prediction error at timestep 816 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of 1
Current timestep = 817. State = [[-0.14067142 -0.24468459]]. Action = [[ 0.02401163  0.08063487  0.0572573  -0.8557905 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, True, False, False]
State prediction error at timestep 817 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of 1
Current timestep = 818. State = [[-0.13945073 -0.23990588]]. Action = [[-0.00981631  0.0264465  -0.0539161  -0.66913795]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, True, False, False]
State prediction error at timestep 818 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 818 of 1
Current timestep = 819. State = [[-0.1391421  -0.23682821]]. Action = [[ 0.00565527 -0.00203795  0.05642992 -0.6298928 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, True, False, False]
State prediction error at timestep 819 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 820. State = [[-0.13895674 -0.23512733]]. Action = [[-0.02390227 -0.02687287  0.03313483  0.22958648]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, True, False, False]
State prediction error at timestep 820 is tensor(1.7270e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of 1
Current timestep = 821. State = [[-0.13893656 -0.2348701 ]]. Action = [[ 0.07260647  0.02898199 -0.09080089 -0.5554606 ]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, True, False, False]
State prediction error at timestep 821 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of 1
Current timestep = 822. State = [[-0.13799219 -0.2339629 ]]. Action = [[ 0.03221444  0.03097553 -0.08977845  0.42412686]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, True, False, False]
State prediction error at timestep 822 is tensor(5.1718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 823. State = [[-0.13679096 -0.23241393]]. Action = [[ 0.05611119 -0.06446724  0.07059116 -0.78278565]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, True, False, False]
State prediction error at timestep 823 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of 1
Current timestep = 824. State = [[-0.13547903 -0.23233795]]. Action = [[-0.09709076  0.01261906 -0.06962161 -0.250634  ]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, True, False, False]
State prediction error at timestep 824 is tensor(3.3182e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of 1
Current timestep = 825. State = [[-0.13557912 -0.23214068]]. Action = [[0.03221854 0.0382048  0.0703145  0.7632587 ]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, True, False, False]
State prediction error at timestep 825 is tensor(3.8381e-05, grad_fn=<MseLossBackward0>)
Current timestep = 826. State = [[-0.13552135 -0.2317322 ]]. Action = [[-0.04323084  0.02229537 -0.07228152 -0.6037196 ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, True, False, False]
State prediction error at timestep 826 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of 1
Current timestep = 827. State = [[-0.13540877 -0.23117676]]. Action = [[ 0.09575047  0.01220951 -0.06875266 -0.2503835 ]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, True, False, False]
State prediction error at timestep 827 is tensor(3.1431e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 827 of 1
Current timestep = 828. State = [[-0.13428801 -0.22981232]]. Action = [[ 0.03873637  0.03501802 -0.08281358  0.21827543]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, True, False, False]
State prediction error at timestep 828 is tensor(9.5713e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 828 of 1
Current timestep = 829. State = [[-0.13260257 -0.22805706]]. Action = [[-0.07949562 -0.02356819  0.08419294 -0.5112123 ]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, True, False, False]
State prediction error at timestep 829 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 830. State = [[-0.13270558 -0.22776295]]. Action = [[-0.05159804  0.05989618  0.0164744   0.13843918]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, True, False, False]
State prediction error at timestep 830 is tensor(1.2552e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of 1
Current timestep = 831. State = [[-0.13271254 -0.22648712]]. Action = [[ 0.01861835 -0.01651846  0.07365499  0.29440737]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, True, False, False]
State prediction error at timestep 831 is tensor(9.5842e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of 1
Current timestep = 832. State = [[-0.13263102 -0.22587657]]. Action = [[ 0.09324577  0.09681164  0.02770091 -0.16837049]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, True, False, False]
State prediction error at timestep 832 is tensor(3.9221e-05, grad_fn=<MseLossBackward0>)
Current timestep = 833. State = [[-0.13202053 -0.22234917]]. Action = [[0.0002685  0.09607906 0.0565815  0.12955153]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, True, False, False]
State prediction error at timestep 833 is tensor(3.4488e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 833 of 1
Current timestep = 834. State = [[-0.13108549 -0.2168115 ]]. Action = [[ 0.02817877  0.02907517  0.07085561 -0.45153844]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, True, False, False]
State prediction error at timestep 834 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 834 of 1
Current timestep = 835. State = [[-0.13002163 -0.21256718]]. Action = [[ 0.02665519  0.09415134 -0.09286635 -0.90281993]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, True, False, False]
State prediction error at timestep 835 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[-0.12918003 -0.2076134 ]]. Action = [[-0.09107224  0.05251307 -0.0841865  -0.2162947 ]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, True, False, False]
State prediction error at timestep 836 is tensor(9.8754e-05, grad_fn=<MseLossBackward0>)
Current timestep = 837. State = [[-0.12894462 -0.20320675]]. Action = [[-0.03402428  0.08878282 -0.0983669  -0.07639384]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, True, False, False]
State prediction error at timestep 837 is tensor(7.3425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 1
Current timestep = 838. State = [[-0.12853982 -0.19861864]]. Action = [[-0.0710154   0.03484718  0.03927476 -0.4916702 ]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, True, False, False]
State prediction error at timestep 838 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 1
Current timestep = 839. State = [[-0.1285231  -0.19520377]]. Action = [[-0.05498669 -0.09069052 -0.07838366  0.77736855]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, True, False, False]
State prediction error at timestep 839 is tensor(1.8871e-05, grad_fn=<MseLossBackward0>)
Current timestep = 840. State = [[-0.13016039 -0.1951446 ]]. Action = [[-0.0547015   0.02963717  0.05108335 -0.45145237]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, True, False, False]
State prediction error at timestep 840 is tensor(6.5617e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of 1
Current timestep = 841. State = [[-0.1316865  -0.19507392]]. Action = [[-0.02641141 -0.07459886 -0.08883696 -0.6012905 ]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, True, False, False]
State prediction error at timestep 841 is tensor(7.5452e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of 1
Current timestep = 842. State = [[-0.13333061 -0.19649014]]. Action = [[ 0.02076982 -0.0054966  -0.07014775  0.42001843]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, True, False, False]
State prediction error at timestep 842 is tensor(1.5600e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 1
Current timestep = 843. State = [[-0.13369231 -0.19721164]]. Action = [[-0.01585338 -0.04542611  0.04398877 -0.8799406 ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, True, False, False]
State prediction error at timestep 843 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 844. State = [[-0.1343397  -0.19890249]]. Action = [[-0.05617619 -0.0014542   0.03207209  0.71848345]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, True, False, False]
State prediction error at timestep 844 is tensor(6.6487e-05, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[-0.13586237 -0.20016709]]. Action = [[-0.09574198  0.08609938  0.05648661 -0.786627  ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, True, False, False]
State prediction error at timestep 845 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of 1
Current timestep = 846. State = [[-0.1377872  -0.19983132]]. Action = [[ 0.02603715 -0.0093616   0.04760308  0.5897409 ]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, True, False, False]
State prediction error at timestep 846 is tensor(2.1643e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 1
Current timestep = 847. State = [[-0.13850063 -0.20002638]]. Action = [[-0.03168162  0.06208558  0.09879971 -0.21612233]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, True, False, False]
State prediction error at timestep 847 is tensor(2.1327e-05, grad_fn=<MseLossBackward0>)
Current timestep = 848. State = [[-0.13932377 -0.19817615]]. Action = [[-0.03728601  0.0656926   0.04648406 -0.16812897]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, True, False, False]
State prediction error at timestep 848 is tensor(3.6803e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of 1
Current timestep = 849. State = [[-0.14050704 -0.19548498]]. Action = [[-0.04200913  0.01585187 -0.03093682 -0.38232958]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, True, False, False]
State prediction error at timestep 849 is tensor(8.5461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 849 of 1
Current timestep = 850. State = [[-0.14324784 -0.1928773 ]]. Action = [[ 0.00715873  0.09562976 -0.00042278 -0.1681025 ]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, True, False, False]
State prediction error at timestep 850 is tensor(5.7495e-05, grad_fn=<MseLossBackward0>)
Current timestep = 851. State = [[-0.14544986 -0.18834399]]. Action = [[ 0.05870632 -0.06928052 -0.03111695 -0.27606106]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, True, False, False]
State prediction error at timestep 851 is tensor(2.6581e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 1
Current timestep = 852. State = [[-0.14607972 -0.1874095 ]]. Action = [[-0.09510251  0.02582771 -0.0330445  -0.6502861 ]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, True, False, False]
State prediction error at timestep 852 is tensor(6.6971e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.14756764 -0.18645875]]. Action = [[ 0.05846205  0.08767309  0.01119687 -0.29749894]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, True, False, False]
State prediction error at timestep 853 is tensor(5.4313e-05, grad_fn=<MseLossBackward0>)
Current timestep = 854. State = [[-0.14771043 -0.18313846]]. Action = [[-0.02644779  0.03484515 -0.03399092 -0.06970519]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, True, False, False]
State prediction error at timestep 854 is tensor(4.3648e-05, grad_fn=<MseLossBackward0>)
Current timestep = 855. State = [[-0.1481916  -0.18014032]]. Action = [[ 0.07200878 -0.04744739 -0.02274358  0.75002813]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, True, False, False]
State prediction error at timestep 855 is tensor(2.2498e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 855 of -1
Current timestep = 856. State = [[-0.14803135 -0.17938076]]. Action = [[-0.07977258  0.0178102   0.01925515 -0.4325356 ]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, True, False, False]
State prediction error at timestep 856 is tensor(3.2170e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of -1
Current timestep = 857. State = [[-0.14891173 -0.17861655]]. Action = [[-0.05372294  0.05957343  0.04431529 -0.04675972]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, True, False, False]
State prediction error at timestep 857 is tensor(3.3470e-05, grad_fn=<MseLossBackward0>)
Current timestep = 858. State = [[-0.14991358 -0.17678185]]. Action = [[ 0.05468077  0.02470664  0.00169426 -0.63560534]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, True, False, False]
State prediction error at timestep 858 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 859. State = [[-0.15044965 -0.17467733]]. Action = [[-0.05916718  0.06470256  0.08953875 -0.3569613 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, True, False, False]
State prediction error at timestep 859 is tensor(4.3741e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of 1
Current timestep = 860. State = [[-0.15209931 -0.17154217]]. Action = [[ 0.067756    0.06189912 -0.07305397 -0.60285205]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, True, False, False]
State prediction error at timestep 860 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 860 of 0
Current timestep = 861. State = [[-0.1528528  -0.16732141]]. Action = [[-0.02092956 -0.06091068 -0.08184823  0.7136519 ]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, True, False, False]
State prediction error at timestep 861 is tensor(3.0806e-05, grad_fn=<MseLossBackward0>)
Current timestep = 862. State = [[-0.15379572 -0.16679242]]. Action = [[-0.03679103 -0.05624522 -0.07383965  0.15922129]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, True, False, False]
State prediction error at timestep 862 is tensor(5.1511e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 862 of 0
Current timestep = 863. State = [[-0.15506448 -0.1672802 ]]. Action = [[-0.09780584 -0.00318958 -0.04481537  0.5720601 ]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, True, False, False]
State prediction error at timestep 863 is tensor(3.0323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of 0
Current timestep = 864. State = [[-0.15725496 -0.16793086]]. Action = [[ 0.02838106  0.03815485  0.0183632  -0.14316821]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, True, False, False]
State prediction error at timestep 864 is tensor(1.8433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 0
Current timestep = 865. State = [[-0.15813762 -0.1679666 ]]. Action = [[ 0.09602711 -0.05637889 -0.00342064 -0.98249555]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, True, False, False]
State prediction error at timestep 865 is tensor(2.3475e-05, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[-0.15796766 -0.16804324]]. Action = [[ 0.03274957 -0.01325069  0.01815411  0.9014244 ]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, True, False, False]
State prediction error at timestep 866 is tensor(3.6606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 867. State = [[-0.15739715 -0.16810639]]. Action = [[ 0.06840832  0.07797862 -0.02219722  0.90759635]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, True, False, False]
State prediction error at timestep 867 is tensor(5.2108e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of 0
Current timestep = 868. State = [[-0.15718149 -0.16750689]]. Action = [[-0.00939817 -0.07373215 -0.09550499  0.34293306]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, True, False, False]
State prediction error at timestep 868 is tensor(1.2899e-05, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[-0.15713844 -0.16760068]]. Action = [[-0.08369327  0.08073402  0.07803937  0.7056439 ]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, True, False, False]
State prediction error at timestep 869 is tensor(5.0370e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of 0
Current timestep = 870. State = [[-0.1571085  -0.16720243]]. Action = [[ 0.00327946 -0.06791785 -0.08360375 -0.01708245]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, True, False, False]
State prediction error at timestep 870 is tensor(1.9044e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of 0
Current timestep = 871. State = [[-0.15711312 -0.16745396]]. Action = [[-0.02881724 -0.04764352  0.08907542  0.3603891 ]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, True, False, False]
State prediction error at timestep 871 is tensor(1.5339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 872. State = [[-0.15717085 -0.16925232]]. Action = [[ 0.04946765 -0.05101901 -0.0781038   0.69379926]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, True, False, False]
State prediction error at timestep 872 is tensor(6.6005e-05, grad_fn=<MseLossBackward0>)
Current timestep = 873. State = [[-0.15687457 -0.17096733]]. Action = [[-0.04834486  0.00834787  0.06818492 -0.35465443]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, True, False, False]
State prediction error at timestep 873 is tensor(4.6212e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 0
Current timestep = 874. State = [[-0.15731266 -0.1721533 ]]. Action = [[-0.05344085 -0.09047839 -0.02251469  0.01289821]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, True, False, False]
State prediction error at timestep 874 is tensor(1.7582e-05, grad_fn=<MseLossBackward0>)
Current timestep = 875. State = [[-0.15859675 -0.17572701]]. Action = [[-0.06502199  0.0705796  -0.07772364  0.6419914 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, True, False, False]
State prediction error at timestep 875 is tensor(3.9430e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 875 of 0
Current timestep = 876. State = [[-0.16011913 -0.17693406]]. Action = [[ 0.01677994  0.04403406 -0.08053464 -0.32115394]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, True, False, False]
State prediction error at timestep 876 is tensor(4.1910e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 0
Current timestep = 877. State = [[-0.1605879  -0.17629358]]. Action = [[-2.9169418e-02 -1.7118454e-04 -9.0486526e-02 -7.2934949e-01]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, True, False, False]
State prediction error at timestep 877 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 878. State = [[-0.16109842 -0.17646511]]. Action = [[ 0.01586342 -0.05248505 -0.03734751  0.9381157 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, True, False, False]
State prediction error at timestep 878 is tensor(2.6109e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of -1
Current timestep = 879. State = [[-0.16158201 -0.17752483]]. Action = [[ 0.0939795   0.01914304 -0.08032335 -0.8928935 ]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, True, False, False]
State prediction error at timestep 879 is tensor(4.2580e-05, grad_fn=<MseLossBackward0>)
Current timestep = 880. State = [[-0.16174129 -0.17755866]]. Action = [[-0.06866579 -0.01075328  0.06898747 -0.7763452 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, True, False, False]
State prediction error at timestep 880 is tensor(9.5090e-05, grad_fn=<MseLossBackward0>)
Current timestep = 881. State = [[-0.16185589 -0.17773004]]. Action = [[ 0.06000283 -0.0338989  -0.00164574 -0.3607118 ]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, True, False, False]
State prediction error at timestep 881 is tensor(7.1854e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of -1
Current timestep = 882. State = [[-0.1617751 -0.1781182]]. Action = [[ 0.01048239 -0.00616379 -0.06580592  0.09305382]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, True, False, False]
State prediction error at timestep 882 is tensor(1.1552e-06, grad_fn=<MseLossBackward0>)
Current timestep = 883. State = [[-0.16149893 -0.1786533 ]]. Action = [[ 0.04682273 -0.00905988  0.07430766 -0.5052337 ]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, True, False, False]
State prediction error at timestep 883 is tensor(2.4441e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 883 of -1
Current timestep = 884. State = [[-0.16129832 -0.17880444]]. Action = [[0.05590706 0.04062345 0.06579871 0.5686568 ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, True, False, False]
State prediction error at timestep 884 is tensor(3.9264e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 884 of -1
Current timestep = 885. State = [[-0.16116254 -0.17852098]]. Action = [[-0.05225331  0.03131593  0.02250335  0.56907237]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, True, False, False]
State prediction error at timestep 885 is tensor(2.8451e-05, grad_fn=<MseLossBackward0>)
Current timestep = 886. State = [[-0.16113292 -0.17779167]]. Action = [[-0.03378115  0.0539998  -0.09707644  0.17783964]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, True, False, False]
State prediction error at timestep 886 is tensor(6.7404e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of -1
Current timestep = 887. State = [[-0.16115472 -0.17635785]]. Action = [[ 0.06874711 -0.06690864  0.05753542  0.453215  ]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, True, False, False]
State prediction error at timestep 887 is tensor(1.5657e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of 0
Current timestep = 888. State = [[-0.16109091 -0.17641152]]. Action = [[ 0.03155101 -0.04287071  0.06332376 -0.44189048]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, True, False, False]
State prediction error at timestep 888 is tensor(5.9547e-06, grad_fn=<MseLossBackward0>)
Current timestep = 889. State = [[-0.16036445 -0.17668477]]. Action = [[-0.05225575  0.01649676 -0.03308453 -0.9182443 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, True, False, False]
State prediction error at timestep 889 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 889 of 0
Current timestep = 890. State = [[-0.15987402 -0.17681296]]. Action = [[ 0.03439318  0.0639459  -0.03399017 -0.42351997]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, True, False, False]
State prediction error at timestep 890 is tensor(3.8797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 891. State = [[-0.15996562 -0.17624016]]. Action = [[-0.05995741  0.00555351 -0.0367858  -0.6766731 ]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, True, False, False]
State prediction error at timestep 891 is tensor(6.6163e-05, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[-0.15996018 -0.17612733]]. Action = [[-0.0539634   0.01519428  0.05206352 -0.69805753]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, True, False, False]
State prediction error at timestep 892 is tensor(4.9783e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of 0
Current timestep = 893. State = [[-0.16000244 -0.17570554]]. Action = [[-0.07658712  0.03460919 -0.08181705  0.33291185]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, True, False, False]
State prediction error at timestep 893 is tensor(9.7320e-06, grad_fn=<MseLossBackward0>)
Current timestep = 894. State = [[-0.16036722 -0.1748461 ]]. Action = [[0.06919251 0.05828118 0.06901174 0.9793253 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, True, False, False]
State prediction error at timestep 894 is tensor(2.6614e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.16016749 -0.17212443]]. Action = [[-0.02289075  0.06312532  0.00941114  0.14232469]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, True, False, False]
State prediction error at timestep 895 is tensor(2.3654e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of 0
Current timestep = 896. State = [[-0.15999728 -0.16878754]]. Action = [[-0.04265913 -0.05495349 -0.07610612  0.34388065]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, True, False, False]
State prediction error at timestep 896 is tensor(1.0026e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of 0
Current timestep = 897. State = [[-0.16048004 -0.16839084]]. Action = [[ 0.06645482  0.08660085 -0.05343105  0.15268195]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, True, False, False]
State prediction error at timestep 897 is tensor(1.4362e-05, grad_fn=<MseLossBackward0>)
Current timestep = 898. State = [[-0.16041477 -0.16580492]]. Action = [[-4.0605891e-02  2.4800539e-02  6.4477324e-05 -5.9234041e-01]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, True, False, False]
State prediction error at timestep 898 is tensor(3.6797e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 0
Current timestep = 899. State = [[-0.1604145  -0.16346757]]. Action = [[ 0.03908438  0.08577742 -0.0055113   0.49991608]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, True, False, False]
State prediction error at timestep 899 is tensor(1.7930e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of 0
Current timestep = 900. State = [[-0.16029616 -0.15916632]]. Action = [[-0.07811226  0.04901991  0.03519947 -0.8710353 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, True, False, False]
State prediction error at timestep 900 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 901. State = [[-0.25812048  0.00821003]]. Action = [[ 0.02287481  0.09595444  0.07456035 -0.22524726]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(0.0201, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of 0
Current timestep = 902. State = [[-0.2573826   0.00762583]]. Action = [[-0.07082898  0.09704841 -0.08821519 -0.4745711 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(7.2209e-05, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[-0.25703487  0.00726605]]. Action = [[ 0.08229385  0.08466619  0.00370093 -0.04812056]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(2.4143e-07, grad_fn=<MseLossBackward0>)
Current timestep = 904. State = [[-0.25698015  0.00710542]]. Action = [[ 0.00111123 -0.04697387  0.04774087 -0.66888833]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(6.5070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 905. State = [[-0.25686225  0.0070465 ]]. Action = [[ 0.03384741 -0.06677881 -0.02983838  0.8437929 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(3.3060e-05, grad_fn=<MseLossBackward0>)
Current timestep = 906. State = [[-0.25685307  0.00686117]]. Action = [[ 0.03614245 -0.06698088  0.07028877  0.2851622 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 906 is [True, False, False, False, True, False]
State prediction error at timestep 906 is tensor(2.6689e-06, grad_fn=<MseLossBackward0>)
Current timestep = 907. State = [[-0.25682935  0.00687595]]. Action = [[-0.03832386  0.07469449  0.01993487  0.61916685]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 907 is [True, False, False, False, True, False]
State prediction error at timestep 907 is tensor(2.5379e-05, grad_fn=<MseLossBackward0>)
Current timestep = 908. State = [[-0.2564621   0.00705164]]. Action = [[ 0.08064988  0.05742862  0.04571832 -0.55438554]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(1.9450e-05, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.256851    0.00673396]]. Action = [[-0.06209224 -0.03553773  0.07295973  0.23610032]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 909 is [True, False, False, False, True, False]
State prediction error at timestep 909 is tensor(4.7753e-06, grad_fn=<MseLossBackward0>)
Current timestep = 910. State = [[-0.25668162  0.00668947]]. Action = [[ 0.06010509  0.0762713  -0.09072345  0.21224356]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(1.5843e-05, grad_fn=<MseLossBackward0>)
Current timestep = 911. State = [[-0.25685602  0.00667561]]. Action = [[ 0.06526088 -0.01045112  0.07109509 -0.31797993]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 911 is [True, False, False, False, True, False]
State prediction error at timestep 911 is tensor(1.6320e-05, grad_fn=<MseLossBackward0>)
Current timestep = 912. State = [[-0.25675976  0.00674852]]. Action = [[ 0.02104406 -0.08971821 -0.02590473 -0.3917489 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(4.5667e-05, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[-0.25681627  0.00664416]]. Action = [[ 0.01984894 -0.02439962 -0.06895153  0.20460439]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 913 is [True, False, False, False, True, False]
State prediction error at timestep 913 is tensor(8.9586e-06, grad_fn=<MseLossBackward0>)
