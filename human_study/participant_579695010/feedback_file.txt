Current timestep = 0. State = [[-0.32600108  0.07271419]]. Action = [[0.00547141 0.09844287 0.         0.72573614]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0707, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33165324  0.07564779]]. Action = [[-0.08991172 -0.00939942  0.          0.92522764]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0629, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.3327267   0.07375138]]. Action = [[ 0.06055454 -0.05036603  0.          0.0222559 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0551, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.32805854  0.07549114]]. Action = [[ 0.08049697  0.0493673   0.         -0.50405097]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0355, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32509932  0.07515267]]. Action = [[ 0.02328773 -0.05199834  0.          0.06493044]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0423, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.32763597  0.07605463]]. Action = [[-0.06590287  0.04403711  0.         -0.71666926]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0182, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.33447975  0.07240126]]. Action = [[-0.0944499  -0.09542103  0.         -0.55179054]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0163, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.34204453  0.06976098]]. Action = [[-0.08649857  0.01675485  0.          0.75424707]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0301, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.34271887  0.06625143]]. Action = [[ 0.06662089 -0.0699874   0.          0.36496592]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0284, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.33857858  0.06122486]]. Action = [[ 0.05232341 -0.04853386  0.          0.96739995]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.3408226   0.06342527]]. Action = [[-0.09423078  0.09772468  0.          0.9580879 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0146, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.3475621   0.07009299]]. Action = [[-0.06761932  0.08877895  0.         -0.7775144 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.35214615  0.06887522]]. Action = [[-0.02129613 -0.08025254  0.         -0.9554103 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35108864  0.06247583]]. Action = [[ 0.06954683 -0.08383784  0.          0.8053738 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.348669    0.05622814]]. Action = [[ 0.03646707 -0.073333    0.          0.9921409 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.34878284  0.05190501]]. Action = [[-0.00998233 -0.03214703  0.         -0.21259898]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.35129938  0.0451481 ]]. Action = [[-0.03489444 -0.09524914  0.         -0.4697644 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.35015348  0.04454947]]. Action = [[ 0.06351859  0.08017237  0.         -0.04260564]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.35226253  0.04923641]]. Action = [[-0.07545817  0.06545127  0.         -0.8053869 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.35565478  0.05160168]]. Action = [[-0.01581043  0.01787832  0.         -0.6201807 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.3527198   0.05373359]]. Action = [[0.08636468 0.02660901 0.         0.3157115 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.35505718  0.05531685]]. Action = [[-0.09389018  0.01115171  0.          0.03352892]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.35583586  0.05154463]]. Action = [[ 0.06814935 -0.0928605   0.         -0.5474867 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.35019192  0.04454175]]. Action = [[ 0.08828076 -0.09024743  0.          0.20450985]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.34371933  0.037613  ]]. Action = [[ 0.07311254 -0.08337417  0.         -0.6372826 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.3377626  0.0312809]]. Action = [[ 0.05850912 -0.0661875   0.         -0.71714765]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.3339746   0.02556366]]. Action = [[ 0.01690851 -0.05249044  0.         -0.50393885]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.3284219   0.01998079]]. Action = [[ 0.07454141 -0.0490321   0.         -0.8450591 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.32831112  0.01981007]]. Action = [[-0.08058392  0.07320239  0.          0.7185173 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.3263681   0.02339332]]. Action = [[ 0.07710604  0.05856781  0.         -0.20689023]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.32202098  0.0269317 ]]. Action = [[0.03713436 0.04328338 0.         0.91617584]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(9.8114e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.32089257  0.03093022]]. Action = [[-0.00833923  0.05610118  0.         -0.69518423]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.3165551   0.02935699]]. Action = [[ 0.09156225 -0.07232538  0.         -0.01507038]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.31441492  0.03036088]]. Action = [[-0.03141296  0.06527863  0.         -0.43838072]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of -1
Current timestep = 34. State = [[-0.31669658  0.03504403]]. Action = [[-0.03604127  0.05576291  0.         -0.9840064 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.31549123  0.03597732]]. Action = [[ 0.04699447 -0.02668592  0.          0.7338867 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of -1
Current timestep = 36. State = [[-0.317594    0.03331994]]. Action = [[-0.07415704 -0.0477575   0.         -0.9383558 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.32064575  0.02735802]]. Action = [[-0.02413616 -0.08675891  0.         -0.6684023 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.31994778  0.02575415]]. Action = [[ 0.02521127  0.02694794  0.         -0.6811435 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.32304928  0.02983744]]. Action = [[-0.08591872  0.07468108  0.         -0.5000256 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.32302535  0.02848358]]. Action = [[ 0.06205731 -0.07921204  0.         -0.47548997]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.3159643   0.02449581]]. Action = [[ 0.09679215 -0.03566115  0.         -0.42129773]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.31017452  0.02819695]]. Action = [[ 0.03650676  0.09633122  0.         -0.67623407]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.3087903   0.02942725]]. Action = [[-0.01206388 -0.03420977  0.          0.8856182 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
