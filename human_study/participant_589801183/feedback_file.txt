Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.33283383 -0.08530019]]. Action = [[ 0.05919518 -0.05022885  0.          0.01753342]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.32827216 -0.08361269]]. Action = [[ 0.07941671  0.04839007  0.         -0.508215  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32541144 -0.08398328]]. Action = [[ 0.02240024 -0.05187739  0.          0.0605756 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.32796615 -0.0831292 ]]. Action = [[-0.0650194   0.04303385  0.         -0.7176986 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.33477747 -0.08681659]]. Action = [[-0.09400087 -0.09524944  0.         -0.55304015]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.34229738 -0.08951566]]. Action = [[-0.08571257  0.01567943  0.          0.7501602 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.34299225 -0.09307053]]. Action = [[ 0.06539033 -0.06981484  0.          0.3592732 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.3389631  -0.09811448]]. Action = [[ 0.05115738 -0.04872453  0.          0.96607924]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.3412554  -0.09593639]]. Action = [[-0.09367862  0.09747539  0.          0.95626867]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.3479356  -0.08932666]]. Action = [[-0.06635894  0.08790591  0.         -0.774692  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.35245597 -0.09056668]]. Action = [[-0.02058688 -0.07997913  0.         -0.95380795]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35142124 -0.09695354]]. Action = [[ 0.06858609 -0.08357317  0.          0.79918194]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.34904855 -0.10319182]]. Action = [[ 0.03613687 -0.07322273  0.          0.9914675 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.34913805 -0.10754969]]. Action = [[-0.00908633 -0.03292213  0.         -0.21378112]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.35156548 -0.11431763]]. Action = [[-0.03349395 -0.09510357  0.         -0.46699262]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.35036954 -0.11494114]]. Action = [[ 0.06326402  0.07898477  0.         -0.04648042]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 0
Current timestep = 18. State = [[-0.3524208  -0.11037341]]. Action = [[-0.07421136  0.06380767  0.         -0.7994707 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 0
Current timestep = 19. State = [[-0.35569212 -0.10817   ]]. Action = [[-0.01442942  0.01605977  0.         -0.613768  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.1655e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 0
Current timestep = 20. State = [[-0.35269898 -0.1062247 ]]. Action = [[0.08605801 0.02472059 0.         0.30756378]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 0
Current timestep = 21. State = [[-0.35499668 -0.10482574]]. Action = [[-0.09333936  0.0094524   0.          0.03082192]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 0
Current timestep = 22. State = [[-0.35572195 -0.10868988]]. Action = [[ 0.06805927 -0.09262836  0.         -0.5395236 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of 0
Current timestep = 23. State = [[-0.35005438 -0.11571436]]. Action = [[ 0.08803911 -0.08997103  0.          0.20055556]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of 0
Current timestep = 24. State = [[-0.34356162 -0.12265086]]. Action = [[ 0.07301382 -0.08306853  0.         -0.6268091 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 0
Current timestep = 25. State = [[-0.3375757  -0.12899116]]. Action = [[ 0.05871179 -0.06603482  0.         -0.70631063]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 0
Current timestep = 26. State = [[-0.33370787 -0.13472325]]. Action = [[ 0.01810668 -0.05253109  0.         -0.49265766]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 0
Current timestep = 27. State = [[-0.32810998 -0.14031903]]. Action = [[ 0.07450754 -0.04904148  0.         -0.83598334]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 0
Current timestep = 28. State = [[-0.32794258 -0.14056173]]. Action = [[-0.07928063  0.07185676  0.          0.7104436 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 0
Current timestep = 29. State = [[-0.32593182 -0.13711621]]. Action = [[ 0.07710012  0.05715362  0.         -0.198829  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 0
Current timestep = 30. State = [[-0.32150334 -0.1337179 ]]. Action = [[0.03813281 0.04195628 0.         0.9116266 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of 0
Current timestep = 31. State = [[-0.3202199  -0.12985261]]. Action = [[-0.00640337  0.05480323  0.         -0.6848134 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 0
Current timestep = 32. State = [[-0.31578213 -0.13147189]]. Action = [[ 0.09153336 -0.07173029  0.         -0.01121795]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 0
Current timestep = 33. State = [[-0.3135358  -0.13052212]]. Action = [[-0.02931146  0.06415439  0.         -0.43019086]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 0
Current timestep = 34. State = [[-0.31563216 -0.12594406]]. Action = [[-0.03398895  0.05468405  0.         -0.98290426]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 0
Current timestep = 35. State = [[-0.31426185 -0.12505919]]. Action = [[ 0.04821243 -0.02647512  0.          0.7301724 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 0
Current timestep = 36. State = [[-0.31623104 -0.12770213]]. Action = [[-0.07290413 -0.04720927  0.         -0.93639827]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(9.2648e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of 0
Current timestep = 37. State = [[-0.31910434 -0.13363588]]. Action = [[-0.02201471 -0.08627918  0.         -0.6652644 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.31820083 -0.13524592]]. Action = [[ 0.02707881  0.02660406  0.         -0.6796875 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of 0
Current timestep = 39. State = [[-0.32115898 -0.13120662]]. Action = [[-0.08520211  0.07422533  0.         -0.5008991 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of 0
Current timestep = 40. State = [[-0.3209973  -0.13256584]]. Action = [[ 0.06328062 -0.07882921  0.         -0.47839212]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of 0
Current timestep = 41. State = [[-0.31383577 -0.1365398 ]]. Action = [[ 0.09691023 -0.03535116  0.         -0.42650068]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of 0
Current timestep = 42. State = [[-0.30789572 -0.13284285]]. Action = [[ 0.03845876  0.09628122  0.         -0.681611  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 0
Current timestep = 43. State = [[-0.3062879  -0.13161917]]. Action = [[-0.00976268 -0.0340964   0.          0.88505244]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 0
Current timestep = 44. State = [[-0.30358097 -0.13187785]]. Action = [[ 0.04565109  0.01297488  0.         -0.2828917 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 0
Current timestep = 45. State = [[-0.2987159  -0.13341387]]. Action = [[ 0.05789668 -0.04023519  0.          0.5336306 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 0
Current timestep = 46. State = [[-0.2953166  -0.13144924]]. Action = [[0.01713137 0.06093697 0.         0.5922203 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 0
Current timestep = 47. State = [[-0.2953067  -0.12509061]]. Action = [[-0.02519311  0.09039479  0.         -0.20252001]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 0
Current timestep = 48. State = [[-0.29194456 -0.12418628]]. Action = [[ 0.07741744 -0.04967476  0.         -0.71626663]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(4.5995e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of 0
Current timestep = 49. State = [[-0.28526947 -0.12123484]]. Action = [[ 0.07172871  0.07190325  0.         -0.43799567]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 0
Current timestep = 50. State = [[-0.27969965 -0.11433603]]. Action = [[ 0.04947998  0.07713526  0.         -0.83602506]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 0
Current timestep = 51. State = [[-0.2818756  -0.11055463]]. Action = [[-0.09371483  0.00941133  0.          0.40181994]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of 0
Current timestep = 52. State = [[-0.28499776 -0.11371608]]. Action = [[-0.02686214 -0.08196346  0.         -0.46882725]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 0
Current timestep = 53. State = [[-0.2884941  -0.11720055]]. Action = [[-0.06948528 -0.02584495  0.         -0.29223037]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of 0
Current timestep = 54. State = [[-0.28745416 -0.1220984 ]]. Action = [[ 0.04853251 -0.08146951  0.         -0.00697434]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of 0
Current timestep = 55. State = [[-0.28742218 -0.12830463]]. Action = [[-0.0463539  -0.0689946   0.          0.31398678]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of 0
Current timestep = 56. State = [[-0.2881103  -0.13423045]]. Action = [[-0.00851305 -0.05730429  0.         -0.2234714 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of 0
Current timestep = 57. State = [[-0.29062936 -0.13427757]]. Action = [[-0.06258044  0.06039272  0.          0.86289644]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of 0
Current timestep = 58. State = [[-0.28846502 -0.1301276 ]]. Action = [[0.0746714  0.06613565 0.         0.25168443]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of 0
Current timestep = 59. State = [[-0.28625256 -0.12493194]]. Action = [[-0.00084428  0.0650444   0.         -0.5659367 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of 0
Current timestep = 60. State = [[-0.28321773 -0.1237359 ]]. Action = [[ 0.06075054 -0.02193963  0.          0.4269824 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of 0
Current timestep = 61. State = [[-0.2854892  -0.12478598]]. Action = [[-0.08422114 -0.01133034  0.         -0.5997571 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of 0
Current timestep = 62. State = [[-0.28648463 -0.12703787]]. Action = [[ 0.03444684 -0.03666675  0.          0.7842412 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of 0
Current timestep = 63. State = [[-0.2835371  -0.12744467]]. Action = [[ 0.04871619  0.01054291  0.         -0.49124122]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of 0
Current timestep = 64. State = [[-0.2843857  -0.12761854]]. Action = [[-0.04287987 -0.01185901  0.          0.20115936]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of 0
Current timestep = 65. State = [[-0.28710848 -0.12818241]]. Action = [[-0.02886529 -0.00081676  0.          0.6119878 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of 0
Current timestep = 66. State = [[-0.29262978 -0.13087775]]. Action = [[-0.09019432 -0.04489497  0.          0.82639   ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of 0
Current timestep = 67. State = [[-0.29769567 -0.13756856]]. Action = [[-0.0433207  -0.09236866  0.         -0.8849174 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of 0
Current timestep = 68. State = [[-0.29699156 -0.14306529]]. Action = [[ 0.04980833 -0.03792502  0.          0.02340651]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of 0
Current timestep = 69. State = [[-0.29300725 -0.14449963]]. Action = [[ 0.05834284  0.00754051  0.         -0.02966201]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of 0
Current timestep = 70. State = [[-0.2871841 -0.1472961]]. Action = [[ 0.08560225 -0.05231243  0.         -0.6912577 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-0.28805673 -0.15137379]]. Action = [[-0.074531   -0.03360166  0.          0.55474484]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of 0
Current timestep = 72. State = [[-0.29291704 -0.15476827]]. Action = [[-0.04940789 -0.02205897  0.         -0.60720074]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of 0
Current timestep = 73. State = [[-0.29224455 -0.15775628]]. Action = [[ 0.05659435 -0.02267543  0.          0.8287406 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of 0
Current timestep = 74. State = [[-0.2915041 -0.163716 ]]. Action = [[-0.00880613 -0.08630516  0.          0.65559244]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of 0
Current timestep = 75. State = [[-0.28822443 -0.1687612 ]]. Action = [[ 0.07383271 -0.02719856  0.          0.10695744]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of 0
Current timestep = 76. State = [[-0.28197655 -0.1725253 ]]. Action = [[ 0.08216514 -0.03802353  0.          0.1763972 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of 0
Current timestep = 77. State = [[-0.28158194 -0.17643434]]. Action = [[-0.0478723  -0.03099365  0.          0.7124344 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of 0
Current timestep = 78. State = [[-0.27866316 -0.17454538]]. Action = [[ 0.08396441  0.08618654  0.         -0.8896944 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of 0
Current timestep = 79. State = [[-0.27653906 -0.17249994]]. Action = [[-0.0054344   0.00659847  0.          0.44935918]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of 0
Current timestep = 80. State = [[-0.27244186 -0.17581847]]. Action = [[ 0.08390147 -0.06315447  0.         -0.71619064]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of 0
Current timestep = 81. State = [[-0.27324775 -0.1744213 ]]. Action = [[-0.08384524  0.08946116  0.         -0.37685597]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of 0
Current timestep = 82. State = [[-0.27406996 -0.16998059]]. Action = [[ 0.04202057  0.04038448  0.         -0.3641023 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of 0
Current timestep = 83. State = [[-0.27060512 -0.1702501 ]]. Action = [[ 0.06119854 -0.04613821  0.          0.6787491 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of 0
Current timestep = 84. State = [[-0.27069828 -0.16665864]]. Action = [[-0.03549282  0.08843083  0.          0.37523985]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of 0
Current timestep = 85. State = [[-0.27700573 -0.16484064]]. Action = [[-0.09278088 -0.02584873  0.         -0.23937535]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of 0
Current timestep = 86. State = [[-0.28283215 -0.16267517]]. Action = [[-0.0448271   0.04821933  0.         -0.08111036]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of 0
Current timestep = 87. State = [[-0.2886163  -0.15954378]]. Action = [[-0.06970582  0.02482589  0.         -0.90382576]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of 0
Current timestep = 88. State = [[-0.289613   -0.15490194]]. Action = [[0.04082765 0.06066138 0.         0.11045349]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of 0
Current timestep = 89. State = [[-0.2874697  -0.14734745]]. Action = [[0.04042859 0.08460758 0.         0.83149934]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of 0
Current timestep = 90. State = [[-0.28517884 -0.14403777]]. Action = [[ 0.04013277 -0.02332377  0.         -0.36090517]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of 0
Current timestep = 91. State = [[-0.28570685 -0.1478252 ]]. Action = [[-0.0186253  -0.09843019  0.          0.7888503 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of 0
Current timestep = 92. State = [[-0.28893825 -0.1531257 ]]. Action = [[-0.04505621 -0.06335823  0.          0.07182121]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of 0
Current timestep = 93. State = [[-0.2882568 -0.150912 ]]. Action = [[ 0.0484058   0.08141381  0.         -0.8303153 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of 0
Current timestep = 94. State = [[-0.2881694  -0.15041824]]. Action = [[-0.02086829 -0.04484574  0.         -0.6519496 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of 0
Current timestep = 95. State = [[-0.28808472 -0.1474686 ]]. Action = [[ 0.01496804  0.07876164  0.         -0.91545886]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of 0
Current timestep = 96. State = [[-0.2887966  -0.14782824]]. Action = [[-0.01933333 -0.05797131  0.         -0.64594954]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of 0
Current timestep = 97. State = [[-0.2857511  -0.14973561]]. Action = [[ 0.07398822 -0.01027647  0.         -0.09954065]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of 0
Current timestep = 98. State = [[-0.28308532 -0.14892265]]. Action = [[ 0.00929968  0.01836927  0.         -0.6431314 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of 0
Current timestep = 99. State = [[-0.28163025 -0.14413498]]. Action = [[0.02013288 0.0818939  0.         0.11399341]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of 0
Current timestep = 100. State = [[-0.2778353  -0.13912712]]. Action = [[ 0.06592128  0.04022164  0.         -0.62038755]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(4.4439e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of 0
Current timestep = 101. State = [[-0.27149853 -0.13251773]]. Action = [[ 0.09157861  0.08375707  0.         -0.90079355]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of 0
Current timestep = 102. State = [[-0.26918778 -0.12657185]]. Action = [[-0.00512664  0.04139107  0.         -0.544078  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(1.2990e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of 0
Current timestep = 103. State = [[-0.2661908  -0.12755455]]. Action = [[ 0.06297589 -0.07295806  0.          0.44006848]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of 0
Current timestep = 104. State = [[-0.26073292 -0.12749518]]. Action = [[ 0.06666677  0.01970188  0.         -0.32772022]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(1.2279e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of 0
Current timestep = 105. State = [[-0.25450233 -0.12966819]]. Action = [[ 0.0712762  -0.07315929  0.         -0.26682663]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(3.1443e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of 0
Current timestep = 106. State = [[-0.25009167 -0.12630285]]. Action = [[ 0.02275755  0.09785328  0.         -0.15117699]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(3.7806e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of 0
Current timestep = 107. State = [[-0.24557005 -0.12420354]]. Action = [[ 0.05855937 -0.02638096  0.          0.53865874]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of 0
Current timestep = 108. State = [[-0.23910889 -0.12378833]]. Action = [[0.07333405 0.00795484 0.         0.56893766]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of 0
Current timestep = 109. State = [[-0.23647851 -0.12203508]]. Action = [[-0.01802902  0.02184683  0.          0.5064368 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of 0
Current timestep = 110. State = [[-0.2365142  -0.11680565]]. Action = [[-0.02154619  0.08635712  0.          0.48070884]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(7.5463e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of 0
Current timestep = 111. State = [[-0.23734564 -0.11002743]]. Action = [[-0.02856217  0.07377481  0.          0.1190418 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of 0
Current timestep = 112. State = [[-0.23674792 -0.11133878]]. Action = [[ 0.00673284 -0.09003825  0.         -0.8231352 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of 0
Current timestep = 113. State = [[-0.23144816 -0.11716656]]. Action = [[ 0.07789228 -0.08105147  0.          0.74330497]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of 0
Current timestep = 114. State = [[-0.23086855 -0.11959903]]. Action = [[-0.07032518 -0.00132728  0.         -0.15887576]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of 0
Current timestep = 115. State = [[-0.22901943 -0.11512344]]. Action = [[0.05518837 0.09037782 0.         0.49436736]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of 0
Current timestep = 116. State = [[-0.22229904 -0.11633985]]. Action = [[ 0.09735719 -0.08889578  0.          0.41435707]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of 0
Current timestep = 117. State = [[-0.21851607 -0.12068074]]. Action = [[-2.1629781e-04 -4.0493857e-02  0.0000000e+00  8.5232437e-01]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of 0
Current timestep = 118. State = [[-0.21690471 -0.11773477]]. Action = [[ 0.00973455  0.09122933  0.         -0.6550857 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of 0
Current timestep = 119. State = [[-0.21840537 -0.11197343]]. Action = [[-0.05180057  0.06640074  0.         -0.20739245]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of 0
Current timestep = 120. State = [[-0.21708888 -0.1117653 ]]. Action = [[ 0.04626902 -0.04185598  0.          0.17478764]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of 0
Current timestep = 121. State = [[-0.21060681 -0.11685105]]. Action = [[ 0.09187887 -0.08393241  0.          0.22131526]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of 0
Current timestep = 122. State = [[-0.20562646 -0.11877634]]. Action = [[0.02764151 0.01345252 0.         0.65190315]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of 0
Current timestep = 123. State = [[-0.20731618 -0.1221416 ]]. Action = [[-0.07684247 -0.06470343  0.         -0.40492934]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of 0
Current timestep = 124. State = [[-0.20565967 -0.12078559]]. Action = [[0.0543722  0.07823739 0.         0.06218743]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of 0
Current timestep = 125. State = [[-0.20005946 -0.11580966]]. Action = [[ 0.07090683  0.05742917  0.         -0.7700217 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of 0
Current timestep = 126. State = [[-0.1941543 -0.1158762]]. Action = [[ 0.06658774 -0.04286771  0.         -0.09827697]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of 0
Current timestep = 127. State = [[-0.18990621 -0.11326096]]. Action = [[0.03208656 0.07177106 0.         0.65603817]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of 0
Current timestep = 128. State = [[-0.1897957  -0.11169478]]. Action = [[-0.03261874 -0.01472062  0.         -0.7546568 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of 0
Current timestep = 129. State = [[-0.18815586 -0.11434912]]. Action = [[ 0.03253884 -0.05074613  0.         -0.9258497 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of 0
Current timestep = 130. State = [[-0.18631515 -0.11755543]]. Action = [[-0.00167721 -0.03513904  0.          0.8974283 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of 0
Current timestep = 131. State = [[-0.18296197 -0.11865694]]. Action = [[ 0.04604826  0.00283255  0.         -0.8864463 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of 0
Current timestep = 132. State = [[-0.18473528 -0.12050539]]. Action = [[-0.08736977 -0.03236939  0.          0.831537  ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of 0
Current timestep = 133. State = [[-0.18340375 -0.12174336]]. Action = [[ 0.05552267  0.001442    0.         -0.45933962]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of 0
Current timestep = 134. State = [[-0.18119457 -0.11752658]]. Action = [[-0.00112186  0.08918526  0.          0.13485193]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of 0
Current timestep = 135. State = [[-0.17766991 -0.11158764]]. Action = [[ 0.06127089  0.06259912  0.         -0.08151269]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of 0
Current timestep = 136. State = [[-0.17656195 -0.11299466]]. Action = [[-0.0207514  -0.07836094  0.         -0.8863528 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of 0
Current timestep = 137. State = [[-0.17734265 -0.11482531]]. Action = [[-0.01846104  0.00334145  0.         -0.01535326]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of 0
Current timestep = 138. State = [[-0.1746943  -0.11823182]]. Action = [[ 0.05359723 -0.06963129  0.          0.8098401 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of 0
Current timestep = 139. State = [[-0.1682913  -0.12492058]]. Action = [[ 0.08821697 -0.09015361  0.         -0.04433542]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of 0
Current timestep = 140. State = [[-0.16232318 -0.12788227]]. Action = [[ 0.05389426  0.00730792  0.         -0.4460501 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, True, False, False]
State prediction error at timestep 140 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of 0
Current timestep = 141. State = [[-0.16320045 -0.12477575]]. Action = [[-0.0746861  0.0797127  0.        -0.6628413]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of 0
Current timestep = 142. State = [[-0.16445154 -0.12143139]]. Action = [[0.00449593 0.03135516 0.         0.04296744]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of 0
Current timestep = 143. State = [[-0.16570662 -0.12537552]]. Action = [[-0.03308395 -0.09504083  0.          0.63216865]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, True, False, False]
State prediction error at timestep 143 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of 0
Current timestep = 144. State = [[-0.16199574 -0.12528592]]. Action = [[0.08906747 0.06382684 0.         0.23596   ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, True, False, False]
State prediction error at timestep 144 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of 0
Current timestep = 145. State = [[-0.16337878 -0.12691508]]. Action = [[-0.08703422 -0.06651594  0.          0.39428318]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of 0
Current timestep = 146. State = [[-0.16858663 -0.1253751 ]]. Action = [[-0.06903772  0.07527956  0.          0.65790284]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, True, False, False]
State prediction error at timestep 146 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of 0
Current timestep = 147. State = [[-0.16744591 -0.12744482]]. Action = [[ 0.07215995 -0.08571282  0.          0.08622551]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, True, False, False]
State prediction error at timestep 147 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of 0
Current timestep = 148. State = [[-0.16251805 -0.12985556]]. Action = [[ 0.05076673  0.00524169  0.         -0.77040684]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of 0
Current timestep = 149. State = [[-0.15775838 -0.13436721]]. Action = [[ 0.05248448 -0.08273879  0.         -0.9417343 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of 0
Current timestep = 150. State = [[-0.15093571 -0.13367319]]. Action = [[0.09446622 0.07400281 0.         0.3938477 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of 0
Current timestep = 151. State = [[-0.14482245 -0.1330826 ]]. Action = [[ 0.0578906 -0.0209157  0.        -0.5950246]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of 0
Current timestep = 152. State = [[-0.14413491 -0.13815582]]. Action = [[-0.0396232  -0.08003525  0.          0.21218538]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of 0
Current timestep = 153. State = [[-0.1430853  -0.13649017]]. Action = [[0.02115884 0.09765909 0.         0.73247564]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of 0
Current timestep = 154. State = [[-0.1434618  -0.13199802]]. Action = [[-0.03645843  0.04333121  0.         -0.8655245 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of 0
Current timestep = 155. State = [[-0.14514485 -0.1348251 ]]. Action = [[-0.03284249 -0.08242241  0.         -0.33787656]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of 0
Current timestep = 156. State = [[-0.14539371 -0.13537341]]. Action = [[-0.00402632  0.04010136  0.         -0.25686413]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, True, False, False]
State prediction error at timestep 156 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of 0
Current timestep = 157. State = [[-0.14335269 -0.13262126]]. Action = [[0.03159044 0.03323729 0.         0.5706034 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of 0
Current timestep = 158. State = [[-0.14247485 -0.13101016]]. Action = [[-0.00985453  0.00564321  0.         -0.6034985 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of 0
Current timestep = 159. State = [[-0.14213426 -0.12563474]]. Action = [[ 0.00426531  0.09493632  0.         -0.4931987 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, True, False, False]
State prediction error at timestep 159 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of 0
Current timestep = 160. State = [[-0.14189854 -0.12298031]]. Action = [[ 3.5442412e-05 -1.8679552e-02  0.0000000e+00  2.7844596e-01]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of 0
Current timestep = 161. State = [[-0.14086369 -0.12078672]]. Action = [[ 0.01907746  0.03446578  0.         -0.7617268 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of 0
Current timestep = 162. State = [[-0.13804898 -0.11873782]]. Action = [[0.0472183  0.00189756 0.         0.11326265]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of 0
Current timestep = 163. State = [[-0.1365365  -0.11896931]]. Action = [[ 0.00292052 -0.02478044  0.          0.6250473 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of 0
Current timestep = 164. State = [[-0.14012715 -0.11709715]]. Action = [[-0.08358517  0.04004999  0.         -0.15477878]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of 0
Current timestep = 165. State = [[-0.14415888 -0.11794981]]. Action = [[-0.0409851  -0.05066758  0.          0.88960326]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of 0
Current timestep = 166. State = [[-0.144143   -0.12289067]]. Action = [[ 0.02195378 -0.07723039  0.          0.49816525]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of 0
Current timestep = 167. State = [[-0.14536355 -0.12216714]]. Action = [[-0.04008989  0.06176912  0.          0.39635253]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of 0
Current timestep = 168. State = [[-0.14218457 -0.12495983]]. Action = [[ 0.09721615 -0.09250093  0.          0.54179525]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of 0
Current timestep = 169. State = [[-0.13467118 -0.1292834 ]]. Action = [[ 0.09560659 -0.02610098  0.         -0.93137133]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, True, False, False]
State prediction error at timestep 169 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of 0
Current timestep = 170. State = [[-0.12765218 -0.13522285]]. Action = [[ 0.07704667 -0.08520547  0.         -0.7747612 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, True, False, False]
State prediction error at timestep 170 is tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of 0
Current timestep = 171. State = [[-0.12822723 -0.13687553]]. Action = [[-0.08790567  0.04791468  0.         -0.89421666]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, True, False, False]
State prediction error at timestep 171 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of 0
Current timestep = 172. State = [[-0.12677036 -0.13975106]]. Action = [[ 0.08159532 -0.06451727  0.          0.90293884]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, True, False, False]
State prediction error at timestep 172 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of 0
Current timestep = 173. State = [[-0.12564184 -0.14624234]]. Action = [[-0.0315484  -0.06757905  0.          0.8827305 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, True, False, False]
State prediction error at timestep 173 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of 0
Current timestep = 174. State = [[-0.12593138 -0.1533443 ]]. Action = [[-0.00652739 -0.06785952  0.         -0.90552956]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, True, False, False]
State prediction error at timestep 174 is tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of 0
Current timestep = 175. State = [[-0.12592392 -0.15247263]]. Action = [[-0.0109341   0.09495798  0.         -0.7826034 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of 0
Current timestep = 176. State = [[-0.12409142 -0.15137543]]. Action = [[ 0.03332441 -0.00560772  0.         -0.3280425 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, True, False, False]
State prediction error at timestep 176 is tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of 0
Current timestep = 177. State = [[-0.12131238 -0.15243164]]. Action = [[ 0.0303778  -0.00284658  0.          0.26337624]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, True, False, False]
State prediction error at timestep 177 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of 0
Current timestep = 178. State = [[-0.12429127 -0.15629071]]. Action = [[-0.09030387 -0.0590042   0.          0.34302974]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, True, False, False]
State prediction error at timestep 178 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of 0
Current timestep = 179. State = [[-0.12525442 -0.16110635]]. Action = [[ 0.01910595 -0.04295989  0.          0.9485997 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, True, False, False]
State prediction error at timestep 179 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of 0
Current timestep = 180. State = [[-0.12246138 -0.15931338]]. Action = [[ 0.0418985   0.07867054  0.         -0.79056174]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, True, False, False]
State prediction error at timestep 180 is tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of 0
Current timestep = 181. State = [[-0.11727956 -0.15918715]]. Action = [[ 0.08059294 -0.03582872  0.         -0.8045928 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, True, False, False]
State prediction error at timestep 181 is tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of 0
Current timestep = 182. State = [[-0.11415466 -0.15744723]]. Action = [[0.01518637 0.05824325 0.         0.34820116]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, True, False, False]
State prediction error at timestep 182 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of 0
Current timestep = 183. State = [[-0.11657485 -0.15108982]]. Action = [[-0.06438921  0.09544268  0.          0.3464427 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, True, False, False]
State prediction error at timestep 183 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of 0
Current timestep = 184. State = [[-0.12245655 -0.14901946]]. Action = [[-0.08988278 -0.02241747  0.          0.7484474 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, True, False, False]
State prediction error at timestep 184 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of 0
Current timestep = 185. State = [[-0.12938435 -0.1536143 ]]. Action = [[-0.0956938  -0.08899031  0.          0.23952293]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, True, False, False]
State prediction error at timestep 185 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of 0
Current timestep = 186. State = [[-0.12817544 -0.15735136]]. Action = [[ 0.08558352 -0.0275173   0.         -0.122109  ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, True, False, False]
State prediction error at timestep 186 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of 0
Current timestep = 187. State = [[-0.12913473 -0.15596485]]. Action = [[-0.07512002  0.04842662  0.          0.19973278]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, True, False, False]
State prediction error at timestep 187 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.1311178  -0.15012208]]. Action = [[0.00573438 0.08380113 0.         0.91623604]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of 0
Current timestep = 189. State = [[-0.1338641  -0.14478439]]. Action = [[-0.04406718  0.04188579  0.          0.76701677]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, True, False, False]
State prediction error at timestep 189 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of 0
Current timestep = 190. State = [[-0.13912977 -0.1472766 ]]. Action = [[-0.06906849 -0.0927167   0.          0.7116102 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, True, False, False]
State prediction error at timestep 190 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of 0
Current timestep = 191. State = [[-0.13947564 -0.14569442]]. Action = [[ 0.05031385  0.0749427   0.         -0.6269659 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of 0
Current timestep = 192. State = [[-0.13786456 -0.14150728]]. Action = [[0.02697093 0.02506966 0.         0.74486744]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of 0
Current timestep = 193. State = [[-0.13800523 -0.13500816]]. Action = [[0.0018486  0.09323383 0.         0.14930034]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, True, False, False]
State prediction error at timestep 193 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of 0
Current timestep = 194. State = [[-0.13642228 -0.13347761]]. Action = [[ 0.05331872 -0.04899274  0.          0.7990519 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, True, False, False]
State prediction error at timestep 194 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of 0
Current timestep = 195. State = [[-0.1368512  -0.13185339]]. Action = [[-0.02239837  0.03841927  0.         -0.6688639 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, True, False, False]
State prediction error at timestep 195 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of 0
Current timestep = 196. State = [[-0.13546975 -0.13436437]]. Action = [[ 0.05547095 -0.08906803  0.          0.78372335]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, True, False, False]
State prediction error at timestep 196 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of 0
Current timestep = 197. State = [[-0.13436618 -0.13773178]]. Action = [[-0.00090816 -0.02493981  0.          0.02042449]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of 0
Current timestep = 198. State = [[-0.13379508 -0.14118779]]. Action = [[ 0.01232813 -0.05151917  0.          0.424232  ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, True, False, False]
State prediction error at timestep 198 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of 0
Current timestep = 199. State = [[-0.13635316 -0.14151955]]. Action = [[-0.0639169  0.0323512  0.        -0.8081414]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, True, False, False]
State prediction error at timestep 199 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of 0
Current timestep = 200. State = [[-0.13456859 -0.13956264]]. Action = [[0.07361212 0.02768166 0.         0.80989647]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, True, False, False]
State prediction error at timestep 200 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of 0
Current timestep = 201. State = [[-0.13083784 -0.13545695]]. Action = [[ 0.03605085  0.06661449  0.         -0.75987273]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, True, False, False]
State prediction error at timestep 201 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of 0
Current timestep = 202. State = [[-0.1259708  -0.13506983]]. Action = [[ 0.07705241 -0.03527083  0.          0.4911859 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, True, False, False]
State prediction error at timestep 202 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of 0
Current timestep = 203. State = [[-0.12000813 -0.13261439]]. Action = [[ 0.0756494   0.06429299  0.         -0.3309909 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of 0
Current timestep = 204. State = [[-0.11997152 -0.12911986]]. Action = [[-0.05074667  0.02830216  0.          0.18277979]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, True, False, False]
State prediction error at timestep 204 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of 0
Current timestep = 205. State = [[-0.12111004 -0.12911831]]. Action = [[-0.00598922 -0.02617029  0.          0.55476594]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of 0
Current timestep = 206. State = [[-0.12503782 -0.12703522]]. Action = [[-0.09026308  0.04872488  0.          0.36790872]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, True, False, False]
State prediction error at timestep 206 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of 0
Current timestep = 207. State = [[-0.1298718 -0.1276957]]. Action = [[-0.0630056  -0.05044293  0.          0.4434421 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, True, False, False]
State prediction error at timestep 207 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of 0
Current timestep = 208. State = [[-0.13540158 -0.12428199]]. Action = [[-0.08786222  0.08740557  0.         -0.3751458 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of 0
Current timestep = 209. State = [[-0.13588846 -0.12573811]]. Action = [[ 0.04607246 -0.0960608   0.          0.3007102 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, True, False, False]
State prediction error at timestep 209 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of 0
Current timestep = 210. State = [[-0.13346891 -0.12722802]]. Action = [[ 0.01501248  0.01742923  0.         -0.91726977]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of 0
Current timestep = 211. State = [[-0.12842998 -0.13140932]]. Action = [[ 0.08566608 -0.09420495  0.         -0.43477035]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, True, False, False]
State prediction error at timestep 211 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of 0
Current timestep = 212. State = [[-0.12534411 -0.13298799]]. Action = [[0.00342181 0.02890069 0.         0.37519002]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, True, False, False]
State prediction error at timestep 212 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of 0
Current timestep = 213. State = [[-0.12286725 -0.13173883]]. Action = [[0.03921058 0.01787867 0.         0.48825634]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, True, False, False]
State prediction error at timestep 213 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of 0
Current timestep = 214. State = [[-0.12398178 -0.13396269]]. Action = [[-0.0527048  -0.04869282  0.         -0.7249527 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, True, False, False]
State prediction error at timestep 214 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of 0
Current timestep = 215. State = [[-0.12185185 -0.13739975]]. Action = [[ 0.06441937 -0.02975544  0.         -0.6943748 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, True, False, False]
State prediction error at timestep 215 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of 0
Current timestep = 216. State = [[-0.11656057 -0.14136502]]. Action = [[ 0.06590139 -0.04706822  0.          0.01502454]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of 0
Current timestep = 217. State = [[-0.11798004 -0.14087363]]. Action = [[-0.08852979  0.05983626  0.          0.8744216 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, True, False, False]
State prediction error at timestep 217 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of 0
Current timestep = 218. State = [[-0.12218721 -0.13690089]]. Action = [[-0.03913287  0.05720686  0.         -0.09444785]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of 0
Current timestep = 219. State = [[-0.11975079 -0.13358644]]. Action = [[ 0.08675767  0.02883121  0.         -0.48026955]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, True, False, False]
State prediction error at timestep 219 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 219 of 0
Current timestep = 220. State = [[-0.11363817 -0.13679868]]. Action = [[ 0.08658796 -0.08955172  0.         -0.70984167]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, True, False, False]
State prediction error at timestep 220 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 220 of 0
Current timestep = 221. State = [[-0.11143565 -0.1440184 ]]. Action = [[-0.00626178 -0.0908982   0.         -0.7660643 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, True, False, False]
State prediction error at timestep 221 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 221 of 0
Current timestep = 222. State = [[-0.10748603 -0.1482525 ]]. Action = [[ 0.07561062 -0.01549026  0.          0.03816164]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, True, False, False]
State prediction error at timestep 222 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 222 of 0
Current timestep = 223. State = [[-0.1046481  -0.14528352]]. Action = [[ 0.00791252  0.08774593  0.         -0.9676843 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, True, False, False]
State prediction error at timestep 223 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 223 of 0
Current timestep = 224. State = [[-0.10109759 -0.13835146]]. Action = [[ 0.06182223  0.09660377  0.         -0.95461804]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, True, False, False]
State prediction error at timestep 224 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 224 of 0
Current timestep = 225. State = [[-0.09555028 -0.13815741]]. Action = [[ 0.07535953 -0.05798236  0.         -0.77512836]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, True, False, False]
State prediction error at timestep 225 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 225 of 0
Current timestep = 226. State = [[-0.09202626 -0.13595758]]. Action = [[ 0.01877949  0.07227593  0.         -0.32610786]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, True, False, False]
State prediction error at timestep 226 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 226 of 0
Current timestep = 227. State = [[-0.08761219 -0.12873116]]. Action = [[ 0.06944764  0.09507961  0.         -0.6956111 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, True, False, False]
State prediction error at timestep 227 is tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 227 of 0
Current timestep = 228. State = [[-0.08704576 -0.128705  ]]. Action = [[-0.04062901 -0.07671484  0.         -0.80058265]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, True, False, False]
State prediction error at timestep 228 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 228 of 0
Current timestep = 229. State = [[-0.08279707 -0.13123007]]. Action = [[ 0.08921198 -0.02188941  0.         -0.250623  ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, True, False, False]
State prediction error at timestep 229 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 229 of 0
Current timestep = 230. State = [[-0.08109047 -0.13499473]]. Action = [[-0.04066164 -0.06732407  0.          0.88945127]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, True, False, False]
State prediction error at timestep 230 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 230 of 0
Current timestep = 231. State = [[-0.08207544 -0.13438801]]. Action = [[-0.03108362  0.05159634  0.          0.245386  ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, True, False, False]
State prediction error at timestep 231 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 231 of 0
Current timestep = 232. State = [[-0.08076476 -0.13678168]]. Action = [[ 0.01602878 -0.07731337  0.         -0.31574595]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, True, False, False]
State prediction error at timestep 232 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 232 of 0
Current timestep = 233. State = [[-0.08173887 -0.14337191]]. Action = [[-0.05926877 -0.08246589  0.         -0.15514064]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, True, False, False]
State prediction error at timestep 233 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 233 of 0
Current timestep = 234. State = [[-0.0857244 -0.1460569]]. Action = [[-0.08308695  0.01210167  0.          0.3184724 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 234 of 0
Current timestep = 235. State = [[-0.08465499 -0.14423026]]. Action = [[ 0.04622246  0.04554527  0.         -0.8938446 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 235 of 0
Current timestep = 236. State = [[-0.08108394 -0.1478073 ]]. Action = [[ 0.03241684 -0.088934    0.          0.93406725]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, True, False, False]
State prediction error at timestep 236 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 236 of 0
Current timestep = 237. State = [[-0.07783802 -0.14625093]]. Action = [[0.03446252 0.09507216 0.         0.09288228]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 237 of 0
Current timestep = 238. State = [[-0.07684874 -0.14363149]]. Action = [[-0.00867604  0.00559971  0.          0.57583344]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, True, False, False]
State prediction error at timestep 238 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 238 of 0
Current timestep = 239. State = [[-0.07400481 -0.14130694]]. Action = [[0.05732515 0.04184327 0.         0.15208042]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, True, False, False]
State prediction error at timestep 239 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 239 of 0
Current timestep = 240. State = [[-0.07010429 -0.13964097]]. Action = [[0.04747883 0.00443862 0.         0.37549984]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, True, False, False]
State prediction error at timestep 240 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 240 of 0
Current timestep = 241. State = [[-0.06506757 -0.1411924 ]]. Action = [[ 0.07481044 -0.03977416  0.          0.6155131 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, True, False, False]
State prediction error at timestep 241 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 241 of 0
Current timestep = 242. State = [[-0.06614944 -0.13947535]]. Action = [[-0.08113241  0.05971182  0.         -0.42258173]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, True, False, False]
State prediction error at timestep 242 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 242 of 0
Current timestep = 243. State = [[-0.06965309 -0.1418074 ]]. Action = [[-0.02881974 -0.08571174  0.         -0.0761404 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, True, False, False]
State prediction error at timestep 243 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 243 of 0
Current timestep = 244. State = [[-0.07335871 -0.14368656]]. Action = [[-0.05855826  0.00927152  0.         -0.78073585]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 244 of 0
Current timestep = 245. State = [[-0.0732555  -0.14468767]]. Action = [[ 0.0371748  -0.02356663  0.         -0.71951467]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, True, False, False]
State prediction error at timestep 245 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 245 of 0
Current timestep = 246. State = [[-0.07427964 -0.14204563]]. Action = [[-0.03945345  0.06710256  0.         -0.03457832]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 246 of 0
Current timestep = 247. State = [[-0.07760234 -0.13542423]]. Action = [[-0.04313007  0.08921301  0.         -0.13569272]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 247 of 0
Current timestep = 248. State = [[-0.07778826 -0.13221905]]. Action = [[ 0.03041273 -0.00466089  0.          0.3954345 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, True, False, False]
State prediction error at timestep 248 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 248 of 0
Current timestep = 249. State = [[-0.0772267  -0.13429134]]. Action = [[ 0.00363535 -0.05669599  0.         -0.78298855]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, True, False, False]
State prediction error at timestep 249 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 249 of 0
Current timestep = 250. State = [[-0.07972658 -0.13810451]]. Action = [[-0.05001768 -0.05058549  0.          0.09775972]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, True, False, False]
State prediction error at timestep 250 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 250 of 0
Current timestep = 251. State = [[-0.07719316 -0.1400641 ]]. Action = [[ 0.08691511 -0.00921731  0.          0.94147134]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, True, False, False]
State prediction error at timestep 251 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 251 of 0
Current timestep = 252. State = [[-0.07113819 -0.1359323 ]]. Action = [[ 0.08613393  0.08989633  0.         -0.59906983]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 252 of 0
Current timestep = 253. State = [[-0.07120623 -0.12845567]]. Action = [[-0.04714155  0.09422351  0.         -0.852186  ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, True, False, False]
State prediction error at timestep 253 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 253 of 0
Current timestep = 254. State = [[-0.07407721 -0.12475657]]. Action = [[-0.0267695   0.00325684  0.         -0.69715047]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 254 of 0
Current timestep = 255. State = [[-0.07063654 -0.12602723]]. Action = [[ 0.09942359 -0.0479731   0.         -0.8944096 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, True, False, False]
State prediction error at timestep 255 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 255 of 0
Current timestep = 256. State = [[-0.07118278 -0.12353497]]. Action = [[-0.07757679  0.07014125  0.          0.4457649 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 256 of 0
Current timestep = 257. State = [[-0.07635589 -0.12004001]]. Action = [[-0.05801818  0.01343129  0.         -0.29723752]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 257 of 0
Current timestep = 258. State = [[-0.08213042 -0.1180026 ]]. Action = [[-0.07565098  0.01110829  0.          0.45419872]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 258 of 0
Current timestep = 259. State = [[-0.08402018 -0.11908932]]. Action = [[ 0.01551829 -0.04791046  0.          0.10095966]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 259 of 0
Current timestep = 260. State = [[-0.08623023 -0.12318359]]. Action = [[-0.04421231 -0.06611542  0.          0.7760923 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 260 of 0
Current timestep = 261. State = [[-0.08888655 -0.12835261]]. Action = [[-0.02506475 -0.06491188  0.          0.7607796 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, True, False, False]
State prediction error at timestep 261 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 261 of 0
Current timestep = 262. State = [[-0.08864027 -0.13326588]]. Action = [[ 0.02512463 -0.05009394  0.          0.71425486]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, True, False, False]
State prediction error at timestep 262 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 262 of 0
Current timestep = 263. State = [[-0.08376289 -0.13196242]]. Action = [[ 0.09774619  0.07132906  0.         -0.57026935]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, True, False, False]
State prediction error at timestep 263 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 263 of 0
Current timestep = 264. State = [[-0.08264735 -0.12691337]]. Action = [[-0.01926585  0.07196607  0.          0.47283518]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, True, False, False]
State prediction error at timestep 264 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 264 of 0
Current timestep = 265. State = [[-0.08469161 -0.1210143 ]]. Action = [[-0.01824425  0.07699735  0.          0.54721105]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 265 of 0
Current timestep = 266. State = [[-0.08514413 -0.11744317]]. Action = [[0.01767687 0.01765635 0.         0.6898856 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 266 of 0
Current timestep = 267. State = [[-0.08709159 -0.11498063]]. Action = [[-0.03556161  0.02534328  0.         -0.1461016 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 267 of 0
Current timestep = 268. State = [[-0.08779845 -0.1184392 ]]. Action = [[ 0.01559273 -0.09753393  0.          0.39698374]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 268 of 0
Current timestep = 269. State = [[-0.09149281 -0.12444784]]. Action = [[-0.08127721 -0.06875269  0.          0.5499711 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 269 of 0
Current timestep = 270. State = [[-0.09037983 -0.12404864]]. Action = [[ 0.07347534  0.05332925  0.         -0.5162935 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 270 of 0
Current timestep = 271. State = [[-0.09101802 -0.11867823]]. Action = [[-0.04778962  0.07755075  0.          0.25756538]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 271 of 0
Current timestep = 272. State = [[-0.09362982 -0.11841194]]. Action = [[-0.02163158 -0.04831253  0.          0.66922057]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 272 of 0
Current timestep = 273. State = [[-0.09278554 -0.12305952]]. Action = [[ 0.03666585 -0.07066025  0.          0.9576081 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 273 of 0
Current timestep = 274. State = [[-0.08937947 -0.12616095]]. Action = [[ 0.05613769 -0.015471    0.         -0.17251146]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 274 of 0
Current timestep = 275. State = [[-0.08793756 -0.13204256]]. Action = [[ 0.00061189 -0.09650522  0.         -0.1996569 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, True, False, False]
State prediction error at timestep 275 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 275 of 0
Current timestep = 276. State = [[-0.08507209 -0.13548203]]. Action = [[0.05630387 0.00685279 0.         0.74497783]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, True, False, False]
State prediction error at timestep 276 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 276 of 0
Current timestep = 277. State = [[-0.08437326 -0.13197674]]. Action = [[-0.01836316  0.08978284  0.          0.5059415 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, True, False, False]
State prediction error at timestep 277 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 277 of 0
Current timestep = 278. State = [[-0.08892082 -0.12519988]]. Action = [[-0.08695927  0.09582157  0.         -0.57575446]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, True, False, False]
State prediction error at timestep 278 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 278 of 0
Current timestep = 279. State = [[-0.08764514 -0.12611064]]. Action = [[ 0.0917409  -0.08227694  0.         -0.8005543 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, True, False, False]
State prediction error at timestep 279 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 279 of 0
Current timestep = 280. State = [[-0.08492086 -0.13155067]]. Action = [[-0.00917187 -0.05799036  0.          0.2048527 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, True, False, False]
State prediction error at timestep 280 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 280 of 0
Current timestep = 281. State = [[-0.08720576 -0.13096735]]. Action = [[-0.06982887  0.05970298  0.         -0.53934413]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, True, False, False]
State prediction error at timestep 281 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 281 of 0
Current timestep = 282. State = [[-0.0878749  -0.12851486]]. Action = [[0.00460124 0.02058616 0.         0.64756644]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, True, False, False]
State prediction error at timestep 282 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 282 of 0
Current timestep = 283. State = [[-0.0886514  -0.12393397]]. Action = [[-0.03144042  0.07454952  0.         -0.91691434]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 283 of 0
Current timestep = 284. State = [[-0.09270283 -0.12279274]]. Action = [[-0.07773722 -0.03189105  0.         -0.70090735]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 284 of 0
Current timestep = 285. State = [[-0.09036326 -0.12695646]]. Action = [[ 0.09441585 -0.07972147  0.          0.912547  ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, True, False, False]
State prediction error at timestep 285 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 285 of 0
Current timestep = 286. State = [[-0.09077828 -0.12525366]]. Action = [[-0.08551909  0.08414764  0.          0.9130206 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, True, False, False]
State prediction error at timestep 286 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 286 of 0
Current timestep = 287. State = [[-0.096727   -0.12018276]]. Action = [[-0.07828786  0.04609808  0.          0.21679962]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 287 of 0
Current timestep = 288. State = [[-0.09656537 -0.11483863]]. Action = [[ 0.06374579  0.05900245  0.         -0.39275432]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 288 of 0
Current timestep = 289. State = [[-0.09277196 -0.11634094]]. Action = [[ 0.0599679  -0.08744714  0.          0.5950382 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 289 of 0
Current timestep = 290. State = [[-0.09499316 -0.11955786]]. Action = [[-0.07853474 -0.0244064   0.         -0.20396942]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 290 of 0
Current timestep = 291. State = [[-0.10049643 -0.1232932 ]]. Action = [[-0.06145746 -0.06242658  0.         -0.12647653]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 291 of 0
Current timestep = 292. State = [[-0.10360719 -0.1252626 ]]. Action = [[-1.6229607e-02 -3.8138777e-04  0.0000000e+00  3.8910425e-01]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, True, False, False]
State prediction error at timestep 292 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.10584433 -0.12919137]]. Action = [[-0.02242444 -0.07088265  0.         -0.7838939 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, True, False, False]
State prediction error at timestep 293 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.10280819 -0.1355734 ]]. Action = [[ 0.09171993 -0.07384977  0.          0.6212177 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, True, False, False]
State prediction error at timestep 294 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.10179602 -0.13647793]]. Action = [[-0.02069871  0.04943938  0.         -0.8976171 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, True, False, False]
State prediction error at timestep 295 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 295 of -1
Current timestep = 296. State = [[-0.09924026 -0.14064984]]. Action = [[ 0.08221353 -0.09118757  0.          0.84464145]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, True, False, False]
State prediction error at timestep 296 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 296 of -1
Current timestep = 297. State = [[-0.09822296 -0.14036933]]. Action = [[-0.0166919   0.08422253  0.          0.6590202 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, True, False, False]
State prediction error at timestep 297 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[-0.0971371  -0.13525547]]. Action = [[0.04818887 0.07336939 0.         0.45038855]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, True, False, False]
State prediction error at timestep 298 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.0965381 -0.1370999]]. Action = [[ 0.00454736 -0.0746327   0.         -0.8881113 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, True, False, False]
State prediction error at timestep 299 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.10111745 -0.13647953]]. Action = [[-0.09221832  0.06508686  0.         -0.65372574]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, True, False, False]
State prediction error at timestep 300 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.1010031  -0.13739483]]. Action = [[ 0.0771258  -0.05466747  0.          0.5387579 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, True, False, False]
State prediction error at timestep 301 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.10291915 -0.13747856]]. Action = [[-0.08716974  0.03711162  0.         -0.22939259]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, True, False, False]
State prediction error at timestep 302 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 302 of -1
Current timestep = 303. State = [[-0.10421127 -0.13467143]]. Action = [[0.02619288 0.03688329 0.         0.6004138 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, True, False, False]
State prediction error at timestep 303 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 303 of -1
Current timestep = 304. State = [[-0.10543448 -0.13501601]]. Action = [[-0.03012381 -0.03446318  0.         -0.64572585]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, True, False, False]
State prediction error at timestep 304 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.10791668 -0.13097239]]. Action = [[-0.02872168  0.09560918  0.          0.42230892]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, True, False, False]
State prediction error at timestep 305 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.10716953 -0.12894687]]. Action = [[ 0.04287342 -0.02670283  0.          0.7115395 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, True, False, False]
State prediction error at timestep 306 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.10180059 -0.12649553]]. Action = [[ 0.09872288  0.0473027   0.         -0.17815149]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, True, False, False]
State prediction error at timestep 307 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.10125274 -0.1284098 ]]. Action = [[-0.03679352 -0.07952744  0.         -0.64115137]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, True, False, False]
State prediction error at timestep 308 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.10579909 -0.13152617]]. Action = [[-0.0775418  -0.02200355  0.          0.9879401 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, True, False, False]
State prediction error at timestep 309 is tensor(1.4820e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.10494948 -0.12843266]]. Action = [[0.06090038 0.07526375 0.         0.2861904 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, True, False, False]
State prediction error at timestep 310 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.10428479 -0.12584679]]. Action = [[-0.01663869  0.00090209  0.          0.85079324]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, True, False, False]
State prediction error at timestep 311 is tensor(5.8082e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.10198665 -0.12690805]]. Action = [[ 0.05512422 -0.03140478  0.         -0.98746216]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, True, False, False]
State prediction error at timestep 312 is tensor(1.0141e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.10088234 -0.12542072]]. Action = [[-0.00829393  0.04405165  0.         -0.95840377]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, True, False, False]
State prediction error at timestep 313 is tensor(1.8837e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.09677391 -0.12066483]]. Action = [[0.0900548  0.06347097 0.         0.8157444 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(3.5471e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.09032474 -0.11628383]]. Action = [[ 0.08793511  0.03628791  0.         -0.8233094 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.08776739 -0.10990455]]. Action = [[0.0032166  0.0893096  0.         0.80862296]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.09112047 -0.10469936]]. Action = [[-0.07543765  0.03048068  0.          0.86770463]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 317 of -1
Current timestep = 318. State = [[-0.08995657 -0.09770176]]. Action = [[ 0.06382097  0.09167633  0.         -0.47393626]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.08668268 -0.09805142]]. Action = [[ 0.02956993 -0.09605557  0.         -0.5352645 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 319 of -1
Current timestep = 320. State = [[-0.08556664 -0.10306722]]. Action = [[-0.00977367 -0.07175825  0.         -0.5611875 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.08219542 -0.10347188]]. Action = [[ 0.05486768  0.02380969  0.         -0.3177001 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 321 of -1
Current timestep = 322. State = [[-0.08274975 -0.09808617]]. Action = [[-0.06317098  0.08681116  0.          0.4872694 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 322 of -1
Current timestep = 323. State = [[-0.08554858 -0.09002298]]. Action = [[-0.03805266  0.09345096  0.         -0.2314341 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.08218423 -0.08820824]]. Action = [[ 0.09204907 -0.04717952  0.          0.17309439]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.07974584 -0.08374219]]. Action = [[-0.01527067  0.09353986  0.          0.10888577]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.08165761 -0.08352713]]. Action = [[-0.0462824  -0.07234381  0.         -0.71278286]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.08449676 -0.08136494]]. Action = [[-0.04908245  0.06356343  0.          0.7566643 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.0845713  -0.07963533]]. Action = [[ 0.01519515 -0.02140953  0.         -0.8324928 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.0804406  -0.07947811]]. Action = [[ 0.07109507 -0.00346141  0.          0.29341996]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.0821964  -0.07804206]]. Action = [[-0.0901375   0.02038572  0.         -0.93761396]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 330 of -1
Current timestep = 331. State = [[-0.08670653 -0.07632706]]. Action = [[-0.04541325  0.0087844   0.          0.6937407 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.08713325 -0.07054389]]. Action = [[0.0263418  0.09548014 0.         0.00440156]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.08684282 -0.06449544]]. Action = [[ 0.00798508  0.04493675  0.         -0.9616678 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.08922016 -0.05994169]]. Action = [[-0.03785488  0.0384162   0.         -0.5926619 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.0938832  -0.05613642]]. Action = [[-0.05998189  0.02643762  0.         -0.68356526]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.09471115 -0.05464625]]. Action = [[ 0.03403551 -0.01390548  0.          0.03889728]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.09894796 -0.04939714]]. Action = [[-0.09252881  0.08783043  0.         -0.36202377]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 337 of 0
Current timestep = 338. State = [[-0.1028017  -0.04129838]]. Action = [[-0.00327594  0.08091206  0.         -0.5856451 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 338 of 0
Current timestep = 339. State = [[-0.10836363 -0.03813142]]. Action = [[-0.08229248 -0.02138986  0.         -0.8705634 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 339 of 0
Current timestep = 340. State = [[-0.11578418 -0.03541643]]. Action = [[-0.08260331  0.03082568  0.         -0.5739577 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 340 of 0
Current timestep = 341. State = [[-0.1155711  -0.03778043]]. Action = [[ 0.08987219 -0.09548692  0.         -0.6952146 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 341 of 0
Current timestep = 342. State = [[-0.1182473  -0.03651159]]. Action = [[-0.09714358  0.06431548  0.         -0.30195326]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 342 of 0
Current timestep = 343. State = [[-0.11797328 -0.03411518]]. Action = [[ 0.09352625 -0.00337885  0.         -0.49712956]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 343 of 0
Current timestep = 344. State = [[-0.11724686 -0.0308653 ]]. Action = [[-0.01425064  0.0546199   0.         -0.7733281 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 344 of 0
Current timestep = 345. State = [[-0.11468372 -0.02471768]]. Action = [[ 0.08037355  0.0800376   0.         -0.6512724 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 345 of 0
Current timestep = 346. State = [[-0.11190128 -0.02248806]]. Action = [[ 0.03478623 -0.01619742  0.         -0.97737205]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 346 of 0
Current timestep = 347. State = [[-0.10741259 -0.02376758]]. Action = [[ 0.08382791 -0.0277713   0.         -0.8399297 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 347 of 0
Current timestep = 348. State = [[-0.10524499 -0.02021252]]. Action = [[0.00324587 0.08368414 0.         0.06080031]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 348 of 0
Current timestep = 349. State = [[-0.10270622 -0.0224037 ]]. Action = [[ 0.04949652 -0.09860247  0.         -0.15161175]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 349 of 0
Current timestep = 350. State = [[-0.10208615 -0.02751354]]. Action = [[-0.02744064 -0.04817209  0.          0.59090567]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 350 of 0
Current timestep = 351. State = [[-0.09784178 -0.02471919]]. Action = [[ 0.08753134  0.09161996  0.         -0.5034573 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 351 of 0
Current timestep = 352. State = [[-0.09229531 -0.02040986]]. Action = [[ 0.05459238  0.03675003  0.         -0.01167792]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 352 of 0
Current timestep = 353. State = [[-0.09404065 -0.01475287]]. Action = [[-0.08927574  0.08570971  0.          0.19712353]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 353 of 0
Current timestep = 354. State = [[-0.09719111 -0.01341438]]. Action = [[-0.0291635 -0.0394318  0.        -0.2991262]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 354 of 0
Current timestep = 355. State = [[-0.09340479 -0.00958201]]. Action = [[0.09247918 0.08055403 0.         0.580199  ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 355 of 0
Current timestep = 356. State = [[-0.09303803 -0.00560811]]. Action = [[-0.05315918  0.01450314  0.          0.37488866]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 356 of 0
Current timestep = 357. State = [[-0.09255391 -0.00227118]]. Action = [[ 0.03221192  0.03270226  0.         -0.90469515]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 357 of 0
Current timestep = 358. State = [[-0.08807167  0.00238445]]. Action = [[ 0.07569747  0.05044515  0.         -0.85859036]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 358 of 0
Current timestep = 359. State = [[-0.08312485  0.0038127 ]]. Action = [[ 0.05855233 -0.02287934  0.          0.8169875 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 359 of 0
Current timestep = 360. State = [[-0.07993479  0.0073017 ]]. Action = [[0.02403643 0.0640995  0.         0.4812013 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 360 of 0
Current timestep = 361. State = [[-0.07478117  0.01123572]]. Action = [[0.08389653 0.02431681 0.         0.8958384 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 361 of 0
Current timestep = 362. State = [[-0.07047047  0.01422037]]. Action = [[ 0.03072757  0.02790696  0.         -0.3630216 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 362 of 0
Current timestep = 363. State = [[-0.07305042  0.02094132]]. Action = [[-0.0923452   0.09714253  0.          0.8945656 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 363 of 0
Current timestep = 364. State = [[-0.0705791   0.02171631]]. Action = [[ 0.0950991  -0.06995855  0.         -0.6456549 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 364 of 0
Current timestep = 365. State = [[-0.06431834  0.01941832]]. Action = [[ 0.05088665 -0.02817761  0.          0.5287596 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 365 of 0
Current timestep = 366. State = [[-0.0629256   0.01905166]]. Action = [[-0.03783808 -0.00314231  0.         -0.6790755 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 366 of 0
Current timestep = 367. State = [[-0.05888444  0.02253542]]. Action = [[ 0.07052273  0.05991372  0.         -0.03852737]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 367 of 0
Current timestep = 368. State = [[-0.05857461  0.02748189]]. Action = [[-0.06618679  0.04972105  0.         -0.47713917]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.05948704  0.03305175]]. Action = [[-0.00896133  0.05777056  0.         -0.47704387]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 369 of -1
Current timestep = 370. State = [[-0.05463534  0.04005197]]. Action = [[ 0.09623645  0.07939825  0.         -0.3859064 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.05158099  0.04682578]]. Action = [[0.00125013 0.06093419 0.         0.31322896]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.12793225  0.00672219]]. Action = [[ 0.07546539 -0.07667518  0.          0.6315203 ]]. Reward = [100.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.12969455  0.00127163]]. Action = [[-0.0506098  -0.05186618  0.         -0.8257159 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.12717363 -0.00118913]]. Action = [[ 0.08140021 -0.00947107  0.          0.5826688 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.12660278 -0.00535962]]. Action = [[-0.04184681 -0.06359867  0.         -0.7964545 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.13017909 -0.00545537]]. Action = [[-0.06248063  0.049505    0.          0.51453257]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 376 of -1
Current timestep = 377. State = [[-0.12988785 -0.00054683]]. Action = [[ 0.04325681  0.08027449  0.         -0.02356339]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.12811974  0.0046668 ]]. Action = [[0.02216573 0.05790254 0.         0.21177387]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.13090995  0.00971544]]. Action = [[-0.06153464  0.05808141  0.          0.6326159 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.13738903  0.01139522]]. Action = [[-0.09135317 -0.0184785   0.          0.03736043]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.14235936  0.00991572]]. Action = [[-0.04305047 -0.04045418  0.         -0.7081939 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.14390898  0.00457622]]. Action = [[ 0.00124244 -0.09718364  0.          0.532199  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.14534219  0.00654117]]. Action = [[-0.01948746  0.09410364  0.         -0.8481204 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.14999524  0.00949282]]. Action = [[-0.06893539 -0.00380564  0.          0.6807599 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.15301293  0.00658426]]. Action = [[-0.00811383 -0.0698549   0.         -0.25519347]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.15651472  0.00070891]]. Action = [[-0.05385514 -0.07925101  0.         -0.4263888 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.16201854 -0.00185183]]. Action = [[-0.07166237  0.00138678  0.         -0.7270489 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.16597636 -0.00032176]]. Action = [[-0.02174469  0.03807961  0.          0.15428233]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.16477478 -0.0027273 ]]. Action = [[ 0.06544612 -0.0632914   0.          0.6564629 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.16541225 -0.00421905]]. Action = [[-0.02491847  0.01918168  0.          0.06828141]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.16421464 -0.00580065]]. Action = [[ 0.06097048 -0.02739668  0.          0.69348085]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.16681184 -0.00377107]]. Action = [[-0.06707968  0.07089522  0.         -0.60321933]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.1665537  -0.00019777]]. Action = [[ 0.0762077   0.03867946  0.         -0.29023743]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.16531028  0.00310606]]. Action = [[0.01362455 0.04531548 0.         0.40959275]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.16497312  0.00411773]]. Action = [[ 0.0212145  -0.00801583  0.          0.23958421]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.16068546  0.00109494]]. Action = [[ 0.0920623  -0.05701687  0.         -0.53180647]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.15910962  0.00130536]]. Action = [[-0.01379631  0.04273469  0.         -0.12330449]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.16403745  0.00076988]]. Action = [[-0.09768673 -0.03349303  0.          0.08796251]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.16711444 -0.00087422]]. Action = [[-0.01326877 -0.01673859  0.         -0.03891587]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.16835706 -0.00598998]]. Action = [[-0.01979972 -0.09145059  0.         -0.06359506]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.16632779 -0.01168694]]. Action = [[ 0.04558886 -0.05154781  0.         -0.9744072 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.16836183 -0.01373824]]. Action = [[-0.0784393   0.00654157  0.          0.8204322 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.16821884 -0.01871209]]. Action = [[ 0.04293097 -0.08639655  0.         -0.5624305 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.16361901 -0.02250602]]. Action = [[ 0.06792832 -0.00207883  0.         -0.33816588]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.16163766 -0.02147178]]. Action = [[-0.00083672  0.04990054  0.         -0.42889822]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.16005   -0.0234367]]. Action = [[ 0.03013111 -0.04743009  0.         -0.5314346 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.16164172 -0.02313436]]. Action = [[-0.05384312  0.05221166  0.         -0.3197775 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.15929243 -0.01724783]]. Action = [[0.08548588 0.09946246 0.         0.69158924]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.15920478 -0.01100793]]. Action = [[-0.03972239  0.06507448  0.         -0.4116547 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.15963568 -0.01064533]]. Action = [[ 0.01709247 -0.04361762  0.         -0.40817237]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 410 of -1
Current timestep = 411. State = [[-0.15948407 -0.01224564]]. Action = [[-0.00462505 -0.01848871  0.          0.77722025]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.15718868 -0.01708351]]. Action = [[ 0.04606172 -0.09158158  0.          0.49090683]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.15245958 -0.02307949]]. Action = [[ 0.06162179 -0.06295578  0.         -0.5286968 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.14825292 -0.02953176]]. Action = [[ 0.03411596 -0.07636488  0.          0.8769423 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.14467755 -0.02918695]]. Action = [[ 0.03390608  0.07438385  0.         -0.87951803]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.14467253 -0.02678092]]. Action = [[-0.03543679  0.02362208  0.         -0.32040972]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.14165723 -0.02812254]]. Action = [[ 0.06660383 -0.03136081  0.          0.6414654 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.13929266 -0.03415768]]. Action = [[-0.00803475 -0.08927908  0.          0.2538556 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.13921043 -0.03476347]]. Action = [[-0.01867972  0.05873252  0.          0.35802698]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.13475902 -0.03710153]]. Action = [[ 0.08593867 -0.06493866  0.          0.03996694]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.12716836 -0.03620232]]. Action = [[0.09002478 0.07080837 0.         0.50330865]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 421 of -1
Current timestep = 422. State = [[-0.12296534 -0.03958594]]. Action = [[ 0.01343031 -0.09434732  0.         -0.17333317]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.11649392 -0.0455954 ]]. Action = [[ 0.09611068 -0.04875504  0.          0.6267799 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 423 of -1
Current timestep = 424. State = [[-0.11619676 -0.04985425]]. Action = [[-0.09124951 -0.02944313  0.         -0.5597533 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.11405621 -0.04949923]]. Action = [[ 0.07621492  0.04557037  0.         -0.64014256]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 425 of -1
Current timestep = 426. State = [[-0.10717416 -0.04444459]]. Action = [[0.087658   0.09048826 0.         0.8024552 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.10659349 -0.03856659]]. Action = [[-0.05478705  0.06879658  0.          0.5137007 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.10829719 -0.03320167]]. Action = [[-0.01352777  0.05620778  0.          0.37994158]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.10421168 -0.02905082]]. Action = [[0.09417453 0.03037488 0.         0.8664081 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 429 of -1
Current timestep = 430. State = [[-0.10147594 -0.02826194]]. Action = [[-0.0011417  -0.02360547  0.          0.763788  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 430 of -1
Current timestep = 431. State = [[-0.10287008 -0.02910084]]. Action = [[-0.04431446 -0.02352998  0.         -0.8386123 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 431 of -1
Current timestep = 432. State = [[-0.0988827  -0.02440928]]. Action = [[0.09841094 0.08957411 0.         0.33502555]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.09919913 -0.02541859]]. Action = [[-0.08709152 -0.09532445  0.         -0.22965652]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 433 of -1
Current timestep = 434. State = [[-0.09798641 -0.0244991 ]]. Action = [[ 0.05981504  0.05642316  0.         -0.23625791]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.0918046 -0.0248005]]. Action = [[ 0.08520598 -0.04906294  0.          0.57625854]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.0922885  -0.02320417]]. Action = [[-0.08551982  0.0525604   0.          0.41241968]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 436 of -1
Current timestep = 437. State = [[-0.0946454  -0.02007584]]. Action = [[-0.01045197  0.0217697   0.          0.89496696]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 437 of -1
Current timestep = 438. State = [[-0.09409085 -0.01559507]]. Action = [[ 0.02004753  0.06159689  0.         -0.8816984 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.09651607 -0.0100104 ]]. Action = [[-0.05702586  0.05680832  0.         -0.701414  ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.09559665 -0.01215107]]. Action = [[ 0.05501669 -0.0990519   0.          0.93154407]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.09158935 -0.01045631]]. Action = [[ 0.05438337  0.08002748  0.         -0.77442   ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 441 of -1
Current timestep = 442. State = [[-0.09285615 -0.01085022]]. Action = [[-0.06179701 -0.0640658   0.         -0.5279035 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.09467973 -0.01107554]]. Action = [[-0.01010104  0.02076874  0.          0.5735743 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.09212455 -0.01249119]]. Action = [[ 0.05744696 -0.04489617  0.         -0.64167035]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.09282983 -0.01576469]]. Action = [[-0.05423668 -0.03936509  0.         -0.59422797]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.09809684 -0.01446137]]. Action = [[-0.08833554  0.05181815  0.         -0.0377056 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.0992364 -0.0162507]]. Action = [[ 0.02629479 -0.06686009  0.         -0.41064537]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.09532385 -0.02179981]]. Action = [[ 0.06579945 -0.06670879  0.          0.21920562]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.09746616 -0.02426145]]. Action = [[-0.09431242  0.00634123  0.         -0.47461486]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.09890375 -0.02425654]]. Action = [[0.02774663 0.0104622  0.         0.8543016 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 450 of -1
Current timestep = 451. State = [[-0.09662429 -0.02719776]]. Action = [[ 0.04329575 -0.05108849  0.         -0.23590893]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.09786467 -0.03332349]]. Action = [[-0.04560071 -0.07435508  0.         -0.27665532]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.09911944 -0.04108403]]. Action = [[-0.00089556 -0.08685185  0.          0.5951338 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(9.1017e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[-0.09691223 -0.04720869]]. Action = [[ 0.0466748  -0.03807848  0.          0.83648825]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(7.6570e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[-0.09807689 -0.04684259]]. Action = [[-0.04972598  0.06675058  0.          0.69413733]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(7.6974e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.09745473 -0.0478414 ]]. Action = [[ 0.04769624 -0.03083576  0.          0.17865396]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.09533081 -0.04872412]]. Action = [[ 0.02694502  0.02354591  0.         -0.98555803]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(4.5374e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.09482467 -0.04570198]]. Action = [[0.00404407 0.06699084 0.         0.83273745]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.09885439 -0.0421428 ]]. Action = [[-0.07667148  0.03994597  0.         -0.1542843 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.10068389 -0.04466607]]. Action = [[ 0.01338999 -0.07590051  0.          0.88602567]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(5.1942e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.09926694 -0.04787238]]. Action = [[ 0.02941964 -0.01833332  0.         -0.9394937 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(4.0899e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.09965217 -0.04996077]]. Action = [[-0.02134238 -0.02396525  0.          0.6412699 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(3.5347e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.10327438 -0.05399681]]. Action = [[-0.0647615  -0.06007319  0.          0.7637032 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(9.1015e-07, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.10079584 -0.05621175]]. Action = [[0.09315688 0.00067534 0.         0.64676094]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(3.0444e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.09981306 -0.05454993]]. Action = [[-0.03561719  0.04707397  0.          0.8929583 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(3.6622e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.09628054 -0.04997507]]. Action = [[0.09560909 0.06927658 0.         0.5949769 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.09649839 -0.04422823]]. Action = [[-0.05836852  0.07218235  0.          0.48830128]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[-0.10128534 -0.04425011]]. Action = [[-0.06392579 -0.0555535   0.         -0.3987165 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(6.2130e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 468 of -1
Current timestep = 469. State = [[-0.10337657 -0.04616885]]. Action = [[-0.00418223 -0.01860629  0.          0.91341233]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(1.8301e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.10569977 -0.05046091]]. Action = [[-0.04345031 -0.0802653   0.         -0.8353396 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.10992285 -0.05181127]]. Action = [[-0.06446664  0.01902547  0.         -0.43539274]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(6.8048e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[-0.11594395 -0.05425067]]. Action = [[-0.08688636 -0.05877845  0.         -0.24954134]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.11843262 -0.06027358]]. Action = [[ 0.00172827 -0.08261387  0.          0.859784  ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(1.4862e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.11716313 -0.06562658]]. Action = [[ 0.03354401 -0.04278919  0.         -0.46801484]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(1.1876e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.11675186 -0.06747398]]. Action = [[-0.00127281  0.01083694  0.         -0.8052308 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(7.1568e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.11371712 -0.07281298]]. Action = [[ 0.07483096 -0.08910441  0.         -0.4184457 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(7.3836e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.11507858 -0.08081222]]. Action = [[-0.06658778 -0.07752298  0.         -0.44994032]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(4.6536e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.11700702 -0.08532628]]. Action = [[ 0.00550834 -0.00845991  0.          0.43156266]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(4.9570e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.11625726 -0.08843309]]. Action = [[ 0.02696051 -0.01987193  0.         -0.36455882]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(7.3953e-07, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.12069205 -0.09455186]]. Action = [[-0.09787998 -0.07511519  0.          0.39338613]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[-0.12023872 -0.10099382]]. Action = [[ 0.08239425 -0.04981592  0.         -0.92221725]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.11951367 -0.10796797]]. Action = [[-0.02400802 -0.06743702  0.         -0.8866344 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.12432409 -0.11016797]]. Action = [[-0.08919632  0.039308    0.          0.7464856 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.12521824 -0.11362871]]. Action = [[ 0.0422209  -0.05619612  0.          0.8696958 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[-0.12758951 -0.11343433]]. Action = [[-0.07093082  0.06950518  0.          0.06098866]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(1.4793e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.12749058 -0.11504004]]. Action = [[ 0.05526309 -0.05080569  0.          0.45942068]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.12447768 -0.11936388]]. Action = [[ 0.03921712 -0.03931283  0.          0.64386785]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[-0.12428327 -0.12313684]]. Action = [[-0.01653792 -0.03057966  0.         -0.22207022]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(1.6576e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.12189414 -0.1215973 ]]. Action = [[ 0.06062657  0.06854618  0.         -0.6429851 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[-0.11705505 -0.11863942]]. Action = [[0.07341712 0.02839976 0.         0.93380666]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.11918529 -0.11364799]]. Action = [[-0.09017419  0.08659647  0.         -0.6538011 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 491 of -1
Current timestep = 492. State = [[-0.12684375 -0.11408705]]. Action = [[-0.09913225 -0.06620808  0.         -0.49897695]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[-0.13409567 -0.11404411]]. Action = [[-0.08120551  0.03035619  0.          0.7121413 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.13794276 -0.11044064]]. Action = [[-0.01556107  0.0452496   0.          0.5246458 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(3.0932e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[-0.14230864 -0.1126362 ]]. Action = [[-0.0606809  -0.08600754  0.         -0.61840415]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 495 of -1
Current timestep = 496. State = [[-0.14343062 -0.11648429]]. Action = [[ 0.02912375 -0.03586207  0.          0.7131703 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.14099345 -0.11411983]]. Action = [[0.05655701 0.06632838 0.         0.33378172]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(4.3301e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.14091069 -0.11632319]]. Action = [[-0.01006083 -0.0876159   0.         -0.7223041 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.13870832 -0.12151216]]. Action = [[ 0.06546686 -0.05141432  0.          0.03231883]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(2.4218e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.13367899 -0.1240944 ]]. Action = [[ 0.0808434  -0.00891887  0.          0.45791733]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.13438076 -0.12145463]]. Action = [[-0.05892683  0.07624235  0.          0.28706038]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(4.6485e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.1354035  -0.12293407]]. Action = [[ 0.0270747  -0.06538311  0.         -0.7790029 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.13584048 -0.12925209]]. Action = [[-0.01212823 -0.07889261  0.          0.44488823]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 503 is [True, False, False, True, False, False]
State prediction error at timestep 503 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.13561787 -0.13753723]]. Action = [[ 0.01500976 -0.09821404  0.         -0.63258505]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 504 is [True, False, False, True, False, False]
State prediction error at timestep 504 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.13254945 -0.14062053]]. Action = [[0.05519872 0.02505774 0.         0.78724265]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 505 is [True, False, False, True, False, False]
State prediction error at timestep 505 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 505 of -1
Current timestep = 506. State = [[-0.13200884 -0.14052317]]. Action = [[-0.0207531   0.01737496  0.         -0.10957116]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 506 is [True, False, False, True, False, False]
State prediction error at timestep 506 is tensor(1.1783e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.12943543 -0.13708304]]. Action = [[ 0.06346668  0.08011562  0.         -0.15001065]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 507 is [True, False, False, True, False, False]
State prediction error at timestep 507 is tensor(1.9112e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.13155542 -0.13510635]]. Action = [[-0.07937563  0.0049178   0.          0.86671424]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 508 is [True, False, False, True, False, False]
State prediction error at timestep 508 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.12976241 -0.13197929]]. Action = [[ 0.08372328  0.06189267  0.         -0.96618   ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 509 is [True, False, False, True, False, False]
State prediction error at timestep 509 is tensor(7.7007e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.1241857 -0.1308482]]. Action = [[ 0.07074255 -0.01858773  0.         -0.1970939 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 510 is [True, False, False, True, False, False]
State prediction error at timestep 510 is tensor(3.4024e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.12454392 -0.12750891]]. Action = [[-0.0589541   0.07214556  0.         -0.4304905 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 511 is [True, False, False, True, False, False]
State prediction error at timestep 511 is tensor(3.8934e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.1245346  -0.12122307]]. Action = [[ 0.0343802   0.06978817  0.         -0.95410264]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[-0.12126989 -0.12085982]]. Action = [[ 0.0545147  -0.05922238  0.          0.54905295]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.12054095 -0.12473939]]. Action = [[-0.01867334 -0.06369976  0.          0.8974807 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.119132   -0.12452466]]. Action = [[ 0.03032089  0.03376954  0.         -0.9117221 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(9.0915e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[-0.11896762 -0.11855682]]. Action = [[-0.01985484  0.09188149  0.         -0.6358045 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(2.5176e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[-0.12221605 -0.11974851]]. Action = [[-0.06607126 -0.09387702  0.          0.2841674 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.1228314  -0.12122402]]. Action = [[0.01288897 0.01587288 0.         0.16477942]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(5.5101e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.11774572 -0.12335078]]. Action = [[ 0.09205491 -0.05524951  0.         -0.7592317 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 519 of -1
Current timestep = 520. State = [[-0.11656399 -0.12575938]]. Action = [[-0.04206735 -0.01471969  0.         -0.5603547 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 520 is [True, False, False, True, False, False]
State prediction error at timestep 520 is tensor(9.7076e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 520 of -1
Current timestep = 521. State = [[-0.11786575 -0.13066347]]. Action = [[-0.01582711 -0.08167626  0.          0.19352639]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 521 is [True, False, False, True, False, False]
State prediction error at timestep 521 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 521 of -1
Current timestep = 522. State = [[-0.12155125 -0.12899339]]. Action = [[-0.07802807  0.09540404  0.          0.9174831 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 522 is [True, False, False, True, False, False]
State prediction error at timestep 522 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 522 of -1
Current timestep = 523. State = [[-0.12003629 -0.12795652]]. Action = [[ 0.08400423 -0.029631    0.          0.78153396]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 523 is [True, False, False, True, False, False]
State prediction error at timestep 523 is tensor(9.4526e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.11534266 -0.1311059 ]]. Action = [[ 0.0411856  -0.04078398  0.          0.37666512]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 524 is [True, False, False, True, False, False]
State prediction error at timestep 524 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.10965247 -0.13311693]]. Action = [[ 0.07845289 -0.00400215  0.         -0.09403408]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 525 is [True, False, False, True, False, False]
State prediction error at timestep 525 is tensor(6.1306e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[-0.10832288 -0.1331098 ]]. Action = [[-0.03314505  0.01670627  0.         -0.9622413 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 526 is [True, False, False, True, False, False]
State prediction error at timestep 526 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 526 of -1
Current timestep = 527. State = [[-0.10475069 -0.12831861]]. Action = [[0.07979334 0.0940974  0.         0.13063848]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 527 is [True, False, False, True, False, False]
State prediction error at timestep 527 is tensor(2.8951e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 527 of -1
Current timestep = 528. State = [[-0.10043921 -0.12681477]]. Action = [[ 0.0383709  -0.02754501  0.         -0.7051345 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 528 is [True, False, False, True, False, False]
State prediction error at timestep 528 is tensor(5.5572e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.10206061 -0.12525314]]. Action = [[-0.0719396   0.0438638   0.          0.55104685]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 529 is [True, False, False, True, False, False]
State prediction error at timestep 529 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.10135814 -0.12724641]]. Action = [[ 0.04768548 -0.07136083  0.          0.7175764 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 530 is [True, False, False, True, False, False]
State prediction error at timestep 530 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.096614   -0.13021754]]. Action = [[ 0.0582793  -0.02055372  0.          0.86436594]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 531 is [True, False, False, True, False, False]
State prediction error at timestep 531 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 531 of -1
Current timestep = 532. State = [[-0.09169376 -0.12848824]]. Action = [[0.05178691 0.05037608 0.         0.07790649]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 532 is [True, False, False, True, False, False]
State prediction error at timestep 532 is tensor(2.2335e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 532 of -1
Current timestep = 533. State = [[-0.09367872 -0.12342813]]. Action = [[-0.09357212  0.07310716  0.         -0.24218488]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(9.1827e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.09396913 -0.12091453]]. Action = [[ 0.03798313 -0.00439604  0.         -0.18329924]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(5.2542e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 534 of -1
Current timestep = 535. State = [[-0.08808353 -0.1204489 ]]. Action = [[ 0.09292436 -0.00240824  0.          0.812942  ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 535 of -1
Current timestep = 536. State = [[-0.08622337 -0.12031381]]. Action = [[-0.02782486 -0.00570937  0.          0.8081819 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.08371849 -0.12468877]]. Action = [[ 0.05127458 -0.09095811  0.         -0.7129183 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(8.2952e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 537 of -1
Current timestep = 538. State = [[-0.08449518 -0.12498963]]. Action = [[-0.06746514  0.0506785   0.          0.34372354]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 538 of -1
Current timestep = 539. State = [[-0.08366837 -0.1193229 ]]. Action = [[ 0.04398061  0.08390527  0.         -0.96708584]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(7.0381e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 539 of -1
Current timestep = 540. State = [[-0.08694842 -0.11673962]]. Action = [[-0.09862798 -0.00854247  0.         -0.6191788 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(4.7578e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 540 of -1
Current timestep = 541. State = [[-0.09313543 -0.11678695]]. Action = [[-0.08321871 -0.00864559  0.         -0.48760355]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(6.7865e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[-0.09453686 -0.11956863]]. Action = [[ 0.01676514 -0.05988716  0.         -0.9405987 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 542 of -1
Current timestep = 543. State = [[-0.09537598 -0.12565133]]. Action = [[-0.02925858 -0.08817284  0.          0.69815946]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 543 is [True, False, False, True, False, False]
State prediction error at timestep 543 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.09890125 -0.12391615]]. Action = [[-0.06037952  0.0952018   0.         -0.7595105 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 544 of -1
Current timestep = 545. State = [[-0.09849133 -0.1256138 ]]. Action = [[ 0.05335159 -0.08661894  0.          0.76077867]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 545 is [True, False, False, True, False, False]
State prediction error at timestep 545 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 545 of -1
Current timestep = 546. State = [[-0.09314767 -0.12391976]]. Action = [[ 0.08618861  0.08806736  0.         -0.31166804]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(9.9625e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 546 of -1
Current timestep = 547. State = [[-0.08791422 -0.11810394]]. Action = [[0.06629486 0.06637686 0.         0.1473372 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(1.3342e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.08215137 -0.11303882]]. Action = [[0.08956248 0.0520181  0.         0.49110758]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(1.5795e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 548 of -1
Current timestep = 549. State = [[-0.07803067 -0.11540212]]. Action = [[ 0.03969497 -0.08986258  0.          0.0233705 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.07629484 -0.12014452]]. Action = [[ 0.00697732 -0.04570999  0.         -0.0715766 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(4.6751e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 550 of -1
Current timestep = 551. State = [[-0.07517117 -0.12467809]]. Action = [[ 0.00643049 -0.05509278  0.         -0.01792133]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(4.8347e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 551 of -1
Current timestep = 552. State = [[-0.0701233  -0.12867889]]. Action = [[ 0.08448967 -0.03383724  0.         -0.8900738 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 552 is [True, False, False, True, False, False]
State prediction error at timestep 552 is tensor(7.3460e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 552 of -1
Current timestep = 553. State = [[-0.06795146 -0.1326166 ]]. Action = [[-0.02255782 -0.03939299  0.         -0.09628999]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 553 is [True, False, False, True, False, False]
State prediction error at timestep 553 is tensor(1.4220e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 553 of -1
Current timestep = 554. State = [[-0.06923744 -0.1383451 ]]. Action = [[-0.041046   -0.06762582  0.         -0.16228086]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 554 is [True, False, False, True, False, False]
State prediction error at timestep 554 is tensor(1.6662e-07, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 554 of -1
Current timestep = 555. State = [[-0.06941752 -0.14156272]]. Action = [[-0.00638837  0.00377993  0.         -0.8790822 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 555 is [True, False, False, True, False, False]
State prediction error at timestep 555 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 555 of -1
Current timestep = 556. State = [[-0.06485506 -0.14293809]]. Action = [[ 0.07816272 -0.00200304  0.         -0.44943655]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 556 is [True, False, False, True, False, False]
State prediction error at timestep 556 is tensor(1.5115e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 556 of -1
Current timestep = 557. State = [[-0.06260571 -0.13938665]]. Action = [[-0.01667698  0.09389941  0.          0.16072428]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 557 is [True, False, False, True, False, False]
State prediction error at timestep 557 is tensor(3.8749e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 557 of -1
Current timestep = 558. State = [[-0.05856969 -0.1408915 ]]. Action = [[ 0.08124707 -0.07528419  0.         -0.21420455]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 558 is [True, False, False, True, False, False]
State prediction error at timestep 558 is tensor(4.6197e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.05207987 -0.13870454]]. Action = [[0.07276916 0.096365   0.         0.7829987 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 559 is [True, False, False, True, False, False]
State prediction error at timestep 559 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 559 of -1
Current timestep = 560. State = [[-0.05267942 -0.13587847]]. Action = [[-0.07592555  0.00688603  0.          0.6653825 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 560 is [True, False, False, True, False, False]
State prediction error at timestep 560 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 560 of -1
Current timestep = 561. State = [[-0.05426936 -0.13932672]]. Action = [[-0.01259683 -0.07609791  0.         -0.92841405]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 561 is [True, False, False, True, False, False]
State prediction error at timestep 561 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.05749342 -0.1421789 ]]. Action = [[-0.07929303 -0.00973064  0.          0.72370327]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 562 is [True, False, False, True, False, False]
State prediction error at timestep 562 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 562 of -1
Current timestep = 563. State = [[-0.05665064 -0.13879   ]]. Action = [[ 0.04411013  0.07709307  0.         -0.22632116]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 563 is [True, False, False, True, False, False]
State prediction error at timestep 563 is tensor(8.3674e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.0520551 -0.1322092]]. Action = [[0.06399263 0.07703445 0.         0.03338003]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 564 is [True, False, False, True, False, False]
State prediction error at timestep 564 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.04608085 -0.1291302 ]]. Action = [[ 0.08525639 -0.00403203  0.          0.54707694]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 565 is [False, True, False, True, False, False]
State prediction error at timestep 565 is tensor(6.1070e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 565 of -1
Current timestep = 566. State = [[-0.04725576 -0.13261835]]. Action = [[-0.08564589 -0.08264261  0.          0.49301457]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 566 is [False, True, False, True, False, False]
State prediction error at timestep 566 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 566 of -1
Current timestep = 567. State = [[-0.04731867 -0.13466588]]. Action = [[ 3.8916178e-02  1.8522143e-05  0.0000000e+00 -4.0619421e-01]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 567 is [False, True, False, True, False, False]
State prediction error at timestep 567 is tensor(2.3501e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.04705819 -0.13604641]]. Action = [[-0.01803077 -0.02944541  0.          0.29762435]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 568 is [False, True, False, True, False, False]
State prediction error at timestep 568 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 568 of -1
Current timestep = 569. State = [[-0.04511631 -0.13364843]]. Action = [[0.04579987 0.06359787 0.         0.59966564]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 569 is [False, True, False, True, False, False]
State prediction error at timestep 569 is tensor(5.2712e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 569 of -1
Current timestep = 570. State = [[-0.04792572 -0.13021   ]]. Action = [[-0.08929993  0.02853227  0.          0.6315352 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 570 is [False, True, False, True, False, False]
State prediction error at timestep 570 is tensor(9.4324e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 570 of -1
Current timestep = 571. State = [[-0.05375286 -0.12508032]]. Action = [[-0.0760667   0.07472418  0.         -0.1020751 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 571 is [True, False, False, True, False, False]
State prediction error at timestep 571 is tensor(6.5567e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.05856297 -0.123555  ]]. Action = [[-0.05437437 -0.03056484  0.         -0.22601873]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(4.1173e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.06411027 -0.12032261]]. Action = [[-0.07879262  0.06312681  0.         -0.8566464 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.06985801 -0.11275039]]. Action = [[-0.06305452  0.09279311  0.          0.34222817]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(1.8693e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.0769497  -0.10822935]]. Action = [[-0.09220493  0.00344096  0.         -0.5816692 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(9.1445e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 575 of -1
Current timestep = 576. State = [[-0.07763449 -0.10598056]]. Action = [[0.07020023 0.00631146 0.         0.41659653]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(1.1781e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.07859229 -0.10489196]]. Action = [[-0.03333036 -0.00953767  0.         -0.55085367]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 577 is [True, False, False, False, True, False]
State prediction error at timestep 577 is tensor(2.5512e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 577 of -1
Current timestep = 578. State = [[-0.07666045 -0.10057113]]. Action = [[0.08122849 0.06889325 0.         0.58092725]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(7.0927e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 578 of -1
Current timestep = 579. State = [[-0.07338209 -0.09988203]]. Action = [[ 0.05273344 -0.04893503  0.          0.06141973]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 579 is [True, False, False, False, True, False]
State prediction error at timestep 579 is tensor(5.2004e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 579 of -1
Current timestep = 580. State = [[-0.07160688 -0.10467951]]. Action = [[ 0.02694745 -0.08203986  0.         -0.3567741 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(7.4735e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 580 of -1
Current timestep = 581. State = [[-0.07502449 -0.109766  ]]. Action = [[-0.07857855 -0.05100805  0.          0.71183383]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 581 of -1
Current timestep = 582. State = [[-0.07363559 -0.11218422]]. Action = [[ 0.08670702 -0.00858164  0.         -0.7376473 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 582 is [True, False, False, False, True, False]
State prediction error at timestep 582 is tensor(4.3390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 582 of -1
Current timestep = 583. State = [[-0.06836107 -0.1140225 ]]. Action = [[ 0.06990761 -0.0153034   0.         -0.43430007]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(2.1491e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.06282552 -0.11777778]]. Action = [[ 0.07601524 -0.04651935  0.          0.16491973]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[-0.06428301 -0.1221259 ]]. Action = [[-0.08879364 -0.03166813  0.         -0.08264416]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(7.4037e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[-0.06604711 -0.12456576]]. Action = [[ 0.00823359 -0.00389598  0.          0.22195745]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(8.8957e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.07008027 -0.12968373]]. Action = [[-0.08782412 -0.07482897  0.          0.26585078]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 587 is [True, False, False, True, False, False]
State prediction error at timestep 587 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[-0.0739951  -0.12978975]]. Action = [[-0.03756366  0.06785371  0.          0.6711242 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 588 is [True, False, False, True, False, False]
State prediction error at timestep 588 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.07097439 -0.12959558]]. Action = [[ 0.09534388 -0.01867108  0.          0.8433502 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 589 is [True, False, False, True, False, False]
State prediction error at timestep 589 is tensor(9.6508e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.06431337 -0.13385274]]. Action = [[ 0.08623145 -0.06073452  0.         -0.6028256 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 590 is [True, False, False, True, False, False]
State prediction error at timestep 590 is tensor(2.3160e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 590 of -1
Current timestep = 591. State = [[-0.06194866 -0.13293616]]. Action = [[-0.00929859  0.07376195  0.         -0.1851387 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 591 is [True, False, False, True, False, False]
State prediction error at timestep 591 is tensor(6.9425e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.06116651 -0.12824762]]. Action = [[0.01879442 0.06215418 0.         0.12380278]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 592 is [True, False, False, True, False, False]
State prediction error at timestep 592 is tensor(1.4392e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.06446114 -0.12822782]]. Action = [[-0.08095533 -0.03703463  0.         -0.9938349 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 593 is [True, False, False, True, False, False]
State prediction error at timestep 593 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.06760845 -0.13088244]]. Action = [[-0.0285692  -0.03198981  0.         -0.27722335]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 594 is [True, False, False, True, False, False]
State prediction error at timestep 594 is tensor(1.0612e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 594 of -1
Current timestep = 595. State = [[-0.06424864 -0.12690794]]. Action = [[ 0.08526874  0.0984725   0.         -0.6900115 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 595 is [True, False, False, True, False, False]
State prediction error at timestep 595 is tensor(3.3348e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.06543583 -0.1199351 ]]. Action = [[-0.07427249  0.07244413  0.         -0.47774363]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(8.9512e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.06463835 -0.11537015]]. Action = [[ 0.0678601   0.02309416  0.         -0.6442143 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 597 is [True, False, False, False, True, False]
State prediction error at timestep 597 is tensor(3.7739e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.06242346 -0.11678903]]. Action = [[ 0.0146986  -0.06708665  0.         -0.4509257 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(3.3182e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.06492072 -0.12122955]]. Action = [[-0.06514392 -0.06416076  0.         -0.92813414]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.0626253  -0.11813313]]. Action = [[ 0.08178692  0.09537841  0.         -0.77901626]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(2.0174e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.06399033 -0.11916231]]. Action = [[-0.07866706 -0.0903886   0.         -0.80873185]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(6.8630e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.06956841 -0.12288208]]. Action = [[-0.08416041 -0.02711252  0.         -0.56172395]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(9.0192e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.06914468 -0.12372061]]. Action = [[ 0.05416488  0.00304721  0.         -0.8606174 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(6.8898e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.0641153  -0.12079743]]. Action = [[0.07463277 0.05964173 0.         0.35857177]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(1.1815e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.05995654 -0.12403212]]. Action = [[ 0.04385226 -0.09898021  0.         -0.01999605]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(8.8545e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 605 of -1
Current timestep = 606. State = [[-0.05497746 -0.12365992]]. Action = [[0.07549114 0.07213571 0.         0.68674564]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(9.3511e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.05207457 -0.12623914]]. Action = [[ 0.01392348 -0.08387735  0.         -0.60618156]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 607 is [True, False, False, True, False, False]
State prediction error at timestep 607 is tensor(2.7256e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.20048708 -0.06229757]]. Action = [[ 0.06072582  0.09196312  0.         -0.7412336 ]]. Reward = [100.]
Curr episode timestep = 235
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0146, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 608 of -1
Current timestep = 609. State = [[-0.19616847 -0.06505933]]. Action = [[ 0.04805071 -0.03008321  0.          0.06761003]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(3.7268e-06, device='cuda:0', grad_fn=<MseLossBackward>)
