Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of 0
Current timestep = 1. State = [[-0.33171427 -0.08338372]]. Action = [[-0.08950704 -0.00990598  0.          0.9261991 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of 0
Current timestep = 2. State = [[-0.3328339 -0.0853002]]. Action = [[ 0.05919344 -0.05022907  0.          0.01759768]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of 1
Current timestep = 3. State = [[-0.32827228 -0.08361239]]. Action = [[ 0.07941721  0.04839625  0.         -0.508093  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32541147 -0.08398286]]. Action = [[ 0.02240249 -0.05187935  0.          0.06094062]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.32796657 -0.08312941]]. Action = [[-0.06502926  0.04302163  0.         -0.71744275]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 0
Current timestep = 6. State = [[-0.33477846 -0.08681735]]. Action = [[-0.09400503 -0.09525112  0.         -0.552466  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.34229898 -0.08951927]]. Action = [[-0.08572083  0.01562569  0.          0.7507297 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of 0
Current timestep = 8. State = [[-0.3429933 -0.0930777]]. Action = [[ 0.06541271 -0.06984965  0.          0.36065435]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.33896095 -0.09812703]]. Action = [[ 0.05120709 -0.04880046  0.          0.9662454 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of 0
Current timestep = 10. State = [[-0.34125108 -0.09595294]]. Action = [[-0.09368511  0.09746248  0.          0.95652974]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.3479307  -0.08934768]]. Action = [[-0.0663424   0.0878428   0.         -0.77390236]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of 0
Current timestep = 12. State = [[-0.35244736 -0.09059425]]. Action = [[-0.02050723 -0.08005663  0.         -0.95366603]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35140505 -0.09698822]]. Action = [[ 0.06869086 -0.08365367  0.          0.800733  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.34902024 -0.10323804]]. Action = [[ 0.03629931 -0.07339015  0.          0.9915823 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.3490949 -0.1076261]]. Action = [[-0.00892501 -0.03339948  0.         -0.21021146]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 0
Current timestep = 16. State = [[-0.3515079  -0.11441314]]. Action = [[-0.03336702 -0.09514406  0.         -0.46414024]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.35029173 -0.11505826]]. Action = [[ 0.06345025  0.07857836  0.         -0.04174   ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 0
Current timestep = 18. State = [[-0.3523319  -0.11054408]]. Action = [[-0.07418723  0.06309222  0.         -0.7984641 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 0
Current timestep = 19. State = [[-0.35558873 -0.10843379]]. Action = [[-0.01425568  0.01485576  0.         -0.6113813 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.0905e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 0
Current timestep = 20. State = [[-0.35257852 -0.10661104]]. Action = [[0.08616925 0.02337196 0.         0.31405735]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 0
Current timestep = 21. State = [[-0.3548658  -0.10535714]]. Action = [[-0.09336501  0.00790472  0.          0.03791213]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 0
Current timestep = 22. State = [[-0.35557175 -0.10930715]]. Action = [[ 0.06818158 -0.09278917  0.         -0.53579533]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of 0
Current timestep = 23. State = [[-0.34988484 -0.11637624]]. Action = [[ 0.08810804 -0.09022754  0.          0.20954084]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of 0
Current timestep = 24. State = [[-0.34337574 -0.12336746]]. Action = [[ 0.07305998 -0.0835644   0.         -0.6234733 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 0
Current timestep = 25. State = [[-0.3373819  -0.12979925]]. Action = [[ 0.0586716  -0.06711313  0.         -0.70374674]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 0
Current timestep = 26. State = [[-0.33352703 -0.13567019]]. Action = [[ 0.01776709 -0.05413292  0.         -0.48643613]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 0
Current timestep = 27. State = [[-0.32794032 -0.14143996]]. Action = [[ 0.07442837 -0.05093045  0.         -0.83424187]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 0
Current timestep = 28. State = [[-0.3277978  -0.14186855]]. Action = [[-0.07979485  0.07022386  0.          0.7202823 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 0
Current timestep = 29. State = [[-0.3258146  -0.13863781]]. Action = [[ 0.07689694  0.05481077  0.         -0.1850676 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 0
Current timestep = 30. State = [[-0.32143438 -0.13551936]]. Action = [[0.0371806  0.03905082 0.         0.9162233 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of 0
Current timestep = 31. State = [[-0.3202558  -0.13195078]]. Action = [[-0.00803894  0.05223995  0.         -0.6777128 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 0
Current timestep = 32. State = [[-0.3158736 -0.1338143]]. Action = [[ 0.0913795  -0.07350455  0.          0.00936604]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 0
Current timestep = 33. State = [[-0.3137188 -0.1331349]]. Action = [[-0.03149703  0.06190469  0.         -0.41405976]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 0
Current timestep = 34. State = [[-0.31599396 -0.12886563]]. Action = [[-0.03641069  0.05197718  0.         -0.9825652 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 0
Current timestep = 35. State = [[-0.3148087  -0.12837163]]. Action = [[ 0.0461796  -0.03045865  0.          0.7447723 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 0
Current timestep = 36. State = [[-0.3169411  -0.13143855]]. Action = [[-0.07471297 -0.05080685  0.         -0.93430793]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of 0
Current timestep = 37. State = [[-0.3200847  -0.13767758]]. Action = [[-0.02573026 -0.08764624  0.         -0.6517867 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.3195173  -0.13967232]]. Action = [[ 0.02334408  0.02238834  0.         -0.6660028 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of 0
Current timestep = 39. State = [[-0.3227098  -0.13600354]]. Action = [[-0.08674236  0.07239955  0.         -0.4790734 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of 0
Current timestep = 40. State = [[-0.32277995 -0.1376502 ]]. Action = [[ 0.06061288 -0.08108164  0.         -0.45435917]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of 0
Current timestep = 41. State = [[-0.31574935 -0.14205395]]. Action = [[ 0.09672291 -0.04023194  0.         -0.3991233 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of 0
Current timestep = 42. State = [[-0.3100466  -0.13866402]]. Action = [[ 0.03369857  0.09605587  0.         -0.66522324]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 0
Current timestep = 43. State = [[-0.30896354 -0.13782234]]. Action = [[-0.01624618 -0.03930001  0.          0.8974091 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 0
Current timestep = 44. State = [[-0.30681548 -0.13864681]]. Action = [[ 0.0407576   0.00752018  0.         -0.24486202]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 0
Current timestep = 45. State = [[-0.30242226 -0.14076787]]. Action = [[ 0.05374403 -0.04546371  0.          0.5722256 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 0
Current timestep = 46. State = [[-0.29959893 -0.13928998]]. Action = [[0.00999278 0.05781772 0.         0.62941134]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 0
Current timestep = 47. State = [[-0.30027914 -0.13326357]]. Action = [[-0.03295085  0.08971167  0.         -0.15379828]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 0
Current timestep = 48. State = [[-0.29743826 -0.13276593]]. Action = [[ 0.0748964  -0.05470588  0.         -0.69403636]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of 0
Current timestep = 49. State = [[-0.29105487 -0.1302552 ]]. Action = [[ 0.06843137  0.06966304  0.         -0.3943293 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 0
Current timestep = 50. State = [[-0.28587225 -0.12368415]]. Action = [[ 0.04361147  0.07536175  0.         -0.8213749 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 0
Current timestep = 51. State = [[-0.28834355 -0.12039988]]. Action = [[-0.09513158  0.00352763  0.          0.4578445 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of 0
Current timestep = 52. State = [[-0.29205474 -0.12410445]]. Action = [[-0.03574523 -0.0845272   0.         -0.41927582]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 0
Current timestep = 53. State = [[-0.29624727 -0.1282352 ]]. Action = [[-0.07497194 -0.03193209  0.         -0.22992337]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of 0
Current timestep = 54. State = [[-0.29589143 -0.13368122]]. Action = [[ 0.04203758 -0.08415519  0.          0.06532562]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of 0
Current timestep = 55. State = [[-0.29667577 -0.140365  ]]. Action = [[-0.05448253 -0.07294428  0.          0.3819257 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of 0
Current timestep = 56. State = [[-0.2983113 -0.1468543]]. Action = [[-0.01834776 -0.06216679  0.         -0.14994186]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of 0
Current timestep = 57. State = [[-0.30167744 -0.14739716]]. Action = [[-0.06923375  0.05752655  0.          0.8834276 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of 0
Current timestep = 58. State = [[-0.3000977  -0.14363334]]. Action = [[0.07108109 0.06376632 0.         0.32827997]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.2986288  -0.13878441]]. Action = [[-0.01091284  0.06261859  0.         -0.5069979 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.2963285  -0.13810493]]. Action = [[ 0.05514067 -0.0279979   0.          0.4960332 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.29915243 -0.13978836]]. Action = [[-0.08761229 -0.01734986  0.         -0.5406841 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.30086342 -0.14270923]]. Action = [[ 0.02594591 -0.04237846  0.          0.8172065 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.29868025 -0.14372739]]. Action = [[ 0.04154573  0.00502446  0.         -0.41878426]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.30034265 -0.14452285]]. Action = [[-0.05157159 -0.01778157  0.          0.28918397]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.30405682 -0.14577237]]. Action = [[-0.03846582 -0.00655277  0.          0.6684903 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.31031418 -0.14914207]]. Action = [[-0.09239057 -0.05014599  0.          0.8551898 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.3163254  -0.15624686]]. Action = [[-0.05193396 -0.09361879  0.         -0.8620955 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.31662235 -0.16218345]]. Action = [[ 0.04254071 -0.04331895  0.          0.12327409]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.31334448 -0.16414826]]. Action = [[0.05203403 0.00216082 0.         0.07190657]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.30790123 -0.16737826]]. Action = [[ 0.0833068  -0.05699679  0.         -0.63522387]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.30925715 -0.17188826]]. Action = [[-0.07941232 -0.03892618  0.          0.6235107 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.31496134 -0.1757899 ]]. Action = [[-0.05745954 -0.02752973  0.         -0.53924567]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.3151138  -0.17932715]]. Action = [[ 0.04996344 -0.02809417  0.          0.8607116 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.31527758 -0.18561265]]. Action = [[-0.01912111 -0.08808038  0.          0.71452534]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.3127164  -0.19101198]]. Action = [[ 0.06961016 -0.03238067  0.          0.21121502]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.3067857  -0.19516928]]. Action = [[ 0.07927463 -0.04280641  0.          0.27830112]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.30697498 -0.1994753 ]]. Action = [[-0.05600368 -0.03583916  0.          0.7641928 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.30461285 -0.19796927]]. Action = [[ 0.08140185  0.08521206  0.         -0.86945236]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.30311123 -0.19636795]]. Action = [[-0.01553826  0.00183494  0.          0.5325072 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.29954436 -0.20005554]]. Action = [[ 0.08135382 -0.06619494  0.         -0.66805685]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.30074978 -0.19903947]]. Action = [[-0.08704804  0.08874155  0.         -0.29081774]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.30224544 -0.19491889]]. Action = [[ 0.03410202  0.03696721  0.         -0.278064  ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.29940605 -0.19547448]]. Action = [[ 0.05533668 -0.04952319  0.          0.7333622 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of -1
Current timestep = 84. State = [[-0.3002064  -0.19216964]]. Action = [[-0.04426252  0.08773812  0.          0.45982158]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.30723917 -0.19056693]]. Action = [[-0.09428581 -0.02934946  0.         -0.14883411]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-0.3140948  -0.18865407]]. Action = [[-0.05286212  0.04575082  0.          0.01339662]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.32091814 -0.18584913]]. Action = [[-0.07493465  0.02194123  0.         -0.8879589 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.32293886 -0.1815077 ]]. Action = [[0.03259356 0.05902698 0.         0.20120549]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.3216048  -0.17416799]]. Action = [[0.03213664 0.08405031 0.         0.8593869 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.32000723 -0.17107333]]. Action = [[ 0.03185453 -0.02569555  0.         -0.2845708 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.32140446 -0.17491904]]. Action = [[-0.02805237 -0.0985414   0.          0.8203294 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of -1
Current timestep = 92. State = [[-0.32575637 -0.18023413]]. Action = [[-0.05271909 -0.06474981  0.          0.1523962 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.32612064 -0.17811146]]. Action = [[ 0.04089048  0.08121031  0.         -0.80534434]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.32690755 -0.17771564]]. Action = [[-0.02991013 -0.04617567  0.         -0.6074817 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.32782027 -0.17484841]]. Action = [[ 0.00565342  0.07876996  0.         -0.9033155 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.3294668  -0.17533109]]. Action = [[-0.02818613 -0.05890858  0.         -0.6046636 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.3271264  -0.17728655]]. Action = [[ 0.06957077 -0.01090966  0.         -0.03548616]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.3251046  -0.17640588]]. Action = [[ 2.2283942e-04  1.8302307e-02  0.0000000e+00 -6.0556012e-01]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.32440796 -0.17161734]]. Action = [[0.01141059 0.08231904 0.         0.1689229 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.32110077 -0.16659579]]. Action = [[ 0.06060033  0.04070175  0.         -0.5860165 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.3148108  -0.15984584]]. Action = [[ 0.09001807  0.08428241  0.         -0.8903318 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(7.9441e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.31279552 -0.15371402]]. Action = [[-0.01355568  0.04210549  0.         -0.5098765 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.31047574 -0.15455283]]. Action = [[ 0.05753232 -0.07350572  0.          0.47218263]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.3055903  -0.15433393]]. Action = [[ 0.0616812   0.02032554  0.         -0.29153645]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.299854  -0.1563446]]. Action = [[ 0.06691521 -0.07372994  0.         -0.23251992]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.29606214 -0.15294223]]. Action = [[ 0.01501305  0.09803038  0.         -0.11838245]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.29243144 -0.15059473]]. Action = [[ 0.05308376 -0.02648973  0.          0.55912673]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.28669626 -0.14988308]]. Action = [[0.06951847 0.00840224 0.         0.5874654 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.28496626 -0.14785881]]. Action = [[-0.02512036  0.02244414  0.          0.5265367 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.2861509  -0.14255846]]. Action = [[-0.02837857  0.08696149  0.          0.501274  ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
State prediction error at timestep 110 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.288077   -0.13583702]]. Action = [[-0.03499565  0.07450033  0.          0.14575648]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.28864262 -0.13724232]]. Action = [[-2.0495057e-04 -9.0517901e-02  0.0000000e+00 -8.1495064e-01]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.2841378 -0.1431988]]. Action = [[ 0.07524293 -0.08176417  0.          0.75676346]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.28414524 -0.14580347]]. Action = [[-0.07377589 -0.00180947  0.         -0.13254768]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.2829831  -0.14140457]]. Action = [[0.05070782 0.09068794 0.         0.51751196]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
State prediction error at timestep 115 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.27662802 -0.14243048]]. Action = [[ 0.09709632 -0.08950587  0.          0.4408002 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(8.6985e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.2731632  -0.14676085]]. Action = [[-0.00667864 -0.04185788  0.          0.8627572 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.27211717 -0.14386883]]. Action = [[ 0.00344368  0.09144051  0.         -0.64057016]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.27412564 -0.13822731]]. Action = [[-0.05677409  0.06636769  0.         -0.17768538]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(8.4173e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.2733814  -0.13819358]]. Action = [[ 0.04164637 -0.04365805  0.          0.20970964]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(8.2646e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.26708972 -0.14338876]]. Action = [[ 0.09120669 -0.08497735  0.          0.25747573]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(3.7364e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.2623358  -0.14535257]]. Action = [[0.02216564 0.01191719 0.         0.67722   ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.26449436 -0.14888936]]. Action = [[-0.07982123 -0.06657366  0.         -0.3760634 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.26342273 -0.14780656]]. Action = [[0.05058695 0.07821398 0.         0.10265589]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(1.0382e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.25805447 -0.14286672]]. Action = [[ 0.06849272  0.05684984  0.         -0.7569886 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.25230753 -0.1429412 ]]. Action = [[ 0.06387926 -0.04542904  0.         -0.05481797]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.24840568 -0.1402904 ]]. Action = [[0.02713115 0.07163658 0.         0.6861923 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.24889028 -0.13883434]]. Action = [[-0.03831253 -0.01731671  0.         -0.73687196]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.24795905 -0.14177707]]. Action = [[ 0.02772129 -0.05345065  0.         -0.92039275]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, True, False, False]
State prediction error at timestep 129 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.24681295 -0.14527997]]. Action = [[-0.00752373 -0.03800657  0.          0.90978193]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, True, False, False]
State prediction error at timestep 130 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.24405037 -0.14666043]]. Action = [[ 4.2093165e-02  5.5327266e-04  0.0000000e+00 -8.7660128e-01]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.24631831 -0.14883345]]. Action = [[-0.08915719 -0.03532914  0.          0.8516077 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, True, False, False]
State prediction error at timestep 132 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.24542214 -0.15052873]]. Action = [[ 0.05227018 -0.00088728  0.         -0.41310322]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.24371839 -0.14649387]]. Action = [[-0.00675642  0.08962549  0.          0.19747484]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.2405827 -0.1406397]]. Action = [[ 0.05855776  0.06267198  0.         -0.01704133]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, True, False, False]
State prediction error at timestep 135 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.23993844 -0.142099  ]]. Action = [[-0.02620997 -0.08053849  0.         -0.8738654 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.2414148  -0.14423057]]. Action = [[-0.02380909  0.00123437  0.          0.05346084]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, True, False, False]
State prediction error at timestep 137 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.23930031 -0.14794733]]. Action = [[ 0.05073636 -0.07227103  0.          0.8359096 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, True, False, False]
State prediction error at timestep 138 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.23310742 -0.15471116]]. Action = [[ 0.0877193  -0.09150521  0.          0.0274483 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.22728793 -0.15762292]]. Action = [[ 0.05131599  0.00556059  0.         -0.38965833]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, True, False, False]
State prediction error at timestep 140 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.22848986 -0.15455371]]. Action = [[-0.07744251  0.08060715  0.         -0.62424564]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, True, False, False]
State prediction error at timestep 141 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.23027945 -0.15134914]]. Action = [[0.00018109 0.03078959 0.         0.11800218]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, True, False, False]
State prediction error at timestep 142 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.23213038 -0.15544178]]. Action = [[-0.03747293 -0.09590785  0.          0.6803417 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, True, False, False]
State prediction error at timestep 143 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.22880222 -0.15545046]]. Action = [[0.08884939 0.06463123 0.         0.31043816]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, True, False, False]
State prediction error at timestep 144 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
