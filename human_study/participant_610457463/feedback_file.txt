Current timestep = 0. State = [[-0.25780663  0.00767722]]. Action = [[ 0.05922106 -0.07896183  0.10002673  0.69163346]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.2574259   0.00736444]]. Action = [[-0.23175809  0.16156906 -0.17457773 -0.90839076]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0206, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25688156  0.00697279]]. Action = [[ 0.10777292 -0.17335998  0.23817012 -0.8266451 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25628468  0.00647899]]. Action = [[-0.15476371 -0.2161498  -0.09076053 -0.9427275 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25610796  0.00622855]]. Action = [[-0.22328448 -0.07975703 -0.01632737 -0.9672625 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25604486  0.00623397]]. Action = [[-0.15625371  0.22396737 -0.17903501 -0.8182473 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25565895  0.0059251 ]]. Action = [[-0.2281697  -0.2354173  -0.01779774 -0.9419487 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.25545704  0.00534374]]. Action = [[ 0.24835357 -0.15926325  0.10204041 -0.446877  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.2554287   0.00457727]]. Action = [[-0.12073457 -0.00840858  0.22876155  0.6863847 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.2554675  0.0042871]]. Action = [[0.11178839 0.22096312 0.17667937 0.3943541 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25568053  0.00330931]]. Action = [[-0.11368433 -0.11519915 -0.12582374 -0.8629796 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.2571412 -0.0007121]]. Action = [[ 0.09159756  0.00998095 -0.18711217 -0.26529086]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.25745925 -0.00334174]]. Action = [[ 0.12576014 -0.05212899  0.24319208 -0.15031105]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.25715965 -0.00394595]]. Action = [[-0.15766542 -0.07686652 -0.1121147  -0.10424656]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25686866 -0.00441471]]. Action = [[-0.19214796 -0.0888308   0.19587553 -0.14970237]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.25673786 -0.00476467]]. Action = [[-0.2287068  -0.14866501 -0.22804306 -0.09136868]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25682244 -0.00692158]]. Action = [[ 0.02723941  0.048089   -0.15913144 -0.68937045]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.25688508 -0.00687378]]. Action = [[ 0.17146271  0.15256277  0.24384636 -0.94949913]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.2564083  -0.00700498]]. Action = [[ 0.09196407 -0.07697973  0.18327081  0.5604546 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.25570318 -0.00767529]]. Action = [[ 0.19213325 -0.04487668 -0.21288915  0.78788257]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25497591 -0.00806743]]. Action = [[ 0.19356507 -0.02196577 -0.20123821 -0.0075981 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.2548412  -0.00836941]]. Action = [[0.16902831 0.06780687 0.23936218 0.69527626]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25469014 -0.00850964]]. Action = [[-0.1045337  -0.20573103  0.09303913  0.8424543 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.2526453  -0.00979287]]. Action = [[ 0.02145317  0.04872891 -0.0298588  -0.9028211 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.25247437 -0.00977383]]. Action = [[-0.14754425 -0.14009257 -0.01692384  0.38609815]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.25228402 -0.00979652]]. Action = [[-0.11405784 -0.18010207  0.15991926  0.66717124]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.2519642  -0.00967538]]. Action = [[-0.18287127 -0.17538737 -0.03402841 -0.28655207]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.25172505 -0.00978463]]. Action = [[-0.11225525 -0.13609508 -0.17242554  0.6875777 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.2517196  -0.00979975]]. Action = [[-0.19175932  0.08130032  0.09911519  0.96520734]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.25087377 -0.00987287]]. Action = [[-0.01748708 -0.07007766 -0.19910163 -0.18843925]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.25094083 -0.0109218 ]]. Action = [[-0.13197483  0.02363816  0.20297661  0.4488666 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.2510628  -0.01116112]]. Action = [[ 0.00615823 -0.16734654 -0.10515952 -0.96765244]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(5.2558e-05, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.2511076  -0.01127623]]. Action = [[0.14685875 0.20163202 0.2043829  0.6647761 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.2510201  -0.01133568]]. Action = [[-0.06077524 -0.17340995 -0.07396291 -0.51665133]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.2510273  -0.01139122]]. Action = [[-0.04987389 -0.18987705  0.01511115 -0.17859149]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.2510558 -0.0114551]]. Action = [[0.0722591  0.20515269 0.1857408  0.63441753]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.2511611  -0.01157772]]. Action = [[ 0.18561304 -0.16949466  0.0282011  -0.79324806]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.2511611  -0.01157772]]. Action = [[ 0.23117423  0.16575426 -0.10018893 -0.43986392]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.25107738 -0.0116217 ]]. Action = [[-0.03891885 -0.21767998 -0.11505295 -0.6974121 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.25121045 -0.01174248]]. Action = [[ 0.14110702 -0.01709238 -0.09597845  0.29783177]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.25109902 -0.01178791]]. Action = [[ 0.09492096 -0.17432757 -0.12092033  0.48050594]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.251123 -0.011802]]. Action = [[-0.04067953 -0.14713849  0.24617606 -0.23932457]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.2512386  -0.01199403]]. Action = [[-0.12544848 -0.01630764 -0.09573266 -0.6639791 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.2519535  -0.01225223]]. Action = [[ 0.10578206  0.14974558 -0.15340725 -0.7891188 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(9.8278e-05, grad_fn=<MseLossBackward0>)
Current timestep = 44. State = [[-0.2522662  -0.01225861]]. Action = [[ 0.00504473  0.18007615  0.05626252 -0.8657933 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(6.0408e-05, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.25248772 -0.01231903]]. Action = [[-0.14261259  0.04219308 -0.24288383 -0.49487948]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.2528016  -0.01249114]]. Action = [[-0.14945143  0.13143945  0.15480465  0.24226463]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.25372034 -0.01259468]]. Action = [[-0.12494467  0.023626   -0.0694069   0.6930933 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 48. State = [[-0.2546871  -0.01235752]]. Action = [[ 0.16068149 -0.14618947 -0.19823723  0.60342455]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.25519413 -0.01236421]]. Action = [[-0.19523221 -0.05654663 -0.24033673 -0.7236273 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(1.5519e-05, grad_fn=<MseLossBackward0>)
Current timestep = 50. State = [[-0.2579744  -0.01219781]]. Action = [[-0.03104565 -0.02949035  0.17460883 -0.6161824 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.25852793 -0.01251833]]. Action = [[-0.23157287 -0.19868042  0.09491962  0.5125494 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.2587223  -0.01258021]]. Action = [[ 0.1695441  -0.21645169 -0.22331242 -0.29645276]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.25910068 -0.0127318 ]]. Action = [[0.16474906 0.1343477  0.19403076 0.7994549 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.2592376 -0.0126745]]. Action = [[ 0.08082592 -0.21833028  0.14825505 -0.02663767]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.25952882 -0.01278088]]. Action = [[-0.05236632  0.23616564  0.01233023  0.02507865]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 56. State = [[-0.26007575 -0.01311669]]. Action = [[0.23312017 0.14026737 0.22782487 0.59030676]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.26020157 -0.01314993]]. Action = [[-0.03874996 -0.13468078  0.08750746 -0.9070088 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(1.2717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.260723   -0.01342437]]. Action = [[ 0.12971449 -0.1930455  -0.16908225  0.9215976 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.26111278 -0.01391612]]. Action = [[ 0.03455433 -0.04216222 -0.02456139  0.91336024]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.26137078 -0.01426646]]. Action = [[0.21639931 0.06005812 0.01090577 0.302961  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.261418   -0.01444339]]. Action = [[-0.15792678 -0.13207415  0.08825517 -0.07374668]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.2613301  -0.01480602]]. Action = [[ 0.23186731 -0.15226443 -0.01292333  0.4160117 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.26152593 -0.01587409]]. Action = [[-0.11054298 -0.06411618 -0.20095967  0.8772123 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.26324245 -0.01871198]]. Action = [[-0.0295801  -0.06649524  0.20575336 -0.5279642 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.26366696 -0.01968482]]. Action = [[ 0.15387589  0.00251055 -0.23739438 -0.6097551 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.26404458 -0.02058806]]. Action = [[-0.1576482  -0.08569174 -0.23328066 -0.80337226]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(3.1059e-05, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.264364   -0.02124477]]. Action = [[-0.2005331  -0.00856718 -0.21952398 -0.3462187 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.26492235 -0.02214644]]. Action = [[-0.11731994  0.23443484  0.11547929  0.85484505]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.265467   -0.02327258]]. Action = [[ 0.19910365 -0.04056932  0.17583501  0.1919936 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.26656678 -0.02475912]]. Action = [[ 0.04659349  0.099556   -0.13043492 -0.17832214]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.2667034  -0.02413434]]. Action = [[-0.22250174  0.19114757 -0.23762085  0.49948335]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.26676673 -0.02365834]]. Action = [[ 0.19687322 -0.03185491 -0.12606008  0.63265836]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 73. State = [[-0.26692107 -0.02341413]]. Action = [[-0.15777424 -0.06407124 -0.06153631  0.09085882]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 74. State = [[-0.2669826 -0.0232063]]. Action = [[ 0.15842122 -0.03553556 -0.128728    0.20991421]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.26725778 -0.02206749]]. Action = [[0.11672613 0.01318786 0.04032126 0.9873438 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.26728696 -0.02198206]]. Action = [[-0.12016863 -0.23766725 -0.04564585  0.56964135]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.26722828 -0.02168867]]. Action = [[ 0.12881708  0.0300276  -0.0940818  -0.0266574 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.26558253 -0.02133951]]. Action = [[ 0.11255413 -0.06513983 -0.08610752  0.3352461 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.26462913 -0.0213798 ]]. Action = [[-0.19551972  0.11251473  0.22412595 -0.3468355 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.2638355  -0.02145466]]. Action = [[ 0.1092661  -0.21557835  0.2312648  -0.80486715]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(6.6311e-05, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.26160294 -0.02161107]]. Action = [[ 0.06375214 -0.00082618  0.16033095 -0.512959  ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.2595103  -0.02174535]]. Action = [[0.07564184 0.09522927 0.23416    0.01592016]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 83. State = [[-0.2571447  -0.02015305]]. Action = [[-0.10558036  0.02928257  0.24252796 -0.65917015]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(7.0676e-05, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.2570973  -0.01963323]]. Action = [[-0.21665359  0.16209233 -0.0416908  -0.67204225]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(8.6725e-05, grad_fn=<MseLossBackward0>)
Current timestep = 85. State = [[-0.25703475 -0.01838087]]. Action = [[ 0.05179152 -0.05501717  0.16562414 -0.547688  ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.25709328 -0.01835608]]. Action = [[-0.05136707  0.17473143 -0.1647459   0.14129043]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.25685114 -0.01839032]]. Action = [[-0.17315091  0.18408662 -0.21895984  0.16711807]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.2559387  -0.01827567]]. Action = [[ 0.0519008  -0.00131004  0.20868868 -0.39139032]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25574398 -0.01824829]]. Action = [[-0.01450862  0.14141148  0.09242761 -0.70260626]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.2553882  -0.01820827]]. Action = [[ 0.17093712 -0.07549296 -0.03817292 -0.94143426]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(5.2342e-05, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.2551059  -0.01816826]]. Action = [[-0.19634725 -0.11618625  0.09009656 -0.263363  ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25336552 -0.01787158]]. Action = [[-0.02098145  0.00343424  0.12375703  0.8751323 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(7.9377e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.25318542 -0.01794641]]. Action = [[ 0.07768321 -0.13712296  0.22097883  0.13912785]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.2531255  -0.01789489]]. Action = [[0.08569252 0.1368967  0.2134968  0.17835522]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.25310758 -0.01783919]]. Action = [[-0.17642044 -0.062254   -0.11179294  0.8007207 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(6.3233e-05, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.25320083 -0.01778659]]. Action = [[-0.21403107 -0.19208583 -0.06894782 -0.74966365]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(2.3175e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.2528415  -0.01759153]]. Action = [[ 0.07285312  0.09760112 -0.16614984 -0.6861339 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(7.7486e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.2526074  -0.01707838]]. Action = [[-0.21019559  0.21846625 -0.21473406 -0.23778743]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.2517584  -0.01644521]]. Action = [[-0.09702104  0.1522116   0.16748908 -0.56686544]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.25149888 -0.01639298]]. Action = [[-0.17718956 -0.09766743 -0.02195717 -0.5208523 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(9.1395e-05, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.2511864  -0.01623069]]. Action = [[ 0.23405504 -0.16161679 -0.16504753  0.56341815]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.24989684 -0.01490837]]. Action = [[-0.11949939  0.03048554  0.14402348 -0.86123854]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.25040314 -0.01349721]]. Action = [[-0.084473   -0.11752068 -0.18934873 -0.04678386]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.25052422 -0.01382497]]. Action = [[ 0.23580527  0.04683161 -0.20207724 -0.81759256]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.25082687 -0.01392427]]. Action = [[ 0.09489164  0.08574605 -0.21772218 -0.83324194]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 106. State = [[-0.2505432  -0.01363473]]. Action = [[-0.16666256  0.21750495 -0.04637206 -0.10242623]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.25053993 -0.0135317 ]]. Action = [[ 0.01728228 -0.20077008 -0.22732168  0.16020405]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.25068203 -0.0132777 ]]. Action = [[-0.02139264  0.06242123  0.19678175  0.95415974]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(4.5662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.25121567 -0.01128817]]. Action = [[ 0.04279497 -0.06744796  0.11788553 -0.8479684 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.25121197 -0.01124258]]. Action = [[-0.15527327 -0.18258718 -0.19108558 -0.14159614]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.2511595  -0.01132239]]. Action = [[ 0.17967933 -0.0068161   0.17254084 -0.9336169 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(7.5219e-05, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.25108525 -0.01142794]]. Action = [[-0.15034193 -0.04261531  0.06704915  0.06703937]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.25122538 -0.01128838]]. Action = [[ 0.24150985 -0.07776079 -0.2119671   0.48099947]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.2511408 -0.01139  ]]. Action = [[ 0.11461502 -0.06724238 -0.1097759   0.4866916 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 115. State = [[-0.24927443 -0.01249871]]. Action = [[-0.09892777 -0.10037217 -0.21319194  0.91662526]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(3.6679e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.24908148 -0.0133405 ]]. Action = [[ 0.16025305  0.08257371 -0.24063864 -0.1481741 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.24910861 -0.01444593]]. Action = [[-0.14196678  0.14270103  0.05208686  0.10982704]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.24928391 -0.01602373]]. Action = [[-0.04155575  0.06275433  0.16342878 -0.18406636]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.24927664 -0.01582467]]. Action = [[ 0.22941741 -0.14718564 -0.11019447 -0.04974812]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.24939904 -0.01572556]]. Action = [[0.00796998 0.18017185 0.12295163 0.8280153 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(3.6734e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.24939676 -0.01593233]]. Action = [[ 0.1793915  -0.21587713  0.17159563  0.9518342 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 122. State = [[-0.2493601  -0.01606377]]. Action = [[ 0.06109047 -0.0692946  -0.2140614   0.8533814 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(2.9430e-06, grad_fn=<MseLossBackward0>)
Current timestep = 123. State = [[-0.24916576 -0.01670254]]. Action = [[-0.13940075 -0.05171876  0.12708092 -0.53374237]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(8.2914e-06, grad_fn=<MseLossBackward0>)
Current timestep = 124. State = [[-0.2492142 -0.0169764]]. Action = [[ 0.11033964  0.2074089  -0.14256637  0.5061679 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.24927554 -0.01707689]]. Action = [[ 0.18790525  0.21082896 -0.11980751 -0.9304563 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.24917763 -0.01734414]]. Action = [[-0.24178907 -0.02204028  0.10081452  0.19468617]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.24920174 -0.01751202]]. Action = [[ 0.15839985  0.12303019  0.22392839 -0.76536846]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(1.2942e-05, grad_fn=<MseLossBackward0>)
Current timestep = 128. State = [[-0.24915858 -0.01800866]]. Action = [[0.04663631 0.09769848 0.12811244 0.2364285 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.2490923  -0.01774435]]. Action = [[-0.06251553  0.16585755  0.0236713   0.6186459 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.24899493 -0.01759024]]. Action = [[ 0.18517753 -0.16072704 -0.1337526  -0.09651297]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.24901101 -0.01754796]]. Action = [[-0.17299725 -0.13032924 -0.02029857 -0.8069309 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.24889968 -0.0172711 ]]. Action = [[-0.05223347  0.0167987   0.23421484 -0.84626794]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 133. State = [[-0.24882782 -0.01710969]]. Action = [[ 0.04281646 -0.14946765  0.20042542 -0.93712795]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.24895197 -0.01692282]]. Action = [[ 0.10823753 -0.19006667  0.05942485 -0.6854064 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(4.3071e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.24892426 -0.01647048]]. Action = [[-0.04983321  0.02837065  0.01373321 -0.5967567 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(2.4681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.24905814 -0.01616524]]. Action = [[0.10015312 0.15250131 0.16691005 0.16583788]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.2491965  -0.01544622]]. Action = [[ 0.00465536 -0.05279329 -0.05680279 -0.55805856]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(2.5847e-05, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.24913037 -0.01582083]]. Action = [[-0.02403811 -0.09939152 -0.10772029  0.76180756]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(1.2818e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.24908574 -0.01655922]]. Action = [[-0.1613788   0.1850428  -0.18555368 -0.7509689 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.24903318 -0.01690745]]. Action = [[-0.13632569  0.20329016 -0.11474861 -0.5948093 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(2.8016e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.24910566 -0.01819703]]. Action = [[ 0.08435994 -0.04823473  0.12939799 -0.51885104]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(4.7318e-06, grad_fn=<MseLossBackward0>)
Current timestep = 142. State = [[-0.24882191 -0.02032965]]. Action = [[-0.07455429 -0.09764247 -0.13192959 -0.4716761 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(1.1174e-05, grad_fn=<MseLossBackward0>)
Current timestep = 143. State = [[-0.24868119 -0.02190071]]. Action = [[-0.23386127 -0.11508736  0.03494272  0.2448715 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(6.4005e-05, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.24866717 -0.02535746]]. Action = [[-0.05116202 -0.12513247 -0.01177202  0.3583492 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 145. State = [[-0.2487508  -0.02823695]]. Action = [[0.15219125 0.18047327 0.20377135 0.5411141 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.24881461 -0.02920292]]. Action = [[-0.18858738 -0.21396454  0.18144268  0.5668634 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.24949077 -0.03452132]]. Action = [[-0.08406147 -0.08543718 -0.16502653  0.15054584]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.25002363 -0.03622188]]. Action = [[-0.15630704 -0.13264093  0.11461958  0.7734318 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.25139248 -0.04102578]]. Action = [[-0.04118562 -0.13309823 -0.07647201  0.617579  ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.2517063  -0.04284159]]. Action = [[0.06110501 0.22450677 0.18868905 0.46759236]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.2521988  -0.04457855]]. Action = [[-0.22462481  0.23152333  0.15613022  0.75262547]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 152. State = [[-0.2527477  -0.04575252]]. Action = [[-0.14846945  0.1834259   0.00207517 -0.79032123]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.25295028 -0.04731617]]. Action = [[0.10181615 0.13530004 0.01372585 0.8487971 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.25349668 -0.04907309]]. Action = [[-0.17837603 -0.13572437 -0.20384014  0.7628293 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 155. State = [[-0.25388047 -0.05047126]]. Action = [[ 0.0206998  -0.13652399  0.05198899  0.77793086]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.25397113 -0.05109066]]. Action = [[ 0.24146673  0.20057401  0.23152858 -0.9963087 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.25430217 -0.05194505]]. Action = [[-0.17798647 -0.05509794  0.10229564  0.60822654]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.25452656 -0.05309266]]. Action = [[ 0.08755136  0.14611918  0.0568583  -0.8468106 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.25465655 -0.05352686]]. Action = [[ 0.2128816   0.06259003  0.00383374 -0.44795114]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.2554218  -0.05532462]]. Action = [[-0.04757209  0.06280959 -0.171844   -0.77370816]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.25587982 -0.0550436 ]]. Action = [[-0.05342683  0.20107919  0.1292859  -0.7570157 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.25607675 -0.05501573]]. Action = [[-0.20675856 -0.05932674 -0.2429985   0.6184466 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.25631872 -0.05515926]]. Action = [[ 0.08364224  0.14892048 -0.16876265  0.08201253]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 164. State = [[-0.25707445 -0.05488759]]. Action = [[-0.10387693  0.09264156  0.05711263 -0.6585563 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.2579844  -0.05391246]]. Action = [[ 0.05122572  0.21350807 -0.2095719  -0.4929788 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.25837904 -0.05344785]]. Action = [[-0.1877164  -0.07749993 -0.19342573 -0.41612768]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.2598904  -0.05201915]]. Action = [[ 0.11984158  0.04472291  0.05165789 -0.05955911]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.26021838 -0.05062153]]. Action = [[-0.12587088  0.00367787 -0.00698303  0.62246394]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.26054788 -0.0502696 ]]. Action = [[ 0.0118407   0.21407521 -0.1523896  -0.827365  ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.26112747 -0.04989071]]. Action = [[ 0.21199977 -0.09623705 -0.04402199  0.90248156]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.26149738 -0.04965119]]. Action = [[-0.07169689  0.14667132  0.06409872  0.8884622 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.2625621  -0.04860583]]. Action = [[ 0.00597599  0.12005427  0.12909412 -0.9448474 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.26290888 -0.04741766]]. Action = [[ 0.09150499 -0.19364953 -0.11187798  0.6616883 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.2631905 -0.0463951]]. Action = [[ 0.02021849 -0.1352305   0.16162246 -0.9526986 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.26332977 -0.04583038]]. Action = [[-0.22676216 -0.09677288 -0.1172408  -0.9055276 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.26360875 -0.04487581]]. Action = [[ 0.10165164  0.13810366  0.24465787 -0.66260356]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.26402825 -0.04367921]]. Action = [[ 0.15906322 -0.04422209  0.19031525 -0.13940859]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.26407814 -0.04307963]]. Action = [[-0.10367513 -0.1593292   0.01187843 -0.62679225]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.26435837 -0.04241035]]. Action = [[-0.14181148 -0.02950007 -0.21558923  0.61019397]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.26450774 -0.04190511]]. Action = [[-0.2097528   0.11852527  0.01350304  0.0659672 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.26461363 -0.04126152]]. Action = [[-0.05397783 -0.1615592   0.04145178  0.12997603]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.26474765 -0.0410819 ]]. Action = [[-0.0735248   0.14023727 -0.08198318  0.12987351]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-0.26498365 -0.04012509]]. Action = [[-0.17812528 -0.08417542 -0.00245003 -0.63847214]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.26499513 -0.0399794 ]]. Action = [[-0.22823511 -0.01736879  0.2049919   0.7693114 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.26501775 -0.0398882 ]]. Action = [[ 0.06131542  0.22381845 -0.09333092  0.83782244]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.26504016 -0.03989732]]. Action = [[ 0.00074866 -0.13961755  0.0495376   0.42762363]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.2651417  -0.03965448]]. Action = [[ 0.11761221 -0.14207886  0.15990028  0.990293  ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.26524305 -0.03951201]]. Action = [[ 0.12567472 -0.14507744 -0.1071102  -0.2983116 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.2652706  -0.03950853]]. Action = [[ 0.13407797  0.18534118 -0.23037276  0.1640439 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.2653524  -0.03954943]]. Action = [[-0.01071486 -0.13222943  0.03585708  0.8977425 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.2653374  -0.04076427]]. Action = [[ 0.24109876 -0.07911718  0.05866551  0.6098795 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.26576892 -0.04223615]]. Action = [[ 0.08790097  0.03981334  0.01235831 -0.9468848 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(7.4627e-05, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.26562226 -0.04214675]]. Action = [[ 0.0506531  -0.06834045  0.05193105  0.67636824]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.2653302  -0.04240794]]. Action = [[ 0.16154078  0.15689257  0.1981991  -0.717799  ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(8.7113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 195. State = [[-0.26467952 -0.04368318]]. Action = [[-0.10876697 -0.11237444 -0.19586392 -0.9147761 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.26452044 -0.04491025]]. Action = [[-0.14633404 -0.10826595  0.18931124  0.6870918 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.26450497 -0.04565457]]. Action = [[ 0.21750537 -0.15330984 -0.14791141 -0.05398029]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.26468697 -0.04653951]]. Action = [[-0.04459     0.20694196 -0.09415241 -0.66029495]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(9.3519e-05, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.26476863 -0.04686538]]. Action = [[-0.24123922  0.05512252 -0.22042082 -0.33827746]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.26491207 -0.04821112]]. Action = [[ 0.09903669 -0.17497413  0.07497877  0.41449225]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.2650946  -0.04892761]]. Action = [[-0.00601782 -0.19425711  0.19555783 -0.48110616]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.26525494 -0.04963201]]. Action = [[-0.10561922 -0.18586795 -0.18780123  0.3384328 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.26530665 -0.04994891]]. Action = [[-0.05339202  0.18602526  0.04330912  0.30633152]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.2654297  -0.05039357]]. Action = [[ 0.17925406  0.20310879 -0.20368735  0.86115265]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 205. State = [[-0.26578614 -0.05173041]]. Action = [[-0.06118196 -0.03131065  0.00556189  0.09648168]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.26693723 -0.05370381]]. Action = [[-0.05220646 -0.00344099 -0.00649413 -0.77104   ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.26861316 -0.05523667]]. Action = [[ 0.10489166  0.06832749 -0.21887153  0.7341076 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.26859036 -0.05502702]]. Action = [[ 0.02599007  0.14650846 -0.12329082 -0.9408045 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.26826507 -0.05476888]]. Action = [[-0.20559968 -0.03565326 -0.19614796 -0.9662132 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.26841822 -0.05462374]]. Action = [[-0.12388784 -0.07337341 -0.00530992 -0.35104567]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.26995865 -0.05614002]]. Action = [[ 0.06630641 -0.11023799 -0.07117198  0.4994769 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.26998657 -0.05755701]]. Action = [[ 0.10956031 -0.22490837 -0.17624383 -0.3839271 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 213. State = [[-0.26973715 -0.05823674]]. Action = [[ 0.17193449 -0.04255141  0.1803999  -0.01038367]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.26971498 -0.05884283]]. Action = [[ 0.19515544  0.02723256 -0.02743563 -0.36056924]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.27019942 -0.059252  ]]. Action = [[-0.16417557  0.2184149   0.19795722 -0.7995303 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 216. State = [[-0.2703874  -0.05977017]]. Action = [[-0.12677665  0.13405186  0.11465469  0.08332205]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.27014044 -0.06084921]]. Action = [[-0.18706742 -0.18752483 -0.11507039  0.10760665]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.27050093 -0.0612025 ]]. Action = [[-0.08808962  0.17308307  0.12898564 -0.19030452]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.2706958  -0.06231176]]. Action = [[-0.06788754  0.03764522 -0.10910654 -0.8145535 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.27090827 -0.06253097]]. Action = [[ 0.03405067 -0.22202866  0.09371635  0.19873762]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 221. State = [[-0.2711181  -0.06252658]]. Action = [[-0.1876726  -0.07265063  0.0910114  -0.7248481 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.271252   -0.06250802]]. Action = [[-0.11203176 -0.22636147 -0.09592815  0.8051076 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.27123633 -0.0626092 ]]. Action = [[-0.14780454  0.20717159 -0.00640193 -0.5169905 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of -1
Current timestep = 224. State = [[-0.27172178 -0.06258433]]. Action = [[-0.05430755  0.11030227 -0.15300824 -0.8698283 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.27288803 -0.06040214]]. Action = [[-0.09784427  0.10362709  0.22015256  0.12429988]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 226. State = [[-0.27508187 -0.05692778]]. Action = [[-0.1330712  -0.11473578 -0.02968189  0.04550564]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.2770581  -0.05737478]]. Action = [[ 0.08102754 -0.2060898   0.18701604 -0.23641568]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.28033575 -0.0576903 ]]. Action = [[-0.0254471  -0.06438188 -0.07779896 -0.86435443]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.2809726  -0.05830885]]. Action = [[-0.19390161  0.07138306 -0.23721991  0.22882974]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.28135034 -0.05862851]]. Action = [[ 0.16526616 -0.18908708 -0.08134739 -0.6198333 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.28249267 -0.06034713]]. Action = [[-0.00757308  0.01357219  0.00065309 -0.05261713]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.28263164 -0.06053301]]. Action = [[-0.22329041  0.13156652  0.13590181 -0.13208604]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 233. State = [[-0.2827353  -0.06068777]]. Action = [[-0.07871357  0.19815004  0.22437632  0.61414135]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.2833047  -0.06163551]]. Action = [[ 0.02862877 -0.12443212  0.10950521 -0.1764226 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.2834514  -0.06264648]]. Action = [[ 0.02073833 -0.14756373 -0.09845185 -0.75821245]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.2834856  -0.06363596]]. Action = [[-0.22722891 -0.2083007  -0.00066803 -0.18049091]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 237. State = [[-0.2835778  -0.06390212]]. Action = [[-0.17683524  0.15201545  0.05966511 -0.29230905]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.2838645  -0.06419433]]. Action = [[ 0.15176868  0.21635157 -0.19899721 -0.28581667]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.28458092 -0.06654688]]. Action = [[ 0.03950861  0.04953372 -0.01735589 -0.38518393]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 240. State = [[-0.28479245 -0.06684063]]. Action = [[ 0.05849198  0.12176895 -0.14940159  0.97298443]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 241. State = [[-0.28506738 -0.06456056]]. Action = [[ 0.01087043 -0.00346267 -0.0220063   0.26455712]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.2851     -0.06415227]]. Action = [[ 0.23180982  0.11381358 -0.14546432 -0.03904271]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 243. State = [[-0.28531918 -0.06284806]]. Action = [[-0.03534582  0.11473104 -0.08812448 -0.4710077 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.28600237 -0.05900645]]. Action = [[ 0.12765229  0.07236513  0.14598006 -0.5946067 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.28595307 -0.05778237]]. Action = [[-0.16681173  0.22211808 -0.19755447 -0.90906656]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 246. State = [[-0.28605747 -0.05664059]]. Action = [[-0.18266442 -0.00821987 -0.03405114 -0.23070204]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.2861665  -0.05336423]]. Action = [[-0.13174753  0.12678862 -0.04651329  0.32410908]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.2866275  -0.05070526]]. Action = [[ 0.00751477 -0.21042925  0.21361145 -0.9608115 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of -1
Current timestep = 249. State = [[-0.2868425  -0.04917308]]. Action = [[-0.15678574  0.20516968 -0.13737482 -0.3734551 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 250. State = [[-0.2870692  -0.04817421]]. Action = [[ 0.09333354 -0.17671798 -0.07019354 -0.37901014]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.2873721  -0.04725298]]. Action = [[-0.2378844  -0.10591781  0.22144303  0.8985537 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 252. State = [[-0.28759006 -0.04605765]]. Action = [[-0.18520322 -0.1392176   0.16028118 -0.4483477 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.28784063 -0.04487007]]. Action = [[-0.19295661 -0.23821975  0.12186924 -0.18213296]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.28820583 -0.0435956 ]]. Action = [[-0.20933431 -0.06711133  0.21986526  0.69883835]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 255. State = [[-0.28833175 -0.04278731]]. Action = [[ 0.23058277  0.10350859 -0.14268622  0.6417935 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 256. State = [[-0.28854176 -0.04196782]]. Action = [[-0.22543857 -0.19397765  0.08128709 -0.48478472]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.28875336 -0.04114874]]. Action = [[-0.1537036   0.19080159  0.19632226 -0.6248969 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.28886485 -0.04052565]]. Action = [[ 0.16404295 -0.1430943  -0.07680289 -0.72246027]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 259. State = [[-0.28907922 -0.03932343]]. Action = [[ 0.02160233  0.02638382 -0.17017008  0.45129633]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.28913438 -0.03886439]]. Action = [[-0.01587608  0.21289772  0.07805768  0.34210515]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.28931862 -0.03769262]]. Action = [[-0.01193178  0.05514657  0.09914264  0.12536561]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.2894005  -0.03719907]]. Action = [[ 0.17933443 -0.16881646  0.06326389  0.7050743 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.28947285 -0.03680123]]. Action = [[-0.19847226  0.0759784  -0.16339259  0.05133474]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.2895674  -0.03626491]]. Action = [[-0.24337143 -0.08314973  0.18314967 -0.8042827 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 265. State = [[-0.28975433 -0.03484769]]. Action = [[-0.04005107 -0.11113609  0.1731441   0.94882965]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.2897335  -0.03517328]]. Action = [[-0.16989136  0.15221721  0.1313659   0.7461796 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 267. State = [[-0.2896511  -0.03552443]]. Action = [[ 0.13942724  0.19952089 -0.20779414 -0.812001  ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.2893871  -0.03562942]]. Action = [[0.12608165 0.09817255 0.1010946  0.2280134 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 269. State = [[-0.28948143 -0.03518139]]. Action = [[-0.17537548 -0.0212632  -0.17674911 -0.48133755]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.2894438  -0.03480593]]. Action = [[ 0.1877948  -0.13238442  0.1704979  -0.19854116]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 271. State = [[-0.28849673 -0.03403468]]. Action = [[-0.07827175  0.00300479  0.11274877  0.02000141]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.28843608 -0.03386063]]. Action = [[-0.07589188 -0.12694108 -0.02422017 -0.9586157 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.28882673 -0.03564283]]. Action = [[-0.02445686 -0.09880871  0.18851936  0.8450651 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 274. State = [[-0.28894615 -0.03710705]]. Action = [[-0.06219746 -0.20676492  0.23709425 -0.48991185]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.28945604 -0.03983779]]. Action = [[-0.01338409  0.12499845  0.15430415 -0.62443864]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.28941825 -0.0392823 ]]. Action = [[-0.1994946   0.18595451  0.06419462 -0.7041477 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.28963298 -0.0385088 ]]. Action = [[ 0.1209541   0.08590627  0.13245642 -0.78255725]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 278. State = [[-0.28974217 -0.03798905]]. Action = [[ 0.20728129  0.12670743 -0.2250693   0.38874245]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.29002494 -0.03696764]]. Action = [[-0.09692429 -0.10481086 -0.14788938 -0.9640682 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 280. State = [[-0.2899363  -0.03754339]]. Action = [[ 0.10923016 -0.0565293  -0.24807338 -0.288781  ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 281. State = [[-0.28942308 -0.03870457]]. Action = [[-0.10653567  0.11850587 -0.21152233  0.38554096]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.2893201  -0.03830488]]. Action = [[ 0.1301862  -0.16173905  0.0047532   0.09933245]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.28960887 -0.0371333 ]]. Action = [[ 0.01650175  0.01386008 -0.1059633  -0.27800632]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 284. State = [[-0.28968105 -0.03697369]]. Action = [[-0.18155752 -0.2284646   0.13592726  0.6862178 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 285. State = [[-0.28995645 -0.03686292]]. Action = [[-0.15765691  0.1852262  -0.14502548  0.7493044 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.29006585 -0.03604779]]. Action = [[ 0.12861753  0.08082736 -0.09114116 -0.01056355]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.2900426 -0.0355058]]. Action = [[-0.03451416  0.21793497 -0.23554143  0.8681836 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.28993505 -0.03364954]]. Action = [[-0.03250714  0.08835036  0.24197337 -0.7915875 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 289. State = [[-0.29029748 -0.03235487]]. Action = [[ 0.05393961 -0.16485868  0.02001426  0.9211484 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.2904329  -0.03155345]]. Action = [[-0.07095136 -0.17712688 -0.0593632   0.34076238]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.2905298  -0.03087093]]. Action = [[ 0.03180447  0.2455644   0.10587966 -0.25839138]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 292. State = [[-0.290588   -0.03006308]]. Action = [[ 0.01075941  0.21491915 -0.18895605  0.6842482 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.2909269  -0.02871818]]. Action = [[ 0.20771712  0.08542541 -0.23676105  0.0496341 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.29133868 -0.02685861]]. Action = [[ 0.05336839  0.0510644   0.02991757 -0.09308708]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 295. State = [[-0.2913056  -0.02624485]]. Action = [[-0.22721902  0.19788438 -0.20985202 -0.46867263]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.29134673 -0.02563195]]. Action = [[-0.19981651 -0.04473209 -0.1558954   0.01959312]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.29143724 -0.02518013]]. Action = [[ 0.1495103   0.01564232 -0.20546392  0.36536002]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.29137692 -0.02343234]]. Action = [[-0.07497343  0.12554902 -0.15163992 -0.75122875]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.29174134 -0.02168561]]. Action = [[-0.00068696 -0.22606632 -0.19410408 -0.1733222 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 300. State = [[-0.2919462  -0.02039997]]. Action = [[-0.13277158  0.13700354  0.0249286  -0.7723397 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.29214263 -0.01902295]]. Action = [[ 0.21544355 -0.21397053  0.22981143 -0.8649565 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 302. State = [[-0.2924182 -0.0178777]]. Action = [[ 0.13905174 -0.1019876  -0.23164152  0.34617734]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.2927598  -0.01675746]]. Action = [[-0.1351809   0.01856846  0.22025806 -0.8292124 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.2929243 -0.0162455]]. Action = [[-0.2191307   0.10935506 -0.0281584   0.5792289 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.29305813 -0.01572106]]. Action = [[ 0.04012579 -0.16331713 -0.1848136  -0.5308003 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.29330564 -0.01475133]]. Action = [[ 0.24198198  0.23063779 -0.21704425  0.5575354 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 307. State = [[-0.29339054 -0.01414689]]. Action = [[ 0.14782494 -0.03553618  0.03817391  0.36468732]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 308. State = [[-0.2936027  -0.01286305]]. Action = [[-0.07388315  0.0817396  -0.09926426  0.29071033]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.29460955 -0.00937473]]. Action = [[ 0.12859333 -0.04108056  0.2055952  -0.6868867 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.29425105 -0.00895249]]. Action = [[-0.16763714  0.18046445  0.23651421  0.8651273 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 311. State = [[-0.2933459  -0.00850561]]. Action = [[-0.08913988 -0.10194281  0.22961897 -0.3177464 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.29322624 -0.00894671]]. Action = [[ 0.20607138  0.07849672 -0.05898313 -0.85144186]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.29342133 -0.00900866]]. Action = [[-0.2066341  -0.01366071 -0.248277   -0.56601346]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.29339263 -0.00910794]]. Action = [[-0.19164924  0.1416223  -0.10309288  0.69170046]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.29322627 -0.00934421]]. Action = [[ 0.20008951  0.12267476 -0.12596272  0.02900791]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.2931685  -0.00970038]]. Action = [[ 0.11360174  0.02376136 -0.24462286  0.3361981 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 317. State = [[-0.29217264 -0.00939184]]. Action = [[-0.0458236   0.02392042 -0.07076624  0.56672287]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 318. State = [[-0.29183328 -0.00929149]]. Action = [[-0.155189    0.04389441 -0.1939274   0.4052416 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.29169497 -0.00911623]]. Action = [[-0.05922127  0.05638939 -0.05763313 -0.49439025]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 320. State = [[-0.29213023 -0.00864178]]. Action = [[-0.16486152 -0.00257896 -0.05841045  0.31195068]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.29237306 -0.00838808]]. Action = [[ 0.1709359   0.21166939 -0.13874564 -0.9614784 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 322. State = [[-0.29214638 -0.00824156]]. Action = [[ 0.19714892 -0.11841567  0.11896271  0.8118248 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.29201904 -0.00814151]]. Action = [[-0.17459057 -0.10702686  0.05264366 -0.76870674]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 324. State = [[-0.29237154 -0.00797741]]. Action = [[ 0.18167073 -0.08460599 -0.09309644  0.53236985]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.2922792  -0.00782191]]. Action = [[-0.00483873  0.17496043 -0.17333984  0.43437195]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.29215208 -0.00775107]]. Action = [[-0.23587069 -0.21850093 -0.18212256 -0.4947306 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 327. State = [[-0.29226667 -0.00747276]]. Action = [[-0.1323821   0.03255436  0.18064052  0.95531964]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 328. State = [[-0.29243782 -0.0071054 ]]. Action = [[ 0.0952692  -0.16934235 -0.22773768  0.6138296 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.29303998 -0.00558385]]. Action = [[ 0.05092412  0.04626358 -0.01937793 -0.92398983]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.29323423 -0.00487842]]. Action = [[-0.21281523 -0.23992732  0.08573413 -0.83277243]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 331. State = [[-0.2933069  -0.00451235]]. Action = [[ 0.15898737 -0.14770436 -0.23220205 -0.6788803 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.29359114 -0.00390814]]. Action = [[-0.08510171 -0.1956541  -0.13794585 -0.34316266]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 333. State = [[-0.29409704 -0.00250387]]. Action = [[ 0.07331383 -0.05166355  0.04087594 -0.18672639]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.29398853 -0.00294907]]. Action = [[-0.07662095 -0.12728678 -0.18989885  0.8342885 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.2937663  -0.00389353]]. Action = [[ 0.07182166 -0.18939464  0.19886363  0.7540839 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 336. State = [[-0.29357252 -0.00489789]]. Action = [[ 0.20228681 -0.07847032 -0.12679255  0.50520563]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.2936094  -0.00511609]]. Action = [[-0.11052954  0.1834991   0.21091378 -0.19405937]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.2939155  -0.00543388]]. Action = [[-0.23366337 -0.19650498  0.11208689  0.39345193]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.2941499  -0.00716012]]. Action = [[-0.08439231  0.03236958 -0.11250907 -0.9508509 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.29443723 -0.00740696]]. Action = [[-0.15753429  0.16965905 -0.03169127 -0.43167794]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 341. State = [[-0.2948282  -0.00802277]]. Action = [[ 0.09986538 -0.05277927 -0.10278851  0.8486453 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.29487422 -0.00935829]]. Action = [[ 0.10216239 -0.0524188   0.03338388 -0.46909386]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.2939566  -0.01117534]]. Action = [[-0.10511237 -0.01204647  0.2008363  -0.81803465]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.29374775 -0.01154727]]. Action = [[-0.10956459  0.21163565 -0.14445092 -0.20146227]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 345. State = [[-0.29369882 -0.01185971]]. Action = [[ 0.2079885   0.05260906 -0.16188504  0.23330247]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 346. State = [[-0.29396683 -0.01226445]]. Action = [[-0.06777498  0.15964904 -0.02890389 -0.54886836]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.29422286 -0.01350841]]. Action = [[-0.07481974 -0.0128448  -0.09699422  0.19936323]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.29440224 -0.01459877]]. Action = [[ 0.09652928  0.05909932 -0.02981578 -0.8906849 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.29437792 -0.01453396]]. Action = [[ 0.22876373  0.0575476  -0.2310202   0.40423858]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 350. State = [[-0.29427278 -0.01446952]]. Action = [[0.16642362 0.16215897 0.01341048 0.8741987 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.29429838 -0.01448199]]. Action = [[ 0.11756712  0.24437976  0.03890932 -0.40248096]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 352. State = [[-0.29438797 -0.0143782 ]]. Action = [[-0.00226521  0.08875924 -0.2399243   0.13072145]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of -1
Current timestep = 353. State = [[-0.29490963 -0.01252193]]. Action = [[-0.09482215 -0.01560204  0.09706891  0.92161655]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.29497397 -0.01220316]]. Action = [[-0.19715148  0.05548757  0.15362555 -0.1788572 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.29503846 -0.0120876 ]]. Action = [[-0.22532025 -0.13318253  0.04149702  0.9452399 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.29506686 -0.01200259]]. Action = [[0.2139253  0.21258485 0.04451546 0.0369128 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 357. State = [[-0.2951142  -0.01183087]]. Action = [[0.18215314 0.04274946 0.18836963 0.8348794 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.29521734 -0.01153088]]. Action = [[-0.02798    -0.12909675 -0.00835072 -0.29880273]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.29572996 -0.01292609]]. Action = [[-0.02120037  0.11787054  0.16213873 -0.1510396 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 360. State = [[-0.29618824 -0.01215667]]. Action = [[-0.16779168  0.05934945 -0.12734279  0.29641342]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 361. State = [[-0.29617906 -0.01195969]]. Action = [[-2.1174550e-04 -1.5140927e-01  5.8380514e-02 -9.0099800e-01]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.29626682 -0.01181942]]. Action = [[-0.00556594 -0.237658   -0.06858099 -0.4241711 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 363. State = [[-0.29664528 -0.01166948]]. Action = [[-0.05318213 -0.08839762 -0.05212647  0.5128157 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.29747307 -0.01311415]]. Action = [[ 0.00549281 -0.046699   -0.09445009  0.77417016]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.2982558  -0.01528246]]. Action = [[ 0.00836095  0.04378456  0.1300838  -0.4603535 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.29848996 -0.01567662]]. Action = [[-0.04155633  0.05000138 -0.02172995  0.73965514]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.29862636 -0.01546411]]. Action = [[-0.22817974  0.2271348  -0.18097153  0.32244802]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 368. State = [[-0.29894695 -0.01517543]]. Action = [[ 0.20889273 -0.16210599  0.1636819  -0.23151857]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 369. State = [[-0.29912266 -0.01501395]]. Action = [[-0.16298865  0.07819113  0.06677866  0.8469262 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.2996653  -0.01414556]]. Action = [[ 0.0578948   0.09420878 -0.18638258  0.07158601]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.29982328 -0.01347416]]. Action = [[ 0.24179235  0.13727164  0.10097206 -0.7512872 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 372. State = [[-0.29997784 -0.01301272]]. Action = [[ 0.15239757  0.19071466 -0.11270204  0.20285618]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.30028462 -0.01229854]]. Action = [[-0.1001368  -0.19822259  0.2476291  -0.21740413]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.30050525 -0.0117236 ]]. Action = [[-0.20531099 -0.0611507  -0.20579956 -0.77725255]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.30064422 -0.01104274]]. Action = [[ 0.04323304  0.2053884  -0.05573964  0.07410538]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.30082548 -0.01070656]]. Action = [[-0.01354067 -0.15546133  0.06688851  0.4979781 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 377. State = [[-0.30132604 -0.00916297]]. Action = [[-0.08865565  0.12442797 -0.08285791 -0.63559884]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.30179566 -0.00801808]]. Action = [[-0.1391743  -0.23183852 -0.18076132 -0.7012771 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 379. State = [[-0.3022651 -0.0067976]]. Action = [[-0.05326568 -0.20631722  0.08981937  0.7142372 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.30278116 -0.0053753 ]]. Action = [[0.17535245 0.13810605 0.21209589 0.5135412 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 381. State = [[-0.30310914 -0.00445932]]. Action = [[0.21030599 0.194864   0.24392265 0.6203239 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 382. State = [[-0.30374804 -0.00287853]]. Action = [[-0.18513396 -0.08455092  0.11336419  0.74244976]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 383. State = [[-0.30465382 -0.00048334]]. Action = [[ 0.06938827  0.04560134  0.21050406 -0.33822453]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-3.0483213e-01  6.7867426e-05]]. Action = [[-0.13555534  0.01186502 -0.15183045 -0.01696032]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 385. State = [[-0.30501756  0.0005653 ]]. Action = [[-0.21457626 -0.0883036  -0.01924339  0.87624216]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 386. State = [[-0.3051318   0.00095145]]. Action = [[ 0.20237768  0.15771648 -0.17089795  0.1259507 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.3056364   0.00222194]]. Action = [[-0.11110201 -0.03950684  0.23471797 -0.8311753 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.30588567  0.00234349]]. Action = [[-0.15598047  0.00492758  0.22001249  0.9390867 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.30610397  0.0024132 ]]. Action = [[ 0.20595676  0.08334291 -0.00472055  0.08005166]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.30646384  0.00255427]]. Action = [[-0.04060988 -0.04304746 -0.17555773 -0.68432814]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.307595    0.00186714]]. Action = [[ 0.06825033  0.07380295 -0.24416286 -0.8088    ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.3077779   0.00210064]]. Action = [[-0.13703845 -0.13747813  0.12416059  0.39499593]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.30796438  0.00286627]]. Action = [[ 0.04700628 -0.10237771  0.04242906 -0.90181535]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.30776143  0.002308  ]]. Action = [[-0.18942496 -0.08239055  0.19831991 -0.83716536]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.3078075   0.00128217]]. Action = [[-0.11533907 -0.00957927 -0.13042134 -0.90061396]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-3.0882481e-01  8.4035215e-05]]. Action = [[-0.08601984 -0.07080334 -0.0577329  -0.8303958 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.3098357  -0.00078752]]. Action = [[-0.2021718  -0.21954119 -0.22003627  0.03634703]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 398. State = [[-0.3107702  -0.00134649]]. Action = [[ 0.09706986  0.18419802 -0.1690209   0.5319557 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.31116533 -0.00156285]]. Action = [[ 0.00747514 -0.16581032 -0.13704176  0.13480222]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 400. State = [[-0.3117493  -0.00189886]]. Action = [[-0.2196068   0.0444428   0.09480208 -0.36770868]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.3134384  -0.00261003]]. Action = [[ 0.16626352 -0.1784034   0.18933809 -0.70020705]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.31456038 -0.00308157]]. Action = [[ 0.06603581  0.243271    0.19015169 -0.28069675]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.31580994 -0.00352168]]. Action = [[ 0.05414137  0.22517812  0.14958149 -0.46659923]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 404. State = [[-0.31660387 -0.00384707]]. Action = [[-0.17585607 -0.01765347 -0.20721823  0.97330666]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.31825346 -0.00476242]]. Action = [[ 0.11359128  0.10155421  0.24222612 -0.68382895]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.31837136 -0.00400723]]. Action = [[-0.10706782 -0.02690928  0.21324113  0.40138674]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.31866676 -0.00381735]]. Action = [[-0.17057778 -0.0428555  -0.1781525  -0.27895236]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.3189387  -0.00376011]]. Action = [[-0.22761542  0.14453489 -0.06709239  0.86425185]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 409. State = [[-0.3189962  -0.00370685]]. Action = [[-0.22618893  0.07309759 -0.13409595  0.31413698]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 410. State = [[-0.3192578  -0.00349475]]. Action = [[ 0.05319121  0.15103936 -0.09335499  0.0990864 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.31949165 -0.00341464]]. Action = [[ 0.18095583  0.04637164 -0.11187497  0.33218622]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 412. State = [[-0.31984934 -0.00338211]]. Action = [[ 0.24482054 -0.06709623  0.1518346  -0.5008903 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.3201083  -0.00344638]]. Action = [[ 0.21419609 -0.1992777   0.08366728 -0.7944258 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.32010397 -0.00334803]]. Action = [[-0.229565   -0.04300405 -0.18136346 -0.03930473]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.32033455 -0.00322395]]. Action = [[-0.1773584   0.18856451 -0.24252042 -0.6231551 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.32062063 -0.00316984]]. Action = [[ 0.14172632 -0.22818919 -0.06856266 -0.8105044 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.3206375  -0.00314397]]. Action = [[ 0.16453815  0.14297506  0.02858409 -0.9146286 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.32079884 -0.00315684]]. Action = [[-0.12186201 -0.2309439  -0.14459348 -0.77045304]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.32089263 -0.00316978]]. Action = [[ 0.19377798  0.13776374 -0.01380061 -0.42224193]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.32101193 -0.0031969 ]]. Action = [[-0.12012744 -0.08794311 -0.04230314 -0.48013818]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.3218468  -0.00372891]]. Action = [[-0.05586889 -0.23322645  0.15010685 -0.7061692 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.3226905  -0.00434522]]. Action = [[ 0.20287967 -0.06037764  0.07052037  0.67073846]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.32305297 -0.00458088]]. Action = [[ 0.00794357 -0.20995265 -0.19566163 -0.22635889]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 424. State = [[-0.32401374 -0.0052378 ]]. Action = [[ 0.22630477  0.05655167  0.03298509 -0.46751976]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 425. State = [[-0.32429594 -0.00567643]]. Action = [[-0.18296637 -0.02227387 -0.04879597 -0.82339543]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of -1
Current timestep = 426. State = [[-0.32454157 -0.00600562]]. Action = [[-0.18672395  0.17760459 -0.19822356  0.8294027 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.32501373 -0.00637021]]. Action = [[-0.23868714 -0.0347563   0.17193097  0.09126675]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.32558495 -0.00707329]]. Action = [[0.0724391  0.00458881 0.07719496 0.4143293 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.32548735 -0.0071081 ]]. Action = [[ 0.24170119  0.09327585 -0.13067849  0.11791718]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.32534054 -0.00748718]]. Action = [[ 0.09346211 -0.07108332  0.14686716  0.25075388]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of -1
Current timestep = 431. State = [[-0.32498616 -0.00804649]]. Action = [[-0.14865197 -0.13508399  0.02005291 -0.6699609 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 432. State = [[-0.3247492  -0.00831551]]. Action = [[0.03819746 0.20820194 0.2225157  0.04717922]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.3244845  -0.00854085]]. Action = [[ 0.20899346  0.23136938 -0.1510458  -0.1796524 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.32443252 -0.00875107]]. Action = [[ 0.1063925   0.1587671  -0.02625534 -0.11265647]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 435. State = [[-0.32398534 -0.00907147]]. Action = [[-0.2151205  -0.05418646 -0.18995903  0.7615404 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.3239966 -0.0093166]]. Action = [[0.17156488 0.07261825 0.19058496 0.33834815]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of -1
Current timestep = 437. State = [[-0.32399282 -0.0094751 ]]. Action = [[ 0.21918696  0.13886923 -0.23102683  0.21404505]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.32367843 -0.00960837]]. Action = [[-0.15776733  0.23433018 -0.01926276 -0.9001404 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 439. State = [[-0.3237699 -0.0097657]]. Action = [[-0.2169627   0.19172975 -0.03223211  0.04369569]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.32379082 -0.00982493]]. Action = [[-0.18387185  0.06848997 -0.06891251  0.34638965]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.32363296 -0.00989846]]. Action = [[ 0.06877574  0.22195062  0.19990829 -0.28568655]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.3236223  -0.00999035]]. Action = [[-0.1755252   0.10084289 -0.10552889  0.00062656]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 443. State = [[-0.32361946 -0.01013388]]. Action = [[-0.06208415 -0.0402955  -0.01019451  0.41666138]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.32378864 -0.01136575]]. Action = [[ 0.05939192  0.03319129 -0.09886159 -0.29605162]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.3237743  -0.01140548]]. Action = [[-0.19515343  0.205785    0.1198507   0.8988304 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 446. State = [[-0.3235654  -0.01136008]]. Action = [[ 0.04136333 -0.12837702 -0.0344215  -0.44719982]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.3226596  -0.01441879]]. Action = [[0.04651684 0.1091485  0.15673041 0.68144095]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.32253167 -0.01429653]]. Action = [[ 0.13864124  0.19757941  0.08814609 -0.14176798]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 449. State = [[-0.32236928 -0.01408573]]. Action = [[0.18782926 0.0695788  0.07544783 0.67418194]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 450. State = [[-0.32178998 -0.01408542]]. Action = [[-0.09216209 -0.08394763 -0.21074817 -0.9179042 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of -1
Current timestep = 451. State = [[-0.3218748 -0.0161871]]. Action = [[ 0.07053608 -0.07636267 -0.07410777 -0.2711488 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 452. State = [[-0.321345   -0.01713184]]. Action = [[ 0.1494242   0.06231615 -0.19866395 -0.2579565 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 453. State = [[-0.32072365 -0.01794935]]. Action = [[-0.23047249  0.19175711 -0.03762864  0.37006664]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.32091808 -0.01847289]]. Action = [[-0.0580454  -0.22785206 -0.16034822  0.5304806 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.32116115 -0.01919284]]. Action = [[ 0.14303184  0.04070115 -0.08787131  0.5355303 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 456. State = [[-0.3202779  -0.02108702]]. Action = [[-0.04590587  0.05271366  0.07045069 -0.24103945]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.3202107  -0.02102114]]. Action = [[ 0.09011537  0.12064171 -0.18739243  0.8258569 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.32049087 -0.01946215]]. Action = [[-0.01155721  0.02360508 -0.13521448  0.72059226]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 459. State = [[-0.32076633 -0.01912405]]. Action = [[ 0.19962797  0.20677531 -0.15849799 -0.12443489]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 460. State = [[-0.3209448  -0.01902713]]. Action = [[-0.15114978 -0.17065221  0.03968251  0.43557322]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.32097182 -0.01875731]]. Action = [[-0.22722988 -0.09267297  0.02231926 -0.61681545]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 462. State = [[-0.32080588 -0.01858433]]. Action = [[-0.15612523 -0.23383398 -0.09949577  0.5736692 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of 1
Current timestep = 463. State = [[-0.32091525 -0.01776025]]. Action = [[-0.11178017  0.09138328  0.1990811   0.3028351 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 464. State = [[-0.32117456 -0.01694348]]. Action = [[ 0.19205812  0.09932274 -0.21517532 -0.36351013]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of 1
Current timestep = 465. State = [[-0.32162583 -0.01595323]]. Action = [[ 0.23831308 -0.03751662  0.19892594 -0.2691989 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.32189703 -0.01520104]]. Action = [[ 0.24597126 -0.07547706 -0.08219275  0.83753204]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.3228866  -0.01213305]]. Action = [[-0.09097892  0.09523466 -0.03739381 -0.06187665]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 467 of 1
Current timestep = 468. State = [[-0.32342067 -0.01100114]]. Action = [[-0.02458516 -0.1593252   0.04630893 -0.8537679 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.324024   -0.00971179]]. Action = [[0.13802499 0.10730547 0.21872866 0.02336323]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 470. State = [[-0.32425287 -0.00883728]]. Action = [[ 0.18556437  0.1099596  -0.01352368 -0.35089874]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.3248759  -0.00731781]]. Action = [[ 0.00478277 -0.15214278 -0.12387022 -0.29916686]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.32536197 -0.00653123]]. Action = [[ 0.16955435  0.01556656 -0.18012825  0.93503976]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 473. State = [[-0.3254361  -0.00577193]]. Action = [[-0.20125607 -0.02815191 -0.06154004 -0.69592714]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.3257321  -0.00503864]]. Action = [[ 0.19577971 -0.03344744 -0.05808181  0.5276848 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 475. State = [[-0.3260772  -0.00445597]]. Action = [[ 0.22479707  0.22052598 -0.00483955  0.91996646]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.326226   -0.00374251]]. Action = [[-0.05717115 -0.20188275 -0.22964332 -0.3203038 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.326713   -0.00271857]]. Action = [[-0.0252187  -0.05401583 -0.07834317 -0.5551627 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.32703486 -0.00299583]]. Action = [[ 0.03894627 -0.0806815  -0.14546593 -0.21843547]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.32722637 -0.00481976]]. Action = [[-0.01212727 -0.00275004  0.11300865  0.2240833 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.3273124 -0.0054256]]. Action = [[-0.00978279  0.1073252  -0.11522901  0.5180237 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.32748654 -0.00508151]]. Action = [[-0.17607069  0.07464331 -0.07966629  0.9384742 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.3275642  -0.00491645]]. Action = [[ 0.20456314  0.0958764  -0.06692845 -0.5619321 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 483. State = [[-0.32768095 -0.00467727]]. Action = [[ 0.23512638  0.22662672 -0.23113044 -0.87582487]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 484. State = [[-0.3276836  -0.00460364]]. Action = [[-0.18645987 -0.0942643  -0.2031263   0.14448488]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 485. State = [[-0.32778138 -0.00427764]]. Action = [[-0.05965021 -0.01289837 -0.15401232  0.9541545 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.3287524  -0.00402621]]. Action = [[ 0.09115458 -0.06208581  0.11646771  0.46197438]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.32870203 -0.00416105]]. Action = [[-0.22428682 -0.1655834   0.00789258  0.957294  ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 488. State = [[-0.32857853 -0.00422771]]. Action = [[ 0.21841353 -0.12268522  0.02543887  0.6844827 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 489. State = [[-0.32823738 -0.00458313]]. Action = [[ 0.01289442 -0.12512252  0.12132099  0.5871613 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[-0.32774687 -0.00593055]]. Action = [[ 0.05682182  0.16762787 -0.14665882 -0.6519132 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.32767856 -0.00653071]]. Action = [[ 0.19348747  0.1692766  -0.0789499   0.8689687 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.32754016 -0.0072127 ]]. Action = [[-0.18419412 -0.2071609  -0.23129863  0.51259947]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[-0.32749802 -0.00785372]]. Action = [[-0.21300128 -0.12655893  0.07336274  0.55506253]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.32726654 -0.00986225]]. Action = [[ 0.04064363 -0.09580058  0.05822119  0.23806524]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 495. State = [[-0.3268326  -0.01086299]]. Action = [[ 0.13272971  0.17575395  0.12458885 -0.9575911 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.32657695 -0.01198429]]. Action = [[-0.16183409 -0.03367467 -0.16176699  0.06515336]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.32667086 -0.01307507]]. Action = [[-0.1946645  -0.15916708  0.21007329 -0.60856766]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.3265398  -0.01377792]]. Action = [[0.11634436 0.18609619 0.01259953 0.29436946]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 499. State = [[-0.32639602 -0.01455123]]. Action = [[-0.05284396  0.2189385  -0.17429695  0.6546397 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.32654807 -0.01539008]]. Action = [[-0.23578975 -0.15402292 -0.02768625 -0.0165903 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 501. State = [[-0.32651615 -0.01612317]]. Action = [[-0.23299287  0.16234034  0.2453846   0.28438354]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.3266966  -0.01798843]]. Action = [[-0.10567167 -0.12796138 -0.02121572  0.9620905 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of 1
Current timestep = 503. State = [[-0.32775468 -0.02207787]]. Action = [[ 0.00554332  0.10850465 -0.09325388 -0.05887502]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of 1
Current timestep = 504. State = [[-0.3278737  -0.02229198]]. Action = [[ 0.19406259  0.00558132 -0.06757301  0.8951161 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.327999   -0.02214614]]. Action = [[-0.06291744  0.16391802  0.19604418  0.19355321]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.3282322  -0.02231974]]. Action = [[-0.20600346  0.19301522  0.22266793  0.82203555]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of 1
Current timestep = 507. State = [[-0.32827023 -0.0223837 ]]. Action = [[ 0.21332276  0.20633927 -0.18758431  0.00316691]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 508. State = [[-0.32848868 -0.02225366]]. Action = [[ 0.23816049  0.08446729 -0.10217464  0.73238444]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 509. State = [[-0.3284713  -0.02229532]]. Action = [[0.16656771 0.02192214 0.1156925  0.02705312]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 510. State = [[-0.32844502 -0.02237344]]. Action = [[0.18070206 0.19742548 0.19586211 0.18253279]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of 1
Current timestep = 511. State = [[-0.32852578 -0.02233119]]. Action = [[ 0.24727625 -0.22217397 -0.14609282  0.16835225]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 512. State = [[-0.32852578 -0.02233119]]. Action = [[-0.22819643 -0.19835825 -0.20465304 -0.35487425]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.32845822 -0.0223342 ]]. Action = [[ 0.17364869 -0.10197979  0.09224084 -0.12139308]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 514. State = [[-0.32860425 -0.02211355]]. Action = [[ 0.00694266  0.11789531  0.10652927 -0.06325209]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of 1
Current timestep = 515. State = [[-0.3286671  -0.02143679]]. Action = [[-0.24023011 -0.16971867 -0.17301314 -0.3264848 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 515 of 1
Current timestep = 516. State = [[-0.32879832 -0.02081372]]. Action = [[-0.18111616  0.04163846  0.24131942  0.0689702 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 517. State = [[-0.32886818 -0.02061862]]. Action = [[-0.21977724 -0.24382919 -0.12490614  0.6561694 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.32892117 -0.0202792 ]]. Action = [[-0.18610105 -0.21435076  0.0211812   0.01780462]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.3289339  -0.02000758]]. Action = [[ 0.22292227  0.14405346 -0.2048154  -0.17983794]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 520. State = [[-0.32901758 -0.01990544]]. Action = [[ 0.21054256  0.09748062  0.02299368 -0.32877862]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 521. State = [[-0.32901776 -0.01972172]]. Action = [[ 0.14312547  0.04124516 -0.00064871  0.21885419]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of 1
Current timestep = 522. State = [[-0.32899934 -0.01947365]]. Action = [[-0.17640188 -0.14832234  0.11153004  0.55302906]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.32898134 -0.01937021]]. Action = [[ 0.03632438 -0.17846453 -0.0444358  -0.48490477]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 524. State = [[-0.3289859  -0.01927964]]. Action = [[0.17215192 0.13415805 0.04912212 0.8910947 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 525. State = [[-0.32899928 -0.01924082]]. Action = [[-0.17112875 -0.23420785  0.20913112  0.28596425]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 526. State = [[-0.3290077 -0.0190562]]. Action = [[ 0.21334195 -0.14211446  0.21442339 -0.5236549 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 527. State = [[-0.32901663 -0.01887509]]. Action = [[ 0.23351195  0.16967079  0.1569277  -0.139925  ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.3290032  -0.01891391]]. Action = [[ 0.19965184 -0.08129051 -0.24664673 -0.93704957]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.32899877 -0.01877161]]. Action = [[-0.03005986 -0.17852914  0.18204308  0.99039733]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.32898983 -0.01871987]]. Action = [[-0.20203674  0.02007079  0.10163146  0.55189526]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.32898098 -0.01866847]]. Action = [[-0.16516428 -0.04118742  0.04004034  0.84477544]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.32899022 -0.01848697]]. Action = [[-0.11153796 -0.20729344  0.05959949  0.7163851 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of -1
Current timestep = 533. State = [[-0.32909366 -0.01792777]]. Action = [[-0.08722633  0.1117349  -0.06130123 -0.7583391 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.3297984 -0.0164514]]. Action = [[ 0.19816834 -0.2297548  -0.2356794   0.73668814]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 535. State = [[-0.3309975  -0.01385946]]. Action = [[ 0.09754032 -0.08775339  0.24748862 -0.7890935 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 535 of -1
Current timestep = 536. State = [[-0.33069634 -0.01431245]]. Action = [[ 0.00447109 -0.11598018 -0.18774164 -0.2906729 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.3306256  -0.01515473]]. Action = [[ 0.20825204 -0.2275117  -0.11893976 -0.05064982]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 538. State = [[-0.33048227 -0.01558604]]. Action = [[ 0.24387485 -0.17765816  0.14729851 -0.7466832 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 539. State = [[-0.33029673 -0.01600522]]. Action = [[ 0.20348066  0.20969027  0.11780199 -0.61885434]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.33044508 -0.01636039]]. Action = [[ 0.2221232   0.24311835  0.02244481 -0.5925411 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 540 of -1
Current timestep = 541. State = [[-0.33049235 -0.01672003]]. Action = [[-0.23654066  0.04228032  0.01784274 -0.44953287]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.33029494 -0.01761282]]. Action = [[-0.07210387  0.02993307  0.09763256  0.8844061 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of -1
Current timestep = 543. State = [[-0.33051983 -0.01758632]]. Action = [[-0.24652278  0.08800551  0.18687314  0.01444864]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.33067688 -0.01768245]]. Action = [[ 0.07509637 -0.02370679 -0.01708502  0.6933905 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(8.3526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of -1
Current timestep = 545. State = [[-0.33063662 -0.01771127]]. Action = [[-0.06596287 -0.21879765 -0.08237538  0.8531647 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 546. State = [[-0.33049688 -0.01783865]]. Action = [[-0.24231304 -0.09221721  0.10293263  0.43050897]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.33050567 -0.01789035]]. Action = [[ 0.16442776 -0.20209508 -0.22946332 -0.4884516 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.33067605 -0.01791516]]. Action = [[ 0.05628803 -0.1796862   0.16061947 -0.80273837]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of -1
Current timestep = 549. State = [[-0.33068976 -0.01805318]]. Action = [[-0.09636778 -0.01698537  0.04780072  0.5186975 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.3309134  -0.01834956]]. Action = [[-0.19015455 -0.01911016 -0.1429114  -0.60660064]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 551. State = [[-0.33103853 -0.01842169]]. Action = [[-0.07224408 -0.24082553  0.14807469  0.45119584]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(9.1247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 552. State = [[-0.3309355  -0.01852319]]. Action = [[-0.15647167 -0.06631976  0.18790272  0.71981966]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.3309041  -0.01860364]]. Action = [[-0.23281606  0.09339073 -0.23465334  0.43312418]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 554. State = [[-0.3311334  -0.01872171]]. Action = [[ 0.09838071 -0.2261729   0.05997396  0.2984897 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(7.1576e-05, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.33115122 -0.01882494]]. Action = [[ 0.01583141  0.22797573 -0.03897524 -0.21559769]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of -1
Current timestep = 556. State = [[-0.33110183 -0.01896306]]. Action = [[-0.21474075  0.09341502  0.2226435  -0.8352651 ]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.33132213 -0.01918704]]. Action = [[ 0.01126969 -0.1439407  -0.03542759  0.54175544]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(4.5539e-05, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.33141747 -0.01948671]]. Action = [[-0.09551525 -0.03642546 -0.15106817  0.5141206 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.33231217 -0.02077034]]. Action = [[ 0.10838404 -0.08011021  0.17500547 -0.84693396]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of -1
Current timestep = 560. State = [[-0.33222857 -0.02177487]]. Action = [[ 0.19081968 -0.11322358 -0.1362825   0.8071065 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(8.9501e-05, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.3321967  -0.02222514]]. Action = [[-0.15296763  0.07092774 -0.15462092  0.15223098]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 562. State = [[-0.33222908 -0.02284223]]. Action = [[ 0.160088   -0.10141438  0.0098891   0.19621384]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.332405  -0.0247299]]. Action = [[0.04623491 0.02315956 0.13945806 0.03583157]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.33232912 -0.02501835]]. Action = [[ 0.14867285  0.02982953 -0.15037934 -0.816619  ]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.332326   -0.02510853]]. Action = [[ 0.09572002 -0.23366101 -0.03927237 -0.31336462]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.3323132  -0.02514702]]. Action = [[ 0.24205607  0.20004666  0.04165664 -0.04106653]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.3322789  -0.02526827]]. Action = [[0.20528683 0.24859005 0.0881916  0.8931577 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(6.9668e-05, grad_fn=<MseLossBackward0>)
Current timestep = 568. State = [[-0.33225158 -0.02548408]]. Action = [[ 0.21610093  0.05751765 -0.22763279 -0.53988266]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.3322162  -0.02551045]]. Action = [[-0.19920461  0.2277588   0.17150325  0.34851146]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 569 of -1
Current timestep = 570. State = [[-0.33219492 -0.02559257]]. Action = [[-0.24701327 -0.17589848  0.03015158  0.95426655]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.33224708 -0.02575938]]. Action = [[ 0.0276528  -0.14532414 -0.07015955 -0.72333753]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 572. State = [[-0.33223858 -0.02575706]]. Action = [[ 0.18741632  0.1367997  -0.2130586  -0.9174184 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 573. State = [[-0.33226678 -0.02586167]]. Action = [[-0.24897149 -0.1223644  -0.14479631 -0.4288714 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.3323143  -0.02580886]]. Action = [[-0.12715732  0.12488109  0.03170663 -0.75963193]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 575. State = [[-0.33257642 -0.02510875]]. Action = [[ 0.00853708  0.21190315  0.05224115 -0.03967321]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of -1
Current timestep = 576. State = [[-0.33311054 -0.0236362 ]]. Action = [[-0.00791301  0.12814254  0.20007342  0.85397005]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(7.8923e-05, grad_fn=<MseLossBackward0>)
Current timestep = 577. State = [[-0.33455065 -0.01907158]]. Action = [[-0.10181114  0.09399828 -0.06466487  0.34867346]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, True, False]
State prediction error at timestep 577 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 578. State = [[-0.33515078 -0.01767055]]. Action = [[-0.2355741  -0.09114113 -0.19495392  0.61135745]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 579. State = [[-0.3357852  -0.01644816]]. Action = [[-0.12253851  0.14905041  0.137142   -0.5656135 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, True, False]
State prediction error at timestep 579 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 579 of -1
Current timestep = 580. State = [[-0.33613855 -0.01547306]]. Action = [[ 0.1507377  -0.18054222  0.21039844  0.13240314]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 581. State = [[-0.33666518 -0.01389152]]. Action = [[-0.05374011  0.20396283 -0.12285167 -0.96897364]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 582. State = [[-0.33744842 -0.01252501]]. Action = [[-0.2214568   0.1632849   0.05214459 -0.51682836]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, True, False]
State prediction error at timestep 582 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 583. State = [[-0.33758754 -0.01179703]]. Action = [[0.07666296 0.14111874 0.22096437 0.11690676]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.3387209  -0.00922915]]. Action = [[ 0.04941547 -0.00785635  0.12914097  0.7678875 ]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(4.9672e-05, grad_fn=<MseLossBackward0>)
Current timestep = 585. State = [[-0.3389178  -0.00906626]]. Action = [[-0.23564294  0.11265981 -0.18250129  0.8123579 ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.33890808 -0.00891479]]. Action = [[ 0.18187472 -0.20951347  0.19153434 -0.81702805]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 587. State = [[-0.3389468  -0.00866252]]. Action = [[ 0.1657005  -0.16699761 -0.22726335 -0.25158107]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, True, False]
State prediction error at timestep 587 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 588. State = [[-0.33898014 -0.00858398]]. Action = [[-0.01565173 -0.22571489  0.14923984  0.84154356]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, True, False]
State prediction error at timestep 588 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 589. State = [[-0.3390237  -0.00847024]]. Action = [[-0.06050213  0.22372553  0.06085253 -0.2972414 ]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.3390482  -0.00825579]]. Action = [[-0.2178572   0.17819399 -0.21490017 -0.70992035]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, True, False]
State prediction error at timestep 590 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 591. State = [[-0.33912084 -0.00792503]]. Action = [[ 0.0994567  -0.02415532  0.16533864 -0.76283336]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.33909437 -0.00791404]]. Action = [[ 0.1103386  -0.224913    0.14255458 -0.83365583]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.33908722 -0.00800349]]. Action = [[-0.08153063 -0.05768582 -0.12551345  0.1275096 ]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, True, False]
State prediction error at timestep 593 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 594. State = [[-0.33906543 -0.00813108]]. Action = [[0.1301257  0.16602102 0.01900169 0.90963936]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, True, False]
State prediction error at timestep 594 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 595. State = [[-0.33905834 -0.00822053]]. Action = [[-0.11162552 -0.22597101 -0.14301054 -0.71783346]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, True, False]
State prediction error at timestep 595 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 596. State = [[-0.33908027 -0.0082344 ]]. Action = [[-0.19127929 -0.02448253 -0.18124558  0.8274797 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 597. State = [[-0.33909512 -0.00833736]]. Action = [[-0.18896256 -0.2256481  -0.24222425 -0.12917596]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, True, False]
State prediction error at timestep 597 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 598. State = [[-0.3390806  -0.00837514]]. Action = [[-0.22285528  0.22021422 -0.13221662 -0.23794448]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.33911046 -0.00858174]]. Action = [[-0.00740193  0.0123753   0.07100466 -0.48279536]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 600. State = [[-0.33913988 -0.0086469 ]]. Action = [[ 0.08010542 -0.13988426 -0.13838668 -0.87956697]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.3391254  -0.00868468]]. Action = [[-0.22634554  0.06286734 -0.02961691 -0.71338993]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 602. State = [[-0.33911794 -0.00863339]]. Action = [[-0.24783985 -0.24305524  0.00911191 -0.6882641 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 603. State = [[-0.3391254  -0.00868468]]. Action = [[ 0.12987313  0.19300604 -0.04451619  0.23773599]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 604. State = [[-0.3391254  -0.00868468]]. Action = [[ 0.17428708  0.15878081  0.19015425 -0.21501815]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.33913124 -0.00882278]]. Action = [[ 0.08068609 -0.08697039 -0.07799011  0.79950106]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(2.0635e-05, grad_fn=<MseLossBackward0>)
Current timestep = 606. State = [[-0.33881688 -0.00938993]]. Action = [[-0.04095729  0.18030617 -0.0171636   0.46988893]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(9.1783e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.33836597 -0.01003732]]. Action = [[ 0.13111025 -0.2360176   0.18653661  0.0206691 ]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 608. State = [[-0.3383257  -0.01021915]]. Action = [[ 0.0936242  -0.17707472  0.09679809 -0.21136224]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.3383775  -0.01048883]]. Action = [[-0.07641429  0.19246233  0.14956382  0.9379238 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.3379562  -0.01162575]]. Action = [[-0.09344861  0.01667729 -0.05295794 -0.8760998 ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.33806333 -0.01185343]]. Action = [[-0.10290024  0.11366168  0.14210886 -0.53009826]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.3393954 -0.0096325]]. Action = [[ 0.09755912 -0.11658564  0.13148907  0.8815286 ]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, True, False]
State prediction error at timestep 612 is tensor(1.9730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.33951303 -0.01028931]]. Action = [[0.17393959 0.2087068  0.14671436 0.12798941]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 614. State = [[-0.33923686 -0.01078926]]. Action = [[ 0.24812675 -0.1321127   0.08640435 -0.8874134 ]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, True, False]
State prediction error at timestep 614 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 615. State = [[-0.33909845 -0.01075214]]. Action = [[-0.08584747 -0.22010759  0.19339094 -0.8133777 ]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.3394131  -0.01092289]]. Action = [[ 0.19051397 -0.22936803  0.12463909  0.81679034]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, True, False]
State prediction error at timestep 616 is tensor(4.6376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 617. State = [[-0.33914813 -0.01116069]]. Action = [[0.1453841  0.21245062 0.17097354 0.4299966 ]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 618. State = [[-0.3391987  -0.01109821]]. Action = [[ 0.00596744 -0.21664491  0.03017989  0.2686354 ]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, True, False]
State prediction error at timestep 618 is tensor(1.9474e-05, grad_fn=<MseLossBackward0>)
Current timestep = 619. State = [[-0.33942962 -0.01121531]]. Action = [[-0.18310131 -0.10971195 -0.11368036  0.92851126]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of -1
Current timestep = 620. State = [[-0.33935145 -0.01130854]]. Action = [[-0.04442602  0.00259432 -0.23875497  0.4010775 ]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(6.2738e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of -1
Current timestep = 621. State = [[-0.33930993 -0.01135234]]. Action = [[-0.06426388 -0.1478594  -0.12390426 -0.54819113]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.33941272 -0.01140049]]. Action = [[-0.05927479 -0.19366942  0.2278018   0.1830653 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(5.0690e-05, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.33954817 -0.011613  ]]. Action = [[-0.12480758 -0.00972763  0.2025184   0.1238153 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.3399679  -0.01176776]]. Action = [[ 0.23976475 -0.22571725 -0.08894062  0.34418654]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(1.3063e-05, grad_fn=<MseLossBackward0>)
Current timestep = 625. State = [[-0.34038356 -0.01193826]]. Action = [[0.0929707  0.21714914 0.06437039 0.77809215]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(9.9985e-06, grad_fn=<MseLossBackward0>)
Current timestep = 626. State = [[-0.3411797  -0.01199351]]. Action = [[ 0.04839903  0.06153151  0.16313678 -0.13365221]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.3410668  -0.01161764]]. Action = [[-0.22381993 -0.0050704   0.02503875  0.8467127 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(7.6432e-05, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.34118983 -0.01150631]]. Action = [[-0.21531355 -0.13562457 -0.09953232  0.33040953]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 629. State = [[-0.34138653 -0.01157709]]. Action = [[ 0.0407798   0.15085897 -0.00139464 -0.21247113]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.34129384 -0.01127122]]. Action = [[-0.09511727 -0.22196019 -0.2067786   0.9728508 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 631. State = [[-0.34154502 -0.01110834]]. Action = [[-0.10351594 -0.07250611  0.16248488  0.87696576]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(6.7801e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of -1
Current timestep = 632. State = [[-0.3420097  -0.01152854]]. Action = [[-0.20569673  0.00965494 -0.10997352  0.46435595]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 633. State = [[-0.34325325 -0.01207471]]. Action = [[-0.02844895 -0.05033764 -0.09553826  0.45912862]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(1.6740e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.34366256 -0.01242413]]. Action = [[-0.15698318  0.0314917   0.11338624 -0.09126318]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.34401163 -0.01262564]]. Action = [[ 0.2264497   0.20602787  0.117616   -0.788342  ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.3443599  -0.01279836]]. Action = [[ 0.11597669 -0.17868656 -0.22879156 -0.8040316 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.34475976 -0.0129843 ]]. Action = [[-0.16380133  0.04915681  0.00694942  0.81788445]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(6.0904e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[-0.34565476 -0.01328503]]. Action = [[0.16437265 0.15401518 0.0403012  0.74535847]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(1.2975e-05, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.34596992 -0.01340689]]. Action = [[ 0.10459572  0.21094084  0.09635445 -0.91427284]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 640. State = [[-0.34657526 -0.01346809]]. Action = [[-0.20161077 -0.21392348  0.18874079  0.54295135]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(5.9264e-05, grad_fn=<MseLossBackward0>)
Current timestep = 641. State = [[-0.3468251  -0.01345226]]. Action = [[-0.20749152  0.13963968 -0.18410526 -0.08238292]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of -1
Current timestep = 642. State = [[-0.34725562 -0.01348899]]. Action = [[ 0.22905764  0.23205787  0.23811406 -0.88575935]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 643. State = [[-0.34782106 -0.01354471]]. Action = [[ 0.19254088 -0.15452403 -0.10375839  0.8861419 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(3.3941e-05, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.3487381  -0.01367752]]. Action = [[ 0.13243744 -0.02322173 -0.09316839  0.9573028 ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, True, False]
State prediction error at timestep 644 is tensor(7.3878e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[-0.3483417  -0.01439504]]. Action = [[-0.16128781 -0.12526402 -0.06895576  0.9011506 ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 646. State = [[-0.34810847 -0.01470441]]. Action = [[-0.23708716  0.02807668  0.19021755  0.8964126 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(8.2250e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[-0.34781387 -0.0151854 ]]. Action = [[-0.1251056   0.02925661 -0.11227638 -0.2257241 ]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 648. State = [[-0.34832218 -0.01518357]]. Action = [[-0.246615    0.11077768  0.18898925  0.19275808]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 649. State = [[-0.3487936  -0.01524409]]. Action = [[-0.04880007  0.23240495  0.18214205 -0.8995478 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.3488342  -0.01513599]]. Action = [[ 0.09551886 -0.19541228  0.08611441 -0.1766473 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.35004327 -0.0150977 ]]. Action = [[ 0.11352631  0.10632288  0.23061338 -0.8299049 ]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.3499404  -0.01438426]]. Action = [[-0.2380048   0.0430434   0.06617522  0.86222243]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, True, False]
State prediction error at timestep 652 is tensor(7.3789e-05, grad_fn=<MseLossBackward0>)
Current timestep = 653. State = [[-0.34969884 -0.01415465]]. Action = [[ 0.19570535 -0.24532813  0.07937235  0.97958803]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(5.3087e-05, grad_fn=<MseLossBackward0>)
Current timestep = 654. State = [[-0.34962282 -0.01394754]]. Action = [[ 0.04075584  0.14709038 -0.17589113  0.68663836]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, True, False]
State prediction error at timestep 654 is tensor(9.1847e-06, grad_fn=<MseLossBackward0>)
Current timestep = 655. State = [[-0.34984004 -0.01345909]]. Action = [[0.15358013 0.15345281 0.11267656 0.18801391]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of -1
Current timestep = 656. State = [[-0.34964842 -0.01320165]]. Action = [[-0.20014848 -0.1912367  -0.14823133  0.1397767 ]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.34968796 -0.01309095]]. Action = [[ 0.05101153 -0.17316413 -0.14665045  0.03589463]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, True, False]
State prediction error at timestep 657 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 658. State = [[-0.34955782 -0.0126208 ]]. Action = [[ 0.10223097  0.10112101  0.16903299 -0.3328091 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(7.8403e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of -1
Current timestep = 659. State = [[-0.34945545 -0.01144424]]. Action = [[-0.22167622 -0.19173384 -0.03571466 -0.20452356]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.3494338  -0.01101921]]. Action = [[ 0.13935381 -0.1993282   0.21702898  0.45351124]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(5.6608e-07, grad_fn=<MseLossBackward0>)
Current timestep = 661. State = [[-0.3494183  -0.01057131]]. Action = [[-0.19651346  0.15495294 -0.14630702  0.54050016]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of -1
Current timestep = 662. State = [[-0.34911874 -0.01013568]]. Action = [[ 0.24110067 -0.17825344  0.05034852  0.6473224 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(3.1516e-05, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.34897995 -0.00871092]]. Action = [[-0.02108794  0.09005767 -0.04895373  0.7878622 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(6.8989e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of -1
Current timestep = 664. State = [[-0.34938264 -0.00566305]]. Action = [[ 6.6950917e-04 -6.5468788e-02  1.6227782e-01 -8.3636850e-01]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of -1
Current timestep = 665. State = [[-0.34947369 -0.00539457]]. Action = [[ 0.23365465  0.03652897 -0.15722004  0.9241713 ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(9.1307e-05, grad_fn=<MseLossBackward0>)
Current timestep = 666. State = [[-0.34948686 -0.00551147]]. Action = [[-0.18712533 -0.21044911 -0.10057509  0.23436224]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 667. State = [[-0.34945932 -0.00541021]]. Action = [[0.1450701  0.09143639 0.08465824 0.35981226]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(3.9042e-05, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.34947616 -0.00532605]]. Action = [[ 0.17774439 -0.19428891  0.09048492 -0.27746105]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, False, True, False]
State prediction error at timestep 668 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 669. State = [[-0.34950274 -0.00533656]]. Action = [[ 0.22021988 -0.01824948 -0.0110027  -0.89659566]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of -1
Current timestep = 670. State = [[-0.34939867 -0.00522107]]. Action = [[0.11678356 0.03109378 0.03076002 0.17078567]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(9.3942e-05, grad_fn=<MseLossBackward0>)
Current timestep = 671. State = [[-0.34904003 -0.0052065 ]]. Action = [[ 0.04772711  0.22911835 -0.22507046  0.3738991 ]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(5.0435e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 671 of -1
Current timestep = 672. State = [[-0.3487838  -0.00515269]]. Action = [[ 0.14410257 -0.17552668  0.16739592 -0.04447508]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(9.1802e-05, grad_fn=<MseLossBackward0>)
Current timestep = 673. State = [[-0.34871197 -0.00502215]]. Action = [[-0.12354328 -0.18987857  0.137483   -0.9152116 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 674. State = [[-0.3479861  -0.00487356]]. Action = [[ 0.10305202 -0.1198988   0.1680994  -0.5279047 ]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, False, True, False]
State prediction error at timestep 674 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 674 of -1
Current timestep = 675. State = [[-0.34661037 -0.00531103]]. Action = [[ 0.08825144  0.16772756 -0.04134928  0.40978467]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, False, True, False]
State prediction error at timestep 675 is tensor(2.8729e-05, grad_fn=<MseLossBackward0>)
Current timestep = 676. State = [[-0.34599435 -0.00547841]]. Action = [[ 0.15339085 -0.16970564 -0.22225657  0.45486426]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, False, True, False]
State prediction error at timestep 676 is tensor(2.0956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 677. State = [[-0.34587958 -0.00567575]]. Action = [[ 0.23450273 -0.04133762 -0.12898183 -0.17429209]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(8.2615e-05, grad_fn=<MseLossBackward0>)
Current timestep = 678. State = [[-0.34492582 -0.00582302]]. Action = [[ 0.16185445 -0.13937514 -0.15980124  0.25615835]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(2.9537e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[-0.34212998 -0.00615272]]. Action = [[-0.01050246 -0.05937716 -0.08467621 -0.8919465 ]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, False, True, False]
State prediction error at timestep 679 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of -1
Current timestep = 680. State = [[-0.34206277 -0.00657   ]]. Action = [[-0.00468932  0.1990388  -0.21924393  0.15287614]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, False, True, False]
State prediction error at timestep 680 is tensor(5.9797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 681. State = [[-0.3413919  -0.00694367]]. Action = [[-0.04468411 -0.22299874 -0.15917356 -0.89024204]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[-0.3406495  -0.00711669]]. Action = [[-0.08014539 -0.14524962  0.14990667 -0.9607598 ]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(8.7281e-05, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[-0.34062266 -0.00747466]]. Action = [[ 0.06176066 -0.18632694 -0.03311756  0.5448384 ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(4.8165e-05, grad_fn=<MseLossBackward0>)
Current timestep = 684. State = [[-0.3406274  -0.00786481]]. Action = [[ 0.20991904  0.18869817 -0.07750922  0.5175595 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [True, False, False, False, True, False]
State prediction error at timestep 684 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 685. State = [[-0.33979738 -0.00846965]]. Action = [[-0.06694064  0.06330591 -0.22275054 -0.37413895]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(8.7492e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.3400039  -0.00839632]]. Action = [[-0.19504751  0.09882417 -0.15261379 -0.45637262]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [True, False, False, False, True, False]
State prediction error at timestep 686 is tensor(1.6432e-05, grad_fn=<MseLossBackward0>)
Current timestep = 687. State = [[-0.34030387 -0.00839158]]. Action = [[-0.13581927  0.1482895  -0.21754836 -0.7133686 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [True, False, False, False, True, False]
State prediction error at timestep 687 is tensor(3.8020e-05, grad_fn=<MseLossBackward0>)
Current timestep = 688. State = [[-0.34021407 -0.00834269]]. Action = [[-0.04464647  0.2258007   0.20687944  0.1553365 ]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(4.4469e-05, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[-0.3398458  -0.00839828]]. Action = [[-0.15339722 -0.10657144  0.20487535 -0.9441069 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [True, False, False, False, True, False]
State prediction error at timestep 689 is tensor(3.5704e-05, grad_fn=<MseLossBackward0>)
Current timestep = 690. State = [[-0.34015808 -0.00842496]]. Action = [[ 0.08696967 -0.2302086   0.05724704  0.49890995]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, False, True, False]
State prediction error at timestep 690 is tensor(6.0581e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of 1
Current timestep = 691. State = [[-0.34008533 -0.00828097]]. Action = [[ 0.10421371  0.04931626 -0.0630073   0.22711647]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(2.3944e-05, grad_fn=<MseLossBackward0>)
Current timestep = 692. State = [[-0.3395441  -0.00790779]]. Action = [[ 0.0555419  -0.00889897  0.10399294 -0.61653405]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [True, False, False, False, True, False]
State prediction error at timestep 692 is tensor(5.1502e-05, grad_fn=<MseLossBackward0>)
Current timestep = 693. State = [[-0.33933547 -0.00788062]]. Action = [[-0.22256103 -0.21712793  0.19389397 -0.8091366 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [True, False, False, False, True, False]
State prediction error at timestep 693 is tensor(3.1660e-05, grad_fn=<MseLossBackward0>)
Current timestep = 694. State = [[-0.33909476 -0.00787041]]. Action = [[-0.19149557 -0.00714728 -0.16014628  0.4679978 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [True, False, False, False, True, False]
State prediction error at timestep 694 is tensor(4.9218e-05, grad_fn=<MseLossBackward0>)
Current timestep = 695. State = [[-0.33863467 -0.00777982]]. Action = [[ 0.19249368  0.18367955 -0.21806248  0.23926556]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(2.0486e-05, grad_fn=<MseLossBackward0>)
Current timestep = 696. State = [[-0.33825576 -0.00781465]]. Action = [[-0.23847052  0.18720463  0.02259904 -0.32536256]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [True, False, False, False, True, False]
State prediction error at timestep 696 is tensor(1.2836e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 696 of 1
Current timestep = 697. State = [[-0.33775696 -0.00765983]]. Action = [[0.12064871 0.13399601 0.16396946 0.32470036]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, False, True, False]
State prediction error at timestep 697 is tensor(6.1033e-07, grad_fn=<MseLossBackward0>)
Current timestep = 698. State = [[-0.3377584  -0.00773298]]. Action = [[-0.23478688 -0.23916772  0.21534795 -0.0122149 ]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, False, True, False]
State prediction error at timestep 698 is tensor(1.6447e-05, grad_fn=<MseLossBackward0>)
Current timestep = 699. State = [[-0.33732924 -0.00771466]]. Action = [[ 0.19858658  0.01764563 -0.14922498  0.46995282]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, False, True, False]
State prediction error at timestep 699 is tensor(9.2472e-05, grad_fn=<MseLossBackward0>)
Current timestep = 700. State = [[-0.33685726 -0.0077009 ]]. Action = [[ 0.06025565 -0.10314727  0.087286    0.15775657]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, False, True, False]
State prediction error at timestep 700 is tensor(2.0233e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of 1
Current timestep = 701. State = [[-0.33510724 -0.00890735]]. Action = [[ 0.00928003 -0.24081664 -0.17121887  0.3428073 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, False, True, False]
State prediction error at timestep 701 is tensor(1.0383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 702. State = [[-0.33470583 -0.00904133]]. Action = [[ 0.23975337  0.22594184 -0.03355479 -0.14469492]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [True, False, False, False, True, False]
State prediction error at timestep 702 is tensor(1.2555e-05, grad_fn=<MseLossBackward0>)
Current timestep = 703. State = [[-0.33419764 -0.00921414]]. Action = [[ 0.2284944  -0.02945647  0.15999204 -0.7389419 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.33410132 -0.00941817]]. Action = [[-0.2352793   0.17803156  0.16233933 -0.10059655]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(8.4934e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of 1
Current timestep = 705. State = [[-0.33308852 -0.0098778 ]]. Action = [[-0.01424953 -0.01423997  0.22857615 -0.36706555]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, False, True, False]
State prediction error at timestep 705 is tensor(2.4846e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of 1
Current timestep = 706. State = [[-0.3324985  -0.00999632]]. Action = [[0.1366353  0.0884327  0.08822307 0.5440464 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, False, True, False]
State prediction error at timestep 706 is tensor(4.9885e-05, grad_fn=<MseLossBackward0>)
Current timestep = 707. State = [[-0.3318439  -0.01052252]]. Action = [[ 0.0118337   0.01099217  0.15882027 -0.32457435]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, False, True, False]
State prediction error at timestep 707 is tensor(2.3476e-05, grad_fn=<MseLossBackward0>)
Current timestep = 708. State = [[-0.33175105 -0.01058235]]. Action = [[ 0.19994527 -0.13489275 -0.06444031  0.91528106]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, False, True, False]
State prediction error at timestep 708 is tensor(2.5811e-07, grad_fn=<MseLossBackward0>)
Current timestep = 709. State = [[-0.3319131  -0.01067383]]. Action = [[-0.22085044  0.14047223  0.06277573 -0.23426932]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(2.3592e-05, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.33196864 -0.01062794]]. Action = [[ 0.19042277  0.01248893 -0.10361622 -0.5781085 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 711. State = [[-0.33161977 -0.01064832]]. Action = [[ 0.13680491  0.2046712  -0.18763547  0.9697709 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, False, True, False]
State prediction error at timestep 711 is tensor(2.9913e-05, grad_fn=<MseLossBackward0>)
Current timestep = 712. State = [[-0.3317223  -0.01083607]]. Action = [[ 0.2307362   0.07507801  0.09322387 -0.61707246]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of 1
Current timestep = 713. State = [[-0.3316271  -0.01082918]]. Action = [[ 0.14022416  0.00867814 -0.12328196 -0.22112924]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(5.4036e-05, grad_fn=<MseLossBackward0>)
Current timestep = 714. State = [[-0.3315766  -0.01087511]]. Action = [[ 0.0732305  -0.16695435 -0.19174673  0.38826942]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(3.9698e-05, grad_fn=<MseLossBackward0>)
Current timestep = 715. State = [[-0.33169287 -0.01091573]]. Action = [[ 0.13045108 -0.21045841 -0.12359798  0.11183369]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, False, True, False]
State prediction error at timestep 715 is tensor(1.6983e-05, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.33163834 -0.0109005 ]]. Action = [[ 0.21723148 -0.00597295  0.03682652 -0.04130995]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(5.3008e-06, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.33153838 -0.01091508]]. Action = [[-0.0943615   0.03263402  0.02013642 -0.01536864]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.33153838 -0.01091508]]. Action = [[ 0.13483572 -0.23054633 -0.01756105 -0.26144713]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(6.8954e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.33166018 -0.01091439]]. Action = [[-0.09316424 -0.208851   -0.18429875 -0.45761216]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, False, True, False]
State prediction error at timestep 719 is tensor(2.7864e-06, grad_fn=<MseLossBackward0>)
Current timestep = 720. State = [[-0.3315657 -0.0109227]]. Action = [[-0.0453586  -0.23739517 -0.01305869 -0.8614427 ]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, False, True, False]
State prediction error at timestep 720 is tensor(4.7668e-05, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[-0.3316336  -0.01092193]]. Action = [[ 0.17631888  0.15555179 -0.12095189  0.6983808 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [True, False, False, False, True, False]
State prediction error at timestep 721 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 722. State = [[-0.3316336  -0.01092193]]. Action = [[-0.06459001 -0.18866718 -0.02560113 -0.0953877 ]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [True, False, False, False, True, False]
State prediction error at timestep 722 is tensor(1.0368e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.3315657 -0.0109227]]. Action = [[ 0.14197978  0.08376688  0.07860219 -0.0523178 ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [True, False, False, False, True, False]
State prediction error at timestep 723 is tensor(3.9747e-05, grad_fn=<MseLossBackward0>)
Current timestep = 724. State = [[-0.3316336  -0.01092193]]. Action = [[-0.18563318 -0.16528155 -0.08121134  0.5456395 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(9.5286e-06, grad_fn=<MseLossBackward0>)
Current timestep = 725. State = [[-0.3315657 -0.0109227]]. Action = [[-0.10216168 -0.19328575  0.08865672 -0.8564675 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [True, False, False, False, True, False]
State prediction error at timestep 725 is tensor(3.7097e-05, grad_fn=<MseLossBackward0>)
Current timestep = 726. State = [[-0.3315657 -0.0109227]]. Action = [[ 0.18778652 -0.06499317 -0.2135199   0.7008368 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [True, False, False, False, True, False]
State prediction error at timestep 726 is tensor(3.4643e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 726 of 1
Current timestep = 727. State = [[-0.3316336  -0.01092193]]. Action = [[ 0.17797136 -0.08349979 -0.09306759 -0.5593861 ]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [True, False, False, False, True, False]
State prediction error at timestep 727 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 728. State = [[-0.33160952 -0.01080409]]. Action = [[-0.02510318  0.05500895  0.23396042  0.7372589 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(3.6054e-05, grad_fn=<MseLossBackward0>)
Current timestep = 729. State = [[-0.33171576 -0.0102987 ]]. Action = [[-0.14092973  0.22845715  0.1307714   0.85676026]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [True, False, False, False, True, False]
State prediction error at timestep 729 is tensor(3.7454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 730. State = [[-0.33203354 -0.00976298]]. Action = [[-0.02119125 -0.11764117  0.15870428  0.7115754 ]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [True, False, False, False, True, False]
State prediction error at timestep 730 is tensor(3.2553e-05, grad_fn=<MseLossBackward0>)
Current timestep = 731. State = [[-0.33204794 -0.0107775 ]]. Action = [[-0.02020937  0.18839723  0.16558337 -0.36854255]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [True, False, False, False, True, False]
State prediction error at timestep 731 is tensor(4.4207e-06, grad_fn=<MseLossBackward0>)
Current timestep = 732. State = [[-0.33196753 -0.01094032]]. Action = [[ 0.20537129  0.1746149  -0.01165091 -0.84860915]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [True, False, False, False, True, False]
State prediction error at timestep 732 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 733. State = [[-0.33202055 -0.01097372]]. Action = [[-0.09711885 -0.24456987 -0.21330199  0.8128363 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [True, False, False, False, True, False]
State prediction error at timestep 733 is tensor(1.6617e-06, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[-0.3322293  -0.01162271]]. Action = [[-0.00926262  0.09292996 -0.14568871 -0.9071545 ]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [True, False, False, False, True, False]
State prediction error at timestep 734 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 735. State = [[-0.33238623 -0.01141843]]. Action = [[ 0.16861022 -0.07702824 -0.19363865 -0.10394114]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [True, False, False, False, True, False]
State prediction error at timestep 735 is tensor(7.2657e-06, grad_fn=<MseLossBackward0>)
Current timestep = 736. State = [[-0.3323292  -0.01112401]]. Action = [[ 0.10588655 -0.169397   -0.09339347 -0.6230502 ]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [True, False, False, False, True, False]
State prediction error at timestep 736 is tensor(5.6360e-05, grad_fn=<MseLossBackward0>)
Current timestep = 737. State = [[-0.3323578  -0.01104669]]. Action = [[ 0.21380058 -0.11286847 -0.16079544  0.8148196 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [True, False, False, False, True, False]
State prediction error at timestep 737 is tensor(4.0508e-06, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[-0.33255935 -0.01073006]]. Action = [[-0.11757764  0.05437002 -0.0577658  -0.01821947]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [True, False, False, False, True, False]
State prediction error at timestep 738 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[-0.33314657 -0.00990716]]. Action = [[ 0.10971755  0.19782108  0.16735    -0.8508585 ]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [True, False, False, False, True, False]
State prediction error at timestep 739 is tensor(7.3649e-05, grad_fn=<MseLossBackward0>)
Current timestep = 740. State = [[-0.33380237 -0.00945612]]. Action = [[-0.0453729   0.23776376  0.186494   -0.8797381 ]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [True, False, False, False, True, False]
State prediction error at timestep 740 is tensor(5.7007e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.33547413 -0.00774498]]. Action = [[0.04688799 0.11444271 0.02771941 0.35964787]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [True, False, False, False, True, False]
State prediction error at timestep 741 is tensor(1.2650e-05, grad_fn=<MseLossBackward0>)
Current timestep = 742. State = [[-0.33570284 -0.00637323]]. Action = [[-0.14461137 -0.18368922  0.04249448  0.12974882]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [True, False, False, False, True, False]
State prediction error at timestep 742 is tensor(3.4722e-06, grad_fn=<MseLossBackward0>)
Current timestep = 743. State = [[-0.33600312 -0.00547468]]. Action = [[-0.07498802  0.24579382  0.192893    0.14854765]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [True, False, False, False, True, False]
State prediction error at timestep 743 is tensor(6.2934e-05, grad_fn=<MseLossBackward0>)
Current timestep = 744. State = [[-0.33650798 -0.00495275]]. Action = [[-0.0816192  -0.22219984  0.12224305  0.97464037]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [True, False, False, False, True, False]
State prediction error at timestep 744 is tensor(6.5025e-06, grad_fn=<MseLossBackward0>)
Current timestep = 745. State = [[-0.33670723 -0.00386464]]. Action = [[ 0.10005248  0.18482584  0.12387401 -0.65347946]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [True, False, False, False, True, False]
State prediction error at timestep 745 is tensor(9.4446e-05, grad_fn=<MseLossBackward0>)
Current timestep = 746. State = [[-0.33705583 -0.00293804]]. Action = [[ 0.1703465   0.05469778  0.20497382 -0.8371686 ]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [True, False, False, False, True, False]
State prediction error at timestep 746 is tensor(2.5060e-05, grad_fn=<MseLossBackward0>)
Current timestep = 747. State = [[-0.33747578 -0.00241215]]. Action = [[ 0.22267956 -0.10759851  0.12756073 -0.18632185]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [True, False, False, False, True, False]
State prediction error at timestep 747 is tensor(1.5652e-05, grad_fn=<MseLossBackward0>)
Current timestep = 748. State = [[-0.33745268 -0.00180471]]. Action = [[-0.15833955 -0.13852437  0.06897238 -0.07185012]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, False, True, False]
State prediction error at timestep 748 is tensor(1.5932e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of -1
Current timestep = 749. State = [[-0.33775517 -0.00139932]]. Action = [[-0.24688394  0.1839689  -0.08946723  0.06143379]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [True, False, False, False, True, False]
State prediction error at timestep 749 is tensor(4.8619e-05, grad_fn=<MseLossBackward0>)
Current timestep = 750. State = [[-0.3379413  -0.00089316]]. Action = [[ 0.20588845  0.21693617  0.05080587 -0.94166404]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, False, True, False]
State prediction error at timestep 750 is tensor(6.1302e-05, grad_fn=<MseLossBackward0>)
Current timestep = 751. State = [[-0.33784702 -0.00067468]]. Action = [[ 0.14445764 -0.22988449 -0.11546588  0.4417535 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, False, True, False]
State prediction error at timestep 751 is tensor(4.5494e-05, grad_fn=<MseLossBackward0>)
Current timestep = 752. State = [[-0.33809486 -0.00041626]]. Action = [[-0.2221006   0.05327576 -0.00082329 -0.7713138 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, False, True, False]
State prediction error at timestep 752 is tensor(9.0441e-06, grad_fn=<MseLossBackward0>)
Current timestep = 753. State = [[-3.382296e-01 -2.593769e-04]]. Action = [[ 0.15199363 -0.16506416 -0.05665056 -0.48214018]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, False, True, False]
State prediction error at timestep 753 is tensor(6.1663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-3.3807027e-01  4.4743865e-06]]. Action = [[ 0.1818684   0.21227026  0.12411636 -0.2256751 ]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, False, True, False]
State prediction error at timestep 754 is tensor(5.3351e-06, grad_fn=<MseLossBackward0>)
Current timestep = 755. State = [[-3.3832809e-01  1.9358919e-04]]. Action = [[-0.19999245  0.14572823 -0.04320183 -0.6687067 ]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, False, True, False]
State prediction error at timestep 755 is tensor(2.3226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 756. State = [[-0.3382282   0.00042607]]. Action = [[-0.10776863  0.17822725  0.03599337  0.20498252]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, False, True, False]
State prediction error at timestep 756 is tensor(3.7769e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 756 of -1
Current timestep = 757. State = [[-0.33832863  0.00096717]]. Action = [[ 0.04776347  0.12382787 -0.0086441   0.4441135 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, False, True, False]
State prediction error at timestep 757 is tensor(3.5331e-05, grad_fn=<MseLossBackward0>)
Current timestep = 758. State = [[-0.33924952  0.00408337]]. Action = [[ 0.10097137  0.06344151 -0.07866523  0.49962628]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, False, True, False]
State prediction error at timestep 758 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 759. State = [[-0.33920494  0.00497525]]. Action = [[ 0.0098094   0.14314044 -0.17216364  0.45626426]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, False, True, False]
State prediction error at timestep 759 is tensor(6.1197e-05, grad_fn=<MseLossBackward0>)
Current timestep = 760. State = [[-0.33863723  0.00749752]]. Action = [[ 0.07277712  0.02873024 -0.20673147 -0.40639603]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, False, True, False]
State prediction error at timestep 760 is tensor(9.7128e-06, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.33808532  0.00814894]]. Action = [[0.07729214 0.19585979 0.14522892 0.71413374]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, False, True, False]
State prediction error at timestep 761 is tensor(8.7428e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of -1
Current timestep = 762. State = [[-0.33768734  0.00871816]]. Action = [[ 0.21851453 -0.01360522  0.09798992 -0.950597  ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, False, True, False]
State prediction error at timestep 762 is tensor(8.7968e-05, grad_fn=<MseLossBackward0>)
Current timestep = 763. State = [[-0.33731857  0.00905771]]. Action = [[ 0.11029029 -0.15087582  0.20357573 -0.9306179 ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, False, True, False]
State prediction error at timestep 763 is tensor(1.0398e-06, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[-0.33707058  0.00931764]]. Action = [[ 0.24707201 -0.23011836  0.23870125 -0.7453029 ]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, False, True, False]
State prediction error at timestep 764 is tensor(2.5544e-05, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.33661744  0.00984619]]. Action = [[ 0.1950084   0.24424297 -0.18045102 -0.3413999 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, False, True, False]
State prediction error at timestep 765 is tensor(2.2505e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of -1
Current timestep = 766. State = [[-0.33587435  0.01059491]]. Action = [[ 0.04142562  0.05972594 -0.04217294 -0.10524023]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(3.0400e-05, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.3355253   0.01136674]]. Action = [[-0.09186995 -0.22982852 -0.15999404  0.26798427]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, False, True, False]
State prediction error at timestep 767 is tensor(1.9341e-05, grad_fn=<MseLossBackward0>)
Current timestep = 768. State = [[-0.3353168   0.01158666]]. Action = [[ 0.05408174 -0.14986005 -0.02155724 -0.76801676]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, False, True, False]
State prediction error at timestep 768 is tensor(3.2565e-06, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[-0.335223   0.0119356]]. Action = [[-0.21528122  0.07468849  0.01204434  0.7876198 ]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, False, True, False]
State prediction error at timestep 769 is tensor(1.0339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[-0.33489817  0.01241703]]. Action = [[ 0.15804791  0.1983856  -0.06885919  0.5824611 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, False, True, False]
State prediction error at timestep 770 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[-0.33466688  0.01311801]]. Action = [[ 0.23080376 -0.02351673  0.22234508 -0.34467894]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(4.3092e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.33435592  0.01408451]]. Action = [[-0.03745885 -0.07176788  0.05409852 -0.20499772]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(4.1026e-05, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[-0.3343518   0.01413803]]. Action = [[ 0.03787839  0.20751151 -0.21278398 -0.7082644 ]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, False, True, False]
State prediction error at timestep 773 is tensor(2.9522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 773 of 1
Current timestep = 774. State = [[-0.33446804  0.01402001]]. Action = [[-0.22889557  0.24618292  0.02652895 -0.6254675 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, False, True, False]
State prediction error at timestep 774 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[-0.33450794  0.01411785]]. Action = [[-0.12222745  0.24060309 -0.13410932 -0.27571893]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, False, True, False]
State prediction error at timestep 775 is tensor(3.2849e-05, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[-0.33438677  0.01412542]]. Action = [[-0.1213685  -0.16100642  0.13112164 -0.21768272]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, False, True, False]
State prediction error at timestep 776 is tensor(2.5773e-05, grad_fn=<MseLossBackward0>)
Current timestep = 777. State = [[-0.3345018   0.01409689]]. Action = [[-0.07655431  0.03701219  0.07536227  0.04007268]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, False, True, False]
State prediction error at timestep 777 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 1
Current timestep = 778. State = [[-0.33455682  0.01430737]]. Action = [[ 0.17966741 -0.11692896  0.22593337  0.02732658]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, False, True, False]
State prediction error at timestep 778 is tensor(2.9684e-05, grad_fn=<MseLossBackward0>)
Current timestep = 779. State = [[-0.33459666  0.01440445]]. Action = [[ 0.17074764  0.11766416 -0.0675194   0.8976917 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(8.9481e-05, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.33461192  0.0144724 ]]. Action = [[ 0.13230169 -0.1590089  -0.05543867 -0.38881516]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, False, True, False]
State prediction error at timestep 780 is tensor(2.9939e-05, grad_fn=<MseLossBackward0>)
Current timestep = 781. State = [[-0.33466437  0.01455815]]. Action = [[-0.146306   -0.04620267 -0.13034715 -0.575206  ]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, False, True, False]
State prediction error at timestep 781 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[-0.3346897   0.01459316]]. Action = [[-0.20059173 -0.0491416   0.22004682  0.03267229]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, False, True, False]
State prediction error at timestep 782 is tensor(4.9794e-05, grad_fn=<MseLossBackward0>)
Current timestep = 783. State = [[-0.334684    0.01472108]]. Action = [[ 0.07948965 -0.23387146 -0.2401399   0.93733895]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, False, True, False]
State prediction error at timestep 783 is tensor(8.8839e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 783 of 1
Current timestep = 784. State = [[-0.3347517   0.01487437]]. Action = [[-0.14915475 -0.24767801 -0.07151914  0.16037178]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, False, True, False]
State prediction error at timestep 784 is tensor(3.4906e-05, grad_fn=<MseLossBackward0>)
Current timestep = 785. State = [[-0.33478564  0.0149512 ]]. Action = [[-0.1773427  -0.2320568  -0.11433393 -0.5440392 ]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, False, True, False]
State prediction error at timestep 785 is tensor(4.8803e-05, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[-0.3347892   0.01494854]]. Action = [[ 0.07998198 -0.11593054  0.08159316 -0.17662495]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, False, True, False]
State prediction error at timestep 786 is tensor(3.0301e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[-0.3339717   0.01395643]]. Action = [[0.06063375 0.05236465 0.09310311 0.78777957]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(7.4695e-05, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[-0.33326465  0.01411904]]. Action = [[ 0.04910848  0.02780628 -0.11796477  0.61838055]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, False, True, False]
State prediction error at timestep 788 is tensor(6.7974e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 788 of 1
Current timestep = 789. State = [[-0.332911    0.01416766]]. Action = [[-0.14703731 -0.04467364  0.157157    0.89948046]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, False, True, False]
State prediction error at timestep 789 is tensor(1.8611e-05, grad_fn=<MseLossBackward0>)
Current timestep = 790. State = [[-0.3316329   0.01428527]]. Action = [[0.06684539 0.0106951  0.24381912 0.8547456 ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, False, True, False]
State prediction error at timestep 790 is tensor(6.7617e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.33076185  0.01444831]]. Action = [[-0.23160468  0.22781664  0.19347268  0.76372313]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, False, True, False]
State prediction error at timestep 791 is tensor(3.7894e-05, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[-0.32920292  0.01461747]]. Action = [[0.00897428 0.02856216 0.18421197 0.09856462]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, False, True, False]
State prediction error at timestep 792 is tensor(6.4167e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.32829666  0.01494479]]. Action = [[-0.02934054  0.10425174 -0.06022489  0.5139973 ]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, False, True, False]
State prediction error at timestep 793 is tensor(2.4829e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 793 of 1
Current timestep = 794. State = [[-0.32830805  0.0157411 ]]. Action = [[ 0.06127411  0.22681665 -0.15004727 -0.65445316]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, False, True, False]
State prediction error at timestep 794 is tensor(8.3727e-06, grad_fn=<MseLossBackward0>)
Current timestep = 795. State = [[-0.32810003  0.01671216]]. Action = [[ 0.19272327 -0.20607199 -0.18037114 -0.93239033]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, False, True, False]
State prediction error at timestep 795 is tensor(5.0411e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of 1
Current timestep = 796. State = [[-0.32790965  0.01716714]]. Action = [[ 0.11957109  0.15322447 -0.15032317 -0.5956492 ]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, False, True, False]
State prediction error at timestep 796 is tensor(3.5725e-06, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[-0.32754958  0.01845617]]. Action = [[-0.10091448 -0.0936718  -0.07798876 -0.6125308 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, False, True, False]
State prediction error at timestep 797 is tensor(8.0415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 798. State = [[-0.32768703  0.01834426]]. Action = [[-0.10461867 -0.1528178   0.22489813  0.43625045]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, False, True, False]
State prediction error at timestep 798 is tensor(2.0775e-06, grad_fn=<MseLossBackward0>)
Current timestep = 799. State = [[-0.32773584  0.01824035]]. Action = [[ 0.21448976 -0.22924727 -0.22930378 -0.884801  ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, False, True, False]
State prediction error at timestep 799 is tensor(4.5062e-05, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[-0.32771856  0.01843796]]. Action = [[-0.12404296  0.11255932 -0.04708442  0.17896605]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, False, True, False]
State prediction error at timestep 800 is tensor(6.7035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of 1
Current timestep = 801. State = [[-0.3294542   0.02137746]]. Action = [[ 0.09234008  0.09625605 -0.04106706  0.3369851 ]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, False, True, False]
State prediction error at timestep 801 is tensor(5.8572e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.33093965  0.02510598]]. Action = [[ 0.01791126 -0.08360943 -0.21197483 -0.02602381]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, False, True, False]
State prediction error at timestep 802 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 803. State = [[-0.3310602   0.02538667]]. Action = [[-0.2097734   0.04843587 -0.09630808  0.1290667 ]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, False, True, False]
State prediction error at timestep 803 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 804. State = [[-0.33090368  0.02541041]]. Action = [[-0.15175654  0.0531947  -0.22232366  0.7971959 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, False, True, False]
State prediction error at timestep 804 is tensor(6.2466e-05, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[-0.3307935   0.02547832]]. Action = [[-0.22333243 -0.09651217 -0.08163948  0.7429702 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(2.2429e-05, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[-0.3308438   0.02547913]]. Action = [[ 0.00470346 -0.24376903  0.14839506  0.03073514]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, False, True, False]
State prediction error at timestep 806 is tensor(3.9523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 807. State = [[-0.33097288  0.02546492]]. Action = [[ 0.22777301  0.13898644 -0.22140107 -0.55368453]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, False, True, False]
State prediction error at timestep 807 is tensor(6.0309e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of -1
Current timestep = 808. State = [[-0.33076388  0.02568696]]. Action = [[ 0.10593793  0.10292077  0.08888188 -0.07062483]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, False, True, False]
State prediction error at timestep 808 is tensor(2.6194e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of -1
Current timestep = 809. State = [[-0.33052468  0.02628305]]. Action = [[-0.24741724  0.02731735 -0.13393869  0.05161428]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, False, True, False]
State prediction error at timestep 809 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 810. State = [[-0.3301695   0.02683003]]. Action = [[ 0.22210059 -0.01232925  0.19295606  0.93984723]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, False, True, False]
State prediction error at timestep 810 is tensor(7.2395e-05, grad_fn=<MseLossBackward0>)
Current timestep = 811. State = [[-0.32975155  0.0271898 ]]. Action = [[-0.19397941 -0.23892598 -0.1649808  -0.5256893 ]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, False, True, False]
State prediction error at timestep 811 is tensor(5.5525e-05, grad_fn=<MseLossBackward0>)
Current timestep = 812. State = [[-0.3291575   0.02865937]]. Action = [[-0.11568412  0.04444212 -0.06627512  0.42622328]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(4.2966e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[-0.33039623  0.03155306]]. Action = [[ 0.09599102  0.06354928  0.08834019 -0.60644954]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, False, True, False]
State prediction error at timestep 813 is tensor(3.3577e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of -1
Current timestep = 814. State = [[-0.33060065  0.03267377]]. Action = [[-0.1552548   0.24416915 -0.23669171 -0.4612894 ]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, False, True, False]
State prediction error at timestep 814 is tensor(5.8850e-05, grad_fn=<MseLossBackward0>)
Current timestep = 815. State = [[-0.33045605  0.03324006]]. Action = [[ 0.2311359  -0.2233579   0.00403914  0.17647958]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(5.9843e-05, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.33026138  0.03394811]]. Action = [[ 0.12966937  0.24777597  0.13908893 -0.15424931]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, False, True, False]
State prediction error at timestep 816 is tensor(2.4841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 817. State = [[-0.3302088   0.03463898]]. Action = [[-0.07627785  0.14708868 -0.0670511  -0.95941544]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(2.3333e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of -1
Current timestep = 818. State = [[-0.32975584  0.03597229]]. Action = [[ 0.12972105  0.22431737 -0.10288897 -0.34588337]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, False, True, False]
State prediction error at timestep 818 is tensor(1.1605e-05, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[-0.3297006   0.03661287]]. Action = [[-0.10745978 -0.2480837   0.04689378  0.5775213 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, False, True, False]
State prediction error at timestep 819 is tensor(1.3222e-05, grad_fn=<MseLossBackward0>)
Current timestep = 820. State = [[-0.32961154  0.03708585]]. Action = [[1.8341839e-01 7.2288513e-04 9.6684426e-02 8.4908533e-01]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, False, True, False]
State prediction error at timestep 820 is tensor(9.0614e-05, grad_fn=<MseLossBackward0>)
Current timestep = 821. State = [[-0.329291   0.0376458]]. Action = [[-0.1601923   0.20888487 -0.18130554 -0.8310967 ]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, False, True, False]
State prediction error at timestep 821 is tensor(4.6558e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[-0.32915697  0.03819214]]. Action = [[-0.11631241 -0.17837675  0.03132969 -0.84850395]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, False, True, False]
State prediction error at timestep 822 is tensor(7.1734e-05, grad_fn=<MseLossBackward0>)
Current timestep = 823. State = [[-0.32915047  0.0385969 ]]. Action = [[-0.16807614 -0.09463494 -0.1224677  -0.23310876]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, False, True, False]
State prediction error at timestep 823 is tensor(8.0511e-05, grad_fn=<MseLossBackward0>)
Current timestep = 824. State = [[-0.32890904  0.03904029]]. Action = [[-0.17551768 -0.16362199  0.22754157 -0.02009583]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, False, True, False]
State prediction error at timestep 824 is tensor(6.4167e-05, grad_fn=<MseLossBackward0>)
Current timestep = 825. State = [[-0.32893312  0.03930528]]. Action = [[-0.19109151  0.23151672  0.09186196 -0.07197368]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, False, True, False]
State prediction error at timestep 825 is tensor(6.8530e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[-0.32886702  0.03974161]]. Action = [[ 0.2222001  -0.0430717  -0.16857585  0.6061981 ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, False, True, False]
State prediction error at timestep 826 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[-0.32856482  0.04017318]]. Action = [[ 0.2194221  -0.22756352 -0.14503904  0.14259481]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, False, True, False]
State prediction error at timestep 827 is tensor(6.3813e-05, grad_fn=<MseLossBackward0>)
Current timestep = 828. State = [[-0.32857895  0.04029549]]. Action = [[ 0.0840742  -0.14096938  0.00274211  0.40887582]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, False, True, False]
State prediction error at timestep 828 is tensor(4.3756e-05, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[-0.32839182  0.04095472]]. Action = [[ 0.1151329   0.09365544 -0.11217776 -0.9921695 ]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, False, True, False]
State prediction error at timestep 829 is tensor(2.5288e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of -1
Current timestep = 830. State = [[-0.3259273  0.0434872]]. Action = [[-0.07074945 -0.02079472  0.2116651  -0.0823741 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, False, True, False]
State prediction error at timestep 830 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 831. State = [[-0.32558325  0.0444494 ]]. Action = [[-0.02459791 -0.00411685  0.14314562  0.16663766]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, False, True, False]
State prediction error at timestep 831 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[-0.32580078  0.04460425]]. Action = [[-0.21778144 -0.22969225  0.22349751 -0.59325916]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, False, True, False]
State prediction error at timestep 832 is tensor(6.4623e-05, grad_fn=<MseLossBackward0>)
Current timestep = 833. State = [[-0.3259215   0.04470377]]. Action = [[ 0.06497318 -0.13415562 -0.06416029  0.86405706]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, False, True, False]
State prediction error at timestep 833 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[-0.32585832  0.04492483]]. Action = [[-0.22765963 -0.06207363 -0.10888931  0.4066342 ]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, False, True, False]
State prediction error at timestep 834 is tensor(6.6523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 835. State = [[-0.32574114  0.0453282 ]]. Action = [[-0.02598564 -0.05074292  0.20725983  0.07848799]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, False, True, False]
State prediction error at timestep 835 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of -1
Current timestep = 836. State = [[-0.32566896  0.04530987]]. Action = [[ 0.0950284  -0.00584918  0.21138138  0.82538366]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, False, True, False]
State prediction error at timestep 836 is tensor(6.8945e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of -1
Current timestep = 837. State = [[-0.32556644  0.04528874]]. Action = [[-0.18337284 -0.07655039 -0.22001399 -0.9385812 ]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, False, True, False]
State prediction error at timestep 837 is tensor(1.8873e-06, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[-0.324496    0.04575273]]. Action = [[ 0.08479011  0.11219901  0.24218059 -0.38622797]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, False, True, False]
State prediction error at timestep 838 is tensor(3.7864e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of -1
Current timestep = 839. State = [[-0.3236247   0.04644799]]. Action = [[ 0.2331396   0.18486157  0.20943773 -0.6245542 ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, False, True, False]
State prediction error at timestep 839 is tensor(7.9676e-10, grad_fn=<MseLossBackward0>)
Current timestep = 840. State = [[-0.32332876  0.04670588]]. Action = [[ 0.15553907  0.1438568   0.08867958 -0.10858697]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, False, True, False]
State prediction error at timestep 840 is tensor(1.3253e-05, grad_fn=<MseLossBackward0>)
Current timestep = 841. State = [[-0.32297108  0.04702168]]. Action = [[-0.20261756  0.15029863  0.03428665 -0.40382093]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, False, True, False]
State prediction error at timestep 841 is tensor(6.5200e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of -1
Current timestep = 842. State = [[-0.32230952  0.04754541]]. Action = [[-0.15724517  0.12536883 -0.02133018  0.10686183]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, False, True, False]
State prediction error at timestep 842 is tensor(7.9652e-05, grad_fn=<MseLossBackward0>)
Current timestep = 843. State = [[-0.31994572  0.04901058]]. Action = [[-0.02483289  0.01465043  0.13554016  0.46807683]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, False, True, False]
State prediction error at timestep 843 is tensor(2.6903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 844. State = [[-0.3195899   0.04936789]]. Action = [[ 0.13172302 -0.17611071 -0.21698307  0.19845843]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, False, True, False]
State prediction error at timestep 844 is tensor(6.6076e-05, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[-0.31949276  0.04954904]]. Action = [[-0.14229628 -0.02558245 -0.10408199 -0.5788977 ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, False, True, False]
State prediction error at timestep 845 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 846. State = [[-0.31911343  0.04980465]]. Action = [[-0.15977395 -0.12222487  0.05933291  0.7700306 ]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, False, True, False]
State prediction error at timestep 846 is tensor(4.2751e-05, grad_fn=<MseLossBackward0>)
Current timestep = 847. State = [[-0.31855708  0.05002709]]. Action = [[-0.10577084  0.21914798  0.14332002  0.5763314 ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, False, True, False]
State prediction error at timestep 847 is tensor(3.7114e-05, grad_fn=<MseLossBackward0>)
Current timestep = 848. State = [[-0.31844208  0.05017759]]. Action = [[ 0.00354683 -0.23093338 -0.06078678 -0.4773878 ]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, False, True, False]
State prediction error at timestep 848 is tensor(6.8728e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of -1
Current timestep = 849. State = [[-0.3178795  0.0505131]]. Action = [[ 0.10288659 -0.1718401  -0.00327912 -0.8787747 ]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, False, True, False]
State prediction error at timestep 849 is tensor(1.3708e-05, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[-0.3174389   0.05070883]]. Action = [[-0.16266863 -0.2176273  -0.15069892 -0.47126067]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, False, True, False]
State prediction error at timestep 850 is tensor(4.8700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 851. State = [[-0.31743255  0.05086306]]. Action = [[0.17091158 0.19792914 0.24191386 0.8305105 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of -1
Current timestep = 852. State = [[-0.31679237  0.05118133]]. Action = [[-0.16992661  0.18030643  0.18344748 -0.60882014]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[-0.3168208   0.05145318]]. Action = [[-0.16434982 -0.16592784 -0.15577579  0.03653502]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, False, True, False]
State prediction error at timestep 853 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 854. State = [[-0.31655365  0.05180367]]. Action = [[ 0.07062709 -0.06850207 -0.18940404  0.6060146 ]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, False, True, False]
State prediction error at timestep 854 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 854 of -1
Current timestep = 855. State = [[-0.3156464  0.051746 ]]. Action = [[ 0.03094363  0.14010164 -0.10365462  0.7862251 ]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, False, True, False]
State prediction error at timestep 855 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 856. State = [[-0.3143995   0.05142664]]. Action = [[-0.05249386 -0.04175182 -0.1197937  -0.86434776]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, False, True, False]
State prediction error at timestep 856 is tensor(1.3490e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of 1
Current timestep = 857. State = [[-0.31443584  0.05112549]]. Action = [[ 0.15463525  0.06393778 -0.21998084 -0.33843523]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(3.6705e-05, grad_fn=<MseLossBackward0>)
Current timestep = 858. State = [[-0.31450403  0.05097532]]. Action = [[-0.09675407 -0.1446231  -0.23571625 -0.356512  ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(1.0535e-05, grad_fn=<MseLossBackward0>)
Current timestep = 859. State = [[-0.314366    0.05094117]]. Action = [[ 0.13768703 -0.14765373  0.0449371   0.77922535]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, False, True, False]
State prediction error at timestep 859 is tensor(3.7038e-05, grad_fn=<MseLossBackward0>)
Current timestep = 860. State = [[-0.31427217  0.05066954]]. Action = [[ 0.18420812 -0.23064138 -0.03610492 -0.18985021]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, False, True, False]
State prediction error at timestep 860 is tensor(1.4772e-05, grad_fn=<MseLossBackward0>)
Current timestep = 861. State = [[-0.31420508  0.05040036]]. Action = [[ 0.09601206  0.05312663 -0.04744485  0.06224668]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, False, True, False]
State prediction error at timestep 861 is tensor(1.8839e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of 1
Current timestep = 862. State = [[-0.3137615   0.05059917]]. Action = [[-0.20802504  0.2279414  -0.12216257 -0.6678601 ]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, False, True, False]
State prediction error at timestep 862 is tensor(8.7260e-05, grad_fn=<MseLossBackward0>)
Current timestep = 863. State = [[-0.31347758  0.05065427]]. Action = [[ 0.18571854 -0.24264461 -0.1643329  -0.14919591]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, False, True, False]
State prediction error at timestep 863 is tensor(2.7739e-05, grad_fn=<MseLossBackward0>)
Current timestep = 864. State = [[-0.3118119  0.0509177]]. Action = [[-0.08361365  0.06556633  0.22766072 -0.2401377 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(1.3090e-05, grad_fn=<MseLossBackward0>)
Current timestep = 865. State = [[-0.312144    0.05203626]]. Action = [[-0.13019788 -0.03791611  0.0521622  -0.03973842]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, False, True, False]
State prediction error at timestep 865 is tensor(7.5615e-05, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[-0.31258076  0.05249077]]. Action = [[-0.10382321 -0.04348215  0.09476036  0.1670357 ]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, False, True, False]
State prediction error at timestep 866 is tensor(5.4749e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of 1
Current timestep = 867. State = [[-0.31315148  0.05229214]]. Action = [[-0.19606014 -0.20632331  0.04052705  0.5320482 ]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, False, True, False]
State prediction error at timestep 867 is tensor(1.4227e-05, grad_fn=<MseLossBackward0>)
Current timestep = 868. State = [[-0.31457567  0.0518732 ]]. Action = [[-0.04718477 -0.05202837 -0.21661523 -0.13427788]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, False, True, False]
State prediction error at timestep 868 is tensor(5.1730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of 1
Current timestep = 869. State = [[-0.31516328  0.05154894]]. Action = [[ 0.19683287  0.22580773 -0.14388809 -0.6640268 ]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, False, True, False]
State prediction error at timestep 869 is tensor(2.8270e-05, grad_fn=<MseLossBackward0>)
Current timestep = 870. State = [[-0.31545666  0.05139537]]. Action = [[0.22065681 0.15336925 0.22239906 0.9819243 ]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, False, True, False]
State prediction error at timestep 870 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 871. State = [[-0.3158975   0.05110537]]. Action = [[-0.05808009  0.1832056  -0.06884651  0.27901268]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, False, True, False]
State prediction error at timestep 871 is tensor(4.6593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 872. State = [[-0.3174113   0.05058618]]. Action = [[ 0.0762611   0.07244253 -0.17561403  0.63635063]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(7.7171e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.3174783   0.05075293]]. Action = [[-0.0113039  -0.18724325  0.18686938 -0.8038971 ]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[-0.31728622  0.05102311]]. Action = [[-0.03214401  0.24369025 -0.17532815  0.6099843 ]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, False, True, False]
State prediction error at timestep 874 is tensor(6.0071e-05, grad_fn=<MseLossBackward0>)
Current timestep = 875. State = [[-0.31729674  0.05101027]]. Action = [[ 0.14140192 -0.15435944  0.11990339 -0.8600601 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, False, True, False]
State prediction error at timestep 875 is tensor(1.6549e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 875 of 1
Current timestep = 876. State = [[-0.31759048  0.05098335]]. Action = [[-0.1428967  -0.1898648  -0.22914502 -0.8513328 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(3.9256e-05, grad_fn=<MseLossBackward0>)
Current timestep = 877. State = [[-0.31746793  0.05125758]]. Action = [[ 0.22513911 -0.186971    0.22115034  0.555058  ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(3.5146e-05, grad_fn=<MseLossBackward0>)
Current timestep = 878. State = [[-0.31758368  0.05121977]]. Action = [[-0.08946799 -0.23941045  0.12409359  0.5601227 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(2.8683e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of -1
Current timestep = 879. State = [[-0.3176982   0.05125432]]. Action = [[-0.2348485  -0.03071561  0.22825205 -0.73465925]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, False, True, False]
State prediction error at timestep 879 is tensor(5.8979e-05, grad_fn=<MseLossBackward0>)
Current timestep = 880. State = [[-0.31767416  0.05129666]]. Action = [[-0.04946673 -0.066001   -0.14735036  0.9643266 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, False, True, False]
State prediction error at timestep 880 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 881. State = [[-0.31855288  0.05078405]]. Action = [[ 0.10978144 -0.02179769  0.06114408 -0.6083251 ]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, False, True, False]
State prediction error at timestep 881 is tensor(7.3974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 882. State = [[-0.31855997  0.05017896]]. Action = [[-0.05014305  0.12367624 -0.05146156  0.23126101]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(2.8437e-05, grad_fn=<MseLossBackward0>)
Current timestep = 883. State = [[-0.31907848  0.05129197]]. Action = [[ 0.1701343   0.2284736  -0.21277003  0.5704067 ]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, False, True, False]
State prediction error at timestep 883 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[-0.31951755  0.05238502]]. Action = [[-0.05161718  0.12049302  0.24236989 -0.75212574]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, False, True, False]
State prediction error at timestep 884 is tensor(4.9151e-05, grad_fn=<MseLossBackward0>)
Current timestep = 885. State = [[-0.32140413  0.05662489]]. Action = [[ 0.06019282  0.1212135  -0.02801815  0.50353396]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, False, True, False]
State prediction error at timestep 885 is tensor(5.4465e-05, grad_fn=<MseLossBackward0>)
Current timestep = 886. State = [[-0.32199183  0.05828091]]. Action = [[ 0.11658531 -0.19827308 -0.17129089  0.9439285 ]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.3238866   0.06263136]]. Action = [[ 0.12731731  0.0374074  -0.15556861  0.47551894]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, False, True, False]
State prediction error at timestep 887 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of -1
Current timestep = 888. State = [[-0.32321283  0.06361673]]. Action = [[ 0.00935549 -0.17890175 -0.19972123  0.81600046]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 889. State = [[-0.32259336  0.06411687]]. Action = [[ 0.24373531 -0.22786944  0.2042427   0.4256333 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, False, True, False]
State prediction error at timestep 889 is tensor(2.6086e-05, grad_fn=<MseLossBackward0>)
Current timestep = 890. State = [[-0.32075086  0.06687741]]. Action = [[-0.03636488  0.07183152  0.10329843 -0.32436764]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, False, True, False]
State prediction error at timestep 890 is tensor(1.6946e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 890 of -1
Current timestep = 891. State = [[-0.32114393  0.06801255]]. Action = [[-0.1992516   0.1707815   0.22042453 -0.78817   ]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, False, True, False]
State prediction error at timestep 891 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[-0.32104784  0.0689311 ]]. Action = [[-0.18723033 -0.1121558  -0.17455223  0.39975512]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, False, True, False]
State prediction error at timestep 892 is tensor(3.9368e-05, grad_fn=<MseLossBackward0>)
Current timestep = 893. State = [[-0.32090265  0.06953441]]. Action = [[ 0.00498435 -0.17331584  0.171808   -0.13886046]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, False, True, False]
State prediction error at timestep 893 is tensor(3.4837e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of -1
Current timestep = 894. State = [[-0.32106453  0.07037729]]. Action = [[ 0.14349979  0.12617445 -0.06066777 -0.7986639 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(6.0009e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.32098046  0.07129039]]. Action = [[ 0.2338118  -0.1759394  -0.22385657  0.5661329 ]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 896. State = [[-0.32102907  0.07183856]]. Action = [[-0.20630407 -0.08704609  0.06756988  0.33607697]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(2.2452e-05, grad_fn=<MseLossBackward0>)
Current timestep = 897. State = [[-0.32108462  0.0721162 ]]. Action = [[-0.14101411  0.02844372  0.13447022  0.3737836 ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(7.5200e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of -1
Current timestep = 898. State = [[-0.321083    0.07262161]]. Action = [[-0.19947551  0.14378488  0.18251723  0.4664657 ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, False, True, False]
State prediction error at timestep 898 is tensor(4.8163e-07, grad_fn=<MseLossBackward0>)
Current timestep = 899. State = [[-0.3212997  0.072764 ]]. Action = [[-0.239951    0.2238743   0.02664641 -0.27922165]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(3.4563e-06, grad_fn=<MseLossBackward0>)
Current timestep = 900. State = [[-0.32091808  0.07332185]]. Action = [[ 0.01507133  0.19376111 -0.19283982 -0.88823485]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, False, True, False]
State prediction error at timestep 900 is tensor(1.6515e-05, grad_fn=<MseLossBackward0>)
Current timestep = 901. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.18399844 -0.11414745  0.17585748  0.24411368]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of -1
Current timestep = 902. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.15743266 -0.16167223 -0.16455057  0.8637481 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(7.5541e-06, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[-0.25931677  0.0080884 ]]. Action = [[ 0.22805554  0.21112233 -0.07104924  0.17023253]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(2.7860e-06, grad_fn=<MseLossBackward0>)
Current timestep = 904. State = [[-0.25931677  0.0080884 ]]. Action = [[ 0.1881749  -0.02266696  0.13072371 -0.7615883 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(4.5694e-06, grad_fn=<MseLossBackward0>)
Current timestep = 905. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.08977991 -0.20596273  0.12954473 -0.45298523]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(3.4444e-05, grad_fn=<MseLossBackward0>)
Current timestep = 906. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.02857192 -0.22849414 -0.11081567 -0.73275524]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 906 is [True, False, False, False, True, False]
State prediction error at timestep 906 is tensor(2.5159e-05, grad_fn=<MseLossBackward0>)
Current timestep = 907. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.19400746  0.16483986 -0.21385133 -0.2896253 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 907 is [True, False, False, False, True, False]
State prediction error at timestep 907 is tensor(5.0698e-05, grad_fn=<MseLossBackward0>)
Current timestep = 908. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.04206701  0.22953713  0.13208193  0.02068818]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(5.2579e-05, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.11695242 -0.22653663  0.17214125 -0.8182922 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 909 is [True, False, False, False, True, False]
State prediction error at timestep 909 is tensor(4.2746e-05, grad_fn=<MseLossBackward0>)
Current timestep = 910. State = [[-0.25931677  0.0080884 ]]. Action = [[ 0.18034932 -0.1870873   0.11891648  0.73849607]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(2.8675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 911. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.1385481  -0.10138243 -0.23042552 -0.12427247]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 911 is [True, False, False, False, True, False]
State prediction error at timestep 911 is tensor(2.2043e-05, grad_fn=<MseLossBackward0>)
Current timestep = 912. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.233606    0.22219765  0.09144863 -0.77946645]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(1.2415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[-0.25931677  0.0080884 ]]. Action = [[ 0.15894163 -0.16730878 -0.21618994 -0.31926072]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 913 is [True, False, False, False, True, False]
State prediction error at timestep 913 is tensor(1.8203e-05, grad_fn=<MseLossBackward0>)
Current timestep = 914. State = [[-0.25931677  0.0080884 ]]. Action = [[0.14943606 0.17219424 0.21244657 0.687631  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 914 is [True, False, False, False, True, False]
State prediction error at timestep 914 is tensor(1.3743e-05, grad_fn=<MseLossBackward0>)
Current timestep = 915. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.21446517  0.07273316 -0.23362681 -0.9522307 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 915 is [True, False, False, False, True, False]
State prediction error at timestep 915 is tensor(1.0348e-05, grad_fn=<MseLossBackward0>)
Current timestep = 916. State = [[-0.25931677  0.0080884 ]]. Action = [[-0.15064113  0.18182942 -0.12005243  0.07536888]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 916 is [True, False, False, False, True, False]
State prediction error at timestep 916 is tensor(7.9487e-05, grad_fn=<MseLossBackward0>)
Current timestep = 917. State = [[-0.25931677  0.0080884 ]]. Action = [[ 0.233881    0.19707096 -0.17217273  0.724025  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 917 is [True, False, False, False, True, False]
State prediction error at timestep 917 is tensor(1.9272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 918. State = [[-0.25931677  0.0080884 ]]. Action = [[ 0.2414794   0.0941323  -0.06319815  0.89264584]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 918 is [True, False, False, False, True, False]
State prediction error at timestep 918 is tensor(1.2018e-05, grad_fn=<MseLossBackward0>)
Current timestep = 919. State = [[-0.2635969   0.01054818]]. Action = [[-0.09720841 -0.11076635 -0.05720212  0.8405373 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 919 is [True, False, False, False, True, False]
State prediction error at timestep 919 is tensor(8.4889e-06, grad_fn=<MseLossBackward0>)
Current timestep = 920. State = [[-0.26831764  0.01402907]]. Action = [[-0.04116297 -0.22085157  0.06265527  0.60450864]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 920 is [True, False, False, False, True, False]
State prediction error at timestep 920 is tensor(1.5749e-05, grad_fn=<MseLossBackward0>)
Current timestep = 921. State = [[-0.27209112  0.01891597]]. Action = [[-0.17850237  0.06964219 -0.15386172 -0.8732644 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 921 is [True, False, False, False, True, False]
State prediction error at timestep 921 is tensor(2.0741e-05, grad_fn=<MseLossBackward0>)
Current timestep = 922. State = [[-0.2753791   0.02423713]]. Action = [[ 0.06390256 -0.18718143 -0.23920278 -0.9600412 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(2.8045e-05, grad_fn=<MseLossBackward0>)
Current timestep = 923. State = [[-0.28235897  0.03287708]]. Action = [[-0.23302588  0.04669195  0.1564331  -0.23785228]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 923 is [True, False, False, False, True, False]
State prediction error at timestep 923 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 924. State = [[-0.2856594   0.03732779]]. Action = [[-0.20439233  0.04182297  0.03795263 -0.60237324]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 924 is [True, False, False, False, True, False]
State prediction error at timestep 924 is tensor(3.4505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 925. State = [[-0.28919128  0.04268923]]. Action = [[-0.18730551  0.15094072 -0.23555622  0.8380668 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 925 is [True, False, False, False, True, False]
State prediction error at timestep 925 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 926. State = [[-0.2943242   0.04824728]]. Action = [[ 0.19574398 -0.21297126 -0.05612329 -0.788668  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 926 is [True, False, False, False, True, False]
State prediction error at timestep 926 is tensor(4.2035e-07, grad_fn=<MseLossBackward0>)
Current timestep = 927. State = [[-0.29760274  0.05273956]]. Action = [[-0.2197541   0.06440738  0.20194697  0.4264754 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 927 is [True, False, False, False, True, False]
State prediction error at timestep 927 is tensor(7.5366e-07, grad_fn=<MseLossBackward0>)
Current timestep = 928. State = [[-0.30024683  0.05601098]]. Action = [[-0.09777032 -0.18544237 -0.13203217 -0.03922689]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 928 is [True, False, False, False, True, False]
State prediction error at timestep 928 is tensor(6.6320e-05, grad_fn=<MseLossBackward0>)
Current timestep = 929. State = [[-0.30630156  0.06460676]]. Action = [[-0.03384921 -0.05382973 -0.06995234  0.84298825]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 929 is [True, False, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[-0.30689752  0.06552472]]. Action = [[ 0.00087816 -0.1577445   0.18236083 -0.77795935]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 930 is [True, False, False, False, True, False]
State prediction error at timestep 930 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 931. State = [[-0.3093205   0.06794791]]. Action = [[ 0.10150602 -0.06713825 -0.09707545  0.16313529]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 931 is [True, False, False, False, True, False]
State prediction error at timestep 931 is tensor(1.2115e-06, grad_fn=<MseLossBackward0>)
Current timestep = 932. State = [[-0.30920225  0.06715782]]. Action = [[ 0.10818133 -0.01092452 -0.06706479 -0.5828865 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 932 is [True, False, False, False, True, False]
State prediction error at timestep 932 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 933. State = [[-0.30906466  0.0670146 ]]. Action = [[-0.10389321  0.22892419  0.15234274  0.13657081]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 933 is [True, False, False, False, True, False]
State prediction error at timestep 933 is tensor(5.8770e-06, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[-0.30891246  0.06682704]]. Action = [[-0.11282231  0.1536586  -0.21251807 -0.20885551]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 934 is [True, False, False, False, True, False]
State prediction error at timestep 934 is tensor(1.2229e-05, grad_fn=<MseLossBackward0>)
Current timestep = 935. State = [[-0.3087754   0.06668345]]. Action = [[-0.04064512 -0.20265143 -0.11785975  0.45425105]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 935 is [True, False, False, False, True, False]
State prediction error at timestep 935 is tensor(1.5466e-07, grad_fn=<MseLossBackward0>)
Current timestep = 936. State = [[-0.3088438   0.06656909]]. Action = [[ 0.22011602 -0.11601493  0.19721466 -0.3000756 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 936 is [True, False, False, False, True, False]
State prediction error at timestep 936 is tensor(3.7732e-05, grad_fn=<MseLossBackward0>)
Current timestep = 937. State = [[-0.30864185  0.06638007]]. Action = [[ 0.1902647  -0.08874361  0.18375635  0.06773603]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 937 is [True, False, False, False, True, False]
State prediction error at timestep 937 is tensor(7.6082e-06, grad_fn=<MseLossBackward0>)
Current timestep = 938. State = [[-0.30844003  0.06628117]]. Action = [[ 0.23576558 -0.1002211   0.08165553 -0.07183903]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 938 is [True, False, False, False, True, False]
State prediction error at timestep 938 is tensor(3.6316e-06, grad_fn=<MseLossBackward0>)
Current timestep = 939. State = [[-0.3082609   0.06577934]]. Action = [[ 0.12497422  0.10238385 -0.17507364  0.17121255]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(8.3755e-07, grad_fn=<MseLossBackward0>)
Current timestep = 940. State = [[-0.30787092  0.06607444]]. Action = [[ 0.14410675 -0.19235867  0.094374   -0.30297947]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 940 is [True, False, False, False, True, False]
State prediction error at timestep 940 is tensor(3.1154e-05, grad_fn=<MseLossBackward0>)
Current timestep = 941. State = [[-0.30717576  0.06652779]]. Action = [[ 0.11338273 -0.20230183 -0.0305239   0.7181102 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 941 is [True, False, False, False, True, False]
State prediction error at timestep 941 is tensor(1.0534e-05, grad_fn=<MseLossBackward0>)
Current timestep = 942. State = [[-0.30609053  0.06722024]]. Action = [[-0.08616352 -0.12196589 -0.21737617 -0.7081406 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 942 is [True, False, False, False, True, False]
State prediction error at timestep 942 is tensor(8.0344e-05, grad_fn=<MseLossBackward0>)
Current timestep = 943. State = [[-0.30607712  0.06680613]]. Action = [[-0.23855615 -0.05527794 -0.1629266  -0.41461766]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 943 is [True, False, False, False, True, False]
State prediction error at timestep 943 is tensor(4.1152e-05, grad_fn=<MseLossBackward0>)
Current timestep = 944. State = [[-0.30575225  0.06628238]]. Action = [[-0.06766616  0.1352635   0.10622567  0.40124786]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 944 is [True, False, False, False, True, False]
State prediction error at timestep 944 is tensor(4.1608e-06, grad_fn=<MseLossBackward0>)
Current timestep = 945. State = [[-0.30569077  0.06546658]]. Action = [[-0.02995679  0.03927594 -0.0888793   0.00631666]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 945 is [True, False, False, False, True, False]
State prediction error at timestep 945 is tensor(1.1543e-05, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[-0.30562103  0.06553825]]. Action = [[-0.14487082 -0.197953    0.10037518  0.8496134 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 946 is [True, False, False, False, True, False]
State prediction error at timestep 946 is tensor(3.2879e-05, grad_fn=<MseLossBackward0>)
Current timestep = 947. State = [[-0.30580637  0.065786  ]]. Action = [[-0.2443937 -0.1167627  0.2164073 -0.3934275]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(5.5977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 948. State = [[-0.30605114  0.06581299]]. Action = [[-0.20034559 -0.17551233  0.09916961 -0.03634018]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 948 is [True, False, False, False, True, False]
State prediction error at timestep 948 is tensor(5.9934e-06, grad_fn=<MseLossBackward0>)
Current timestep = 949. State = [[-0.30597818  0.06572025]]. Action = [[ 0.15636939 -0.17621899  0.23596793 -0.797863  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 949 is [True, False, False, False, True, False]
State prediction error at timestep 949 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 950. State = [[-0.30574882  0.06568336]]. Action = [[-0.08617289 -0.20215084 -0.17432916 -0.05404246]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 950 is [True, False, False, False, True, False]
State prediction error at timestep 950 is tensor(4.9533e-05, grad_fn=<MseLossBackward0>)
Current timestep = 951. State = [[-0.30606803  0.06575129]]. Action = [[-0.21483552  0.05131006 -0.05675286 -0.01864249]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(3.4685e-05, grad_fn=<MseLossBackward0>)
Current timestep = 952. State = [[-0.30595824  0.06574002]]. Action = [[-0.18634592 -0.05706318  0.23111033  0.6442368 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 952 is [True, False, False, False, True, False]
State prediction error at timestep 952 is tensor(2.9668e-05, grad_fn=<MseLossBackward0>)
Current timestep = 953. State = [[-0.305843    0.06571423]]. Action = [[-0.19761969 -0.11621316 -0.13841072  0.41298223]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 953 is [True, False, False, False, True, False]
State prediction error at timestep 953 is tensor(2.7809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 954. State = [[-0.30604595  0.06571012]]. Action = [[ 0.02375656 -0.17622697 -0.12480751  0.5919633 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 954 is [True, False, False, False, True, False]
State prediction error at timestep 954 is tensor(2.9127e-06, grad_fn=<MseLossBackward0>)
Current timestep = 955. State = [[-0.30581927  0.06564213]]. Action = [[ 0.10922319 -0.07519233 -0.07639462  0.39240742]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 955 is [True, False, False, False, True, False]
State prediction error at timestep 955 is tensor(1.3793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 956. State = [[-0.3055063   0.06524468]]. Action = [[-0.22897644  0.07464448  0.11749074  0.85114884]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 956 is [True, False, False, False, True, False]
State prediction error at timestep 956 is tensor(3.2537e-05, grad_fn=<MseLossBackward0>)
Current timestep = 957. State = [[-0.30522355  0.06495418]]. Action = [[-0.19585143  0.21689123  0.03309757  0.9448155 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 957 is [True, False, False, False, True, False]
State prediction error at timestep 957 is tensor(2.5813e-05, grad_fn=<MseLossBackward0>)
Current timestep = 958. State = [[-0.30497214  0.06454924]]. Action = [[ 0.16255677 -0.10947034 -0.0074186  -0.23212314]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 958 is [True, False, False, False, True, False]
State prediction error at timestep 958 is tensor(6.1014e-05, grad_fn=<MseLossBackward0>)
Current timestep = 959. State = [[-0.30493465  0.06427655]]. Action = [[ 0.17287856 -0.13468343 -0.01683694  0.6167445 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 959 is [True, False, False, False, True, False]
State prediction error at timestep 959 is tensor(1.2717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 960. State = [[-0.3046472   0.06404579]]. Action = [[ 0.04727539 -0.21987662  0.06140029 -0.31301755]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 960 is [True, False, False, False, True, False]
State prediction error at timestep 960 is tensor(3.1223e-05, grad_fn=<MseLossBackward0>)
Current timestep = 961. State = [[-0.3043966   0.06382192]]. Action = [[-0.12392509  0.21044207 -0.12227538  0.101385  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 961 is [True, False, False, False, True, False]
State prediction error at timestep 961 is tensor(1.2922e-06, grad_fn=<MseLossBackward0>)
Current timestep = 962. State = [[-0.30430704  0.06340699]]. Action = [[ 0.10883784  0.16676575 -0.13975753 -0.5650972 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 962 is [True, False, False, False, True, False]
State prediction error at timestep 962 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 963. State = [[-0.3038349   0.06300842]]. Action = [[ 0.21668467 -0.05777049  0.21049571 -0.8991849 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 963 is [True, False, False, False, True, False]
State prediction error at timestep 963 is tensor(4.6176e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 963 of 1
Current timestep = 964. State = [[-0.30390033  0.06276016]]. Action = [[-0.21448217 -0.21849263 -0.07035713  0.50121236]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 964 is [True, False, False, False, True, False]
State prediction error at timestep 964 is tensor(2.5674e-05, grad_fn=<MseLossBackward0>)
Current timestep = 965. State = [[-0.3035056   0.06253706]]. Action = [[ 0.11147237  0.07724866 -0.19204119  0.6343404 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(2.1240e-05, grad_fn=<MseLossBackward0>)
Current timestep = 966. State = [[-0.30295196  0.06281604]]. Action = [[ 0.03009477  0.24171352  0.02112794 -0.3352968 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 966 is [True, False, False, False, True, False]
State prediction error at timestep 966 is tensor(4.6692e-05, grad_fn=<MseLossBackward0>)
Current timestep = 967. State = [[-0.30268762  0.06290112]]. Action = [[ 0.07368851 -0.1835076  -0.19090562  0.11701107]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 967 is [True, False, False, False, True, False]
State prediction error at timestep 967 is tensor(5.2813e-07, grad_fn=<MseLossBackward0>)
Current timestep = 968. State = [[-0.30265176  0.06295355]]. Action = [[ 0.14574605 -0.0077655  -0.2079142   0.5181899 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 968 is [True, False, False, False, True, False]
State prediction error at timestep 968 is tensor(2.6412e-05, grad_fn=<MseLossBackward0>)
Current timestep = 969. State = [[-0.30226114  0.06313651]]. Action = [[-0.04225379 -0.20355134 -0.01585607 -0.7308635 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 969 is [True, False, False, False, True, False]
State prediction error at timestep 969 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 970. State = [[-0.30180892  0.06333159]]. Action = [[ 0.00752527 -0.09770843  0.10212591 -0.12275863]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 970 is [True, False, False, False, True, False]
State prediction error at timestep 970 is tensor(1.8904e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.30115688  0.06222881]]. Action = [[ 0.12904817  0.12753728  0.17090291 -0.3046664 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 971 is [True, False, False, False, True, False]
State prediction error at timestep 971 is tensor(8.4004e-05, grad_fn=<MseLossBackward0>)
Current timestep = 972. State = [[-0.29768407  0.06307844]]. Action = [[ 0.04285282  0.00918815 -0.22640832  0.17210257]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 972 is [True, False, False, False, True, False]
State prediction error at timestep 972 is tensor(2.4350e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 972 of 1
Current timestep = 973. State = [[-0.2964603   0.06341714]]. Action = [[ 0.04554871 -0.20829113  0.06792805 -0.39250386]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 973 is [True, False, False, False, True, False]
State prediction error at timestep 973 is tensor(6.0564e-05, grad_fn=<MseLossBackward0>)
Current timestep = 974. State = [[-0.2955213   0.06354711]]. Action = [[-0.09952395  0.19395286 -0.16940182  0.93171453]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 974 is [True, False, False, False, True, False]
State prediction error at timestep 974 is tensor(4.6189e-05, grad_fn=<MseLossBackward0>)
Current timestep = 975. State = [[-0.29441544  0.06359342]]. Action = [[-0.2438272   0.18578494  0.09508598  0.7119175 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 975 is [True, False, False, False, True, False]
State prediction error at timestep 975 is tensor(3.4256e-05, grad_fn=<MseLossBackward0>)
Current timestep = 976. State = [[-0.29340827  0.0639796 ]]. Action = [[-0.22449936 -0.10532339 -0.17891248  0.26121783]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 976 is [True, False, False, False, True, False]
State prediction error at timestep 976 is tensor(4.8886e-05, grad_fn=<MseLossBackward0>)
Current timestep = 977. State = [[-0.29210275  0.06433754]]. Action = [[ 0.02827722  0.2323491  -0.09157909 -0.8057196 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 977 is [True, False, False, False, True, False]
State prediction error at timestep 977 is tensor(6.4797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 978. State = [[-0.29106236  0.06446055]]. Action = [[ 0.19800031  0.06482807 -0.10131884 -0.50734556]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 978 is [True, False, False, False, True, False]
State prediction error at timestep 978 is tensor(8.6112e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of 1
Current timestep = 979. State = [[-0.2900749   0.06473096]]. Action = [[-0.11072156 -0.22120018 -0.10124847 -0.18305439]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 979 is [True, False, False, False, True, False]
State prediction error at timestep 979 is tensor(1.2879e-05, grad_fn=<MseLossBackward0>)
Current timestep = 980. State = [[-0.28930935  0.06475243]]. Action = [[ 0.08718607 -0.22430012  0.23834643  0.906126  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 980 is [True, False, False, False, True, False]
State prediction error at timestep 980 is tensor(4.3160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 981. State = [[-0.2887406   0.06478494]]. Action = [[-0.19662657  0.1265136  -0.23983306 -0.71406883]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 981 is [True, False, False, False, True, False]
State prediction error at timestep 981 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 982. State = [[-0.28825143  0.06492266]]. Action = [[-0.20111375  0.14502752 -0.22277945 -0.6790011 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 982 is [True, False, False, False, True, False]
State prediction error at timestep 982 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of 1
Current timestep = 983. State = [[-0.28781703  0.06497258]]. Action = [[-0.02486551 -0.19808704  0.05239117 -0.7813733 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 983 is [True, False, False, False, True, False]
State prediction error at timestep 983 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 984. State = [[-0.28757057  0.06505254]]. Action = [[-0.03430885 -0.23551327 -0.17973597  0.61230516]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 984 is [True, False, False, False, True, False]
State prediction error at timestep 984 is tensor(1.4348e-05, grad_fn=<MseLossBackward0>)
Current timestep = 985. State = [[-0.2875948   0.06528816]]. Action = [[ 0.20275694  0.05053452 -0.01714578 -0.69167715]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 985 is [True, False, False, False, True, False]
State prediction error at timestep 985 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 985 of 1
Current timestep = 986. State = [[-0.2875251   0.06524345]]. Action = [[ 0.00672352  0.21995145  0.24481007 -0.3866297 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 986 is [True, False, False, False, True, False]
State prediction error at timestep 986 is tensor(9.2035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 987. State = [[-0.28759223  0.0652343 ]]. Action = [[ 0.05397242  0.21984935  0.18703845 -0.06079781]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 987 is [True, False, False, False, True, False]
State prediction error at timestep 987 is tensor(1.3608e-05, grad_fn=<MseLossBackward0>)
Current timestep = 988. State = [[-0.28757197  0.06538387]]. Action = [[ 0.21429783 -0.22109675 -0.01062196  0.14391088]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 988 is [True, False, False, False, True, False]
State prediction error at timestep 988 is tensor(4.5226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 989. State = [[-0.2873501   0.06543001]]. Action = [[-0.13384892 -0.06531237 -0.1594117   0.25290847]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 989 is [True, False, False, False, True, False]
State prediction error at timestep 989 is tensor(4.7994e-05, grad_fn=<MseLossBackward0>)
Current timestep = 990. State = [[-0.2872312   0.06540086]]. Action = [[-0.15898995  0.03958538  0.12333262 -0.49318683]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 990 is [True, False, False, False, True, False]
State prediction error at timestep 990 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 990 of 1
Current timestep = 991. State = [[-0.2873954   0.06535298]]. Action = [[-0.18325292 -0.07842064 -0.08470784  0.59560287]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(3.1239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 992. State = [[-0.28683564  0.06553199]]. Action = [[-0.23716784  0.21346599  0.03423107  0.55137527]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 992 is [True, False, False, False, True, False]
State prediction error at timestep 992 is tensor(5.0195e-05, grad_fn=<MseLossBackward0>)
Current timestep = 993. State = [[-0.286834    0.06547024]]. Action = [[ 0.19910002 -0.09452546  0.12521172  0.71850145]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 993 is [True, False, False, False, True, False]
State prediction error at timestep 993 is tensor(2.0603e-06, grad_fn=<MseLossBackward0>)
Current timestep = 994. State = [[-0.286789    0.06552255]]. Action = [[0.05952251 0.15648162 0.11483777 0.4439535 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 994 is [True, False, False, False, True, False]
State prediction error at timestep 994 is tensor(5.8426e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 994 of 1
Current timestep = 995. State = [[-0.286789    0.06552255]]. Action = [[-0.08522078  0.24557334  0.2337628  -0.81849176]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 995 is [True, False, False, False, True, False]
State prediction error at timestep 995 is tensor(7.9878e-05, grad_fn=<MseLossBackward0>)
Current timestep = 996. State = [[-0.2867864  0.0654686]]. Action = [[-0.21133223  0.17359978  0.02878967 -0.18747437]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 996 is [True, False, False, False, True, False]
State prediction error at timestep 996 is tensor(4.7020e-06, grad_fn=<MseLossBackward0>)
Current timestep = 997. State = [[-0.286797    0.06557379]]. Action = [[ 0.06733677  0.09794584  0.06881365 -0.62879777]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 997 is [True, False, False, False, True, False]
State prediction error at timestep 997 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 997 of 1
Current timestep = 998. State = [[-0.28626272  0.0663912 ]]. Action = [[ 0.23938036 -0.2357388   0.01533547  0.5077512 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 998 is [True, False, False, False, True, False]
State prediction error at timestep 998 is tensor(1.4994e-05, grad_fn=<MseLossBackward0>)
Current timestep = 999. State = [[-0.28556252  0.06725065]]. Action = [[ 0.14185897 -0.09102249 -0.15034173 -0.4095477 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 999 is [True, False, False, False, True, False]
State prediction error at timestep 999 is tensor(7.7108e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1000. State = [[-0.28494102  0.06806879]]. Action = [[ 0.15466106  0.1303072  -0.02837874 -0.47417873]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1000 is [True, False, False, False, True, False]
State prediction error at timestep 1000 is tensor(7.6507e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1001. State = [[-0.28446078  0.06839908]]. Action = [[ 0.21763512 -0.19485971  0.03833205  0.25540805]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1001 is [True, False, False, False, True, False]
State prediction error at timestep 1001 is tensor(5.4968e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1002. State = [[-0.28440797  0.06859876]]. Action = [[-0.00488827  0.18555495  0.20683974  0.33033466]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1002 is [True, False, False, False, True, False]
State prediction error at timestep 1002 is tensor(8.4640e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of 1
Current timestep = 1003. State = [[-0.28407538  0.06895175]]. Action = [[ 0.03990558  0.14337087 -0.12971817  0.1647911 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1003 is [True, False, False, False, True, False]
State prediction error at timestep 1003 is tensor(1.7264e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1004. State = [[-0.28382856  0.06915121]]. Action = [[ 0.16992706  0.08743995 -0.01773445  0.7921593 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1004 is [True, False, False, False, True, False]
State prediction error at timestep 1004 is tensor(8.9228e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1005. State = [[-0.28376323  0.06928629]]. Action = [[ 0.1171051  -0.22817339  0.17270663 -0.3688395 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1005 is [True, False, False, False, True, False]
State prediction error at timestep 1005 is tensor(4.7777e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1006. State = [[-0.28327957  0.06961657]]. Action = [[ 0.241875   -0.21417294  0.05078426  0.6723728 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1006 is [True, False, False, False, True, False]
State prediction error at timestep 1006 is tensor(2.2653e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.28327978  0.06956062]]. Action = [[ 0.20751733 -0.2075782  -0.15831637 -0.02875167]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1007 is [True, False, False, False, True, False]
State prediction error at timestep 1007 is tensor(3.6515e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1008. State = [[-0.28320938  0.07005643]]. Action = [[0.04668975 0.06112105 0.21096814 0.5713463 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1008 is [True, False, False, False, True, False]
State prediction error at timestep 1008 is tensor(1.2884e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1009. State = [[-0.28267857  0.07080393]]. Action = [[ 0.03002515 -0.23753095 -0.07848291  0.5217893 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1009 is [True, False, False, False, True, False]
State prediction error at timestep 1009 is tensor(2.8248e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1010. State = [[-0.282576    0.07120375]]. Action = [[ 0.1355812  -0.01368959  0.2158643  -0.04874879]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1010 is [True, False, False, False, True, False]
State prediction error at timestep 1010 is tensor(1.9504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1011. State = [[-0.28194517  0.07254035]]. Action = [[-0.12146741  0.01880449 -0.2376353   0.4424286 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1011 is [True, False, False, False, True, False]
State prediction error at timestep 1011 is tensor(3.0075e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1012. State = [[-0.28235456  0.07325865]]. Action = [[ 0.16588819  0.02926517  0.11540592 -0.403574  ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1012 is [True, False, False, False, True, False]
State prediction error at timestep 1012 is tensor(8.7356e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1013. State = [[-0.28276217  0.07385059]]. Action = [[ 0.20830217 -0.00673723  0.15629655  0.33522153]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1013 is [True, False, False, False, True, False]
State prediction error at timestep 1013 is tensor(3.1255e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1014. State = [[-0.28377235  0.07576006]]. Action = [[-0.06841716 -0.02548368  0.06511143  0.7909937 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1014 is [True, False, False, False, True, False]
State prediction error at timestep 1014 is tensor(2.5452e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1014 of 1
Current timestep = 1015. State = [[-0.28397462  0.07594866]]. Action = [[-0.16903855 -0.15838075  0.20740071 -0.65515786]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1015 is [True, False, False, False, True, False]
State prediction error at timestep 1015 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1016. State = [[-0.28427652  0.07652326]]. Action = [[ 0.00481537 -0.08187968  0.07626078 -0.19260126]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1016 is [True, False, False, False, True, False]
State prediction error at timestep 1016 is tensor(3.8048e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1017. State = [[-0.28422374  0.07620712]]. Action = [[ 0.09641224  0.24143171 -0.02897228 -0.5546523 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1017 is [True, False, False, False, True, False]
State prediction error at timestep 1017 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of 1
Current timestep = 1018. State = [[-0.2841575   0.07584623]]. Action = [[0.05679697 0.18707791 0.22442251 0.9410962 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1018 is [True, False, False, False, True, False]
State prediction error at timestep 1018 is tensor(2.6438e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1019. State = [[-0.28412637  0.07540816]]. Action = [[ 0.02423164 -0.05064507 -0.2346037  -0.29733157]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1019 is [True, False, False, False, True, False]
State prediction error at timestep 1019 is tensor(6.5747e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1020. State = [[-0.28400713  0.07485074]]. Action = [[-0.1350218  0.2204302 -0.2122696 -0.7886263]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1020 is [True, False, False, False, True, False]
State prediction error at timestep 1020 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1021. State = [[-0.28399596  0.07369015]]. Action = [[-0.08332758 -0.01815781 -0.00710423  0.18062901]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1021 is [True, False, False, False, True, False]
State prediction error at timestep 1021 is tensor(4.4431e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1022. State = [[-0.28479037  0.0726689 ]]. Action = [[-0.07466838  0.1224384  -0.17356394 -0.2184819 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1022 is [True, False, False, False, True, False]
State prediction error at timestep 1022 is tensor(5.3007e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1022 of 1
Current timestep = 1023. State = [[-0.28573444  0.07336181]]. Action = [[-0.22129826  0.14031798  0.02897149  0.8216276 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1023 is [True, False, False, False, True, False]
State prediction error at timestep 1023 is tensor(6.9091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1024. State = [[-0.28622374  0.07390454]]. Action = [[-0.23382507  0.1485849  -0.20920871  0.46620512]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1024 is [True, False, False, False, True, False]
State prediction error at timestep 1024 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1025. State = [[-0.28760704  0.07574342]]. Action = [[ 0.05807683  0.04124886  0.13783973 -0.6267046 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1025 is [True, False, False, False, True, False]
State prediction error at timestep 1025 is tensor(9.2526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1025 of 1
Current timestep = 1026. State = [[-0.28773928  0.07615224]]. Action = [[-0.21142952  0.12793869  0.09448516 -0.19388556]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1026 is [True, False, False, False, True, False]
State prediction error at timestep 1026 is tensor(4.9040e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1027. State = [[-0.2884802   0.07744154]]. Action = [[-0.10891503 -0.00074838  0.2031056  -0.08115065]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1027 is [True, False, False, False, True, False]
State prediction error at timestep 1027 is tensor(1.3844e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of 1
Current timestep = 1028. State = [[-0.28909677  0.0779338 ]]. Action = [[-0.05830657 -0.14460859 -0.17152546  0.9205172 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1028 is [True, False, False, False, True, False]
State prediction error at timestep 1028 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1029. State = [[-0.2904698  0.0794717]]. Action = [[-0.07794526  0.0952805  -0.12803373  0.78803325]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1029 is [True, False, False, False, True, False]
State prediction error at timestep 1029 is tensor(5.9102e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1030. State = [[-0.29147986  0.08067498]]. Action = [[ 0.14931655 -0.0175781  -0.00765471 -0.3233173 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1030 is [True, False, False, False, True, False]
State prediction error at timestep 1030 is tensor(9.7621e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1031. State = [[-0.2923881   0.08166523]]. Action = [[ 0.03215009 -0.19800663 -0.13631505 -0.8864447 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1031 is [True, False, False, False, True, False]
State prediction error at timestep 1031 is tensor(8.5311e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1032. State = [[-0.29288942  0.08268023]]. Action = [[ 0.21403545 -0.18044984 -0.05817072 -0.63446873]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1032 is [True, False, False, False, True, False]
State prediction error at timestep 1032 is tensor(8.1480e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1033. State = [[-0.29321468  0.0832869 ]]. Action = [[0.19431567 0.09442416 0.04612738 0.5596957 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1033 is [True, False, False, False, True, False]
State prediction error at timestep 1033 is tensor(8.1111e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1034. State = [[-0.2950706   0.08543438]]. Action = [[-0.11569947 -0.07099524  0.09275258 -0.01943022]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1034 is [True, False, False, False, True, False]
State prediction error at timestep 1034 is tensor(3.9758e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of 1
Current timestep = 1035. State = [[-0.29784006  0.0852657 ]]. Action = [[-0.13143776  0.07350862  0.1214174   0.8444457 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1035 is [True, False, False, False, True, False]
State prediction error at timestep 1035 is tensor(5.3502e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1035 of -1
Current timestep = 1036. State = [[-0.2988755   0.08587999]]. Action = [[ 5.3712726e-04 -1.8865184e-01 -1.3086125e-01  5.4253006e-01]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1036 is [True, False, False, False, True, False]
State prediction error at timestep 1036 is tensor(7.5639e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1037. State = [[-0.30009204  0.08659404]]. Action = [[ 0.2163463   0.05397862 -0.07867864  0.7326709 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1037 is [True, False, False, False, True, False]
State prediction error at timestep 1037 is tensor(1.7766e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1038. State = [[-0.3008368   0.08693636]]. Action = [[-0.14813265 -0.23349886 -0.19387312 -0.24586326]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1038 is [True, False, False, False, True, False]
State prediction error at timestep 1038 is tensor(8.1312e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1039. State = [[-0.3018859   0.08736037]]. Action = [[ 0.1384381  -0.03728308  0.19608027 -0.66256964]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1039 is [True, False, False, False, True, False]
State prediction error at timestep 1039 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of -1
Current timestep = 1040. State = [[-0.3055985   0.08893325]]. Action = [[-0.06224716 -0.09231141  0.15009558  0.9865513 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1040 is [True, False, False, False, True, False]
State prediction error at timestep 1040 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of -1
Current timestep = 1041. State = [[-0.30617377  0.08847974]]. Action = [[ 0.11404213 -0.215227   -0.18374498  0.2546153 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1041 is [True, False, False, False, True, False]
State prediction error at timestep 1041 is tensor(5.1322e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1042. State = [[-0.3069733   0.08786304]]. Action = [[-0.1352089   0.01784784 -0.13583387 -0.4687274 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1042 is [True, False, False, False, True, False]
State prediction error at timestep 1042 is tensor(8.0039e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1043. State = [[-0.30771896  0.08750988]]. Action = [[-0.24810193  0.05012634  0.03267318  0.8276477 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1043 is [True, False, False, False, True, False]
State prediction error at timestep 1043 is tensor(8.6630e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1044. State = [[-0.30877933  0.08722465]]. Action = [[-0.15896036 -0.17433912 -0.19993089  0.01315045]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1044 is [True, False, False, False, True, False]
State prediction error at timestep 1044 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1045. State = [[-0.30974472  0.08650872]]. Action = [[-0.13283066 -0.22458977  0.16073507  0.56082976]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1045 is [True, False, False, False, True, False]
State prediction error at timestep 1045 is tensor(4.2650e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1045 of -1
Current timestep = 1046. State = [[-0.310673    0.08605905]]. Action = [[ 0.21502405  0.15602565 -0.13807113  0.5448544 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1046 is [True, False, False, False, True, False]
State prediction error at timestep 1046 is tensor(2.0937e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1047. State = [[-0.31139362  0.08573566]]. Action = [[-0.08451629  0.16247082  0.09498122 -0.2650069 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1047 is [True, False, False, False, True, False]
State prediction error at timestep 1047 is tensor(6.5504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1048. State = [[-0.31265047  0.08493966]]. Action = [[-0.22172022 -0.19322145 -0.18191464 -0.91438055]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1048 is [True, False, False, False, True, False]
State prediction error at timestep 1048 is tensor(7.8145e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1048 of -1
Current timestep = 1049. State = [[-0.31312948  0.08469   ]]. Action = [[-0.14370744 -0.21980394  0.09506756 -0.08926201]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1049 is [True, False, False, False, True, False]
State prediction error at timestep 1049 is tensor(4.4960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1050. State = [[-0.31357387  0.08435504]]. Action = [[-0.18835156 -0.05151568 -0.17655778 -0.00608319]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1050 is [True, False, False, False, True, False]
State prediction error at timestep 1050 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1051. State = [[-0.31397873  0.08411953]]. Action = [[0.00841692 0.21231166 0.05339846 0.689219  ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1051 is [True, False, False, False, True, False]
State prediction error at timestep 1051 is tensor(6.9786e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1052. State = [[-0.3142747   0.08392692]]. Action = [[0.2256369  0.1834163  0.06108168 0.76381576]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1052 is [True, False, False, False, True, False]
State prediction error at timestep 1052 is tensor(2.2337e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[-0.31476584  0.0835436 ]]. Action = [[ 0.1704107  -0.19652028  0.19563764  0.72249615]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1053 is [True, False, False, False, True, False]
State prediction error at timestep 1053 is tensor(2.7815e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1054. State = [[-0.31598252  0.08299144]]. Action = [[0.09761882 0.02599341 0.10794812 0.5896535 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1054 is [True, False, False, False, True, False]
State prediction error at timestep 1054 is tensor(1.4565e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1055. State = [[-0.31585494  0.08310191]]. Action = [[ 0.17162663 -0.20449561  0.22166091  0.6310561 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1055 is [True, False, False, False, True, False]
State prediction error at timestep 1055 is tensor(2.4442e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1056. State = [[-0.31575903  0.08302202]]. Action = [[ 0.18508258 -0.03266436 -0.15217404  0.332085  ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1056 is [True, False, False, False, True, False]
State prediction error at timestep 1056 is tensor(1.0356e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1057. State = [[-0.31582624  0.08300919]]. Action = [[ 0.08335131 -0.14522463  0.03130293 -0.39560986]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1057 is [True, False, False, False, True, False]
State prediction error at timestep 1057 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1058. State = [[-0.31596264  0.08301124]]. Action = [[-0.14379716 -0.15559547 -0.0182078   0.05774045]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1058 is [True, False, False, False, True, False]
State prediction error at timestep 1058 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1059. State = [[-0.3157101   0.08276977]]. Action = [[ 0.07574677 -0.12190911 -0.03258178 -0.6986177 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1059 is [True, False, False, False, True, False]
State prediction error at timestep 1059 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[-0.31500795  0.08002277]]. Action = [[ 0.0350368   0.01379541 -0.23030697  0.26053298]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1060 is [True, False, False, False, True, False]
State prediction error at timestep 1060 is tensor(1.5919e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1060 of -1
Current timestep = 1061. State = [[-0.31437957  0.07892597]]. Action = [[-0.06447196  0.08288789 -0.24761319  0.80452764]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1061 is [True, False, False, False, True, False]
State prediction error at timestep 1061 is tensor(6.7890e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.3145013   0.07917651]]. Action = [[-0.24651366  0.02275226  0.0173288   0.04776657]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1062 is [True, False, False, False, True, False]
State prediction error at timestep 1062 is tensor(3.5974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1063. State = [[-0.31469715  0.07935752]]. Action = [[-0.02223963  0.2359007  -0.23942363 -0.0363608 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1063 is [True, False, False, False, True, False]
State prediction error at timestep 1063 is tensor(4.6436e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1064. State = [[-0.31472638  0.07952489]]. Action = [[ 0.05789244 -0.08943093  0.1173439  -0.0068171 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1064 is [True, False, False, False, True, False]
State prediction error at timestep 1064 is tensor(3.7470e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of -1
Current timestep = 1065. State = [[-0.31428936  0.07812787]]. Action = [[-0.01724102 -0.09089831  0.11081553 -0.08784735]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1065 is [True, False, False, False, True, False]
State prediction error at timestep 1065 is tensor(2.6589e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of -1
Current timestep = 1066. State = [[-0.31366992  0.07693893]]. Action = [[-0.19601059  0.19586572  0.06829229  0.4036225 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1066 is [True, False, False, False, True, False]
State prediction error at timestep 1066 is tensor(3.8529e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1067. State = [[-0.31341565  0.07411275]]. Action = [[ 0.01341185  0.0152716  -0.18643063  0.17986763]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1067 is [True, False, False, False, True, False]
State prediction error at timestep 1067 is tensor(3.3637e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1067 of -1
Current timestep = 1068. State = [[-0.313217    0.07338183]]. Action = [[-0.22436723  0.02982455 -0.13679284 -0.5544195 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1068 is [True, False, False, False, True, False]
State prediction error at timestep 1068 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1069. State = [[-0.31340578  0.07317276]]. Action = [[ 0.21679449 -0.24325341  0.18260852 -0.61636925]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1069 is [True, False, False, False, True, False]
State prediction error at timestep 1069 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1070. State = [[-0.31300536  0.07208993]]. Action = [[ 0.07613385  0.03503919 -0.15504953 -0.46175122]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1070 is [True, False, False, False, True, False]
State prediction error at timestep 1070 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1070 of -1
Current timestep = 1071. State = [[-0.31288546  0.07238962]]. Action = [[-0.19902453  0.20618871  0.00173748 -0.62031746]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1071 is [True, False, False, False, True, False]
State prediction error at timestep 1071 is tensor(7.8485e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1072. State = [[-0.31199318  0.07263102]]. Action = [[ 0.02741888 -0.05007397 -0.21717884  0.73978984]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1072 is [True, False, False, False, True, False]
State prediction error at timestep 1072 is tensor(1.2614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1072 of -1
Current timestep = 1073. State = [[-0.31149518  0.07243058]]. Action = [[-0.20241924 -0.16566496 -0.10445347  0.23152661]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1073 is [True, False, False, False, True, False]
State prediction error at timestep 1073 is tensor(4.0221e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1074. State = [[-0.3111854   0.07212017]]. Action = [[-0.15257946 -0.07807291  0.16271347  0.5097439 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1074 is [True, False, False, False, True, False]
State prediction error at timestep 1074 is tensor(3.0105e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1075. State = [[-0.31111372  0.07197919]]. Action = [[ 0.09685117  0.14087039 -0.10954291 -0.7769157 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1075 is [True, False, False, False, True, False]
State prediction error at timestep 1075 is tensor(8.7872e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1076. State = [[-0.3110251   0.07193824]]. Action = [[ 0.2194483  -0.02415773 -0.13843729 -0.20060086]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1076 is [True, False, False, False, True, False]
State prediction error at timestep 1076 is tensor(6.3362e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1076 of -1
Current timestep = 1077. State = [[-0.31076816  0.07171187]]. Action = [[-0.23169053 -0.10202409  0.22951588 -0.55216354]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1077 is [True, False, False, False, True, False]
State prediction error at timestep 1077 is tensor(8.5162e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1078. State = [[-0.3105108   0.07153049]]. Action = [[ 0.1457839  -0.24017364 -0.14490822 -0.2513101 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1078 is [True, False, False, False, True, False]
State prediction error at timestep 1078 is tensor(8.5318e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1079. State = [[-0.3105288   0.07141879]]. Action = [[ 0.20788515 -0.18982548 -0.14303985  0.8708341 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1079 is [True, False, False, False, True, False]
State prediction error at timestep 1079 is tensor(2.4771e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1080. State = [[-0.31036702  0.07134067]]. Action = [[ 0.13995558  0.13588884 -0.04622127  0.27742243]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1080 is [True, False, False, False, True, False]
State prediction error at timestep 1080 is tensor(1.1730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1080 of -1
Current timestep = 1081. State = [[-0.3103317   0.07118519]]. Action = [[ 0.22502947 -0.06210263  0.22994614 -0.60383785]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1081 is [True, False, False, False, True, False]
State prediction error at timestep 1081 is tensor(6.7210e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1082. State = [[-0.31038016  0.07106198]]. Action = [[-0.17791493  0.19792557  0.08848655  0.24997711]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1082 is [True, False, False, False, True, False]
State prediction error at timestep 1082 is tensor(2.0691e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1083. State = [[-0.31032026  0.07093507]]. Action = [[ 0.21173626 -0.22953887 -0.16808222  0.8740828 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1083 is [True, False, False, False, True, False]
State prediction error at timestep 1083 is tensor(1.9098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1084. State = [[-0.31030455  0.07067018]]. Action = [[ 0.01482356  0.00960234 -0.12877423  0.5417582 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1084 is [True, False, False, False, True, False]
State prediction error at timestep 1084 is tensor(2.9644e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1084 of -1
Current timestep = 1085. State = [[-0.31008443  0.07091282]]. Action = [[ 0.04682899  0.12761271 -0.23879257 -0.4123022 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1085 is [True, False, False, False, True, False]
State prediction error at timestep 1085 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of -1
Current timestep = 1086. State = [[-0.30857408  0.07253419]]. Action = [[0.08566403 0.1006884  0.12747243 0.7266922 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1086 is [True, False, False, False, True, False]
State prediction error at timestep 1086 is tensor(1.5331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1086 of -1
Current timestep = 1087. State = [[-0.30786502  0.07350627]]. Action = [[ 0.14614001 -0.22531794  0.14132482  0.8902637 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1087 is [True, False, False, False, True, False]
State prediction error at timestep 1087 is tensor(2.5995e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1088. State = [[-0.30688637  0.07456648]]. Action = [[ 0.14659429 -0.09250107 -0.15676738  0.81969404]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1088 is [True, False, False, False, True, False]
State prediction error at timestep 1088 is tensor(1.8621e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1089. State = [[-0.30644158  0.07505773]]. Action = [[-0.22229314  0.15592271 -0.21096514 -0.87241745]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1089 is [True, False, False, False, True, False]
State prediction error at timestep 1089 is tensor(8.6869e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1090. State = [[-0.30579597  0.07551242]]. Action = [[ 0.18691057 -0.00909787  0.1953536   0.07195473]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1090 is [True, False, False, False, True, False]
State prediction error at timestep 1090 is tensor(1.4376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1091. State = [[-0.3051026   0.07608201]]. Action = [[-0.09005822 -0.18544447  0.22690213  0.02859259]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1091 is [True, False, False, False, True, False]
State prediction error at timestep 1091 is tensor(6.1296e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of -1
Current timestep = 1092. State = [[-0.3042711   0.07663017]]. Action = [[ 0.2177155  -0.0892911   0.02284953  0.49287975]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1092 is [True, False, False, False, True, False]
State prediction error at timestep 1092 is tensor(3.0378e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1093. State = [[-0.3016694   0.07834174]]. Action = [[-0.0650524  -0.1328792   0.07281485 -0.80694497]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1093 is [True, False, False, False, True, False]
State prediction error at timestep 1093 is tensor(7.4176e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1094. State = [[-0.30152446  0.07799417]]. Action = [[-0.00127599  0.11415905  0.14937264 -0.96661615]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1094 is [True, False, False, False, True, False]
State prediction error at timestep 1094 is tensor(1.4524e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of -1
Current timestep = 1095. State = [[-0.3016496   0.07820562]]. Action = [[-0.24025248 -0.01555125 -0.04737628 -0.4051788 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1095 is [True, False, False, False, True, False]
State prediction error at timestep 1095 is tensor(4.3263e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1096. State = [[-0.30180457  0.0789395 ]]. Action = [[0.11539876 0.04596141 0.06420386 0.18148875]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1096 is [True, False, False, False, True, False]
State prediction error at timestep 1096 is tensor(3.9702e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1097. State = [[-0.30110458  0.07935014]]. Action = [[-0.1961843   0.21587044  0.24268535 -0.69719976]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1097 is [True, False, False, False, True, False]
State prediction error at timestep 1097 is tensor(9.5510e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1098. State = [[-0.30049083  0.07966936]]. Action = [[0.1755771  0.10177124 0.22092384 0.63231814]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1098 is [True, False, False, False, True, False]
State prediction error at timestep 1098 is tensor(2.5746e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1099. State = [[-0.30021986  0.07992513]]. Action = [[-0.08774939  0.1665524  -0.17780165  0.5111598 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1099 is [True, False, False, False, True, False]
State prediction error at timestep 1099 is tensor(3.3108e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1100. State = [[-0.29969752  0.08014342]]. Action = [[-0.00213246 -0.19296424  0.10922772  0.1785469 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1100 is [True, False, False, False, True, False]
State prediction error at timestep 1100 is tensor(9.1153e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1101. State = [[-0.29917535  0.08053283]]. Action = [[-0.24263479 -0.02053042  0.08124858  0.4606917 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1101 is [True, False, False, False, True, False]
State prediction error at timestep 1101 is tensor(3.4557e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1102. State = [[-0.29790774  0.08138239]]. Action = [[-0.00969248 -0.02383098 -0.11847462 -0.57001686]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1102 is [True, False, False, False, True, False]
State prediction error at timestep 1102 is tensor(6.2384e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of -1
Current timestep = 1103. State = [[-0.29721543  0.08156589]]. Action = [[-0.09039143 -0.00868937 -0.18723558 -0.77619094]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1103 is [True, False, False, False, True, False]
State prediction error at timestep 1103 is tensor(6.7591e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of 1
Current timestep = 1104. State = [[-0.2973899  0.0815883]]. Action = [[ 0.24382609 -0.14524336 -0.24312387  0.66795135]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1104 is [True, False, False, False, True, False]
State prediction error at timestep 1104 is tensor(1.6927e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1105. State = [[-0.29745626  0.08178041]]. Action = [[-0.19607429 -0.21970129  0.23807132 -0.98996544]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1105 is [True, False, False, False, True, False]
State prediction error at timestep 1105 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1106. State = [[-0.29750982  0.0820821 ]]. Action = [[ 0.10896567  0.04783601 -0.14879009 -0.86249584]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1106 is [True, False, False, False, True, False]
State prediction error at timestep 1106 is tensor(3.3481e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1106 of 1
Current timestep = 1107. State = [[-0.29728147  0.08237121]]. Action = [[-0.17264363  0.00887403  0.12308577  0.8614955 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1107 is [True, False, False, False, True, False]
State prediction error at timestep 1107 is tensor(3.3489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1107 of 1
Current timestep = 1108. State = [[-0.29711157  0.08250442]]. Action = [[-0.21579371  0.20329711  0.00769785 -0.80836415]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1108 is [True, False, False, False, True, False]
State prediction error at timestep 1108 is tensor(7.6506e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1109. State = [[-0.29683986  0.08273571]]. Action = [[-0.12430339 -0.08956081 -0.03270149  0.64145195]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1109 is [True, False, False, False, True, False]
State prediction error at timestep 1109 is tensor(1.4390e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1110. State = [[-0.29703456  0.08261549]]. Action = [[-0.14394748  0.2001763  -0.20070603  0.7038461 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1110 is [True, False, False, False, True, False]
State prediction error at timestep 1110 is tensor(6.0122e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1111. State = [[-0.29705507  0.08259679]]. Action = [[ 0.12111476  0.04254705 -0.14542933 -0.36325818]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1111 is [True, False, False, False, True, False]
State prediction error at timestep 1111 is tensor(8.2955e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1112. State = [[-0.29697293  0.08268213]]. Action = [[ 0.23241222 -0.18957259  0.08674705 -0.31717592]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1112 is [True, False, False, False, True, False]
State prediction error at timestep 1112 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1113. State = [[-0.29686037  0.08271703]]. Action = [[-0.07300179  0.24088174  0.00665763 -0.37144142]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1113 is [True, False, False, False, True, False]
State prediction error at timestep 1113 is tensor(3.6822e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1114. State = [[-0.29669422  0.08254354]]. Action = [[ 0.08552489 -0.10087797  0.04982808 -0.18557757]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1114 is [True, False, False, False, True, False]
State prediction error at timestep 1114 is tensor(4.0861e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.29575828  0.08178354]]. Action = [[-0.09968109  0.23595536  0.12845594  0.05675781]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1115 is [True, False, False, False, True, False]
State prediction error at timestep 1115 is tensor(8.5066e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1116. State = [[-0.2953144   0.08153807]]. Action = [[-0.14685689  0.11131424 -0.10621598 -0.5643337 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1116 is [True, False, False, False, True, False]
State prediction error at timestep 1116 is tensor(7.4150e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1117. State = [[-0.29418743  0.08045065]]. Action = [[ 0.01826206 -0.05172695  0.23391026  0.75710213]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1117 is [True, False, False, False, True, False]
State prediction error at timestep 1117 is tensor(5.8373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of 1
Current timestep = 1118. State = [[-0.29403117  0.07997638]]. Action = [[-0.18746111 -0.10438648  0.17361495  0.09758651]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1118 is [True, False, False, False, True, False]
State prediction error at timestep 1118 is tensor(1.8349e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1119. State = [[-0.29344505  0.07952835]]. Action = [[ 0.06863263 -0.21659331  0.2322756  -0.08743405]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1119 is [True, False, False, False, True, False]
State prediction error at timestep 1119 is tensor(5.3080e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of 1
Current timestep = 1120. State = [[-0.29290396  0.07912064]]. Action = [[-0.23078857  0.1547403   0.1164822  -0.5560466 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1120 is [True, False, False, False, True, False]
State prediction error at timestep 1120 is tensor(6.6846e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1121. State = [[-0.29201502  0.07837066]]. Action = [[-0.10809638  0.06688446 -0.16233571 -0.6069671 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1121 is [True, False, False, False, True, False]
State prediction error at timestep 1121 is tensor(8.1572e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1122. State = [[-0.29240933  0.07866864]]. Action = [[-0.06322914  0.02610138  0.22626978 -0.7898468 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1122 is [True, False, False, False, True, False]
State prediction error at timestep 1122 is tensor(6.1213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1123. State = [[-0.29289907  0.07962281]]. Action = [[-0.18629304 -0.19612327 -0.02116749  0.17687953]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1123 is [True, False, False, False, True, False]
State prediction error at timestep 1123 is tensor(2.4404e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1124. State = [[-0.29301652  0.0797525 ]]. Action = [[-0.21165493 -0.07779512 -0.10801136 -0.45294333]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1124 is [True, False, False, False, True, False]
State prediction error at timestep 1124 is tensor(5.3628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1125. State = [[-0.29298624  0.07984737]]. Action = [[ 0.08566311 -0.10899976 -0.09036493 -0.2519567 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1125 is [True, False, False, False, True, False]
State prediction error at timestep 1125 is tensor(5.0695e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of 1
Current timestep = 1126. State = [[-0.2926822   0.07914185]]. Action = [[ 0.04730079 -0.14584029 -0.13977438  0.01663816]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1126 is [True, False, False, False, True, False]
State prediction error at timestep 1126 is tensor(1.7800e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1127. State = [[-0.29252094  0.07882317]]. Action = [[ 0.24881244 -0.05320644 -0.0058974  -0.9382945 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1127 is [True, False, False, False, True, False]
State prediction error at timestep 1127 is tensor(4.4316e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1127 of 1
Current timestep = 1128. State = [[-0.29242158  0.07838226]]. Action = [[ 0.15939033 -0.202008    0.148287    0.1001637 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1128 is [True, False, False, False, True, False]
State prediction error at timestep 1128 is tensor(2.1673e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1129. State = [[-0.29220387  0.07735927]]. Action = [[ 0.06448534  0.02431399 -0.15795891  0.31211138]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1129 is [True, False, False, False, True, False]
State prediction error at timestep 1129 is tensor(1.9215e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1130. State = [[-0.29215586  0.07734567]]. Action = [[-0.05949637 -0.20133117  0.14489672 -0.3857916 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1130 is [True, False, False, False, True, False]
State prediction error at timestep 1130 is tensor(6.9439e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1131. State = [[-0.2921055   0.07727063]]. Action = [[-0.04918964  0.13770205 -0.21754749 -0.52168417]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1131 is [True, False, False, False, True, False]
State prediction error at timestep 1131 is tensor(8.2328e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1132. State = [[-0.29175162  0.07715195]]. Action = [[-0.03157699  0.0526458   0.13414016  0.36417603]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1132 is [True, False, False, False, True, False]
State prediction error at timestep 1132 is tensor(6.8154e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[-0.29178816  0.07724814]]. Action = [[-0.06477436 -0.04165004  0.10075808  0.62010574]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1133 is [True, False, False, False, True, False]
State prediction error at timestep 1133 is tensor(2.5338e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 1
Current timestep = 1134. State = [[-0.29178816  0.07724814]]. Action = [[ 0.2177091  -0.05232216  0.18258089 -0.91420835]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1134 is [True, False, False, False, True, False]
State prediction error at timestep 1134 is tensor(2.0742e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1135. State = [[-0.29178816  0.07724814]]. Action = [[ 0.09041339 -0.19925766  0.12287438 -0.012357  ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1135 is [True, False, False, False, True, False]
State prediction error at timestep 1135 is tensor(2.6013e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1136. State = [[-0.29178816  0.07724814]]. Action = [[ 0.22057635 -0.1054084   0.08043981  0.32278848]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1136 is [True, False, False, False, True, False]
State prediction error at timestep 1136 is tensor(3.7634e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1137. State = [[-0.29178816  0.07724814]]. Action = [[-0.04618827  0.15931353  0.10544637  0.06432915]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1137 is [True, False, False, False, True, False]
State prediction error at timestep 1137 is tensor(2.3734e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1138. State = [[-0.29176942  0.07705095]]. Action = [[-0.10795653 -0.10185573  0.06838101 -0.64675856]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1138 is [True, False, False, False, True, False]
State prediction error at timestep 1138 is tensor(8.6827e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1138 of 1
Current timestep = 1139. State = [[-0.29221913  0.07627649]]. Action = [[-0.10323754 -0.13792986 -0.09368867  0.5206444 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1139 is [True, False, False, False, True, False]
State prediction error at timestep 1139 is tensor(1.3953e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1140. State = [[-0.2926235   0.07557716]]. Action = [[-0.21826656 -0.18106657 -0.0823665  -0.13261956]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1140 is [True, False, False, False, True, False]
State prediction error at timestep 1140 is tensor(1.0911e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1140 of 1
Current timestep = 1141. State = [[-0.29321444  0.07338775]]. Action = [[ 0.0003463  -0.07450579  0.18890554 -0.32595944]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1141 is [True, False, False, False, True, False]
State prediction error at timestep 1141 is tensor(7.9325e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1142. State = [[-0.29346845  0.07238226]]. Action = [[ 0.07624513 -0.13598727  0.12817347  0.343444  ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1142 is [True, False, False, False, True, False]
State prediction error at timestep 1142 is tensor(1.4773e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1143. State = [[-0.29369408  0.07153457]]. Action = [[ 0.17291617 -0.22509155  0.10082641  0.0523411 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1143 is [True, False, False, False, True, False]
State prediction error at timestep 1143 is tensor(2.0149e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1144. State = [[-0.2934454   0.07082006]]. Action = [[ 0.0440177   0.19067627 -0.08578235  0.96678495]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1144 is [True, False, False, False, True, False]
State prediction error at timestep 1144 is tensor(8.1877e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1145. State = [[-0.29354444  0.06843778]]. Action = [[-0.10140279 -0.07432294 -0.13446487  0.7586148 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1145 is [True, False, False, False, True, False]
State prediction error at timestep 1145 is tensor(5.8171e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1145 of 1
Current timestep = 1146. State = [[-0.29423147  0.06761245]]. Action = [[-0.1709868   0.1024307  -0.01048529 -0.31155443]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1146 is [True, False, False, False, True, False]
State prediction error at timestep 1146 is tensor(2.7137e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1147. State = [[-0.2947953   0.06678192]]. Action = [[ 0.17221111 -0.22533645 -0.21486278 -0.3338381 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1147 is [True, False, False, False, True, False]
State prediction error at timestep 1147 is tensor(7.8482e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1148. State = [[-0.29477412  0.06626853]]. Action = [[ 0.2110011  -0.22307935 -0.21320589  0.27556992]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1148 is [True, False, False, False, True, False]
State prediction error at timestep 1148 is tensor(3.5923e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1149. State = [[-0.29495618  0.06577336]]. Action = [[ 0.09179652 -0.2491445   0.07252008  0.64699507]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1149 is [True, False, False, False, True, False]
State prediction error at timestep 1149 is tensor(2.9030e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of 1
Current timestep = 1150. State = [[-0.29584378  0.06463291]]. Action = [[0.04012468 0.1470426  0.13348162 0.8502275 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1150 is [True, False, False, False, True, False]
State prediction error at timestep 1150 is tensor(2.8043e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1151. State = [[-0.29617995  0.06376497]]. Action = [[ 0.20143056 -0.23303989 -0.07977244 -0.5024795 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1151 is [True, False, False, False, True, False]
State prediction error at timestep 1151 is tensor(9.4403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1152. State = [[-0.2964751   0.06326994]]. Action = [[0.1965631  0.2093825  0.24171382 0.7675772 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1152 is [True, False, False, False, True, False]
State prediction error at timestep 1152 is tensor(3.8951e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1153. State = [[-0.296757    0.06273686]]. Action = [[-0.13574776  0.06947997  0.11572754  0.85825825]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1153 is [True, False, False, False, True, False]
State prediction error at timestep 1153 is tensor(2.3829e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of 1
Current timestep = 1154. State = [[-0.29714307  0.0619789 ]]. Action = [[ 0.24776447  0.11436385 -0.06786877 -0.7248401 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1154 is [True, False, False, False, True, False]
State prediction error at timestep 1154 is tensor(5.4678e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1155. State = [[-0.29722473  0.06163131]]. Action = [[ 0.19559598  0.01100546 -0.19860208 -0.4413904 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1155 is [True, False, False, False, True, False]
State prediction error at timestep 1155 is tensor(9.3936e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1156. State = [[-0.29729202  0.06155089]]. Action = [[ 0.23401222 -0.20613985  0.03243124 -0.47867334]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1156 is [True, False, False, False, True, False]
State prediction error at timestep 1156 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1157. State = [[-0.297504    0.06115415]]. Action = [[ 0.20334023  0.09745556  0.06135294 -0.32866573]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1157 is [True, False, False, False, True, False]
State prediction error at timestep 1157 is tensor(5.3901e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of 1
Current timestep = 1158. State = [[-0.29749575  0.06086906]]. Action = [[ 0.06660438 -0.23286864  0.18418038  0.21813154]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1158 is [True, False, False, False, True, False]
State prediction error at timestep 1158 is tensor(1.9039e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1159. State = [[-0.29770198  0.06070348]]. Action = [[-0.00577968  0.2122936  -0.01521514 -0.75584877]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1159 is [True, False, False, False, True, False]
State prediction error at timestep 1159 is tensor(4.8903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1160. State = [[-0.2977909   0.06040134]]. Action = [[-0.17208716 -0.02302317  0.19017565  0.65047765]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1160 is [True, False, False, False, True, False]
State prediction error at timestep 1160 is tensor(3.6961e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.2977672   0.06002448]]. Action = [[ 0.1266081  -0.01234631  0.06720373 -0.0051409 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1161 is [True, False, False, False, True, False]
State prediction error at timestep 1161 is tensor(3.8348e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1162. State = [[-0.29766116  0.05984174]]. Action = [[ 0.17559588  0.07094061 -0.22470579  0.01151156]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1162 is [True, False, False, False, True, False]
State prediction error at timestep 1162 is tensor(4.0985e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1163. State = [[-0.29761294  0.05959262]]. Action = [[ 0.16250032 -0.01279733 -0.18425545 -0.51293314]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1163 is [True, False, False, False, True, False]
State prediction error at timestep 1163 is tensor(6.1961e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1164. State = [[-0.2975865   0.05938593]]. Action = [[-0.07124776 -0.23240888  0.02693799 -0.26563704]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1164 is [True, False, False, False, True, False]
State prediction error at timestep 1164 is tensor(1.4831e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1165. State = [[-0.29757556  0.05921098]]. Action = [[-0.11983213 -0.01547262 -0.1923669  -0.40301275]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1165 is [True, False, False, False, True, False]
State prediction error at timestep 1165 is tensor(4.5555e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of 1
Current timestep = 1166. State = [[-0.2977035  0.0589481]]. Action = [[ 0.15847138 -0.20395587  0.03718874 -0.6241522 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(6.4694e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1167. State = [[-0.2977202   0.05882551]]. Action = [[ 0.21995616  0.15312326 -0.02274074 -0.4308132 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1167 is [True, False, False, False, True, False]
State prediction error at timestep 1167 is tensor(5.8825e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1168. State = [[-0.29762423  0.05878608]]. Action = [[ 0.07077193  0.22751516  0.18779856 -0.11624706]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1168 is [True, False, False, False, True, False]
State prediction error at timestep 1168 is tensor(9.4712e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1169. State = [[-0.2977135  0.058666 ]]. Action = [[ 0.24401098 -0.10953975  0.24704373  0.6500497 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1169 is [True, False, False, False, True, False]
State prediction error at timestep 1169 is tensor(1.9020e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 1
Current timestep = 1170. State = [[-0.29773247  0.05853908]]. Action = [[-0.16318859 -0.13743328 -0.17824052  0.2199974 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1170 is [True, False, False, False, True, False]
State prediction error at timestep 1170 is tensor(1.4832e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1171. State = [[-0.29773247  0.05853908]]. Action = [[ 0.0598675  -0.19479895  0.19942486 -0.9603508 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1171 is [True, False, False, False, True, False]
State prediction error at timestep 1171 is tensor(7.6612e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1172. State = [[-0.29772344  0.05841898]]. Action = [[-0.21177219  0.16946459  0.18982345 -0.6455725 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1172 is [True, False, False, False, True, False]
State prediction error at timestep 1172 is tensor(5.9681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1173. State = [[-0.29778412  0.05831339]]. Action = [[-0.24564381 -0.00844328  0.12846094 -0.6255899 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(1.9110e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.29785818  0.05830716]]. Action = [[ 0.21996063 -0.00650363 -0.05596469  0.1440413 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1174 is [True, False, False, False, True, False]
State prediction error at timestep 1174 is tensor(1.3848e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1175. State = [[-0.29794428  0.05817173]]. Action = [[ 0.02656499 -0.22744752 -0.15010783  0.05732656]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1175 is [True, False, False, False, True, False]
State prediction error at timestep 1175 is tensor(1.2568e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1176. State = [[-0.29780802  0.05815752]]. Action = [[ 0.08467871 -0.14040601  0.12622362 -0.8630528 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1176 is [True, False, False, False, True, False]
State prediction error at timestep 1176 is tensor(2.9096e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1177. State = [[-0.2979352   0.05797731]]. Action = [[-0.11800545 -0.06408221  0.08500585 -0.25033057]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1177 is [True, False, False, False, True, False]
State prediction error at timestep 1177 is tensor(9.2542e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1178. State = [[-0.2982689   0.05759254]]. Action = [[ 0.00826675 -0.15029165 -0.14839311 -0.11709511]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1178 is [True, False, False, False, True, False]
State prediction error at timestep 1178 is tensor(2.2237e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1179. State = [[-0.29882887  0.05705456]]. Action = [[-0.2375061  -0.22640747  0.06508189 -0.2696783 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1179 is [True, False, False, False, True, False]
State prediction error at timestep 1179 is tensor(2.5306e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1180. State = [[-0.29982325  0.05584299]]. Action = [[ 0.10789436 -0.05543715  0.13272291 -0.7629649 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1180 is [True, False, False, False, True, False]
State prediction error at timestep 1180 is tensor(3.5594e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1180 of 1
Current timestep = 1181. State = [[-0.29980025  0.05513986]]. Action = [[-0.12990247  0.15044832 -0.08683597 -0.6720843 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(4.3863e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1182. State = [[-0.29984012  0.05476262]]. Action = [[ 0.1620645  -0.23860274  0.01569915 -0.04047966]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1182 is [True, False, False, False, True, False]
State prediction error at timestep 1182 is tensor(8.5881e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1183. State = [[-0.29982525  0.05425728]]. Action = [[-0.18044263 -0.01733561 -0.12305915 -0.97673887]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1183 is [True, False, False, False, True, False]
State prediction error at timestep 1183 is tensor(4.8595e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1184. State = [[-0.29986212  0.05369437]]. Action = [[-0.03842728 -0.17966591  0.10919741  0.2666695 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1184 is [True, False, False, False, True, False]
State prediction error at timestep 1184 is tensor(7.4071e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[-0.29981193  0.05306468]]. Action = [[-0.20376113  0.15521795 -0.22866818 -0.39083707]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1185 is [True, False, False, False, True, False]
State prediction error at timestep 1185 is tensor(4.9126e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1186. State = [[-0.29969043  0.05265474]]. Action = [[-0.15005141 -0.0271218  -0.10225666  0.50847065]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1186 is [True, False, False, False, True, False]
State prediction error at timestep 1186 is tensor(7.7814e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1187. State = [[-0.29984182  0.05227616]]. Action = [[0.23472852 0.14076799 0.19294104 0.95916915]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1187 is [True, False, False, False, True, False]
State prediction error at timestep 1187 is tensor(1.3566e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1187 of -1
Current timestep = 1188. State = [[-0.29985425  0.05109304]]. Action = [[-0.03301519  0.05670616 -0.171169   -0.01099277]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1188 is [True, False, False, False, True, False]
State prediction error at timestep 1188 is tensor(1.2472e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1189. State = [[-0.29997888  0.05139926]]. Action = [[ 0.22453028  0.19546807 -0.11141443  0.57972217]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1189 is [True, False, False, False, True, False]
State prediction error at timestep 1189 is tensor(5.4167e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1190. State = [[-0.3000279   0.05153993]]. Action = [[ 0.24284092 -0.11128199 -0.01144552 -0.674208  ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1190 is [True, False, False, False, True, False]
State prediction error at timestep 1190 is tensor(3.1725e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1191. State = [[-0.3000663   0.05146874]]. Action = [[ 0.1604251   0.03646475 -0.02645662 -0.21778089]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1191 is [True, False, False, False, True, False]
State prediction error at timestep 1191 is tensor(2.5248e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1192. State = [[-0.30010718  0.05149812]]. Action = [[ 0.19571936 -0.11614577  0.22578812  0.6298982 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1192 is [True, False, False, False, True, False]
State prediction error at timestep 1192 is tensor(2.8096e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1193. State = [[-0.30019018  0.05166603]]. Action = [[-0.20563574 -0.07904902 -0.06658544 -0.30704284]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1193 is [True, False, False, False, True, False]
State prediction error at timestep 1193 is tensor(4.3590e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of -1
Current timestep = 1194. State = [[-0.30021024  0.05165358]]. Action = [[ 0.18088624 -0.1480725  -0.02136207 -0.10270029]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1194 is [True, False, False, False, True, False]
State prediction error at timestep 1194 is tensor(1.7284e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1195. State = [[-0.30021083  0.0517078 ]]. Action = [[-0.09754723  0.2314201   0.15743116  0.93287516]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1195 is [True, False, False, False, True, False]
State prediction error at timestep 1195 is tensor(2.3331e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1196. State = [[-0.30020952  0.05182888]]. Action = [[-0.01438314  0.06742865  0.04304785  0.0931673 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1196 is [True, False, False, False, True, False]
State prediction error at timestep 1196 is tensor(5.3696e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1196 of -1
Current timestep = 1197. State = [[-0.3004642   0.05225237]]. Action = [[ 0.24643868 -0.15405011 -0.09837368 -0.8636229 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1197 is [True, False, False, False, True, False]
State prediction error at timestep 1197 is tensor(1.8113e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1198. State = [[-0.30069047  0.05278078]]. Action = [[ 0.09555131 -0.24891019  0.18266642  0.8033371 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1198 is [True, False, False, False, True, False]
State prediction error at timestep 1198 is tensor(2.9362e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1198 of -1
Current timestep = 1199. State = [[-0.3008867   0.05310077]]. Action = [[ 0.04911244 -0.1606627   0.06709176 -0.24505323]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1199 is [True, False, False, False, True, False]
State prediction error at timestep 1199 is tensor(2.1681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1200. State = [[-0.30108476  0.05341252]]. Action = [[ 0.19307435  0.07578123  0.05789837 -0.6055077 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1200 is [True, False, False, False, True, False]
State prediction error at timestep 1200 is tensor(5.4755e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1201. State = [[-0.30138904  0.05410461]]. Action = [[-0.22949502  0.04598704  0.23758847  0.02714455]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1201 is [True, False, False, False, True, False]
State prediction error at timestep 1201 is tensor(1.5192e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1202. State = [[-0.30175516  0.05477341]]. Action = [[-0.09660798 -0.03658597  0.18098885  0.11713743]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1202 is [True, False, False, False, True, False]
State prediction error at timestep 1202 is tensor(1.9477e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1202 of -1
Current timestep = 1203. State = [[-0.30211124  0.05480962]]. Action = [[ 0.02686372 -0.19467926 -0.02922598  0.11113238]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1203 is [True, False, False, False, True, False]
State prediction error at timestep 1203 is tensor(4.2283e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1204. State = [[-0.3031037   0.05495651]]. Action = [[-0.10697673  0.03249416  0.2338152  -0.8917887 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1204 is [True, False, False, False, True, False]
State prediction error at timestep 1204 is tensor(1.9773e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of -1
Current timestep = 1205. State = [[-0.30393004  0.05555959]]. Action = [[-0.02729514 -0.16669989  0.18911883 -0.9325778 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1205 is [True, False, False, False, True, False]
State prediction error at timestep 1205 is tensor(3.3598e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1206. State = [[-0.30437523  0.05584463]]. Action = [[-0.2296461  -0.05177946 -0.21822724 -0.5788136 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1206 is [True, False, False, False, True, False]
State prediction error at timestep 1206 is tensor(4.5012e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of -1
Current timestep = 1207. State = [[-0.3050004   0.05618479]]. Action = [[ 0.04132858 -0.21543774 -0.03029031  0.43836212]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1207 is [True, False, False, False, True, False]
State prediction error at timestep 1207 is tensor(1.8443e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1208. State = [[-0.3077491   0.05684144]]. Action = [[ 0.00463718 -0.08982874  0.04414088  0.58587074]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1208 is [True, False, False, False, True, False]
State prediction error at timestep 1208 is tensor(9.3049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1209. State = [[-0.3080925   0.05626027]]. Action = [[ 0.13850862 -0.16149582  0.20683104  0.93529546]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1209 is [True, False, False, False, True, False]
State prediction error at timestep 1209 is tensor(5.4791e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1210. State = [[-0.30838716  0.05562939]]. Action = [[ 0.12474573 -0.17109342 -0.2044306   0.00573337]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1210 is [True, False, False, False, True, False]
State prediction error at timestep 1210 is tensor(3.4382e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1211. State = [[-0.30968776  0.0541976 ]]. Action = [[-0.10544094  0.0689019  -0.11383983 -0.07429194]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1211 is [True, False, False, False, True, False]
State prediction error at timestep 1211 is tensor(1.7444e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of -1
Current timestep = 1212. State = [[-0.31390736  0.05573252]]. Action = [[-0.10005951  0.10158807 -0.21771443 -0.82001245]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1212 is [True, False, False, False, True, False]
State prediction error at timestep 1212 is tensor(8.7157e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1212 of -1
Current timestep = 1213. State = [[-0.31988806  0.05914361]]. Action = [[-0.10981473  0.00858563  0.20415494  0.933846  ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1213 is [True, False, False, False, True, False]
State prediction error at timestep 1213 is tensor(3.8774e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1214. State = [[-0.3257117   0.06144956]]. Action = [[-0.0861894  -0.09479979  0.21999231 -0.57126987]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1214 is [True, False, False, False, True, False]
State prediction error at timestep 1214 is tensor(4.3531e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1215. State = [[-0.32761973  0.06093623]]. Action = [[ 0.06224784  0.14785856 -0.03637633 -0.5532581 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1215 is [True, False, False, False, True, False]
State prediction error at timestep 1215 is tensor(8.8401e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1216. State = [[-0.3285564   0.06068802]]. Action = [[-0.19109726 -0.24386205  0.14135933 -0.6381557 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1216 is [True, False, False, False, True, False]
State prediction error at timestep 1216 is tensor(2.8545e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1217. State = [[-0.32997546  0.06057101]]. Action = [[ 0.1509991   0.11491737  0.00515965 -0.15704292]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1217 is [True, False, False, False, True, False]
State prediction error at timestep 1217 is tensor(2.0385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1218. State = [[-0.33128482  0.06038633]]. Action = [[ 0.07104528 -0.1624206  -0.03448106  0.41719627]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1218 is [True, False, False, False, True, False]
State prediction error at timestep 1218 is tensor(1.7264e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1219. State = [[-0.33257172  0.05994175]]. Action = [[ 0.2152049  -0.03579874 -0.10704544  0.26721346]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1219 is [True, False, False, False, True, False]
State prediction error at timestep 1219 is tensor(2.0291e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1220. State = [[-0.33302736  0.05989598]]. Action = [[ 0.18541247 -0.21245351 -0.00518957 -0.4896984 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1220 is [True, False, False, False, True, False]
State prediction error at timestep 1220 is tensor(9.5185e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1221. State = [[-0.33396462  0.05958254]]. Action = [[ 0.24342811 -0.1303684  -0.00073622  0.37652194]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1221 is [True, False, False, False, True, False]
State prediction error at timestep 1221 is tensor(8.5020e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of -1
Current timestep = 1222. State = [[-0.33486757  0.05929157]]. Action = [[-0.23734017 -0.00210656 -0.02488679 -0.9332531 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1222 is [True, False, False, False, True, False]
State prediction error at timestep 1222 is tensor(4.8382e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1223. State = [[-0.33500433  0.0592473 ]]. Action = [[-0.1459512   0.11167026 -0.19708726  0.3906988 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1223 is [True, False, False, False, True, False]
State prediction error at timestep 1223 is tensor(5.2529e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1224. State = [[-0.3352314   0.05904287]]. Action = [[ 0.20940971 -0.14578025  0.15080947  0.23086464]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1224 is [True, False, False, False, True, False]
State prediction error at timestep 1224 is tensor(5.5105e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1225. State = [[-0.33546636  0.05895841]]. Action = [[-0.02390154  0.22615063 -0.02513805  0.63745856]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1225 is [True, False, False, False, True, False]
State prediction error at timestep 1225 is tensor(4.5493e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of -1
Current timestep = 1226. State = [[-0.33573     0.05885068]]. Action = [[-0.19991273 -0.2256203   0.09419018 -0.22136939]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1226 is [True, False, False, False, True, False]
State prediction error at timestep 1226 is tensor(6.8295e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1227. State = [[-0.33657512  0.05879696]]. Action = [[-0.04112142  0.08693886 -0.11756682 -0.83100086]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1227 is [True, False, False, False, True, False]
State prediction error at timestep 1227 is tensor(7.5814e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1228. State = [[-0.33722678  0.05967383]]. Action = [[-0.22483857 -0.03659666  0.19333124 -0.9653769 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1228 is [True, False, False, False, True, False]
State prediction error at timestep 1228 is tensor(4.0274e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1229. State = [[-0.3375266   0.06021161]]. Action = [[-0.210839    0.06883052 -0.15662119  0.04883647]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1229 is [True, False, False, False, True, False]
State prediction error at timestep 1229 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1230. State = [[-0.33825132  0.06100779]]. Action = [[-0.18947     0.22960371  0.048637    0.13379347]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1230 is [True, False, False, False, True, False]
State prediction error at timestep 1230 is tensor(2.8385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1231. State = [[-0.3399868   0.06242725]]. Action = [[ 0.0904609  -0.02027157  0.07501844  0.9206269 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1231 is [True, False, False, False, True, False]
State prediction error at timestep 1231 is tensor(5.9256e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of -1
Current timestep = 1232. State = [[-0.34016725  0.0623572 ]]. Action = [[-1.3589570e-01  5.7166815e-04 -1.5111762e-01 -9.1785389e-01]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1232 is [True, False, False, False, True, False]
State prediction error at timestep 1232 is tensor(4.1023e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1233. State = [[-0.34013897  0.06234001]]. Action = [[ 0.01671943 -0.00170016 -0.19523296 -0.9548028 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1233 is [True, False, False, False, True, False]
State prediction error at timestep 1233 is tensor(6.6245e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1233 of -1
Current timestep = 1234. State = [[-0.34013897  0.06234001]]. Action = [[ 0.2121889  -0.15533884  0.17931008  0.39285374]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1234 is [True, False, False, False, True, False]
State prediction error at timestep 1234 is tensor(2.2687e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1235. State = [[-0.34013897  0.06234001]]. Action = [[-0.23398302  0.15460753  0.08659917 -0.30318958]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1235 is [True, False, False, False, True, False]
State prediction error at timestep 1235 is tensor(1.4961e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of -1
Current timestep = 1236. State = [[-0.34013897  0.06234001]]. Action = [[-0.02821949 -0.02472095 -0.21596958 -0.966782  ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1236 is [True, False, False, False, True, False]
State prediction error at timestep 1236 is tensor(5.4188e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1237. State = [[-0.34013084  0.06213153]]. Action = [[ 0.15799862 -0.06619018 -0.15966076 -0.4525603 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1237 is [True, False, False, False, True, False]
State prediction error at timestep 1237 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1238. State = [[-0.34012887  0.06208011]]. Action = [[ 0.06067139 -0.16556999 -0.18999995  0.06001031]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1238 is [True, False, False, False, True, False]
State prediction error at timestep 1238 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1239. State = [[-0.3401249   0.06197726]]. Action = [[ 0.23088408 -0.05885017  0.05708584 -0.9813801 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1239 is [True, False, False, False, True, False]
State prediction error at timestep 1239 is tensor(5.3511e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1240. State = [[-0.34012097  0.06187441]]. Action = [[-0.12179868 -0.21106517  0.17422497 -0.13200259]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1240 is [True, False, False, False, True, False]
State prediction error at timestep 1240 is tensor(8.7216e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1241. State = [[-0.3400896   0.06169954]]. Action = [[ 0.1126295   0.0916492  -0.19472557  0.83593035]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1241 is [True, False, False, False, True, False]
State prediction error at timestep 1241 is tensor(3.5561e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of -1
Current timestep = 1242. State = [[-0.33990383  0.06191553]]. Action = [[-0.23748848  0.03268039 -0.032838    0.4971342 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1242 is [True, False, False, False, True, False]
State prediction error at timestep 1242 is tensor(4.4155e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1243. State = [[-0.33980992  0.06206389]]. Action = [[-0.2257275   0.09591654  0.24770868  0.81611454]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1243 is [True, False, False, False, True, False]
State prediction error at timestep 1243 is tensor(1.5041e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1244. State = [[-0.33971864  0.06216015]]. Action = [[-0.24268566  0.18752038  0.07259944 -0.8144182 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1244 is [True, False, False, False, True, False]
State prediction error at timestep 1244 is tensor(3.0088e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of -1
Current timestep = 1245. State = [[-0.33943334  0.0624593 ]]. Action = [[ 0.11036503  0.02393901 -0.20383647  0.88573444]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1245 is [True, False, False, False, True, False]
State prediction error at timestep 1245 is tensor(5.0134e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1246. State = [[-0.3385309  0.0626199]]. Action = [[-0.16732025  0.03138748  0.01534641 -0.82079226]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1246 is [True, False, False, False, True, False]
State prediction error at timestep 1246 is tensor(2.9569e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1247. State = [[-0.3372205   0.06289838]]. Action = [[-0.04929109 -0.07918337  0.1757291   0.29128742]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1247 is [True, False, False, False, True, False]
State prediction error at timestep 1247 is tensor(1.1624e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1248. State = [[-0.33742544  0.06278763]]. Action = [[-0.13368085  0.13614306  0.02501503  0.83426785]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1248 is [True, False, False, False, True, False]
State prediction error at timestep 1248 is tensor(5.5456e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1248 of -1
Current timestep = 1249. State = [[-0.33743307  0.06267767]]. Action = [[-0.10777476  0.01670331  0.13510293 -0.3477323 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1249 is [True, False, False, False, True, False]
State prediction error at timestep 1249 is tensor(1.7114e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1250. State = [[-0.33729756  0.06275411]]. Action = [[-0.24601948  0.11957133  0.13256538 -0.92456216]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1250 is [True, False, False, False, True, False]
State prediction error at timestep 1250 is tensor(3.0919e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1250 of -1
Current timestep = 1251. State = [[-0.33720997  0.06283363]]. Action = [[ 0.03128237 -0.23166464  0.22051042 -0.5525178 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1251 is [True, False, False, False, True, False]
State prediction error at timestep 1251 is tensor(6.2529e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1252. State = [[-0.33736563  0.06286398]]. Action = [[ 0.12685138  0.05809212  0.20374113 -0.87118685]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1252 is [True, False, False, False, True, False]
State prediction error at timestep 1252 is tensor(4.1100e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1253. State = [[-0.33736563  0.06286398]]. Action = [[-0.19411764 -0.01517522 -0.22792666 -0.16699696]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1253 is [True, False, False, False, True, False]
State prediction error at timestep 1253 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1254. State = [[-0.3373455   0.06293362]]. Action = [[ 4.6013832e-02 -2.1963535e-01 -1.8383563e-04 -1.2553418e-01]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1254 is [True, False, False, False, True, False]
State prediction error at timestep 1254 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1255. State = [[-0.33714655  0.06281742]]. Action = [[ 0.03141373 -0.12042035 -0.02373719  0.6686497 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1255 is [True, False, False, False, True, False]
State prediction error at timestep 1255 is tensor(3.9264e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1255 of -1
Current timestep = 1256. State = [[-0.33682913  0.06241159]]. Action = [[-0.12690154 -0.21866779 -0.09468108 -0.5354503 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1256 is [True, False, False, False, True, False]
State prediction error at timestep 1256 is tensor(1.0174e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1257. State = [[-0.33592626  0.06183814]]. Action = [[0.11539438 0.05818775 0.20194668 0.47713614]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1257 is [True, False, False, False, True, False]
State prediction error at timestep 1257 is tensor(1.4071e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1257 of -1
Current timestep = 1258. State = [[-0.33522978  0.06209875]]. Action = [[-0.17476279 -0.13074899 -0.14103854 -0.19120336]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1258 is [True, False, False, False, True, False]
State prediction error at timestep 1258 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1259. State = [[-0.33455497  0.06235284]]. Action = [[ 0.14612263  0.17695904 -0.13566355  0.5372975 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 1259 is [True, False, False, False, True, False]
State prediction error at timestep 1259 is tensor(2.7628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1260. State = [[-0.33407617  0.06255221]]. Action = [[-0.24631265 -0.08201423 -0.07441255 -0.14289856]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 1260 is [True, False, False, False, True, False]
State prediction error at timestep 1260 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1261. State = [[-0.33341765  0.0626567 ]]. Action = [[ 0.09690714  0.22350442  0.15402213 -0.7299748 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 1261 is [True, False, False, False, True, False]
State prediction error at timestep 1261 is tensor(1.6971e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1261 of -1
Current timestep = 1262. State = [[-0.33145615  0.06315441]]. Action = [[-0.12064722 -0.03631869  0.0204818   0.10157907]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 1262 is [True, False, False, False, True, False]
State prediction error at timestep 1262 is tensor(6.8082e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1263. State = [[-0.3316829   0.06308344]]. Action = [[-0.18389186 -0.13002008 -0.09335355  0.659122  ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 1263 is [True, False, False, False, True, False]
State prediction error at timestep 1263 is tensor(1.5531e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1264. State = [[-0.33165425  0.06294385]]. Action = [[-0.24294257  0.15624845 -0.05612454 -0.7175491 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 1264 is [True, False, False, False, True, False]
State prediction error at timestep 1264 is tensor(2.9026e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1265. State = [[-0.3315355   0.06286519]]. Action = [[-0.03406493  0.20279562 -0.11460146 -0.35323447]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 1265 is [True, False, False, False, True, False]
State prediction error at timestep 1265 is tensor(2.5528e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1266. State = [[-0.33156306  0.06286298]]. Action = [[-0.17315497  0.22754443  0.203376   -0.2853601 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 1266 is [True, False, False, False, True, False]
State prediction error at timestep 1266 is tensor(3.0505e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1267. State = [[-0.33160722  0.06288426]]. Action = [[ 0.04031897 -0.23399325 -0.16402738  0.5108193 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 1267 is [True, False, False, False, True, False]
State prediction error at timestep 1267 is tensor(3.0383e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1267 of 1
Current timestep = 1268. State = [[-0.3315725  0.0625874]]. Action = [[ 0.09749049 -0.01009642 -0.15926135 -0.30742645]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 1268 is [True, False, False, False, True, False]
State prediction error at timestep 1268 is tensor(4.6164e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1268 of 1
Current timestep = 1269. State = [[-0.3315191   0.06244619]]. Action = [[ 0.10939425  0.18013215 -0.03700447  0.60763574]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 1269 is [True, False, False, False, True, False]
State prediction error at timestep 1269 is tensor(2.0375e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1270. State = [[-0.33123335  0.06209163]]. Action = [[ 0.07301918  0.00910011 -0.0178446   0.24681747]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 1270 is [True, False, False, False, True, False]
State prediction error at timestep 1270 is tensor(3.1849e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1270 of 1
Current timestep = 1271. State = [[-0.3307457   0.06210344]]. Action = [[0.24394006 0.17271009 0.20827022 0.5255127 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 1271 is [True, False, False, False, True, False]
State prediction error at timestep 1271 is tensor(6.5028e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1272. State = [[-0.32940763  0.06198457]]. Action = [[-0.08666888 -0.02683499 -0.02933216 -0.25977254]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 1272 is [True, False, False, False, True, False]
State prediction error at timestep 1272 is tensor(8.0106e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1272 of 1
Current timestep = 1273. State = [[-0.32970726  0.06179294]]. Action = [[ 0.22280347 -0.00560981  0.06374806 -0.77663857]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 1273 is [True, False, False, False, True, False]
State prediction error at timestep 1273 is tensor(4.1490e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1274. State = [[-0.3297726   0.06173057]]. Action = [[ 0.21196255 -0.0951369  -0.16918297 -0.47620165]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 1274 is [True, False, False, False, True, False]
State prediction error at timestep 1274 is tensor(7.8177e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1275. State = [[-0.32959482  0.06137797]]. Action = [[ 0.07181177 -0.0825488   0.04102492  0.0310806 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 1275 is [True, False, False, False, True, False]
State prediction error at timestep 1275 is tensor(5.3004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1275 of 1
Current timestep = 1276. State = [[-0.3287525   0.06050186]]. Action = [[-0.22876124 -0.14333907 -0.1875552  -0.7047141 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 1276 is [True, False, False, False, True, False]
State prediction error at timestep 1276 is tensor(2.5276e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1277. State = [[-0.32827127  0.05974641]]. Action = [[ 0.09724271  0.19040334  0.22494873 -0.92997915]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 1277 is [True, False, False, False, True, False]
State prediction error at timestep 1277 is tensor(4.1522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1277 of 1
Current timestep = 1278. State = [[-0.32839325  0.05903953]]. Action = [[ 0.23433873  0.22682682  0.22658253 -0.18409729]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 1278 is [True, False, False, False, True, False]
State prediction error at timestep 1278 is tensor(2.1287e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1279. State = [[-0.32757008  0.05703025]]. Action = [[-0.09476827 -0.13025331 -0.24487723  0.66246724]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 1279 is [True, False, False, False, True, False]
State prediction error at timestep 1279 is tensor(8.7031e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1279 of 1
Current timestep = 1280. State = [[-0.32718897  0.05579685]]. Action = [[ 0.21798062  0.01803142  0.20365024 -0.38575572]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 1280 is [True, False, False, False, True, False]
State prediction error at timestep 1280 is tensor(4.6525e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1281. State = [[-0.32691708  0.05479508]]. Action = [[ 0.08335787 -0.17181054 -0.12288594 -0.800599  ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 1281 is [True, False, False, False, True, False]
State prediction error at timestep 1281 is tensor(4.1273e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1282. State = [[-0.32688016  0.05400129]]. Action = [[ 0.16571367  0.17739725 -0.08266276  0.36330342]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 1282 is [True, False, False, False, True, False]
State prediction error at timestep 1282 is tensor(2.6161e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1283. State = [[-0.32696205  0.05326092]]. Action = [[ 0.04912814 -0.24354708  0.05979058 -0.48150688]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 1283 is [True, False, False, False, True, False]
State prediction error at timestep 1283 is tensor(2.8284e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1283 of 1
Current timestep = 1284. State = [[-0.32680747  0.05172052]]. Action = [[-0.06499824 -0.16016564  0.20608497  0.8197663 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 1284 is [True, False, False, False, True, False]
State prediction error at timestep 1284 is tensor(2.5636e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1285. State = [[-0.3268047   0.05081206]]. Action = [[ 0.24885777  0.00809672 -0.1958749   0.20954466]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 1285 is [True, False, False, False, True, False]
State prediction error at timestep 1285 is tensor(7.0312e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1286. State = [[-0.32692513  0.05020609]]. Action = [[ 0.10501623  0.2256164  -0.08290721  0.8993199 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 1286 is [True, False, False, False, True, False]
State prediction error at timestep 1286 is tensor(2.2533e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1287. State = [[-0.32687724  0.04770755]]. Action = [[ 0.01037318 -0.08271584  0.10826498 -0.5502017 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 1287 is [True, False, False, False, True, False]
State prediction error at timestep 1287 is tensor(4.2265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1287 of 1
Current timestep = 1288. State = [[-0.3267029   0.04650821]]. Action = [[ 0.14576608 -0.22413456 -0.21460849 -0.95250905]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 1288 is [True, False, False, False, True, False]
State prediction error at timestep 1288 is tensor(1.9413e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1289. State = [[-0.326788    0.04574841]]. Action = [[-0.21764535 -0.19108504  0.03135097 -0.5220969 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 1289 is [True, False, False, False, True, False]
State prediction error at timestep 1289 is tensor(1.6415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1290. State = [[-0.32681334  0.04499666]]. Action = [[-0.15283187  0.19059521 -0.01035333  0.5928018 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 1290 is [True, False, False, False, True, False]
State prediction error at timestep 1290 is tensor(5.1203e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1291. State = [[-0.32682154  0.04405139]]. Action = [[ 0.05103225 -0.14176923 -0.09052026  0.14460862]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 1291 is [True, False, False, False, True, False]
State prediction error at timestep 1291 is tensor(6.2097e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1292. State = [[-0.32680833  0.04337867]]. Action = [[-0.21041353  0.05351859  0.04859978  0.39195585]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 1292 is [True, False, False, False, True, False]
State prediction error at timestep 1292 is tensor(1.1780e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1292 of 1
Current timestep = 1293. State = [[-0.32685748  0.0413704 ]]. Action = [[-0.15494283  0.23226446  0.24325222  0.7111399 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 1293 is [True, False, False, False, True, False]
State prediction error at timestep 1293 is tensor(5.6035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1294. State = [[-0.3267121   0.04007868]]. Action = [[ 0.12753314 -0.09921962  0.04646811 -0.5502506 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 1294 is [True, False, False, False, True, False]
State prediction error at timestep 1294 is tensor(2.6670e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1295. State = [[-0.32627892  0.03910813]]. Action = [[-0.20368427  0.00144237 -0.09840336 -0.16578114]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 1295 is [True, False, False, False, True, False]
State prediction error at timestep 1295 is tensor(1.2464e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1296. State = [[-0.32575187  0.03823512]]. Action = [[ 0.04469082 -0.15642506 -0.18931192  0.37401605]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 1296 is [True, False, False, False, True, False]
State prediction error at timestep 1296 is tensor(1.0987e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1297. State = [[-0.32548404  0.03747844]]. Action = [[ 0.18332419 -0.08282347  0.14180425  0.526263  ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 1297 is [True, False, False, False, True, False]
State prediction error at timestep 1297 is tensor(2.0085e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1298. State = [[-0.32547694  0.03659477]]. Action = [[-0.15188794 -0.17477925  0.05824351 -0.1585179 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 1298 is [True, False, False, False, True, False]
State prediction error at timestep 1298 is tensor(5.1700e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1299. State = [[-0.3238943   0.03435933]]. Action = [[ 0.12649971  0.01925522 -0.03261355 -0.20783746]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 1299 is [True, False, False, False, True, False]
State prediction error at timestep 1299 is tensor(4.0604e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1299 of 1
Current timestep = 1300. State = [[-0.3232867   0.03428176]]. Action = [[-0.13172376 -0.20279108 -0.04477222 -0.03170389]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 1300 is [True, False, False, False, True, False]
State prediction error at timestep 1300 is tensor(2.3522e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1301. State = [[-0.32287544  0.03409649]]. Action = [[ 0.09419519 -0.17962506  0.19457686  0.82300425]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 1301 is [True, False, False, False, True, False]
State prediction error at timestep 1301 is tensor(2.8928e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1302. State = [[-0.32090485  0.03334373]]. Action = [[-0.0363871  -0.07457015  0.04340959 -0.6304535 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 1302 is [True, False, False, False, True, False]
State prediction error at timestep 1302 is tensor(8.7907e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1303. State = [[-0.3204614  0.0326436]]. Action = [[-0.21097837 -0.07193199  0.10524046  0.8483889 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 1303 is [True, False, False, False, True, False]
State prediction error at timestep 1303 is tensor(1.3490e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1304. State = [[-0.31900266  0.03075621]]. Action = [[-0.02122277  0.1179319  -0.1095008  -0.723872  ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 1304 is [True, False, False, False, True, False]
State prediction error at timestep 1304 is tensor(3.0608e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1305. State = [[-0.3190992   0.03103928]]. Action = [[ 0.21731201  0.01889384  0.05449125 -0.00119108]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 1305 is [True, False, False, False, True, False]
State prediction error at timestep 1305 is tensor(2.1387e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1306. State = [[-0.3195014   0.03139151]]. Action = [[-0.11486396 -0.21898969 -0.1993549  -0.13716412]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 1306 is [True, False, False, False, True, False]
State prediction error at timestep 1306 is tensor(2.5887e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1307. State = [[-0.3195031   0.03154107]]. Action = [[-0.24040549  0.01910159  0.01312986  0.9602046 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 1307 is [True, False, False, False, True, False]
State prediction error at timestep 1307 is tensor(1.0242e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1308. State = [[-0.3192118   0.03131492]]. Action = [[-0.23284225  0.01918176 -0.08951901  0.08752489]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 1308 is [True, False, False, False, True, False]
State prediction error at timestep 1308 is tensor(5.3014e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1309. State = [[-0.3191021   0.03150307]]. Action = [[-0.02853456 -0.10735647  0.17635491 -0.7925283 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 1309 is [True, False, False, False, True, False]
State prediction error at timestep 1309 is tensor(1.3307e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1309 of 1
Current timestep = 1310. State = [[-0.3188624   0.03009015]]. Action = [[ 0.04692277  0.05576202 -0.10251835  0.53864026]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 1310 is [True, False, False, False, True, False]
State prediction error at timestep 1310 is tensor(7.2161e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1310 of 1
Current timestep = 1311. State = [[-0.31891972  0.02993837]]. Action = [[-0.2101374   0.02897388 -0.1449237  -0.4934709 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 1311 is [True, False, False, False, True, False]
State prediction error at timestep 1311 is tensor(2.8318e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1312. State = [[-0.31892076  0.0300429 ]]. Action = [[-0.20332067 -0.07123017 -0.15026975  0.02665436]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 1312 is [True, False, False, False, True, False]
State prediction error at timestep 1312 is tensor(5.7013e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1312 of 1
Current timestep = 1313. State = [[-0.3188818   0.03023427]]. Action = [[ 0.23406401 -0.0169781   0.13208011 -0.9134078 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 1313 is [True, False, False, False, True, False]
State prediction error at timestep 1313 is tensor(2.5009e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1314. State = [[-0.31893936  0.03008314]]. Action = [[-0.22569822 -0.16134179 -0.07345322  0.9250176 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 1314 is [True, False, False, False, True, False]
State prediction error at timestep 1314 is tensor(5.4122e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1315. State = [[-0.31877783  0.03015912]]. Action = [[-0.06348369 -0.2218668   0.24438822 -0.06507111]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 1315 is [True, False, False, False, True, False]
State prediction error at timestep 1315 is tensor(8.1617e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1316. State = [[-0.31889728  0.03025893]]. Action = [[-0.04473922  0.10845494  0.18388718  0.5096252 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 1316 is [True, False, False, False, True, False]
State prediction error at timestep 1316 is tensor(1.0843e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1316 of 1
Current timestep = 1317. State = [[-0.31919104  0.03090658]]. Action = [[ 0.23788479  0.03001079 -0.14586768  0.12814367]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 1317 is [True, False, False, False, True, False]
State prediction error at timestep 1317 is tensor(7.4338e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1317 of 1
Current timestep = 1318. State = [[-0.3192973   0.03152379]]. Action = [[ 0.12077558 -0.15212859  0.1100176   0.72856903]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 1318 is [True, False, False, False, True, False]
State prediction error at timestep 1318 is tensor(1.9834e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1319. State = [[-0.31937405  0.03163038]]. Action = [[ 0.2433694  -0.24241129 -0.1554538  -0.61397755]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 1319 is [True, False, False, False, True, False]
State prediction error at timestep 1319 is tensor(1.9915e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1320. State = [[-0.31952488  0.03193282]]. Action = [[ 0.05233344 -0.2276495   0.0571593   0.6370857 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 1320 is [True, False, False, False, True, False]
State prediction error at timestep 1320 is tensor(1.7589e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1321. State = [[-0.31952974  0.03230796]]. Action = [[ 0.15312618  0.20374751 -0.15825348 -0.39993405]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 1321 is [True, False, False, False, True, False]
State prediction error at timestep 1321 is tensor(3.6438e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1322. State = [[-0.3195706   0.03238979]]. Action = [[-0.1196813  -0.16139393 -0.10631941  0.94716024]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 1322 is [True, False, False, False, True, False]
State prediction error at timestep 1322 is tensor(6.4654e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1322 of 1
Current timestep = 1323. State = [[-0.31958172  0.03253702]]. Action = [[ 0.23354915 -0.13615009 -0.21571192  0.34784675]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 1323 is [True, False, False, False, True, False]
State prediction error at timestep 1323 is tensor(4.6903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1324. State = [[-0.319638    0.03265674]]. Action = [[ 0.1819337   0.0853703  -0.12521625  0.23571825]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 1324 is [True, False, False, False, True, False]
State prediction error at timestep 1324 is tensor(3.8039e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1325. State = [[-0.3196338   0.03276606]]. Action = [[ 0.09400618 -0.02271299  0.21495861 -0.62197846]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 1325 is [True, False, False, False, True, False]
State prediction error at timestep 1325 is tensor(3.8331e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1326. State = [[-0.31884152  0.03277867]]. Action = [[-0.11762002 -0.08938    -0.22008395 -0.95282835]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 1326 is [True, False, False, False, True, False]
State prediction error at timestep 1326 is tensor(8.7037e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1326 of 1
Current timestep = 1327. State = [[-0.31915152  0.03252594]]. Action = [[-0.20354138 -0.01567043  0.06206635  0.02913892]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 1327 is [True, False, False, False, True, False]
State prediction error at timestep 1327 is tensor(3.0033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1328. State = [[-0.31915256  0.03236469]]. Action = [[-0.13259982  0.18383163 -0.17083633  0.72202325]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 1328 is [True, False, False, False, True, False]
State prediction error at timestep 1328 is tensor(2.0819e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1329. State = [[-0.318953    0.03192979]]. Action = [[-0.07027757  0.00188151 -0.17151411 -0.37653077]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 1329 is [True, False, False, False, True, False]
State prediction error at timestep 1329 is tensor(1.1224e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1329 of 1
Current timestep = 1330. State = [[-0.3189561   0.03170242]]. Action = [[ 0.10665131 -0.03826311 -0.01556417  0.6679857 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 1330 is [True, False, False, False, True, False]
State prediction error at timestep 1330 is tensor(1.5904e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1330 of 1
Current timestep = 1331. State = [[-0.31860787  0.03071425]]. Action = [[-0.06953061  0.05859101  0.20950341 -0.6279307 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 1331 is [True, False, False, False, True, False]
State prediction error at timestep 1331 is tensor(1.6654e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1331 of 1
Current timestep = 1332. State = [[-0.3187276   0.03069676]]. Action = [[0.21932483 0.14480925 0.22586274 0.07088661]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 1332 is [True, False, False, False, True, False]
State prediction error at timestep 1332 is tensor(3.1945e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1333. State = [[-0.31862655  0.03075452]]. Action = [[-0.16733377 -0.10048652  0.12153628 -0.92300075]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 1333 is [True, False, False, False, True, False]
State prediction error at timestep 1333 is tensor(1.8768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1334. State = [[-0.3186243   0.03080762]]. Action = [[-0.15777838  0.11336052  0.22799283 -0.59447163]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 1334 is [True, False, False, False, True, False]
State prediction error at timestep 1334 is tensor(2.0257e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1334 of 1
Current timestep = 1335. State = [[-0.3186243   0.03080762]]. Action = [[0.1073316  0.01667634 0.22809398 0.03788614]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 1335 is [True, False, False, False, True, False]
State prediction error at timestep 1335 is tensor(4.1128e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1336. State = [[-0.3186243   0.03080762]]. Action = [[ 0.069823   -0.21624605  0.03691423  0.8731711 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 1336 is [True, False, False, False, True, False]
State prediction error at timestep 1336 is tensor(4.8129e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1337. State = [[-0.31862202  0.03086073]]. Action = [[0.14814189 0.15630662 0.11907393 0.69161224]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 1337 is [True, False, False, False, True, False]
State prediction error at timestep 1337 is tensor(2.2884e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1338. State = [[-0.31857216  0.0310103 ]]. Action = [[ 0.08826274  0.09053215 -0.06919625 -0.18119955]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 1338 is [True, False, False, False, True, False]
State prediction error at timestep 1338 is tensor(8.3777e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1339. State = [[-0.31818828  0.03147025]]. Action = [[-0.24782251  0.17786449  0.18393141  0.8936186 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 1339 is [True, False, False, False, True, False]
State prediction error at timestep 1339 is tensor(1.9078e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1340. State = [[-0.31671     0.03270883]]. Action = [[ 0.11799064  0.13251674 -0.1329484   0.2626791 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 1340 is [True, False, False, False, True, False]
State prediction error at timestep 1340 is tensor(8.1136e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1340 of 1
Current timestep = 1341. State = [[-0.31594568  0.03376404]]. Action = [[ 0.1347866   0.00303459 -0.15033434  0.2994672 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 1341 is [True, False, False, False, True, False]
State prediction error at timestep 1341 is tensor(1.1066e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1342. State = [[-0.31470066  0.03507378]]. Action = [[-0.0823231  -0.23785605 -0.14608778 -0.38933975]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 1342 is [True, False, False, False, True, False]
State prediction error at timestep 1342 is tensor(5.2649e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1343. State = [[-0.31391054  0.03568694]]. Action = [[-0.10796618  0.15493894 -0.09534153 -0.48103917]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 1343 is [True, False, False, False, True, False]
State prediction error at timestep 1343 is tensor(1.1685e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1344. State = [[-0.30949727  0.03827189]]. Action = [[ 0.06260628 -0.0762862  -0.1234034  -0.1837411 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 1344 is [True, False, False, False, True, False]
State prediction error at timestep 1344 is tensor(1.3531e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1344 of 1
Current timestep = 1345. State = [[-0.30847877  0.03836173]]. Action = [[-0.20571354 -0.0886952   0.06278509  0.3352542 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 1345 is [True, False, False, False, True, False]
State prediction error at timestep 1345 is tensor(5.2030e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1346. State = [[-0.30732527  0.03819825]]. Action = [[-0.15604515 -0.17326267  0.02138045 -0.2739575 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 1346 is [True, False, False, False, True, False]
State prediction error at timestep 1346 is tensor(5.2341e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1346 of 1
Current timestep = 1347. State = [[-0.30654186  0.03813181]]. Action = [[ 0.05052108 -0.16178705  0.08009923 -0.4781651 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 1347 is [True, False, False, False, True, False]
State prediction error at timestep 1347 is tensor(1.4475e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1348. State = [[-0.30518126  0.03851825]]. Action = [[ 0.2425102  -0.03582746 -0.06953564 -0.42257905]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 1348 is [True, False, False, False, True, False]
State prediction error at timestep 1348 is tensor(2.4207e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1349. State = [[-0.3037398  0.0384924]]. Action = [[-0.13630754 -0.22748257  0.18333411  0.35892034]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 1349 is [True, False, False, False, True, False]
State prediction error at timestep 1349 is tensor(1.8821e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1350. State = [[-0.30307686  0.03847426]]. Action = [[ 0.14242709  0.22124353  0.16727656 -0.29089022]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 1350 is [True, False, False, False, True, False]
State prediction error at timestep 1350 is tensor(1.9053e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1350 of 1
Current timestep = 1351. State = [[-0.30184424  0.03875753]]. Action = [[-0.11893854  0.24275607  0.16070646 -0.69612974]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 1351 is [True, False, False, False, True, False]
State prediction error at timestep 1351 is tensor(2.7771e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1352. State = [[-0.3008607   0.03882199]]. Action = [[ 0.12266287 -0.1523366   0.2481662  -0.02512449]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 1352 is [True, False, False, False, True, False]
State prediction error at timestep 1352 is tensor(9.5286e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1353. State = [[-0.30047348  0.03882895]]. Action = [[-0.05244662  0.16749641 -0.20578894 -0.10767651]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 1353 is [True, False, False, False, True, False]
State prediction error at timestep 1353 is tensor(9.0413e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1354. State = [[-0.3000001   0.03890964]]. Action = [[-0.1279562  -0.05127649 -0.08839187  0.95025325]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 1354 is [True, False, False, False, True, False]
State prediction error at timestep 1354 is tensor(5.9613e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1354 of 1
Current timestep = 1355. State = [[-0.2999345   0.03896302]]. Action = [[-0.16514066 -0.1846555  -0.19460833  0.34059715]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 1355 is [True, False, False, False, True, False]
State prediction error at timestep 1355 is tensor(1.8106e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1356. State = [[-0.30005044  0.03887165]]. Action = [[ 0.03900468 -0.24182297  0.23840648  0.88338804]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 1356 is [True, False, False, False, True, False]
State prediction error at timestep 1356 is tensor(3.2898e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1356 of 1
Current timestep = 1357. State = [[-0.30006388  0.03881235]]. Action = [[ 0.15521708 -0.03824    -0.13236105  0.01931214]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 1357 is [True, False, False, False, True, False]
State prediction error at timestep 1357 is tensor(3.2435e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1358. State = [[-0.3000834   0.03885453]]. Action = [[-0.1777767   0.02117619 -0.14382994 -0.03908455]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 1358 is [True, False, False, False, True, False]
State prediction error at timestep 1358 is tensor(4.8958e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1359. State = [[-0.3000844   0.03879893]]. Action = [[-0.11343116  0.14877746 -0.05090016 -0.57983243]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 1359 is [True, False, False, False, True, False]
State prediction error at timestep 1359 is tensor(1.6728e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1360. State = [[-0.30012476  0.03873661]]. Action = [[-0.06721064 -0.0293896   0.2283054  -0.74055153]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 1360 is [True, False, False, False, True, False]
State prediction error at timestep 1360 is tensor(1.0430e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1360 of 1
Current timestep = 1361. State = [[-0.30024767  0.03849063]]. Action = [[ 0.19672433  0.11650413  0.02524319 -0.08491755]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 1361 is [True, False, False, False, True, False]
State prediction error at timestep 1361 is tensor(9.6859e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1362. State = [[-0.30028716  0.03838904]]. Action = [[ 0.17832899  0.20662493 -0.15811156  0.27468944]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 1362 is [True, False, False, False, True, False]
State prediction error at timestep 1362 is tensor(2.9035e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1363. State = [[-0.3005937   0.03775054]]. Action = [[ 0.0156849  -0.07585025 -0.14968866  0.48738384]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 1363 is [True, False, False, False, True, False]
State prediction error at timestep 1363 is tensor(1.4889e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1364. State = [[-0.3006535   0.03689646]]. Action = [[ 0.24189752  0.02317452 -0.02788442 -0.781727  ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 1364 is [True, False, False, False, True, False]
State prediction error at timestep 1364 is tensor(3.0031e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1365. State = [[-0.3005471  0.0365082]]. Action = [[0.22728735 0.1673302  0.16600442 0.5762217 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 1365 is [True, False, False, False, True, False]
State prediction error at timestep 1365 is tensor(9.8714e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1366. State = [[-0.30052513  0.03632082]]. Action = [[-0.09886897  0.24764591 -0.22632398 -0.50687635]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 1366 is [True, False, False, False, True, False]
State prediction error at timestep 1366 is tensor(1.5636e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1367. State = [[-0.3007533  0.0358264]]. Action = [[-0.23280282 -0.03058264  0.24370229  0.7106489 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 1367 is [True, False, False, False, True, False]
State prediction error at timestep 1367 is tensor(2.2825e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1367 of 1
Current timestep = 1368. State = [[-0.30065754  0.03540941]]. Action = [[ 0.20658171 -0.06260869 -0.22348714 -0.14956057]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 1368 is [True, False, False, False, True, False]
State prediction error at timestep 1368 is tensor(2.0671e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1369. State = [[-0.30059105  0.03521435]]. Action = [[-0.21722522 -0.0382143  -0.21632956 -0.3486783 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 1369 is [True, False, False, False, True, False]
State prediction error at timestep 1369 is tensor(4.9024e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1370. State = [[-0.30077252  0.03509892]]. Action = [[-0.04026982 -0.19664155 -0.06891268  0.8175725 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 1370 is [True, False, False, False, True, False]
State prediction error at timestep 1370 is tensor(1.8428e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1370 of 1
Current timestep = 1371. State = [[-0.30073532  0.03481464]]. Action = [[-0.08993763 -0.1587117   0.17045069  0.8151535 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 1371 is [True, False, False, False, True, False]
State prediction error at timestep 1371 is tensor(2.5053e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1372. State = [[-0.3010909   0.03414427]]. Action = [[ 0.04335839 -0.12676154 -0.131827   -0.79203874]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 1372 is [True, False, False, False, True, False]
State prediction error at timestep 1372 is tensor(2.0400e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1372 of 1
Current timestep = 1373. State = [[-0.30072263  0.03279601]]. Action = [[0.20036697 0.10163066 0.19653273 0.60942614]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 1373 is [True, False, False, False, True, False]
State prediction error at timestep 1373 is tensor(1.4973e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1374. State = [[-0.30040666  0.03040408]]. Action = [[ 0.01692611  0.07953364 -0.02732132  0.41353214]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 1374 is [True, False, False, False, True, False]
State prediction error at timestep 1374 is tensor(5.8426e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1374 of 1
Current timestep = 1375. State = [[-0.30022517  0.03059842]]. Action = [[-0.0649344  -0.22045437 -0.12363781 -0.35613668]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 1375 is [True, False, False, False, True, False]
State prediction error at timestep 1375 is tensor(1.0883e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1376. State = [[-0.3003327   0.03074919]]. Action = [[-0.01161124  0.10296628 -0.2045056  -0.32181215]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 1376 is [True, False, False, False, True, False]
State prediction error at timestep 1376 is tensor(6.5923e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1376 of 1
Current timestep = 1377. State = [[-0.30100772  0.03234771]]. Action = [[-0.08972731  0.10380909  0.23478925 -0.4802711 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 1377 is [True, False, False, False, True, False]
State prediction error at timestep 1377 is tensor(3.4314e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1377 of 1
Current timestep = 1378. State = [[-0.3014508   0.03345168]]. Action = [[ 0.17924863 -0.10477081  0.1926817   0.12240839]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 1378 is [True, False, False, False, True, False]
State prediction error at timestep 1378 is tensor(2.4046e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1379. State = [[-0.30184057  0.03462442]]. Action = [[ 0.18623137 -0.22613932 -0.07706562  0.97936666]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 1379 is [True, False, False, False, True, False]
State prediction error at timestep 1379 is tensor(1.6532e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1380. State = [[-0.30207008  0.03521055]]. Action = [[ 0.08910012 -0.13551654  0.17397991  0.4948696 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 1380 is [True, False, False, False, True, False]
State prediction error at timestep 1380 is tensor(1.2329e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1381. State = [[-0.30319962  0.03772463]]. Action = [[0.04588053 0.05414224 0.15752795 0.77719223]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 1381 is [True, False, False, False, True, False]
State prediction error at timestep 1381 is tensor(3.2804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1381 of 1
Current timestep = 1382. State = [[-0.30357325  0.03858517]]. Action = [[-0.2342261  -0.13523513  0.1744003  -0.63872963]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 1382 is [True, False, False, False, True, False]
State prediction error at timestep 1382 is tensor(7.9785e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1383. State = [[-0.30384684  0.03927666]]. Action = [[ 0.24825221  0.19046158  0.17359552 -0.03293246]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 1383 is [True, False, False, False, True, False]
State prediction error at timestep 1383 is tensor(1.2780e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1383 of 1
Current timestep = 1384. State = [[-0.30458057  0.04139206]]. Action = [[-0.05169174  0.08864143  0.19919777  0.01592863]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 1384 is [True, False, False, False, True, False]
State prediction error at timestep 1384 is tensor(7.1225e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1385. State = [[-0.30501142  0.04263631]]. Action = [[ 0.16505504 -0.23783983  0.06254339  0.4378674 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 1385 is [True, False, False, False, True, False]
State prediction error at timestep 1385 is tensor(1.6052e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1385 of 1
Current timestep = 1386. State = [[-0.30555847  0.04382296]]. Action = [[-0.19781889 -0.09281501  0.0980677   0.22700405]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 1386 is [True, False, False, False, True, False]
State prediction error at timestep 1386 is tensor(8.5123e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1387. State = [[-0.30580878  0.04436333]]. Action = [[0.2232824  0.2042597  0.23897976 0.24529696]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 1387 is [True, False, False, False, True, False]
State prediction error at timestep 1387 is tensor(3.2903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1388. State = [[-0.3062177   0.04543933]]. Action = [[ 0.10251611  0.19601372 -0.03473276 -0.0632723 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 1388 is [True, False, False, False, True, False]
State prediction error at timestep 1388 is tensor(2.6072e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1389. State = [[-0.30657104  0.04615476]]. Action = [[-0.23763946  0.20317048 -0.19599856  0.7603332 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 1389 is [True, False, False, False, True, False]
State prediction error at timestep 1389 is tensor(1.7118e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1389 of 1
Current timestep = 1390. State = [[-0.3068133  0.0467051]]. Action = [[ 0.05923247  0.23076189  0.20340544 -0.5402053 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 1390 is [True, False, False, False, True, False]
State prediction error at timestep 1390 is tensor(1.2242e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1391. State = [[-0.3069443   0.04714665]]. Action = [[ 0.2364564  -0.14267798  0.09835377  0.47587645]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 1391 is [True, False, False, False, True, False]
State prediction error at timestep 1391 is tensor(1.9713e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1392. State = [[-0.30715075  0.04755762]]. Action = [[-0.10117257 -0.17672908  0.05490685  0.13598537]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 1392 is [True, False, False, False, True, False]
State prediction error at timestep 1392 is tensor(2.1274e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1393. State = [[-0.3072614   0.04776095]]. Action = [[-0.15473165  0.16941279 -0.09041941 -0.4433586 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 1393 is [True, False, False, False, True, False]
State prediction error at timestep 1393 is tensor(9.6393e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1394. State = [[-0.30754665  0.04863169]]. Action = [[ 0.04573929 -0.01234795  0.13243812 -0.46453834]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 1394 is [True, False, False, False, True, False]
State prediction error at timestep 1394 is tensor(1.3722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1394 of 1
Current timestep = 1395. State = [[-0.307484    0.04860323]]. Action = [[-0.14333493 -0.14593273  0.19454482 -0.71691805]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 1395 is [True, False, False, False, True, False]
State prediction error at timestep 1395 is tensor(8.2971e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1396. State = [[-0.30753535  0.04858793]]. Action = [[ 0.06392455  0.16961181  0.10192966 -0.88910246]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 1396 is [True, False, False, False, True, False]
State prediction error at timestep 1396 is tensor(4.4570e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1396 of 1
Current timestep = 1397. State = [[-0.30753604  0.04869888]]. Action = [[-0.19747397  0.18817443  0.06233194  0.8918128 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 1397 is [True, False, False, False, True, False]
State prediction error at timestep 1397 is tensor(8.3454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1398. State = [[-0.30748507  0.04876966]]. Action = [[-0.13316327 -0.23156543  0.05452842  0.90483534]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 1398 is [True, False, False, False, True, False]
State prediction error at timestep 1398 is tensor(4.9449e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1399. State = [[-0.30753717  0.0488653 ]]. Action = [[-0.2234142  -0.03756256  0.17978808 -0.36421645]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 1399 is [True, False, False, False, True, False]
State prediction error at timestep 1399 is tensor(1.6831e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1399 of 1
Current timestep = 1400. State = [[-0.30747256  0.04879827]]. Action = [[-0.03948736 -0.11519852 -0.13094422 -0.4271446 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 1400 is [True, False, False, False, True, False]
State prediction error at timestep 1400 is tensor(7.5768e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1401. State = [[-0.3073081   0.04755442]]. Action = [[ 0.07309803 -0.04714328 -0.1411216  -0.72117627]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 1401 is [True, False, False, False, True, False]
State prediction error at timestep 1401 is tensor(3.4550e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1402. State = [[-0.30692396  0.04676462]]. Action = [[ 0.19815552 -0.1520928   0.24750686  0.18335915]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 1402 is [True, False, False, False, True, False]
State prediction error at timestep 1402 is tensor(4.5924e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1403. State = [[-0.30660078  0.04629926]]. Action = [[0.15906543 0.15570068 0.19883999 0.94919133]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 1403 is [True, False, False, False, True, False]
State prediction error at timestep 1403 is tensor(4.1628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1404. State = [[-0.3065757   0.04602739]]. Action = [[0.20020717 0.0108268  0.22237986 0.62927306]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 1404 is [True, False, False, False, True, False]
State prediction error at timestep 1404 is tensor(1.9331e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1405. State = [[-0.3066768   0.04573148]]. Action = [[-0.03813764  0.20623958  0.03026602 -0.15933746]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 1405 is [True, False, False, False, True, False]
State prediction error at timestep 1405 is tensor(8.9542e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1406. State = [[-0.3063232   0.04537704]]. Action = [[-0.24457774  0.09408009  0.05875549  0.16478014]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 1406 is [True, False, False, False, True, False]
State prediction error at timestep 1406 is tensor(7.9188e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1407. State = [[-0.3063326   0.04520711]]. Action = [[-0.14803106 -0.05931976  0.12212008  0.5811901 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 1407 is [True, False, False, False, True, False]
State prediction error at timestep 1407 is tensor(1.3280e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1407 of 1
Current timestep = 1408. State = [[-0.30632526  0.04505118]]. Action = [[-0.24851973  0.13534904 -0.20736383  0.01191676]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 1408 is [True, False, False, False, True, False]
State prediction error at timestep 1408 is tensor(3.1862e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1409. State = [[-0.3061791   0.04450488]]. Action = [[ 0.09711874  0.1193617   0.06475636 -0.9512233 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 1409 is [True, False, False, False, True, False]
State prediction error at timestep 1409 is tensor(3.4515e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1409 of 1
Current timestep = 1410. State = [[-0.30592695  0.04490976]]. Action = [[ 0.18815953 -0.2229381   0.1167649  -0.07329458]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 1410 is [True, False, False, False, True, False]
State prediction error at timestep 1410 is tensor(3.0567e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1411. State = [[-0.3057619   0.04514718]]. Action = [[-0.18078218  0.08159119 -0.0394996   0.10077035]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 1411 is [True, False, False, False, True, False]
State prediction error at timestep 1411 is tensor(3.9460e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1412. State = [[-0.30547437  0.0452468 ]]. Action = [[-0.17717205 -0.13453603 -0.18969953 -0.86949486]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 1412 is [True, False, False, False, True, False]
State prediction error at timestep 1412 is tensor(1.2225e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1413. State = [[-0.3052302   0.04535403]]. Action = [[-0.18910407  0.23033291 -0.18600945  0.424289  ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 1413 is [True, False, False, False, True, False]
State prediction error at timestep 1413 is tensor(6.6809e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1413 of 1
Current timestep = 1414. State = [[-0.30498365  0.04562725]]. Action = [[ 0.23600686 -0.2246202  -0.04917227 -0.28433776]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 1414 is [True, False, False, False, True, False]
State prediction error at timestep 1414 is tensor(2.1797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1415. State = [[-0.30444506  0.04598745]]. Action = [[-0.12453339 -0.056705   -0.20132558 -0.87291616]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 1415 is [True, False, False, False, True, False]
State prediction error at timestep 1415 is tensor(6.9496e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1416. State = [[-0.30453187  0.04596549]]. Action = [[-0.08202618  0.02065116  0.05739206  0.6404207 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 1416 is [True, False, False, False, True, False]
State prediction error at timestep 1416 is tensor(7.9123e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1416 of 1
Current timestep = 1417. State = [[-0.30493173  0.04604909]]. Action = [[-0.06419489 -0.214655   -0.16574259  0.48696685]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 1417 is [True, False, False, False, True, False]
State prediction error at timestep 1417 is tensor(4.0531e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1418. State = [[-0.30510268  0.04624744]]. Action = [[-0.19243453  0.00969306 -0.11787933 -0.80758804]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 1418 is [True, False, False, False, True, False]
State prediction error at timestep 1418 is tensor(1.9162e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1419. State = [[-0.30500132  0.04636813]]. Action = [[ 0.10821176  0.2222594   0.07628116 -0.52022785]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 1419 is [True, False, False, False, True, False]
State prediction error at timestep 1419 is tensor(3.5405e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1420. State = [[-0.30562142  0.04656455]]. Action = [[0.12958086 0.12061685 0.00579727 0.25430632]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 1420 is [True, False, False, False, True, False]
State prediction error at timestep 1420 is tensor(8.3610e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1420 of 1
Current timestep = 1421. State = [[-0.3058231   0.04727311]]. Action = [[0.15043509 0.01509082 0.15920413 0.85032   ]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 1421 is [True, False, False, False, True, False]
State prediction error at timestep 1421 is tensor(1.6435e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1422. State = [[-0.30585378  0.04791703]]. Action = [[-0.19424745 -0.2197327  -0.02059206 -0.74140394]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 1422 is [True, False, False, False, True, False]
State prediction error at timestep 1422 is tensor(1.0115e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1423. State = [[-0.3060287   0.04875947]]. Action = [[ 0.09597766  0.01814443 -0.16127586  0.81091356]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 1423 is [True, False, False, False, True, False]
State prediction error at timestep 1423 is tensor(1.0701e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1423 of 1
Current timestep = 1424. State = [[-0.3053918   0.04914077]]. Action = [[ 0.17097509 -0.24212494  0.19910932 -0.7691845 ]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 1424 is [True, False, False, False, True, False]
State prediction error at timestep 1424 is tensor(1.5128e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1424 of 1
Current timestep = 1425. State = [[-0.30427152  0.04952847]]. Action = [[ 0.0463804  -0.12132546  0.01859993  0.5891309 ]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 1425 is [True, False, False, False, True, False]
State prediction error at timestep 1425 is tensor(3.0924e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1426. State = [[-0.30364054  0.04905857]]. Action = [[-0.17207724  0.19564706 -0.24288148  0.46086657]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 1426 is [True, False, False, False, True, False]
State prediction error at timestep 1426 is tensor(5.0928e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1427. State = [[-0.30237734  0.04851524]]. Action = [[ 0.04356456 -0.00129458 -0.09100401  0.06258142]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 1427 is [True, False, False, False, True, False]
State prediction error at timestep 1427 is tensor(1.6335e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1428. State = [[-0.30200142  0.04849608]]. Action = [[ 0.21104404 -0.06395218 -0.00503491  0.31586325]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 1428 is [True, False, False, False, True, False]
State prediction error at timestep 1428 is tensor(2.0724e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1429. State = [[-0.3017547   0.04856077]]. Action = [[ 0.02838928 -0.17980012 -0.02592762 -0.138067  ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 1429 is [True, False, False, False, True, False]
State prediction error at timestep 1429 is tensor(1.1088e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1430. State = [[-0.30140194  0.0486485 ]]. Action = [[ 0.2213667  -0.22448593 -0.06150204  0.3258158 ]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 1430 is [True, False, False, False, True, False]
State prediction error at timestep 1430 is tensor(4.7105e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1431. State = [[-0.30106342  0.04860482]]. Action = [[-0.1956903   0.24676311 -0.18473884 -0.32413197]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 1431 is [True, False, False, False, True, False]
State prediction error at timestep 1431 is tensor(1.1577e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1432. State = [[-0.3002075   0.04883401]]. Action = [[-0.22785813 -0.22396314 -0.20199065  0.58512425]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 1432 is [True, False, False, False, True, False]
State prediction error at timestep 1432 is tensor(2.3429e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1432 of 1
Current timestep = 1433. State = [[-0.29912013  0.04914029]]. Action = [[-0.12435694 -0.01868436  0.03067136 -0.6529513 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 1433 is [True, False, False, False, True, False]
State prediction error at timestep 1433 is tensor(1.6078e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1434. State = [[-0.29935306  0.04902386]]. Action = [[-0.19413759  0.0594098   0.14958602 -0.60135096]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 1434 is [True, False, False, False, True, False]
State prediction error at timestep 1434 is tensor(1.0786e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1435. State = [[-0.2994191   0.04895983]]. Action = [[-0.20314045 -0.21320255 -0.23478514 -0.5061633 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 1435 is [True, False, False, False, True, False]
State prediction error at timestep 1435 is tensor(1.3437e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1436. State = [[-0.29932883  0.04898211]]. Action = [[ 0.14684504  0.07306898 -0.21469875 -0.5217444 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 1436 is [True, False, False, False, True, False]
State prediction error at timestep 1436 is tensor(3.9700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1437. State = [[-0.2990995   0.04886808]]. Action = [[-0.00280671  0.21822333  0.22166413 -0.7255404 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 1437 is [True, False, False, False, True, False]
State prediction error at timestep 1437 is tensor(3.6641e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1438. State = [[-0.29931566  0.0489818 ]]. Action = [[0.01179513 0.12701064 0.07415366 0.3363279 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 1438 is [True, False, False, False, True, False]
State prediction error at timestep 1438 is tensor(1.1313e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1438 of 1
Current timestep = 1439. State = [[-0.30004475  0.0507382 ]]. Action = [[ 0.07914668  0.12612343  0.14667404 -0.10671443]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 1439 is [True, False, False, False, True, False]
State prediction error at timestep 1439 is tensor(7.4615e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1439 of 1
Current timestep = 1440. State = [[-0.3000805   0.05178211]]. Action = [[-0.15123045 -0.0808441  -0.09888068 -0.5418418 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 1440 is [True, False, False, False, True, False]
State prediction error at timestep 1440 is tensor(1.3635e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1441. State = [[-0.3000537   0.05270295]]. Action = [[-0.16808498  0.06602341  0.24233067 -0.1302213 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 1441 is [True, False, False, False, True, False]
State prediction error at timestep 1441 is tensor(2.0091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1442. State = [[-0.30009708  0.05305082]]. Action = [[ 0.04717642 -0.15707603 -0.05942249 -0.8509606 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 1442 is [True, False, False, False, True, False]
State prediction error at timestep 1442 is tensor(1.0484e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1443. State = [[-0.3001592   0.05368678]]. Action = [[ 0.1996668   0.10896978 -0.05434811  0.5263566 ]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 1443 is [True, False, False, False, True, False]
State prediction error at timestep 1443 is tensor(3.2892e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1443 of 1
Current timestep = 1444. State = [[-0.30024055  0.05465154]]. Action = [[ 0.23535585 -0.19647641 -0.13206826 -0.5128368 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 1444 is [True, False, False, False, True, False]
State prediction error at timestep 1444 is tensor(2.3086e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1445. State = [[-0.30042675  0.05565696]]. Action = [[ 0.08285645 -0.08440112 -0.22249195 -0.35740054]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 1445 is [True, False, False, False, True, False]
State prediction error at timestep 1445 is tensor(3.7916e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1445 of 1
Current timestep = 1446. State = [[-0.2996413   0.05574518]]. Action = [[ 0.23721477 -0.23474663  0.01277375  0.5336149 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 1446 is [True, False, False, False, True, False]
State prediction error at timestep 1446 is tensor(2.7820e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1447. State = [[-0.2990861   0.05564031]]. Action = [[-0.23281398 -0.01385558 -0.16084334  0.4532168 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 1447 is [True, False, False, False, True, False]
State prediction error at timestep 1447 is tensor(9.4989e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1448. State = [[-0.29894546  0.05561628]]. Action = [[ 0.16066617 -0.00092702 -0.05079851 -0.38330495]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 1448 is [True, False, False, False, True, False]
State prediction error at timestep 1448 is tensor(3.1643e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1448 of 1
Current timestep = 1449. State = [[-0.2983496   0.05598793]]. Action = [[0.10261059 0.2115441  0.03083411 0.7178552 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 1449 is [True, False, False, False, True, False]
State prediction error at timestep 1449 is tensor(2.5698e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1450. State = [[-0.29773065  0.055918  ]]. Action = [[-0.04972281 -0.20760192 -0.15654105 -0.5629015 ]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 1450 is [True, False, False, False, True, False]
State prediction error at timestep 1450 is tensor(1.3485e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1451. State = [[-0.29751676  0.05583914]]. Action = [[ 0.08405787 -0.15382956 -0.22308101 -0.26670706]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 1451 is [True, False, False, False, True, False]
State prediction error at timestep 1451 is tensor(1.2954e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1452. State = [[-0.29719254  0.05608501]]. Action = [[ 0.10015088  0.15040845  0.11469236 -0.7989332 ]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 1452 is [True, False, False, False, True, False]
State prediction error at timestep 1452 is tensor(9.0470e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1452 of 1
Current timestep = 1453. State = [[-0.29686555  0.05612857]]. Action = [[ 0.1060394  -0.21061902  0.00636771 -0.8438114 ]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 1453 is [True, False, False, False, True, False]
State prediction error at timestep 1453 is tensor(7.1921e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1454. State = [[-0.29665276  0.05604985]]. Action = [[-0.18184336  0.18290353  0.22351533 -0.36885893]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 1454 is [True, False, False, False, True, False]
State prediction error at timestep 1454 is tensor(1.9282e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1455. State = [[-0.2960915   0.05617027]]. Action = [[ 0.18687257  0.11386591 -0.04477525  0.69833386]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 1455 is [True, False, False, False, True, False]
State prediction error at timestep 1455 is tensor(2.3828e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1456. State = [[-0.2960405   0.05618348]]. Action = [[ 0.02729267 -0.17207374 -0.09611377  0.6017648 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 1456 is [True, False, False, False, True, False]
State prediction error at timestep 1456 is tensor(1.0433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1456 of 1
Current timestep = 1457. State = [[-0.29497698  0.05641527]]. Action = [[-0.04284588  0.12686282  0.16334876  0.92219174]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 1457 is [True, False, False, False, True, False]
State prediction error at timestep 1457 is tensor(3.5426e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1458. State = [[-0.29526126  0.05734196]]. Action = [[ 0.02362931  0.14460725  0.18272749 -0.16097856]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 1458 is [True, False, False, False, True, False]
State prediction error at timestep 1458 is tensor(4.8460e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1459. State = [[-0.29538766  0.05797305]]. Action = [[ 0.17675507  0.04566887 -0.2231832  -0.64463824]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 1459 is [True, False, False, False, True, False]
State prediction error at timestep 1459 is tensor(2.0132e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1460. State = [[-0.2952421  0.0584432]]. Action = [[ 0.23980418 -0.1996834   0.10079694  0.49686182]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 1460 is [True, False, False, False, True, False]
State prediction error at timestep 1460 is tensor(3.5384e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1461. State = [[-0.29533356  0.05887191]]. Action = [[-0.20973402  0.0240784  -0.03317416  0.3011316 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 1461 is [True, False, False, False, True, False]
State prediction error at timestep 1461 is tensor(8.0135e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1462. State = [[-0.29580596  0.0595918 ]]. Action = [[-0.1754163   0.2159653   0.11995453  0.73212826]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 1462 is [True, False, False, False, True, False]
State prediction error at timestep 1462 is tensor(1.7085e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1462 of 1
Current timestep = 1463. State = [[-0.29586855  0.05990422]]. Action = [[ 0.23142827 -0.18063527  0.2176562  -0.22734565]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 1463 is [True, False, False, False, True, False]
State prediction error at timestep 1463 is tensor(2.9158e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1464. State = [[-0.29600003  0.06021499]]. Action = [[-0.17932288  0.09792301  0.1361407  -0.4729321 ]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 1464 is [True, False, False, False, True, False]
State prediction error at timestep 1464 is tensor(4.1816e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1465. State = [[-0.296114    0.06071997]]. Action = [[-0.1090824  -0.02662185 -0.13648269 -0.42739713]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 1465 is [True, False, False, False, True, False]
State prediction error at timestep 1465 is tensor(2.0719e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1465 of 1
Current timestep = 1466. State = [[-0.2962224   0.06093171]]. Action = [[-0.17416397 -0.18770353  0.17448276 -0.08981401]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 1466 is [True, False, False, False, True, False]
State prediction error at timestep 1466 is tensor(2.9551e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1467. State = [[-0.2963133   0.06110032]]. Action = [[-0.08023056 -0.24283665 -0.05342352  0.67165613]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 1467 is [True, False, False, False, True, False]
State prediction error at timestep 1467 is tensor(3.3138e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1468. State = [[-0.29635674  0.06118506]]. Action = [[-0.02715601 -0.23091215 -0.12499884 -0.57898426]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 1468 is [True, False, False, False, True, False]
State prediction error at timestep 1468 is tensor(7.9215e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1468 of 1
Current timestep = 1469. State = [[-0.29645914  0.06132476]]. Action = [[ 0.06351829  0.17002988 -0.01028067  0.13929033]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 1469 is [True, False, False, False, True, False]
State prediction error at timestep 1469 is tensor(2.5624e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1470. State = [[-0.2967456   0.06175986]]. Action = [[-0.18204297  0.22570947 -0.04675679 -0.7279295 ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 1470 is [True, False, False, False, True, False]
State prediction error at timestep 1470 is tensor(2.2440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1471. State = [[-0.29679143  0.06190121]]. Action = [[0.1957801  0.09040689 0.00751033 0.04734802]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 1471 is [True, False, False, False, True, False]
State prediction error at timestep 1471 is tensor(1.6092e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1472. State = [[-0.29681313  0.06194333]]. Action = [[-0.12924087 -0.15183398  0.07582709  0.03701246]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 1472 is [True, False, False, False, True, False]
State prediction error at timestep 1472 is tensor(1.8564e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1473. State = [[-0.2968109   0.06188663]]. Action = [[-0.21124507 -0.18935366 -0.15416875  0.16962314]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 1473 is [True, False, False, False, True, False]
State prediction error at timestep 1473 is tensor(1.3277e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1473 of 1
Current timestep = 1474. State = [[-0.2969041   0.06201395]]. Action = [[ 0.14444548 -0.15416199 -0.18614006 -0.5561334 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 1474 is [True, False, False, False, True, False]
State prediction error at timestep 1474 is tensor(1.8624e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1475. State = [[-0.2970057   0.06218781]]. Action = [[-0.09060258 -0.07880309 -0.19948055 -0.41466057]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 1475 is [True, False, False, False, True, False]
State prediction error at timestep 1475 is tensor(1.7830e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1476. State = [[-0.29711846  0.06204883]]. Action = [[-0.02866279 -0.18956856 -0.05739215  0.8445685 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 1476 is [True, False, False, False, True, False]
State prediction error at timestep 1476 is tensor(5.5632e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1477. State = [[-0.2974006  0.0617134]]. Action = [[-0.07528381 -0.02133237 -0.21879071  0.2470926 ]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 1477 is [True, False, False, False, True, False]
State prediction error at timestep 1477 is tensor(2.5934e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1477 of 1
Current timestep = 1478. State = [[-0.29860198  0.06058102]]. Action = [[ 0.09090763 -0.11626478 -0.15124644  0.26214492]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 1478 is [True, False, False, False, True, False]
State prediction error at timestep 1478 is tensor(8.7978e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1478 of 1
Current timestep = 1479. State = [[-0.29842192  0.05778344]]. Action = [[ 0.09215647  0.12905762 -0.0327692  -0.92262006]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 1479 is [True, False, False, False, True, False]
State prediction error at timestep 1479 is tensor(1.8350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1480. State = [[-0.29827946  0.05792179]]. Action = [[ 0.09652126 -0.08846509 -0.0190134   0.3985548 ]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 1480 is [True, False, False, False, True, False]
State prediction error at timestep 1480 is tensor(2.0034e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1480 of 1
Current timestep = 1481. State = [[-0.2970359   0.05669184]]. Action = [[ 0.08905736  0.01223686 -0.00504875  0.59120643]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 1481 is [True, False, False, False, True, False]
State prediction error at timestep 1481 is tensor(5.0587e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1481 of 1
Current timestep = 1482. State = [[-0.2961277   0.05661753]]. Action = [[-0.24040659  0.13656604  0.02809626 -0.9805267 ]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 1482 is [True, False, False, False, True, False]
State prediction error at timestep 1482 is tensor(4.4242e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1483. State = [[-0.2956516   0.05660801]]. Action = [[ 0.16800904  0.2457152  -0.14475672 -0.8137766 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 1483 is [True, False, False, False, True, False]
State prediction error at timestep 1483 is tensor(3.3642e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1484. State = [[-0.2954099  0.056515 ]]. Action = [[-0.22944845 -0.15427546  0.20402884 -0.40915716]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 1484 is [True, False, False, False, True, False]
State prediction error at timestep 1484 is tensor(9.5800e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1484 of 1
Current timestep = 1485. State = [[-0.29481706  0.05645988]]. Action = [[ 0.23141721 -0.09822366  0.13812774  0.33372605]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 1485 is [True, False, False, False, True, False]
State prediction error at timestep 1485 is tensor(1.0751e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1486. State = [[-0.29307544  0.05623042]]. Action = [[ 0.09870425 -0.03493901  0.0605391   0.535058  ]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 1486 is [True, False, False, False, True, False]
State prediction error at timestep 1486 is tensor(5.1631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1487. State = [[-0.28985023  0.055024  ]]. Action = [[ 0.04489523 -0.04261392 -0.03914839  0.51583135]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 1487 is [True, False, False, False, True, False]
State prediction error at timestep 1487 is tensor(5.6665e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1488. State = [[-0.2887298   0.05461023]]. Action = [[ 0.17669398 -0.09964834 -0.15094662  0.83270407]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 1488 is [True, False, False, False, True, False]
State prediction error at timestep 1488 is tensor(6.0432e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1489. State = [[-0.2854974   0.05313837]]. Action = [[0.03072289 0.04981524 0.08555222 0.8365836 ]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 1489 is [True, False, False, False, True, False]
State prediction error at timestep 1489 is tensor(4.4419e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1490. State = [[-0.2846169   0.05307515]]. Action = [[-0.16866003  0.04224968 -0.05309835  0.10839128]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 1490 is [True, False, False, False, True, False]
State prediction error at timestep 1490 is tensor(3.0911e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1491. State = [[-0.28115883  0.0535255 ]]. Action = [[-0.01829454 -0.00872532  0.20278639  0.20668924]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 1491 is [True, False, False, False, True, False]
State prediction error at timestep 1491 is tensor(7.6950e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1492. State = [[-0.28059748  0.05362137]]. Action = [[-0.22406016  0.23533064 -0.05008668  0.2892189 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 1492 is [True, False, False, False, True, False]
State prediction error at timestep 1492 is tensor(8.6674e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1493. State = [[-0.2799547   0.05369895]]. Action = [[ 0.18970335 -0.01439504 -0.04901257  0.2582314 ]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 1493 is [True, False, False, False, True, False]
State prediction error at timestep 1493 is tensor(1.4839e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1494. State = [[-0.2791122   0.05367883]]. Action = [[ 0.23185936  0.12695107  0.05758086 -0.07371783]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 1494 is [True, False, False, False, True, False]
State prediction error at timestep 1494 is tensor(9.2088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1495. State = [[-0.2787214   0.05383017]]. Action = [[ 0.18867016 -0.1804333  -0.14917381 -0.8569295 ]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 1495 is [True, False, False, False, True, False]
State prediction error at timestep 1495 is tensor(9.6923e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1496. State = [[-0.27830258  0.05383945]]. Action = [[-0.21094626  0.00752401  0.08953899 -0.4402851 ]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 1496 is [True, False, False, False, True, False]
State prediction error at timestep 1496 is tensor(1.4808e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1496 of 1
Current timestep = 1497. State = [[-0.27817822  0.05381402]]. Action = [[ 0.1432882  -0.11435515  0.1018967   0.21372044]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 1497 is [True, False, False, False, True, False]
State prediction error at timestep 1497 is tensor(1.9835e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1498. State = [[-0.27821282  0.05384536]]. Action = [[ 0.18713576 -0.19512834  0.0382002  -0.21220928]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 1498 is [True, False, False, False, True, False]
State prediction error at timestep 1498 is tensor(3.0716e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1499. State = [[-0.27784395  0.05390621]]. Action = [[ 0.20716992 -0.15995581  0.02721897  0.6104524 ]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 1499 is [True, False, False, False, True, False]
State prediction error at timestep 1499 is tensor(2.9497e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1500. State = [[-0.27779934  0.05385232]]. Action = [[-0.18719359 -0.2147327   0.08071983  0.8896475 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 1500 is [True, False, False, False, True, False]
State prediction error at timestep 1500 is tensor(1.4462e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1500 of 1
Current timestep = 1501. State = [[-0.27797288  0.05381515]]. Action = [[ 0.21407425 -0.07440487 -0.19021624  0.03991723]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 1501 is [True, False, False, False, True, False]
State prediction error at timestep 1501 is tensor(1.0838e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1502. State = [[-0.2777483   0.05394744]]. Action = [[ 0.22799471  0.21031317 -0.02524754 -0.6356033 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 1502 is [True, False, False, False, True, False]
State prediction error at timestep 1502 is tensor(2.5387e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1503. State = [[-0.27773803  0.05389136]]. Action = [[-0.17143463  0.09878778  0.06252685  0.23156142]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 1503 is [True, False, False, False, True, False]
State prediction error at timestep 1503 is tensor(5.4063e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1504. State = [[-0.27782777  0.05386837]]. Action = [[ 0.12477809 -0.24386975 -0.18714121 -0.3979786 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 1504 is [True, False, False, False, True, False]
State prediction error at timestep 1504 is tensor(2.2013e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1504 of 1
Current timestep = 1505. State = [[-0.27773803  0.05389136]]. Action = [[ 0.04633915  0.145738   -0.19730543 -0.7983515 ]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 1505 is [True, False, False, False, True, False]
State prediction error at timestep 1505 is tensor(1.2320e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1506. State = [[-0.27773803  0.05389136]]. Action = [[ 0.24667472  0.16550815 -0.14479397 -0.91690314]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 1506 is [True, False, False, False, True, False]
State prediction error at timestep 1506 is tensor(7.9992e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1507. State = [[-0.27773803  0.05389136]]. Action = [[-0.14293224  0.20340383 -0.21117659  0.58282363]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 1507 is [True, False, False, False, True, False]
State prediction error at timestep 1507 is tensor(1.1297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1508. State = [[-0.27773803  0.05389136]]. Action = [[0.17624539 0.00223365 0.02103859 0.30245852]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 1508 is [True, False, False, False, True, False]
State prediction error at timestep 1508 is tensor(9.5890e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1508 of 1
Current timestep = 1509. State = [[-0.27773803  0.05389136]]. Action = [[-0.19851129 -0.11191645 -0.22316675 -0.21331096]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 1509 is [True, False, False, False, True, False]
State prediction error at timestep 1509 is tensor(2.4662e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1510. State = [[-0.27773803  0.05389136]]. Action = [[-0.02868104  0.14028725  0.20637456  0.15573668]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 1510 is [True, False, False, False, True, False]
State prediction error at timestep 1510 is tensor(2.9577e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1511. State = [[-0.27773803  0.05389136]]. Action = [[-0.12524913 -0.20030543 -0.15942381  0.41092277]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 1511 is [True, False, False, False, True, False]
State prediction error at timestep 1511 is tensor(8.6133e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1511 of 1
Current timestep = 1512. State = [[-0.27767238  0.05381736]]. Action = [[ 0.02679411 -0.08320737  0.14462343 -0.77272385]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 1512 is [True, False, False, False, True, False]
State prediction error at timestep 1512 is tensor(1.2621e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1513. State = [[-0.27730507  0.05308114]]. Action = [[ 0.15764767  0.23068684  0.05848658 -0.25668848]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 1513 is [True, False, False, False, True, False]
State prediction error at timestep 1513 is tensor(3.9164e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1513 of 1
Current timestep = 1514. State = [[-0.27676022  0.05243138]]. Action = [[-0.12410143  0.18698335  0.14635435  0.18480372]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 1514 is [True, False, False, False, True, False]
State prediction error at timestep 1514 is tensor(2.0903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1515. State = [[-0.27670023  0.05209809]]. Action = [[-0.11450258 -0.14975719 -0.19873728  0.41339564]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 1515 is [True, False, False, False, True, False]
State prediction error at timestep 1515 is tensor(4.7624e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1516. State = [[-0.2761023   0.05060941]]. Action = [[ 0.12654471  0.10913172 -0.18645287 -0.36077476]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 1516 is [True, False, False, False, True, False]
State prediction error at timestep 1516 is tensor(3.9223e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1516 of 1
Current timestep = 1517. State = [[-0.27517098  0.05085153]]. Action = [[-0.15657271  0.13759387 -0.05060826  0.30816984]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 1517 is [True, False, False, False, True, False]
State prediction error at timestep 1517 is tensor(9.4558e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1518. State = [[-0.27447507  0.05111887]]. Action = [[-0.19815448  0.08443236 -0.14884852  0.23043168]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 1518 is [True, False, False, False, True, False]
State prediction error at timestep 1518 is tensor(3.0179e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1519. State = [[-0.2723781   0.05148666]]. Action = [[ 0.00804806  0.04293966 -0.23539008 -0.70756495]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 1519 is [True, False, False, False, True, False]
State prediction error at timestep 1519 is tensor(1.5093e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1519 of 1
Current timestep = 1520. State = [[-0.27111253  0.05251919]]. Action = [[-0.06440157  0.11278421  0.0689005   0.17816162]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 1520 is [True, False, False, False, True, False]
State prediction error at timestep 1520 is tensor(1.7207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1520 of 1
Current timestep = 1521. State = [[-0.27144724  0.05391138]]. Action = [[-0.22872593 -0.02121174  0.06794164 -0.6982253 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 1521 is [True, False, False, False, True, False]
State prediction error at timestep 1521 is tensor(6.5701e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1522. State = [[-0.27139172  0.05488823]]. Action = [[ 0.22925901  0.00171459 -0.07792881 -0.07369763]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 1522 is [True, False, False, False, True, False]
State prediction error at timestep 1522 is tensor(7.1472e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1523. State = [[-0.2714924   0.05529324]]. Action = [[ 0.16090918  0.19561812 -0.05180141 -0.21004951]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 1523 is [True, False, False, False, True, False]
State prediction error at timestep 1523 is tensor(7.9510e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1524. State = [[-0.27181932  0.05655647]]. Action = [[ 0.07174462  0.01130474 -0.14968416 -0.19678849]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 1524 is [True, False, False, False, True, False]
State prediction error at timestep 1524 is tensor(1.0284e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1524 of 1
Current timestep = 1525. State = [[-0.27129713  0.05700122]]. Action = [[-0.16752614 -0.12866841 -0.11226405  0.97385097]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 1525 is [True, False, False, False, True, False]
State prediction error at timestep 1525 is tensor(3.5953e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1526. State = [[-0.27049446  0.05780686]]. Action = [[ 0.09571478  0.12955028  0.14766258 -0.8039537 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 1526 is [True, False, False, False, True, False]
State prediction error at timestep 1526 is tensor(1.4509e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1526 of 1
Current timestep = 1527. State = [[-0.2697761   0.05933582]]. Action = [[ 0.19198483 -0.06811623  0.04285952  0.9553435 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 1527 is [True, False, False, False, True, False]
State prediction error at timestep 1527 is tensor(2.2596e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1528. State = [[-0.2693295   0.06001168]]. Action = [[ 0.14144176 -0.19579573 -0.02275273  0.1955117 ]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 1528 is [True, False, False, False, True, False]
State prediction error at timestep 1528 is tensor(2.6301e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1529. State = [[-0.26651376  0.06296166]]. Action = [[-0.06269571  0.01750723 -0.17485541  0.5156547 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 1529 is [True, False, False, False, True, False]
State prediction error at timestep 1529 is tensor(2.4874e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1529 of 1
Current timestep = 1530. State = [[-0.26609373  0.06408542]]. Action = [[0.24711895 0.16938779 0.13570285 0.04943657]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 1530 is [True, False, False, False, True, False]
State prediction error at timestep 1530 is tensor(2.7168e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1531. State = [[-0.26592812  0.06482133]]. Action = [[ 0.01893005 -0.1590984  -0.08637209  0.7023907 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 1531 is [True, False, False, False, True, False]
State prediction error at timestep 1531 is tensor(6.8369e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1532. State = [[-0.26554564  0.06554957]]. Action = [[-0.1861873  -0.04196556 -0.19689223  0.7961817 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 1532 is [True, False, False, False, True, False]
State prediction error at timestep 1532 is tensor(2.2496e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1533. State = [[-0.26504707  0.06615472]]. Action = [[-0.16890018 -0.14196569  0.20208955 -0.9622856 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 1533 is [True, False, False, False, True, False]
State prediction error at timestep 1533 is tensor(3.4544e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1533 of 1
Current timestep = 1534. State = [[-0.26450336  0.06740833]]. Action = [[ 0.14091605  0.00106972 -0.15330686  0.64161944]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 1534 is [True, False, False, False, True, False]
State prediction error at timestep 1534 is tensor(1.8264e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1535. State = [[-0.26414138  0.06798586]]. Action = [[ 0.16269928 -0.07969695  0.07606962 -0.05177218]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 1535 is [True, False, False, False, True, False]
State prediction error at timestep 1535 is tensor(9.6902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1536. State = [[-0.26393262  0.06830957]]. Action = [[ 0.2298972   0.0291307  -0.13162161 -0.30668128]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 1536 is [True, False, False, False, True, False]
State prediction error at timestep 1536 is tensor(2.7166e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1537. State = [[-0.2637202  0.069235 ]]. Action = [[ 0.06060857  0.10236087 -0.02077878 -0.46259826]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 1537 is [True, False, False, False, True, False]
State prediction error at timestep 1537 is tensor(1.7322e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1537 of 1
Current timestep = 1538. State = [[-0.26331615  0.07067344]]. Action = [[-0.18713565  0.06956357  0.0544591  -0.8899072 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 1538 is [True, False, False, False, True, False]
State prediction error at timestep 1538 is tensor(9.5048e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1539. State = [[-0.2630514   0.07127267]]. Action = [[-0.1883606  -0.12589116 -0.00908068  0.76655555]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 1539 is [True, False, False, False, True, False]
State prediction error at timestep 1539 is tensor(1.2486e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1540. State = [[-0.26282138  0.07170532]]. Action = [[ 0.2307769   0.045535   -0.03620288 -0.2553318 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 1540 is [True, False, False, False, True, False]
State prediction error at timestep 1540 is tensor(2.7095e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1540 of 1
Current timestep = 1541. State = [[-0.2622539   0.07283355]]. Action = [[-0.09116051 -0.22649656  0.04619732  0.9562348 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 1541 is [True, False, False, False, True, False]
State prediction error at timestep 1541 is tensor(3.0692e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1542. State = [[-0.26190862  0.07358619]]. Action = [[-0.19645551  0.053489   -0.02811103  0.17950296]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 1542 is [True, False, False, False, True, False]
State prediction error at timestep 1542 is tensor(1.1213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1543. State = [[-0.26167634  0.07404838]]. Action = [[-0.10634755  0.2044996   0.11107862  0.17863977]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 1543 is [True, False, False, False, True, False]
State prediction error at timestep 1543 is tensor(9.2284e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1544. State = [[-0.2611546   0.07459211]]. Action = [[ 0.12613326 -0.19382775  0.03028461 -0.42911565]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 1544 is [True, False, False, False, True, False]
State prediction error at timestep 1544 is tensor(3.2207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1544 of 1
Current timestep = 1545. State = [[-0.25916538  0.07685512]]. Action = [[-0.01797886  0.01606044 -0.21738654  0.56617   ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 1545 is [True, False, False, False, True, False]
State prediction error at timestep 1545 is tensor(1.5223e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1546. State = [[-0.25886822  0.07852986]]. Action = [[-0.13125437  0.11775142  0.16526157  0.03432703]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 1546 is [True, False, False, False, True, False]
State prediction error at timestep 1546 is tensor(1.0395e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1547. State = [[-0.2594113   0.07970038]]. Action = [[-0.20508294  0.19906205  0.24675772  0.32879198]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 1547 is [True, False, False, False, True, False]
State prediction error at timestep 1547 is tensor(4.1778e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1548. State = [[-0.26003718  0.08151928]]. Action = [[ 0.24049431 -0.2029394   0.1139926   0.4495983 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 1548 is [True, False, False, False, True, False]
State prediction error at timestep 1548 is tensor(3.2497e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1548 of 1
Current timestep = 1549. State = [[-0.26065975  0.08256218]]. Action = [[-0.24839368  0.01130828 -0.14781164  0.28110123]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 1549 is [True, False, False, False, True, False]
State prediction error at timestep 1549 is tensor(2.1212e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1550. State = [[-0.2625267   0.08608072]]. Action = [[-0.0574851  -0.00616461  0.18193302 -0.30158353]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 1550 is [True, False, False, False, True, False]
State prediction error at timestep 1550 is tensor(1.0608e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1551. State = [[-0.26276153  0.08676606]]. Action = [[ 0.1304701  -0.1594202  -0.24681965  0.5777967 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 1551 is [True, False, False, False, True, False]
State prediction error at timestep 1551 is tensor(7.0215e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1552. State = [[-0.26378486  0.08861502]]. Action = [[ 0.1148023  -0.02490155 -0.12667622 -0.9032771 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 1552 is [True, False, False, False, True, False]
State prediction error at timestep 1552 is tensor(2.0801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1553. State = [[-0.26367962  0.08864211]]. Action = [[0.21815372 0.08240899 0.17619604 0.19590557]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 1553 is [True, False, False, False, True, False]
State prediction error at timestep 1553 is tensor(2.7992e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1554. State = [[-0.26355115  0.08835274]]. Action = [[ 0.1376786  -0.15958223 -0.05365759 -0.8433014 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 1554 is [True, False, False, False, True, False]
State prediction error at timestep 1554 is tensor(5.1198e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1555. State = [[-0.2635021   0.08826124]]. Action = [[ 0.1509      0.21610028 -0.0836456  -0.6979938 ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 1555 is [True, False, False, False, True, False]
State prediction error at timestep 1555 is tensor(9.7849e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1556. State = [[-0.2636429  0.0886737]]. Action = [[ 0.20561084 -0.19444495  0.12349659 -0.4648136 ]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 1556 is [True, False, False, False, True, False]
State prediction error at timestep 1556 is tensor(4.6809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1557. State = [[-0.26360315  0.08873282]]. Action = [[-0.09869649  0.24317634  0.08700359  0.7197244 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 1557 is [True, False, False, False, True, False]
State prediction error at timestep 1557 is tensor(3.0502e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1558. State = [[-0.26350513  0.08855025]]. Action = [[ 0.17623174 -0.04431079 -0.05362853 -0.46336794]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 1558 is [True, False, False, False, True, False]
State prediction error at timestep 1558 is tensor(1.0866e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1559. State = [[-0.2635112   0.08861157]]. Action = [[-0.20133394  0.08783585 -0.23021339  0.63550293]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 1559 is [True, False, False, False, True, False]
State prediction error at timestep 1559 is tensor(1.0066e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1559 of -1
Current timestep = 1560. State = [[-0.26356617  0.08876354]]. Action = [[ 0.09012455 -0.14653662  0.021229    0.7571869 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 1560 is [True, False, False, False, True, False]
State prediction error at timestep 1560 is tensor(2.9003e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1561. State = [[-0.26354173  0.08871801]]. Action = [[-0.04193065 -0.23718502 -0.19988431  0.1316768 ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 1561 is [True, False, False, False, True, False]
State prediction error at timestep 1561 is tensor(3.5179e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1562. State = [[-0.26355395  0.08884065]]. Action = [[-0.21792378  0.03552613 -0.15402213  0.5626575 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 1562 is [True, False, False, False, True, False]
State prediction error at timestep 1562 is tensor(1.1563e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1563. State = [[-0.26363787  0.0890389 ]]. Action = [[-0.07644995  0.02067509  0.07447225 -0.13245916]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 1563 is [True, False, False, False, True, False]
State prediction error at timestep 1563 is tensor(8.2020e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1563 of -1
Current timestep = 1564. State = [[-0.2637569   0.08946677]]. Action = [[-0.12665114 -0.19662316  0.22804499  0.25847578]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 1564 is [True, False, False, False, True, False]
State prediction error at timestep 1564 is tensor(1.1421e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1565. State = [[-0.2638739   0.08963426]]. Action = [[-0.02657907 -0.10413831  0.20010412 -0.7555268 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 1565 is [True, False, False, False, True, False]
State prediction error at timestep 1565 is tensor(1.1148e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1565 of -1
Current timestep = 1566. State = [[-0.263773    0.08919024]]. Action = [[-0.20925038  0.10161647 -0.05752821 -0.89252543]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 1566 is [True, False, False, False, True, False]
State prediction error at timestep 1566 is tensor(2.6469e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1567. State = [[-0.263694   0.0886269]]. Action = [[-0.0564132  -0.05715865  0.2013588  -0.88217294]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 1567 is [True, False, False, False, True, False]
State prediction error at timestep 1567 is tensor(2.8312e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1567 of -1
Current timestep = 1568. State = [[-0.2637501   0.08828753]]. Action = [[ 0.21427959 -0.04593948  0.04293829 -0.44210106]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 1568 is [True, False, False, False, True, False]
State prediction error at timestep 1568 is tensor(2.5637e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1569. State = [[-0.26393133  0.08755667]]. Action = [[ 0.20217931  0.1732139  -0.1917036   0.79698765]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 1569 is [True, False, False, False, True, False]
State prediction error at timestep 1569 is tensor(1.2962e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1569 of -1
Current timestep = 1570. State = [[-0.26384655  0.08703321]]. Action = [[-0.15640487 -0.00201215  0.10925254  0.23839843]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 1570 is [True, False, False, False, True, False]
State prediction error at timestep 1570 is tensor(3.7111e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1571. State = [[-0.26379636  0.08660788]]. Action = [[ 0.23104304  0.03652921 -0.17136908  0.42454922]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 1571 is [True, False, False, False, True, False]
State prediction error at timestep 1571 is tensor(3.1309e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1572. State = [[-0.26396587  0.08525849]]. Action = [[-0.08455619  0.06561682 -0.17388745 -0.3810737 ]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 1572 is [True, False, False, False, True, False]
State prediction error at timestep 1572 is tensor(2.0004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1572 of -1
Current timestep = 1573. State = [[-0.26453614  0.08573657]]. Action = [[ 0.24535859 -0.23682639 -0.24248357 -0.5242378 ]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 1573 is [True, False, False, False, True, False]
State prediction error at timestep 1573 is tensor(5.9621e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1574. State = [[-0.26577955  0.08626489]]. Action = [[-0.0125512  -0.02362561 -0.10896462 -0.13777983]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 1574 is [True, False, False, False, True, False]
State prediction error at timestep 1574 is tensor(1.1977e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1574 of -1
Current timestep = 1575. State = [[-0.26612428  0.08614506]]. Action = [[-0.12300764  0.2100051  -0.04543537  0.8981507 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 1575 is [True, False, False, False, True, False]
State prediction error at timestep 1575 is tensor(2.9974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1576. State = [[-0.2661543  0.085998 ]]. Action = [[-0.22587931  0.09992474 -0.21366586 -0.42673194]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 1576 is [True, False, False, False, True, False]
State prediction error at timestep 1576 is tensor(2.8960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1577. State = [[-0.26668456  0.08562312]]. Action = [[-0.07012212 -0.05316621  0.11456403 -0.5654387 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 1577 is [True, False, False, False, True, False]
State prediction error at timestep 1577 is tensor(1.5527e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1577 of -1
Current timestep = 1578. State = [[-0.2672352   0.08514399]]. Action = [[-0.24197085 -0.14707597  0.03654903  0.8011403 ]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 1578 is [True, False, False, False, True, False]
State prediction error at timestep 1578 is tensor(2.1910e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1579. State = [[-0.2677126   0.08473302]]. Action = [[ 0.16761705  0.11370623 -0.06488141 -0.18655008]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 1579 is [True, False, False, False, True, False]
State prediction error at timestep 1579 is tensor(1.6870e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1580. State = [[-0.2676755   0.08435638]]. Action = [[-0.13849612 -0.06703237  0.06890693  0.70675945]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 1580 is [True, False, False, False, True, False]
State prediction error at timestep 1580 is tensor(3.0716e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1580 of -1
Current timestep = 1581. State = [[-0.26831463  0.08386727]]. Action = [[-0.21127203  0.19194588  0.1098038   0.78509355]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 1581 is [True, False, False, False, True, False]
State prediction error at timestep 1581 is tensor(4.2309e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1582. State = [[-0.26845565  0.08366532]]. Action = [[ 0.05864671 -0.19329491 -0.0352755  -0.29569423]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 1582 is [True, False, False, False, True, False]
State prediction error at timestep 1582 is tensor(2.8008e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1583. State = [[-0.26841035  0.08366109]]. Action = [[ 0.08413208 -0.22843355  0.22296545  0.11701179]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 1583 is [True, False, False, False, True, False]
State prediction error at timestep 1583 is tensor(1.4508e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1583 of -1
Current timestep = 1584. State = [[-0.26878402  0.08333647]]. Action = [[-0.0610193  -0.22305936 -0.11786528 -0.59152776]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 1584 is [True, False, False, False, True, False]
State prediction error at timestep 1584 is tensor(1.7586e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1585. State = [[-0.26877457  0.08327593]]. Action = [[ 0.19327143 -0.20119333  0.19641256 -0.44248247]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 1585 is [True, False, False, False, True, False]
State prediction error at timestep 1585 is tensor(3.8901e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1586. State = [[-0.26928744  0.0831087 ]]. Action = [[ 0.07819575  0.10476339 -0.23056439 -0.9129491 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 1586 is [True, False, False, False, True, False]
State prediction error at timestep 1586 is tensor(1.0546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1586 of -1
Current timestep = 1587. State = [[-0.2700923   0.08452649]]. Action = [[-0.00832     0.04949066  0.12186375  0.29500675]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 1587 is [True, False, False, False, True, False]
State prediction error at timestep 1587 is tensor(3.8441e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1587 of -1
Current timestep = 1588. State = [[-0.27036414  0.085323  ]]. Action = [[ 0.06482896 -0.15402077 -0.14640094 -0.09055877]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 1588 is [True, False, False, False, True, False]
State prediction error at timestep 1588 is tensor(1.4700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1589. State = [[-0.27056378  0.08572549]]. Action = [[ 0.00293776 -0.2188112  -0.12375739  0.5792699 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 1589 is [True, False, False, False, True, False]
State prediction error at timestep 1589 is tensor(4.1387e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1590. State = [[-0.2710213  0.0862506]]. Action = [[-0.18139471  0.21724424  0.21033093  0.8769951 ]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 1590 is [True, False, False, False, True, False]
State prediction error at timestep 1590 is tensor(2.1508e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1591. State = [[-0.27124792  0.08677677]]. Action = [[ 0.01872656 -0.14795664 -0.15666422 -0.19908881]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 1591 is [True, False, False, False, True, False]
State prediction error at timestep 1591 is tensor(2.7572e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1592. State = [[-0.27152765  0.08752593]]. Action = [[ 0.2047979   0.21408021 -0.24059471 -0.48767352]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 1592 is [True, False, False, False, True, False]
State prediction error at timestep 1592 is tensor(1.9534e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1592 of -1
Current timestep = 1593. State = [[-0.27185202  0.08783801]]. Action = [[0.13913152 0.16904622 0.12616503 0.23383856]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 1593 is [True, False, False, False, True, False]
State prediction error at timestep 1593 is tensor(3.1005e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1594. State = [[-0.27200493  0.08816771]]. Action = [[ 0.18420833 -0.03796005 -0.14395386  0.81270146]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 1594 is [True, False, False, False, True, False]
State prediction error at timestep 1594 is tensor(5.7088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1595. State = [[-0.2720584   0.08844908]]. Action = [[-0.01826924 -0.2116688  -0.2083029   0.92353797]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 1595 is [True, False, False, False, True, False]
State prediction error at timestep 1595 is tensor(6.8994e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1596. State = [[-0.27269804  0.08948804]]. Action = [[-0.03166218  0.10000381  0.18336925  0.1096431 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 1596 is [True, False, False, False, True, False]
State prediction error at timestep 1596 is tensor(2.9914e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1596 of -1
Current timestep = 1597. State = [[-0.27308983  0.09042066]]. Action = [[-0.23218788 -0.15954915  0.22851646  0.44969296]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 1597 is [True, False, False, False, True, False]
State prediction error at timestep 1597 is tensor(7.8024e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1598. State = [[-0.27362368  0.0913312 ]]. Action = [[ 0.23608035  0.09815776  0.06069952 -0.0054754 ]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 1598 is [True, False, False, False, True, False]
State prediction error at timestep 1598 is tensor(1.4722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1599. State = [[-0.27397105  0.09204427]]. Action = [[-0.18568678 -0.21918772  0.1266759  -0.47745144]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 1599 is [True, False, False, False, True, False]
State prediction error at timestep 1599 is tensor(7.3925e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1599 of -1
Current timestep = 1600. State = [[-0.27479073  0.09342762]]. Action = [[ 0.21788949 -0.23233274 -0.18421361 -0.26046073]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 1600 is [True, False, False, False, True, False]
State prediction error at timestep 1600 is tensor(4.5713e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1601. State = [[-0.27500212  0.09393223]]. Action = [[-0.21045054  0.1698137  -0.15954055 -0.31923234]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 1601 is [True, False, False, False, True, False]
State prediction error at timestep 1601 is tensor(4.0911e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1602. State = [[-0.27518898  0.09416723]]. Action = [[-0.24439466 -0.17316045  0.21350324  0.89199376]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 1602 is [True, False, False, False, True, False]
State prediction error at timestep 1602 is tensor(1.9472e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1603. State = [[-0.2760133   0.09558292]]. Action = [[ 0.03843683  0.04096675 -0.06915413  0.6805937 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 1603 is [True, False, False, False, True, False]
State prediction error at timestep 1603 is tensor(1.3608e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1603 of -1
Current timestep = 1604. State = [[-0.27624786  0.09604266]]. Action = [[-0.1798704   0.14400244  0.10951731  0.21451914]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 1604 is [True, False, False, False, True, False]
State prediction error at timestep 1604 is tensor(5.4855e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1605. State = [[-0.27676016  0.09713621]]. Action = [[0.09994048 0.06463602 0.22560105 0.23559749]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 1605 is [True, False, False, False, True, False]
State prediction error at timestep 1605 is tensor(1.7831e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1605 of -1
Current timestep = 1606. State = [[-0.27613994  0.09803105]]. Action = [[ 0.20283884  0.03732726 -0.0446426  -0.6055901 ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 1606 is [True, False, False, False, True, False]
State prediction error at timestep 1606 is tensor(3.1902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1607. State = [[-0.27574435  0.09842723]]. Action = [[-0.18043657 -0.00094348 -0.06123267  0.34476018]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 1607 is [True, False, False, False, True, False]
State prediction error at timestep 1607 is tensor(6.4309e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1607 of -1
Current timestep = 1608. State = [[-0.2756048   0.09868147]]. Action = [[ 0.15854532  0.20331031 -0.15523244  0.3096105 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 1608 is [True, False, False, False, True, False]
State prediction error at timestep 1608 is tensor(1.9922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1609. State = [[-0.27478072  0.09992238]]. Action = [[-0.11562513 -0.00990473 -0.0628126   0.21027136]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 1609 is [True, False, False, False, True, False]
State prediction error at timestep 1609 is tensor(1.2944e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1610. State = [[-0.27514938  0.10024446]]. Action = [[ 0.23018691  0.04968914  0.23655567 -0.6990994 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 1610 is [True, False, False, False, True, False]
State prediction error at timestep 1610 is tensor(9.4081e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1611. State = [[-0.27545998  0.10057902]]. Action = [[-0.07104093 -0.23080634 -0.10913414  0.6356294 ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 1611 is [True, False, False, False, True, False]
State prediction error at timestep 1611 is tensor(1.1568e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1612. State = [[-0.27540365  0.10071739]]. Action = [[-0.14813264  0.12193626  0.11119881  0.96324396]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 1612 is [True, False, False, False, True, False]
State prediction error at timestep 1612 is tensor(2.0860e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1613. State = [[-0.27584746  0.10157667]]. Action = [[ 0.08681265  0.0655987  -0.03185022  0.3093716 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 1613 is [True, False, False, False, True, False]
State prediction error at timestep 1613 is tensor(6.0645e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1613 of -1
Current timestep = 1614. State = [[-0.27640742  0.10329602]]. Action = [[ 0.02015698  0.08359855 -0.14840716  0.88494253]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 1614 is [True, False, False, False, True, False]
State prediction error at timestep 1614 is tensor(8.5912e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1614 of -1
Current timestep = 1615. State = [[-0.27652806  0.10417455]]. Action = [[ 0.24073148  0.24108803  0.03335282 -0.4405769 ]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 1615 is [True, False, False, False, True, False]
State prediction error at timestep 1615 is tensor(3.9883e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1616. State = [[-0.27636236  0.10684998]]. Action = [[ 0.12769938 -0.01500075  0.03205401 -0.50297457]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 1616 is [True, False, False, False, True, False]
State prediction error at timestep 1616 is tensor(1.2763e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1616 of -1
Current timestep = 1617. State = [[-0.27536008  0.10718176]]. Action = [[-0.13661373 -0.13604243 -0.03657143 -0.10909033]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 1617 is [True, False, False, False, True, False]
State prediction error at timestep 1617 is tensor(1.1213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1618. State = [[-0.27450892  0.10745579]]. Action = [[ 0.19628382  0.16175762 -0.1854455  -0.06801492]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 1618 is [True, False, False, False, True, False]
State prediction error at timestep 1618 is tensor(5.5088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1619. State = [[-0.27415588  0.10764688]]. Action = [[-0.14111407 -0.19034375 -0.2327871  -0.286938  ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 1619 is [True, False, False, False, True, False]
State prediction error at timestep 1619 is tensor(3.6319e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1620. State = [[-0.2723156   0.10876441]]. Action = [[ 0.10413745 -0.04495518  0.01132858 -0.5942023 ]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 1620 is [True, False, False, False, True, False]
State prediction error at timestep 1620 is tensor(1.5410e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1620 of -1
Current timestep = 1621. State = [[-0.2708272   0.10919347]]. Action = [[ 0.18891752 -0.0709683  -0.18685295  0.44209552]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 1621 is [True, False, False, False, True, False]
State prediction error at timestep 1621 is tensor(1.4965e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1622. State = [[-0.2702468   0.10931762]]. Action = [[-0.02644891  0.22709882 -0.10009959 -0.29882634]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 1622 is [True, False, False, False, True, False]
State prediction error at timestep 1622 is tensor(6.9733e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1622 of -1
Current timestep = 1623. State = [[-0.26967588  0.1097206 ]]. Action = [[-0.21418475  0.22075248 -0.0624326  -0.7024422 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 1623 is [True, False, False, False, True, False]
State prediction error at timestep 1623 is tensor(2.8843e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1624. State = [[-0.26836145  0.11008572]]. Action = [[-0.05382709 -0.24366601  0.24889052 -0.30403244]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 1624 is [True, False, False, False, True, False]
State prediction error at timestep 1624 is tensor(3.4272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1625. State = [[-0.2677713   0.11008032]]. Action = [[-0.2430934  -0.09694771 -0.22374846 -0.82739735]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 1625 is [True, False, False, False, True, False]
State prediction error at timestep 1625 is tensor(4.2318e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1626. State = [[-0.26763713  0.1100795 ]]. Action = [[-0.00381783 -0.21394148  0.08596522  0.40505266]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 1626 is [True, False, False, False, True, False]
State prediction error at timestep 1626 is tensor(7.2435e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1626 of -1
Current timestep = 1627. State = [[-0.26563126  0.11009277]]. Action = [[ 0.01047271 -0.13141115  0.01610532  0.7025764 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 1627 is [True, False, False, False, True, False]
State prediction error at timestep 1627 is tensor(2.0671e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1628. State = [[-0.26491225  0.10931283]]. Action = [[ 0.0809834   0.15343499 -0.17180993 -0.4999848 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 1628 is [True, False, False, False, True, False]
State prediction error at timestep 1628 is tensor(9.0391e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1629. State = [[-0.26454398  0.1083429 ]]. Action = [[ 0.23742622 -0.19864383  0.04068011 -0.03806967]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 1629 is [True, False, False, False, True, False]
State prediction error at timestep 1629 is tensor(2.7484e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1630. State = [[-0.26443115  0.10766196]]. Action = [[-0.14210549 -0.04658207 -0.09233168 -0.7649792 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 1630 is [True, False, False, False, True, False]
State prediction error at timestep 1630 is tensor(4.0423e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1631. State = [[-0.26418597  0.10732769]]. Action = [[-0.19745624 -0.23517175  0.09326464 -0.70879537]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 1631 is [True, False, False, False, True, False]
State prediction error at timestep 1631 is tensor(9.9209e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1631 of 1
Current timestep = 1632. State = [[-0.26373553  0.10676032]]. Action = [[ 0.11411917  0.2162379  -0.10092437  0.2745397 ]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 1632 is [True, False, False, False, True, False]
State prediction error at timestep 1632 is tensor(2.3896e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1633. State = [[-0.2635745   0.10634784]]. Action = [[ 0.14556265 -0.18583253 -0.22065166 -0.01875854]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 1633 is [True, False, False, False, True, False]
State prediction error at timestep 1633 is tensor(2.3757e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1634. State = [[-0.26332596  0.10613865]]. Action = [[-0.24357598 -0.23968485 -0.11483406 -0.40919954]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 1634 is [True, False, False, False, True, False]
State prediction error at timestep 1634 is tensor(3.5196e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1635. State = [[-0.26301983  0.10616606]]. Action = [[ 0.21726465 -0.15850689  0.14656454  0.70269084]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 1635 is [True, False, False, False, True, False]
State prediction error at timestep 1635 is tensor(3.3649e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1635 of 1
Current timestep = 1636. State = [[-0.26295817  0.10599498]]. Action = [[ 0.20752674  0.04480305 -0.18114024 -0.17156756]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 1636 is [True, False, False, False, True, False]
State prediction error at timestep 1636 is tensor(1.5679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1637. State = [[-0.26274288  0.10584093]]. Action = [[ 0.20703971 -0.05664323  0.12502071 -0.90144336]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 1637 is [True, False, False, False, True, False]
State prediction error at timestep 1637 is tensor(2.0606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1638. State = [[-0.2623389   0.10584574]]. Action = [[ 0.18444636 -0.17169951 -0.24566984  0.40902066]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 1638 is [True, False, False, False, True, False]
State prediction error at timestep 1638 is tensor(1.3708e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1639. State = [[-0.26230428  0.10571688]]. Action = [[-0.20824446 -0.23889148  0.01500955 -0.84357333]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 1639 is [True, False, False, False, True, False]
State prediction error at timestep 1639 is tensor(6.9810e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1639 of 1
Current timestep = 1640. State = [[-0.26177135  0.10563312]]. Action = [[ 0.00656816 -0.01989043 -0.17581585 -0.3905667 ]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 1640 is [True, False, False, False, True, False]
State prediction error at timestep 1640 is tensor(1.4164e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1640 of 1
Current timestep = 1641. State = [[-0.26136515  0.10550746]]. Action = [[-0.2055225  -0.13669592  0.22166628 -0.48047817]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 1641 is [True, False, False, False, True, False]
State prediction error at timestep 1641 is tensor(5.3679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1642. State = [[-0.26123554  0.10539856]]. Action = [[0.22869995 0.0929504  0.13468236 0.63266134]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 1642 is [True, False, False, False, True, False]
State prediction error at timestep 1642 is tensor(2.1691e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1643. State = [[-0.2612675   0.10530134]]. Action = [[-0.18302411 -0.13856553 -0.20481528 -0.12494278]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 1643 is [True, False, False, False, True, False]
State prediction error at timestep 1643 is tensor(6.2249e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1644. State = [[-0.2611674  0.1052428]]. Action = [[-0.24693246  0.18880466 -0.22181737  0.2668451 ]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 1644 is [True, False, False, False, True, False]
State prediction error at timestep 1644 is tensor(3.2350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1645. State = [[-0.26100522  0.10512491]]. Action = [[-0.03180192  0.2481097   0.09923285  0.35957718]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 1645 is [True, False, False, False, True, False]
State prediction error at timestep 1645 is tensor(1.2968e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1645 of 1
Current timestep = 1646. State = [[-0.26094744  0.10498903]]. Action = [[ 0.04979071 -0.21403982 -0.10649896  0.83478665]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 1646 is [True, False, False, False, True, False]
State prediction error at timestep 1646 is tensor(4.9075e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1647. State = [[-0.26090154  0.10501901]]. Action = [[ 0.24685967  0.2325786  -0.0205249   0.42326033]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 1647 is [True, False, False, False, True, False]
State prediction error at timestep 1647 is tensor(2.0569e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1648. State = [[-0.26083273  0.10506391]]. Action = [[-0.22540781 -0.1558998  -0.16903274 -0.00521922]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 1648 is [True, False, False, False, True, False]
State prediction error at timestep 1648 is tensor(1.0332e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1649. State = [[-0.26070222  0.10497081]]. Action = [[ 0.22447094 -0.13818821  0.20435458 -0.6909124 ]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 1649 is [True, False, False, False, True, False]
State prediction error at timestep 1649 is tensor(2.1481e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1650. State = [[-0.2608075   0.10487755]]. Action = [[ 0.22063398 -0.02247855  0.06148022  0.78555036]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 1650 is [True, False, False, False, True, False]
State prediction error at timestep 1650 is tensor(2.3210e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1651. State = [[-0.26078448  0.10489254]]. Action = [[-0.20864592  0.06540638 -0.02590853  0.28767192]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 1651 is [True, False, False, False, True, False]
State prediction error at timestep 1651 is tensor(1.0691e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1652. State = [[-0.26070976  0.10467105]]. Action = [[-0.11967912 -0.09365471 -0.03040877 -0.95790136]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 1652 is [True, False, False, False, True, False]
State prediction error at timestep 1652 is tensor(4.1947e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1652 of 1
Current timestep = 1653. State = [[-0.26061127  0.10394003]]. Action = [[-0.14783867 -0.05708902 -0.21537223 -0.5995741 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 1653 is [True, False, False, False, True, False]
State prediction error at timestep 1653 is tensor(3.4959e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1653 of 1
Current timestep = 1654. State = [[-0.26052737  0.10315286]]. Action = [[-0.11166725 -0.21907258 -0.17106383  0.2410574 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 1654 is [True, False, False, False, True, False]
State prediction error at timestep 1654 is tensor(1.6274e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1655. State = [[-0.2603016   0.10129324]]. Action = [[-0.09784362 -0.12918456 -0.16459079 -0.19000113]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 1655 is [True, False, False, False, True, False]
State prediction error at timestep 1655 is tensor(2.1806e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1656. State = [[-0.26066098  0.09978688]]. Action = [[0.20240021 0.05259624 0.12881935 0.03257656]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 1656 is [True, False, False, False, True, False]
State prediction error at timestep 1656 is tensor(2.1706e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1657. State = [[-0.2619596   0.09572194]]. Action = [[-0.0893527  -0.0047722  -0.07978156  0.19031513]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 1657 is [True, False, False, False, True, False]
State prediction error at timestep 1657 is tensor(4.3154e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1658. State = [[-0.262861    0.09472872]]. Action = [[-0.22118899 -0.03759858  0.17525557 -0.5380025 ]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 1658 is [True, False, False, False, True, False]
State prediction error at timestep 1658 is tensor(2.7736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1659. State = [[-0.26337314  0.09423087]]. Action = [[-0.19258185 -0.24257877 -0.16148241  0.5716114 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 1659 is [True, False, False, False, True, False]
State prediction error at timestep 1659 is tensor(1.1917e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1660. State = [[-0.2637816   0.09370212]]. Action = [[ 0.07323545  0.13912654  0.18357256 -0.5830745 ]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 1660 is [True, False, False, False, True, False]
State prediction error at timestep 1660 is tensor(5.3140e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1661. State = [[-0.2640736   0.09295917]]. Action = [[-0.18134604 -0.04056892  0.15360147 -0.6874812 ]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 1661 is [True, False, False, False, True, False]
State prediction error at timestep 1661 is tensor(2.5444e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1662. State = [[-0.2643671  0.0924974]]. Action = [[ 0.05221763 -0.14624205 -0.09525216  0.71746993]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 1662 is [True, False, False, False, True, False]
State prediction error at timestep 1662 is tensor(1.7591e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1662 of 1
Current timestep = 1663. State = [[-0.2649923   0.09159262]]. Action = [[ 0.04583052 -0.13639884  0.12906009  0.49218035]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 1663 is [True, False, False, False, True, False]
State prediction error at timestep 1663 is tensor(2.5258e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1664. State = [[-0.26510543  0.09133548]]. Action = [[-0.06223845 -0.22389463  0.19788015 -0.6889252 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 1664 is [True, False, False, False, True, False]
State prediction error at timestep 1664 is tensor(2.9109e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1665. State = [[-0.26552096  0.09105007]]. Action = [[-0.23116367 -0.0662567   0.05203956 -0.67061347]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 1665 is [True, False, False, False, True, False]
State prediction error at timestep 1665 is tensor(3.6374e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1666. State = [[-0.26575482  0.09077287]]. Action = [[0.23334524 0.07571703 0.00680637 0.7645116 ]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 1666 is [True, False, False, False, True, False]
State prediction error at timestep 1666 is tensor(1.4798e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1667. State = [[-0.26570615  0.09063823]]. Action = [[-0.18353394  0.04291949  0.14812303 -0.64428544]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 1667 is [True, False, False, False, True, False]
State prediction error at timestep 1667 is tensor(2.1226e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1667 of 1
Current timestep = 1668. State = [[-0.2661081   0.09027395]]. Action = [[-0.14558846 -0.21583436  0.21267676 -0.8484959 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 1668 is [True, False, False, False, True, False]
State prediction error at timestep 1668 is tensor(3.0960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1669. State = [[-0.26600116  0.09042466]]. Action = [[-0.19261153  0.10553855  0.04106426  0.35035455]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 1669 is [True, False, False, False, True, False]
State prediction error at timestep 1669 is tensor(3.4146e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1670. State = [[-0.266343    0.09002982]]. Action = [[ 0.13823229 -0.20751576  0.10099733  0.6908227 ]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 1670 is [True, False, False, False, True, False]
State prediction error at timestep 1670 is tensor(2.8763e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1670 of 1
Current timestep = 1671. State = [[-0.266563    0.08985393]]. Action = [[ 0.08902752  0.12408894 -0.11927836 -0.6779889 ]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 1671 is [True, False, False, False, True, False]
State prediction error at timestep 1671 is tensor(3.3132e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1672. State = [[-0.26658675  0.09058198]]. Action = [[-0.03522366 -0.19747406 -0.11655833  0.33297253]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 1672 is [True, False, False, False, True, False]
State prediction error at timestep 1672 is tensor(4.7634e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1673. State = [[-0.26690638  0.09097784]]. Action = [[-0.22946388  0.18678251 -0.20409769  0.7794292 ]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 1673 is [True, False, False, False, True, False]
State prediction error at timestep 1673 is tensor(3.1248e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1674. State = [[-0.26711115  0.09098779]]. Action = [[-0.18454567 -0.2150187   0.08827072  0.15353191]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 1674 is [True, False, False, False, True, False]
State prediction error at timestep 1674 is tensor(5.6513e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1675. State = [[-0.2675354   0.09169284]]. Action = [[ 0.03883651  0.09171978  0.18611807 -0.02402103]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 1675 is [True, False, False, False, True, False]
State prediction error at timestep 1675 is tensor(1.9222e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1675 of 1
Current timestep = 1676. State = [[-0.26825294  0.09438724]]. Action = [[ 0.10919303  0.12898979 -0.13072562 -0.3840195 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 1676 is [True, False, False, False, True, False]
State prediction error at timestep 1676 is tensor(2.8105e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1677. State = [[-0.2680754   0.09568474]]. Action = [[-0.13587323  0.11539811 -0.1854268  -0.6501043 ]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 1677 is [True, False, False, False, True, False]
State prediction error at timestep 1677 is tensor(2.3358e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1678. State = [[-0.26744094  0.09712026]]. Action = [[-0.22849317 -0.12447292  0.20254484 -0.8145714 ]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 1678 is [True, False, False, False, True, False]
State prediction error at timestep 1678 is tensor(2.8770e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1679. State = [[-0.26703757  0.09783155]]. Action = [[-0.20658006 -0.06714864  0.13531226  0.39911127]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 1679 is [True, False, False, False, True, False]
State prediction error at timestep 1679 is tensor(4.1096e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1680. State = [[-0.26700088  0.09843052]]. Action = [[ 0.20732385  0.06255257  0.22607139 -0.5616753 ]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 1680 is [True, False, False, False, True, False]
State prediction error at timestep 1680 is tensor(5.3710e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1681. State = [[-0.2663652   0.10003202]]. Action = [[-0.21704897 -0.01257101  0.21902493  0.3119943 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 1681 is [True, False, False, False, True, False]
State prediction error at timestep 1681 is tensor(4.0258e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1681 of 1
Current timestep = 1682. State = [[-0.26652235  0.10141972]]. Action = [[-0.01977998 -0.12382245 -0.16769618  0.05451369]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 1682 is [True, False, False, False, True, False]
State prediction error at timestep 1682 is tensor(3.5427e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1683. State = [[-0.26617363  0.10126437]]. Action = [[-0.20954087  0.03041813  0.22363278  0.9334352 ]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 1683 is [True, False, False, False, True, False]
State prediction error at timestep 1683 is tensor(1.6921e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1684. State = [[-0.2660127   0.10121951]]. Action = [[ 0.15528017  0.03925765 -0.07239634  0.23379076]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 1684 is [True, False, False, False, True, False]
State prediction error at timestep 1684 is tensor(1.1446e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1685. State = [[-0.26603034  0.10103977]]. Action = [[ 0.24651814 -0.1869264   0.09343436 -0.90614027]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 1685 is [True, False, False, False, True, False]
State prediction error at timestep 1685 is tensor(3.1174e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1686. State = [[-0.26619563  0.1010993 ]]. Action = [[ 0.17519528 -0.22908852  0.04481506 -0.7829433 ]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 1686 is [True, False, False, False, True, False]
State prediction error at timestep 1686 is tensor(1.0830e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1687. State = [[-0.26609793  0.10125428]]. Action = [[-0.2036177   0.01710522  0.23094389  0.16785371]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 1687 is [True, False, False, False, True, False]
State prediction error at timestep 1687 is tensor(9.1886e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1688. State = [[-0.26602048  0.10111803]]. Action = [[ 6.6972077e-03  4.1510731e-02  2.7543306e-04 -4.6576428e-01]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 1688 is [True, False, False, False, True, False]
State prediction error at timestep 1688 is tensor(2.1273e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1688 of 1
Current timestep = 1689. State = [[-0.26593503  0.10091976]]. Action = [[0.08029842 0.24164918 0.1993292  0.59524715]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 1689 is [True, False, False, False, True, False]
State prediction error at timestep 1689 is tensor(1.3290e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1690. State = [[-0.26607218  0.10120901]]. Action = [[-0.06339556 -0.00256042 -0.19353922  0.46889615]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 1690 is [True, False, False, False, True, False]
State prediction error at timestep 1690 is tensor(2.4152e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1690 of 1
Current timestep = 1691. State = [[-0.26599848  0.10110276]]. Action = [[ 0.0739522  -0.2044316  -0.08596379  0.46778178]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 1691 is [True, False, False, False, True, False]
State prediction error at timestep 1691 is tensor(1.5060e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1692. State = [[-0.26623946  0.10134418]]. Action = [[-0.0930575  -0.00537977 -0.11883429  0.59962463]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 1692 is [True, False, False, False, True, False]
State prediction error at timestep 1692 is tensor(5.8966e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1692 of 1
Current timestep = 1693. State = [[-0.26664504  0.10188918]]. Action = [[-0.00108457  0.01344731 -0.18498823  0.21740603]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 1693 is [True, False, False, False, True, False]
State prediction error at timestep 1693 is tensor(6.6775e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1693 of 1
Current timestep = 1694. State = [[-0.26704916  0.10239958]]. Action = [[-0.1149309   0.00921729 -0.04500253  0.15133119]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 1694 is [True, False, False, False, True, False]
State prediction error at timestep 1694 is tensor(1.9510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1694 of 1
Current timestep = 1695. State = [[-0.2676954   0.10277291]]. Action = [[ 0.22044379  0.2162602  -0.18906336  0.58749974]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 1695 is [True, False, False, False, True, False]
State prediction error at timestep 1695 is tensor(1.9242e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1696. State = [[-0.26861975  0.10326157]]. Action = [[-0.1597721   0.21429503  0.24881458 -0.85465205]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 1696 is [True, False, False, False, True, False]
State prediction error at timestep 1696 is tensor(2.1051e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1697. State = [[-0.26880768  0.10341211]]. Action = [[-0.04969193  0.14130414 -0.01581015  0.8634342 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 1697 is [True, False, False, False, True, False]
State prediction error at timestep 1697 is tensor(8.2643e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1698. State = [[-0.26908457  0.1037144 ]]. Action = [[-0.06545529  0.1412763  -0.07877868  0.27261567]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 1698 is [True, False, False, False, True, False]
State prediction error at timestep 1698 is tensor(1.8947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1699. State = [[-0.2695373   0.10395161]]. Action = [[-0.04987541  0.24153191 -0.01542275 -0.68117166]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 1699 is [True, False, False, False, True, False]
State prediction error at timestep 1699 is tensor(2.6874e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1699 of 1
Current timestep = 1700. State = [[-0.27002993  0.10428461]]. Action = [[-0.2380192   0.18262649 -0.07595325  0.02044272]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 1700 is [True, False, False, False, True, False]
State prediction error at timestep 1700 is tensor(1.2764e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1701. State = [[-0.27004698  0.10454183]]. Action = [[ 0.17039919 -0.15689918  0.18744531  0.5197749 ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 1701 is [True, False, False, False, True, False]
State prediction error at timestep 1701 is tensor(3.4700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1702. State = [[-0.27033573  0.10470544]]. Action = [[-0.01531436 -0.1917436   0.21263391 -0.10440731]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 1702 is [True, False, False, False, True, False]
State prediction error at timestep 1702 is tensor(5.8454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1703. State = [[-0.27055168  0.10505235]]. Action = [[-0.11811078 -0.2218455  -0.07608157 -0.65730894]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 1703 is [True, False, False, False, True, False]
State prediction error at timestep 1703 is tensor(1.8821e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1703 of 1
Current timestep = 1704. State = [[-0.2707307  0.1049549]]. Action = [[ 0.21219623  0.23408341 -0.1970997  -0.16040134]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 1704 is [True, False, False, False, True, False]
State prediction error at timestep 1704 is tensor(4.1372e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1705. State = [[-0.2707428   0.10507709]]. Action = [[-0.02738072  0.24720263 -0.19588669 -0.11796319]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 1705 is [True, False, False, False, True, False]
State prediction error at timestep 1705 is tensor(3.5476e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1706. State = [[-0.27105978  0.10519125]]. Action = [[-0.02164227 -0.03134358 -0.09063931  0.96253216]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 1706 is [True, False, False, False, True, False]
State prediction error at timestep 1706 is tensor(4.1090e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1706 of -1
Current timestep = 1707. State = [[-0.27131173  0.1051245 ]]. Action = [[0.18486798 0.06475729 0.06640908 0.8273616 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 1707 is [True, False, False, False, True, False]
State prediction error at timestep 1707 is tensor(1.3938e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1708. State = [[-0.27151155  0.10506023]]. Action = [[0.06675425 0.2468803  0.17285892 0.3114326 ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 1708 is [True, False, False, False, True, False]
State prediction error at timestep 1708 is tensor(1.7639e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1709. State = [[-0.27143794  0.10509106]]. Action = [[ 0.18471545 -0.02790815 -0.12591606  0.7314848 ]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 1709 is [True, False, False, False, True, False]
State prediction error at timestep 1709 is tensor(1.6942e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1709 of -1
Current timestep = 1710. State = [[-0.2720027   0.10507572]]. Action = [[-0.01462354  0.06720409  0.125896    0.3830539 ]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 1710 is [True, False, False, False, True, False]
State prediction error at timestep 1710 is tensor(1.0777e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1711. State = [[-0.27293372  0.10653638]]. Action = [[0.08128047 0.13236189 0.24847412 0.6916449 ]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 1711 is [True, False, False, False, True, False]
State prediction error at timestep 1711 is tensor(1.4329e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1712. State = [[-0.27334926  0.10728542]]. Action = [[ 0.21935678 -0.24035223 -0.13769531  0.207124  ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 1712 is [True, False, False, False, True, False]
State prediction error at timestep 1712 is tensor(2.5206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1713. State = [[-0.27379403  0.10835084]]. Action = [[0.15541649 0.20118302 0.03771886 0.12987292]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 1713 is [True, False, False, False, True, False]
State prediction error at timestep 1713 is tensor(1.3654e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1713 of -1
Current timestep = 1714. State = [[-0.27437985  0.10916406]]. Action = [[ 0.1376116  -0.02133313 -0.09413122  0.08535504]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 1714 is [True, False, False, False, True, False]
State prediction error at timestep 1714 is tensor(6.4773e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1715. State = [[-0.27593634  0.11165231]]. Action = [[-0.11576754 -0.02970916  0.13391566  0.2906928 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 1715 is [True, False, False, False, True, False]
State prediction error at timestep 1715 is tensor(1.3718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1716. State = [[-0.27625236  0.1122179 ]]. Action = [[-0.16014299 -0.02534373 -0.05693498 -0.17763388]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 1716 is [True, False, False, False, True, False]
State prediction error at timestep 1716 is tensor(8.4143e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1717. State = [[-0.2764413  0.1125287]]. Action = [[-0.14066957 -0.21169046  0.15055186 -0.5825042 ]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 1717 is [True, False, False, False, True, False]
State prediction error at timestep 1717 is tensor(5.7717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1718. State = [[-0.27731925  0.11392912]]. Action = [[-0.00214416 -0.10306323 -0.04469198 -0.15161699]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 1718 is [True, False, False, False, True, False]
State prediction error at timestep 1718 is tensor(1.8500e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1718 of -1
Current timestep = 1719. State = [[-0.27725857  0.11339294]]. Action = [[ 0.16924    -0.16379191  0.11576533 -0.6397783 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 1719 is [True, False, False, False, True, False]
State prediction error at timestep 1719 is tensor(9.2344e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1720. State = [[-0.2771302   0.11285591]]. Action = [[ 0.1404973   0.17882779  0.18463647 -0.32858527]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 1720 is [True, False, False, False, True, False]
State prediction error at timestep 1720 is tensor(1.4967e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1721. State = [[-0.27718627  0.11286479]]. Action = [[ 0.13360524 -0.20902653 -0.05763368 -0.45818108]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 1721 is [True, False, False, False, True, False]
State prediction error at timestep 1721 is tensor(2.6736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1722. State = [[-0.27716002  0.11232273]]. Action = [[ 0.04638124  0.05094942 -0.03447862  0.5506129 ]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 1722 is [True, False, False, False, True, False]
State prediction error at timestep 1722 is tensor(8.8340e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1722 of -1
Current timestep = 1723. State = [[-0.27739182  0.11269468]]. Action = [[ 0.1811148   0.01843375  0.1138165  -0.41623843]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 1723 is [True, False, False, False, True, False]
State prediction error at timestep 1723 is tensor(2.8645e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1724. State = [[-0.27747205  0.11265986]]. Action = [[-0.12752418  0.15452397  0.20037282  0.24846494]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 1724 is [True, False, False, False, True, False]
State prediction error at timestep 1724 is tensor(2.3079e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1725. State = [[-0.27746332  0.11276643]]. Action = [[-0.04530838  0.05191699 -0.16284104 -0.7096553 ]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 1725 is [True, False, False, False, True, False]
State prediction error at timestep 1725 is tensor(5.3151e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1725 of -1
Current timestep = 1726. State = [[-0.27760267  0.11304766]]. Action = [[ 0.23149675  0.15392995  0.16575977 -0.6992134 ]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 1726 is [True, False, False, False, True, False]
State prediction error at timestep 1726 is tensor(6.8870e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1727. State = [[-0.27781472  0.11334125]]. Action = [[-0.24047804 -0.22441004  0.1440587   0.35274553]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 1727 is [True, False, False, False, True, False]
State prediction error at timestep 1727 is tensor(3.9855e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1728. State = [[-0.2780229   0.11360574]]. Action = [[ 0.13856906  0.12022132 -0.11825976 -0.13569081]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 1728 is [True, False, False, False, True, False]
State prediction error at timestep 1728 is tensor(1.0899e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1729. State = [[-0.2784161   0.11437377]]. Action = [[ 0.08095244 -0.03932668 -0.06092867 -0.725243  ]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 1729 is [True, False, False, False, True, False]
State prediction error at timestep 1729 is tensor(1.2917e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1729 of -1
Current timestep = 1730. State = [[-0.27840608  0.11431239]]. Action = [[-0.23021339  0.04374588  0.20840383 -0.03108376]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 1730 is [True, False, False, False, True, False]
State prediction error at timestep 1730 is tensor(2.2182e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1731. State = [[-0.27840608  0.11431239]]. Action = [[-0.20314434  0.12104538  0.1045292  -0.54990065]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 1731 is [True, False, False, False, True, False]
State prediction error at timestep 1731 is tensor(1.1648e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1731 of -1
Current timestep = 1732. State = [[-0.27840608  0.11431239]]. Action = [[ 0.02494007 -0.16284591 -0.0652979   0.3471725 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 1732 is [True, False, False, False, True, False]
State prediction error at timestep 1732 is tensor(6.6481e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1733. State = [[-0.2783341   0.11425479]]. Action = [[ 0.13290122  0.0116953  -0.09484389  0.12243462]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 1733 is [True, False, False, False, True, False]
State prediction error at timestep 1733 is tensor(1.1989e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1733 of -1
Current timestep = 1734. State = [[-0.27780592  0.11425371]]. Action = [[-0.2125625  -0.13285053  0.01072004 -0.80445355]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 1734 is [True, False, False, False, True, False]
State prediction error at timestep 1734 is tensor(1.8697e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1735. State = [[-0.2768719   0.11397146]]. Action = [[ 0.08534533 -0.05333    -0.08470762  0.6025839 ]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 1735 is [True, False, False, False, True, False]
State prediction error at timestep 1735 is tensor(3.8614e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1736. State = [[-0.27424616  0.11257248]]. Action = [[-0.1040154   0.09285006  0.24470386 -0.3210411 ]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 1736 is [True, False, False, False, True, False]
State prediction error at timestep 1736 is tensor(2.0859e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1737. State = [[-0.27447152  0.1132327 ]]. Action = [[ 6.7263842e-05  1.0726702e-01  2.2902316e-01 -4.0223420e-02]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 1737 is [True, False, False, False, True, False]
State prediction error at timestep 1737 is tensor(7.8102e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1737 of -1
Current timestep = 1738. State = [[-0.2754515   0.11609156]]. Action = [[ 0.101668    0.04962835 -0.07967466  0.58249307]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 1738 is [True, False, False, False, True, False]
State prediction error at timestep 1738 is tensor(6.1808e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1739. State = [[-0.27532277  0.11648896]]. Action = [[-0.07918665  0.17730772  0.18527442  0.7461653 ]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 1739 is [True, False, False, False, True, False]
State prediction error at timestep 1739 is tensor(1.0761e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1740. State = [[-0.2749487   0.11715757]]. Action = [[ 0.04843733  0.21096307 -0.01441063  0.06509185]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 1740 is [True, False, False, False, True, False]
State prediction error at timestep 1740 is tensor(1.4835e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1740 of -1
Current timestep = 1741. State = [[-0.2737799  0.1192162]]. Action = [[-0.12101182  0.06010902 -0.17449321  0.7558948 ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 1741 is [True, False, False, False, True, False]
State prediction error at timestep 1741 is tensor(3.3392e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1742. State = [[-0.27439246  0.12013027]]. Action = [[ 0.02909067 -0.22053789  0.17834619  0.32775593]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 1742 is [True, False, False, False, True, False]
State prediction error at timestep 1742 is tensor(3.6952e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1742 of -1
Current timestep = 1743. State = [[-0.2748513   0.12084018]]. Action = [[ 0.03244263  0.18610218 -0.00081094 -0.05766797]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 1743 is [True, False, False, False, True, False]
State prediction error at timestep 1743 is tensor(3.1971e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1744. State = [[-0.27514446  0.12134188]]. Action = [[-0.185245   -0.15185075  0.24299088  0.13458574]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 1744 is [True, False, False, False, True, False]
State prediction error at timestep 1744 is tensor(5.6750e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1745. State = [[-0.27549255  0.12203196]]. Action = [[-0.13454442  0.1384272   0.02441895 -0.428272  ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 1745 is [True, False, False, False, True, False]
State prediction error at timestep 1745 is tensor(2.1955e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1746. State = [[-0.275945    0.12263254]]. Action = [[-0.1156064   0.21046388  0.17893028  0.04912627]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 1746 is [True, False, False, False, True, False]
State prediction error at timestep 1746 is tensor(5.2820e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1747. State = [[-0.2763432   0.12342332]]. Action = [[-0.05861814  0.23297036  0.23182026 -0.3485546 ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 1747 is [True, False, False, False, True, False]
State prediction error at timestep 1747 is tensor(6.9473e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1748. State = [[-0.27701908  0.12447213]]. Action = [[ 0.07392269 -0.101945    0.17700559  0.70635605]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 1748 is [True, False, False, False, True, False]
State prediction error at timestep 1748 is tensor(1.0443e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1748 of -1
Current timestep = 1749. State = [[-0.27682328  0.12432614]]. Action = [[-0.10752535  0.22213721  0.14721996 -0.48245287]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 1749 is [True, False, False, False, True, False]
State prediction error at timestep 1749 is tensor(3.8741e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1750. State = [[-0.276392    0.12393466]]. Action = [[ 0.08938283  0.07557189 -0.16890657  0.27829933]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 1750 is [True, False, False, False, True, False]
State prediction error at timestep 1750 is tensor(7.3118e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1750 of -1
Current timestep = 1751. State = [[-0.27568623  0.12417584]]. Action = [[ 0.12169605 -0.18776466 -0.24511254 -0.3820398 ]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 1751 is [True, False, False, False, True, False]
State prediction error at timestep 1751 is tensor(4.3502e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1752. State = [[-0.27519727  0.12439825]]. Action = [[0.10855761 0.13354743 0.17984605 0.00167429]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 1752 is [True, False, False, False, True, False]
State prediction error at timestep 1752 is tensor(5.7025e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1753. State = [[-0.27475616  0.12455874]]. Action = [[ 0.18689978  0.16450846  0.1214582  -0.9025898 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 1753 is [True, False, False, False, True, False]
State prediction error at timestep 1753 is tensor(1.3018e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1754. State = [[-0.27422288  0.12467036]]. Action = [[ 0.16286802 -0.01263373 -0.2212532  -0.30859542]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 1754 is [True, False, False, False, True, False]
State prediction error at timestep 1754 is tensor(2.7507e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1754 of -1
Current timestep = 1755. State = [[-0.2734011  0.1248607]]. Action = [[ 0.2313478  -0.02195433 -0.01714276 -0.91868126]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 1755 is [True, False, False, False, True, False]
State prediction error at timestep 1755 is tensor(3.6707e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1756. State = [[-0.27291882  0.12511645]]. Action = [[ 0.20628282  0.14861065  0.03013092 -0.45225322]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 1756 is [True, False, False, False, False, True]
State prediction error at timestep 1756 is tensor(4.4452e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1757. State = [[-0.27234757  0.12532881]]. Action = [[ 0.22368318 -0.21357614 -0.07176346 -0.27757007]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 1757 is [True, False, False, False, False, True]
State prediction error at timestep 1757 is tensor(3.7200e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1758. State = [[-0.26955363  0.12613085]]. Action = [[ 0.05476704  0.00114712  0.09226048 -0.8858614 ]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 1758 is [True, False, False, False, False, True]
State prediction error at timestep 1758 is tensor(2.4434e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1758 of -1
Current timestep = 1759. State = [[-0.26632196  0.12710646]]. Action = [[-0.02246694  0.1086463  -0.06056663 -0.73290825]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 1759 is [True, False, False, False, False, True]
State prediction error at timestep 1759 is tensor(3.7665e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1759 of -1
Current timestep = 1760. State = [[-0.26469305  0.12980221]]. Action = [[ 0.06104013 -0.01544209  0.24626684 -0.70081925]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 1760 is [True, False, False, False, False, True]
State prediction error at timestep 1760 is tensor(1.1447e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1761. State = [[-0.26409116  0.13028063]]. Action = [[-0.23864813 -0.10488075  0.01184931  0.8498254 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 1761 is [True, False, False, False, False, True]
State prediction error at timestep 1761 is tensor(3.3730e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1762. State = [[-0.2637628   0.13040705]]. Action = [[-0.20644902  0.18877876  0.24581179  0.25974536]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 1762 is [True, False, False, False, False, True]
State prediction error at timestep 1762 is tensor(6.6647e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1762 of -1
Current timestep = 1763. State = [[-0.263183    0.13051116]]. Action = [[ 0.18844116  0.01930884 -0.11156195  0.20186412]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 1763 is [True, False, False, False, False, True]
State prediction error at timestep 1763 is tensor(5.1989e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1764. State = [[-0.2628348   0.13082753]]. Action = [[ 0.23444563 -0.0149885  -0.18001409 -0.837328  ]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 1764 is [True, False, False, False, False, True]
State prediction error at timestep 1764 is tensor(1.8849e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1765. State = [[-0.26185018  0.1317906 ]]. Action = [[ 0.00296482  0.12744462 -0.11529356 -0.22931117]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 1765 is [True, False, False, False, False, True]
State prediction error at timestep 1765 is tensor(2.0096e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1766. State = [[-0.26184645  0.13277876]]. Action = [[ 0.1397683   0.14153379 -0.13842256  0.5419022 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 1766 is [True, False, False, False, False, True]
State prediction error at timestep 1766 is tensor(1.6617e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1767. State = [[-0.2616225   0.13527223]]. Action = [[ 0.05528831 -0.06584761  0.21205884  0.33178043]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 1767 is [True, False, False, False, False, True]
State prediction error at timestep 1767 is tensor(1.5413e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1767 of 1
Current timestep = 1768. State = [[-0.26110277  0.13557716]]. Action = [[ 0.23496988 -0.21024509 -0.0157681  -0.02771199]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 1768 is [True, False, False, False, False, True]
State prediction error at timestep 1768 is tensor(1.0002e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1768 of 1
Current timestep = 1769. State = [[-0.26071796  0.13549408]]. Action = [[ 0.0459702   0.19827199 -0.09243721 -0.11604851]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 1769 is [True, False, False, False, False, True]
State prediction error at timestep 1769 is tensor(3.9363e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1770. State = [[-0.26046482  0.13543044]]. Action = [[ 0.16287756  0.02535582  0.23579854 -0.77620286]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 1770 is [True, False, False, False, False, True]
State prediction error at timestep 1770 is tensor(5.4197e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1771. State = [[-0.2587999   0.13610913]]. Action = [[ 0.05381134 -0.05976544  0.14909759 -0.33933753]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 1771 is [True, False, False, False, False, True]
State prediction error at timestep 1771 is tensor(1.2559e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1772. State = [[-0.25820506  0.13613589]]. Action = [[-0.22366475 -0.1019319  -0.15624803  0.19405413]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 1772 is [True, False, False, False, False, True]
State prediction error at timestep 1772 is tensor(1.0542e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1773. State = [[-0.2578189   0.13611384]]. Action = [[-0.14513268  0.05447048  0.04812118 -0.95124894]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 1773 is [True, False, False, False, False, True]
State prediction error at timestep 1773 is tensor(2.2169e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1774. State = [[-0.2571611  0.1361221]]. Action = [[-0.24538158  0.14313641 -0.05988814 -0.00087547]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 1774 is [True, False, False, False, False, True]
State prediction error at timestep 1774 is tensor(1.4788e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1775. State = [[-0.25678074  0.1362332 ]]. Action = [[ 0.07376057  0.16655016 -0.02115053  0.31183267]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 1775 is [True, False, False, False, False, True]
State prediction error at timestep 1775 is tensor(1.4077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1776. State = [[-0.25630227  0.13637103]]. Action = [[ 0.22733006  0.21740764 -0.0008458  -0.0172897 ]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 1776 is [True, False, False, False, False, True]
State prediction error at timestep 1776 is tensor(4.2166e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1776 of 1
Current timestep = 1777. State = [[-0.25530416  0.13639753]]. Action = [[-0.07175237 -0.08982283  0.0387359  -0.71939635]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 1777 is [True, False, False, False, False, True]
State prediction error at timestep 1777 is tensor(3.4593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1778. State = [[-0.2548669   0.13578032]]. Action = [[-0.12293594  0.20411184 -0.0698096   0.18318748]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 1778 is [True, False, False, False, False, True]
State prediction error at timestep 1778 is tensor(2.0360e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1779. State = [[-0.25474736  0.1353626 ]]. Action = [[-0.1527189  -0.20833465 -0.22426748  0.8649528 ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 1779 is [True, False, False, False, False, True]
State prediction error at timestep 1779 is tensor(5.7054e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1780. State = [[-0.25482282  0.13512905]]. Action = [[ 0.11678743 -0.23718765  0.06693712  0.21340704]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 1780 is [True, False, False, False, False, True]
State prediction error at timestep 1780 is tensor(1.1531e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1781. State = [[-0.25481623  0.1348987 ]]. Action = [[ 0.07092375  0.21138489 -0.1875573  -0.78247184]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 1781 is [True, False, False, False, False, True]
State prediction error at timestep 1781 is tensor(2.4111e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1782. State = [[-0.25462008  0.13441136]]. Action = [[ 0.18972087 -0.18921983  0.13922286  0.9216759 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 1782 is [True, False, False, False, False, True]
State prediction error at timestep 1782 is tensor(4.7048e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1783. State = [[-0.254552    0.13390033]]. Action = [[-0.00773284 -0.00108419  0.17078182 -0.18134648]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 1783 is [True, False, False, False, False, True]
State prediction error at timestep 1783 is tensor(8.3534e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1783 of 1
Current timestep = 1784. State = [[-0.2546403   0.13373029]]. Action = [[ 0.0925318   0.20167232 -0.04967661  0.31762648]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 1784 is [True, False, False, False, False, True]
State prediction error at timestep 1784 is tensor(2.3892e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1785. State = [[-0.25458166  0.13348065]]. Action = [[ 0.09686264  0.08057797 -0.21080737 -0.52064097]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 1785 is [True, False, False, False, False, True]
State prediction error at timestep 1785 is tensor(1.1108e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1785 of 1
Current timestep = 1786. State = [[-0.25432658  0.13360332]]. Action = [[ 0.08933362  0.24283564 -0.19320594 -0.0803898 ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 1786 is [True, False, False, False, False, True]
State prediction error at timestep 1786 is tensor(9.7354e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1787. State = [[-0.25410923  0.1336887 ]]. Action = [[-0.22933385 -0.2381162  -0.08994198 -0.9874942 ]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 1787 is [True, False, False, False, False, True]
State prediction error at timestep 1787 is tensor(4.8282e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1788. State = [[-0.25414112  0.13369101]]. Action = [[-0.19969343 -0.09232482 -0.18650222 -0.36810017]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 1788 is [True, False, False, False, False, True]
State prediction error at timestep 1788 is tensor(1.3715e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1789. State = [[-0.2535156   0.13388932]]. Action = [[-0.11626491 -0.11880538  0.21730858  0.23318827]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 1789 is [True, False, False, False, False, True]
State prediction error at timestep 1789 is tensor(1.9698e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1789 of 1
Current timestep = 1790. State = [[-0.25322002  0.13350737]]. Action = [[ 0.21562666  0.02641031 -0.1270438  -0.741729  ]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 1790 is [True, False, False, False, False, True]
State prediction error at timestep 1790 is tensor(1.3187e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1791. State = [[-0.2532259   0.13315362]]. Action = [[ 0.01776615 -0.20197834  0.03124267  0.75255966]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 1791 is [True, False, False, False, False, True]
State prediction error at timestep 1791 is tensor(2.7677e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1791 of 1
Current timestep = 1792. State = [[-0.2531215   0.13293469]]. Action = [[ 0.14335686  0.19784606  0.21758372 -0.3023123 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 1792 is [True, False, False, False, False, True]
State prediction error at timestep 1792 is tensor(8.6961e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1793. State = [[-0.25331622  0.13282908]]. Action = [[-0.07552224 -0.15086344 -0.0463222   0.75784445]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 1793 is [True, False, False, False, False, True]
State prediction error at timestep 1793 is tensor(3.5576e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1794. State = [[-0.25326446  0.13238522]]. Action = [[-0.22400728 -0.09621662 -0.08323109  0.15250432]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 1794 is [True, False, False, False, False, True]
State prediction error at timestep 1794 is tensor(9.8759e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1795. State = [[-0.25304952  0.13222852]]. Action = [[-0.22552975 -0.12014174  0.1912784   0.05864263]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 1795 is [True, False, False, False, False, True]
State prediction error at timestep 1795 is tensor(7.5650e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1796. State = [[-0.25326672  0.13211906]]. Action = [[-0.0701265   0.21522975 -0.10678485 -0.9770847 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 1796 is [True, False, False, False, False, True]
State prediction error at timestep 1796 is tensor(1.3347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1796 of 1
Current timestep = 1797. State = [[-0.25318518  0.1319457 ]]. Action = [[ 0.16751921  0.09356666 -0.20520672  0.23263872]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 1797 is [True, False, False, False, False, True]
State prediction error at timestep 1797 is tensor(4.4404e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1798. State = [[-0.25316122  0.13185215]]. Action = [[-0.07968375 -0.16984914 -0.13366285  0.8088652 ]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 1798 is [True, False, False, False, False, True]
State prediction error at timestep 1798 is tensor(4.6709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1799. State = [[-0.25323454  0.13162376]]. Action = [[ 0.23963112 -0.00188471  0.08110565 -0.46913147]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 1799 is [True, False, False, False, False, True]
State prediction error at timestep 1799 is tensor(5.5990e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1800. State = [[-0.2531475   0.13144623]]. Action = [[-0.24466819 -0.08474794  0.14052653 -0.390046  ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 1800 is [True, False, False, False, False, True]
State prediction error at timestep 1800 is tensor(2.9943e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1800 of 1
Current timestep = 1801. State = [[-0.25303963  0.13105072]]. Action = [[-0.00283065 -0.10021532 -0.10883173 -0.01639491]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 1801 is [True, False, False, False, False, True]
State prediction error at timestep 1801 is tensor(1.6922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1802. State = [[-0.25265244  0.12969713]]. Action = [[0.14378965 0.14417657 0.21623331 0.48449123]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 1802 is [True, False, False, False, False, True]
State prediction error at timestep 1802 is tensor(9.6896e-06, grad_fn=<MseLossBackward0>)
