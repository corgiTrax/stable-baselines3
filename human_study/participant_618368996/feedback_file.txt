Current timestep = 0. State = [[-0.32469195 -0.09222239]]. Action = [[ 0.02229474 -0.03176285  0.          0.6945392 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0657, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.32169    -0.08889792]]. Action = [[0.07273958 0.08031679 0.         0.5653672 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0604, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.31575733 -0.09026128]]. Action = [[ 0.09720076 -0.09275587  0.          0.741688  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0470, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.31539977 -0.09703942]]. Action = [[-0.05017178 -0.08716325  0.         -0.943911  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0102, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.3189274  -0.09997803]]. Action = [[-0.03822258  0.00587471  0.          0.94105077]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0348, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.3192561  -0.10158626]]. Action = [[ 0.02517965 -0.01759406  0.         -0.7861393 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0068, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.32323053 -0.10333747]]. Action = [[-0.08941043 -0.0064505   0.          0.7858386 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0282, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.32839146 -0.1074706 ]]. Action = [[-0.04470081 -0.05430931  0.          0.82288647]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0238, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.33124778 -0.10866386]]. Action = [[-0.0169847   0.03191321  0.         -0.3618747 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0114, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.3366494  -0.10637921]]. Action = [[-0.08447038  0.04732276  0.         -0.9840242 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0009, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.33833092 -0.10441676]]. Action = [[ 0.03703273  0.01931207  0.         -0.12192076]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0116, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.3342493  -0.10812412]]. Action = [[ 0.08420695 -0.09167871  0.          0.74440813]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0113, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.33626583 -0.10851888]]. Action = [[-0.09670325  0.05813963  0.          0.8216846 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0082, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.3442532  -0.10352097]]. Action = [[-0.07300423  0.06877359  0.         -0.41722924]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.34546912 -0.09616181]]. Action = [[0.07069906 0.08796903 0.         0.86406636]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0053, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.34294635 -0.09649497]]. Action = [[ 0.05248416 -0.09627838  0.         -0.39803505]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0010, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.3449673  -0.10226007]]. Action = [[-0.04108862 -0.0803601   0.         -0.47553897]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.34400058 -0.10923959]]. Action = [[ 0.06968062 -0.09461781  0.          0.1686213 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0064, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.33848074 -0.10828365]]. Action = [[ 0.09102681  0.07833097  0.         -0.71004915]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0010, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.3325999  -0.10463141]]. Action = [[ 0.06959952  0.01834466  0.         -0.08724403]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.33051404 -0.10386423]]. Action = [[ 0.00016807 -0.00547317  0.         -0.13060194]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.32830134 -0.10723535]]. Action = [[ 0.03979058 -0.06269988  0.         -0.18645298]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0027, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.32547617 -0.11232667]]. Action = [[ 0.02437621 -0.05620865  0.          0.79062796]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0014, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.31957176 -0.11426886]]. Action = [[0.08753114 0.00821797 0.         0.5873673 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.31309777 -0.11336567]]. Action = [[ 0.05777419  0.02241634  0.         -0.9861307 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.3103534  -0.11678445]]. Action = [[-0.0045436  -0.07093754  0.         -0.19830889]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0030, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.30916542 -0.12370919]]. Action = [[-0.00143009 -0.07512547  0.         -0.40117687]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0023, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.3117814  -0.13249643]]. Action = [[-0.08038585 -0.09643894  0.          0.7585678 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.31214648 -0.14170246]]. Action = [[ 0.01289518 -0.08279353  0.          0.6356    ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0033, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.3115234  -0.14942949]]. Action = [[-0.01849688 -0.05840928  0.         -0.50469697]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.30875516 -0.15492862]]. Action = [[ 0.04161952 -0.02692783  0.          0.35449076]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0066, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.30840242 -0.15996177]]. Action = [[-0.03869091 -0.03813425  0.          0.6267662 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.31305218 -0.15967366]]. Action = [[-0.09091046  0.08160389  0.          0.7057352 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0035, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.31577054 -0.15902108]]. Action = [[-0.00402316  0.00764774  0.          0.6131947 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0043, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.31159502 -0.16097516]]. Action = [[ 0.09087718 -0.02516168  0.         -0.96932036]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0007, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.31125543 -0.16675065]]. Action = [[-0.04839773 -0.08810036  0.         -0.21727526]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0065, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.30964687 -0.17251141]]. Action = [[ 0.05786072 -0.04145825  0.         -0.43856788]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0056, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.30406824 -0.17508757]]. Action = [[ 0.07644675 -0.00989364  0.          0.40563416]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0057, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.29715264 -0.17963302]]. Action = [[ 0.08511069 -0.07509582  0.         -0.8732388 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0014, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.29192883 -0.18418851]]. Action = [[ 0.04162639 -0.0346697   0.         -0.41661298]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0050, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.2927413  -0.18320239]]. Action = [[-0.05808891  0.07000787  0.         -0.71424955]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0020, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of 0
Current timestep = 41. State = [[-0.29353902 -0.18204808]]. Action = [[ 0.01744413  0.00466637  0.         -0.5892104 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0028, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.2931848  -0.17980494]]. Action = [[0.00065404 0.05158176 0.         0.43119264]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0044, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.29555914 -0.17445688]]. Action = [[-0.04684295  0.08387423  0.         -0.27695876]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0046, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.29570287 -0.16759865]]. Action = [[ 0.02950452  0.08452747  0.         -0.9637595 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0007, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.29640025 -0.16558322]]. Action = [[-0.02413131 -0.02835485  0.         -0.46523118]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0027, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.29387936 -0.16180705]]. Action = [[ 0.07075315  0.06816966  0.         -0.64940655]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.29591495 -0.16173671]]. Action = [[-0.08260342 -0.07557026  0.          0.35634768]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0028, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.3011401  -0.16368128]]. Action = [[-0.05194209 -0.00829579  0.         -0.94474494]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(0.0004, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.30703595 -0.16739549]]. Action = [[-0.0777939  -0.06980391  0.          0.6738839 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0016, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.31221113 -0.16777614]]. Action = [[-0.04864633  0.03735519  0.         -0.7946645 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, True, False, False]
State prediction error at timestep 50 is tensor(0.0004, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.31231257 -0.17101721]]. Action = [[ 0.04254516 -0.08913099  0.          0.35698485]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, True, False, False]
State prediction error at timestep 51 is tensor(0.0037, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.31427574 -0.17833112]]. Action = [[-0.06184388 -0.08949476  0.         -0.9393412 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, True, False, False]
State prediction error at timestep 52 is tensor(0.0006, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.31133518 -0.18375152]]. Action = [[ 0.09421966 -0.03872861  0.          0.11226165]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0057, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.30423397 -0.18929246]]. Action = [[ 0.08198894 -0.07654944  0.         -0.6582404 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0020, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.30230245 -0.19256786]]. Action = [[-0.02663101  0.0009457   0.         -0.32229596]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0045, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.30532002 -0.19734687]]. Action = [[-0.04987905 -0.06569768  0.          0.9261811 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.3085222 -0.1977979]]. Action = [[-0.03078951  0.06540418  0.          0.0798614 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0064, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.30951038 -0.19962773]]. Action = [[ 0.00753858 -0.04291618  0.          0.25515378]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0065, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.31042454 -0.19889061]]. Action = [[-0.0170082   0.06472065  0.          0.13889289]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0068, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.3108053 -0.1963421]]. Action = [[ 0.00855626  0.03354759  0.         -0.48857105]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0043, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of 1
Current timestep = 61. State = [[-0.3077432  -0.19948201]]. Action = [[ 0.06589394 -0.08229218  0.         -0.91798717]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.3017633  -0.20049955]]. Action = [[0.07952542 0.025827   0.         0.76469254]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0028, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.29940414 -0.20012711]]. Action = [[-0.00624534 -0.00670759  0.         -0.5443864 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0030, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.3008012  -0.20031454]]. Action = [[-0.02763791  0.00512973  0.         -0.8383193 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0011, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.30573222 -0.19939093]]. Action = [[-0.07990558  0.02791148  0.         -0.20271534]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0048, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.30796102 -0.19996409]]. Action = [[ 0.00897948 -0.01687945  0.         -0.13574022]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0049, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.30863878 -0.19750749]]. Action = [[-0.01416697  0.06199572  0.         -0.4905854 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.31349757 -0.2003462 ]]. Action = [[-0.08792152 -0.09475216  0.         -0.02675372]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0054, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.31392586 -0.205084  ]]. Action = [[ 0.0519922  -0.03687932  0.         -0.7779054 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0015, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.3150729  -0.20338239]]. Action = [[-0.0516196   0.06600701  0.          0.9127767 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0020, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.32074818 -0.2054449 ]]. Action = [[-0.07371411 -0.07469855  0.         -0.71328807]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.32870954 -0.21007884]]. Action = [[-0.09704373 -0.04108163  0.          0.5745995 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0048, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.3355902  -0.20839562]]. Action = [[-0.05720449  0.07653277  0.         -0.85508054]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0015, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.34099644 -0.20851484]]. Action = [[-0.04700837 -0.03534964  0.         -0.8414261 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of 1
Current timestep = 75. State = [[-0.33910444 -0.20969087]]. Action = [[ 0.08989281 -0.00482066  0.         -0.50741583]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0051, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.3349954  -0.21295616]]. Action = [[ 0.04249585 -0.07546566  0.          0.87692595]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0030, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.33349133 -0.2143795 ]]. Action = [[0.01020322 0.00825013 0.         0.83933187]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.33479536 -0.20979007]]. Action = [[-0.0223187   0.09091101  0.          0.55085635]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0052, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.33765268 -0.20511289]]. Action = [[-0.02550819  0.04106606  0.         -0.44007742]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0046, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.33612865 -0.19901669]]. Action = [[ 0.06580243  0.08596642  0.         -0.66634506]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0022, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.33682328 -0.19091904]]. Action = [[-0.03684022  0.09110386  0.          0.16825831]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0059, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.34315178 -0.1841725 ]]. Action = [[-0.0836518   0.06119355  0.         -0.6011471 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0022, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.34220466 -0.17750654]]. Action = [[ 0.09882703  0.06748375  0.         -0.3738672 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0040, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.33634493 -0.17095779]]. Action = [[ 0.07679487  0.04070277  0.         -0.8586319 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.33339235 -0.16430788]]. Action = [[ 0.02301552  0.05380971  0.         -0.7741102 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0005, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.33616838 -0.1626865 ]]. Action = [[-0.05424985 -0.04784738  0.          0.3835485 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0033, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of 1
Current timestep = 87. State = [[-0.3371647  -0.15910073]]. Action = [[ 0.03247175  0.06224417  0.         -0.76462257]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0007, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.33567217 -0.15812066]]. Action = [[ 0.02652853 -0.05442989  0.         -0.4401579 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0024, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.33680072 -0.1580501 ]]. Action = [[-3.1861305e-02 -2.7105212e-05  0.0000000e+00  7.3157954e-01]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.3399589  -0.16048458]]. Action = [[-0.03879628 -0.06560057  0.         -0.07986933]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.34613216 -0.15968648]]. Action = [[-0.09812424  0.05320794  0.          0.78626895]]. Reward = [0.]
