Current timestep = 0. State = [[-0.25698802  0.00805507]]. Action = [[ 0.23694551 -0.3158666   0.4001255   0.69159937]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0370, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25698802  0.00805507]]. Action = [[-0.92702425  0.6463051  -0.69828475 -0.90838593]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0309, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25698802  0.00805507]]. Action = [[ 0.431172   -0.69346577  0.95268154 -0.8266194 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25698802  0.00805507]]. Action = [[-0.6191224  -0.8646175  -0.36299372 -0.9427137 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25698802  0.00805507]]. Action = [[-0.8932218  -0.31902516 -0.06521535 -0.96725   ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25698802  0.00805507]]. Action = [[-0.62518495  0.8958969  -0.71612394 -0.81819224]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25698802  0.00805507]]. Action = [[-0.9127674  -0.9416795  -0.07108521 -0.9419276 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.25698802  0.00805507]]. Action = [[ 0.99342537 -0.637028    0.4082291  -0.44676226]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.25698802  0.00805507]]. Action = [[-0.48321366 -0.03351337  0.9150748   0.68643   ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.25698802  0.00805507]]. Action = [[0.44725406 0.8839445  0.7067809  0.39446414]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25698802  0.00805507]]. Action = [[-0.4550606  -0.46076268 -0.5033027  -0.8629158 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.25698802  0.00805507]]. Action = [[ 0.3665377   0.04012442 -0.74847436 -0.2651012 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.25698802  0.00805507]]. Action = [[ 0.50367045 -0.20835054  0.97279    -0.15000635]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.25698802  0.00805507]]. Action = [[-0.6311672  -0.3067553  -0.448704   -0.10343075]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25698802  0.00805507]]. Action = [[-0.76916033 -0.3549472   0.7835748  -0.14893824]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.25698802  0.00805507]]. Action = [[-0.9152112  -0.59463996 -0.9122581  -0.09049177]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25698802  0.00805507]]. Action = [[ 0.10896587  0.19390273 -0.6366157  -0.6888959 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25698802  0.00805507]]. Action = [[ 0.6865511   0.6120584   0.97547174 -0.94940495]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25698802  0.00805507]]. Action = [[ 0.3678918  -0.30702996  0.73363686  0.56173706]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.25698802  0.00805507]]. Action = [[ 0.76889205 -0.17811167 -0.851774    0.7887063 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25698802  0.00805507]]. Action = [[ 0.77439594 -0.08540064 -0.805004   -0.00450951]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25698802  0.00805507]]. Action = [[0.6760119  0.27485108 0.9576297  0.69684565]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25698802  0.00805507]]. Action = [[-0.42037773 -0.8225846   0.37246072  0.84357786]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.25698802  0.00805507]]. Action = [[ 0.08399677  0.19906151 -0.11951125 -0.9020031 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.25698802  0.00805507]]. Action = [[-0.5929377  -0.5578975  -0.06806111  0.39090002]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.25698802  0.00805507]]. Action = [[-0.45966953 -0.7186602   0.63950217  0.67077756]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(5.5447e-05, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.25698802  0.00805507]]. Action = [[-0.73432755 -0.699328   -0.13758105 -0.2798949 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.25698802  0.00805507]]. Action = [[-0.4534943  -0.54032785 -0.6910059   0.6918738 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.25698802  0.00805507]]. Action = [[-0.7702899   0.33318293  0.39405107  0.96581674]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.25698802  0.00805507]]. Action = [[-0.07576185 -0.27259874 -0.7978225  -0.17855942]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.25698802  0.00805507]]. Action = [[-0.5339249   0.10506427  0.8101945   0.45782804]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.25698802  0.00805507]]. Action = [[ 0.01738322 -0.6642034  -0.42519718 -0.96678317]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(2.6282e-05, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.25698802  0.00805507]]. Action = [[0.58361554 0.8121438  0.81506526 0.6722473 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.25698802  0.00805507]]. Action = [[-0.25267625 -0.6874462  -0.30250978 -0.5050797 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.25698802  0.00805507]]. Action = [[-0.21008587 -0.7540221   0.05198181 -0.16228473]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.25698802  0.00805507]]. Action = [[0.27968276 0.8272779  0.7381642  0.64459586]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.25698802  0.00805507]]. Action = [[ 0.73849106 -0.6691862   0.10292792 -0.7854772 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.25698802  0.00805507]]. Action = [[ 0.9235611   0.67556787 -0.40862918 -0.42284244]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.25698802  0.00805507]]. Action = [[-0.1704188  -0.8666302  -0.4676999  -0.68568236]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(7.7378e-05, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.25698802  0.00805507]]. Action = [[ 0.5548382  -0.04602474 -0.39227724  0.3171879 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.25698802  0.00805507]]. Action = [[ 0.36589515 -0.68662035 -0.4908116   0.49703622]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(6.6713e-05, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.25698802  0.00805507]]. Action = [[-0.18105257 -0.57387227  0.9840752  -0.21631497]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.25698802  0.00805507]]. Action = [[-0.5175274  -0.0400759  -0.39051962 -0.6488696 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(9.8625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.25698802  0.00805507]]. Action = [[ 0.40644765  0.61707735 -0.6179224  -0.7783033 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 44. State = [[-0.25698802  0.00805507]]. Action = [[-0.0023886   0.73445296  0.21530557 -0.8582306 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.25698802  0.00805507]]. Action = [[-0.58737564  0.19545102 -0.97146136 -0.47359496]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.25698802  0.00805507]]. Action = [[-0.6148267   0.5467396   0.61256313  0.26652086]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(6.1518e-05, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.25698802  0.00805507]]. Action = [[-0.5204169   0.12184691 -0.28201783  0.7055266 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 48. State = [[-0.25698802  0.00805507]]. Action = [[ 0.6272323  -0.56849223 -0.7928127   0.6191089 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.25698802  0.00805507]]. Action = [[-0.79304737 -0.20136231 -0.960901   -0.7089109 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 50. State = [[-0.25698802  0.00805507]]. Action = [[-0.15403253 -0.09194607  0.69577813 -0.5973964 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 51. State = [[-0.25698802  0.00805507]]. Action = [[-0.9313038  -0.78687716  0.37851274  0.5313916 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.25698802  0.00805507]]. Action = [[ 0.6623535  -0.8609554  -0.8917167  -0.27041554]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(6.4010e-05, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.25698802  0.00805507]]. Action = [[0.6420971  0.55812716 0.77612317 0.80808735]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.25698802  0.00805507]]. Action = [[ 2.9459155e-01 -8.6913258e-01  5.9499347e-01  5.0914288e-04]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.25698802  0.00805507]]. Action = [[-0.24272478  0.94863975  0.05600071  0.051826  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 56. State = [[-0.25698802  0.00805507]]. Action = [[0.9292371  0.5818095  0.91197634 0.60630476]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.25698802  0.00805507]]. Action = [[-0.18958938 -0.52374226  0.35751438 -0.9016983 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.25698802  0.00805507]]. Action = [[ 0.49624813 -0.765409   -0.66960585  0.9249307 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.25698802  0.00805507]]. Action = [[ 0.10518944 -0.14509416 -0.08672976  0.9170437 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(6.0655e-05, grad_fn=<MseLossBackward0>)
Current timestep = 60. State = [[-0.25698802  0.00805507]]. Action = [[0.85991263 0.2669661  0.05559266 0.32487512]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.25698802  0.00805507]]. Action = [[-0.65613055 -0.5145428   0.36343324 -0.04960501]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.25698802  0.00805507]]. Action = [[ 0.9252895  -0.5984902  -0.03937781  0.43501663]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 63. State = [[-0.25698802  0.00805507]]. Action = [[-0.472234   -0.23629475 -0.7992111   0.88237333]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.25698802  0.00805507]]. Action = [[-0.15052974 -0.24686217  0.8261895  -0.51220596]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.25698802  0.00805507]]. Action = [[ 0.6025801   0.03387475 -0.948449   -0.596968  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(6.7852e-05, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.25698802  0.00805507]]. Action = [[-0.65351915 -0.32728016 -0.9318528  -0.7967628 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.25698802  0.00805507]]. Action = [[-0.81707954 -0.01195705 -0.87628675 -0.32912707]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.25698802  0.00805507]]. Action = [[-0.49463606  0.9440954   0.4659561   0.8611014 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.25698802  0.00805507]]. Action = [[ 0.7947638  -0.1441161   0.7049494   0.21145082]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(3.8049e-05, grad_fn=<MseLossBackward0>)
Current timestep = 70. State = [[-0.25698802  0.00805507]]. Action = [[ 0.16980529  0.42384338 -0.5207874  -0.16010624]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.25698802  0.00805507]]. Action = [[-0.89880395  0.78262997 -0.9505956   0.5156548 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.25698802  0.00805507]]. Action = [[ 0.7895415  -0.11160803 -0.5067423   0.6464095 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(8.2651e-05, grad_fn=<MseLossBackward0>)
Current timestep = 73. State = [[-0.25698802  0.00805507]]. Action = [[-0.6470164  -0.24604738 -0.25130272  0.11188507]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 74. State = [[-0.25698802  0.00805507]]. Action = [[ 0.63764596 -0.12986308 -0.52089614  0.23174238]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.25698802  0.00805507]]. Action = [[0.47108948 0.06947768 0.15216601 0.98823214]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(4.6730e-05, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.25698802  0.00805507]]. Action = [[-0.49156034 -0.9546974  -0.1943152   0.58883834]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(4.8662e-05, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.25698802  0.00805507]]. Action = [[ 0.52605987  0.1354686  -0.38913155  0.00070691]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.25698802  0.00805507]]. Action = [[ 0.46352458 -0.2597915  -0.35947955  0.3632778 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.25698802  0.00805507]]. Action = [[-0.78911114  0.4688164   0.894258   -0.3202995 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.25698802  0.00805507]]. Action = [[ 0.45664454 -0.87217176  0.9234464  -0.79507166]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(2.2297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.25698802  0.00805507]]. Action = [[ 2.7634013e-01 -3.1334162e-04  6.3150072e-01 -4.8775464e-01]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(6.0118e-05, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.25698802  0.00805507]]. Action = [[0.3279996  0.39389718 0.935259   0.05652738]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 83. State = [[-0.25698802  0.00805507]]. Action = [[-0.41351092  0.11980581  0.969625   -0.63733715]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(8.6462e-05, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.25698802  0.00805507]]. Action = [[-0.8692123   0.6620593  -0.19158357 -0.6491152 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(4.8802e-05, grad_fn=<MseLossBackward0>)
Current timestep = 85. State = [[-0.25698802  0.00805507]]. Action = [[ 0.24240434 -0.23471081  0.65226555 -0.5149017 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(6.2907e-05, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.25698802  0.00805507]]. Action = [[-0.17780113  0.7095411  -0.676943    0.1954329 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.25698802  0.00805507]]. Action = [[-0.6857069   0.74551487 -0.8845221   0.2244699 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.25698802  0.00805507]]. Action = [[ 0.25377858 -0.02273393  0.8308723  -0.34176064]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.25698802  0.00805507]]. Action = [[-0.01390928  0.5673108   0.35178196 -0.67336696]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.25698802  0.00805507]]. Action = [[ 0.7217853  -0.33303666 -0.17787087 -0.9356513 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(7.6181e-06, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.25698802  0.00805507]]. Action = [[-0.77766883 -0.49841368  0.34349477 -0.19895697]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(6.3936e-05, grad_fn=<MseLossBackward0>)
Current timestep = 92. State = [[-0.25685224  0.00805496]]. Action = [[-0.03088373 -0.01419193  0.4825158   0.8946476 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 93. State = [[-0.25677872  0.00799986]]. Action = [[ 0.37146282 -0.5837705   0.8827493   0.21520472]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.25678238  0.00783514]]. Action = [[0.40535283 0.5396099  0.85254407 0.25644517]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.25681198  0.00771935]]. Action = [[-0.6873096  -0.28858787 -0.46673757  0.83294666]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(7.8307e-06, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.25665072  0.00766649]]. Action = [[-0.8494976  -0.7954104  -0.29502487 -0.7164721 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 97. State = [[-0.25646862  0.00762579]]. Action = [[ 0.36135352  0.371539   -0.6786905  -0.64394265]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 98. State = [[-0.25650397  0.00756497]]. Action = [[-0.83231765  0.87601316 -0.8665913  -0.15479755]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.25662047  0.00748648]]. Action = [[-0.3383733   0.600713    0.66745055 -0.50823545]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.25653413  0.00744607]]. Action = [[-0.687538   -0.4324721  -0.10131931 -0.45595914]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(3.2599e-05, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.256518    0.00739981]]. Action = [[ 0.95106626 -0.6810357  -0.6713868   0.6306076 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 102. State = [[-0.25633058  0.00744566]]. Action = [[-0.4342414   0.09182155  0.57324755 -0.8396911 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(4.0508e-06, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.25634274  0.00732992]]. Action = [[-0.28200364 -0.50879186 -0.7657926   0.04995942]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.2564267   0.00737634]]. Action = [[ 0.9572016   0.16306973 -0.81520015 -0.78828657]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.25634396  0.00743088]]. Action = [[ 0.4554248   0.32772422 -0.87587816 -0.8061488 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(6.8079e-06, grad_fn=<MseLossBackward0>)
Current timestep = 106. State = [[-0.25624764  0.00733571]]. Action = [[-0.6420751   0.8749287  -0.19580895 -0.00531769]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.2563276   0.00726602]]. Action = [[ 0.14698243 -0.8259205  -0.91271466  0.25652337]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(4.5905e-05, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.2563162   0.00716468]]. Action = [[-0.01369959  0.2350657   0.7863536   0.96392655]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(7.8373e-05, grad_fn=<MseLossBackward0>)
Current timestep = 109. State = [[-0.25622195  0.0072806 ]]. Action = [[ 0.24999964 -0.30105847  0.46675074 -0.82353646]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(6.5626e-06, grad_fn=<MseLossBackward0>)
Current timestep = 110. State = [[-0.25621647  0.00722552]]. Action = [[-0.5937926  -0.75630045 -0.77029455 -0.04823619]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(1.3140e-05, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.25631672  0.00715586]]. Action = [[ 0.7689109  -0.04820371  0.6870258  -0.9235506 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.25622118  0.00717044]]. Action = [[-0.573701   -0.19452095  0.25988007  0.16016638]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(8.0625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 113. State = [[-0.2562259   0.00711536]]. Action = [[ 0.97526646 -0.3372755  -0.8518788   0.5531709 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.25632614  0.0070461 ]]. Action = [[ 0.52822256 -0.29261416 -0.44884163  0.5570495 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 115. State = [[-0.25623062  0.00706066]]. Action = [[-0.35267723 -0.4266938  -0.8565931   0.9323224 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 116. State = [[-0.25630257  0.00706085]]. Action = [[ 0.69688916  0.33542752 -0.96367127 -0.06544596]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(1.1870e-05, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.25632614  0.0070461 ]]. Action = [[-0.5425887   0.58543754  0.19690752  0.1927234 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.25623062  0.00706066]]. Action = [[-0.11028123  0.25649738  0.6468532  -0.10740554]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.25632614  0.0070461 ]]. Action = [[ 0.9366839  -0.6125183  -0.45185137  0.02800119]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.25632614  0.0070461 ]]. Action = [[0.09598517 0.74021244 0.48160148 0.85513616]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(3.1255e-05, grad_fn=<MseLossBackward0>)
Current timestep = 121. State = [[-0.25630257  0.00706085]]. Action = [[ 0.7644794  -0.8795363   0.67900753  0.960484  ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 122. State = [[-0.25632614  0.0070461 ]]. Action = [[ 0.31181276 -0.29002994 -0.8603794   0.87615323]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(7.4634e-05, grad_fn=<MseLossBackward0>)
Current timestep = 123. State = [[-0.25632614  0.0070461 ]]. Action = [[-0.53618425 -0.215006    0.49688447 -0.48980463]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(6.0790e-05, grad_fn=<MseLossBackward0>)
Current timestep = 124. State = [[-0.25632614  0.0070461 ]]. Action = [[ 0.5055157   0.84967196 -0.5813271   0.5596093 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(8.9418e-05, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.25632614  0.0070461 ]]. Action = [[ 0.79453754  0.8630812  -0.49227673 -0.9253691 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(8.9132e-05, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.25632614  0.0070461 ]]. Action = [[-0.9696646  -0.08660853  0.38867354  0.25522542]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(3.0372e-06, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.2563309   0.00699104]]. Action = [[ 0.6877024   0.5205071   0.8922163  -0.74807376]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 128. State = [[-0.2563309   0.00699104]]. Action = [[0.25215495 0.4179293  0.4989159  0.2909646 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 129. State = [[-0.2563309   0.00699104]]. Action = [[-0.20541489  0.6940565   0.07607222  0.6564958 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.2563309   0.00699104]]. Action = [[ 0.7855798  -0.6631588  -0.54855084 -0.05023545]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(3.0758e-05, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.2563309   0.00699104]]. Action = [[-0.6826586  -0.53863657 -0.10033149 -0.79793745]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.2563309   0.00699104]]. Action = [[-0.16007537  0.08280349  0.93444407 -0.8405711 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(3.3633e-05, grad_fn=<MseLossBackward0>)
Current timestep = 133. State = [[-0.2563309   0.00699104]]. Action = [[ 0.23919773 -0.61755425  0.79470456 -0.9361399 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(7.7902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.2563309   0.00699104]]. Action = [[ 0.50089777 -0.7809814   0.21964824 -0.67406505]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.2563309   0.00699104]]. Action = [[-0.14730275  0.13248229  0.03633595 -0.5831369 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(8.4308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.25634038  0.00688091]]. Action = [[0.47155738 0.644441   0.6576936  0.19902909]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(1.4226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.25634038  0.00688091]]. Action = [[ 0.08628404 -0.21441472 -0.2439146  -0.5463826 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.25631678  0.00689565]]. Action = [[-0.03316414 -0.4124608  -0.44476002  0.7805369 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.25634038  0.00688091]]. Action = [[-0.628935    0.76952434 -0.74973816 -0.7482655 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(3.8651e-06, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.25634038  0.00688091]]. Action = [[-0.5182585   0.8381299  -0.47166908 -0.5882754 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(7.9860e-05, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.25634038  0.00688091]]. Action = [[ 0.4173956  -0.20059043  0.50674736 -0.5110891 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(5.9901e-06, grad_fn=<MseLossBackward0>)
Current timestep = 142. State = [[-0.25634038  0.00688091]]. Action = [[-0.24481088 -0.4103307  -0.5386233  -0.4636866 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 143. State = [[-0.25634038  0.00688091]]. Action = [[-0.9381415  -0.4840585   0.12626565  0.2674135 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(1.8760e-05, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.25634038  0.00688091]]. Action = [[-0.14011836 -0.52655536 -0.06024909  0.3805207 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 145. State = [[-0.25634038  0.00688091]]. Action = [[0.6807766  0.7486429  0.81152606 0.5616677 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.25634038  0.00688091]]. Action = [[-0.7447626  -0.87744325  0.72090805  0.586797  ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(6.2043e-05, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.25634515  0.00682584]]. Action = [[-0.2796557  -0.36624825 -0.6674079   0.16983926]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 148. State = [[-0.25634515  0.00682584]]. Action = [[-0.60048157 -0.5626839   0.45034623  0.78811264]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(7.6398e-06, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.25634038  0.00688091]]. Action = [[-0.08831984 -0.5660467  -0.31635827  0.636631  ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.25634515  0.00682584]]. Action = [[0.34203625 0.9142482  0.7512932  0.48883152]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.25634515  0.00682584]]. Action = [[-0.8995958   0.9391544   0.6196363   0.76882195]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 152. State = [[-0.25634515  0.00682584]]. Action = [[-5.6190288e-01  7.5601971e-01 -4.0471554e-04 -7.9232126e-01]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(1.7411e-05, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.25634515  0.00682584]]. Action = [[0.5067718  0.5584414  0.0472014  0.86153865]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(4.2201e-05, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.25634515  0.00682584]]. Action = [[-0.6953326 -0.5850191 -0.8184671  0.7802131]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(8.4844e-06, grad_fn=<MseLossBackward0>)
Current timestep = 155. State = [[-0.25634515  0.00682584]]. Action = [[ 0.18608248 -0.58958596  0.20224047  0.7949543 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.25634515  0.00682584]]. Action = [[ 0.97891974  0.82148707  0.92567253 -0.99665445]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.25634515  0.00682584]]. Action = [[-0.6929743  -0.25778538  0.40577173  0.6322458 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(1.4877e-05, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.25634515  0.00682584]]. Action = [[ 0.45765984  0.5997329   0.22419262 -0.8478043 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(6.0865e-06, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.25634515  0.00682584]]. Action = [[ 0.8970423   0.24361062  0.01278877 -0.4364674 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.25634515  0.00682584]]. Action = [[-0.10356319  0.24400926 -0.6882887  -0.7722848 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(6.0000e-05, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.25634515  0.00682584]]. Action = [[-0.12961715  0.8236089   0.5173423  -0.75467795]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(6.2518e-05, grad_fn=<MseLossBackward0>)
Current timestep = 162. State = [[-0.25634515  0.00682584]]. Action = [[-0.8226468  -0.2803132  -0.97193444  0.64387584]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(1.4923e-05, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.25634515  0.00682584]]. Action = [[ 0.44455242  0.61199594 -0.6734417   0.11133361]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(1.0720e-05, grad_fn=<MseLossBackward0>)
Current timestep = 164. State = [[-0.25634515  0.00682584]]. Action = [[-0.3570342   0.37341893  0.23227954 -0.6516422 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(2.4730e-05, grad_fn=<MseLossBackward0>)
Current timestep = 165. State = [[-0.25634515  0.00682584]]. Action = [[ 0.31542063  0.8730353  -0.83678997 -0.479275  ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(4.3239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.25634515  0.00682584]]. Action = [[-0.7394724  -0.35620248 -0.7714201  -0.3989873 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(6.8768e-06, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.25634515  0.00682584]]. Action = [[ 0.5816637   0.1697458   0.21212661 -0.03098965]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(8.7649e-06, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.25634515  0.00682584]]. Action = [[-0.45981103 -0.00721943 -0.02185565  0.6485014 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(3.3346e-06, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.25634515  0.00682584]]. Action = [[ 0.15091419  0.8782666  -0.6054318  -0.82486665]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(8.1577e-06, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.25634515  0.00682584]]. Action = [[ 0.895977   -0.43063045 -0.17034793  0.9124665 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(5.7128e-05, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.25634515  0.00682584]]. Action = [[-0.21589565  0.6143793   0.2609266   0.89931154]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(1.1016e-05, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.25634515  0.00682584]]. Action = [[ 0.12439108  0.505136    0.51904464 -0.9451778 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(1.3328e-05, grad_fn=<MseLossBackward0>)
Current timestep = 173. State = [[-0.25634515  0.00682584]]. Action = [[ 0.47306728 -0.81314    -0.4445727   0.68397176]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(1.8183e-05, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.25634515  0.00682584]]. Action = [[ 0.18213487 -0.58418375  0.6465939  -0.9533036 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(1.5531e-05, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.25634515  0.00682584]]. Action = [[-0.91269827 -0.42239416 -0.46847522 -0.90579724]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.25634515  0.00682584]]. Action = [[ 0.507792    0.5891788   0.97822905 -0.6577919 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(3.6902e-05, grad_fn=<MseLossBackward0>)
Current timestep = 177. State = [[-0.25634515  0.00682584]]. Action = [[ 0.71851516 -0.19130337  0.75757885 -0.12220806]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.25634515  0.00682584]]. Action = [[-0.37240446 -0.6736984   0.040012   -0.6234124 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(8.7122e-05, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.25634515  0.00682584]]. Action = [[-0.5461378  -0.1214785  -0.86302036  0.6275587 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(3.4544e-05, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.25634515  0.00682584]]. Action = [[-0.84469193  0.51624966  0.04188454  0.08096766]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(9.0909e-05, grad_fn=<MseLossBackward0>)
Current timestep = 181. State = [[-0.25634515  0.00682584]]. Action = [[-0.1569395  -0.67804885  0.15128696  0.14380956]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(2.9603e-06, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.25634515  0.00682584]]. Action = [[-0.24589801  0.6077981  -0.3392372   0.14170492]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(8.2578e-06, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-0.25634515  0.00682584]]. Action = [[-0.711714   -0.34926224 -0.02595782 -0.6421287 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(5.7057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.25634515  0.00682584]]. Action = [[-0.92140454 -0.05673015  0.81129575  0.7791102 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(8.9002e-06, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.25634515  0.00682584]]. Action = [[ 0.32962704  0.9212897  -0.38512588  0.8455713 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(4.5113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.25634515  0.00682584]]. Action = [[ 0.07323849 -0.5807768   0.18078184  0.43607974]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(2.4441e-05, grad_fn=<MseLossBackward0>)
Current timestep = 187. State = [[-0.25634515  0.00682584]]. Action = [[ 0.5516329  -0.58974636  0.62679195  0.99127436]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(7.1049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.25634515  0.00682584]]. Action = [[ 0.58182764 -0.60123503 -0.43711567 -0.30617023]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(1.0725e-05, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.25634515  0.00682584]]. Action = [[ 0.613189    0.7879691  -0.9216503   0.16405535]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.25634515  0.00682584]]. Action = [[ 0.01957774 -0.5453941   0.13256204  0.9030106 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(8.6969e-05, grad_fn=<MseLossBackward0>)
Current timestep = 191. State = [[-0.25634515  0.00682584]]. Action = [[ 0.97720325 -0.31579864  0.2257154   0.6164887 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(1.5668e-07, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.25634515  0.00682584]]. Action = [[ 0.4309492   0.20189929  0.04348397 -0.9517287 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(8.1663e-06, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.25634515  0.00682584]]. Action = [[ 0.27793384 -0.2681535   0.20319831  0.68344307]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(2.9440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 194. State = [[-0.25634515  0.00682584]]. Action = [[ 0.7129892  0.6813712  0.7899296 -0.731683 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(2.7266e-05, grad_fn=<MseLossBackward0>)
Current timestep = 195. State = [[-0.25634515  0.00682584]]. Action = [[-0.41479135 -0.45853418 -0.781151   -0.92192256]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(9.4265e-06, grad_fn=<MseLossBackward0>)
Current timestep = 196. State = [[-0.25634515  0.00682584]]. Action = [[-0.58043    -0.44045663  0.7557057   0.69508004]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(5.3062e-05, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.25634515  0.00682584]]. Action = [[ 0.9054868  -0.63191915 -0.5868624  -0.06083977]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(7.1971e-05, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.25634515  0.00682584]]. Action = [[-0.1330862   0.8643241  -0.37013376 -0.6743676 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(2.5899e-06, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.25634515  0.00682584]]. Action = [[-0.9713784   0.26508415 -0.8784495  -0.35014093]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(4.9081e-05, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.25634515  0.00682584]]. Action = [[ 0.47297502 -0.7221411   0.30591738  0.42135966]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(2.1949e-05, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.25634515  0.00682584]]. Action = [[ 0.03470874 -0.79981905  0.78379774 -0.49410224]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(1.5898e-05, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.25634515  0.00682584]]. Action = [[-0.40269756 -0.76713026 -0.7440559   0.34527946]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(3.4784e-05, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.25634515  0.00682584]]. Action = [[-0.17184234  0.78650045  0.18363559  0.3131883 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(6.4605e-05, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.25634515  0.00682584]]. Action = [[ 0.7774644   0.84787536 -0.80827534  0.87004375]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(3.9601e-06, grad_fn=<MseLossBackward0>)
Current timestep = 205. State = [[-0.25634515  0.00682584]]. Action = [[-0.20579618 -0.11892682  0.03512645  0.09977925]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(1.0340e-05, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.25634515  0.00682584]]. Action = [[-0.16602802 -0.00115323 -0.01235759 -0.7815848 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 207. State = [[-0.25634515  0.00682584]]. Action = [[ 0.4992032   0.30442846 -0.87030554  0.747396  ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(4.2181e-05, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.25634515  0.00682584]]. Action = [[ 0.17469442  0.62624145 -0.4799431  -0.94566786]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(1.5102e-05, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.25634515  0.00682584]]. Action = [[-0.8341196  -0.15058136 -0.77647626 -0.96940917]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(4.1637e-05, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.25634515  0.00682584]]. Action = [[-0.48269773 -0.31559038 -0.00680602 -0.35367668]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(6.4513e-05, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.25634515  0.00682584]]. Action = [[ 0.34564567 -0.47379363 -0.2695278   0.5165336 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(2.3971e-05, grad_fn=<MseLossBackward0>)
Current timestep = 212. State = [[-0.25634515  0.00682584]]. Action = [[ 0.52061963 -0.92153376 -0.6943219  -0.38588715]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(6.0557e-07, grad_fn=<MseLossBackward0>)
Current timestep = 213. State = [[-0.25634515  0.00682584]]. Action = [[ 0.7551527  -0.19064003  0.72548676 -0.00154001]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.25634515  0.00682584]]. Action = [[ 0.8358638   0.10926437 -0.09356904 -0.36134654]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.25634515  0.00682584]]. Action = [[-0.6584583   0.8960867   0.79424715 -0.80682904]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(1.4040e-05, grad_fn=<MseLossBackward0>)
Current timestep = 216. State = [[-0.25634515  0.00682584]]. Action = [[-0.49393737  0.5617219   0.4672904   0.09379244]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(7.6068e-06, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.25634515  0.00682584]]. Action = [[-0.7565775  -0.7908407  -0.44417727  0.11786568]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(2.7079e-06, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.25634515  0.00682584]]. Action = [[-0.3219006   0.7187387   0.5227635  -0.18860042]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(1.1377e-05, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.25634515  0.00682584]]. Action = [[-0.23198926  0.14035428 -0.42018408 -0.822525  ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 220. State = [[-0.25634515  0.00682584]]. Action = [[ 0.21257949 -0.9153056   0.3831519   0.2091819 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(2.2404e-05, grad_fn=<MseLossBackward0>)
Current timestep = 221. State = [[-0.25634515  0.00682584]]. Action = [[-0.7585977  -0.3399346   0.3714441  -0.73433006]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(1.1300e-05, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.25634515  0.00682584]]. Action = [[-0.42820936 -0.93017155 -0.36876148  0.81668544]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(8.1188e-06, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.25634515  0.00682584]]. Action = [[-0.5863075   0.84784675 -0.01496607 -0.52656007]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(2.6683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.25634515  0.00682584]]. Action = [[-0.17225218  0.44399738 -0.59969693 -0.8781744 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(4.5409e-05, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.25634515  0.00682584]]. Action = [[-0.36534274  0.41384828  0.8768122   0.12712598]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(8.7736e-06, grad_fn=<MseLossBackward0>)
Current timestep = 226. State = [[-0.25634515  0.00682584]]. Action = [[-0.52136594 -0.5189898  -0.11241281  0.04462945]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(1.6178e-05, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.25634515  0.00682584]]. Action = [[ 0.40562963 -0.8630207   0.74055827 -0.24615395]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(2.4942e-05, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.25634515  0.00682584]]. Action = [[-0.04572469 -0.31256723 -0.30522394 -0.87426424]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(4.8815e-05, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.25634515  0.00682584]]. Action = [[-0.784882    0.2736956  -0.9449105   0.22697604]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(6.1625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.25634515  0.00682584]]. Action = [[ 0.72902155 -0.802556   -0.32192302 -0.6371182 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(3.6729e-06, grad_fn=<MseLossBackward0>)
