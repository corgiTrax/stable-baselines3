Current timestep = 0. State = [[-0.21704605  0.00037768]]. Action = [[ 0.05939123 -0.07927082  0.10012287  0.6909311 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0313, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-2.1590613e-01  2.2650458e-05]]. Action = [[-0.23153628  0.16153532 -0.17434996 -0.9090076 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.21687022  0.00299587]]. Action = [[ 0.10802305 -0.17362125  0.23815227 -0.8276546 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.21667324  0.00059585]]. Action = [[-0.15419286 -0.21632348 -0.09049267 -0.94304866]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.21805623 -0.00612251]]. Action = [[-0.22296141 -0.07993466 -0.01627429 -0.9673414 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.22300643 -0.01233884]]. Action = [[-0.15543786  0.22404304 -0.17889625 -0.8187015 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.23113464 -0.0112004 ]]. Action = [[-0.2278362  -0.23545943 -0.01769003 -0.9421377 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.24091543 -0.01648228]]. Action = [[ 0.24833506 -0.15937541  0.10211629 -0.44760424]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.24512847 -0.02452528]]. Action = [[-0.12025011 -0.00859395  0.2287625   0.6862172 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.2477196  -0.02888864]]. Action = [[0.11148423 0.22071403 0.1767213  0.39399183]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.24824788 -0.02685316]]. Action = [[-0.11325008 -0.11508468 -0.12567496 -0.8630421 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.24907951 -0.02836591]]. Action = [[ 0.09126121  0.00968063 -0.18700755 -0.26551354]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.2489846  -0.02875422]]. Action = [[ 0.12546158 -0.05236441  0.24320617 -0.15087706]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.24806195 -0.03013101]]. Action = [[-0.15744665 -0.07693389 -0.11191818 -0.10452712]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.24859306 -0.03312111]]. Action = [[-0.19193779 -0.08904263  0.19588721 -0.14982545]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.25204933 -0.03829446]]. Action = [[-0.22857748 -0.14873181 -0.22797848 -0.09138632]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25949734 -0.04521965]]. Action = [[ 0.02711627  0.04732975 -0.15895736 -0.68828994]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.26715645 -0.0484042 ]]. Action = [[ 0.17114705  0.15192097  0.24379629 -0.94905996]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.2691195  -0.04730139]]. Action = [[ 0.09158096 -0.07660645  0.18295664  0.5605012 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.26816916 -0.04736145]]. Action = [[ 0.19182318 -0.04447299 -0.21271054  0.7879083 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.26449415 -0.04812358]]. Action = [[ 0.19319957 -0.02139559 -0.20098884 -0.00463778]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25867376 -0.04897703]]. Action = [[0.16858223 0.06809673 0.23923153 0.69525266]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25131747 -0.04866206]]. Action = [[-0.10460716 -0.20515494  0.09254193  0.8421359 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.2475625  -0.05342153]]. Action = [[ 0.02117613  0.0491184  -0.0301666  -0.9009138 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.24542049 -0.05521027]]. Action = [[-0.14758524 -0.13917275 -0.01737162  0.38861477]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.24570537 -0.06132875]]. Action = [[-0.1142216  -0.1792503   0.15916878  0.6680392 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.24768697 -0.06940559]]. Action = [[-0.18289714 -0.17433776 -0.03467748 -0.27924943]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.25334078 -0.08027779]]. Action = [[-0.11247011 -0.1345611  -0.17240135  0.68846345]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.25926894 -0.08982606]]. Action = [[-0.1917067   0.08224994  0.09753367  0.96473217]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.2667379  -0.09538908]]. Action = [[-0.01825163 -0.0675936  -0.19889921 -0.17823118]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.27118686 -0.10073461]]. Action = [[-0.1322693   0.02604124  0.20157468  0.4535693 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.27687988 -0.10392723]]. Action = [[ 0.00487339 -0.16433369 -0.10591546 -0.9656291 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.28293574 -0.10891768]]. Action = [[0.1452874  0.20185721 0.20267442 0.667228  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.28377265 -0.10799663]]. Action = [[-0.06212309 -0.16966376 -0.07551676 -0.50248027]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.285073   -0.11081392]]. Action = [[-0.05150424 -0.18638517  0.01222318 -0.16290623]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.28746405 -0.11718791]]. Action = [[0.06993374 0.20583937 0.18321824 0.6388624 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.28752714 -0.11675713]]. Action = [[ 0.18396324 -0.16457506  0.02479553 -0.78266734]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.28452057 -0.11917368]]. Action = [[ 0.2304931   0.16825426 -0.10195526 -0.42245638]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.27987143 -0.11738076]]. Action = [[-0.0413184  -0.21484579 -0.11666909 -0.6839149 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.27675086 -0.12076987]]. Action = [[ 0.13871306 -0.00889286 -0.09801263  0.3093171 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.27269462 -0.12346036]]. Action = [[ 0.09202641 -0.16875009 -0.12253898  0.4879825 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.2670848  -0.13050254]]. Action = [[-0.0435845  -0.14012603  0.2458905  -0.22209889]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.26436388 -0.13847601]]. Action = [[-0.12750413 -0.00676352 -0.09774983 -0.6501611 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.26410288 -0.14409229]]. Action = [[ 0.10246551  0.15511107 -0.15439372 -0.7787015 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 44. State = [[-0.26362047 -0.14331694]]. Action = [[ 0.00121978  0.18413767  0.05291083 -0.8583268 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.26288414 -0.13847119]]. Action = [[-0.1449038   0.05235663 -0.24281079 -0.4814164 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.2632733  -0.13470782]]. Action = [[-0.15189707  0.13893908  0.15240872  0.24881351]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.2662444  -0.12818302]]. Action = [[-0.12820983  0.03519008 -0.07110815  0.69280744]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of -1
Current timestep = 48. State = [[-0.2712745 -0.1234349]]. Action = [[ 0.1571945  -0.13747884 -0.19841973  0.6036024 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.27173114 -0.12372871]]. Action = [[-0.19715407 -0.04392996 -0.24027863 -0.717578  ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.2743982  -0.12544958]]. Action = [[-0.03682894 -0.01607311  0.17358857 -0.6106774 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, True, False, False]
State prediction error at timestep 50 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.27753288 -0.12878713]]. Action = [[-0.23231056 -0.1931913   0.09398031  0.5108299 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, True, False, False]
State prediction error at timestep 51 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.2846732  -0.13530551]]. Action = [[ 0.16574585 -0.21254788 -0.22311313 -0.29464054]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, True, False, False]
State prediction error at timestep 52 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.28761807 -0.14446916]]. Action = [[0.16060585 0.14436308 0.193672   0.79625523]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.28628746 -0.14597544]]. Action = [[ 0.07447824 -0.21424447  0.14809918 -0.02983183]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.28451163 -0.1512787 ]]. Action = [[-0.05872259  0.23765194  0.01303464  0.01983023]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.28423664 -0.14979574]]. Action = [[0.23209381 0.15059549 0.22776344 0.58307755]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.2799175 -0.1449766]]. Action = [[-0.04524222 -0.12244824  0.08837134 -0.9070259 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.27864462 -0.14504136]]. Action = [[ 0.12489772 -0.18590637 -0.16785634  0.9187852 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.2745502  -0.14949614]]. Action = [[ 0.02835578 -0.02565782 -0.02273794  0.9100839 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.27222085 -0.15178853]]. Action = [[0.21506849 0.0756194  0.01278508 0.28869545]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.2671777  -0.15191235]]. Action = [[-0.1615947  -0.11953601  0.08972132 -0.08929765]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.26561564 -0.15443303]]. Action = [[ 0.23142377 -0.14144275 -0.01084398  0.3995545 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.26017287 -0.16033573]]. Action = [[-0.11464366 -0.04850495 -0.1999241   0.87116265]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.25802508 -0.16557288]]. Action = [[-0.03343683 -0.05137567  0.20602211 -0.5410458 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.25817108 -0.16951916]]. Action = [[ 0.15291864  0.01820105 -0.23707795 -0.6217427 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.25496617 -0.17191808]]. Action = [[-0.15986894 -0.07215489 -0.23292094 -0.8097719 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.25549805 -0.17519116]]. Action = [[-0.20191692  0.00618225 -0.21903484 -0.36694795]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.25861436 -0.1782119 ]]. Action = [[-0.11861345  0.2363104   0.11583593  0.8450607 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.26229075 -0.17463648]]. Action = [[ 0.20048517 -0.02727161  0.17573619  0.1630832 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 70. State = [[-0.262004   -0.17321506]]. Action = [[ 0.0487037   0.11124176 -0.13023154 -0.20586753]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.26126826 -0.16881457]]. Action = [[-0.22299188  0.19716567 -0.2375789   0.4728769 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.26271683 -0.16111705]]. Action = [[ 0.19981742 -0.02031836 -0.1265953   0.60992146]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.26263833 -0.1566986 ]]. Action = [[-0.15670677 -0.05417186 -0.06281078  0.05817235]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 74. State = [[-0.263274   -0.15602784]]. Action = [[ 0.16395724 -0.02562603 -0.13010217  0.17721546]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.2629572  -0.15556498]]. Action = [[0.12417775 0.02320728 0.03791201 0.9859762 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.26072836 -0.15566286]]. Action = [[-0.11546499 -0.2376732  -0.04853699  0.54380846]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.2607514  -0.16006148]]. Action = [[ 0.13810945  0.03846684 -0.0971621  -0.05817586]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.25823933 -0.16115601]]. Action = [[ 0.12349141 -0.06007004 -0.08976167  0.30434763]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.25450295 -0.16282184]]. Action = [[-0.192708    0.11926448  0.22334975 -0.37066507]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.25449705 -0.16266821]]. Action = [[ 0.12281543 -0.21639463  0.23066926 -0.81158274]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.25364894 -0.16685009]]. Action = [[ 0.07988825  0.00289211  0.15735668 -0.52962995]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.25147724 -0.16955176]]. Action = [[ 0.09286606  0.09973335  0.23358253 -0.01064211]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.24781613 -0.16921322]]. Action = [[-0.09213667  0.03174421  0.24225774 -0.66830033]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.24728888 -0.16840765]]. Action = [[-0.2137155   0.1654728  -0.04866785 -0.6794358 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.24782401 -0.16513447]]. Action = [[ 0.07363284 -0.05669984  0.16200626 -0.5565734 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.24789031 -0.16420941]]. Action = [[-0.03028843  0.17718968 -0.16969272  0.12502694]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.24802239 -0.15923004]]. Action = [[-0.16332392  0.18611163 -0.22126843  0.1540401 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.24995282 -0.15064955]]. Action = [[ 0.0776805  -0.00408509  0.20694217 -0.39696026]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.25024673 -0.14483368]]. Action = [[ 0.0121251   0.14216048  0.08623061 -0.70373124]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.25038818 -0.13831052]]. Action = [[ 0.18821263 -0.08080238 -0.04610711 -0.94122374]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.24993475 -0.13578281]]. Action = [[-0.18838629 -0.12200776  0.08430165 -0.26255548]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.2501859 -0.1372846]]. Action = [[ 8.6285770e-03 -8.7298453e-04  1.1941925e-01  8.7597013e-01]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.2505416  -0.13812551]]. Action = [[ 0.10740843 -0.143025    0.22039282  0.14542794]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.25043675 -0.14115234]]. Action = [[0.11559838 0.13639665 0.21288142 0.18729019]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.24881849 -0.1404875 ]]. Action = [[-0.1633779  -0.0679764  -0.11731769  0.8058795 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.2491635  -0.14188373]]. Action = [[-0.20807984 -0.19629619 -0.07411952 -0.7456504 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.25268596 -0.14799125]]. Action = [[ 0.10536122  0.09639961 -0.16978289 -0.68054503]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.25295192 -0.14846352]]. Action = [[-0.20318891  0.21954262 -0.21664351 -0.22384226]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.2571146 -0.1437479]]. Action = [[-0.06972799  0.15313     0.16782695 -0.55797964]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.2621184  -0.13737378]]. Action = [[-0.16332904 -0.10113156 -0.02352546 -0.51126504]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.27009952 -0.13616875]]. Action = [[ 0.23947823 -0.16455984 -0.16683394  0.5786116 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.271483   -0.13890554]]. Action = [[-0.09552352  0.03198403  0.14586687 -0.85970527]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of -1
Current timestep = 103. State = [[-0.2723598  -0.14032641]]. Action = [[-0.05576751 -0.11839585 -0.19021471 -0.03072977]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.2740409  -0.14372225]]. Action = [[ 0.24071509  0.05097678 -0.20260073 -0.8157206 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(9.5718e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.2726413 -0.1437632]]. Action = [[ 0.12628168  0.0915148  -0.21801315 -0.8319848 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(7.1346e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.2697212  -0.14172089]]. Action = [[-0.15151924  0.22078246 -0.04293436 -0.09104979]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.2694788  -0.13642056]]. Action = [[ 0.0517123  -0.20100391 -0.22727917  0.17171884]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.26959008 -0.1365891 ]]. Action = [[0.01153323 0.07145143 0.2002219  0.9563954 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 109. State = [[-0.26961216 -0.13609898]]. Action = [[ 0.076345   -0.06068893  0.12431127 -0.84950244]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(3.4629e-05, grad_fn=<MseLossBackward0>)
Current timestep = 110. State = [[-0.26831433 -0.13673064]]. Action = [[-0.13955025 -0.18086231 -0.18902679 -0.13919824]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
State prediction error at timestep 110 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.2685488  -0.14131626]]. Action = [[ 0.19827598  0.0044021   0.17796645 -0.9352803 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.26616287 -0.14299317]]. Action = [[-0.13469386 -0.03163208  0.0760884   0.06652784]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.2652435 -0.1451981]]. Action = [[ 0.2445845  -0.06764406 -0.20993984  0.48125768]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.26173756 -0.14807396]]. Action = [[ 0.14179748 -0.05567014 -0.1019409   0.48492634]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.2560247 -0.1518405]]. Action = [[-0.07747146 -0.09026274 -0.21078427  0.91691494]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
State prediction error at timestep 115 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 116. State = [[-0.25304925 -0.15610068]]. Action = [[ 0.18114567  0.09884787 -0.24004488 -0.15865922]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.24662416 -0.15694132]]. Action = [[-0.12780322  0.15678853  0.06387705  0.09888852]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.24492504 -0.15421243]]. Action = [[-0.01609628  0.08038962  0.171063   -0.19793159]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.24453203 -0.15035684]]. Action = [[ 0.23597622 -0.13945551 -0.10033171 -0.06529278]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.24006635 -0.15080439]]. Action = [[0.03529039 0.19197607 0.13328117 0.8245355 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.23510592 -0.1471909 ]]. Action = [[ 0.19656235 -0.2142505   0.17903379  0.9509548 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.22769646 -0.14949955]]. Action = [[ 0.08823904 -0.05416356 -0.21092771  0.84977794]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.21942829 -0.15219033]]. Action = [[-0.1277521  -0.03532314  0.13736933 -0.5493377 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.21666385 -0.15378115]]. Action = [[ 0.1350508   0.21624404 -0.13399862  0.4924798 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.2124721  -0.15006801]]. Action = [[ 0.20337379  0.21916541 -0.10984436 -0.9338745 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.20555028 -0.14239518]]. Action = [[-0.2422543  -0.00404483  0.11239117  0.17472863]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.20522857 -0.13758811]]. Action = [[ 0.17837358  0.1409893   0.22691396 -0.7759145 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.20265058 -0.13027145]]. Action = [[0.07257348 0.11680782 0.13851085 0.21646547]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.19847631 -0.12166193]]. Action = [[-0.04350281  0.18042865  0.0371629   0.60701394]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.19648127 -0.11234593]]. Action = [[ 0.20146793 -0.1558394  -0.12475967 -0.11903816]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.19133264 -0.1111673 ]]. Action = [[-0.16823788 -0.1230996  -0.00705837 -0.8170956 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.19045623 -0.11276839]]. Action = [[-0.03235963  0.03413624  0.23623025 -0.85479176]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 133. State = [[-0.19025561 -0.11338629]]. Action = [[ 0.0691953  -0.14524479  0.205984   -0.94118184]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.18940309 -0.1175411 ]]. Action = [[ 0.13396257 -0.18950252  0.07279474 -0.69960904]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.18622    -0.12399112]]. Action = [[-0.02899782  0.04398903  0.02772245 -0.6131744 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.18415707 -0.12649034]]. Action = [[0.12715259 0.16629738 0.1757048  0.14891052]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.18122347 -0.12517932]]. Action = [[ 0.03121823 -0.04380876 -0.04385167 -0.57481176]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, True, False, False]
State prediction error at timestep 137 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 138. State = [[-0.17747833 -0.12522228]]. Action = [[ 0.00089714 -0.09481648 -0.09706247  0.7591356 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, True, False, False]
State prediction error at timestep 138 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.17424701 -0.12636435]]. Action = [[-0.1544159   0.1951817  -0.18044001 -0.7629417 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.17484285 -0.12316952]]. Action = [[-0.12525313  0.21139738 -0.10458168 -0.61004335]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.17577867 -0.11629618]]. Action = [[ 0.11500087 -0.04249758  0.1411078  -0.5349385 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 142. State = [[-0.17611648 -0.11348905]]. Action = [[-0.05336307 -0.09589082 -0.12287284 -0.4882601 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.17633197 -0.11425678]]. Action = [[-0.234876   -0.11488318  0.04960522  0.23586583]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.18065506 -0.11821887]]. Action = [[-0.02562942 -0.12584487  0.00274485  0.35222483]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.18403037 -0.12302095]]. Action = [[0.17906499 0.19015494 0.20980814 0.5394416 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.18405543 -0.12198661]]. Action = [[-0.1845706  -0.21720521  0.18961444  0.56635284]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.18623577 -0.12659048]]. Action = [[-0.06153581 -0.08404386 -0.15806274  0.14274442]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, True, False, False]
State prediction error at timestep 147 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.18983011 -0.13284469]]. Action = [[-0.14623402 -0.13424194  0.12716097  0.7765217 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.1958275 -0.1393745]]. Action = [[-0.01123339 -0.13469483 -0.06385866  0.6196902 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.20004283 -0.14700885]]. Action = [[0.09797341 0.22986794 0.19585276 0.46836126]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.20004208 -0.14541443]]. Action = [[-0.22475575  0.23574072  0.16586232  0.75677896]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.2034654 -0.1387364]]. Action = [[-0.13556385  0.19315943  0.01692122 -0.7989049 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.20905985 -0.12856078]]. Action = [[0.13819143 0.14716393 0.0286859  0.8533695 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.2118078 -0.1137775]]. Action = [[-0.17113659 -0.13723108 -0.1993341   0.7683301 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.21656053 -0.11349794]]. Action = [[ 0.05881274 -0.13808782  0.06651688  0.78373814]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.21920568 -0.11600399]]. Action = [[ 0.24579513  0.20954335  0.23387045 -0.9967231 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.21836057 -0.11298902]]. Action = [[-0.1705999  -0.05044653  0.11510849  0.61444116]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(9.9652e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.21835491 -0.11214969]]. Action = [[ 0.12592459  0.15900967  0.07148784 -0.85305065]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.21830535 -0.10743602]]. Action = [[ 0.22823834  0.07504442  0.01943618 -0.4553424 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.21375272 -0.10155761]]. Action = [[-0.01530445  0.07550409 -0.16350625 -0.7807599 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.21129638 -0.09690133]]. Action = [[-0.02193284  0.21073401  0.14101428 -0.7644272 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.21059266 -0.08771164]]. Action = [[-0.20461963 -0.05499724 -0.2421406   0.6237724 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of 1
Current timestep = 163. State = [[-0.21156766 -0.08279943]]. Action = [[ 0.12320018  0.16286266 -0.15967636  0.07998896]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of 1
Current timestep = 164. State = [[-0.21218075 -0.07618054]]. Action = [[-0.08108525  0.10722011  0.07293826 -0.6677036 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.21310994 -0.06771139]]. Action = [[ 0.09179124  0.22207874 -0.20461386 -0.5026967 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.2138595 -0.0573125]]. Action = [[-0.18249719 -0.07440735 -0.18670882 -0.4259194 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.21499838 -0.05241245]]. Action = [[ 0.15661013  0.05842477  0.06776923 -0.06631321]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.21551779 -0.04891249]]. Action = [[-0.10777357  0.01458347  0.00993407  0.6250961 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.21630396 -0.04540342]]. Action = [[ 0.05147943  0.223279   -0.14182755 -0.83503777]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.21729217 -0.03829289]]. Action = [[ 0.22841877 -0.09457892 -0.02772474  0.9048493 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.21309069 -0.03538004]]. Action = [[-0.04259741  0.16335925  0.07962132  0.890687  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.20993043 -0.02963137]]. Action = [[ 0.04551682  0.13765943  0.14118811 -0.94850415]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.20604697 -0.02170615]]. Action = [[ 0.13180268 -0.19845629 -0.09937088  0.660964  ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.20218259 -0.02190995]]. Action = [[ 0.06044513 -0.13686062  0.17085344 -0.9562972 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.19862786 -0.02518342]]. Action = [[-0.2278668  -0.09451532 -0.10617736 -0.9118493 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.19860089 -0.02967377]]. Action = [[ 0.14038515  0.15625519  0.24533671 -0.6780506 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.19810154 -0.02936146]]. Action = [[ 0.18939275 -0.0356534   0.19602364 -0.16002625]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.19394885 -0.0294774 ]]. Action = [[-0.08344543 -0.16215134  0.02356848 -0.64471364]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.19288452 -0.03324204]]. Action = [[-0.13002345 -0.01895547 -0.21319255  0.6017324 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.1933811  -0.03553135]]. Action = [[-0.20999871  0.1377194   0.02282596  0.04325616]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.1955566  -0.03450621]]. Action = [[-0.0272354  -0.16442068  0.0494653   0.10672808]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.19767897 -0.03764492]]. Action = [[-0.05091359  0.15906158 -0.07584609  0.10498869]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.20042916 -0.03649696]]. Action = [[-0.17500018 -0.07911481  0.00356627 -0.6597558 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.20587638 -0.03796979]]. Action = [[-0.23036958 -0.0046128   0.20682454  0.76167274]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.21417099 -0.03829156]]. Action = [[ 0.09506202  0.23216787 -0.08973613  0.8324212 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.21801709 -0.03216074]]. Action = [[ 0.03012708 -0.13975057  0.05322194  0.40448284]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.22006658 -0.03287344]]. Action = [[ 0.14841315 -0.1422191   0.16205925  0.99024403]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.21949552 -0.03667417]]. Action = [[ 0.15535492 -0.1453493  -0.10461804 -0.3345468 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.21755701 -0.04181915]]. Action = [[ 0.1625703   0.20099345 -0.23000269  0.12956595]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.21249269 -0.04102333]]. Action = [[ 0.01460093 -0.13108464  0.03903842  0.89379287]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 191. State = [[-0.20817164 -0.04245263]]. Action = [[ 0.24536836 -0.07201421  0.06189728  0.59075856]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.2000938  -0.04516634]]. Action = [[ 0.11756629  0.05849719  0.01587963 -0.9529801 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.19291638 -0.04549266]]. Action = [[ 0.07922524 -0.06033161  0.05564126  0.66033864]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-0.18486223 -0.04637258]]. Action = [[ 0.18531975  0.17506704  0.19993836 -0.7416046 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.17528586 -0.04365378]]. Action = [[-0.09995945 -0.10956891 -0.19463727 -0.9238138 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.17209317 -0.0442193 ]]. Action = [[-0.14348131 -0.10508469  0.19143656  0.6726315 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.17243914 -0.04768379]]. Action = [[ 0.22955656 -0.15448716 -0.14537129 -0.0913536 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.16999424 -0.05330731]]. Action = [[-0.02729012  0.21834776 -0.09030722 -0.68554246]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.16742866 -0.05199001]]. Action = [[-0.243358    0.07331139 -0.21952139 -0.37269235]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.16832948 -0.0497137 ]]. Action = [[ 0.12641776 -0.17741382  0.07995701  0.39091456]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.1679744  -0.05279029]]. Action = [[ 0.01543951 -0.19751388  0.1980032  -0.51185626]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.16710153 -0.06051262]]. Action = [[-0.09813496 -0.18878038 -0.18538956  0.31396854]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.16685317 -0.06946153]]. Action = [[-0.03817102  0.19973135  0.04963571  0.28157663]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.16717374 -0.06945921]]. Action = [[ 0.199682    0.21420488 -0.20158418  0.85782933]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.16591713 -0.06470329]]. Action = [[-0.04737312 -0.02159163  0.01270172  0.06856596]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.16609102 -0.06293865]]. Action = [[-0.0374292   0.00815934  0.00095192 -0.78768975]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 207. State = [[-0.16635013 -0.06141495]]. Action = [[ 0.1316433   0.08391559 -0.21723454  0.72796965]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.1643053  -0.05761744]]. Action = [[ 0.04961339  0.1618987  -0.11734328 -0.9463532 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.16210067 -0.05159436]]. Action = [[-0.20991284 -0.02875368 -0.19314234 -0.9696276 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.163038   -0.04820233]]. Action = [[-0.12048379 -0.07053444  0.00367063 -0.3757223 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of 1
Current timestep = 211. State = [[-0.1646407  -0.04811979]]. Action = [[ 0.09240139 -0.11106005 -0.06269614  0.48917305]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of 1
Current timestep = 212. State = [[-0.16464289 -0.05110245]]. Action = [[ 0.13610837 -0.22894198 -0.17164825 -0.40767503]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-0.16343036 -0.05850626]]. Action = [[ 0.19347683 -0.03801598  0.18553817 -0.03192848]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.15852031 -0.06362344]]. Action = [[ 0.21269804  0.03732088 -0.0169301  -0.3839503 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.15027045 -0.06533413]]. Action = [[-0.16588014  0.2259863   0.20227647 -0.8129639 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 216. State = [[-0.14848794 -0.0619692 ]]. Action = [[-0.12372041  0.14732414  0.12386423  0.06108177]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.15014829 -0.05629702]]. Action = [[-0.1908098  -0.19376625 -0.10562977  0.08441436]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.15263586 -0.05738092]]. Action = [[-0.07943654  0.18486765  0.13782242 -0.21792907]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.15540482 -0.05314146]]. Action = [[-0.0561571   0.04583344 -0.09879631 -0.82820976]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.15879731 -0.04925223]]. Action = [[ 0.05844492 -0.22701651  0.10424304  0.17317712]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.15998146 -0.05286184]]. Action = [[-0.19142777 -0.07487778  0.10161713 -0.74423546]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.16429745 -0.05747537]]. Action = [[-0.10682841 -0.23100744 -0.08459207  0.7979094 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.17156366 -0.06699026]]. Action = [[-0.1474773   0.21506256  0.00597066 -0.54596096]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.17861116 -0.06649171]]. Action = [[-0.04044305  0.11973476 -0.14441991 -0.8810782 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.1845753  -0.06250443]]. Action = [[-0.09047925  0.11240652  0.2219871   0.08637583]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.18993805 -0.05741955]]. Action = [[-0.1307723  -0.12152639 -0.01854284  0.00401115]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(9.5056e-05, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.19598562 -0.05832564]]. Action = [[ 0.10735393 -0.21289763  0.19019714 -0.27952397]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.19975671 -0.06456713]]. Action = [[-0.0079636  -0.06863114 -0.06806834 -0.87746304]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.20258927 -0.06980202]]. Action = [[-0.19783252  0.07750109 -0.23557091  0.18175435]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(5.1486e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.20754491 -0.07153086]]. Action = [[ 0.18718857 -0.19707611 -0.07293439 -0.6519747 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(6.5730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.20819639 -0.07688012]]. Action = [[ 0.01164085  0.01545131  0.00765169 -0.10686874]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(4.9516e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.20849863 -0.07897048]]. Action = [[-0.22749479  0.14068967  0.13807955 -0.1871568 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(6.7523e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 232 of -1
Current timestep = 233. State = [[-0.21247087 -0.07726434]]. Action = [[-0.06942478  0.20620805  0.22387272  0.5760393 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(3.8502e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.21610214 -0.07140347]]. Action = [[ 0.05106279 -0.13199012  0.11086702 -0.23395419]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(7.4225e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.21717948 -0.07170092]]. Action = [[ 0.04240987 -0.15562762 -0.09327847 -0.78296745]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(1.8917e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.2176905  -0.07600886]]. Action = [[-0.2313054  -0.21493801  0.00202376 -0.2412895 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(7.4210e-06, grad_fn=<MseLossBackward0>)
Current timestep = 237. State = [[-0.22364105 -0.08491464]]. Action = [[-0.18012929  0.16231     0.05982116 -0.3503096 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(3.2685e-05, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.23230655 -0.08544008]]. Action = [[ 0.17486024  0.22315556 -0.19568849 -0.34477675]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.2334542  -0.07995608]]. Action = [[ 0.06264621  0.05680254 -0.01661596 -0.43993688]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(8.3373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.23361266 -0.07522023]]. Action = [[ 0.08281991  0.13305196 -0.14629549  0.96872866]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(8.5978e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.23403135 -0.06860776]]. Action = [[ 0.0313372   0.00093353 -0.0225454   0.19839442]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.23425268 -0.06442679]]. Action = [[ 0.23935708  0.12581936 -0.14312628 -0.10842985]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.22875732 -0.05834062]]. Action = [[-0.02035192  0.12710977 -0.08752179 -0.52311814]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.22508599 -0.05170568]]. Action = [[ 0.15288234  0.08348063  0.14035785 -0.637797  ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.21868919 -0.04456231]]. Action = [[-0.16932626  0.22882056 -0.19518532 -0.9203431 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(8.1016e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.2185178  -0.03395819]]. Action = [[-0.18636504 -0.00275715 -0.03589474 -0.296382  ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.22124846 -0.02648239]]. Action = [[-0.1297734   0.13984615 -0.04790419  0.2595303 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.22492395 -0.01810275]]. Action = [[ 0.02943012 -0.21581589  0.20962515 -0.9657932 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of -1
Current timestep = 249. State = [[-0.22690429 -0.01839817]]. Action = [[-0.15743789  0.21427107 -0.13565587 -0.431072  ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.22997941 -0.01373065]]. Action = [[ 0.12090307 -0.18261942 -0.0703916  -0.43522537]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.23096982 -0.01570123]]. Action = [[-0.24066865 -0.10860932  0.2180891   0.88489413]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.23737383 -0.02033023]]. Action = [[-0.18763453 -0.14396691  0.15498763 -0.49860036]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.24732561 -0.02772684]]. Action = [[-0.19560774 -0.24076714  0.11688167 -0.2428323 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.25815618 -0.0390219 ]]. Action = [[-0.21252774 -0.06702775  0.2165426   0.66496205]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.26996332 -0.0470325 ]]. Action = [[ 0.23868161  0.11399937 -0.1391396   0.6037251 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.2728814  -0.04949239]]. Action = [[-0.2282774  -0.19901997  0.07797408 -0.52754503]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.2792412  -0.05533998]]. Action = [[-0.15024385  0.19913748  0.19220209 -0.65738076]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.2851227 -0.05435  ]]. Action = [[ 0.18698442 -0.14734314 -0.07348092 -0.746554  ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 259. State = [[-0.2873469  -0.05679134]]. Action = [[ 0.04803306  0.03211617 -0.16497886  0.40399218]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.28736895 -0.05737718]]. Action = [[0.00712386 0.21883234 0.07624847 0.2912836 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.2880243  -0.05304677]]. Action = [[0.01139966 0.06176466 0.09685877 0.07163107]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 262. State = [[-0.28870913 -0.04910328]]. Action = [[ 0.19906202 -0.17339143  0.06266147  0.6746006 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.28655267 -0.05026113]]. Action = [[-0.19918661  0.0831812  -0.15664114 -0.00107008]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.28746173 -0.04703234]]. Action = [[-0.24466015 -0.08446619  0.17939746 -0.8193292 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.29265296 -0.04197115]]. Action = [[-0.02023527 -0.11361513  0.16949129  0.94210386]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.29888156 -0.03820714]]. Action = [[-0.16753992  0.16065204  0.12873378  0.719571  ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 267. State = [[-0.30578008 -0.03294795]]. Action = [[ 0.1635766   0.20618734 -0.20156682 -0.8264925 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.30860132 -0.02319176]]. Action = [[0.15127939 0.10722369 0.09974504 0.17704606]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Current timestep = 269. State = [[-0.3084768  -0.01482908]]. Action = [[-0.17368686 -0.01645966 -0.16830777 -0.51900333]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.31097966 -0.00830747]]. Action = [[ 0.20544147 -0.13336678  0.16683209 -0.2486974 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[-0.31098923 -0.00593092]]. Action = [[-0.06317413  0.01030841  0.11126104 -0.03469861]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.31105065 -0.00582976]]. Action = [[-0.06011887 -0.12697479 -0.01772578 -0.96222043]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.31117523 -0.007574  ]]. Action = [[-0.00199097 -0.09692544  0.18463719  0.8263571 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 274. State = [[-0.31143007 -0.01060213]]. Action = [[-0.04361324 -0.20924667  0.23482561 -0.53011745]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.3126053  -0.01579743]]. Action = [[ 0.01127711  0.13707834  0.15135336 -0.6560617 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of -1
Current timestep = 276. State = [[-0.3128629  -0.01653675]]. Action = [[-0.19901319  0.19557655  0.06621581 -0.7298908 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.31633565 -0.01552621]]. Action = [[ 0.14859575  0.09855467  0.1303572  -0.8017898 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-0.31746256 -0.01231722]]. Action = [[ 0.22146893  0.13953817 -0.21899664  0.33237803]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.31445283 -0.00778555]]. Action = [[-0.08052829 -0.1026575  -0.13671303 -0.96682787]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 280. State = [[-0.31470633 -0.00587219]]. Action = [[ 0.13829392 -0.05105032 -0.24714603 -0.34166127]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.31308335 -0.00528938]]. Action = [[-0.09076029  0.13183779 -0.2037645   0.32664657]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.31311956 -0.0044671 ]]. Action = [[ 0.15765977 -0.16401552  0.0095399   0.035725  ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.3096643  -0.00678744]]. Action = [[ 0.04636267  0.02363828 -0.09611091 -0.33072543]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 284. State = [[-0.3063707  -0.00823585]]. Action = [[-0.17663631 -0.23138952  0.13186136  0.64383686]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 285. State = [[-0.30719104 -0.00869348]]. Action = [[-0.14887387  0.19578236 -0.1355431   0.71251655]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.30901292 -0.00698655]]. Action = [[ 0.15565917  0.09293246 -0.08342996 -0.07113051]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.30954924 -0.00546241]]. Action = [[-0.00876543  0.22489434 -0.23174298  0.8449371 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.3098234  -0.00220977]]. Action = [[-0.0068711   0.10024512  0.23972997 -0.8047929 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 289. State = [[-0.31001464  0.00082966]]. Action = [[ 0.08323771 -0.16976856  0.01950607  0.90551496]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.30896595  0.00143269]]. Action = [[-0.05023539 -0.18260962 -0.05549265  0.28140366]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.30893117  0.00140569]]. Action = [[ 0.05959535  0.2470603   0.0996145  -0.30401957]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.30784315  0.00213684]]. Action = [[ 0.03717399  0.22206968 -0.18212014  0.6417093 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Current timestep = 293. State = [[-0.30531034  0.00606811]]. Action = [[ 0.21960202  0.09539801 -0.23351876 -0.00514024]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.29829526  0.00665173]]. Action = [[ 0.07923487  0.05867854  0.02684966 -0.14225614]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.29091856  0.00801966]]. Action = [[-0.22717696  0.20640767 -0.20400053 -0.49735463]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 295 of -1
Current timestep = 296. State = [[-0.29051745  0.01411152]]. Action = [[-0.19747403 -0.0453236  -0.14904697 -0.03113198]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 296 of -1
Current timestep = 297. State = [[-0.29252467  0.01799301]]. Action = [[ 0.16861776  0.01986203 -0.19912587  0.31285465]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.2932617   0.02220368]]. Action = [[-0.0595517   0.13559926 -0.1440996  -0.76090133]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.294452    0.02569288]]. Action = [[ 0.01949927 -0.23034646 -0.1867016  -0.21434045]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.29523453  0.02745221]]. Action = [[-0.12444796  0.14723718  0.02490327 -0.78065526]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.29623172  0.02974137]]. Action = [[ 0.22379857 -0.21906726  0.22586536 -0.86846155]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.29501152  0.02910077]]. Action = [[ 0.15667802 -0.10538331 -0.22694857  0.29806817]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.2913206  0.026712 ]]. Action = [[-0.12780854  0.02424154  0.21595347 -0.8348662 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.29086947  0.02702322]]. Action = [[-0.21915486  0.11974719 -0.02123095  0.53782535]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.2920619   0.02822368]]. Action = [[ 0.0598889  -0.16831069 -0.17450148 -0.5540912 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.2920623   0.02787362]]. Action = [[ 0.24445125  0.23514122 -0.20942655  0.51468754]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 307. State = [[-0.2893784   0.02667299]]. Action = [[ 0.16485947 -0.03202212  0.04276842  0.31542456]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 308. State = [[-0.28510392  0.02653173]]. Action = [[-0.06007239  0.09268558 -0.0868779   0.2393564 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 309. State = [[-0.28277528  0.02773939]]. Action = [[ 0.1475913  -0.03745002  0.20189184 -0.7043002 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of 1
Current timestep = 310. State = [[-0.2787262   0.02923006]]. Action = [[-0.16269049  0.19076884  0.23410445  0.8436091 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of 1
Current timestep = 311. State = [[-0.27806208  0.03090897]]. Action = [[-0.07543159 -0.10259449  0.22667104 -0.36115777]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.27817783  0.03203035]]. Action = [[ 0.21673948  0.0900957  -0.04766014 -0.8603143 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.27496725  0.03220899]]. Action = [[-0.20504226 -0.00738475 -0.2473481  -0.5982642 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.27532092  0.03300477]]. Action = [[-0.18838023  0.1542309  -0.09038307  0.64589393]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.27844006  0.03785104]]. Action = [[ 0.21239132  0.13569355 -0.11297411 -0.04067224]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of 1
Current timestep = 316. State = [[-0.27817163  0.04327202]]. Action = [[ 0.13638651  0.03317764 -0.24235539  0.2631662 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.27424616  0.04753403]]. Action = [[-0.02529879  0.03331286 -0.05925891  0.50240684]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.27158037  0.05142711]]. Action = [[-0.14674409  0.05442339 -0.1835157   0.32770395]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.27317333  0.05539347]]. Action = [[-0.03915964  0.0676502  -0.04658364 -0.5482126 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 320. State = [[-0.2750581   0.05941219]]. Action = [[-0.1573701   0.0043771  -0.04713872  0.22479916]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 321. State = [[-0.2779131  0.0636143]]. Action = [[ 0.18897069  0.22007054 -0.12580015 -0.9653325 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of -1
Current timestep = 322. State = [[-0.27781707  0.07111384]]. Action = [[ 0.21083343 -0.12156788  0.1197367   0.77015233]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 322 of -1
Current timestep = 323. State = [[-0.2747313   0.07180858]]. Action = [[-0.16829069 -0.1095151   0.05823874 -0.7975684 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.27428514  0.07062569]]. Action = [[ 0.19808644 -0.08563504 -0.07990783  0.45607066]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.27194318  0.0677616 ]]. Action = [[ 0.01998806  0.18650323 -0.16137636  0.3514104 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 326. State = [[-0.26905924  0.06974059]]. Action = [[-0.23599735 -0.22342865 -0.17076927 -0.5537456 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.2712909   0.06681137]]. Action = [[-0.12035418  0.04068187  0.17918363  0.9440987 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.2736685   0.06529272]]. Action = [[ 0.11995015 -0.1754412  -0.22190851  0.5513741 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.27265382  0.06083077]]. Action = [[ 0.07629222  0.05502868 -0.0089801  -0.9321501 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.27171835  0.05898477]]. Action = [[-0.21096413 -0.24230152  0.08945131 -0.8507628 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.27501604  0.05392681]]. Action = [[ 0.177423   -0.15317188 -0.22746779 -0.7120816 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.27530068  0.05260401]]. Action = [[-0.06819612 -0.20145221 -0.12589274 -0.40183616]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.27672514  0.04514925]]. Action = [[ 0.09706286 -0.05044861  0.04656747 -0.25129664]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.2770622   0.04057985]]. Action = [[-0.05934243 -0.13118231 -0.18044603  0.8059411 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 334 of 1
Current timestep = 335. State = [[-0.27870086  0.036617  ]]. Action = [[ 0.0948613  -0.19474594  0.1961487   0.71668506]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.2789101  0.0306475]]. Action = [[ 0.21322095 -0.07809484 -0.11578622  0.448367  ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.27636626  0.02583567]]. Action = [[-0.09742945  0.19426116  0.2078825  -0.25233388]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.2753494   0.02421889]]. Action = [[-0.23348187 -0.20099008  0.11157784  0.3314042 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.2776858   0.01863132]]. Action = [[-0.06936003  0.04314664 -0.10285227 -0.9540983 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 340. State = [[-0.28042528  0.017692  ]]. Action = [[-0.15010244  0.18220228 -0.02516639 -0.4777701 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of 1
Current timestep = 341. State = [[-0.28510445  0.01684992]]. Action = [[ 0.11972979 -0.04739498 -0.09410992  0.8215275 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[-0.2860668   0.01851952]]. Action = [[ 0.12162495 -0.0465548   0.0353412  -0.51404464]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 1
Current timestep = 343. State = [[-0.2851525   0.01597312]]. Action = [[-0.09274501 -0.00259092  0.19656664 -0.833316  ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.28575864  0.01708344]]. Action = [[-0.09760916  0.22031358 -0.13666645 -0.26541317]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 345. State = [[-0.28719074  0.01928555]]. Action = [[ 0.21671999  0.06604239 -0.1545438   0.15867531]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 346. State = [[-0.28530177  0.02068664]]. Action = [[-0.05231977  0.17316243 -0.02563193 -0.58851683]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 346 of 1
Current timestep = 347. State = [[-0.28418967  0.02261745]]. Action = [[-0.05990507 -0.00485119 -0.09137693  0.1243546 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of 1
Current timestep = 348. State = [[-0.28433216  0.02410588]]. Action = [[ 0.11530539  0.07114795 -0.02727455 -0.8982332 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 348 of 1
Current timestep = 349. State = [[-0.28246793  0.02612768]]. Action = [[ 0.23359436  0.06874907 -0.22781624  0.33383858]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 350. State = [[-0.27579337  0.02824017]]. Action = [[0.18002197 0.17379856 0.01357028 0.84752035]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.265941    0.03507917]]. Action = [[ 0.13501081  0.24616429  0.03788784 -0.45053256]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of 1
Current timestep = 352. State = [[-0.25529402  0.04574865]]. Action = [[ 0.01647708  0.09812063 -0.23802856  0.0595696 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of 1
Current timestep = 353. State = [[-0.2480797   0.05472524]]. Action = [[-0.08143638 -0.01435792  0.09412584  0.9041078 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(6.9218e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.2451558  0.0592448]]. Action = [[-0.19340836  0.06085032  0.14969784 -0.2385462 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(2.3674e-05, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.2473297   0.06317367]]. Action = [[-0.22411263 -0.1410563   0.04110765  0.93288255]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.25187707  0.06302951]]. Action = [[ 0.22101292  0.21851996  0.04438189 -0.02820235]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of 1
Current timestep = 357. State = [[-0.25151974  0.06745726]]. Action = [[0.1936028  0.04372796 0.18486497 0.80679834]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(7.6784e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.24761815  0.07105101]]. Action = [[-0.01033078 -0.13953522 -0.00583832 -0.3507961 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(9.8458e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.24673846  0.07122199]]. Action = [[-0.00326413  0.12220562  0.15877905 -0.20914227]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(2.1821e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 1
Current timestep = 360. State = [[-0.24571463  0.07324024]]. Action = [[-0.16095667  0.05895153 -0.12122267  0.23412478]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of 1
Current timestep = 361. State = [[-0.24713632  0.07615622]]. Action = [[ 0.01870808 -0.16356125  0.05811161 -0.9094581 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.24745403  0.07441322]]. Action = [[ 0.01328623 -0.2412713  -0.06338686 -0.47300303]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 363. State = [[-0.24778111  0.06711406]]. Action = [[-0.03644598 -0.100759   -0.04748565  0.4601376 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(3.1990e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of 1
Current timestep = 364. State = [[-0.24684657  0.05989905]]. Action = [[ 0.02460206 -0.05659556 -0.08852513  0.7433218 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(7.9444e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 1
Current timestep = 365. State = [[-0.24545184  0.05550794]]. Action = [[ 0.02741408  0.04126525  0.12699366 -0.5096064 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.24445902  0.0529432 ]]. Action = [[-0.02416602  0.04867467 -0.01853564  0.7039263 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(2.2532e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of 1
Current timestep = 367. State = [[-0.24496041  0.05373014]]. Action = [[-0.22652546  0.2310543  -0.17495243  0.25698042]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(4.2878e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of 1
Current timestep = 368. State = [[-0.24839011  0.05932   ]]. Action = [[ 0.21564078 -0.17160669  0.16000229 -0.29450184]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(2.4124e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.24749152  0.05807357]]. Action = [[-0.15473074  0.0820449   0.06590089  0.8238528 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.248163    0.05958628]]. Action = [[ 0.07438177  0.09994444 -0.18136923  0.00410414]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(2.7863e-06, grad_fn=<MseLossBackward0>)
Current timestep = 371. State = [[-0.24902026  0.06212388]]. Action = [[ 0.24313283  0.14510792  0.09890127 -0.7749953 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of 1
Current timestep = 372. State = [[-0.24566789  0.06787697]]. Action = [[ 0.1629315   0.19803646 -0.10820755  0.13847733]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(6.0618e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.24034344  0.07641988]]. Action = [[-0.08785802 -0.2027655   0.24718454 -0.27524674]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of 1
Current timestep = 374. State = [[-0.23936833  0.07688824]]. Action = [[-0.20111437 -0.06042576 -0.20218965 -0.79787374]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.2403596   0.07628413]]. Action = [[ 0.05789673  0.21226084 -0.05299346  0.0115093 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.24170098  0.07985085]]. Action = [[ 0.0012469  -0.15856786  0.06538332  0.44767833]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(8.7013e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.24163036  0.07961656]]. Action = [[-0.07608195  0.13496453 -0.08004192 -0.6677879 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 378. State = [[-0.24296522  0.08134978]]. Action = [[-0.12928733 -0.2341318  -0.1770823  -0.7282278 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.24652529  0.07672882]]. Action = [[-0.03885469 -0.20948265  0.08658683  0.6827569 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of 1
Current timestep = 380. State = [[-0.2503971   0.06927601]]. Action = [[0.18344784 0.14867657 0.20874262 0.46893692]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of 1
Current timestep = 381. State = [[-0.24902135  0.06900811]]. Action = [[0.21496654 0.20271677 0.24288082 0.58341193]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(2.8299e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.2445072   0.07332636]]. Action = [[-0.17741732 -0.08306026  0.10881329  0.71587706]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 383. State = [[-0.24533163  0.07446298]]. Action = [[ 0.08445331  0.05408567  0.2068826  -0.38394094]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.24517693  0.07528474]]. Action = [[-0.12288144  0.0185858  -0.14931491 -0.06903875]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.24586305  0.07622191]]. Action = [[-0.20934053 -0.08745524 -0.0205121   0.8635838 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.24965993  0.07553092]]. Action = [[ 0.20726532  0.16686144 -0.16825351  0.07567477]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(6.8803e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.24923678  0.07783756]]. Action = [[-0.09599608 -0.03585161  0.23278886 -0.84617466]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.24992707  0.07909296]]. Action = [[-0.14372195  0.01106507  0.21716392  0.93322885]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.2525095   0.08125977]]. Action = [[ 0.20985293  0.09258533 -0.00531645  0.03262341]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(3.9062e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.25206137  0.08359692]]. Action = [[-0.02372797 -0.03905977 -0.17214899 -0.70888937]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.25211203  0.0842879 ]]. Action = [[ 0.08206439  0.08297288 -0.24328294 -0.82449764]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 392. State = [[-0.2508742   0.08583889]]. Action = [[-0.12307772 -0.13818383  0.12175024  0.35949492]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.25147474  0.08494934]]. Action = [[ 0.06158036 -0.10121277  0.04268587 -0.9099485 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.2509813   0.08192635]]. Action = [[-0.17975709 -0.07995944  0.19587478 -0.849404  ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.253736    0.07862183]]. Action = [[-0.09987916 -0.00325198 -0.12481618 -0.9080295 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.2561289   0.07606035]]. Action = [[-0.06929305 -0.0676792  -0.05261064 -0.8417064 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.25830618  0.07278153]]. Action = [[-0.19351809 -0.22148097 -0.2162833   0.0031538 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(6.6505e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.26780418  0.06264621]]. Action = [[ 0.10926557  0.19167882 -0.16255319  0.50912285]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(4.0505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.27033645  0.06229091]]. Action = [[ 0.02467757 -0.16690741 -0.1297009   0.10318601]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(3.4330e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.27214065  0.05921711]]. Action = [[-0.21303657  0.05277434  0.09563386 -0.3952173 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of 1
Current timestep = 401. State = [[-0.27638805  0.06218601]]. Action = [[ 0.17332238 -0.1798178   0.18727708 -0.71587384]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.27761984  0.06448048]]. Action = [[ 0.08119217  0.24470544  0.18792593 -0.31099427]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.27641764  0.06519923]]. Action = [[ 0.07022232  0.22922868  0.14794019 -0.49283695]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.27465522  0.06771969]]. Action = [[-0.16251232 -0.01111519 -0.20181835  0.9705031 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.27596146  0.07007366]]. Action = [[ 0.12578988  0.11171252  0.24103215 -0.7024085 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.27590916  0.07388771]]. Action = [[-0.08788976 -0.0199827   0.21034175  0.36011958]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(4.4283e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.2770303   0.07592769]]. Action = [[-0.15588616 -0.03629237 -0.1717964  -0.32020205]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.2800544   0.07770522]]. Action = [[-0.22165577  0.15517807 -0.0621445   0.84669185]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.2870794   0.08349793]]. Action = [[-0.21983045  0.08500087 -0.12796159  0.26051795]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.296109    0.09163893]]. Action = [[ 0.07025686  0.16205537 -0.08824572  0.04066503]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.30251572  0.10260443]]. Action = [[ 0.18663749  0.05906835 -0.10671335  0.27394056]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 412. State = [[-0.30365434  0.10986532]]. Action = [[ 0.24486938 -0.05871919  0.14726532 -0.5409866 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 413. State = [[-0.29923365  0.11098193]]. Action = [[ 0.21646541 -0.19894105  0.08029833 -0.81204486]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.29303136  0.10710468]]. Action = [[-0.22397633 -0.03328988 -0.17694558 -0.10270882]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(4.4140e-05, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.29286122  0.10592973]]. Action = [[-0.16334412  0.19694579 -0.24116679 -0.655995  ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.29454485  0.10801002]]. Action = [[ 0.15045887 -0.22879837 -0.06734496 -0.82695794]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(9.0217e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.29453427  0.10790788]]. Action = [[ 0.1710285   0.15347499  0.02550268 -0.9214495 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.29159358  0.1081681 ]]. Action = [[-0.10355783 -0.2316039  -0.14135875 -0.79023725]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.29112193  0.10624897]]. Action = [[ 0.19748515  0.14746827 -0.01517898 -0.4689319 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.2875135   0.10462077]]. Action = [[-0.10145193 -0.08319736 -0.04215704 -0.5224224 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 421. State = [[-0.2857149   0.10129025]]. Action = [[-0.03563184 -0.23392364  0.14511609 -0.73071957]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.28566432  0.10057332]]. Action = [[ 0.2060943  -0.05539595  0.06796542  0.62843204]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(2.4734e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.28380096  0.09951255]]. Action = [[ 0.02731183 -0.21063653 -0.1913348  -0.2808491 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(1.3445e-05, grad_fn=<MseLossBackward0>)
Current timestep = 424. State = [[-0.28234062  0.094243  ]]. Action = [[ 0.2276094   0.06337386  0.03369844 -0.5083219 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.27625683  0.08831322]]. Action = [[-0.16850358 -0.01799338 -0.04429434 -0.8368929 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of -1
Current timestep = 426. State = [[-0.2738532   0.08435071]]. Action = [[-0.17246008  0.18232074 -0.19302464  0.8035922 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(9.9403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 427. State = [[-0.2727175   0.08063839]]. Action = [[-0.23480055 -0.03215906  0.1704064   0.03047061]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.2773735   0.08142519]]. Action = [[0.09029207 0.00759834 0.07948068 0.35781503]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(1.7696e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.27888668  0.08225178]]. Action = [[ 0.24204457  0.09740609 -0.12247521  0.05721939]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(1.0077e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.2757922   0.08183289]]. Action = [[ 0.10983774 -0.07097125  0.1473636   0.1903162 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(3.1222e-05, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.27204195  0.07946756]]. Action = [[-0.12856805 -0.1367629   0.02626184 -0.6956175 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 432. State = [[-0.2712784   0.07633534]]. Action = [[ 0.05939895  0.21063042  0.22149196 -0.01418501]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(6.3421e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.26971748  0.07799666]]. Action = [[ 0.21276873  0.23281413 -0.1423637  -0.23498851]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.263788    0.08247087]]. Action = [[ 0.12199053  0.16153485 -0.01832968 -0.16990048]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 435. State = [[-0.25564322  0.09134156]]. Action = [[-0.20482971 -0.05609499 -0.183176    0.7284293 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(6.4204e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of 1
Current timestep = 436. State = [[-0.25574014  0.09471564]]. Action = [[0.17953822 0.07393229 0.18936127 0.2811488 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(5.2199e-05, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.25200877  0.09887091]]. Action = [[ 0.22157896  0.14166307 -0.22790563  0.15668535]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(9.2156e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.24378805  0.10635509]]. Action = [[-0.13880467  0.23585069 -0.01371542 -0.906131  ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of 1
Current timestep = 439. State = [[-0.24028261  0.11602288]]. Action = [[-0.20746483  0.19500333 -0.0267562  -0.00833714]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 439 of 1
Current timestep = 440. State = [[-0.24270348  0.12780583]]. Action = [[-0.16870688  0.07067227 -0.06276472  0.2988944 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(1.9877e-05, grad_fn=<MseLossBackward0>)
Current timestep = 441. State = [[-0.24893896  0.1378635 ]]. Action = [[ 0.08570457  0.22468543  0.19763094 -0.3229643 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, False, True]
State prediction error at timestep 441 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.2529994   0.14992993]]. Action = [[-0.1597864   0.10502237 -0.09893686 -0.04004645]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[-0.25868896  0.16007544]]. Action = [[-0.04055819 -0.04031122 -0.00617352  0.38197517]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 444. State = [[-0.26228976  0.16503632]]. Action = [[ 0.07541913  0.03698552 -0.0925172  -0.3230474 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, False, True]
State prediction error at timestep 444 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.26411664  0.16903444]]. Action = [[-0.18283084  0.21053159  0.11867574  0.89023054]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.26877958  0.17692818]]. Action = [[ 0.05775174 -0.12961853 -0.02994056 -0.46496034]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.26965287  0.1786151 ]]. Action = [[0.06218609 0.11703488 0.15423295 0.66423583]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, False, True]
State prediction error at timestep 447 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 448. State = [[-0.26932487  0.18267536]]. Action = [[ 0.14666864  0.20361394  0.08724001 -0.16306883]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, False, True]
State prediction error at timestep 448 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.26580802  0.19104615]]. Action = [[0.19126534 0.07813925 0.07433131 0.6577599 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.2600222   0.19790438]]. Action = [[-0.07342136 -0.08111618 -0.20681432 -0.9193421 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 451. State = [[-0.25788084  0.20103703]]. Action = [[ 0.0832729  -0.07286467 -0.07145835 -0.29058444]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, False, True]
State prediction error at timestep 451 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.25550792  0.20046082]]. Action = [[ 0.15553784  0.07186711 -0.1950328  -0.27922356]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.2504465   0.20295636]]. Action = [[-0.22487849  0.19916219 -0.0379785   0.34319162]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, False, True]
State prediction error at timestep 453 is tensor(9.9681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.250454    0.20888613]]. Action = [[-0.04039247 -0.22945541 -0.15776731  0.50460005]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[-0.2505953   0.20765717]]. Action = [[ 0.14845237  0.0501526  -0.08813035  0.5082886 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, False, True]
State prediction error at timestep 455 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.24939361  0.20732634]]. Action = [[-0.02937466  0.0626983   0.06385091 -0.27230805]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, False, True]
State prediction error at timestep 456 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 457. State = [[-0.24913253  0.2082386 ]]. Action = [[ 0.0986194   0.13134527 -0.18600316  0.81011856]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, False, True]
State prediction error at timestep 457 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 458. State = [[-0.24685694  0.21270902]]. Action = [[ 0.00276724  0.03315794 -0.13569885  0.6968479 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, False, True]
State prediction error at timestep 458 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of 1
Current timestep = 459. State = [[-0.24442153  0.21710396]]. Action = [[ 0.1997546   0.21317053 -0.1585042  -0.1650868 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, False, True]
State prediction error at timestep 459 is tensor(8.6734e-05, grad_fn=<MseLossBackward0>)
Current timestep = 460. State = [[-0.2392399   0.22479427]]. Action = [[-0.13716543 -0.17059538  0.03243834  0.39628267]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.23761101  0.22561514]]. Action = [[-0.22144264 -0.0881373   0.01541358 -0.64205647]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, False, True]
State prediction error at timestep 461 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.23944436  0.2247536 ]]. Action = [[-0.1432341  -0.23480482 -0.10209516  0.5386859 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, False, True]
State prediction error at timestep 462 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.24254106  0.2188905 ]]. Action = [[-0.09803407  0.10329175  0.1940082   0.25682855]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, False, True]
State prediction error at timestep 463 is tensor(1.8917e-05, grad_fn=<MseLossBackward0>)
Current timestep = 464. State = [[-0.24743843  0.21735154]]. Action = [[ 0.19131124  0.11129802 -0.21429347 -0.4043914 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, False, True]
State prediction error at timestep 464 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.24769181  0.21869645]]. Action = [[ 0.2371018  -0.02769925  0.19446102 -0.31453216]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 0
Current timestep = 466. State = [[-0.24429214  0.21817325]]. Action = [[ 0.24529976 -0.06689689 -0.08400914  0.8189343 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of 0
Current timestep = 467. State = [[-0.2373001   0.21610323]]. Action = [[-0.07765546  0.108089   -0.04022428 -0.11503029]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, False, True]
State prediction error at timestep 467 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 468. State = [[-0.23181593  0.21765547]]. Action = [[-0.01284775 -0.15452065  0.04168212 -0.8664316 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, False, True]
State prediction error at timestep 468 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.22971149  0.21512069]]. Action = [[ 0.14032054  0.12067005  0.21581912 -0.03275818]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of 0
Current timestep = 470. State = [[-0.2263737   0.21601637]]. Action = [[ 0.18541369  0.12349284 -0.01637079 -0.3983395 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of 0
Current timestep = 471. State = [[-0.2202952  0.2194549]]. Action = [[ 0.01673046 -0.14549765 -0.12355527 -0.34941465]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, False, True]
State prediction error at timestep 471 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 471 of 0
Current timestep = 472. State = [[-0.21381225  0.21820268]]. Action = [[ 0.1711154   0.03042951 -0.17886645  0.9262183 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, False, True]
State prediction error at timestep 472 is tensor(7.3441e-05, grad_fn=<MseLossBackward0>)
Current timestep = 473. State = [[-0.20577765  0.2185398 ]]. Action = [[-0.19072397 -0.01442008 -0.06282759 -0.72295356]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, False, True]
State prediction error at timestep 473 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 0
Current timestep = 474. State = [[-0.20354237  0.21827947]]. Action = [[ 0.19664311 -0.02018073 -0.05935664  0.48264682]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, False, True]
State prediction error at timestep 474 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of 0
Current timestep = 475. State = [[-0.19908245  0.21726544]]. Action = [[ 0.22441041  0.22514856 -0.00732903  0.90955365]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, False, True]
State prediction error at timestep 475 is tensor(2.3065e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of 0
Current timestep = 476. State = [[-0.18692847  0.22597255]]. Action = [[-0.03838143 -0.19963302 -0.22860777 -0.36939502]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of 0
Current timestep = 477. State = [[-0.18155415  0.2232158 ]]. Action = [[-0.00687204 -0.04315096 -0.07920474 -0.59156424]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of 0
Current timestep = 478. State = [[-0.17895801  0.21999535]]. Action = [[ 0.05414671 -0.07169285 -0.14494945 -0.27003455]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, False, True]
State prediction error at timestep 478 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of 0
Current timestep = 479. State = [[-0.17633252  0.21593542]]. Action = [[0.00657973 0.00895184 0.10861373 0.17142045]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, False, True]
State prediction error at timestep 479 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 480. State = [[-0.17513184  0.21452275]]. Action = [[ 0.00908494  0.11898276 -0.11532494  0.47693658]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, False, True]
State prediction error at timestep 480 is tensor(9.2643e-05, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.17421523  0.21655856]]. Action = [[-0.15946446  0.08650413 -0.08051795  0.93148994]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(3.7346e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 0
Current timestep = 482. State = [[-0.17638059  0.21999924]]. Action = [[ 0.20531958  0.10700575 -0.06804273 -0.5946688 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, False, True]
State prediction error at timestep 482 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of 0
Current timestep = 483. State = [[-0.17384318  0.22536023]]. Action = [[ 0.23432902  0.23004079 -0.2302934  -0.88598   ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, False, True]
State prediction error at timestep 483 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 484. State = [[-0.1671321   0.23521507]]. Action = [[-0.17102231 -0.088862   -0.20210752  0.0975194 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, False, True]
State prediction error at timestep 484 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of 0
Current timestep = 485. State = [[-0.16489738  0.23975332]]. Action = [[-0.03961341 -0.00477318 -0.15368432  0.94950795]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, False, True]
State prediction error at timestep 485 is tensor(2.1642e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of 0
Current timestep = 486. State = [[-0.16537035  0.2412862 ]]. Action = [[ 0.10069194 -0.05652714  0.11204794  0.42629266]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, False, True]
State prediction error at timestep 486 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 487. State = [[-0.16387257  0.23998262]]. Action = [[-0.21546344 -0.16509417  0.00447428  0.9532378 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, False, True]
State prediction error at timestep 487 is tensor(9.5987e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of 0
Current timestep = 488. State = [[-0.16457933  0.23628365]]. Action = [[ 0.21711612 -0.12089558  0.02163738  0.6613002 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, False, True]
State prediction error at timestep 488 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 0
Current timestep = 489. State = [[-0.16097374  0.2299428 ]]. Action = [[ 0.02786365 -0.12383944  0.11691064  0.5598403 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, False, True]
State prediction error at timestep 489 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 490. State = [[-0.15680821  0.22393543]]. Action = [[ 0.06788695  0.17380911 -0.14693631 -0.6721014 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, False, True]
State prediction error at timestep 490 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.15477993  0.22391488]]. Action = [[ 0.19247323  0.1750224  -0.08061829  0.8590214 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.15067086  0.22785504]]. Action = [[-0.16797727 -0.20857555 -0.23084186  0.4847598 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, False, True]
State prediction error at timestep 492 is tensor(5.4522e-05, grad_fn=<MseLossBackward0>)
Current timestep = 493. State = [[-0.14907575  0.22571109]]. Action = [[-0.20114268 -0.12678334  0.06964025  0.5292127 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of 0
Current timestep = 494. State = [[-0.14985864  0.22217143]]. Action = [[ 0.05219102 -0.09513941  0.05483305  0.20381904]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, False, True]
State prediction error at timestep 494 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of 0
Current timestep = 495. State = [[-0.14950965  0.21726292]]. Action = [[ 0.13555917  0.18035704  0.12137198 -0.9601559 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, False, True]
State prediction error at timestep 495 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.14918618  0.21741238]]. Action = [[-0.14297785 -0.03103378 -0.16140038  0.02994287]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.14972015  0.21776904]]. Action = [[-0.17901231 -0.15945211  0.20848072 -0.6286387 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 0
Current timestep = 498. State = [[-0.1529491   0.21390498]]. Action = [[0.1211473  0.19008902 0.01158646 0.2631035 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, False, True]
State prediction error at timestep 498 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of 0
Current timestep = 499. State = [[-0.15426707  0.2163214 ]]. Action = [[-0.03308886  0.22139871 -0.17297532  0.6345792 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(2.2582e-05, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.15688995  0.22298501]]. Action = [[-0.22901085 -0.15209457 -0.02719143 -0.04602009]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, False, True]
State prediction error at timestep 500 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of 0
Current timestep = 501. State = [[-0.16163786  0.2245729 ]]. Action = [[-0.22535051  0.1676665   0.24508104  0.25813293]]. Reward = [0.]
Curr episode timestep = 501
Human Feedback received at timestep 501 of 0
Current timestep = 502. State = [[-0.16916934  0.2306188 ]]. Action = [[-0.08451152 -0.12356041 -0.02042347  0.95934105]]. Reward = [0.]
Curr episode timestep = 502
Current timestep = 503. State = [[-0.17554554  0.23164484]]. Action = [[ 0.02082714  0.11611509 -0.09146884 -0.08010429]]. Reward = [0.]
Curr episode timestep = 503
Human Feedback received at timestep 503 of 0
Current timestep = 504. State = [[-0.18079107  0.23425746]]. Action = [[ 0.19233018  0.01452976 -0.06606185  0.88878226]]. Reward = [0.]
Curr episode timestep = 504
Current timestep = 505. State = [[-0.18145107  0.23533256]]. Action = [[-0.04381537  0.16991347  0.19443712  0.17443883]]. Reward = [0.]
Curr episode timestep = 505
Current timestep = 506. State = [[-0.18350334  0.24035709]]. Action = [[-0.19245075  0.19739398  0.22138456  0.81277955]]. Reward = [0.]
Curr episode timestep = 506
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.18805929  0.24910975]]. Action = [[ 0.21062547  0.209809   -0.18581663 -0.01288301]]. Reward = [0.]
Curr episode timestep = 507
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.18870492  0.25952527]]. Action = [[ 0.2360464   0.09295058 -0.10118076  0.720531  ]]. Reward = [0.]
Curr episode timestep = 508
Current timestep = 509. State = [[-0.1846523   0.26745147]]. Action = [[0.16595346 0.03083953 0.11288637 0.01257944]]. Reward = [0.]
Curr episode timestep = 509
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.17807598  0.2748435 ]]. Action = [[0.17914444 0.20127201 0.19328919 0.16612613]]. Reward = [0.]
Curr episode timestep = 510
Current timestep = 511. State = [[-0.16979171  0.2844472 ]]. Action = [[ 0.24640453 -0.22118917 -0.14540595  0.1504662 ]]. Reward = [0.]
Curr episode timestep = 511
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.1590416  0.2837175]]. Action = [[-0.21979028 -0.19678806 -0.20372571 -0.3674693 ]]. Reward = [0.]
Curr episode timestep = 512
Current timestep = 513. State = [[-0.15289067  0.27947277]]. Action = [[ 0.17261362 -0.09772561  0.08850992 -0.14105111]]. Reward = [0.]
Curr episode timestep = 513
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.14698465  0.2738815 ]]. Action = [[ 0.02078658  0.12348884  0.10268471 -0.08730507]]. Reward = [0.]
Curr episode timestep = 514
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.14289097  0.27311942]]. Action = [[-0.23524009 -0.16856867 -0.17265584 -0.3491782 ]]. Reward = [0.]
Curr episode timestep = 515
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[-0.1429505   0.27059722]]. Action = [[-0.16393062  0.04623669  0.24049217  0.03582549]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, False, True]
State prediction error at timestep 516 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 517. State = [[-0.14540023  0.26947662]]. Action = [[-0.2086439  -0.2440674  -0.12566815  0.6309619 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(1.3986e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.14970824  0.26245293]]. Action = [[-0.16920671 -0.21493599  0.01768684 -0.02043599]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, False, True]
State prediction error at timestep 518 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.15676709  0.25204483]]. Action = [[ 0.2199375   0.14718968 -0.20445444 -0.21640795]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, False, True]
State prediction error at timestep 519 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of -1
Current timestep = 520. State = [[-0.15915486  0.24975179]]. Action = [[ 0.20754713  0.10015905  0.01978087 -0.36203682]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, False, True]
State prediction error at timestep 520 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of -1
Current timestep = 521. State = [[-0.1575862   0.25005576]]. Action = [[ 0.14416555  0.04300126 -0.00370857  0.17687166]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, False, True]
State prediction error at timestep 521 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 522. State = [[-0.15455297  0.25064358]]. Action = [[-0.15801452 -0.14984483  0.10749134  0.52046895]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, False, True]
State prediction error at timestep 522 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.15393284  0.24798518]]. Action = [[ 0.04788202 -0.18065971 -0.04838383 -0.5092477 ]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, False, True]
State prediction error at timestep 523 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.15173213  0.2419959 ]]. Action = [[0.17035645 0.13566768 0.04434296 0.8801017 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, False, True]
State prediction error at timestep 524 is tensor(6.1813e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.14864525  0.24003424]]. Action = [[-0.15232015 -0.23521005  0.206806    0.2480675 ]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, False, True]
State prediction error at timestep 525 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[-0.14738296  0.23430258]]. Action = [[ 0.20960122 -0.14476378  0.21223766 -0.5471524 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, False, True]
State prediction error at timestep 526 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 527. State = [[-0.14247513  0.22647583]]. Action = [[ 0.2303999   0.17113793  0.15267974 -0.1769895 ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, False, True]
State prediction error at timestep 527 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.13492136  0.22453842]]. Action = [[ 0.19613957 -0.0833092  -0.24660441 -0.9400633 ]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, False, True]
State prediction error at timestep 528 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.12580572  0.2209392 ]]. Action = [[-0.01330817 -0.18091188  0.17819789  0.98905146]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, False, True]
State prediction error at timestep 529 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.11834537  0.2139409 ]]. Action = [[-0.18691587  0.01968899  0.09563094  0.5169902 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, False, True]
State prediction error at timestep 530 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.11772677  0.21153274]]. Action = [[-0.14570752 -0.0428139   0.03358999  0.8280504 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, False, True]
State prediction error at timestep 531 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.11922251  0.20952801]]. Action = [[-0.09055212 -0.20928359  0.0526481   0.68880975]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, False, True]
State prediction error at timestep 532 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.12159485  0.20255984]]. Action = [[-0.0667284   0.11325741 -0.06721137 -0.77516073]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, False, True]
State prediction error at timestep 533 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.12448535  0.2008767 ]]. Action = [[ 0.19517684 -0.23091008 -0.23589751  0.7090585 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, False, True]
State prediction error at timestep 534 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.12282131  0.19388197]]. Action = [[ 0.10259712 -0.08904181  0.24714082 -0.8045573 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, False, True]
State prediction error at timestep 535 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 535 of 1
Current timestep = 536. State = [[-0.11850184  0.18617281]]. Action = [[ 0.01865327 -0.11728387 -0.18987669 -0.335998  ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, False, True]
State prediction error at timestep 536 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 1
Current timestep = 537. State = [[-0.11461115  0.17878497]]. Action = [[ 0.204784   -0.22840683 -0.12406152 -0.10381484]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, False, True]
State prediction error at timestep 537 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.10821503  0.16626666]]. Action = [[ 0.24211556 -0.17866418  0.13971746 -0.76725066]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, False, True]
State prediction error at timestep 538 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of 1
Current timestep = 539. State = [[-0.09877206  0.15393677]]. Action = [[ 0.2001464   0.21136707  0.10934228 -0.6496905 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, False, True]
State prediction error at timestep 539 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 1
Current timestep = 540. State = [[-0.08937825  0.1525459 ]]. Action = [[ 0.21880823  0.24354455  0.01404321 -0.6265932 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, False, True]
State prediction error at timestep 540 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 541. State = [[-0.07819681  0.16036424]]. Action = [[-0.229929    0.04444396  0.00987428 -0.49299216]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, False, True]
State prediction error at timestep 541 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.07553644  0.16486692]]. Action = [[-0.05329251  0.03197509  0.08979654  0.870527  ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, False, True]
State prediction error at timestep 542 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[-0.07590665  0.16820204]]. Action = [[-0.24419418  0.09056586  0.1821323  -0.04064924]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, False, True]
State prediction error at timestep 543 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of 1
Current timestep = 544. State = [[-0.07913773  0.17386304]]. Action = [[ 0.08139241 -0.02175805 -0.02372156  0.664847  ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, False, True]
State prediction error at timestep 544 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of 1
Current timestep = 545. State = [[-0.07983757  0.17495613]]. Action = [[-0.04827487 -0.21891253 -0.08737358  0.8396131 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, False, True]
State prediction error at timestep 545 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of 1
Current timestep = 546. State = [[-0.07990985  0.17135006]]. Action = [[-0.23808771 -0.09054649  0.09683004  0.3928336 ]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, False, True]
State prediction error at timestep 546 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.08474314  0.16535419]]. Action = [[ 0.16296709 -0.2017477  -0.22984277 -0.52349335]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, False, True]
State prediction error at timestep 547 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.08497729  0.15679711]]. Action = [[ 0.06496024 -0.17862386  0.15684107 -0.81940436]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, False, True]
State prediction error at timestep 548 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[-0.08452543  0.14631072]]. Action = [[-0.07625806 -0.01346421  0.04338214  0.49435496]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, False, True]
State prediction error at timestep 549 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[-0.08552942  0.14030433]]. Action = [[-0.17328116 -0.0151576  -0.1449572  -0.63259184]]. Reward = [0.]
Curr episode timestep = 550
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.08839177  0.13560411]]. Action = [[-0.05236873 -0.24043459  0.14558074  0.4315623 ]]. Reward = [0.]
Curr episode timestep = 551
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.09273767  0.12673156]]. Action = [[-0.13610691 -0.06175517  0.18640763  0.71307087]]. Reward = [0.]
Curr episode timestep = 552
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.0982815   0.11894315]]. Action = [[-0.22427851  0.09710196 -0.23490578  0.41918206]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of 1
Current timestep = 554. State = [[-0.10714891  0.11573046]]. Action = [[ 0.10264736 -0.2246677   0.05758864  0.2824862 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 554 of 1
Current timestep = 555. State = [[-0.111815    0.10844854]]. Action = [[ 0.02888083  0.22826692 -0.04151106 -0.2399056 ]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 1
Current timestep = 556. State = [[-0.11391136  0.11060175]]. Action = [[-0.20075396  0.09690371  0.22216362 -0.84822756]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of -1
Current timestep = 557. State = [[-0.11889072  0.11410116]]. Action = [[ 0.0244574  -0.13958043 -0.03869864  0.5341277 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.12266912  0.11228972]]. Action = [[-0.07444598 -0.03117271 -0.15373158  0.5050168 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.12653744  0.10949342]]. Action = [[ 0.1104154  -0.07470256  0.17292643 -0.8600388 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of -1
Current timestep = 560. State = [[-0.12793662  0.10601371]]. Action = [[ 0.18615168 -0.10779876 -0.13991122  0.8038409 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of -1
Current timestep = 561. State = [[-0.12544265  0.1012918 ]]. Action = [[-0.13156526  0.07613045 -0.1582322   0.12207162]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(9.6536e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.1253973   0.10101593]]. Action = [[ 0.15735921 -0.09486684  0.00370687  0.16322029]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(9.9089e-05, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.12374232  0.09776195]]. Action = [[ 0.05526173  0.03045973  0.1348094  -0.00441945]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 564. State = [[-0.12239452  0.09652259]]. Action = [[ 0.14666992  0.03775215 -0.15532453 -0.83617324]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.11899112  0.09631413]]. Action = [[ 0.09883308 -0.23218863 -0.04730396 -0.3583979 ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 565 of -1
Current timestep = 566. State = [[-0.11458778  0.09009577]]. Action = [[ 0.23929858  0.20295006  0.03309092 -0.08896399]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of -1
Current timestep = 567. State = [[-0.10474406  0.09227952]]. Action = [[0.20007268 0.24866107 0.08036867 0.8870218 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(4.3190e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.09310926  0.10127796]]. Action = [[ 0.21097958  0.06734377 -0.22943707 -0.5809357 ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.08141977  0.10830424]]. Action = [[-0.18226455  0.2293492   0.16644886  0.3075986 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.07551473  0.1181489 ]]. Action = [[-0.24447589 -0.1696282   0.02013406  0.9520793 ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.07693575  0.11961302]]. Action = [[ 0.03797477 -0.13727471 -0.07985532 -0.7513667 ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 572. State = [[-0.07716239  0.11691587]]. Action = [[ 0.18223429  0.14409447 -0.21606319 -0.9275708 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of 1
Current timestep = 573. State = [[-0.07544295  0.1185393 ]]. Action = [[-0.24783196 -0.11295308 -0.151397   -0.4712032 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of 1
Current timestep = 574. State = [[-0.07678029  0.11744758]]. Action = [[-0.10483843  0.13208222  0.02298334 -0.78320205]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[-0.07948668  0.12039095]]. Action = [[ 0.02169186  0.2138828   0.04447955 -0.08561116]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 576. State = [[-0.08239407  0.12825473]]. Action = [[0.00696149 0.13412398 0.19748625 0.8455088 ]]. Reward = [0.]
Curr episode timestep = 576
Current timestep = 577. State = [[-0.08522442  0.1352927 ]]. Action = [[-0.08017787  0.10112321 -0.07189083  0.31529558]]. Reward = [0.]
Curr episode timestep = 577
Human Feedback received at timestep 577 of 1
Current timestep = 578. State = [[-0.08883213  0.14300115]]. Action = [[-0.22710852 -0.08150232 -0.19789481  0.5906012 ]]. Reward = [0.]
Curr episode timestep = 578
Human Feedback received at timestep 578 of 1
Current timestep = 579. State = [[-0.09418285  0.14639038]]. Action = [[-0.10126773  0.15327984  0.13151231 -0.5944638 ]]. Reward = [0.]
Curr episode timestep = 579
Current timestep = 580. State = [[-0.09950752  0.1524351 ]]. Action = [[ 0.14590132 -0.17397186  0.20770979  0.09719515]]. Reward = [0.]
Curr episode timestep = 580
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.10054095  0.15309173]]. Action = [[-0.03788975  0.20547464 -0.12879333 -0.97184837]]. Reward = [0.]
Curr episode timestep = 581
Human Feedback received at timestep 581 of 1
Current timestep = 582. State = [[-0.1027162   0.15737815]]. Action = [[-0.20919661  0.16665372  0.04302919 -0.54486847]]. Reward = [0.]
Curr episode timestep = 582
Current timestep = 583. State = [[-0.10721552  0.16585667]]. Action = [[0.07676944 0.14547515 0.2180489  0.08144212]]. Reward = [0.]
Curr episode timestep = 583
Human Feedback received at timestep 583 of 1
Current timestep = 584. State = [[-0.11113189  0.1740166 ]]. Action = [[0.05130178 0.00136513 0.11979726 0.75351405]]. Reward = [0.]
Curr episode timestep = 584
Human Feedback received at timestep 584 of 1
Current timestep = 585. State = [[-0.11364394  0.17883691]]. Action = [[-0.22812049  0.11819604 -0.18686174  0.7995347 ]]. Reward = [0.]
Curr episode timestep = 585
Current timestep = 586. State = [[-0.11771537  0.1856191 ]]. Action = [[ 0.17183346 -0.2055755   0.18468916 -0.8290501 ]]. Reward = [0.]
Curr episode timestep = 586
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.11717232  0.1845256 ]]. Action = [[ 0.15526003 -0.16089034 -0.22883514 -0.28839403]]. Reward = [0.]
Curr episode timestep = 587
Current timestep = 588. State = [[-0.11413489  0.17899577]]. Action = [[-0.01002418 -0.22343527  0.13876492  0.82713294]]. Reward = [0.]
Curr episode timestep = 588
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.11082953  0.17170797]]. Action = [[-0.05018201  0.22453377  0.0473057  -0.34021705]]. Reward = [0.]
Curr episode timestep = 589
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.11105609  0.17209353]]. Action = [[-0.20634693  0.18075204 -0.21751681 -0.7345683 ]]. Reward = [0.]
Curr episode timestep = 590
Current timestep = 591. State = [[-0.11393087  0.17696176]]. Action = [[ 0.09343582 -0.01745898  0.15712357 -0.78474826]]. Reward = [0.]
Curr episode timestep = 591
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.11492044  0.17868568]]. Action = [[ 0.10317618 -0.22336443  0.13304517 -0.8498713 ]]. Reward = [0.]
Curr episode timestep = 592
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.11275553  0.17210595]]. Action = [[-0.06907225 -0.05145352 -0.13442634  0.06649113]]. Reward = [0.]
Curr episode timestep = 593
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.11379772  0.16992399]]. Action = [[0.1220769  0.16949078 0.0065541  0.8973403 ]]. Reward = [0.]
Curr episode timestep = 594
Human Feedback received at timestep 594 of -1
Current timestep = 595. State = [[-0.11397739  0.17044312]]. Action = [[-0.09635511 -0.22476873 -0.15102278 -0.74634403]]. Reward = [0.]
Curr episode timestep = 595
Current timestep = 596. State = [[-0.11455954  0.16637337]]. Action = [[-0.17631929 -0.01737469 -0.18708804  0.8064023 ]]. Reward = [0.]
Curr episode timestep = 596
Current timestep = 597. State = [[-0.11768552  0.16313376]]. Action = [[-0.17379569 -0.22447905 -0.24302137 -0.1881063 ]]. Reward = [0.]
Curr episode timestep = 597
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.12336469  0.15524742]]. Action = [[-0.21236813  0.22242272 -0.14153288 -0.29199564]]. Reward = [0.]
Curr episode timestep = 598
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.13170691  0.15656945]]. Action = [[-2.4980307e-04  2.1571964e-02  5.9542239e-02 -5.2518326e-01]]. Reward = [0.]
Curr episode timestep = 599
Current timestep = 600. State = [[-0.13892363  0.15722634]]. Action = [[ 0.07796419 -0.13356891 -0.14771073 -0.8915927 ]]. Reward = [0.]
Curr episode timestep = 600
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.14215007  0.15421237]]. Action = [[-0.21708237  0.07354191 -0.04290068 -0.7374293 ]]. Reward = [0.]
Curr episode timestep = 601
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.14812657  0.15351088]]. Action = [[-0.24612397 -0.2426234  -0.00438333 -0.71261567]]. Reward = [0.]
Curr episode timestep = 602
Current timestep = 603. State = [[-0.15621252  0.14775869]]. Action = [[ 0.12369734  0.19879138 -0.05800831  0.2006948 ]]. Reward = [0.]
Curr episode timestep = 603
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.16186076  0.14949872]]. Action = [[ 0.16618878  0.16742066  0.18494192 -0.25172192]]. Reward = [0.]
Curr episode timestep = 604
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.16312878  0.15371048]]. Action = [[ 0.07872087 -0.07470179 -0.09075987  0.788991  ]]. Reward = [0.]
Curr episode timestep = 605
Current timestep = 606. State = [[-0.16286641  0.15466762]]. Action = [[-0.03119901  0.18775839 -0.03112839  0.44672072]]. Reward = [0.]
Curr episode timestep = 606
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.16267934  0.15876843]]. Action = [[ 0.12514287 -0.2346005   0.18090242 -0.0113607 ]]. Reward = [0.]
Curr episode timestep = 607
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.16090806  0.15632421]]. Action = [[ 0.09067276 -0.16974534  0.08534479 -0.24306023]]. Reward = [0.]
Curr episode timestep = 608
Current timestep = 609. State = [[-0.15739793  0.15146971]]. Action = [[-0.06284951  0.19871807  0.14137483  0.9345732 ]]. Reward = [0.]
Curr episode timestep = 609
Human Feedback received at timestep 609 of -1
Current timestep = 610. State = [[-0.1570541   0.15235998]]. Action = [[-0.0777943   0.03163463 -0.06483603 -0.8854533 ]]. Reward = [0.]
Curr episode timestep = 610
Current timestep = 611. State = [[-0.1579626   0.15426116]]. Action = [[-0.08614838  0.12500173  0.13432667 -0.55805945]]. Reward = [0.]
Curr episode timestep = 611
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.1600173   0.15830131]]. Action = [[ 0.09664646 -0.10474804  0.1233069   0.87303925]]. Reward = [0.]
Curr episode timestep = 612
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.15950242  0.15845895]]. Action = [[0.16731483 0.2125299  0.1391012  0.09027028]]. Reward = [0.]
Curr episode timestep = 613
Current timestep = 614. State = [[-0.15632266  0.16290356]]. Action = [[ 0.24694157 -0.12115031  0.07616168 -0.8948617 ]]. Reward = [0.]
Curr episode timestep = 614
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.1490657   0.16302793]]. Action = [[-0.06906673 -0.21634553  0.18833166 -0.8244023 ]]. Reward = [0.]
Curr episode timestep = 615
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.14473991  0.15765934]]. Action = [[ 0.18312672 -0.22666821  0.11519361  0.80280817]]. Reward = [0.]
Curr episode timestep = 616
Current timestep = 617. State = [[-0.13876188  0.14902578]]. Action = [[0.13987389 0.21499324 0.16410601 0.3987348 ]]. Reward = [0.]
Curr episode timestep = 617
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.13127905  0.1475936 ]]. Action = [[ 0.01484364 -0.21243918  0.01863477  0.23373711]]. Reward = [0.]
Curr episode timestep = 618
Human Feedback received at timestep 618 of -1
Current timestep = 619. State = [[-0.12518859  0.14143997]]. Action = [[-0.1654124  -0.09891674 -0.12238291  0.92162657]]. Reward = [0.]
Curr episode timestep = 619
Current timestep = 620. State = [[-0.12306535  0.13636552]]. Action = [[-0.03024629  0.01354983 -0.23957638  0.368531  ]]. Reward = [0.]
Curr episode timestep = 620
Human Feedback received at timestep 620 of 1
Current timestep = 621. State = [[-0.12227384  0.13361998]]. Action = [[-0.04831401 -0.13863018 -0.13193457 -0.56604874]]. Reward = [0.]
Curr episode timestep = 621
Human Feedback received at timestep 621 of 1
Current timestep = 622. State = [[-0.12246495  0.12796625]]. Action = [[-0.04388388 -0.1870887   0.22551578  0.15014088]]. Reward = [0.]
Curr episode timestep = 622
Current timestep = 623. State = [[-0.12428423  0.11898909]]. Action = [[-0.10536107  0.00123218  0.19804531  0.09156799]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(2.3982e-05, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.12640996  0.1133992 ]]. Action = [[ 0.2352817  -0.22190747 -0.09837222  0.3105526 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(5.1966e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.12393332  0.10479557]]. Action = [[0.09096581 0.21779197 0.053709   0.7572746 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.12020449  0.10427557]]. Action = [[ 0.05177587  0.06991401  0.15656975 -0.16454828]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(8.3315e-05, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.11781897  0.10573307]]. Action = [[-0.21152365  0.00578111  0.01412937  0.8293668 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of 1
Current timestep = 628. State = [[-0.11909396  0.10692435]]. Action = [[-0.20119499 -0.12481207 -0.1087673   0.29312956]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(9.2481e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.12319673  0.10432691]]. Action = [[ 0.04300895  0.15481028 -0.01274052 -0.24122071]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of 1
Current timestep = 630. State = [[-0.12460112  0.10656295]]. Action = [[-0.07866068 -0.21710981 -0.21035303  0.96837187]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 631. State = [[-0.1269646   0.10242406]]. Action = [[-0.08698541 -0.05986525  0.1559093   0.8618555 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of 1
Current timestep = 632. State = [[-0.13164665  0.0974105 ]]. Action = [[-0.19044101  0.02078256 -0.11936626  0.42746162]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(4.6929e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of 1
Current timestep = 633. State = [[-0.1385096   0.09431553]]. Action = [[-0.01911408 -0.0375797  -0.10543957  0.42170906]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(8.9907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of 1
Current timestep = 634. State = [[-0.1435088   0.09258069]]. Action = [[-0.13863899  0.04193598  0.10454854 -0.12564439]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(8.4410e-06, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.14787848  0.09341611]]. Action = [[ 0.21808851  0.20654774  0.10872263 -0.7937642 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(6.4652e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of 1
Current timestep = 636. State = [[-0.14923076  0.09918515]]. Action = [[ 0.10811025 -0.16823742 -0.23106743 -0.808196  ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(7.0538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.14837332  0.09795452]]. Action = [[-0.1463198   0.05908293 -0.00630501  0.79868555]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 1
Current timestep = 638. State = [[-0.14912947  0.09950145]]. Action = [[0.15330908 0.15734738 0.02728155 0.7222229 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.1480364   0.10408387]]. Action = [[ 0.09770164  0.21105483  0.08495158 -0.9140422 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(2.4717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 640. State = [[-0.14563282  0.11159043]]. Action = [[-0.18682753 -0.20699961  0.18313548  0.51565146]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(7.9749e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of 1
Current timestep = 641. State = [[-0.1473485   0.11174903]]. Action = [[-0.19375257  0.14365554 -0.19170721 -0.10350835]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(2.0198e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of 1
Current timestep = 642. State = [[-0.1513105   0.11610114]]. Action = [[ 0.2215091   0.2314964   0.23701379 -0.88487697]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(3.4634e-05, grad_fn=<MseLossBackward0>)
Current timestep = 643. State = [[-0.15250833  0.12411218]]. Action = [[ 0.18167901 -0.14266856 -0.11785951  0.8761896 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(6.1805e-05, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.14987949  0.12542383]]. Action = [[ 0.12285632 -0.01089987 -0.10856977  0.95286894]]. Reward = [0.]
Curr episode timestep = 644
Human Feedback received at timestep 644 of 1
Current timestep = 645. State = [[-0.14648192  0.1254205 ]]. Action = [[-0.14560127 -0.11231911 -0.08564898  0.892993  ]]. Reward = [0.]
Curr episode timestep = 645
Human Feedback received at timestep 645 of 1
Current timestep = 646. State = [[-0.14539345  0.12302182]]. Action = [[-0.2303303  0.0383411  0.1833325  0.8880321]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 647. State = [[-0.14783792  0.1229477 ]]. Action = [[-0.11058739  0.03980654 -0.12701514 -0.24096966]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(4.2365e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of 1
Current timestep = 648. State = [[-0.15249458  0.12375014]]. Action = [[-0.24401577  0.11672312  0.18207115  0.1709137 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(5.7770e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of 1
Current timestep = 649. State = [[-0.16002221  0.12884516]]. Action = [[-0.0411033   0.23175061  0.1744802  -0.89971745]]. Reward = [0.]
Curr episode timestep = 649
Human Feedback received at timestep 649 of 1
Current timestep = 650. State = [[-0.16529894  0.13741781]]. Action = [[ 0.08744881 -0.1845153   0.07035124 -0.19282532]]. Reward = [0.]
Curr episode timestep = 650
Current timestep = 651. State = [[-0.16754395  0.13843006]]. Action = [[ 0.1029357   0.11447883  0.22814333 -0.83032376]]. Reward = [0.]
Curr episode timestep = 651
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.16841573  0.1403785 ]]. Action = [[-0.23251924  0.05681536  0.048527    0.85378766]]. Reward = [0.]
Curr episode timestep = 652
Current timestep = 653. State = [[-0.17150159  0.14380148]]. Action = [[ 0.183074   -0.24334511  0.06218699  0.97781086]]. Reward = [0.]
Curr episode timestep = 653
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.17211439  0.14023966]]. Action = [[ 0.03326502  0.15385756 -0.18611656  0.6747837 ]]. Reward = [0.]
Curr episode timestep = 654
Human Feedback received at timestep 654 of -1
Current timestep = 655. State = [[-0.17249173  0.14118391]]. Action = [[0.13850701 0.15999171 0.09775978 0.17535174]]. Reward = [0.]
Curr episode timestep = 655
Current timestep = 656. State = [[-0.17049089  0.14482488]]. Action = [[-0.19094983 -0.1762546  -0.1609008   0.13093603]]. Reward = [0.]
Curr episode timestep = 656
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.1709912   0.14367068]]. Action = [[ 0.03969574 -0.15525062 -0.15944107  0.03151441]]. Reward = [0.]
Curr episode timestep = 657
Human Feedback received at timestep 657 of -1
Current timestep = 658. State = [[-0.17134844  0.13963859]]. Action = [[ 0.08673823  0.11400205  0.15968412 -0.33228183]]. Reward = [0.]
Curr episode timestep = 658
Current timestep = 659. State = [[-0.17132066  0.13905548]]. Action = [[-0.21457668 -0.17597283 -0.05319527 -0.20441598]]. Reward = [0.]
Curr episode timestep = 659
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.17389481  0.13450848]]. Action = [[ 0.12239048 -0.18486397  0.213346    0.45145535]]. Reward = [0.]
Curr episode timestep = 660
Human Feedback received at timestep 660 of -1
Current timestep = 661. State = [[-0.17450811  0.12797958]]. Action = [[-0.18804261  0.16131067 -0.15805325  0.53845763]]. Reward = [0.]
Curr episode timestep = 661
Current timestep = 662. State = [[-0.17674227  0.12644586]]. Action = [[ 0.23591566 -0.16089848  0.03487524  0.6447164 ]]. Reward = [0.]
Curr episode timestep = 662
Human Feedback received at timestep 662 of -1
Current timestep = 663. State = [[-0.17495766  0.12267239]]. Action = [[-0.02487762  0.10177353 -0.06488475  0.78547764]]. Reward = [0.]
Curr episode timestep = 663
Human Feedback received at timestep 663 of -1
Current timestep = 664. State = [[-0.17482609  0.12221802]]. Action = [[-0.00478277 -0.04454765  0.1538527  -0.8359639 ]]. Reward = [0.]
Curr episode timestep = 664
Current timestep = 665. State = [[-0.17397805  0.12094867]]. Action = [[ 0.22606266  0.05111536 -0.16734041  0.9225477 ]]. Reward = [0.]
Curr episode timestep = 665
Human Feedback received at timestep 665 of -1
Current timestep = 666. State = [[-0.17160206  0.12024839]]. Action = [[-0.17680386 -0.19915825 -0.1137372   0.22605371]]. Reward = [0.]
Curr episode timestep = 666
Current timestep = 667. State = [[-0.17071934  0.11620384]]. Action = [[0.13088584 0.09979981 0.07241222 0.3490597 ]]. Reward = [0.]
Curr episode timestep = 667
Human Feedback received at timestep 667 of -1
Current timestep = 668. State = [[-0.16957887  0.11504561]]. Action = [[ 0.1644797  -0.18085873  0.07950699 -0.2907107 ]]. Reward = [0.]
Curr episode timestep = 668
Current timestep = 669. State = [[-0.1658606   0.11006392]]. Action = [[ 0.2108171  -0.00257823 -0.02354006 -0.89935803]]. Reward = [0.]
Curr episode timestep = 669
Current timestep = 670. State = [[-0.15860137  0.1059855 ]]. Action = [[0.10708487 0.04312772 0.01971731 0.14726532]]. Reward = [0.]
Curr episode timestep = 670
Human Feedback received at timestep 670 of 0
Current timestep = 671. State = [[-0.14973278  0.10469433]]. Action = [[ 0.04518858  0.22736531 -0.22754543  0.3484633 ]]. Reward = [0.]
Curr episode timestep = 671
Human Feedback received at timestep 671 of 0
Current timestep = 672. State = [[-0.14119965  0.10909736]]. Action = [[ 0.1345188  -0.16034319  0.16286206 -0.07578093]]. Reward = [0.]
Curr episode timestep = 672
Current timestep = 673. State = [[-0.13469797  0.10715165]]. Action = [[-0.11008158 -0.17605963  0.131363   -0.91958165]]. Reward = [0.]
Curr episode timestep = 673
Current timestep = 674. State = [[-0.13208547  0.10232843]]. Action = [[ 0.09644514 -0.10167736  0.16363508 -0.55112386]]. Reward = [0.]
Curr episode timestep = 674
Human Feedback received at timestep 674 of 1
Current timestep = 675. State = [[-0.1279392   0.09599992]]. Action = [[ 0.08316135  0.16924435 -0.05034265  0.37729347]]. Reward = [0.]
Curr episode timestep = 675
Current timestep = 676. State = [[-0.12324104  0.09632556]]. Action = [[ 0.14411846 -0.15356043 -0.22447374  0.42213094]]. Reward = [0.]
Curr episode timestep = 676
Human Feedback received at timestep 676 of 1
Current timestep = 677. State = [[-0.11758007  0.09307888]]. Action = [[ 0.22886455 -0.02373385 -0.1358071  -0.21036553]]. Reward = [0.]
Curr episode timestep = 677
Human Feedback received at timestep 677 of 1
Current timestep = 678. State = [[-0.10728452  0.09019934]]. Action = [[ 0.15203571 -0.1216056  -0.16544646  0.21817172]]. Reward = [0.]
Curr episode timestep = 678
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.09686912  0.08516177]]. Action = [[-0.00628421 -0.04203215 -0.09296323 -0.89804554]]. Reward = [0.]
Curr episode timestep = 679
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.08880938  0.08094852]]. Action = [[-0.00111222  0.19835693 -0.2217408   0.11246824]]. Reward = [0.]
Curr episode timestep = 680
Current timestep = 681. State = [[-0.08375148  0.08284338]]. Action = [[-0.03743872 -0.21529943 -0.16532601 -0.89623225]]. Reward = [0.]
Curr episode timestep = 681
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.08205044  0.07871442]]. Action = [[-0.07054698 -0.13013062  0.14396691 -0.9624732 ]]. Reward = [0.]
Curr episode timestep = 682
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.08099262  0.07262804]]. Action = [[ 0.05708075 -0.17436944 -0.04337138  0.5128186 ]]. Reward = [0.]
Curr episode timestep = 683
Current timestep = 684. State = [[-0.07918484  0.06593207]]. Action = [[ 0.1997422   0.18768781 -0.0870759   0.48388684]]. Reward = [0.]
Curr episode timestep = 684
Current timestep = 685. State = [[-0.07519098  0.0652851 ]]. Action = [[-0.058062    0.06957346 -0.22519477 -0.4058854 ]]. Reward = [0.]
Curr episode timestep = 685
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.07327062  0.0667804 ]]. Action = [[-0.18187018  0.10230741 -0.15943067 -0.485453  ]]. Reward = [0.]
Curr episode timestep = 686
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.07468119  0.07028921]]. Action = [[-0.12231991  0.14867139 -0.220441   -0.7296432 ]]. Reward = [0.]
Curr episode timestep = 687
Current timestep = 688. State = [[-0.07748518  0.07708881]]. Action = [[-0.03886393  0.2239815   0.20418596  0.11212575]]. Reward = [0.]
Curr episode timestep = 688
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[-0.08098225  0.0859374 ]]. Action = [[-0.14017399 -0.09348652  0.20176855 -0.94653606]]. Reward = [0.]
Curr episode timestep = 689
Human Feedback received at timestep 689 of 1
Current timestep = 690. State = [[-0.08392615  0.0898347 ]]. Action = [[ 0.07667261 -0.22506161  0.04619011  0.46426857]]. Reward = [0.]
Curr episode timestep = 690
Current timestep = 691. State = [[-0.08507715  0.08687752]]. Action = [[ 0.09206659  0.05696642 -0.07468769  0.18681681]]. Reward = [0.]
Curr episode timestep = 691
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.08494236  0.08592995]]. Action = [[ 0.0480805   0.00178903  0.09450439 -0.6367495 ]]. Reward = [0.]
Curr episode timestep = 692
Current timestep = 693. State = [[-0.08493686  0.08542936]]. Action = [[-0.2126208  -0.20991348  0.18975759 -0.81919044]]. Reward = [0.]
Curr episode timestep = 693
Human Feedback received at timestep 693 of -1
Current timestep = 694. State = [[-0.08773486  0.07990504]]. Action = [[-0.17809351  0.0025869  -0.16725987  0.42420924]]. Reward = [0.]
Curr episode timestep = 694
Current timestep = 695. State = [[-0.09153032  0.07726806]]. Action = [[ 0.17889136  0.18198407 -0.2209356   0.18688011]]. Reward = [0.]
Curr episode timestep = 695
Human Feedback received at timestep 695 of -1
Current timestep = 696. State = [[-0.09255754  0.08026242]]. Action = [[-0.23189886  0.18494359  0.0131712  -0.36789185]]. Reward = [0.]
Curr episode timestep = 696
Current timestep = 697. State = [[-0.09579139  0.08708493]]. Action = [[0.10892564 0.13353479 0.15919012 0.26996017]]. Reward = [0.]
Curr episode timestep = 697
Current timestep = 698. State = [[-0.09822442  0.09346739]]. Action = [[-0.22705479 -0.23585218  0.21331298 -0.06558728]]. Reward = [0.]
Curr episode timestep = 698
Current timestep = 699. State = [[-0.10302727  0.09180119]]. Action = [[ 0.18570504  0.02355626 -0.15616155  0.4194305 ]]. Reward = [0.]
Curr episode timestep = 699
Current timestep = 700. State = [[-0.1038698   0.09130972]]. Action = [[ 0.05488318 -0.09211254  0.07911736  0.0996089 ]]. Reward = [0.]
Curr episode timestep = 700
Current timestep = 701. State = [[-0.10320482  0.08837007]]. Action = [[ 0.01042688 -0.2377999  -0.17697497  0.28393424]]. Reward = [0.]
Curr episode timestep = 701
Human Feedback received at timestep 701 of -1
Current timestep = 702. State = [[-0.10229146  0.08162774]]. Action = [[ 0.23443824  0.22254056 -0.04329517 -0.20098048]]. Reward = [0.]
Curr episode timestep = 702
Current timestep = 703. State = [[-0.0994025   0.08245536]]. Action = [[ 0.22061661 -0.02217963  0.15509358 -0.7586703 ]]. Reward = [0.]
Curr episode timestep = 703
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-0.09310368  0.08299839]]. Action = [[-0.22761326  0.17372936  0.15727317 -0.16398656]]. Reward = [0.]
Curr episode timestep = 704
Current timestep = 705. State = [[-0.09138905  0.08645261]]. Action = [[-0.00796589 -0.00749433  0.22723639 -0.4181472 ]]. Reward = [0.]
Curr episode timestep = 705
Current timestep = 706. State = [[-0.0912677   0.08891602]]. Action = [[0.12756258 0.08880365 0.07882345 0.48173833]]. Reward = [0.]
Curr episode timestep = 706
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[-0.08919018  0.09208386]]. Action = [[ 0.01469865  0.0170728   0.15199482 -0.37992597]]. Reward = [0.]
Curr episode timestep = 707
Human Feedback received at timestep 707 of -1
Current timestep = 708. State = [[-0.08744523  0.09445653]]. Action = [[ 0.18946743 -0.12251332 -0.07657522  0.8956635 ]]. Reward = [0.]
Curr episode timestep = 708
Current timestep = 709. State = [[-0.08327891  0.09421865]]. Action = [[-0.21028903  0.13924086  0.04993266 -0.29479134]]. Reward = [0.]
Curr episode timestep = 709
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[-0.08245872  0.09657089]]. Action = [[ 0.17917591  0.020504   -0.11535522 -0.6141508 ]]. Reward = [0.]
Curr episode timestep = 710
Human Feedback received at timestep 710 of 1
Current timestep = 711. State = [[-0.07990658  0.09964307]]. Action = [[ 0.12576896  0.201953   -0.19420543  0.9621637 ]]. Reward = [0.]
Curr episode timestep = 711
Human Feedback received at timestep 711 of 1
Current timestep = 712. State = [[-0.07470647  0.10696003]]. Action = [[ 0.22302902  0.08045471  0.08048764 -0.64957577]]. Reward = [0.]
Curr episode timestep = 712
Current timestep = 713. State = [[-0.06708188  0.11344601]]. Action = [[ 0.1270749   0.01952142 -0.13560812 -0.27916473]]. Reward = [0.]
Curr episode timestep = 713
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.05759991  0.11832576]]. Action = [[ 0.06352043 -0.15292026 -0.19907555  0.3266375 ]]. Reward = [0.]
Curr episode timestep = 714
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.05008776  0.11600704]]. Action = [[ 0.11570498 -0.20083174 -0.13717245  0.0480864 ]]. Reward = [0.]
Curr episode timestep = 715
Human Feedback received at timestep 715 of 1
Current timestep = 716. State = [[-0.04187663  0.11005764]]. Action = [[ 0.20547825  0.00584862  0.02006793 -0.10343677]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [False, True, False, False, True, False]
State prediction error at timestep 716 is tensor(5.4570e-05, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[0.01750522 0.1050304 ]]. Action = [[-0.08596858  0.04108068  0.00290129 -0.07960284]]. Reward = [1.]
Curr episode timestep = 717
Scene graph at timestep 717 is [False, True, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[0.05387085 0.10912046]]. Action = [[ 0.12114602 -0.2254064  -0.03514288 -0.32429957]]. Reward = [1.]
Curr episode timestep = 0
Scene graph at timestep 718 is [False, False, True, False, True, False]
State prediction error at timestep 718 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of -1
Current timestep = 719. State = [[-0.09757563  0.15694611]]. Action = [[-0.08348386 -0.20113344 -0.19363166 -0.51046216]]. Reward = [1.]
Curr episode timestep = 0
Human Feedback received at timestep 719 of -1
Current timestep = 720. State = [[-0.09474993  0.13420653]]. Action = [[-0.04791544 -0.2326029  -0.03538638 -0.86906224]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 721. State = [[-0.09438494  0.12871785]]. Action = [[ 0.1586563   0.15541923 -0.1376652   0.6665704 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 722. State = [[-0.09368239  0.12772778]]. Action = [[-0.06127015 -0.17598522 -0.04606059 -0.14644194]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 723. State = [[-0.09262336  0.1236277 ]]. Action = [[ 0.125023    0.08826926  0.06075007 -0.10561204]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 724. State = [[-0.09198032  0.12212155]]. Action = [[-0.17253761 -0.1510791  -0.10052451  0.50101733]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 725. State = [[-0.09172269  0.11832754]]. Action = [[-0.0921692  -0.18118894  0.07142454 -0.86872673]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 726. State = [[-0.09161333  0.11160551]]. Action = [[ 0.17287725 -0.05097979 -0.22008674  0.666401  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 727. State = [[-0.08980226  0.10533796]]. Action = [[ 0.1634798  -0.06873348 -0.11170204 -0.59766585]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 728. State = [[-0.08758271  0.0992638 ]]. Action = [[-0.01994558  0.06133133  0.23226655  0.7050257 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 729. State = [[-0.08660605  0.09757766]]. Action = [[-0.12409344  0.22566795  0.11719948  0.8372457 ]]. Reward = [0.]
Curr episode timestep = 9
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.08776837  0.10098523]]. Action = [[-0.01463537 -0.10137343  0.14757335  0.6768637 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 731. State = [[-0.08781856  0.10141028]]. Action = [[-0.01367331  0.18615347  0.15461266 -0.42188406]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 732. State = [[-0.08917574  0.10551471]]. Action = [[ 0.19345498  0.1733051  -0.03383555 -0.86541325]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 733. State = [[-0.08894432  0.11101285]]. Action = [[-0.08230188 -0.24170844 -0.2201294   0.7920166 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 734. State = [[-0.08810434  0.11015111]]. Action = [[-0.00482392  0.09932062 -0.16204599 -0.9172407 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 735. State = [[-0.08820364  0.1111973 ]]. Action = [[ 0.15515271 -0.05808336 -0.20378183 -0.15762514]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 736. State = [[-0.08668959  0.11030962]]. Action = [[ 0.09612787 -0.15199107 -0.11482546 -0.6557182 ]]. Reward = [0.]
Curr episode timestep = 16
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.08366492  0.10568524]]. Action = [[ 0.2018463  -0.0932512  -0.17551535  0.80022967]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 738. State = [[-0.07831924  0.10026015]]. Action = [[-0.10097203  0.06442672 -0.0806126  -0.06868446]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 739. State = [[-0.07681856  0.09902805]]. Action = [[ 0.1000295   0.19584167  0.15545604 -0.8675735 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 740. State = [[-0.07508854  0.1027157 ]]. Action = [[-0.03533816  0.23580268  0.17746457 -0.8937256 ]]. Reward = [0.]
Curr episode timestep = 20
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.07372712  0.11112025]]. Action = [[0.04428342 0.11844167 0.00575879 0.3242904 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 742. State = [[-0.0707702   0.11895845]]. Action = [[-0.1266511  -0.16824055  0.02062041  0.09277773]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 743. State = [[-0.06997801  0.12079088]]. Action = [[-0.0633433   0.24478072  0.18471524  0.11788344]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 744. State = [[-0.07197604  0.12710285]]. Action = [[-0.0693751  -0.21268618  0.10585612  0.9752784 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 745. State = [[-0.07246145  0.12736763]]. Action = [[ 0.08868307  0.18418756  0.10750443 -0.6768879 ]]. Reward = [0.]
Curr episode timestep = 25
Human Feedback received at timestep 745 of 1
Current timestep = 746. State = [[-0.07332223  0.13114142]]. Action = [[ 0.15458626  0.0657008   0.19871062 -0.85172683]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 747. State = [[-0.07020371  0.13466364]]. Action = [[ 0.21091273 -0.08700903  0.11123079 -0.2113908 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 748. State = [[-0.06479526  0.13488582]]. Action = [[-0.14214736 -0.11796631  0.04733577 -0.09318823]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 749. State = [[-0.06282022  0.13400364]]. Action = [[-0.24387603  0.18459353 -0.11134696  0.04371881]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 750. State = [[-0.06498803  0.13817677]]. Action = [[ 0.19136715  0.2157402   0.02792186 -0.9481884 ]]. Reward = [0.]
Curr episode timestep = 30
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.06478944  0.14439534]]. Action = [[ 0.12817693 -0.22114849 -0.13582462  0.43792927]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 752. State = [[-0.06164403  0.14368492]]. Action = [[-0.21103175  0.06878543 -0.02610099 -0.7860829 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 753. State = [[-0.06233774  0.14492632]]. Action = [[ 0.13505727 -0.14349236 -0.08091164 -0.5003416 ]]. Reward = [0.]
Curr episode timestep = 33
Human Feedback received at timestep 753 of 1
Current timestep = 754. State = [[-0.0610108   0.14224648]]. Action = [[ 0.16509002  0.21201774  0.10674304 -0.23883647]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 755. State = [[-0.05901044  0.14527294]]. Action = [[-0.18663287  0.15215951 -0.06717989 -0.68330306]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 756. State = [[-0.05911079  0.15179425]]. Action = [[-0.09756605  0.18121755  0.01316205  0.2061559 ]]. Reward = [0.]
Curr episode timestep = 36
Human Feedback received at timestep 756 of 1
Current timestep = 757. State = [[-0.0614409   0.16090026]]. Action = [[ 0.03862512  0.13350195 -0.03251232  0.4540453 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 758. State = [[-0.06334642  0.1700628 ]]. Action = [[ 0.08579907  0.08084488 -0.10121897  0.51367855]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 759. State = [[-0.0628768  0.1775054]]. Action = [[ 0.00365806  0.15074456 -0.18653122  0.47258854]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 760. State = [[-0.06213317  0.18594062]]. Action = [[ 0.05875489  0.05008495 -0.21592201 -0.40742016]]. Reward = [0.]
Curr episode timestep = 40
Human Feedback received at timestep 760 of 1
Current timestep = 761. State = [[-0.0597246   0.19334063]]. Action = [[0.06189835 0.19713798 0.13135207 0.73109174]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 762. State = [[-0.05739434  0.20229635]]. Action = [[ 0.204454    0.01123494  0.07855827 -0.9537959 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 763. State = [[-0.05085105  0.20816658]]. Action = [[ 0.09054247 -0.12506807  0.1974299  -0.9343795 ]]. Reward = [0.]
Curr episode timestep = 43
Human Feedback received at timestep 763 of -1
Current timestep = 764. State = [[-0.04313693  0.20912683]]. Action = [[ 0.24401093 -0.21997426  0.23764956 -0.7508695 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 764 is [False, True, False, False, False, True]
State prediction error at timestep 764 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.0314866   0.20479612]]. Action = [[ 0.17587477  0.24314672 -0.19503354 -0.3428803 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 765 is [False, True, False, False, False, True]
State prediction error at timestep 765 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of -1
Current timestep = 766. State = [[-0.02167224  0.2090434 ]]. Action = [[ 0.02676347  0.07419813 -0.06837699 -0.10731155]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 766 is [False, True, False, False, False, True]
State prediction error at timestep 766 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.01376373  0.2134254 ]]. Action = [[-0.09008771 -0.22038305 -0.17808674  0.27098536]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 767 is [False, True, False, False, False, True]
State prediction error at timestep 767 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of -1
Current timestep = 768. State = [[-0.00648298  0.21051525]]. Action = [[ 0.03687909 -0.12765647 -0.04991904 -0.7791435 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 768 is [False, True, False, False, False, True]
State prediction error at timestep 768 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[-0.00219212  0.20547014]]. Action = [[-0.20646429  0.0853208  -0.01610243  0.79226184]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 769 is [False, True, False, False, False, True]
State prediction error at timestep 769 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 769 of -1
Current timestep = 770. State = [[-0.00364699  0.20551005]]. Action = [[ 0.13583845  0.19690025 -0.09678701  0.58140504]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 771. State = [[-0.00435044  0.20877428]]. Action = [[ 0.2193411  -0.00498402  0.21844608 -0.364951  ]]. Reward = [0.]
Curr episode timestep = 51
Human Feedback received at timestep 771 of -1
Current timestep = 772. State = [[-0.00104693  0.21053436]]. Action = [[-0.04354534 -0.05047387  0.02537674 -0.22371596]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 773. State = [[0.00311445 0.21175483]]. Action = [[ 0.02225798  0.20572397 -0.22299896 -0.7237397 ]]. Reward = [0.]
Curr episode timestep = 53
Human Feedback received at timestep 773 of -1
Current timestep = 774. State = [[0.00573849 0.2180945 ]]. Action = [[-0.22204468  0.24519849 -0.00602548 -0.64298815]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 775. State = [[0.00381687 0.22890858]]. Action = [[-0.11884585  0.23905683 -0.15974525 -0.2938643 ]]. Reward = [0.]
Curr episode timestep = 55
Human Feedback received at timestep 775 of -1
Current timestep = 776. State = [[-0.0006016   0.24182034]]. Action = [[-0.11912847 -0.14056584  0.10603318 -0.23086607]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 777. State = [[-0.00413075  0.24716477]]. Action = [[-0.08134457  0.05255169  0.04115987  0.03580523]]. Reward = [0.]
Curr episode timestep = 57
Human Feedback received at timestep 777 of -1
Current timestep = 778. State = [[-0.00755738  0.25269967]]. Action = [[ 0.15585732 -0.09364477  0.21985799  0.02546453]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 779. State = [[-0.00747852  0.2526335 ]]. Action = [[ 0.1452691   0.12555304 -0.10398166  0.90275645]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 780. State = [[-0.00723632  0.25378567]]. Action = [[ 0.10449877 -0.13744135 -0.09307478 -0.39403522]]. Reward = [0.]
Curr episode timestep = 60
Human Feedback received at timestep 780 of -1
Current timestep = 781. State = [[-0.00382538  0.25206897]]. Action = [[-0.14494385 -0.02315418 -0.15927476 -0.5815411 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 782. State = [[-0.00254023  0.25174662]]. Action = [[-0.19504888 -0.02574161  0.21123478  0.03633368]]. Reward = [0.]
Curr episode timestep = 62
Human Feedback received at timestep 782 of -1
Current timestep = 783. State = [[-0.00310494  0.2506947 ]]. Action = [[ 0.05056265 -0.22612393 -0.24335647  0.9415295 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 784. State = [[-0.00107579  0.24502401]]. Action = [[-0.1493697  -0.24597345 -0.10917436  0.16883004]]. Reward = [0.]
Curr episode timestep = 64
Human Feedback received at timestep 784 of -1
Current timestep = 785. State = [[0.00032815 0.23584963]]. Action = [[-0.17463829 -0.22351864 -0.14632721 -0.5480511 ]]. Reward = [0.]
Curr episode timestep = 65
Human Feedback received at timestep 785 of -1
Current timestep = 786. State = [[-1.5882717e-04  2.2571456e-01]]. Action = [[ 0.04891074 -0.09079134  0.04342812 -0.17538887]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 787. State = [[-0.00181761  0.2165952 ]]. Action = [[0.03099915 0.06861958 0.05676985 0.7961794 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 788. State = [[-0.00270956  0.21211489]]. Action = [[ 0.0205715   0.04650024 -0.14801084  0.62782145]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 789. State = [[-0.00332249  0.21082379]]. Action = [[-0.1477395  -0.02001815  0.13183826  0.9051933 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 790. State = [[-0.005949    0.20952433]]. Action = [[0.03640291 0.03210899 0.24188083 0.86229455]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 791. State = [[-0.00787911  0.20964566]]. Action = [[-0.22672643  0.22663593  0.17674485  0.7737508 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 792. State = [[-0.01201129  0.21640056]]. Action = [[-0.01423962  0.04971212  0.16528267  0.10465229]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 793. State = [[-0.01632419  0.22227469]]. Action = [[-0.04714906  0.11730984 -0.09599729  0.52894986]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 794. State = [[-0.02133443  0.22992988]]. Action = [[ 0.03227037  0.22608232 -0.17350611 -0.6610081 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 795. State = [[-0.02602789  0.23859097]]. Action = [[ 0.1664232  -0.18691915 -0.19771102 -0.9369084 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 796. State = [[-0.02589517  0.23915851]]. Action = [[ 0.08827016  0.16143209 -0.17357849 -0.60062426]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 797. State = [[-0.02667201  0.24233465]]. Action = [[-0.10588901 -0.06095913 -0.11129868 -0.6193982 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 798. State = [[-0.026884    0.24347743]]. Action = [[-0.10823864 -0.12315542  0.21699828  0.4568274 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 799. State = [[-0.0265874   0.24188231]]. Action = [[ 0.19540834 -0.21720734 -0.23469046 -0.892367  ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 800. State = [[-0.02405203  0.23572157]]. Action = [[-0.12304838  0.12594604 -0.08123212  0.19340754]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 801. State = [[-0.02482596  0.23533584]]. Action = [[ 0.06829345  0.11112976 -0.07417849  0.35293567]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 802. State = [[-0.02547435  0.2366136 ]]. Action = [[ 0.00246209 -0.05297007 -0.22075716 -0.02115846]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 803. State = [[-0.02525328  0.23598956]]. Action = [[-0.20099448  0.06836784 -0.12372622  0.14009845]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 804. State = [[-0.02800821  0.23869762]]. Action = [[-0.14430134  0.0718458  -0.22862433  0.8134304 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 805. State = [[-0.03323535  0.24246232]]. Action = [[-0.21505144 -0.06714299 -0.10986236  0.7618263 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 806. State = [[-0.03978993  0.24378403]]. Action = [[-0.00572658 -0.23902951  0.12556428  0.04298341]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 807. State = [[-0.04522545  0.23810373]]. Action = [[ 0.21542716  0.14506894 -0.22759692 -0.56071776]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 808. State = [[-0.04607388  0.23737662]]. Action = [[ 0.08681896  0.1128599   0.06050295 -0.05916429]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 809. State = [[-0.04631646  0.23923206]]. Action = [[-0.24521579  0.04588228 -0.15434314  0.06956542]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 810. State = [[-0.04875872  0.24305671]]. Action = [[0.20859817 0.00974199 0.17944622 0.9493618 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 811. State = [[-0.04838211  0.24381022]]. Action = [[-0.18273053 -0.23188083 -0.18080966 -0.52656215]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 812. State = [[-0.04894478  0.24034947]]. Action = [[-0.10873264  0.06000555 -0.09327079  0.4628638 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 813. State = [[-0.05189143  0.23977637]]. Action = [[ 0.07798719  0.07642031  0.06176457 -0.60857207]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 814. State = [[-0.05418266  0.24129187]]. Action = [[-0.14472745  0.24262914 -0.23951256 -0.45618933]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 815. State = [[-0.05801261  0.24751359]]. Action = [[ 0.21995047 -0.2109287  -0.02532622  0.21340561]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 816. State = [[-0.05779725  0.246203  ]]. Action = [[ 0.108024    0.24695808  0.1172787  -0.12810194]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 817. State = [[-0.05915754  0.2497207 ]]. Action = [[-0.07560194  0.15036774 -0.0942654  -0.9635935 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 818. State = [[-0.06196899  0.25606972]]. Action = [[ 0.10676411  0.22204524 -0.12762037 -0.32959032]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 819. State = [[-0.06353562  0.26347625]]. Action = [[-0.10496104 -0.24637906  0.01467639  0.6199007 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 820. State = [[-0.06298514  0.26403674]]. Action = [[0.16031355 0.02198756 0.06573576 0.87322605]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 821. State = [[-0.06213319  0.26394674]]. Action = [[-0.15397751  0.20784086 -0.19655555 -0.8344095 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 822. State = [[-0.0645069  0.268933 ]]. Action = [[-0.11565863 -0.15531595 -0.00623599 -0.8515255 ]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 823. State = [[-0.06549919  0.27020833]]. Action = [[-0.16292606 -0.06524736 -0.149806   -0.20335221]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 824. State = [[-0.06795646  0.27035198]]. Action = [[-0.17025809 -0.13678335  0.21876669  0.01939416]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 825. State = [[-0.0714251   0.26888135]]. Action = [[-0.18512748  0.23031288  0.05283228 -0.03479308]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 826. State = [[-0.07865261  0.27377677]]. Action = [[ 0.20635414 -0.01212108 -0.18843766  0.6428307 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 827. State = [[-0.0805973   0.27503538]]. Action = [[ 0.2026974  -0.21412607 -0.17018078  0.18510747]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 828. State = [[-0.07745083  0.2698068 ]]. Action = [[ 0.05844873 -0.10814048 -0.04107137  0.45091903]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 829. State = [[-0.07446181  0.26527232]]. Action = [[ 0.08916894  0.111321   -0.14351617 -0.9927482 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 830. State = [[-0.07289653  0.26406732]]. Action = [[-0.07685116  0.01114932  0.19632775 -0.05067891]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 831. State = [[-0.07256263  0.26317665]]. Action = [[-0.03607887  0.02637386  0.11021599  0.20256007]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 832. State = [[-0.07275026  0.26304042]]. Action = [[-0.21104816 -0.21632726  0.21269268 -0.58342206]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 833. State = [[-0.07365957  0.25923675]]. Action = [[ 0.04480407 -0.10004464 -0.10094276  0.8795066 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 834. State = [[-0.07369342  0.25465393]]. Action = [[-0.22120653 -0.02803503 -0.139047    0.43963718]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 835. State = [[-0.07676437  0.2515956 ]]. Action = [[-0.03388749 -0.01803499  0.19269758  0.10662627]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 836. State = [[-0.08079702  0.2480852 ]]. Action = [[0.07518262 0.02240986 0.19862083 0.8412261 ]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 837. State = [[-0.08318871  0.2456314 ]]. Action = [[-0.17401843 -0.04349698 -0.22740655 -0.9408442 ]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 838. State = [[-0.0877986  0.2442734]]. Action = [[ 0.06640878  0.12321824  0.23968044 -0.37922502]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 839. State = [[-0.09037001  0.24680987]]. Action = [[ 0.22285303  0.1854623   0.19799536 -0.6257118 ]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 840. State = [[-0.09095762  0.25008595]]. Action = [[ 0.13333744  0.1493963   0.05726621 -0.09782749]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 841. State = [[-0.09041905  0.2551667 ]]. Action = [[-1.9330564e-01  1.5475011e-01 -4.9501657e-05 -4.0327716e-01]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 842. State = [[-0.09389214  0.263227  ]]. Action = [[-0.15032926  0.13328493 -0.05563486  0.11817741]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 843. State = [[-0.09913836  0.27232414]]. Action = [[-0.03647301  0.03842348  0.10955507  0.48467088]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 844. State = [[-0.10327363  0.27894464]]. Action = [[ 0.10342583 -0.14765096 -0.22536866  0.2124486 ]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 845. State = [[-0.10329331  0.27954376]]. Action = [[-0.14109191  0.00351658 -0.13274215 -0.584294  ]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 846. State = [[-0.10425033  0.28079256]]. Action = [[-0.15739705 -0.08908086  0.02526674  0.7835443 ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 847. State = [[-0.10651525  0.2815663 ]]. Action = [[-0.11168802  0.21688828  0.11881602  0.59208524]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 848. State = [[-0.1112157   0.28716096]]. Action = [[-0.01971141 -0.21788459 -0.09294534 -0.48265135]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 849. State = [[-0.11357956  0.28519803]]. Action = [[ 0.06928903 -0.14258508 -0.03811097 -0.8848161 ]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 850. State = [[-0.11419627  0.2797289 ]]. Action = [[-0.16246429 -0.19923161 -0.17178035 -0.47481287]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 851. State = [[-0.11702842  0.27229643]]. Action = [[0.140098   0.19692141 0.23989236 0.84179854]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 852. State = [[-0.12002191  0.2711553 ]]. Action = [[-0.16846551  0.1809128   0.16828853 -0.616362  ]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 853. State = [[-0.12464032  0.27539474]]. Action = [[-0.16328806 -0.13724674 -0.17477602  0.0424639 ]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 854. State = [[-0.12940647  0.27589408]]. Action = [[ 0.04080874 -0.03822164 -0.2026048   0.6213007 ]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 855. State = [[-0.13206695  0.27515665]]. Action = [[ 0.0054478   0.14524037 -0.12896682  0.7990267 ]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 856. State = [[-0.13533635  0.27747312]]. Action = [[-0.06646292 -0.01392849 -0.14294489 -0.8700659 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 857. State = [[-0.13885629  0.2786654 ]]. Action = [[ 0.12553501  0.07924294 -0.2268132  -0.33600402]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 858. State = [[-0.14011982  0.2802222 ]]. Action = [[-0.10391963 -0.11509791 -0.23911981 -0.35300434]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 859. State = [[-0.14115942  0.27863306]]. Action = [[ 0.10831746 -0.11846548  0.0155125   0.7944968 ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 860. State = [[-0.14047016  0.27436373]]. Action = [[ 0.15906447 -0.21832077 -0.06510861 -0.1791476 ]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 861. State = [[-0.13690464  0.26652375]]. Action = [[ 0.06801948  0.06918681 -0.075268    0.07907021]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 862. State = [[-0.1341372   0.26314443]]. Action = [[-0.20278507  0.22483212 -0.14337786 -0.6710613 ]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 863. State = [[-0.13616183  0.2663138 ]]. Action = [[ 0.16193411 -0.23665124 -0.18005195 -0.13891023]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 864. State = [[-0.13425454  0.2623647 ]]. Action = [[-0.09168951  0.08000299  0.22290224 -0.22877276]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 865. State = [[-0.1345346   0.26242265]]. Action = [[-0.1321291  -0.01159249  0.02473584 -0.01998293]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 866. State = [[-0.13632861  0.2631188 ]]. Action = [[-0.10967213 -0.01623262  0.06949747  0.19589162]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 867. State = [[-0.13865955  0.26292753]]. Action = [[-0.19226617 -0.18621726  0.01130572  0.5645623 ]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 868. State = [[-0.14428964  0.25783652]]. Action = [[-0.06189045 -0.02313554 -0.22375008 -0.10266471]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 869. State = [[-0.15126055  0.25345263]]. Action = [[ 0.17438108  0.22358352 -0.16342235 -0.6544817 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 870. State = [[-0.15459004  0.25519753]]. Action = [[0.20440549 0.15822273 0.21539322 0.9854841 ]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 871. State = [[-0.1548301   0.25881448]]. Action = [[-0.07187575  0.18486077 -0.09767245  0.33186316]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 872. State = [[-0.1568842   0.26505595]]. Action = [[ 0.04724231  0.09006089 -0.19115843  0.6809077 ]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 873. State = [[-0.15807104  0.2708333 ]]. Action = [[-0.0329835  -0.16119266  0.17045349 -0.79452866]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 874. State = [[-0.15796351  0.2719817 ]]. Action = [[-0.05172503  0.24245244 -0.19178659  0.6644665 ]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 875. State = [[-0.16072105  0.27678978]]. Action = [[ 0.11036831 -0.12242456  0.09154803 -0.85346276]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 876. State = [[-0.16030578  0.27718943]]. Action = [[-0.1468326  -0.16334055 -0.23434147 -0.84357494]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 877. State = [[-0.1597096  0.2759496]]. Action = [[ 0.20933568 -0.1594385   0.21294907  0.6188443 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 878. State = [[-0.15590312  0.27078778]]. Action = [[-0.10068727 -0.23102684  0.09712413  0.622329  ]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 879. State = [[-0.15345672  0.26324618]]. Action = [[-0.23138815  0.00386027  0.22248864 -0.7212032 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 880. State = [[-0.15573424  0.25825047]]. Action = [[-0.06485707 -0.0294527  -0.16760366  0.9728538 ]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 881. State = [[-0.15970261  0.253768  ]]. Action = [[ 0.07989305  0.01225659  0.03155026 -0.59394795]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 882. State = [[-0.16176336  0.25076553]]. Action = [[-0.0638133   0.13880014 -0.07994136  0.2803862 ]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 883. State = [[-0.16441306  0.25250694]]. Action = [[ 0.14309764  0.22856724 -0.22086632  0.6111946 ]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 884. State = [[-0.16678925  0.2566644 ]]. Action = [[-0.0647665   0.13660708  0.24080801 -0.756566  ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 885. State = [[-0.1703257   0.26289096]]. Action = [[ 0.03317222  0.13729233 -0.0567338   0.54179573]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 886. State = [[-0.17280969  0.2689341 ]]. Action = [[ 0.08607799 -0.17444594 -0.18685521  0.95442677]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 887. State = [[-0.17155476  0.270032  ]]. Action = [[ 0.09626681  0.06538153 -0.17380673  0.51314783]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 888. State = [[-0.17031685  0.27054676]]. Action = [[-0.0134394  -0.15120631 -0.2106375   0.8401184 ]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 889. State = [[-0.16835664  0.26891747]]. Action = [[ 0.23739713 -0.21477398  0.19445747  0.4606192 ]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 890. State = [[-0.16242842  0.26251343]]. Action = [[-0.05218199  0.09369466  0.07847434 -0.32079315]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 891. State = [[-0.16038123  0.2604293 ]]. Action = [[-0.1938505   0.17739671  0.21433273 -0.8009473 ]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 892. State = [[-0.16247626  0.26350585]]. Action = [[-0.18213229 -0.07937348 -0.18911715  0.43147635]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 893. State = [[-0.1642721  0.2651331]]. Action = [[-0.01451194 -0.14638467  0.15739003 -0.12752962]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 894. State = [[-0.16483334  0.26337594]]. Action = [[ 0.11523139  0.13824919 -0.08613873 -0.81161284]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 895. State = [[-0.1647122  0.2631746]]. Action = [[ 0.22224915 -0.1494812  -0.2292558   0.6020229 ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 896. State = [[-0.16115834  0.25918448]]. Action = [[-0.19955873 -0.05451065  0.04437077  0.37073755]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 897. State = [[-0.16086584  0.25755775]]. Action = [[-0.13799794  0.05259386  0.1178802   0.4078232 ]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 898. State = [[-0.16312706  0.25856838]]. Action = [[-0.19200064  0.15213299  0.17252943  0.5005022 ]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 899. State = [[-0.16851898  0.26356712]]. Action = [[-0.23557639  0.22283664  0.00504449 -0.27694452]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 900. State = [[-0.17647366  0.27269697]]. Action = [[-4.5500696e-04  1.9533071e-01 -2.0201929e-01 -8.9913636e-01]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 901. State = [[-0.18517792  0.28344464]]. Action = [[-0.17681198 -0.0806779   0.16544688  0.275151  ]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 902. State = [[-0.19232975  0.28806964]]. Action = [[-0.15299919 -0.1300892  -0.17704567  0.88629496]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 903. State = [[-0.19922131  0.28920665]]. Action = [[ 0.21542946  0.21131557 -0.09042335  0.20598793]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 904. State = [[-0.20258981  0.29365477]]. Action = [[ 0.16702265  0.01037061  0.11651868 -0.7702513 ]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 905. State = [[-0.20224115  0.2943839 ]]. Action = [[-0.09263763 -0.18283056  0.11507261 -0.44761992]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 906. State = [[-0.20148586  0.29253468]]. Action = [[-0.03840542 -0.21413794 -0.12750106 -0.73878175]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 907. State = [[-0.2016237   0.28693208]]. Action = [[-0.18758096  0.17129523 -0.21974388 -0.2727434 ]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 908. State = [[-0.20606357  0.28754163]]. Action = [[-0.05014285  0.2282627   0.11791706  0.05369401]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 909. State = [[-0.21138977  0.2927674 ]]. Action = [[-0.11635035 -0.21067108  0.16225201 -0.8221792 ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 910. State = [[-0.21560991  0.2909731 ]]. Action = [[ 0.16076207 -0.1584664   0.10400695  0.7732041 ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 911. State = [[-0.21642329  0.28504592]]. Action = [[-0.13624144 -0.06431602 -0.23381081 -0.0839811 ]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 912. State = [[-0.21863309  0.28117758]]. Action = [[-0.22931154  0.22175854  0.07610545 -0.77776843]]. Reward = [0.]
Curr episode timestep = 192
Current timestep = 913. State = [[-0.22419494  0.28481242]]. Action = [[ 0.14065751 -0.13664496 -0.22163789 -0.28433365]]. Reward = [0.]
Curr episode timestep = 193
Current timestep = 914. State = [[-0.22661632  0.2825999 ]]. Action = [[0.13073003 0.17773187 0.20863104 0.7344686 ]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 915. State = [[-0.22741902  0.28413174]]. Action = [[-0.20912702  0.09349996 -0.2367506  -0.9536239 ]]. Reward = [0.]
Curr episode timestep = 195
Current timestep = 916. State = [[-0.23193449  0.28895402]]. Action = [[-0.1476695   0.18576846 -0.13657823  0.14149725]]. Reward = [0.]
Curr episode timestep = 196
Current timestep = 917. State = [[-0.23874898  0.29676163]]. Action = [[ 0.22619599  0.19874853 -0.18467668  0.7682991 ]]. Reward = [0.]
Curr episode timestep = 197
Current timestep = 918. State = [[-0.24192894  0.30364838]]. Action = [[ 0.23653072  0.11075717 -0.08467534  0.91422653]]. Reward = [0.]
Curr episode timestep = 198
Current timestep = 919. State = [[-0.23996781  0.30932087]]. Action = [[-0.10211572 -0.07531849 -0.08023262  0.86936116]]. Reward = [0.]
Curr episode timestep = 199
Current timestep = 920. State = [[-0.23861624  0.3118315 ]]. Action = [[-0.05297485 -0.20328796  0.04043314  0.657387  ]]. Reward = [0.]
Curr episode timestep = 200
Current timestep = 921. State = [[-0.23761208  0.31123906]]. Action = [[-0.17699024  0.08935753 -0.17160517 -0.86789024]]. Reward = [0.]
Curr episode timestep = 201
Current timestep = 922. State = [[-0.2367138   0.31043613]]. Action = [[ 0.04291722 -0.16040574 -0.2421334  -0.9599075 ]]. Reward = [0.]
Curr episode timestep = 202
Current timestep = 923. State = [[-0.23494971  0.307403  ]]. Action = [[-0.23017529  0.06878093  0.14325333 -0.19448805]]. Reward = [0.]
Curr episode timestep = 203
Current timestep = 924. State = [[-0.23779656  0.30774403]]. Action = [[-0.20116307  0.06425533  0.01291025 -0.58391273]]. Reward = [0.]
Curr episode timestep = 204
Current timestep = 925. State = [[-0.24416544  0.30931228]]. Action = [[-0.18461421  0.15768737 -0.23952238  0.85896325]]. Reward = [0.]
Curr episode timestep = 205
Current timestep = 926. State = [[-0.24883087  0.30923438]]. Action = [[ 0.1789433  -0.19401534 -0.08231963 -0.78346145]]. Reward = [0.]
Curr episode timestep = 206
Current timestep = 927. State = [[-0.24874325  0.30446044]]. Action = [[-0.21624644  0.08401266  0.19609481  0.46710765]]. Reward = [0.]
Curr episode timestep = 207
Current timestep = 928. State = [[-0.25320783  0.30341864]]. Action = [[-0.10342021 -0.15865743 -0.15324365 -0.00130719]]. Reward = [0.]
Curr episode timestep = 208
Current timestep = 929. State = [[-0.25687778  0.30008242]]. Action = [[-0.04693633 -0.02066326 -0.09510802  0.8625636 ]]. Reward = [0.]
Curr episode timestep = 209
Current timestep = 930. State = [[-0.26049215  0.2972386 ]]. Action = [[-0.0159325  -0.1260794   0.17514119 -0.77089876]]. Reward = [0.]
Curr episode timestep = 210
Current timestep = 931. State = [[-0.2636127  0.2914304]]. Action = [[ 0.07770258 -0.03290659 -0.11996582  0.21175039]]. Reward = [0.]
Curr episode timestep = 211
Current timestep = 932. State = [[-0.26552862  0.2860543 ]]. Action = [[ 0.08385453  0.01931652 -0.0903627  -0.56143486]]. Reward = [0.]
Curr episode timestep = 212
Current timestep = 933. State = [[-0.26577792  0.2832116 ]]. Action = [[-0.10964212  0.22702461  0.14365792  0.18975425]]. Reward = [0.]
Curr episode timestep = 213
Current timestep = 934. State = [[-0.2687429  0.2869421]]. Action = [[-0.11763741  0.16002983 -0.22140794 -0.16222435]]. Reward = [0.]
Curr episode timestep = 214
Current timestep = 935. State = [[-0.27403829  0.29261392]]. Action = [[-0.05482441 -0.17720059 -0.13783269  0.5080831 ]]. Reward = [0.]
Curr episode timestep = 215
Current timestep = 936. State = [[-0.27660504  0.29317027]]. Action = [[ 0.20539111 -0.07845622  0.1943621  -0.24956918]]. Reward = [0.]
Curr episode timestep = 216
Current timestep = 937. State = [[-0.27444166  0.29101905]]. Action = [[ 0.16853607 -0.05077004  0.1795564   0.13333046]]. Reward = [0.]
Curr episode timestep = 217
Current timestep = 938. State = [[-0.2703621   0.28781253]]. Action = [[ 0.22651935 -0.06172428  0.06677875 -0.00889778]]. Reward = [0.]
Curr episode timestep = 218
Current timestep = 939. State = [[-0.2636341   0.28319392]]. Action = [[ 0.09780723  0.11695647 -0.18969667  0.23638201]]. Reward = [0.]
Curr episode timestep = 219
Current timestep = 940. State = [[-0.25813195  0.2812179 ]]. Action = [[ 0.1174795  -0.1625591   0.08048272 -0.25175196]]. Reward = [0.]
Curr episode timestep = 220
Current timestep = 941. State = [[-0.25178835  0.27628654]]. Action = [[ 0.08587664 -0.1751786  -0.05254886  0.7545315 ]]. Reward = [0.]
Curr episode timestep = 221
Current timestep = 942. State = [[-0.24408476  0.26883584]]. Action = [[-0.09535001 -0.08331245 -0.22545208 -0.686966  ]]. Reward = [0.]
Curr episode timestep = 222
Current timestep = 943. State = [[-0.2397504   0.26373303]]. Action = [[-0.23550728 -0.01865362 -0.1790887  -0.37533522]]. Reward = [0.]
Curr episode timestep = 223
Current timestep = 944. State = [[-0.24088675  0.26126343]]. Action = [[-0.0788095   0.1441775   0.09409243  0.45049608]]. Reward = [0.]
Curr episode timestep = 224
Current timestep = 945. State = [[-0.24319492  0.26327208]]. Action = [[-0.0455673   0.06424606 -0.10883528  0.05540788]]. Reward = [0.]
Curr episode timestep = 225
Current timestep = 946. State = [[-0.24552175  0.2656788 ]]. Action = [[-0.14550501 -0.16952041  0.08915001  0.8687172 ]]. Reward = [0.]
Curr episode timestep = 226
Current timestep = 947. State = [[-0.24865371  0.26399556]]. Action = [[-0.24214584 -0.07825054  0.21615672 -0.3604287 ]]. Reward = [0.]
Curr episode timestep = 227
Current timestep = 948. State = [[-0.2569117   0.26052263]]. Action = [[-0.19535851 -0.14265999  0.09030205  0.00704157]]. Reward = [0.]
Curr episode timestep = 228
Current timestep = 949. State = [[-0.26708698  0.2544023 ]]. Action = [[ 0.13371983 -0.14386003  0.23689696 -0.7914536 ]]. Reward = [0.]
Curr episode timestep = 229
Current timestep = 950. State = [[-0.27290702  0.24453509]]. Action = [[-0.08852038 -0.1760227  -0.18480861 -0.01750034]]. Reward = [0.]
Curr episode timestep = 230
Current timestep = 951. State = [[-0.27817518  0.23444907]]. Action = [[-0.20779896  0.07225284 -0.06709048  0.01368797]]. Reward = [0.]
Curr episode timestep = 231
Current timestep = 952. State = [[-0.28649312  0.2304192 ]]. Action = [[-0.17764984 -0.02351202  0.2332164   0.6703303 ]]. Reward = [0.]
Curr episode timestep = 232
Current timestep = 953. State = [[-0.29628614  0.22709928]]. Action = [[-0.18818021 -0.08098057 -0.14579348  0.44332218]]. Reward = [0.]
Curr episode timestep = 233
Current timestep = 954. State = [[-0.30756763  0.22291213]]. Action = [[ 0.01834583 -0.1456961  -0.13071407  0.6189277 ]]. Reward = [0.]
Curr episode timestep = 234
Current timestep = 955. State = [[-0.31479022  0.21583755]]. Action = [[ 0.09805375 -0.04089805 -0.07954028  0.4220885 ]]. Reward = [0.]
Curr episode timestep = 235
Current timestep = 956. State = [[-0.31914407  0.20967712]]. Action = [[-0.22162618  0.09150431  0.12355828  0.86577487]]. Reward = [0.]
Curr episode timestep = 236
Current timestep = 957. State = [[-0.325614    0.20907722]]. Action = [[-0.18442458  0.21473193  0.03712392  0.95170546]]. Reward = [0.]
Curr episode timestep = 237
Current timestep = 958. State = [[-0.33377597  0.21322039]]. Action = [[ 0.15126264 -0.07472609 -0.00505222 -0.21894777]]. Reward = [0.]
Curr episode timestep = 238
Current timestep = 959. State = [[-0.3377156   0.21345186]]. Action = [[ 0.1612789  -0.10035282 -0.01522785  0.6418127 ]]. Reward = [0.]
Curr episode timestep = 239
Current timestep = 960. State = [[-0.33653378  0.21072549]]. Action = [[ 0.0425505  -0.20131253  0.06558827 -0.30069733]]. Reward = [0.]
Curr episode timestep = 240
Current timestep = 961. State = [[-0.33464476  0.20471469]]. Action = [[-0.11368635  0.20927471 -0.12491569  0.12485802]]. Reward = [0.]
Curr episode timestep = 241
Current timestep = 962. State = [[-0.33569285  0.20605361]]. Action = [[ 0.09922063  0.17101818 -0.14276154 -0.56366307]]. Reward = [0.]
Curr episode timestep = 242
Current timestep = 963. State = [[-0.33611473  0.20855452]]. Action = [[ 0.20744067 -0.02335358  0.21334112 -0.90267336]]. Reward = [0.]
Curr episode timestep = 243
Current timestep = 964. State = [[-0.33413514  0.20920464]]. Action = [[-0.20570453 -0.19940145 -0.07361931  0.5248803 ]]. Reward = [0.]
Curr episode timestep = 244
Current timestep = 965. State = [[-0.33455774  0.2074259 ]]. Action = [[ 0.10004705  0.0960952  -0.1959427   0.6554482 ]]. Reward = [0.]
Curr episode timestep = 245
Current timestep = 966. State = [[-0.33410808  0.20780453]]. Action = [[ 0.02463409  0.24013501  0.01857406 -0.32956946]]. Reward = [0.]
Curr episode timestep = 246
Current timestep = 967. State = [[-0.33382186  0.21135718]]. Action = [[ 0.06383985 -0.15491244 -0.19530486  0.13302863]]. Reward = [0.]
Curr episode timestep = 247
Current timestep = 968. State = [[-0.33256572  0.2109291 ]]. Action = [[ 0.1313937   0.02275884 -0.21178277  0.53749955]]. Reward = [0.]
Curr episode timestep = 248
Current timestep = 969. State = [[-0.329871    0.21031936]]. Action = [[-0.04277039 -0.17979933 -0.0225188  -0.7375072 ]]. Reward = [0.]
Curr episode timestep = 249
Current timestep = 970. State = [[-0.32678896  0.20762357]]. Action = [[ 0.00148568 -0.06152718  0.09829631 -0.11765903]]. Reward = [0.]
Curr episode timestep = 250
Current timestep = 971. State = [[-0.32429463  0.20464425]]. Action = [[ 0.11382675  0.13922715  0.16936171 -0.30527902]]. Reward = [0.]
Curr episode timestep = 251
Current timestep = 972. State = [[-0.32253903  0.20401193]]. Action = [[ 0.03316879  0.03849295 -0.22896749  0.18292308]]. Reward = [0.]
Curr episode timestep = 252
Current timestep = 973. State = [[-0.32098866  0.2042149 ]]. Action = [[ 0.03527647 -0.18562421  0.0618827  -0.3949238 ]]. Reward = [0.]
Curr episode timestep = 253
Current timestep = 974. State = [[-0.31794634  0.20139319]]. Action = [[-0.09540646  0.19613034 -0.17549272  0.9384825 ]]. Reward = [0.]
Curr episode timestep = 254
Current timestep = 975. State = [[-0.31900293  0.20331717]]. Action = [[-0.24071512  0.18910635  0.08997437  0.72797   ]]. Reward = [0.]
Curr episode timestep = 255
Current timestep = 976. State = [[-0.32371855  0.20864366]]. Action = [[-0.21735865 -0.06688589 -0.18401304  0.2785777 ]]. Reward = [0.]
Curr episode timestep = 256
Current timestep = 977. State = [[-0.33007428  0.21347846]]. Action = [[ 0.01828972  0.23077208 -0.0986032  -0.8106418 ]]. Reward = [0.]
Curr episode timestep = 257
Current timestep = 978. State = [[-0.33583713  0.21995796]]. Action = [[ 0.18241087  0.0881497  -0.10778476 -0.5045368 ]]. Reward = [0.]
Curr episode timestep = 258
Current timestep = 979. State = [[-0.3370151   0.22510785]]. Action = [[-0.10658407 -0.20103492 -0.10716844 -0.16634452]]. Reward = [0.]
Curr episode timestep = 259
Current timestep = 980. State = [[-0.33772883  0.22550958]]. Action = [[ 0.06983685 -0.20536986  0.23856166  0.91729605]]. Reward = [0.]
Curr episode timestep = 260
Current timestep = 981. State = [[-0.33633682  0.22174802]]. Action = [[-0.18774708  0.13821155 -0.24069889 -0.71219164]]. Reward = [0.]
Curr episode timestep = 261
Current timestep = 982. State = [[-0.33827782  0.22277404]]. Action = [[-0.19202559  0.15309319 -0.22441554 -0.6755892 ]]. Reward = [0.]
Curr episode timestep = 262
Current timestep = 983. State = [[-0.34394887  0.22726044]]. Action = [[-0.03079604 -0.1695269   0.05244991 -0.78058195]]. Reward = [0.]
Curr episode timestep = 263
Current timestep = 984. State = [[-0.34846696  0.22631691]]. Action = [[-0.03931347 -0.22257257 -0.18161005  0.64595914]]. Reward = [0.]
Curr episode timestep = 264
Current timestep = 985. State = [[-0.35091004  0.2213161 ]]. Action = [[ 0.18562382  0.07336396 -0.01589376 -0.68591404]]. Reward = [0.]
Curr episode timestep = 265
Current timestep = 986. State = [[-0.34973815  0.21964225]]. Action = [[-0.00271766  0.21730483  0.24534059 -0.36693358]]. Reward = [0.]
Curr episode timestep = 266
Current timestep = 987. State = [[-0.3501763   0.22197253]]. Action = [[ 0.03903392  0.21709043  0.19055438 -0.02799451]]. Reward = [0.]
Curr episode timestep = 267
Current timestep = 988. State = [[-0.34983838  0.22740583]]. Action = [[ 0.19892234 -0.20158488 -0.00713323  0.18240952]]. Reward = [0.]
Curr episode timestep = 268
Current timestep = 989. State = [[-0.3470279   0.22670287]]. Action = [[-0.12760246 -0.03107896 -0.1595437   0.29345584]]. Reward = [0.]
Curr episode timestep = 269
Current timestep = 990. State = [[-0.34709322  0.22658256]]. Action = [[-0.1508388   0.06090024  0.12718195 -0.48037875]]. Reward = [0.]
Curr episode timestep = 270
Current timestep = 991. State = [[-0.3490976   0.22827323]]. Action = [[-0.1740332  -0.04517913 -0.08406901  0.62845004]]. Reward = [0.]
Curr episode timestep = 271
Current timestep = 992. State = [[-0.35308263  0.22975431]]. Action = [[-0.23166038  0.21048704  0.03599691  0.5839015 ]]. Reward = [0.]
Curr episode timestep = 272
Current timestep = 993. State = [[-0.36058295  0.23608606]]. Action = [[ 0.18030208 -0.06184605  0.12696475  0.7429726 ]]. Reward = [0.]
Curr episode timestep = 273
Current timestep = 994. State = [[-0.3630384  0.2383098]]. Action = [[0.04157284 0.15920976 0.1154004  0.47386217]]. Reward = [0.]
Curr episode timestep = 274
Current timestep = 995. State = [[-0.3653057   0.24187583]]. Action = [[-0.08554736  0.24392009  0.23426557 -0.8219265 ]]. Reward = [0.]
Curr episode timestep = 275
Current timestep = 996. State = [[-0.3698412   0.25023878]]. Action = [[-0.20301695  0.17423636  0.02572337 -0.17490011]]. Reward = [0.]
Curr episode timestep = 276
Current timestep = 997. State = [[-0.37743506  0.25980502]]. Action = [[ 0.0476276   0.10903993  0.0650273  -0.6298013 ]]. Reward = [0.]
Curr episode timestep = 277
Current timestep = 998. State = [[-0.38259187  0.26644334]]. Action = [[ 0.23152709 -0.22518748  0.00877574  0.5316094 ]]. Reward = [0.]
Curr episode timestep = 278
Current timestep = 999. State = [[-0.38098556  0.2664456 ]]. Action = [[ 0.11805156 -0.05884787 -0.15710485 -0.4036023 ]]. Reward = [0.]
Curr episode timestep = 279
Current timestep = 1000. State = [[-0.3773391   0.26477984]]. Action = [[ 0.13131773  0.13777733 -0.03789848 -0.47076684]]. Reward = [0.]
Curr episode timestep = 280
Current timestep = 1001. State = [[-0.3731554   0.26571324]]. Action = [[ 0.20234951 -0.17066804  0.02902859  0.27719104]]. Reward = [0.]
Curr episode timestep = 281
Current timestep = 1002. State = [[-0.36652365  0.26274416]]. Action = [[-0.01702547  0.18627149  0.20471835  0.35286736]]. Reward = [0.]
Curr episode timestep = 282
Current timestep = 1003. State = [[-0.36082676  0.26679525]]. Action = [[ 0.02328286  0.14971232 -0.14059186  0.18386614]]. Reward = [0.]
Curr episode timestep = 283
Current timestep = 1004. State = [[-0.35615683  0.2730722 ]]. Action = [[ 0.14819533  0.10223505 -0.03253207  0.8076632 ]]. Reward = [0.]
Curr episode timestep = 284
Current timestep = 1005. State = [[-0.34966645  0.2794646 ]]. Action = [[ 0.09451896 -0.21416843  0.16577083 -0.3581952 ]]. Reward = [0.]
Curr episode timestep = 285
Current timestep = 1006. State = [[-0.34364977  0.27767456]]. Action = [[ 0.23562992 -0.19475931  0.03391215  0.6948848 ]]. Reward = [0.]
Curr episode timestep = 286
Current timestep = 1007. State = [[-0.33492643  0.27170408]]. Action = [[ 0.1904194  -0.18629406 -0.1708188  -0.00545359]]. Reward = [0.]
Curr episode timestep = 287
Current timestep = 1008. State = [[-0.32414052  0.26378006]]. Action = [[0.03038064 0.07962927 0.20704347 0.5958667 ]]. Reward = [0.]
Curr episode timestep = 288
Current timestep = 1009. State = [[-0.314356   0.2595945]]. Action = [[ 0.01657575 -0.22839263 -0.09651619  0.5461323 ]]. Reward = [0.]
Curr episode timestep = 289
Current timestep = 1010. State = [[-0.307104    0.25260583]]. Action = [[ 0.11497304  0.0128437   0.21267971 -0.02948344]]. Reward = [0.]
Curr episode timestep = 290
Current timestep = 1011. State = [[-0.30035853  0.24790654]]. Action = [[-0.1155204   0.04145369 -0.24017477  0.46637404]]. Reward = [0.]
Curr episode timestep = 291
Current timestep = 1012. State = [[-0.2982113   0.24682789]]. Action = [[ 0.14615095  0.05038029  0.10273415 -0.3982036 ]]. Reward = [0.]
Curr episode timestep = 292
Current timestep = 1013. State = [[-0.2951897   0.24670893]]. Action = [[0.19236785 0.01866594 0.1473687  0.35851288]]. Reward = [0.]
Curr episode timestep = 293
Current timestep = 1014. State = [[-0.28842908  0.24704413]]. Action = [[-0.06758136  0.00230816  0.04916647  0.8081231 ]]. Reward = [0.]
Curr episode timestep = 294
Current timestep = 1015. State = [[-0.28443903  0.24753512]]. Action = [[-0.15901776 -0.12870786  0.20390815 -0.6588587 ]]. Reward = [0.]
Curr episode timestep = 295
Current timestep = 1016. State = [[-0.2835788   0.24597178]]. Action = [[-0.00419928 -0.04927096  0.06118271 -0.18185407]]. Reward = [0.]
Curr episode timestep = 296
Current timestep = 1017. State = [[-0.28276014  0.24413796]]. Action = [[ 0.0776594   0.23975474 -0.04743093 -0.55746174]]. Reward = [0.]
Curr episode timestep = 297
Current timestep = 1018. State = [[-0.282783    0.24736631]]. Action = [[0.04155177 0.18903372 0.22306216 0.9494474 ]]. Reward = [0.]
Curr episode timestep = 298
Current timestep = 1019. State = [[-0.2821763  0.2530665]]. Action = [[ 0.01214218 -0.0163708  -0.23787601 -0.29098165]]. Reward = [0.]
Curr episode timestep = 299
Current timestep = 1020. State = [[-0.2812298  0.258198 ]]. Action = [[-0.12827958  0.21960926 -0.21928513 -0.7962294 ]]. Reward = [0.]
Curr episode timestep = 300
Current timestep = 1021. State = [[-0.28254035  0.26613435]]. Action = [[-0.08311567  0.01605213 -0.02531219  0.20939755]]. Reward = [0.]
Curr episode timestep = 301
Current timestep = 1022. State = [[-0.2849695   0.27304032]]. Action = [[-0.0764949   0.13728714 -0.1857861  -0.20215982]]. Reward = [0.]
Curr episode timestep = 302
Current timestep = 1023. State = [[-0.28899398  0.28039113]]. Action = [[-0.21370858  0.1529674   0.01108533  0.84559214]]. Reward = [0.]
Curr episode timestep = 303
Current timestep = 1024. State = [[-0.2965828  0.2886528]]. Action = [[-0.22805375  0.16042936 -0.21704918  0.5093466 ]]. Reward = [0.]
Curr episode timestep = 304
Current timestep = 1025. State = [[-0.3066086  0.2980061]]. Action = [[ 0.03750306  0.07344717  0.12776625 -0.62178504]]. Reward = [0.]
Curr episode timestep = 305
Current timestep = 1026. State = [[-0.31481215  0.3061269 ]]. Action = [[-0.20394039  0.14487052  0.08080783 -0.15879798]]. Reward = [0.]
Curr episode timestep = 306
Current timestep = 1027. State = [[-0.32371858  0.3144971 ]]. Action = [[-0.10879079  0.03969878  0.19996673 -0.03502661]]. Reward = [0.]
Curr episode timestep = 307
Current timestep = 1028. State = [[-0.3324332   0.32166994]]. Action = [[-0.06468774 -0.09880206 -0.18348812  0.93752134]]. Reward = [0.]
Curr episode timestep = 308
Current timestep = 1029. State = [[-0.33742094  0.32518718]]. Action = [[-0.08142194  0.11930642 -0.14270577  0.8252728 ]]. Reward = [0.]
Curr episode timestep = 309
Current timestep = 1030. State = [[-0.34038776  0.3272465 ]]. Action = [[ 0.12561625  0.02574912 -0.02242577 -0.2795899 ]]. Reward = [0.]
Curr episode timestep = 310
Current timestep = 1031. State = [[-0.341982   0.3284326]]. Action = [[ 0.01583558 -0.16354047 -0.1488973  -0.88581157]]. Reward = [0.]
Curr episode timestep = 311
Current timestep = 1032. State = [[-0.34271264  0.32592186]]. Action = [[ 0.19762611 -0.14102273 -0.07106151 -0.61221004]]. Reward = [0.]
Curr episode timestep = 312
Current timestep = 1033. State = [[-0.34300083  0.324402  ]]. Action = [[0.17442542 0.11654183 0.03827024 0.62689495]]. Reward = [0.]
Curr episode timestep = 313
Current timestep = 1034. State = [[-0.34307948  0.32351685]]. Action = [[-0.11227843 -0.02563378  0.08906502  0.05858505]]. Reward = [0.]
Curr episode timestep = 314
Current timestep = 1035. State = [[-0.34334627  0.32296592]]. Action = [[-0.12606175  0.09797513  0.12033457  0.8809724 ]]. Reward = [0.]
Curr episode timestep = 315
Current timestep = 1036. State = [[-0.34348065  0.3225812 ]]. Action = [[-0.01018812 -0.15408248 -0.13981272  0.6206653 ]]. Reward = [0.]
Curr episode timestep = 316
Current timestep = 1037. State = [[-0.3431934   0.31940553]]. Action = [[ 0.20072278  0.08004683 -0.08690536  0.7910192 ]]. Reward = [0.]
Curr episode timestep = 317
Current timestep = 1038. State = [[-0.3427651   0.31735748]]. Action = [[-0.14115696 -0.21845941 -0.20039396 -0.1636703 ]]. Reward = [0.]
Curr episode timestep = 318
Current timestep = 1039. State = [[-0.34448695  0.31104675]]. Action = [[ 1.1559656e-01  1.2817979e-04  1.9857222e-01 -6.2572539e-01]]. Reward = [0.]
Curr episode timestep = 319
Current timestep = 1040. State = [[-0.3454828   0.30487266]]. Action = [[-0.06561746 -0.05214094  0.15161204  0.9915223 ]]. Reward = [0.]
Curr episode timestep = 320
Current timestep = 1041. State = [[-0.34585348  0.299864  ]]. Action = [[ 0.09147117 -0.19235948 -0.190946    0.36403978]]. Reward = [0.]
Curr episode timestep = 321
Current timestep = 1042. State = [[-0.34555995  0.29185995]]. Action = [[-0.13038564  0.04576936 -0.14465757 -0.40016252]]. Reward = [0.]
Curr episode timestep = 322
Current timestep = 1043. State = [[-0.34735993  0.2891318 ]]. Action = [[-0.24655849  0.07228917  0.02776009  0.8729677 ]]. Reward = [0.]
Curr episode timestep = 323
Current timestep = 1044. State = [[-0.35269582  0.28988865]]. Action = [[-0.15252343 -0.14325635 -0.20638591  0.12012076]]. Reward = [0.]
Curr episode timestep = 324
Current timestep = 1045. State = [[-0.3603186   0.28597468]]. Action = [[-0.12937364 -0.20760453  0.16033775  0.64686227]]. Reward = [0.]
Curr episode timestep = 325
Current timestep = 1046. State = [[-0.36939183  0.27774218]]. Action = [[ 0.19826233  0.16128168 -0.14813635  0.6303941 ]]. Reward = [0.]
Curr episode timestep = 326
Current timestep = 1047. State = [[-0.37200105  0.27615565]]. Action = [[-0.08645502  0.16715896  0.09082186 -0.18358362]]. Reward = [0.]
Curr episode timestep = 327
Current timestep = 1048. State = [[-0.3749819   0.27877763]]. Action = [[-0.21476941 -0.166478   -0.18969446 -0.90994346]]. Reward = [0.]
Curr episode timestep = 328
Current timestep = 1049. State = [[-0.37956664  0.27833277]]. Action = [[-0.13986365 -0.20110476  0.08981013 -0.00264466]]. Reward = [0.]
Curr episode timestep = 329
Current timestep = 1050. State = [[-0.3869869  0.2719719]]. Action = [[-0.18147051 -0.01615039 -0.18478437  0.07968342]]. Reward = [0.]
Curr episode timestep = 330
Current timestep = 1051. State = [[-0.3934041   0.26764464]]. Action = [[-0.00601149  0.21221098  0.04666978  0.7457857 ]]. Reward = [0.]
Curr episode timestep = 331
Current timestep = 1052. State = [[-0.39791197  0.2703217 ]]. Action = [[0.2127403  0.18732554 0.05543548 0.8088434 ]]. Reward = [0.]
Curr episode timestep = 332
Current timestep = 1053. State = [[-0.3979525  0.2735312]]. Action = [[ 0.14863887 -0.16910179  0.19596103  0.77196836]]. Reward = [0.]
Curr episode timestep = 333
Current timestep = 1054. State = [[-0.39527863  0.27303454]]. Action = [[0.07612145 0.05590785 0.10451975 0.65170705]]. Reward = [0.]
Curr episode timestep = 334
Current timestep = 1055. State = [[-0.39308313  0.272241  ]]. Action = [[ 0.15015152 -0.17930819  0.22244048  0.688532  ]]. Reward = [0.]
Curr episode timestep = 335
Current timestep = 1056. State = [[-0.38848963  0.2679559 ]]. Action = [[ 0.16492146  0.00351825 -0.16042045  0.402519  ]]. Reward = [0.]
Curr episode timestep = 336
Current timestep = 1057. State = [[-0.38187903  0.26373565]]. Action = [[ 0.06362721 -0.10911134  0.02458724 -0.35823047]]. Reward = [0.]
Curr episode timestep = 337
Current timestep = 1058. State = [[-0.37497222  0.2587294 ]]. Action = [[-0.13848096 -0.12097916 -0.02695961  0.11887813]]. Reward = [0.]
Curr episode timestep = 338
Current timestep = 1059. State = [[-0.3704468   0.25530547]]. Action = [[ 0.05726457 -0.08494051 -0.04202037 -0.68845206]]. Reward = [0.]
Curr episode timestep = 339
Current timestep = 1060. State = [[-0.36569306  0.25147748]]. Action = [[ 0.02042845  0.04316869 -0.23321436  0.32079887]]. Reward = [0.]
Curr episode timestep = 340
Current timestep = 1061. State = [[-0.36338362  0.24973458]]. Action = [[-0.06721227  0.10200229 -0.24809684  0.8367491 ]]. Reward = [0.]
Curr episode timestep = 341
Current timestep = 1062. State = [[-0.36379746  0.2506454 ]]. Action = [[-0.2443775   0.05004475  0.00840959  0.10104513]]. Reward = [0.]
Curr episode timestep = 342
Current timestep = 1063. State = [[-0.3676248  0.2540059]]. Action = [[-0.03000219  0.23437876 -0.24116035  0.01327705]]. Reward = [0.]
Curr episode timestep = 343
Current timestep = 1064. State = [[-0.37185302  0.26002166]]. Action = [[ 0.04135832 -0.05421223  0.11248511  0.04415107]]. Reward = [0.]
Curr episode timestep = 344
Current timestep = 1065. State = [[-0.3746213   0.26263478]]. Action = [[-0.02615754 -0.05571602  0.1056017  -0.03990287]]. Reward = [0.]
Curr episode timestep = 345
Current timestep = 1066. State = [[-0.37618715  0.26416746]]. Action = [[-0.18808708  0.19667107  0.06164339  0.46201396]]. Reward = [0.]
Curr episode timestep = 346
Current timestep = 1067. State = [[-0.38166884  0.26889488]]. Action = [[ 0.00084779  0.04149589 -0.1929097   0.23801208]]. Reward = [0.]
Curr episode timestep = 347
Current timestep = 1068. State = [[-0.386158   0.2726351]]. Action = [[-0.21741787  0.05386609 -0.14498991 -0.5372959 ]]. Reward = [0.]
Curr episode timestep = 348
Current timestep = 1069. State = [[-0.3885381  0.2746235]]. Action = [[ 0.2019453  -0.23678245  0.18283486 -0.6034696 ]]. Reward = [0.]
Curr episode timestep = 349
Current timestep = 1070. State = [[-0.38579774  0.27183458]]. Action = [[ 0.05739582  0.05830288 -0.16197552 -0.43760931]]. Reward = [0.]
Curr episode timestep = 350
Current timestep = 1071. State = [[-0.3834691   0.27053422]]. Action = [[-0.19035213  0.20484221 -0.00394334 -0.60928094]]. Reward = [0.]
Curr episode timestep = 351
Current timestep = 1072. State = [[-0.38219458  0.26963857]]. Action = [[ 0.014146   -0.01700249 -0.2208419   0.7794802 ]]. Reward = [0.]
Curr episode timestep = 352
Current timestep = 1073. State = [[-0.38122943  0.2690107 ]]. Action = [[-0.19372065 -0.13347432 -0.11207619  0.2872324 ]]. Reward = [0.]
Curr episode timestep = 353
Current timestep = 1074. State = [[-0.38081375  0.26860118]]. Action = [[-0.14547071 -0.04259996  0.16203111  0.5618024 ]]. Reward = [0.]
Curr episode timestep = 354
Current timestep = 1075. State = [[-0.38064364  0.2685671 ]]. Action = [[ 0.07796502  0.1486125  -0.11790058 -0.7777116 ]]. Reward = [0.]
Curr episode timestep = 355
Current timestep = 1076. State = [[-0.37895787  0.27109683]]. Action = [[ 0.20603418  0.00788206 -0.14686769 -0.16653073]]. Reward = [0.]
Curr episode timestep = 356
Current timestep = 1077. State = [[-0.37379783  0.27274737]]. Action = [[-0.22565319 -0.06527576  0.22950178 -0.54175377]]. Reward = [0.]
Curr episode timestep = 357
Current timestep = 1078. State = [[-0.3692032   0.27410644]]. Action = [[ 0.125126   -0.23134354 -0.15502352 -0.22225279]]. Reward = [0.]
Curr episode timestep = 358
Current timestep = 1079. State = [[-0.36284178  0.26960006]]. Action = [[ 0.19186515 -0.16176052 -0.15426208  0.89402294]]. Reward = [0.]
Curr episode timestep = 359
Current timestep = 1080. State = [[-0.35362357  0.262608  ]]. Action = [[ 0.11905742  0.14459571 -0.061258    0.32668757]]. Reward = [0.]
Curr episode timestep = 360
Current timestep = 1081. State = [[-0.34477547  0.26056343]]. Action = [[ 0.21290326 -0.0283668   0.2289294  -0.59932595]]. Reward = [0.]
Curr episode timestep = 361
Current timestep = 1082. State = [[-0.33370873  0.2597116 ]]. Action = [[-0.1698158   0.19773895  0.07577333  0.29854882]]. Reward = [0.]
Curr episode timestep = 362
Current timestep = 1083. State = [[-0.32889116  0.26343244]]. Action = [[ 0.19601521 -0.21571505 -0.17916733  0.8967526 ]]. Reward = [0.]
Curr episode timestep = 363
Current timestep = 1084. State = [[-0.3238846  0.2604376]]. Action = [[ 0.00165963  0.03474206 -0.14326125  0.59369326]]. Reward = [0.]
Curr episode timestep = 364
Current timestep = 1085. State = [[-0.31939328  0.26020795]]. Action = [[ 0.03024393  0.1354276  -0.24093974 -0.38750565]]. Reward = [0.]
Curr episode timestep = 365
Current timestep = 1086. State = [[-0.31647548  0.26337892]]. Action = [[0.06560645 0.11192197 0.11714777 0.7709919 ]]. Reward = [0.]
Curr episode timestep = 366
Current timestep = 1087. State = [[-0.31244048  0.26814365]]. Action = [[ 0.12364307 -0.20980173  0.13254988  0.9149883 ]]. Reward = [0.]
Curr episode timestep = 367
Current timestep = 1088. State = [[-0.30735832  0.26561764]]. Action = [[ 0.12389579 -0.06066027 -0.1676094   0.85753965]]. Reward = [0.]
Curr episode timestep = 368
Current timestep = 1089. State = [[-0.3011052   0.26294366]]. Action = [[-0.21450143  0.15803528 -0.21634914 -0.87272733]]. Reward = [0.]
Curr episode timestep = 369
Current timestep = 1090. State = [[-0.2995645  0.2647259]]. Action = [[0.16663712 0.01516971 0.19228786 0.1477673 ]]. Reward = [0.]
Curr episode timestep = 370
Current timestep = 1091. State = [[-0.29716918  0.26565135]]. Action = [[-0.08827794 -0.15949623  0.22612205  0.10697913]]. Reward = [0.]
Curr episode timestep = 371
Current timestep = 1092. State = [[-0.29604548  0.26424915]]. Action = [[ 0.20277977 -0.05920269  0.01225218  0.57075036]]. Reward = [0.]
Curr episode timestep = 372
Current timestep = 1093. State = [[-0.2918944  0.2604247]]. Action = [[-0.06512153 -0.10270409  0.06448108 -0.7976465 ]]. Reward = [0.]
Curr episode timestep = 373
Current timestep = 1094. State = [[-0.2883344  0.2566275]]. Action = [[-0.00900836  0.11994243  0.14499587 -0.9676166 ]]. Reward = [0.]
Curr episode timestep = 374
Current timestep = 1095. State = [[-0.28776494  0.2567467 ]]. Action = [[-0.23490691  0.00749215 -0.05809399 -0.34907818]]. Reward = [0.]
Curr episode timestep = 375
Current timestep = 1096. State = [[-0.28984228  0.2583205 ]]. Action = [[0.09657168 0.0612427  0.05577415 0.2724774 ]]. Reward = [0.]
Curr episode timestep = 376
Current timestep = 1097. State = [[-0.29076618  0.25960883]]. Action = [[-0.18459871  0.21157539  0.24274057 -0.6694122 ]]. Reward = [0.]
Curr episode timestep = 377
Current timestep = 1098. State = [[-0.29479128  0.2660243 ]]. Action = [[0.15652329 0.10952246 0.22025591 0.7012501 ]]. Reward = [0.]
Curr episode timestep = 378
Current timestep = 1099. State = [[-0.29571116  0.27232504]]. Action = [[-0.08254695  0.16594973 -0.18566886  0.5943165 ]]. Reward = [0.]
Curr episode timestep = 379
Current timestep = 1100. State = [[-0.2971542  0.2806147]]. Action = [[-0.00761227 -0.16840671  0.10175884  0.27939343]]. Reward = [0.]
Curr episode timestep = 380
Current timestep = 1101. State = [[-0.2978395   0.28213337]]. Action = [[-0.23802388  0.00441802  0.07184285  0.5510981 ]]. Reward = [0.]
Curr episode timestep = 381
Current timestep = 1102. State = [[-0.30150822  0.285402  ]]. Action = [[-0.01305494  0.0015994  -0.13021691 -0.5177226 ]]. Reward = [0.]
Curr episode timestep = 382
Current timestep = 1103. State = [[-0.30386448  0.28754982]]. Action = [[-0.08294073  0.01532763 -0.19499083 -0.7517671 ]]. Reward = [0.]
Curr episode timestep = 383
Current timestep = 1104. State = [[-0.30655822  0.28992662]]. Action = [[ 0.23907423 -0.11479422 -0.24446169  0.7317164 ]]. Reward = [0.]
Curr episode timestep = 384
Current timestep = 1105. State = [[-0.3047668  0.2875838]]. Action = [[-0.18236439 -0.20222753  0.23846    -0.9900014 ]]. Reward = [0.]
Curr episode timestep = 385
Current timestep = 1106. State = [[-0.3056962   0.28250495]]. Action = [[ 0.09649175  0.06488329 -0.15858592 -0.8509649 ]]. Reward = [0.]
Curr episode timestep = 386
Current timestep = 1107. State = [[-0.30699825  0.27919558]]. Action = [[-0.15676163  0.03114539  0.12029561  0.89007974]]. Reward = [0.]
Curr episode timestep = 387
Current timestep = 1108. State = [[-0.31008193  0.27853712]]. Action = [[-2.0281076e-01  2.0084417e-01  3.2305717e-04 -7.9325819e-01]]. Reward = [0.]
Curr episode timestep = 388
Current timestep = 1109. State = [[-0.31653038  0.2831346 ]]. Action = [[-0.10833606 -0.05850095 -0.0411147   0.69233584]]. Reward = [0.]
Curr episode timestep = 389
Current timestep = 1110. State = [[-0.32362524  0.2854784 ]]. Action = [[-0.12697075  0.19842368 -0.20716691  0.74667907]]. Reward = [0.]
Curr episode timestep = 390
Current timestep = 1111. State = [[-0.3315162  0.2914638]]. Action = [[ 0.11047661  0.06248462 -0.1550324  -0.31487155]]. Reward = [0.]
Curr episode timestep = 391
Current timestep = 1112. State = [[-0.33580828  0.29554802]]. Action = [[ 0.22422713 -0.16349855  0.08295649 -0.26743817]]. Reward = [0.]
Curr episode timestep = 392
Current timestep = 1113. State = [[-0.3346871   0.29410946]]. Action = [[-0.06150709  0.23886183 -0.00215784 -0.3264042 ]]. Reward = [0.]
Curr episode timestep = 393
Current timestep = 1114. State = [[-0.33521396  0.2972133 ]]. Action = [[ 0.07765317 -0.0668842   0.04268646 -0.13417947]]. Reward = [0.]
Curr episode timestep = 394
Current timestep = 1115. State = [[-0.33494255  0.29828048]]. Action = [[-0.08599572  0.23383722  0.12538898  0.11412692]]. Reward = [0.]
Curr episode timestep = 395
Current timestep = 1116. State = [[-0.33623588  0.30460787]]. Action = [[-0.13046259  0.12366462 -0.12010136 -0.53667086]]. Reward = [0.]
Curr episode timestep = 396
Current timestep = 1117. State = [[-0.33949277  0.31141162]]. Action = [[ 0.0169439  -0.01829943  0.23483926  0.7891178 ]]. Reward = [0.]
Curr episode timestep = 397
Current timestep = 1118. State = [[-0.34203887  0.31545064]]. Action = [[-0.17220934 -0.06852758  0.17158222  0.15567207]]. Reward = [0.]
Curr episode timestep = 398
Current timestep = 1119. State = [[-0.3453535   0.31795275]]. Action = [[ 0.05992731 -0.19673522  0.23292714 -0.03201157]]. Reward = [0.]
Curr episode timestep = 399
Current timestep = 1120. State = [[-0.3458812  0.3151174]]. Action = [[-0.22173464  0.16198713  0.10853845 -0.52539515]]. Reward = [0.]
Curr episode timestep = 400
Current timestep = 1121. State = [[-0.34636033  0.3133696 ]]. Action = [[-0.09663789  0.08850151 -0.17628321 -0.57964945]]. Reward = [0.]
Curr episode timestep = 401
Current timestep = 1122. State = [[-0.34633598  0.3122597 ]]. Action = [[-0.05679139  0.05395225  0.22633988 -0.7759418 ]]. Reward = [0.]
Curr episode timestep = 402
Current timestep = 1123. State = [[-0.34646386  0.31126782]]. Action = [[-0.17249936 -0.16897146 -0.03918156  0.23931086]]. Reward = [0.]
Curr episode timestep = 403
Current timestep = 1124. State = [[-0.34895054  0.30786997]]. Action = [[-0.19982202 -0.04029447 -0.12541066 -0.4096912 ]]. Reward = [0.]
Curr episode timestep = 404
Current timestep = 1125. State = [[-0.35491914  0.30468282]]. Action = [[ 0.07360262 -0.07124712 -0.10786361 -0.19619066]]. Reward = [0.]
Curr episode timestep = 405
Current timestep = 1126. State = [[-0.35890698  0.29975772]]. Action = [[ 0.03896913 -0.11013201 -0.15451355  0.08155656]]. Reward = [0.]
Curr episode timestep = 406
Current timestep = 1127. State = [[-0.3592064   0.29508185]]. Action = [[ 0.2474229  -0.01759602 -0.02114582 -0.934287  ]]. Reward = [0.]
Curr episode timestep = 407
Current timestep = 1128. State = [[-0.35615924  0.28990534]]. Action = [[ 0.14223301 -0.1765432   0.14330587  0.16761231]]. Reward = [0.]
Curr episode timestep = 408
Current timestep = 1129. State = [[-0.35109487  0.2818411 ]]. Action = [[ 0.05275768  0.05111182 -0.1699721   0.37690723]]. Reward = [0.]
Curr episode timestep = 409
Current timestep = 1130. State = [[-0.34664458  0.27794465]]. Action = [[-0.05696082 -0.1760138   0.1399405  -0.33292574]]. Reward = [0.]
Curr episode timestep = 410
Current timestep = 1131. State = [[-0.34396943  0.2723985 ]]. Action = [[-0.04890832  0.14649004 -0.22319242 -0.47677583]]. Reward = [0.]
Curr episode timestep = 411
Current timestep = 1132. State = [[-0.3436046  0.2720259]]. Action = [[-0.0341571   0.07478133  0.12882417  0.4346174 ]]. Reward = [0.]
Curr episode timestep = 412
Current timestep = 1133. State = [[-0.3447511   0.27306208]]. Action = [[-0.0638065  -0.00792761  0.0933786   0.6769781 ]]. Reward = [0.]
Curr episode timestep = 413
Current timestep = 1134. State = [[-0.34598967  0.27385768]]. Action = [[ 0.2037549  -0.01768009  0.18050724 -0.90635425]]. Reward = [0.]
Curr episode timestep = 414
Current timestep = 1135. State = [[-0.34345475  0.2720109 ]]. Action = [[ 0.07294768 -0.17315853  0.1166063   0.08006644]]. Reward = [0.]
Curr episode timestep = 415
Current timestep = 1136. State = [[-0.33902526  0.2672236 ]]. Action = [[ 0.20728594 -0.06874973  0.07153207  0.41523397]]. Reward = [0.]
Curr episode timestep = 416
Current timestep = 1137. State = [[-0.3331316  0.2624208]]. Action = [[-0.04873808  0.16415548  0.09786662  0.16548467]]. Reward = [0.]
Curr episode timestep = 417
Current timestep = 1138. State = [[-0.33076602  0.26194403]]. Action = [[-0.10324182 -0.06650729  0.05894524 -0.5985382 ]]. Reward = [0.]
Curr episode timestep = 418
Current timestep = 1139. State = [[-0.33057544  0.26135263]]. Action = [[-0.09915909 -0.10403946 -0.10552415  0.6059984 ]]. Reward = [0.]
Curr episode timestep = 419
Current timestep = 1140. State = [[-0.3321903   0.25912398]]. Action = [[-0.20960677 -0.15254249 -0.09381148 -0.02677673]]. Reward = [0.]
Curr episode timestep = 420
Current timestep = 1141. State = [[-0.33546492  0.25519428]]. Action = [[-0.00792301 -0.04234265  0.18691677 -0.23202056]]. Reward = [0.]
Curr episode timestep = 421
Current timestep = 1142. State = [[-0.33748502  0.2512131 ]]. Action = [[ 0.06013298 -0.10460526  0.1238158   0.4510064 ]]. Reward = [0.]
Curr episode timestep = 422
Current timestep = 1143. State = [[-0.3376616   0.24584875]]. Action = [[ 0.1539715  -0.21026379  0.09634113  0.16720831]]. Reward = [0.]
Curr episode timestep = 423
Current timestep = 1144. State = [[-0.33689636  0.23608762]]. Action = [[ 0.03274244  0.18894428 -0.09354749  0.97716594]]. Reward = [0.]
Curr episode timestep = 424
Current timestep = 1145. State = [[-0.3350705   0.23422839]]. Action = [[-0.09411667 -0.04782259 -0.14081281  0.8138175 ]]. Reward = [0.]
Curr episode timestep = 425
Current timestep = 1146. State = [[-0.3351399   0.23296265]]. Action = [[-0.15872034  0.10926706 -0.01487762 -0.2265631 ]]. Reward = [0.]
Curr episode timestep = 426
Current timestep = 1147. State = [[-0.33768058  0.23379552]]. Action = [[ 0.1541838  -0.21271011 -0.21806681 -0.2539512 ]]. Reward = [0.]
Curr episode timestep = 427
Current timestep = 1148. State = [[-0.33786497  0.22875042]]. Action = [[ 0.19621381 -0.20996946 -0.2163832   0.3742895 ]]. Reward = [0.]
Curr episode timestep = 428
Current timestep = 1149. State = [[-0.33457926  0.2202123 ]]. Action = [[ 0.07711336 -0.24817051  0.07368082  0.71421003]]. Reward = [0.]
Curr episode timestep = 429
Current timestep = 1150. State = [[-0.3292441   0.20852502]]. Action = [[0.03138259 0.14741999 0.13689184 0.8845917 ]]. Reward = [0.]
Curr episode timestep = 430
Current timestep = 1151. State = [[-0.3244869   0.20350407]]. Action = [[ 0.18556377 -0.22395566 -0.08193758 -0.4575767 ]]. Reward = [0.]
Curr episode timestep = 431
Current timestep = 1152. State = [[-0.31803623  0.19509137]]. Action = [[0.17991853 0.2058205  0.24286205 0.8112364 ]]. Reward = [0.]
Curr episode timestep = 432
Current timestep = 1153. State = [[-0.3102475   0.19343227]]. Action = [[-0.12357682  0.078989    0.11924648  0.88682127]]. Reward = [0.]
Curr episode timestep = 433
Current timestep = 1154. State = [[-0.3069681   0.19495305]]. Action = [[ 0.24522913  0.11926371 -0.07138668 -0.7110289 ]]. Reward = [0.]
Curr episode timestep = 434
Current timestep = 1155. State = [[-0.30126137  0.19827199]]. Action = [[ 0.1775114   0.02793643 -0.2030724  -0.40558535]]. Reward = [0.]
Curr episode timestep = 435
Current timestep = 1156. State = [[-0.2945342   0.20084865]]. Action = [[ 0.2237536  -0.18969594  0.03024268 -0.44669092]]. Reward = [0.]
Curr episode timestep = 436
Current timestep = 1157. State = [[-0.28481215  0.19732554]]. Action = [[ 0.18501222  0.10581636  0.05985406 -0.2864312 ]]. Reward = [0.]
Curr episode timestep = 437
Current timestep = 1158. State = [[-0.2730007   0.19776385]]. Action = [[ 0.04994893 -0.2237886   0.18662912  0.2801304 ]]. Reward = [0.]
Curr episode timestep = 438
Current timestep = 1159. State = [[-0.26301175  0.19315103]]. Action = [[-0.01394315  0.20973518 -0.02035156 -0.74930346]]. Reward = [0.]
Curr episode timestep = 439
Current timestep = 1160. State = [[-0.25626808  0.19498427]]. Action = [[-0.16045117 -0.00283989  0.19289845  0.69528794]]. Reward = [0.]
Curr episode timestep = 440
Current timestep = 1161. State = [[-0.25450706  0.19658081]]. Action = [[0.10430473 0.00695628 0.06656593 0.04236805]]. Reward = [0.]
Curr episode timestep = 441
Current timestep = 1162. State = [[-0.25303128  0.19816108]]. Action = [[ 0.15371522  0.08178815 -0.22824281  0.05723357]]. Reward = [0.]
Curr episode timestep = 442
Current timestep = 1163. State = [[-0.24825668  0.20124087]]. Action = [[ 0.14057958  0.00680131 -0.18992771 -0.49919236]]. Reward = [0.]
Curr episode timestep = 443
Current timestep = 1164. State = [[-0.24287714  0.20306352]]. Action = [[-0.06807655 -0.22326504  0.02767253 -0.23819   ]]. Reward = [0.]
Curr episode timestep = 444
Current timestep = 1165. State = [[-0.23943965  0.19869351]]. Action = [[-0.11023667  0.00387198 -0.19721672 -0.38595748]]. Reward = [0.]
Curr episode timestep = 445
Current timestep = 1166. State = [[-0.23781422  0.19595738]]. Action = [[ 0.13753867 -0.18736705  0.04159343 -0.6223084 ]]. Reward = [0.]
Curr episode timestep = 446
Current timestep = 1167. State = [[-0.23441364  0.19061796]]. Action = [[ 0.20525461  0.15416104 -0.01992495 -0.41946656]]. Reward = [0.]
Curr episode timestep = 447
Current timestep = 1168. State = [[-0.22920085  0.18996912]]. Action = [[ 0.05758619  0.22444937  0.19547224 -0.08913493]]. Reward = [0.]
Curr episode timestep = 448
Current timestep = 1169. State = [[-0.22364344  0.19498761]]. Action = [[ 0.23856944 -0.08546349  0.24786758  0.6880374 ]]. Reward = [0.]
Curr episode timestep = 449
Current timestep = 1170. State = [[-0.21427405  0.19610292]]. Action = [[-0.14874388 -0.11318296 -0.18177849  0.26118052]]. Reward = [0.]
Curr episode timestep = 450
Current timestep = 1171. State = [[-0.20971237  0.19381261]]. Action = [[ 0.04737663 -0.17575969  0.20582086 -0.96496767]]. Reward = [0.]
Curr episode timestep = 451
Current timestep = 1172. State = [[-0.2070439   0.18887657]]. Action = [[-0.19909142  0.16988057  0.19639117 -0.6490228 ]]. Reward = [0.]
Curr episode timestep = 452
Current timestep = 1173. State = [[-0.20789821  0.18977183]]. Action = [[-0.24192809  0.01251754  0.13534069 -0.6283125 ]]. Reward = [0.]
Curr episode timestep = 453
Current timestep = 1174. State = [[-0.21162556  0.19123976]]. Action = [[ 0.20594859  0.01544115 -0.05787456  0.18257928]]. Reward = [0.]
Curr episode timestep = 454
Current timestep = 1175. State = [[-0.2116291   0.19107482]]. Action = [[ 0.01838225 -0.21557315 -0.15574946  0.09515905]]. Reward = [0.]
Curr episode timestep = 455
Current timestep = 1176. State = [[-0.21019825  0.18714556]]. Action = [[ 0.06983015 -0.11236063  0.12954903 -0.86961925]]. Reward = [0.]
Curr episode timestep = 456
Current timestep = 1177. State = [[-0.2075719   0.18201478]]. Action = [[-0.10790733 -0.03444068  0.08572447 -0.22930193]]. Reward = [0.]
Curr episode timestep = 457
Current timestep = 1178. State = [[-0.20709041  0.17813754]]. Action = [[ 0.00331685 -0.12155464 -0.15565267 -0.0908829 ]]. Reward = [0.]
Curr episode timestep = 458
Current timestep = 1179. State = [[-0.20704837  0.17197299]]. Action = [[-0.2307529  -0.213296    0.06263632 -0.2515968 ]]. Reward = [0.]
Curr episode timestep = 459
Current timestep = 1180. State = [[-0.21106467  0.16259621]]. Action = [[ 0.09292191 -0.02349675  0.13241082 -0.76710445]]. Reward = [0.]
Curr episode timestep = 460
Current timestep = 1181. State = [[-0.21368916  0.1563622 ]]. Action = [[-0.1175063   0.15939105 -0.09516782 -0.67405903]]. Reward = [0.]
Curr episode timestep = 461
Current timestep = 1182. State = [[-0.21695204  0.15517561]]. Action = [[ 0.14614907 -0.23075806  0.00971326 -0.0191927 ]]. Reward = [0.]
Curr episode timestep = 462
Current timestep = 1183. State = [[-0.21748719  0.14958446]]. Action = [[-0.16601858  0.01381516 -0.13186619 -0.97869384]]. Reward = [0.]
Curr episode timestep = 463
Current timestep = 1184. State = [[-0.21951701  0.14621562]]. Action = [[-0.03397268 -0.15299316  0.1063678   0.28988504]]. Reward = [0.]
Curr episode timestep = 464
Current timestep = 1185. State = [[-0.22192174  0.14037782]]. Action = [[-0.18994339  0.16424304 -0.23170419 -0.3860687 ]]. Reward = [0.]
Curr episode timestep = 465
Current timestep = 1186. State = [[-0.22713941  0.14006771]]. Action = [[-0.13435085  0.00470576 -0.1109488   0.52463245]]. Reward = [0.]
Curr episode timestep = 466
Current timestep = 1187. State = [[-0.23332642  0.1403768 ]]. Action = [[0.2267614  0.15179735 0.19318902 0.9624164 ]]. Reward = [0.]
Curr episode timestep = 467
Current timestep = 1188. State = [[-0.23529823  0.14339757]]. Action = [[-0.02706927  0.07966244 -0.1785034  -0.00335312]]. Reward = [0.]
Curr episode timestep = 468
Current timestep = 1189. State = [[-0.23723559  0.14772159]]. Action = [[ 0.21373665  0.19826227 -0.12113285  0.59009767]]. Reward = [0.]
Curr episode timestep = 469
Current timestep = 1190. State = [[-0.23506592  0.154621  ]]. Action = [[ 0.23761714 -0.07791843 -0.02167636 -0.6743731 ]]. Reward = [0.]
Curr episode timestep = 470
Current timestep = 1191. State = [[-0.22979885  0.15717107]]. Action = [[ 0.14428419  0.06100318 -0.03839618 -0.20844686]]. Reward = [0.]
Curr episode timestep = 471
Current timestep = 1192. State = [[-0.22143827  0.16037808]]. Action = [[ 0.17974594 -0.08326077  0.2251311   0.6413486 ]]. Reward = [0.]
Curr episode timestep = 472
Current timestep = 1193. State = [[-0.21316418  0.15959918]]. Action = [[-0.19112936 -0.04634634 -0.08089693 -0.29593712]]. Reward = [0.]
Curr episode timestep = 473
Current timestep = 1194. State = [[-0.20966576  0.15846564]]. Action = [[ 0.16280365 -0.11775333 -0.03691679 -0.08862084]]. Reward = [0.]
Curr episode timestep = 474
Current timestep = 1195. State = [[-0.20580044  0.15529068]]. Action = [[-0.08691007  0.22985393  0.14977232  0.9363241 ]]. Reward = [0.]
Curr episode timestep = 475
Current timestep = 1196. State = [[-0.20379326  0.15845835]]. Action = [[-0.01489463  0.08566028  0.02754781  0.10890305]]. Reward = [0.]
Curr episode timestep = 476
Current timestep = 1197. State = [[-0.20277867  0.16276674]]. Action = [[ 0.24256003 -0.12436132 -0.11431468 -0.86213946]]. Reward = [0.]
Curr episode timestep = 477
Current timestep = 1198. State = [[-0.19818923  0.16271652]]. Action = [[ 0.07931542 -0.24761112  0.17616236  0.8138182 ]]. Reward = [0.]
Curr episode timestep = 478
Current timestep = 1199. State = [[-0.1931391   0.15606244]]. Action = [[ 0.03806585 -0.13155465  0.05116773 -0.23175961]]. Reward = [0.]
Curr episode timestep = 479
Current timestep = 1200. State = [[-0.18730485  0.14842293]]. Action = [[ 0.17401189  0.09229219  0.04219115 -0.6031637 ]]. Reward = [0.]
Curr episode timestep = 480
Current timestep = 1201. State = [[-0.18068653  0.14524633]]. Action = [[-0.21882242  0.06600839  0.23690027  0.03502297]]. Reward = [0.]
Curr episode timestep = 481
Current timestep = 1202. State = [[-0.17935036  0.14525221]]. Action = [[-0.08606508 -0.00851257  0.17573828  0.1232456 ]]. Reward = [0.]
Curr episode timestep = 482
Current timestep = 1203. State = [[-0.17985165  0.1457257 ]]. Action = [[ 0.02029967 -0.17228949 -0.04448858  0.11695397]]. Reward = [0.]
Curr episode timestep = 483
Current timestep = 1204. State = [[-0.17895904  0.14294104]]. Action = [[-0.09443891  0.05228835  0.23342171 -0.8969395 ]]. Reward = [0.]
Curr episode timestep = 484
Current timestep = 1205. State = [[-0.1789234   0.14225267]]. Action = [[-0.02451204 -0.14081779  0.18652254 -0.93705416]]. Reward = [0.]
Curr episode timestep = 485
Current timestep = 1206. State = [[-0.18017496  0.1381542 ]]. Action = [[-0.21836539 -0.02597663 -0.22285345 -0.5874086 ]]. Reward = [0.]
Curr episode timestep = 486
Current timestep = 1207. State = [[-0.18414824  0.13444686]]. Action = [[ 0.03535911 -0.19946581 -0.04143533  0.450503  ]]. Reward = [0.]
Curr episode timestep = 487
Current timestep = 1208. State = [[-0.1869928   0.12700082]]. Action = [[ 0.00510255 -0.06389785  0.03644964  0.59953094]]. Reward = [0.]
Curr episode timestep = 488
Current timestep = 1209. State = [[-0.18930991  0.11957727]]. Action = [[ 0.12369502 -0.13784972  0.20743379  0.94103444]]. Reward = [0.]
Curr episode timestep = 489
Current timestep = 1210. State = [[-0.18858805  0.11192428]]. Action = [[ 0.11199278 -0.1490913  -0.20943221  0.00395846]]. Reward = [0.]
Curr episode timestep = 490
Current timestep = 1211. State = [[-0.18617909  0.1036588 ]]. Action = [[-0.08746226  0.07717085 -0.12094741 -0.08091921]]. Reward = [0.]
Curr episode timestep = 491
Current timestep = 1212. State = [[-0.1851055   0.10021072]]. Action = [[-0.08197743  0.10490674 -0.22140731 -0.8320884 ]]. Reward = [0.]
Curr episode timestep = 492
Current timestep = 1213. State = [[-0.18600328  0.10011707]]. Action = [[-0.09055445  0.02096805  0.2069624   0.93849254]]. Reward = [0.]
Curr episode timestep = 493
Current timestep = 1214. State = [[-0.1883196   0.10048439]]. Action = [[-0.06910846 -0.07522362  0.2224895  -0.5865743 ]]. Reward = [0.]
Curr episode timestep = 494
Current timestep = 1215. State = [[-0.19073008  0.09950262]]. Action = [[ 0.05906457  0.14463872 -0.03981625 -0.5672732 ]]. Reward = [0.]
Curr episode timestep = 495
Current timestep = 1216. State = [[-0.19185953  0.10107958]]. Action = [[-0.17060116 -0.23969463  0.14462978 -0.6509571 ]]. Reward = [0.]
Curr episode timestep = 496
Current timestep = 1217. State = [[-0.19477858  0.09779315]]. Action = [[ 0.13813299  0.11398575  0.00351512 -0.16028279]]. Reward = [0.]
Curr episode timestep = 497
Current timestep = 1218. State = [[-0.19449726  0.09720837]]. Action = [[ 0.06741536 -0.14349975 -0.03729783  0.42785442]]. Reward = [0.]
Curr episode timestep = 498
Current timestep = 1219. State = [[-0.19320302  0.09456484]]. Action = [[ 0.20304716 -0.02156998 -0.11198798  0.27472258]]. Reward = [0.]
Curr episode timestep = 499
Current timestep = 1220. State = [[-0.19052768  0.0910965 ]]. Action = [[ 0.17200655 -0.19884452 -0.00645243 -0.50084025]]. Reward = [0.]
Curr episode timestep = 500
Current timestep = 1221. State = [[-0.18520722  0.08324093]]. Action = [[ 0.23844445 -0.11093886 -0.00157319  0.38325584]]. Reward = [0.]
Curr episode timestep = 501
Current timestep = 1222. State = [[-0.17683043  0.07484109]]. Action = [[-0.2270451   0.00867191 -0.02611478 -0.9391487 ]]. Reward = [0.]
Curr episode timestep = 502
Current timestep = 1223. State = [[-0.17473248  0.07000997]]. Action = [[-0.12128192  0.11022127 -0.2008794   0.38823557]]. Reward = [0.]
Curr episode timestep = 503
Current timestep = 1224. State = [[-0.17527471  0.06992639]]. Action = [[ 0.19762611 -0.12674642  0.15568143  0.22069502]]. Reward = [0.]
Curr episode timestep = 504
Current timestep = 1225. State = [[-0.17384462  0.06743341]]. Action = [[-0.01025969  0.2208457  -0.0252555   0.63700414]]. Reward = [0.]
Curr episode timestep = 505
Current timestep = 1226. State = [[-0.17249987  0.0695774 ]]. Action = [[-0.17805184 -0.21487835  0.09829769 -0.24575347]]. Reward = [0.]
Curr episode timestep = 506
Current timestep = 1227. State = [[-0.17416388  0.06773821]]. Action = [[-0.02517305  0.08923256 -0.12042168 -0.8452889 ]]. Reward = [0.]
Curr episode timestep = 507
Current timestep = 1228. State = [[-0.1750089   0.06783139]]. Action = [[-0.20884208 -0.02080598  0.19753146 -0.9696695 ]]. Reward = [0.]
Curr episode timestep = 508
Current timestep = 1229. State = [[-0.17818922  0.06747245]]. Action = [[-0.19090301  0.07371768 -0.15975012  0.02321076]]. Reward = [0.]
Curr episode timestep = 509
Current timestep = 1230. State = [[-0.1831679   0.06978867]]. Action = [[-0.16598198  0.22489065  0.05208671  0.1093235 ]]. Reward = [0.]
Curr episode timestep = 510
Current timestep = 1231. State = [[-0.19062045  0.07670127]]. Action = [[ 0.08623415 -0.00502847  0.07938266  0.9213394 ]]. Reward = [0.]
Curr episode timestep = 511
Current timestep = 1232. State = [[-0.1965978  0.080818 ]]. Action = [[-0.1115396   0.0143007  -0.15367584 -0.92668587]]. Reward = [0.]
Curr episode timestep = 512
Current timestep = 1233. State = [[-0.20204762  0.08410304]]. Action = [[ 0.02262664  0.01259515 -0.19781998 -0.96025753]]. Reward = [0.]
Curr episode timestep = 513
Current timestep = 1234. State = [[-0.20543502  0.08607543]]. Action = [[ 0.19927698 -0.13335025  0.18377978  0.3808967 ]]. Reward = [0.]
Curr episode timestep = 514
Current timestep = 1235. State = [[-0.20545156  0.08443509]]. Action = [[-0.22189905  0.15220839  0.09065065 -0.32938254]]. Reward = [0.]
Curr episode timestep = 515
Current timestep = 1236. State = [[-0.20786329  0.08670281]]. Action = [[-0.01639758 -0.00743835 -0.21825415 -0.970768  ]]. Reward = [0.]
Curr episode timestep = 516
Current timestep = 1237. State = [[-0.21008798  0.08787355]]. Action = [[ 0.14458036 -0.04501957 -0.16281494 -0.47795713]]. Reward = [0.]
Curr episode timestep = 517
Current timestep = 1238. State = [[-0.20988625  0.08697213]]. Action = [[ 0.05871263 -0.14330493 -0.19323108  0.03942776]]. Reward = [0.]
Curr episode timestep = 518
Current timestep = 1239. State = [[-0.20831782  0.0838071 ]]. Action = [[ 0.22123122 -0.03708348  0.05834094 -0.98375595]]. Reward = [0.]
Curr episode timestep = 519
Current timestep = 1240. State = [[-0.20516212  0.08005691]]. Action = [[-0.09943017 -0.19510387  0.17703438 -0.16005355]]. Reward = [0.]
Curr episode timestep = 520
Current timestep = 1241. State = [[-0.20287496  0.07334034]]. Action = [[ 0.10469335  0.09867406 -0.19868644  0.8322185 ]]. Reward = [0.]
Curr episode timestep = 521
Current timestep = 1242. State = [[-0.20065844  0.07075002]]. Action = [[-0.22731733  0.04686889 -0.03598391  0.47531748]]. Reward = [0.]
Curr episode timestep = 522
Current timestep = 1243. State = [[-0.20187557  0.07083467]]. Action = [[-0.21063285  0.10312548  0.24803847  0.8069366 ]]. Reward = [0.]
Curr episode timestep = 523
Current timestep = 1244. State = [[-0.20589523  0.07341019]]. Action = [[-0.23557171  0.1857346   0.07243836 -0.8307509 ]]. Reward = [0.]
Curr episode timestep = 524
Current timestep = 1245. State = [[-0.21277606  0.0803911 ]]. Action = [[ 0.10493714  0.04046506 -0.20785949  0.8794863 ]]. Reward = [0.]
Curr episode timestep = 525
Current timestep = 1246. State = [[-0.21669202  0.0860475 ]]. Action = [[-0.14334124  0.04790929  0.01298735 -0.83538383]]. Reward = [0.]
Curr episode timestep = 526
Current timestep = 1247. State = [[-0.22124207  0.09106035]]. Action = [[-0.03224526 -0.05290703  0.17803746  0.26241255]]. Reward = [0.]
Curr episode timestep = 527
Current timestep = 1248. State = [[-0.2259953   0.09289113]]. Action = [[-0.10951862  0.14148009  0.02323863  0.8279519 ]]. Reward = [0.]
Curr episode timestep = 528
Current timestep = 1249. State = [[-0.23257877  0.09741296]]. Action = [[-0.08427089  0.03769708  0.13739714 -0.37602103]]. Reward = [0.]
Curr episode timestep = 529
Current timestep = 1250. State = [[-0.23913713  0.10163645]]. Action = [[-0.24155974  0.12889236  0.13539049 -0.9303008 ]]. Reward = [0.]
Curr episode timestep = 530
Current timestep = 1251. State = [[-0.24865246  0.10670823]]. Action = [[ 0.03994119 -0.22081077  0.22313827 -0.5709575 ]]. Reward = [0.]
Curr episode timestep = 531
Current timestep = 1252. State = [[-0.25533906  0.10561317]]. Action = [[ 0.12327245  0.07719716  0.20755437 -0.8784578 ]]. Reward = [0.]
Curr episode timestep = 532
Current timestep = 1253. State = [[-0.25700763  0.10633042]]. Action = [[-0.17083237  0.01242158 -0.23003505 -0.18052828]]. Reward = [0.]
Curr episode timestep = 533
Current timestep = 1254. State = [[-0.25960827  0.1062088 ]]. Action = [[ 0.05578846 -0.20410877  0.00363106 -0.13639319]]. Reward = [0.]
Curr episode timestep = 534
Current timestep = 1255. State = [[-0.26277214  0.10079547]]. Action = [[ 0.04441741 -0.08943459 -0.01936011  0.67040205]]. Reward = [0.]
Curr episode timestep = 535
Current timestep = 1256. State = [[-0.2644752   0.09451193]]. Action = [[-0.0969625  -0.2033145  -0.09220943 -0.5463052 ]]. Reward = [0.]
Curr episode timestep = 536
Current timestep = 1257. State = [[-0.2675462   0.08666123]]. Action = [[0.11755556 0.07768106 0.20937532 0.47504294]]. Reward = [0.]
Curr episode timestep = 537
Current timestep = 1258. State = [[-0.26642868  0.08328331]]. Action = [[-0.14538269 -0.10232854 -0.1390578  -0.20462602]]. Reward = [0.]
Curr episode timestep = 538
Current timestep = 1259. State = [[-0.26794294  0.07936463]]. Action = [[ 0.1444183   0.18066266 -0.13267964  0.532665  ]]. Reward = [0.]
Curr episode timestep = 539
Current timestep = 1260. State = [[-0.26744023  0.07964139]]. Action = [[-0.24146208 -0.05414793 -0.06588492 -0.15830499]]. Reward = [0.]
Curr episode timestep = 540
Current timestep = 1261. State = [[-0.27053863  0.0792043 ]]. Action = [[ 0.10115799  0.22273314  0.16806048 -0.7411531 ]]. Reward = [0.]
Curr episode timestep = 541
Current timestep = 1262. State = [[-0.2728013  0.0834687]]. Action = [[-0.08976999 -0.01083219  0.03577346  0.09072304]]. Reward = [0.]
Curr episode timestep = 542
Current timestep = 1263. State = [[-0.27535808  0.08636382]]. Action = [[-0.15641224 -0.10365659 -0.08605194  0.6602261 ]]. Reward = [0.]
Curr episode timestep = 543
Current timestep = 1264. State = [[-0.28120738  0.08608837]]. Action = [[-0.23525149  0.16163254 -0.04655986 -0.7259303 ]]. Reward = [0.]
Curr episode timestep = 544
Current timestep = 1265. State = [[-0.28912938  0.09027815]]. Action = [[-0.01427276  0.20339814 -0.10979122 -0.359684  ]]. Reward = [0.]
Curr episode timestep = 545
Current timestep = 1266. State = [[-0.29615492  0.09769515]]. Action = [[-0.14646772  0.22665489  0.21137246 -0.28708094]]. Reward = [0.]
Curr episode timestep = 546
Current timestep = 1267. State = [[-0.3033744   0.10725352]]. Action = [[ 0.04633671 -0.22498295 -0.16364855  0.5239687 ]]. Reward = [0.]
Curr episode timestep = 547
Current timestep = 1268. State = [[-0.3088651  0.1093493]]. Action = [[ 0.09371448  0.01706049 -0.15911326 -0.3013867 ]]. Reward = [0.]
Curr episode timestep = 548
Current timestep = 1269. State = [[-0.3099145   0.11130576]]. Action = [[ 0.10354659  0.1846171  -0.03085983  0.6253084 ]]. Reward = [0.]
Curr episode timestep = 549
Current timestep = 1270. State = [[-0.30964872  0.11524475]]. Action = [[ 0.07165527  0.03612471 -0.01125473  0.26517475]]. Reward = [0.]
Curr episode timestep = 550
Current timestep = 1271. State = [[-0.30831113  0.11916078]]. Action = [[0.23924553 0.17882496 0.21332961 0.54488635]]. Reward = [0.]
Curr episode timestep = 551
Current timestep = 1272. State = [[-0.30353576  0.1248939 ]]. Action = [[-0.06720458  0.00461763 -0.02415057 -0.2518803 ]]. Reward = [0.]
Curr episode timestep = 552
Current timestep = 1273. State = [[-0.30100608  0.12944396]]. Action = [[ 0.21155149  0.02451009  0.07071045 -0.7803556 ]]. Reward = [0.]
Curr episode timestep = 553
Current timestep = 1274. State = [[-0.29526594  0.13318552]]. Action = [[ 0.19886613 -0.06084687 -0.16959137 -0.4752099 ]]. Reward = [0.]
Curr episode timestep = 554
Current timestep = 1275. State = [[-0.2880135  0.1344233]]. Action = [[ 0.06655228 -0.04811382  0.0466691   0.04297388]]. Reward = [0.]
Curr episode timestep = 555
Current timestep = 1276. State = [[-0.2808738  0.1337611]]. Action = [[-0.21554375 -0.11131015 -0.18822303 -0.7101859 ]]. Reward = [0.]
Curr episode timestep = 556
Current timestep = 1277. State = [[-0.27969286  0.13197611]]. Action = [[ 0.08794576  0.19396555  0.22721526 -0.9338548 ]]. Reward = [0.]
Curr episode timestep = 557
Current timestep = 1278. State = [[-0.27750945  0.1350684 ]]. Action = [[ 0.2251696   0.2264387   0.22859347 -0.18596369]]. Reward = [0.]
Curr episode timestep = 558
Current timestep = 1279. State = [[-0.27181697  0.14133541]]. Action = [[-0.07695545 -0.0967453  -0.24517693  0.67063606]]. Reward = [0.]
Curr episode timestep = 559
Current timestep = 1280. State = [[-0.26823136  0.14397317]]. Action = [[ 0.20469481  0.04575491  0.20576465 -0.39196694]]. Reward = [0.]
Curr episode timestep = 560
Current timestep = 1281. State = [[-0.26332083  0.1459752 ]]. Action = [[ 0.07518277 -0.1429145  -0.12350801 -0.80785286]]. Reward = [0.]
Curr episode timestep = 561
Current timestep = 1282. State = [[-0.25774214  0.14468317]]. Action = [[ 0.14971423  0.18211418 -0.08320317  0.3661605 ]]. Reward = [0.]
Curr episode timestep = 562
Current timestep = 1283. State = [[-0.25042796  0.14756872]]. Action = [[ 0.04554078 -0.23812562  0.06053069 -0.4925043 ]]. Reward = [0.]
Curr episode timestep = 563
Current timestep = 1284. State = [[-0.24367462  0.14377485]]. Action = [[-0.05227803 -0.12936151  0.20685953  0.8248445 ]]. Reward = [0.]
Curr episode timestep = 564
Current timestep = 1285. State = [[-0.23976706  0.13889164]]. Action = [[ 0.24719137  0.03684327 -0.19721961  0.20671058]]. Reward = [0.]
Curr episode timestep = 565
Current timestep = 1286. State = [[-0.23244521  0.13666266]]. Action = [[ 0.09356833  0.22509617 -0.08442831  0.9030937 ]]. Reward = [0.]
Curr episode timestep = 566
Current timestep = 1287. State = [[-0.22432098  0.14038152]]. Action = [[ 0.01218888 -0.04839775  0.10850018 -0.5646122 ]]. Reward = [0.]
Curr episode timestep = 567
Current timestep = 1288. State = [[-0.21805902  0.14090402]]. Action = [[ 0.12980852 -0.20919536 -0.2157031  -0.9566382 ]]. Reward = [0.]
Curr episode timestep = 568
Current timestep = 1289. State = [[-0.21262613  0.1365395 ]]. Action = [[-0.20208381 -0.16654411  0.03042871 -0.5353592 ]]. Reward = [0.]
Curr episode timestep = 569
Current timestep = 1290. State = [[-0.21042253  0.13123952]]. Action = [[-0.13307844  0.19310877 -0.0115104   0.5996394 ]]. Reward = [0.]
Curr episode timestep = 570
Current timestep = 1291. State = [[-0.2114904   0.13170704]]. Action = [[ 0.04552609 -0.11169484 -0.09217884  0.14240205]]. Reward = [0.]
Curr episode timestep = 571
Current timestep = 1292. State = [[-0.21189967  0.12988971]]. Action = [[-0.19349448  0.0739215   0.04818717  0.39789653]]. Reward = [0.]
Curr episode timestep = 572
Current timestep = 1293. State = [[-0.21488827  0.13061203]]. Action = [[-0.13533345  0.23107767  0.24353236  0.7213967 ]]. Reward = [0.]
Curr episode timestep = 573
Current timestep = 1294. State = [[-0.2196131   0.13664128]]. Action = [[ 0.11212543 -0.06931791  0.04504168 -0.5615701 ]]. Reward = [0.]
Curr episode timestep = 574
Current timestep = 1295. State = [[-0.22156543  0.1388357 ]]. Action = [[-0.18630409  0.02596012 -0.10220543 -0.16625226]]. Reward = [0.]
Curr episode timestep = 575
Current timestep = 1296. State = [[-0.22483687  0.1410269 ]]. Action = [[ 0.03829956 -0.13066915 -0.19259273  0.38945627]]. Reward = [0.]
Curr episode timestep = 576
Current timestep = 1297. State = [[-0.22706904  0.13926163]]. Action = [[ 0.16503921 -0.05452697  0.14072934  0.5454329 ]]. Reward = [0.]
Curr episode timestep = 577
Current timestep = 1298. State = [[-0.22626993  0.13603136]]. Action = [[-0.13384439 -0.15190782  0.05495101 -0.15523493]]. Reward = [0.]
Curr episode timestep = 578
Current timestep = 1299. State = [[-0.22802195  0.13084519]]. Action = [[ 0.11047244  0.04085791 -0.0379805  -0.20718592]]. Reward = [0.]
Curr episode timestep = 579
Current timestep = 1300. State = [[-0.22765072  0.12785324]]. Action = [[-0.11473151 -0.18547143 -0.0502059  -0.0265156 ]]. Reward = [0.]
Curr episode timestep = 580
Current timestep = 1301. State = [[-0.22873291  0.12208386]]. Action = [[ 0.08202064 -0.15817367  0.19531941  0.83621883]]. Reward = [0.]
Curr episode timestep = 581
Current timestep = 1302. State = [[-0.2285419   0.11454283]]. Action = [[-0.02999979 -0.0473872   0.04038367 -0.64064604]]. Reward = [0.]
Curr episode timestep = 582
Current timestep = 1303. State = [[-0.22905487  0.10796335]]. Action = [[-0.19550113 -0.04483198  0.10465983  0.8594134 ]]. Reward = [0.]
Curr episode timestep = 583
Current timestep = 1304. State = [[-0.23323075  0.10314117]]. Action = [[-0.01603261  0.12895674 -0.11540261 -0.7333898 ]]. Reward = [0.]
Curr episode timestep = 584
Current timestep = 1305. State = [[-0.23619375  0.10226924]]. Action = [[0.20490181 0.04109135 0.05342126 0.00634968]]. Reward = [0.]
Curr episode timestep = 585
Current timestep = 1306. State = [[-0.23602311  0.10205986]]. Action = [[-0.09840769 -0.20572683 -0.20354429 -0.13038814]]. Reward = [0.]
Curr episode timestep = 586
Current timestep = 1307. State = [[-0.23716828  0.09831512]]. Action = [[-0.23337953  0.0419341   0.01156342  0.96484613]]. Reward = [0.]
Curr episode timestep = 587
Current timestep = 1308. State = [[-0.24067271  0.09698575]]. Action = [[-0.22280309  0.04244617 -0.09370559  0.10256803]]. Reward = [0.]
Curr episode timestep = 588
Current timestep = 1309. State = [[-0.24686013  0.0971078 ]]. Action = [[-0.02106732 -0.07850426  0.18040782 -0.7970157 ]]. Reward = [0.]
Curr episode timestep = 589
Current timestep = 1310. State = [[-0.25306416  0.09481751]]. Action = [[ 0.04436892  0.07645634 -0.10607706  0.5603347 ]]. Reward = [0.]
Curr episode timestep = 590
Current timestep = 1311. State = [[-0.25745812  0.09491154]]. Action = [[-0.1947177   0.05322823 -0.14893638 -0.48981774]]. Reward = [0.]
Curr episode timestep = 591
Current timestep = 1312. State = [[-0.26348978  0.09601437]]. Action = [[-0.18687397 -0.04008868 -0.15395337  0.04559171]]. Reward = [0.]
Curr episode timestep = 592
Current timestep = 1313. State = [[-0.27169198  0.09641857]]. Action = [[ 0.22605598  0.01208514  0.1388557  -0.91635084]]. Reward = [0.]
Curr episode timestep = 593
Current timestep = 1314. State = [[-0.27322292  0.09673432]]. Action = [[-0.21331973 -0.13416559 -0.07277961  0.93279016]]. Reward = [0.]
Curr episode timestep = 594
Current timestep = 1315. State = [[-0.27718067  0.09317864]]. Action = [[-0.05071244 -0.20792991  0.245632   -0.05164129]]. Reward = [0.]
Curr episode timestep = 595
Current timestep = 1316. State = [[-0.28370842  0.08546805]]. Action = [[-0.03357553  0.12410975  0.19138402  0.52706146]]. Reward = [0.]
Curr episode timestep = 596
Current timestep = 1317. State = [[-0.2879588   0.08402571]]. Action = [[ 0.23155618  0.0555048  -0.14652584  0.13584054]]. Reward = [0.]
Curr episode timestep = 597
Current timestep = 1318. State = [[-0.28723586  0.08347129]]. Action = [[ 0.11316404 -0.12341733  0.12082633  0.73700285]]. Reward = [0.]
Curr episode timestep = 598
Current timestep = 1319. State = [[-0.2848859   0.08012836]]. Action = [[ 0.23912197 -0.23720588 -0.15581198 -0.62373304]]. Reward = [0.]
Curr episode timestep = 599
Current timestep = 1320. State = [[-0.2794689   0.07213618]]. Action = [[ 0.05392748 -0.21591271  0.06816864  0.63731885]]. Reward = [0.]
Curr episode timestep = 600
Current timestep = 1321. State = [[-0.27431506  0.06473491]]. Action = [[ 0.1441558   0.20549124 -0.15803275 -0.41894424]]. Reward = [0.]
Curr episode timestep = 601
Current timestep = 1322. State = [[-0.2674847  0.0612013]]. Action = [[-0.09763618 -0.13547246 -0.10264757  0.9463378 ]]. Reward = [0.]
Curr episode timestep = 602
Current timestep = 1323. State = [[-0.2645396   0.05513804]]. Action = [[ 0.22667098 -0.10840884 -0.21709463  0.32531703]]. Reward = [0.]
Curr episode timestep = 603
Current timestep = 1324. State = [[-0.25915152  0.04955506]]. Action = [[ 0.17206305  0.10033324 -0.12220713  0.20588827]]. Reward = [0.]
Curr episode timestep = 604
Current timestep = 1325. State = [[-0.25044838  0.04783195]]. Action = [[ 0.09234011  0.00209439  0.2212213  -0.64584595]]. Reward = [0.]
Curr episode timestep = 605
Current timestep = 1326. State = [[-0.24207042  0.04641648]]. Action = [[-0.09489101 -0.06264564 -0.22124788 -0.9567622 ]]. Reward = [0.]
Curr episode timestep = 606
Current timestep = 1327. State = [[-0.23878558  0.04413963]]. Action = [[-0.18436101  0.00654331  0.07521436 -0.01507455]]. Reward = [0.]
Curr episode timestep = 607
Current timestep = 1328. State = [[-0.2389583   0.04390481]]. Action = [[-0.10910317  0.18421292 -0.16978678  0.6993098 ]]. Reward = [0.]
Curr episode timestep = 608
Current timestep = 1329. State = [[-0.2411553   0.04773069]]. Action = [[-0.05043676  0.02042714 -0.17038648 -0.41671526]]. Reward = [0.]
Curr episode timestep = 609
Current timestep = 1330. State = [[-0.24253441  0.05044524]]. Action = [[ 0.1033628  -0.01740038 -0.00445759  0.6439915 ]]. Reward = [0.]
Curr episode timestep = 610
Current timestep = 1331. State = [[-0.24298464  0.05123235]]. Action = [[-0.05030692  0.0704284   0.21660954 -0.65457517]]. Reward = [0.]
Curr episode timestep = 611
Current timestep = 1332. State = [[-0.24381179  0.0539967 ]]. Action = [[0.21042255 0.14749473 0.23063129 0.0307759 ]]. Reward = [0.]
Curr episode timestep = 612
Current timestep = 1333. State = [[-0.24163337  0.05775534]]. Action = [[-0.14457928 -0.07789224  0.13387668 -0.93028724]]. Reward = [0.]
Curr episode timestep = 613
Current timestep = 1334. State = [[-0.2425219   0.05969767]]. Action = [[-0.13515076  0.11946306  0.23213908 -0.62123317]]. Reward = [0.]
Curr episode timestep = 614
Current timestep = 1335. State = [[-0.24515517  0.06400024]]. Action = [[0.1025756  0.03311858 0.23206991 0.00536966]]. Reward = [0.]
Curr episode timestep = 615
Current timestep = 1336. State = [[-0.2464286   0.06702536]]. Action = [[ 0.06870902 -0.20315877  0.04567525  0.87098813]]. Reward = [0.]
Curr episode timestep = 616
Current timestep = 1337. State = [[-0.24534546  0.06541464]]. Action = [[0.13778612 0.15959185 0.1281251  0.6846111 ]]. Reward = [0.]
Curr episode timestep = 617
Current timestep = 1338. State = [[-0.2423898  0.0679639]]. Action = [[ 0.08283865  0.10177904 -0.06692305 -0.21266103]]. Reward = [0.]
Curr episode timestep = 618
Current timestep = 1339. State = [[-0.23880397  0.07187154]]. Action = [[-0.24538304  0.18004352  0.19026971  0.8949945 ]]. Reward = [0.]
Curr episode timestep = 619
Current timestep = 1340. State = [[-0.23907079  0.07891212]]. Action = [[ 0.10688692  0.14035302 -0.13500859  0.24776232]]. Reward = [0.]
Curr episode timestep = 620
Current timestep = 1341. State = [[-0.2383534   0.08681457]]. Action = [[ 0.12124175  0.02616554 -0.1533516   0.29211044]]. Reward = [0.]
Curr episode timestep = 621
Current timestep = 1342. State = [[-0.23519912  0.09173735]]. Action = [[-0.07191962 -0.23063289 -0.14955577 -0.4093485 ]]. Reward = [0.]
Curr episode timestep = 622
Current timestep = 1343. State = [[-0.2352344   0.09134861]]. Action = [[-0.09587029  0.16156572 -0.09803164 -0.4988041 ]]. Reward = [0.]
Curr episode timestep = 623
Current timestep = 1344. State = [[-0.23625675  0.0940243 ]]. Action = [[ 0.05362907 -0.04623267 -0.12645595 -0.19181967]]. Reward = [0.]
Curr episode timestep = 624
Current timestep = 1345. State = [[-0.23658192  0.09460039]]. Action = [[-0.19163509 -0.05799334  0.06722343  0.34980798]]. Reward = [0.]
Curr episode timestep = 625
Current timestep = 1346. State = [[-0.23835264  0.09347029]]. Action = [[-0.14023913 -0.147749    0.02516451 -0.27788353]]. Reward = [0.]
Curr episode timestep = 626
Current timestep = 1347. State = [[-0.24139453  0.09060299]]. Action = [[ 0.04452971 -0.13462952  0.08653927 -0.48683393]]. Reward = [0.]
Curr episode timestep = 627
Current timestep = 1348. State = [[-0.24438609  0.08454917]]. Action = [[ 0.23720291 -0.00646828 -0.0674292  -0.42869067]]. Reward = [0.]
Curr episode timestep = 628
Current timestep = 1349. State = [[-0.24294728  0.07972676]]. Action = [[-0.11791463 -0.21565597  0.19070116  0.37943935]]. Reward = [0.]
Curr episode timestep = 629
Current timestep = 1350. State = [[-0.24297549  0.07278389]]. Action = [[ 0.13156664  0.22133934  0.17601275 -0.2932887 ]]. Reward = [0.]
Curr episode timestep = 630
Current timestep = 1351. State = [[-0.24230239  0.07234997]]. Action = [[-0.09880951  0.24203819  0.17027706 -0.70834506]]. Reward = [0.]
Curr episode timestep = 631
Current timestep = 1352. State = [[-0.24231835  0.07730879]]. Action = [[ 0.11551732 -0.12591182  0.24865991 -0.01687366]]. Reward = [0.]
Curr episode timestep = 632
Current timestep = 1353. State = [[-0.24167384  0.07832903]]. Action = [[-0.03708966  0.17381734 -0.20683075 -0.09889829]]. Reward = [0.]
Curr episode timestep = 633
Current timestep = 1354. State = [[-0.24107651  0.08221566]]. Action = [[-0.10609081 -0.02172595 -0.08396083  0.95795226]]. Reward = [0.]
Curr episode timestep = 634
Current timestep = 1355. State = [[-0.24233933  0.0850648 ]]. Action = [[-0.14303595 -0.16213048 -0.19542214  0.3694334 ]]. Reward = [0.]
Curr episode timestep = 635
Current timestep = 1356. State = [[-0.24559514  0.08332694]]. Action = [[ 0.04308665 -0.23657459  0.24062267  0.8993931 ]]. Reward = [0.]
Curr episode timestep = 636
Current timestep = 1357. State = [[-0.24841395  0.07689565]]. Action = [[ 0.1462892  -0.00758663 -0.13052711  0.04384005]]. Reward = [0.]
Curr episode timestep = 637
Current timestep = 1358. State = [[-0.24735124  0.07245404]]. Action = [[-0.1553964   0.04795441 -0.14230497 -0.01846206]]. Reward = [0.]
Curr episode timestep = 638
Current timestep = 1359. State = [[-0.24773106  0.0704755 ]]. Action = [[-0.09053086  0.15977961 -0.04473251 -0.5823977 ]]. Reward = [0.]
Curr episode timestep = 639
Current timestep = 1360. State = [[-0.24980642  0.07261882]]. Action = [[-0.04786432  0.00145361  0.23175627 -0.74704593]]. Reward = [0.]
Curr episode timestep = 640
Current timestep = 1361. State = [[-0.25220028  0.07489871]]. Action = [[ 0.18700945  0.13245943  0.03304937 -0.06808734]]. Reward = [0.]
Curr episode timestep = 641
Current timestep = 1362. State = [[-0.25224364  0.07882178]]. Action = [[ 0.16867518  0.20993423 -0.15853335  0.30126786]]. Reward = [0.]
Curr episode timestep = 642
Current timestep = 1363. State = [[-0.24966806  0.08648344]]. Action = [[ 0.02350646 -0.04412676 -0.15027758  0.51442456]]. Reward = [0.]
Curr episode timestep = 643
Current timestep = 1364. State = [[-0.24640487  0.09035048]]. Action = [[ 0.23708045  0.04947752 -0.02456968 -0.7871345 ]]. Reward = [0.]
Curr episode timestep = 644
Current timestep = 1365. State = [[-0.23991007  0.09558734]]. Action = [[0.21855283 0.17498964 0.1717591  0.60113645]]. Reward = [0.]
Curr episode timestep = 645
Current timestep = 1366. State = [[-0.23063852  0.10350045]]. Action = [[-0.08074053  0.24730161 -0.22811273 -0.50540745]]. Reward = [0.]
Curr episode timestep = 646
Current timestep = 1367. State = [[-0.22519045  0.11363189]]. Action = [[-0.22245115 -0.003745    0.24458468  0.73137   ]]. Reward = [0.]
Curr episode timestep = 647
Current timestep = 1368. State = [[-0.22497343  0.11978324]]. Action = [[ 0.1934287  -0.03552465 -0.22591645 -0.13559026]]. Reward = [0.]
Curr episode timestep = 648
Current timestep = 1369. State = [[-0.22319335  0.12313212]]. Action = [[-0.20318292 -0.01304378 -0.21933617 -0.34244502]]. Reward = [0.]
Curr episode timestep = 649
Current timestep = 1370. State = [[-0.2243243   0.12474353]]. Action = [[-0.0322971  -0.17950876 -0.07295182  0.83333397]]. Reward = [0.]
Curr episode timestep = 650
Current timestep = 1371. State = [[-0.22538687  0.12254336]]. Action = [[-0.07603204 -0.13712105  0.17324048  0.83047557]]. Reward = [0.]
Curr episode timestep = 651
Current timestep = 1372. State = [[-0.22660112  0.11798375]]. Action = [[ 0.04070944 -0.10405983 -0.13652565 -0.7999159 ]]. Reward = [0.]
Curr episode timestep = 652
Current timestep = 1373. State = [[-0.22760844  0.11263907]]. Action = [[0.18709713 0.11118698 0.19994071 0.62534034]]. Reward = [0.]
Curr episode timestep = 653
Current timestep = 1374. State = [[-0.22581567  0.11062249]]. Action = [[ 0.02009228  0.09065765 -0.02803449  0.42519736]]. Reward = [0.]
Curr episode timestep = 654
Current timestep = 1375. State = [[-0.22409019  0.11113295]]. Action = [[-0.05043232 -0.21051712 -0.12701052 -0.36587715]]. Reward = [0.]
Curr episode timestep = 655
Current timestep = 1376. State = [[-0.2230877   0.10783139]]. Action = [[-0.00291859  0.11036268 -0.20785314 -0.33197045]]. Reward = [0.]
Curr episode timestep = 656
Current timestep = 1377. State = [[-0.22306132  0.10771486]]. Action = [[-0.071347    0.11051556  0.23684472 -0.49408668]]. Reward = [0.]
Curr episode timestep = 657
Current timestep = 1378. State = [[-0.22416776  0.1099233 ]]. Action = [[ 0.16881388 -0.08614416  0.1972875   0.12115908]]. Reward = [0.]
Curr episode timestep = 658
Current timestep = 1379. State = [[-0.22242492  0.10940634]]. Action = [[ 0.1760377  -0.21806662 -0.07863891  0.981565  ]]. Reward = [0.]
Curr episode timestep = 659
Current timestep = 1380. State = [[-0.21808536  0.10340904]]. Action = [[ 0.08740047 -0.11785617  0.17887336  0.5023129 ]]. Reward = [0.]
Curr episode timestep = 660
Current timestep = 1381. State = [[-0.21238412  0.09685925]]. Action = [[0.0503903  0.06365025 0.16261113 0.7845533 ]]. Reward = [0.]
Curr episode timestep = 661
Current timestep = 1382. State = [[-0.20647348  0.09316548]]. Action = [[-0.22426131 -0.11807516  0.17919922 -0.65499073]]. Reward = [0.]
Curr episode timestep = 662
Current timestep = 1383. State = [[-0.20554557  0.08990829]]. Action = [[ 0.24682182  0.18947476  0.17800292 -0.04553336]]. Reward = [0.]
Curr episode timestep = 663
Current timestep = 1384. State = [[-0.20315343  0.09145311]]. Action = [[-0.03490129  0.09587809  0.20293778  0.00261748]]. Reward = [0.]
Curr episode timestep = 664
Current timestep = 1385. State = [[-0.20105757  0.09399524]]. Action = [[ 0.1575473  -0.23285972  0.06392768  0.43621325]]. Reward = [0.]
Curr episode timestep = 665
Current timestep = 1386. State = [[-0.19809623  0.09087705]]. Action = [[-0.18010181 -0.07347968  0.10027361  0.22085166]]. Reward = [0.]
Curr episode timestep = 666
Current timestep = 1387. State = [[-0.19642195  0.08790715]]. Action = [[0.21604854 0.20334947 0.24038762 0.23798895]]. Reward = [0.]
Curr episode timestep = 667
Current timestep = 1388. State = [[-0.19269066  0.09021793]]. Action = [[ 0.10018972  0.19577318 -0.03628719 -0.07912898]]. Reward = [0.]
Curr episode timestep = 668
Current timestep = 1389. State = [[-0.18726198  0.09706441]]. Action = [[-0.22999252  0.2026264  -0.1998502   0.76605475]]. Reward = [0.]
Curr episode timestep = 669
Current timestep = 1390. State = [[-0.18620506  0.10551216]]. Action = [[ 0.05976841  0.2292284   0.20657921 -0.5581822 ]]. Reward = [0.]
Curr episode timestep = 670
Current timestep = 1391. State = [[-0.18516962  0.1164351 ]]. Action = [[ 0.23065335 -0.11948079  0.1007573   0.4853983 ]]. Reward = [0.]
Curr episode timestep = 671
Current timestep = 1392. State = [[-0.18155421  0.120855  ]]. Action = [[-0.08652002 -0.15555844  0.05514887  0.14366364]]. Reward = [0.]
Curr episode timestep = 672
Current timestep = 1393. State = [[-0.18017505  0.12052113]]. Action = [[-0.13885681  0.17192322 -0.09491628 -0.45074964]]. Reward = [0.]
Curr episode timestep = 673
Current timestep = 1394. State = [[-0.18106788  0.12386036]]. Action = [[ 0.04267111  0.01026744  0.13375899 -0.47097468]]. Reward = [0.]
Curr episode timestep = 674
Current timestep = 1395. State = [[-0.18117462  0.12597279]]. Action = [[-0.12928727 -0.12050085  0.19614735 -0.726274  ]]. Reward = [0.]
Curr episode timestep = 675
Current timestep = 1396. State = [[-0.18133873  0.12605721]]. Action = [[ 0.05681697  0.17196286  0.101273   -0.8969176 ]]. Reward = [0.]
Curr episode timestep = 676
Current timestep = 1397. State = [[-0.18270685  0.12949336]]. Action = [[-0.1847236   0.18858731  0.05985171  0.9057578 ]]. Reward = [0.]
Curr episode timestep = 677
Current timestep = 1398. State = [[-0.18609548  0.13558309]]. Action = [[-0.12138186 -0.22174974  0.05122086  0.91809225]]. Reward = [0.]
Curr episode timestep = 678
Current timestep = 1399. State = [[-0.1895819   0.13586716]]. Action = [[-0.21404804 -0.01311161  0.17979512 -0.35923958]]. Reward = [0.]
Curr episode timestep = 679
Current timestep = 1400. State = [[-0.19612221  0.13511185]]. Action = [[-0.03755109 -0.08839335 -0.13691446 -0.42350602]]. Reward = [0.]
Curr episode timestep = 680
Current timestep = 1401. State = [[-0.20257214  0.13279895]]. Action = [[ 0.06223348 -0.02200074 -0.14648773 -0.72777814]]. Reward = [0.]
Curr episode timestep = 681
Current timestep = 1402. State = [[-0.20644031  0.13022771]]. Action = [[ 0.1838342  -0.12693079  0.24770835  0.21606922]]. Reward = [0.]
Curr episode timestep = 682
Current timestep = 1403. State = [[-0.20504552  0.1268524 ]]. Action = [[0.143848   0.1592732  0.19997054 0.9585196 ]]. Reward = [0.]
Curr episode timestep = 683
Current timestep = 1404. State = [[-0.20303805  0.12753133]]. Action = [[0.18666878 0.03115547 0.22347027 0.66114974]]. Reward = [0.]
Curr episode timestep = 684
Current timestep = 1405. State = [[-0.19937716  0.12865798]]. Action = [[-0.03488125  0.20520055  0.02919635 -0.14428723]]. Reward = [0.]
Curr episode timestep = 685
Current timestep = 1406. State = [[-0.19665949  0.1341883 ]]. Action = [[-0.24094906  0.10521632  0.05822664  0.19453907]]. Reward = [0.]
Curr episode timestep = 686
Current timestep = 1407. State = [[-0.19824806  0.13999455]]. Action = [[-0.13554318 -0.03333703  0.12237528  0.61773825]]. Reward = [0.]
Curr episode timestep = 687
Current timestep = 1408. State = [[-0.20180205  0.14454457]]. Action = [[-0.24712795  0.14174145 -0.2095531   0.0414046 ]]. Reward = [0.]
Curr episode timestep = 688
Current timestep = 1409. State = [[-0.20766883  0.15101747]]. Action = [[ 0.08573434  0.12788057  0.0636228  -0.95662373]]. Reward = [0.]
Curr episode timestep = 689
Current timestep = 1410. State = [[-0.21179835  0.15824693]]. Action = [[ 0.17404407 -0.21042308  0.11638996 -0.04094249]]. Reward = [0.]
Curr episode timestep = 690
Current timestep = 1411. State = [[-0.21187109  0.15775293]]. Action = [[-0.16796246  0.09463012 -0.04272451  0.14663541]]. Reward = [0.]
Curr episode timestep = 691
Current timestep = 1412. State = [[-0.2130597   0.15934297]]. Action = [[-0.16414954 -0.10897875 -0.19245861 -0.87624943]]. Reward = [0.]
Curr episode timestep = 692
Current timestep = 1413. State = [[-0.21755499  0.15827449]]. Action = [[-0.17629798  0.22875386 -0.18897536  0.47814584]]. Reward = [0.]
Curr episode timestep = 693
Current timestep = 1414. State = [[-0.22391099  0.1630036 ]]. Action = [[ 0.22892976 -0.21274926 -0.05294447 -0.2562738 ]]. Reward = [0.]
Curr episode timestep = 694
Current timestep = 1415. State = [[-0.22608043  0.16098215]]. Action = [[-0.11308795 -0.03039314 -0.20404942 -0.8780599 ]]. Reward = [0.]
Curr episode timestep = 695
Current timestep = 1416. State = [[-0.22663349  0.15978512]]. Action = [[-0.0742082   0.0417724   0.05436206  0.69104767]]. Reward = [0.]
Curr episode timestep = 696
Current timestep = 1417. State = [[-0.2283578   0.15920475]]. Action = [[-0.05825557 -0.19975924 -0.16988926  0.5447333 ]]. Reward = [0.]
Curr episode timestep = 697
Current timestep = 1418. State = [[-0.23190022  0.15368585]]. Action = [[-0.17971846  0.03278449 -0.12301913 -0.8094508 ]]. Reward = [0.]
Curr episode timestep = 698
Current timestep = 1419. State = [[-0.23864806  0.15164648]]. Action = [[ 0.09571332  0.22194946  0.0733102  -0.5053517 ]]. Reward = [0.]
Curr episode timestep = 699
Current timestep = 1420. State = [[-0.2420848   0.15523605]]. Action = [[0.11593103 0.13211545 0.00177431 0.3079002 ]]. Reward = [0.]
Curr episode timestep = 700
Current timestep = 1421. State = [[-0.24386324  0.15963195]]. Action = [[0.13545412 0.0390006  0.15835196 0.8764168 ]]. Reward = [0.]
Curr episode timestep = 701
Current timestep = 1422. State = [[-0.24304932  0.16289155]]. Action = [[-0.18116304 -0.20597741 -0.02527869 -0.7393932 ]]. Reward = [0.]
Curr episode timestep = 702
Current timestep = 1423. State = [[-0.24353115  0.16192074]]. Action = [[ 0.08285201  0.04217559 -0.1655889   0.8400341 ]]. Reward = [0.]
Curr episode timestep = 703
Current timestep = 1424. State = [[-0.24312493  0.16142103]]. Action = [[ 0.1543237  -0.23693307  0.19858399 -0.76842797]]. Reward = [0.]
Curr episode timestep = 704
Current timestep = 1425. State = [[-0.24086429  0.15587126]]. Action = [[ 0.03782287 -0.09174725  0.01330775  0.6287687 ]]. Reward = [0.]
Curr episode timestep = 705
Current timestep = 1426. State = [[-0.23743467  0.15019387]]. Action = [[-0.15824029  0.19824287 -0.24354637  0.49906147]]. Reward = [0.]
Curr episode timestep = 706
Current timestep = 1427. State = [[-0.23792578  0.15061739]]. Action = [[ 0.03570408  0.02513084 -0.09716633  0.09222972]]. Reward = [0.]
Curr episode timestep = 707
Current timestep = 1428. State = [[-0.23836368  0.1512332 ]]. Action = [[ 0.19696772 -0.03409488 -0.01102573  0.3475411 ]]. Reward = [0.]
Curr episode timestep = 708
Current timestep = 1429. State = [[-0.23683633  0.15065348]]. Action = [[ 0.0230312  -0.15645085 -0.03195095 -0.11709058]]. Reward = [0.]
Curr episode timestep = 709
Current timestep = 1430. State = [[-0.23375882  0.14618564]]. Action = [[ 0.20901847 -0.21205415 -0.06738248  0.35439014]]. Reward = [0.]
Curr episode timestep = 710
Current timestep = 1431. State = [[-0.2287164   0.13899563]]. Action = [[-0.18081918  0.2462253  -0.18835837 -0.31498277]]. Reward = [0.]
Curr episode timestep = 711
Current timestep = 1432. State = [[-0.22680287  0.13901778]]. Action = [[-0.21725051 -0.21172513 -0.20478259  0.60415673]]. Reward = [0.]
Curr episode timestep = 712
Current timestep = 1433. State = [[-0.22912364  0.13582236]]. Action = [[-0.10945544  0.00781399  0.02735299 -0.6574866 ]]. Reward = [0.]
Curr episode timestep = 713
Current timestep = 1434. State = [[-0.23217551  0.1338172 ]]. Action = [[-0.17760573  0.07838917  0.14952406 -0.6069059 ]]. Reward = [0.]
Curr episode timestep = 714
Current timestep = 1435. State = [[-0.2365009   0.13444263]]. Action = [[-0.18685186 -0.19859503 -0.235842   -0.5122378 ]]. Reward = [0.]
Curr episode timestep = 715
Current timestep = 1436. State = [[-0.2441871   0.12982847]]. Action = [[ 0.13323936  0.08964521 -0.21648362 -0.529376  ]]. Reward = [0.]
Curr episode timestep = 716
Current timestep = 1437. State = [[-0.24797165  0.12904938]]. Action = [[ 0.00293693  0.21844286  0.22333705 -0.7353872 ]]. Reward = [0.]
Curr episode timestep = 717
Current timestep = 1438. State = [[-0.25041857  0.1334521 ]]. Action = [[0.01639026 0.1367811  0.07691514 0.34145892]]. Reward = [0.]
Curr episode timestep = 718
Current timestep = 1439. State = [[-0.2534041   0.13952397]]. Action = [[ 0.07464826  0.13588482  0.15025866 -0.11074084]]. Reward = [0.]
Curr episode timestep = 719
Current timestep = 1440. State = [[-0.2545702   0.14585179]]. Action = [[-0.12906282 -0.05535769 -0.10006252 -0.55229336]]. Reward = [0.]
Curr episode timestep = 720
Current timestep = 1441. State = [[-0.25694597  0.14990932]]. Action = [[-0.14585572  0.08221549  0.24315667 -0.13112044]]. Reward = [0.]
Curr episode timestep = 721
Current timestep = 1442. State = [[-0.26088613  0.15455149]]. Action = [[ 0.04745823 -0.13474742 -0.06070662 -0.8593659 ]]. Reward = [0.]
Curr episode timestep = 722
Current timestep = 1443. State = [[-0.26321617  0.1546964 ]]. Action = [[ 0.18651325  0.1207     -0.05596074  0.5446702 ]]. Reward = [0.]
Curr episode timestep = 723
Current timestep = 1444. State = [[-0.2621573   0.15625905]]. Action = [[ 0.22740442 -0.1793946  -0.13525878 -0.51901805]]. Reward = [0.]
Curr episode timestep = 724
Current timestep = 1445. State = [[-0.25803325  0.1530126 ]]. Action = [[ 0.07825822 -0.05831929 -0.22461812 -0.3584261 ]]. Reward = [0.]
Curr episode timestep = 725
Current timestep = 1446. State = [[-0.25294405  0.1492703 ]]. Action = [[ 0.2297408  -0.22751147  0.0114629   0.55412424]]. Reward = [0.]
Curr episode timestep = 726
Current timestep = 1447. State = [[-0.24598143  0.14153165]]. Action = [[-0.22107306  0.00993043 -0.16527694  0.47212338]]. Reward = [0.]
Curr episode timestep = 727
Current timestep = 1448. State = [[-0.24290834  0.13672729]]. Action = [[ 0.14788043  0.02209836 -0.05483626 -0.38848698]]. Reward = [0.]
Curr episode timestep = 728
Current timestep = 1449. State = [[-0.23933601  0.1341511 ]]. Action = [[0.09519374 0.21268886 0.02882358 0.73245883]]. Reward = [0.]
Curr episode timestep = 729
Current timestep = 1450. State = [[-0.23382622  0.1371635 ]]. Action = [[-0.03662419 -0.19224484 -0.16193977 -0.5719961 ]]. Reward = [0.]
Curr episode timestep = 730
Current timestep = 1451. State = [[-0.23117766  0.13487364]]. Action = [[ 0.0771651  -0.12938167 -0.22595029 -0.26868355]]. Reward = [0.]
Curr episode timestep = 731
Current timestep = 1452. State = [[-0.2280044   0.13060938]]. Action = [[ 0.09077239  0.15837318  0.11441347 -0.8076587 ]]. Reward = [0.]
Curr episode timestep = 732
Current timestep = 1453. State = [[-0.22383966  0.13094127]]. Action = [[ 0.09555635 -0.19552444  0.00244531 -0.851596  ]]. Reward = [0.]
Curr episode timestep = 733
Current timestep = 1454. State = [[-0.21921654  0.12683448]]. Action = [[-0.16289198  0.18642372  0.22477028 -0.3733225 ]]. Reward = [0.]
Curr episode timestep = 734
Current timestep = 1455. State = [[-0.2180983   0.12866148]]. Action = [[ 0.17090929  0.12508762 -0.05074131  0.7101121 ]]. Reward = [0.]
Curr episode timestep = 735
Current timestep = 1456. State = [[-0.21490172  0.13236128]]. Action = [[ 0.02424002 -0.15014417 -0.10317084  0.6152041 ]]. Reward = [0.]
Curr episode timestep = 736
Current timestep = 1457. State = [[-0.21243808  0.13189846]]. Action = [[-0.0372701   0.13536355  0.16313767  0.9286282 ]]. Reward = [0.]
Curr episode timestep = 737
Current timestep = 1458. State = [[-0.21074714  0.13512914]]. Action = [[ 0.0188807   0.15074325  0.18306169 -0.15790194]]. Reward = [0.]
Curr episode timestep = 738
Current timestep = 1459. State = [[-0.2087371   0.14062543]]. Action = [[ 0.15790784  0.06296951 -0.22640166 -0.6493899 ]]. Reward = [0.]
Curr episode timestep = 739
Current timestep = 1460. State = [[-0.20478894  0.14558096]]. Action = [[ 0.23196775 -0.18164158  0.09809196  0.5169729 ]]. Reward = [0.]
Curr episode timestep = 740
Current timestep = 1461. State = [[-0.19886099  0.14436233]]. Action = [[-0.19649729  0.04280278 -0.04076818  0.32220387]]. Reward = [0.]
Curr episode timestep = 741
Current timestep = 1462. State = [[-0.1957789   0.14523189]]. Action = [[-0.16122386  0.21420044  0.1180703   0.74932253]]. Reward = [0.]
Curr episode timestep = 742
Current timestep = 1463. State = [[-0.19678712  0.14981307]]. Action = [[ 0.22008166 -0.15956801  0.21871215 -0.21663511]]. Reward = [0.]
Curr episode timestep = 743
Current timestep = 1464. State = [[-0.19467838  0.1496754 ]]. Action = [[-0.16589199  0.1074788   0.13508642 -0.46744812]]. Reward = [0.]
Curr episode timestep = 744
Current timestep = 1465. State = [[-0.19510815  0.1521997 ]]. Action = [[-0.1005182  -0.00398271 -0.14467067 -0.41946256]]. Reward = [0.]
Curr episode timestep = 745
Current timestep = 1466. State = [[-0.19642091  0.15383384]]. Action = [[-0.16126792 -0.16744217  0.17482054 -0.06858921]]. Reward = [0.]
Curr episode timestep = 746
Current timestep = 1467. State = [[-0.19959435  0.15097849]]. Action = [[-0.0750552  -0.23806623 -0.06168045  0.69658697]]. Reward = [0.]
Curr episode timestep = 747
Current timestep = 1468. State = [[-0.20345894  0.14363563]]. Action = [[-0.02805799 -0.2210238  -0.13330612 -0.57335407]]. Reward = [0.]
Curr episode timestep = 748
Current timestep = 1469. State = [[-0.20796843  0.13380045]]. Action = [[ 0.05310729  0.17362422 -0.01598482  0.16546905]]. Reward = [0.]
Curr episode timestep = 749
Current timestep = 1470. State = [[-0.20840389  0.13227238]]. Action = [[-0.16812557  0.22493136 -0.05237733 -0.72891384]]. Reward = [0.]
Curr episode timestep = 750
Current timestep = 1471. State = [[-0.21080677  0.136345  ]]. Action = [[0.18232682 0.10563737 0.00433677 0.06632066]]. Reward = [0.]
Curr episode timestep = 751
Current timestep = 1472. State = [[-0.21221557  0.14011307]]. Action = [[-0.116263   -0.12437931  0.07577273  0.05749202]]. Reward = [0.]
Curr episode timestep = 752
Current timestep = 1473. State = [[-0.2132496   0.14064087]]. Action = [[-0.1994815  -0.16675049 -0.16023888  0.19414389]]. Reward = [0.]
Curr episode timestep = 753
Current timestep = 1474. State = [[-0.21737985  0.13684349]]. Action = [[ 0.13284624 -0.1254368  -0.19116087 -0.5524292 ]]. Reward = [0.]
Curr episode timestep = 754
Current timestep = 1475. State = [[-0.21982564  0.13130198]]. Action = [[-0.07876752 -0.04558553 -0.20374985 -0.40708387]]. Reward = [0.]
Curr episode timestep = 755
Current timestep = 1476. State = [[-0.22236884  0.12592573]]. Action = [[-0.02181144 -0.16572632 -0.05949806  0.85742545]]. Reward = [0.]
Curr episode timestep = 756
Current timestep = 1477. State = [[-0.22495833  0.11894999]]. Action = [[-0.06359568  0.01097187 -0.22209874  0.26689637]]. Reward = [0.]
Curr episode timestep = 757
Current timestep = 1478. State = [[-0.2269349   0.11491057]]. Action = [[ 0.08657539 -0.08275548 -0.15553822  0.27842104]]. Reward = [0.]
Curr episode timestep = 758
Current timestep = 1479. State = [[-0.22747838  0.11046857]]. Action = [[ 0.08806846  0.14366141 -0.03241324 -0.9259481 ]]. Reward = [0.]
Curr episode timestep = 759
Current timestep = 1480. State = [[-0.22696929  0.10971931]]. Action = [[ 0.09208441 -0.05457008 -0.01770279  0.4121455 ]]. Reward = [0.]
Curr episode timestep = 760
Current timestep = 1481. State = [[-0.22618543  0.10879856]]. Action = [[ 0.08489767  0.04129526 -0.00352842  0.60563064]]. Reward = [0.]
Curr episode timestep = 761
Current timestep = 1482. State = [[-0.22465973  0.10897224]]. Action = [[-0.23499925  0.14977956  0.03044486 -0.9819692 ]]. Reward = [0.]
Curr episode timestep = 762
Current timestep = 1483. State = [[-0.22601254  0.1128481 ]]. Action = [[ 0.15758911  0.24539196 -0.14988893 -0.81859607]]. Reward = [0.]
Curr episode timestep = 763
Current timestep = 1484. State = [[-0.22502796  0.11994591]]. Action = [[-0.22064412 -0.12722404  0.2083815  -0.40599036]]. Reward = [0.]
Curr episode timestep = 764
Current timestep = 1485. State = [[-0.22684859  0.12268746]]. Action = [[ 0.22410443 -0.06768472  0.14231229  0.35625374]]. Reward = [0.]
Curr episode timestep = 765
Current timestep = 1486. State = [[-0.225903    0.12213572]]. Action = [[ 0.09041834 -0.00561005  0.06136039  0.5585183 ]]. Reward = [0.]
Curr episode timestep = 766
Current timestep = 1487. State = [[-0.22350739  0.12200055]]. Action = [[ 0.04211456 -0.01365438 -0.04349966  0.5391357 ]]. Reward = [0.]
Curr episode timestep = 767
Current timestep = 1488. State = [[-0.22098164  0.12106239]]. Action = [[ 0.16417465 -0.07111794 -0.15796414  0.846192  ]]. Reward = [0.]
Curr episode timestep = 768
Current timestep = 1489. State = [[-0.21695462  0.11924227]]. Action = [[0.02971965 0.07092446 0.08608115 0.84904003]]. Reward = [0.]
Curr episode timestep = 769
Current timestep = 1490. State = [[-0.21271707  0.1195826 ]]. Action = [[-0.15241052  0.06343758 -0.05880535  0.1232127 ]]. Reward = [0.]
Curr episode timestep = 770
Current timestep = 1491. State = [[-0.21240188  0.12067665]]. Action = [[-0.01290177  0.01576924  0.20599967  0.22097981]]. Reward = [0.]
Curr episode timestep = 771
Current timestep = 1492. State = [[-0.21301274  0.12297621]]. Action = [[-0.21256809  0.23518151 -0.05536844  0.30421376]]. Reward = [0.]
Curr episode timestep = 772
Current timestep = 1493. State = [[-0.21587698  0.12911989]]. Action = [[ 0.17690107  0.00957409 -0.0542516   0.272398  ]]. Reward = [0.]
Curr episode timestep = 773
Current timestep = 1494. State = [[-0.21591385  0.13371491]]. Action = [[ 0.22362953  0.13749093  0.05714017 -0.06279188]]. Reward = [0.]
Curr episode timestep = 774
Current timestep = 1495. State = [[-0.21078497  0.1410595 ]]. Action = [[ 0.17549181 -0.16158667 -0.15623212 -0.8600129 ]]. Reward = [0.]
Curr episode timestep = 775
Current timestep = 1496. State = [[-0.20522572  0.14117621]]. Action = [[-0.19643155  0.02899984  0.08964932 -0.43768132]]. Reward = [0.]
Curr episode timestep = 776
Current timestep = 1497. State = [[-0.20369884  0.14164974]]. Action = [[ 0.13081336 -0.09020658  0.10227022  0.22637415]]. Reward = [0.]
Curr episode timestep = 777
Current timestep = 1498. State = [[-0.20116414  0.13983646]]. Action = [[ 0.17375639 -0.17893794  0.0354327  -0.20891213]]. Reward = [0.]
Curr episode timestep = 778
Current timestep = 1499. State = [[-0.19606835  0.13387454]]. Action = [[ 0.19495326 -0.13921246  0.02384478  0.6207125 ]]. Reward = [0.]
Curr episode timestep = 779
Current timestep = 1500. State = [[-0.18838482  0.12691943]]. Action = [[-0.16939375 -0.20278886  0.07965907  0.8939905 ]]. Reward = [0.]
Curr episode timestep = 780
Current timestep = 1501. State = [[-0.18297414  0.11695901]]. Action = [[ 0.20286804 -0.05056088 -0.19682555  0.03712106]]. Reward = [0.]
Curr episode timestep = 781
Current timestep = 1502. State = [[-0.17734502  0.11001791]]. Action = [[ 0.21911502  0.21141678 -0.03171891 -0.64502174]]. Reward = [0.]
Curr episode timestep = 782
Current timestep = 1503. State = [[-0.1681616   0.11108132]]. Action = [[-0.15269472  0.11150169  0.05985102  0.21930957]]. Reward = [0.]
Curr episode timestep = 783
Current timestep = 1504. State = [[-0.16391157  0.11315787]]. Action = [[ 0.11430341 -0.24066153 -0.19413768 -0.4150595 ]]. Reward = [0.]
Curr episode timestep = 784
Current timestep = 1505. State = [[-0.16057236  0.10955307]]. Action = [[ 0.04359692  0.15311718 -0.20372906 -0.8083242 ]]. Reward = [0.]
Curr episode timestep = 785
Current timestep = 1506. State = [[-0.15680258  0.11135248]]. Action = [[ 0.24383312  0.17057884 -0.15346706 -0.92241293]]. Reward = [0.]
Curr episode timestep = 786
Current timestep = 1507. State = [[-0.15014559  0.11745887]]. Action = [[-0.12700151  0.20446223 -0.21639052  0.571661  ]]. Reward = [0.]
Curr episode timestep = 787
Current timestep = 1508. State = [[-0.145486    0.12616888]]. Action = [[0.16059864 0.02292901 0.0151301  0.28690195]]. Reward = [0.]
Curr episode timestep = 788
Current timestep = 1509. State = [[-0.14112304  0.13221693]]. Action = [[-0.18508935 -0.08747526 -0.22722179 -0.23252225]]. Reward = [0.]
Curr episode timestep = 789
Current timestep = 1510. State = [[-0.14031546  0.13410275]]. Action = [[-0.02880296  0.14647329  0.2077991   0.14467096]]. Reward = [0.]
Curr episode timestep = 790
Current timestep = 1511. State = [[-0.14060412  0.1378776 ]]. Action = [[-0.11530122 -0.18354546 -0.16765365  0.4079733 ]]. Reward = [0.]
Curr episode timestep = 791
Current timestep = 1512. State = [[-0.14114699  0.1369236 ]]. Action = [[ 0.01793006 -0.05823793  0.14470097 -0.7845718 ]]. Reward = [0.]
Curr episode timestep = 792
Current timestep = 1513. State = [[-0.14109422  0.13562231]]. Action = [[ 0.13843206  0.22894025  0.05526412 -0.2671584 ]]. Reward = [0.]
Curr episode timestep = 793
Current timestep = 1514. State = [[-0.14019462  0.13919577]]. Action = [[-0.11574656  0.18728647  0.14760429  0.18687534]]. Reward = [0.]
Curr episode timestep = 794
Current timestep = 1515. State = [[-0.1408496   0.14567706]]. Action = [[-0.1078214  -0.1260479  -0.20429306  0.42576098]]. Reward = [0.]
Curr episode timestep = 795
Current timestep = 1516. State = [[-0.1420337  0.1479367]]. Action = [[ 0.10616976  0.11764959 -0.19263472 -0.36302984]]. Reward = [0.]
Curr episode timestep = 796
Current timestep = 1517. State = [[-0.14182942  0.15136072]]. Action = [[-0.1476352   0.14312512 -0.05735159  0.32891822]]. Reward = [0.]
Curr episode timestep = 797
Current timestep = 1518. State = [[-0.1449869   0.15771891]]. Action = [[-0.1885243   0.0969528  -0.15638943  0.25461447]]. Reward = [0.]
Curr episode timestep = 798
Current timestep = 1519. State = [[-0.1494856   0.16530544]]. Action = [[-0.00380774  0.06125879 -0.23764437 -0.71120536]]. Reward = [0.]
Curr episode timestep = 799
Current timestep = 1520. State = [[-0.15321122  0.17200528]]. Action = [[-0.06761044  0.12314194  0.06397074  0.21465445]]. Reward = [0.]
Curr episode timestep = 800
Current timestep = 1521. State = [[-0.1579386   0.17944227]]. Action = [[-0.22190681  0.00611752  0.06173027 -0.6961745 ]]. Reward = [0.]
Curr episode timestep = 801
Current timestep = 1522. State = [[-0.16381101  0.1851694 ]]. Action = [[ 0.21690315  0.02815062 -0.08844161 -0.0340302 ]]. Reward = [0.]
Curr episode timestep = 802
Current timestep = 1523. State = [[-0.16517241  0.18838595]]. Action = [[ 0.13766316  0.19704187 -0.06273937 -0.1730864 ]]. Reward = [0.]
Curr episode timestep = 803
Current timestep = 1524. State = [[-0.16467446  0.19450533]]. Action = [[ 0.05166599  0.0388591  -0.15893762 -0.1568501 ]]. Reward = [0.]
Curr episode timestep = 804
Current timestep = 1525. State = [[-0.16354315  0.19893748]]. Action = [[-0.15986481 -0.0942373  -0.12364823  0.9801667 ]]. Reward = [0.]
Curr episode timestep = 805
Current timestep = 1526. State = [[-0.16460241  0.20044002]]. Action = [[ 0.07377627  0.1413019   0.14148778 -0.80064124]]. Reward = [0.]
Curr episode timestep = 806
Current timestep = 1527. State = [[-0.16511598  0.20404722]]. Action = [[ 0.17135555 -0.03227246  0.03129059  0.96518636]]. Reward = [0.]
Curr episode timestep = 807
Current timestep = 1528. State = [[-0.16317491  0.20571512]]. Action = [[ 0.11841124 -0.16983646 -0.03539594  0.25480032]]. Reward = [0.]
Curr episode timestep = 808
Current timestep = 1529. State = [[-0.15955496  0.20301566]]. Action = [[-0.06409508  0.04578084 -0.18275344  0.56926537]]. Reward = [0.]
Curr episode timestep = 809
Current timestep = 1530. State = [[-0.1575584   0.20275275]]. Action = [[0.2438482  0.17437693 0.13015291 0.10395348]]. Reward = [0.]
Curr episode timestep = 810
Current timestep = 1531. State = [[-0.15277843  0.20608996]]. Action = [[ 0.00785992 -0.12576091 -0.09659983  0.7433779 ]]. Reward = [0.]
Curr episode timestep = 811
Current timestep = 1532. State = [[-0.14676009  0.20603879]]. Action = [[-0.17433845 -0.00775002 -0.20215937  0.82833815]]. Reward = [0.]
Curr episode timestep = 812
Current timestep = 1533. State = [[-0.14619385  0.20589018]]. Action = [[-0.15718041 -0.10643096  0.20175451 -0.9635593 ]]. Reward = [0.]
Curr episode timestep = 813
Current timestep = 1534. State = [[-0.14707746  0.20408896]]. Action = [[ 0.11792403  0.03039765 -0.1593327   0.6884732 ]]. Reward = [0.]
Curr episode timestep = 814
Current timestep = 1535. State = [[-0.14669174  0.20316169]]. Action = [[ 0.13928151 -0.04353768  0.07402265  0.00570369]]. Reward = [0.]
Curr episode timestep = 815
Current timestep = 1536. State = [[-0.14423098  0.20144647]]. Action = [[ 0.21635497  0.05339029 -0.13634333 -0.25984347]]. Reward = [0.]
Curr episode timestep = 816
Current timestep = 1537. State = [[-0.13932806  0.20081481]]. Action = [[ 0.04260722  0.11417478 -0.02300709 -0.42401713]]. Reward = [0.]
Curr episode timestep = 817
Current timestep = 1538. State = [[-0.13420588  0.20385341]]. Action = [[-0.17437688  0.08576664  0.05498818 -0.8851934 ]]. Reward = [0.]
Curr episode timestep = 818
Current timestep = 1539. State = [[-0.1330762   0.20759489]]. Action = [[-0.17590165 -0.09150216 -0.01015586  0.80397296]]. Reward = [0.]
Curr episode timestep = 819
Current timestep = 1540. State = [[-0.1343157  0.2087425]]. Action = [[ 0.21578455  0.06372264 -0.03820409 -0.19582403]]. Reward = [0.]
Curr episode timestep = 820
Current timestep = 1541. State = [[-0.13340963  0.20948245]]. Action = [[-0.08983614 -0.20981632  0.04630339  0.9656522 ]]. Reward = [0.]
Curr episode timestep = 821
Current timestep = 1542. State = [[-0.13267972  0.20636402]]. Action = [[-0.18470311  0.06919649 -0.03043672  0.2509464 ]]. Reward = [0.]
Curr episode timestep = 822
Current timestep = 1543. State = [[-0.13490161  0.2067094 ]]. Action = [[-0.10320234  0.19979256  0.11225531  0.24582219]]. Reward = [0.]
Curr episode timestep = 823
Current timestep = 1544. State = [[-0.13846314  0.21128528]]. Action = [[ 0.09639069 -0.16832887  0.0288451  -0.38507104]]. Reward = [0.]
Curr episode timestep = 824
Current timestep = 1545. State = [[-0.1391301   0.21038896]]. Action = [[-0.02971403  0.03495449 -0.22040886  0.6143453 ]]. Reward = [0.]
Curr episode timestep = 825
Current timestep = 1546. State = [[-0.13965374  0.21064323]]. Action = [[-0.12433696  0.12095395  0.16650057  0.08991659]]. Reward = [0.]
Curr episode timestep = 826
Current timestep = 1547. State = [[-0.14246084  0.2143678 ]]. Action = [[-0.19295897  0.1935294   0.24711722  0.37799263]]. Reward = [0.]
Curr episode timestep = 827
Current timestep = 1548. State = [[-0.14799601  0.22156541]]. Action = [[ 0.23013541 -0.18173128  0.11252874  0.493163  ]]. Reward = [0.]
Curr episode timestep = 828
Current timestep = 1549. State = [[-0.1493501   0.22149743]]. Action = [[-0.24633336  0.02741644 -0.15544581  0.32899618]]. Reward = [0.]
Curr episode timestep = 829
Current timestep = 1550. State = [[-0.1526204   0.22243734]]. Action = [[-0.06172818  0.01222706  0.18131977 -0.2620476 ]]. Reward = [0.]
Curr episode timestep = 830
Current timestep = 1551. State = [[-0.15704958  0.22254221]]. Action = [[ 0.10103273 -0.1335695  -0.24744256  0.61358654]]. Reward = [0.]
Curr episode timestep = 831
Current timestep = 1552. State = [[-0.15878843  0.2197237 ]]. Action = [[ 0.08647752 -0.00507179 -0.13664752 -0.8990811 ]]. Reward = [0.]
Curr episode timestep = 832
Current timestep = 1553. State = [[-0.15795784  0.21835692]]. Action = [[0.19767609 0.08858135 0.17401987 0.24328792]]. Reward = [0.]
Curr episode timestep = 833
Current timestep = 1554. State = [[-0.15661822  0.2181972 ]]. Action = [[ 0.10863209 -0.13448824 -0.06558806 -0.8351583 ]]. Reward = [0.]
Curr episode timestep = 834
Current timestep = 1555. State = [[-0.15305518  0.21462578]]. Action = [[ 0.12154961  0.21086645 -0.09639572 -0.6800973 ]]. Reward = [0.]
Curr episode timestep = 835
Current timestep = 1556. State = [[-0.14817451  0.21632117]]. Action = [[ 0.18191305 -0.173114    0.11560518 -0.43303967]]. Reward = [0.]
Curr episode timestep = 836
Current timestep = 1557. State = [[-0.1417165  0.2138836]]. Action = [[-0.09868529  0.2407066   0.07556605  0.7500186 ]]. Reward = [0.]
Curr episode timestep = 837
Current timestep = 1558. State = [[-0.13798378  0.21833631]]. Action = [[ 0.14761093 -0.01960783 -0.06981075 -0.43193048]]. Reward = [0.]
Curr episode timestep = 838
Current timestep = 1559. State = [[-0.13339978  0.22163443]]. Action = [[-0.19241811  0.09803489 -0.23359264  0.6722629 ]]. Reward = [0.]
Curr episode timestep = 839
Current timestep = 1560. State = [[-0.13270526  0.2253469 ]]. Action = [[ 0.06093943 -0.1171822   0.00264385  0.7851391 ]]. Reward = [0.]
Curr episode timestep = 840
Current timestep = 1561. State = [[-0.13201424  0.22458982]]. Action = [[-0.0544447  -0.22759923 -0.20757587  0.18248284]]. Reward = [0.]
Curr episode timestep = 841
Current timestep = 1562. State = [[-0.13083746  0.2204656 ]]. Action = [[-0.21077068  0.05727214 -0.16698816  0.603222  ]]. Reward = [0.]
Curr episode timestep = 842
Current timestep = 1563. State = [[-0.13200922  0.21922061]]. Action = [[-0.08337981  0.04572022  0.05500469 -0.08969194]]. Reward = [0.]
Curr episode timestep = 843
Current timestep = 1564. State = [[-0.1338337   0.21947257]]. Action = [[-0.12634347 -0.17164993  0.2242918   0.30632377]]. Reward = [0.]
Curr episode timestep = 844
Current timestep = 1565. State = [[-0.13733824  0.21492104]]. Action = [[-0.03982043 -0.0690935   0.19156009 -0.7448124 ]]. Reward = [0.]
Curr episode timestep = 845
Current timestep = 1566. State = [[-0.14215715  0.21048146]]. Action = [[-0.20217112  0.11669087 -0.07786198 -0.8899386 ]]. Reward = [0.]
Curr episode timestep = 846
Current timestep = 1567. State = [[-0.14990723  0.21024661]]. Action = [[-0.06261669 -0.02386668  0.1927661  -0.87939274]]. Reward = [0.]
Curr episode timestep = 847
Current timestep = 1568. State = [[-0.15650645  0.21009876]]. Action = [[ 0.19861656 -0.01393278  0.0210326  -0.4134205 ]]. Reward = [0.]
Curr episode timestep = 848
Current timestep = 1569. State = [[-0.15718715  0.20985456]]. Action = [[ 0.18496853  0.1772458  -0.1997912   0.8234031 ]]. Reward = [0.]
Curr episode timestep = 849
Current timestep = 1570. State = [[-0.15592283  0.21246074]]. Action = [[-0.14928958  0.02472925  0.08934766  0.29107356]]. Reward = [0.]
Curr episode timestep = 850
Current timestep = 1571. State = [[-0.15705247  0.2150762 ]]. Action = [[ 0.22139564  0.05828804 -0.18228762  0.4760933 ]]. Reward = [0.]
Curr episode timestep = 851
Current timestep = 1572. State = [[-0.15556102  0.21800323]]. Action = [[-0.08273098  0.08294806 -0.18443605 -0.34401715]]. Reward = [0.]
Curr episode timestep = 852
Current timestep = 1573. State = [[-0.15522689  0.22064853]]. Action = [[ 0.24185514 -0.22818638 -0.24353786 -0.49587828]]. Reward = [0.]
Curr episode timestep = 853
Current timestep = 1574. State = [[-0.1523418   0.21739995]]. Action = [[-0.01783569  0.00196472 -0.12614812 -0.08433682]]. Reward = [0.]
Curr episode timestep = 854
Current timestep = 1575. State = [[-0.1501263   0.21567746]]. Action = [[-0.11638641  0.2083497  -0.06626651  0.91682553]]. Reward = [0.]
Curr episode timestep = 855
Current timestep = 1576. State = [[-0.15044378  0.22016758]]. Action = [[-0.21855012  0.10980672 -0.21861632 -0.38869643]]. Reward = [0.]
Curr episode timestep = 856
Current timestep = 1577. State = [[-0.15346572  0.22568159]]. Action = [[-0.06719068 -0.02746825  0.0981175  -0.53600556]]. Reward = [0.]
Curr episode timestep = 857
Current timestep = 1578. State = [[-0.15650222  0.22966616]]. Action = [[-0.23791151 -0.12060271  0.018125    0.8386409 ]]. Reward = [0.]
Curr episode timestep = 858
Current timestep = 1579. State = [[-0.16179553  0.22959001]]. Action = [[ 0.15173513  0.1203087  -0.08100855 -0.12314504]]. Reward = [0.]
Curr episode timestep = 859
Current timestep = 1580. State = [[-0.16516098  0.23185085]]. Action = [[-0.12858814 -0.04096672  0.05395496  0.76044846]]. Reward = [0.]
Curr episode timestep = 860
Current timestep = 1581. State = [[-0.16760118  0.23322445]]. Action = [[-0.20101975  0.18983245  0.09804401  0.83048904]]. Reward = [0.]
Curr episode timestep = 861
Current timestep = 1582. State = [[-0.17347078  0.23877807]]. Action = [[ 0.04872319 -0.1716184  -0.0494535  -0.23413956]]. Reward = [0.]
Curr episode timestep = 862
Current timestep = 1583. State = [[-0.17690034  0.2383218 ]]. Action = [[ 0.07147253 -0.21600409  0.22124907  0.20501459]]. Reward = [0.]
Curr episode timestep = 863
Current timestep = 1584. State = [[-0.1790885  0.230891 ]]. Action = [[-0.05701573 -0.20881398 -0.12892051 -0.56134135]]. Reward = [0.]
Curr episode timestep = 864
Current timestep = 1585. State = [[-0.18099867  0.22178939]]. Action = [[ 0.17772168 -0.18143241  0.19447994 -0.40013993]]. Reward = [0.]
Curr episode timestep = 865
Current timestep = 1586. State = [[-0.18110292  0.2119967 ]]. Action = [[ 0.06686676  0.11104015 -0.23335971 -0.9146114 ]]. Reward = [0.]
Curr episode timestep = 866
Current timestep = 1587. State = [[-0.17839032  0.20830691]]. Action = [[-0.00898899  0.0628435   0.11842817  0.36797857]]. Reward = [0.]
Curr episode timestep = 867
Current timestep = 1588. State = [[-0.17720799  0.20665427]]. Action = [[ 0.05493495 -0.12851061 -0.15495558 -0.03265798]]. Reward = [0.]
Curr episode timestep = 868
Current timestep = 1589. State = [[-0.17456809  0.20282736]]. Action = [[ 3.2871962e-04 -2.0342557e-01 -1.3275459e-01  6.3811612e-01]]. Reward = [0.]
Curr episode timestep = 869
Current timestep = 1590. State = [[-0.17135666  0.19600217]]. Action = [[-0.1671723  0.2133477  0.2118167  0.9028361]]. Reward = [0.]
Curr episode timestep = 870
Current timestep = 1591. State = [[-0.17217028  0.19646436]]. Action = [[ 0.01370719 -0.12142324 -0.16439983 -0.1566    ]]. Reward = [0.]
Curr episode timestep = 871
Current timestep = 1592. State = [[-0.17282717  0.19420946]]. Action = [[ 0.18885338  0.2100144  -0.24235232 -0.46683133]]. Reward = [0.]
Curr episode timestep = 872
Current timestep = 1593. State = [[-0.17217726  0.1962673 ]]. Action = [[0.12093151 0.16791701 0.12818527 0.2947538 ]]. Reward = [0.]
Curr episode timestep = 873
Current timestep = 1594. State = [[-0.16908148  0.20151106]]. Action = [[ 0.16521996 -0.01356097 -0.15116186  0.8480191 ]]. Reward = [0.]
Curr episode timestep = 874
Current timestep = 1595. State = [[-0.16484816  0.20405078]]. Action = [[-0.02093387 -0.19285347 -0.21358798  0.94209266]]. Reward = [0.]
Curr episode timestep = 875
Current timestep = 1596. State = [[-0.16218416  0.2013605 ]]. Action = [[-0.03405593  0.10756153  0.18706968  0.16893709]]. Reward = [0.]
Curr episode timestep = 876
Current timestep = 1597. State = [[-0.16116123  0.2024294 ]]. Action = [[-0.22334789 -0.13198319  0.23101977  0.5118115 ]]. Reward = [0.]
Curr episode timestep = 877
Current timestep = 1598. State = [[-0.16176742  0.2011891 ]]. Action = [[0.22497007 0.1058881  0.05895308 0.04726207]]. Reward = [0.]
Curr episode timestep = 878
Current timestep = 1599. State = [[-0.16106871  0.20140347]]. Action = [[-0.17317522 -0.20204179  0.12779653 -0.4608438 ]]. Reward = [0.]
Curr episode timestep = 879
Current timestep = 1600. State = [[-0.16153243  0.19795628]]. Action = [[ 0.19902968 -0.22008303 -0.19274954 -0.22641152]]. Reward = [0.]
Curr episode timestep = 880
Current timestep = 1601. State = [[-0.16045433  0.1907711 ]]. Action = [[-0.1989014  0.1683962 -0.1698043 -0.2904644]]. Reward = [0.]
Curr episode timestep = 881
Current timestep = 1602. State = [[-0.160335   0.1896721]]. Action = [[-0.23994637 -0.14530666  0.21540919  0.9160266 ]]. Reward = [0.]
Curr episode timestep = 882
Current timestep = 1603. State = [[-0.16344427  0.18621281]]. Action = [[ 0.01958954  0.05810374 -0.08181079  0.7294651 ]]. Reward = [0.]
Curr episode timestep = 883
Current timestep = 1604. State = [[-0.16582656  0.18509611]]. Action = [[-0.1685803   0.14643776  0.10462189  0.27156413]]. Reward = [0.]
Curr episode timestep = 884
Current timestep = 1605. State = [[-0.17033288  0.1875579 ]]. Action = [[0.07403502 0.07939833 0.226246   0.29324353]]. Reward = [0.]
Curr episode timestep = 885
Current timestep = 1606. State = [[-0.17380276  0.19076894]]. Action = [[ 0.1793043   0.05694497 -0.05863404 -0.5970533 ]]. Reward = [0.]
Curr episode timestep = 886
Current timestep = 1607. State = [[-0.1746388   0.19299927]]. Action = [[-0.16940325  0.02456984 -0.07549056  0.40970826]]. Reward = [0.]
Curr episode timestep = 887
Current timestep = 1608. State = [[-0.17661619  0.19640699]]. Action = [[ 0.1302501   0.20014393 -0.16643986  0.3775251 ]]. Reward = [0.]
Curr episode timestep = 888
Current timestep = 1609. State = [[-0.17738225  0.20265526]]. Action = [[-0.11140412  0.01790991 -0.07659209  0.28258383]]. Reward = [0.]
Curr episode timestep = 889
Current timestep = 1610. State = [[-0.17916912  0.20720571]]. Action = [[ 0.21480918  0.06921965  0.23645419 -0.68821996]]. Reward = [0.]
Curr episode timestep = 890
Current timestep = 1611. State = [[-0.1770584   0.21171242]]. Action = [[-0.07406405 -0.21466446 -0.12116763  0.7041125 ]]. Reward = [0.]
Curr episode timestep = 891
Current timestep = 1612. State = [[-0.17636956  0.21048763]]. Action = [[-0.1406392   0.1297364   0.10310709  0.9756222 ]]. Reward = [0.]
Curr episode timestep = 892
Current timestep = 1613. State = [[-0.17781663  0.21258047]]. Action = [[ 0.0609926   0.08342853 -0.04416509  0.4050089 ]]. Reward = [0.]
Curr episode timestep = 893
Current timestep = 1614. State = [[-0.17896286  0.21532497]]. Action = [[ 0.00276813  0.09848201 -0.15772627  0.9189992 ]]. Reward = [0.]
Curr episode timestep = 894
Current timestep = 1615. State = [[-0.1795177   0.21937275]]. Action = [[ 0.23141652  0.2377964   0.02254412 -0.383349  ]]. Reward = [0.]
Curr episode timestep = 895
Current timestep = 1616. State = [[-0.1769153  0.2274962]]. Action = [[ 0.10021615  0.01688242  0.02129346 -0.4518146 ]]. Reward = [0.]
Curr episode timestep = 896
Current timestep = 1617. State = [[-0.17214432  0.23371953]]. Action = [[-0.13011043 -0.09556864 -0.0487285  -0.01009309]]. Reward = [0.]
Curr episode timestep = 897
Current timestep = 1618. State = [[-0.170767    0.23648317]]. Action = [[ 0.17229897  0.16325182 -0.19234388  0.03501499]]. Reward = [0.]
Curr episode timestep = 898
Current timestep = 1619. State = [[-0.16797611  0.2410151 ]]. Action = [[-0.13483633 -0.15645036 -0.23516276 -0.20695162]]. Reward = [0.]
Curr episode timestep = 899
Current timestep = 1620. State = [[-0.16757673  0.24087706]]. Action = [[ 0.07717559 -0.00861672 -0.00327614 -0.55800855]]. Reward = [0.]
Curr episode timestep = 900
Current timestep = 1621. State = [[-0.17376515  0.25503626]]. Action = [[ 0.16366321 -0.03211772 -0.19475594  0.5455282 ]]. Reward = [0.]
Curr episode timestep = 901
Current timestep = 1622. State = [[-0.16927996  0.24670434]]. Action = [[-0.03884253  0.22234592 -0.11505693 -0.22582263]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1623. State = [[-0.17284511  0.25208923]]. Action = [[-0.20563625  0.21610051 -0.07966271 -0.68882805]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1624. State = [[-0.17901322  0.260685  ]]. Action = [[-0.06211384 -0.23524284  0.2487942  -0.24301279]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1625. State = [[-0.18232328  0.26190257]]. Action = [[-0.2390419  -0.05746454 -0.22797784 -0.8290068 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1626. State = [[-0.18862793  0.26233047]]. Action = [[-0.02034682 -0.18825522  0.06954157  0.4998529 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1627. State = [[-0.1933614   0.25743827]]. Action = [[-0.00790815 -0.09260282 -0.00251634  0.771085  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1628. State = [[-0.19711791  0.25177872]]. Action = [[ 0.05502084  0.15450978 -0.18217357 -0.4740615 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1629. State = [[-0.19989935  0.25046334]]. Action = [[ 0.22660184 -0.16944885  0.02502438  0.0322547 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1630. State = [[-0.19675788  0.24596973]]. Action = [[-0.13653363 -0.01536317 -0.10690874 -0.7672506 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1631. State = [[-0.19685167  0.24327372]]. Action = [[-0.18787429 -0.22132283  0.08193207 -0.70682615]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1632. State = [[-0.19906084  0.23665278]]. Action = [[ 0.08763304  0.2098797  -0.11406669  0.35978985]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1633. State = [[-0.20204887  0.23599431]]. Action = [[ 0.11891964 -0.1561242  -0.22482973  0.05335808]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1634. State = [[-0.20125036  0.23140761]]. Action = [[-0.23920701 -0.22924556 -0.12593769 -0.3725512 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1635. State = [[-0.20318392  0.22424117]]. Action = [[ 0.19908625 -0.12680577  0.14217073  0.767225  ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1636. State = [[-0.20164768  0.21488093]]. Action = [[ 0.18739223  0.0577257  -0.18785006 -0.10843652]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1637. State = [[-0.19811276  0.21006696]]. Action = [[ 0.18743941 -0.02976266  0.12205717 -0.90630955]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1638. State = [[-0.1931317   0.20479557]]. Action = [[ 0.16214186 -0.14195596 -0.24636257  0.4882419 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1639. State = [[-0.18630497  0.19689175]]. Action = [[-0.19603243 -0.22821836  0.00976968 -0.84579563]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1640. State = [[-0.18335184  0.18809581]]. Action = [[-0.00215343  0.00259244 -0.18135564 -0.35762358]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1641. State = [[-0.18248405  0.18265231]]. Action = [[-0.19260216 -0.104848    0.22274029 -0.45797616]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1642. State = [[-0.18449079  0.17679712]]. Action = [[0.21554619 0.09833407 0.13489637 0.681005  ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1643. State = [[-0.18346827  0.17464253]]. Action = [[-0.16864929 -0.10616529 -0.20848274 -0.08581889]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1644. State = [[-0.18434055  0.17142285]]. Action = [[-0.24383499  0.18269238 -0.22453691  0.31311655]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1645. State = [[-0.19095303  0.17457688]]. Action = [[-0.03165266  0.24652302  0.10008407  0.400481  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1646. State = [[-0.19632941  0.18193248]]. Action = [[ 0.03896013 -0.19200592 -0.11042887  0.85532975]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1647. State = [[-0.19765577  0.18201236]]. Action = [[ 0.24315834  0.2269414  -0.02181971  0.45952535]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1648. State = [[-0.19758597  0.18481839]]. Action = [[-0.21319333 -0.12429699 -0.17282252  0.02105284]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1649. State = [[-0.19803573  0.18546449]]. Action = [[ 0.21119714 -0.10579795  0.20741412 -0.6913549 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1650. State = [[-0.19616339  0.18285343]]. Action = [[0.20698217 0.00240454 0.06348658 0.8040805 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1651. State = [[-0.19305377  0.18066989]]. Action = [[-0.19242541  0.07780641 -0.02654453  0.31061316]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1652. State = [[-0.1934239   0.18160883]]. Action = [[-0.10209656 -0.06196673 -0.0311023  -0.9605594 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1653. State = [[-0.19381821  0.1819806 ]]. Action = [[-0.12797387 -0.02793935 -0.21800928 -0.60137737]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1654. State = [[-0.19549856  0.18152747]]. Action = [[-0.09313159 -0.19990335 -0.17440999  0.26019633]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1655. State = [[-0.19737005  0.17700826]]. Action = [[-0.07975823 -0.09657565 -0.16772062 -0.1791954 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1656. State = [[-0.1994811   0.17161334]]. Action = [[0.18970275 0.06874821 0.13277909 0.04999661]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1657. State = [[-0.19899563  0.16889918]]. Action = [[-0.07082933  0.01949486 -0.08106087  0.21082354]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1658. State = [[-0.19972168  0.1681887 ]]. Action = [[-0.20624228 -0.00928611  0.17905438 -0.5372132 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1659. State = [[-0.20446637  0.16729333]]. Action = [[-0.17282775 -0.23557097 -0.1643573   0.595196  ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1660. State = [[-0.21112207  0.16158816]]. Action = [[ 0.06975555  0.14353466  0.18713453 -0.5839421 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1661. State = [[-0.21673855  0.15995431]]. Action = [[-0.16045327 -0.01236993  0.15798211 -0.6915737 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1662. State = [[-0.22264926  0.15869297]]. Action = [[ 0.05207318 -0.11608672 -0.09594396  0.7397479 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1663. State = [[-0.22687173  0.15490445]]. Action = [[ 0.04664472 -0.10582924  0.13400084  0.5211233 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1664. State = [[-0.22894752  0.14868867]]. Action = [[-0.04661824 -0.20885576  0.2018171  -0.69280314]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1665. State = [[-0.23020296  0.14064138]]. Action = [[-0.21925764 -0.03675103  0.05684844 -0.67393976]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1666. State = [[-0.23404974  0.13513164]]. Action = [[0.22558403 0.09059924 0.01121554 0.7865956 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1667. State = [[-0.23565571  0.13191722]]. Action = [[-0.16177449  0.062475    0.15427926 -0.64782304]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1668. State = [[-0.23966138  0.13136563]]. Action = [[-0.12222743 -0.19904606  0.21629271 -0.8549057 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1669. State = [[-0.24473885  0.1271193 ]]. Action = [[-0.17109868  0.1173299   0.04775637  0.3804505 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1670. State = [[-0.25097227  0.12695578]]. Action = [[ 0.13087201 -0.18865757  0.10886753  0.71681046]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1671. State = [[-0.25309405  0.12256613]]. Action = [[ 0.0881862   0.13353175 -0.11625262 -0.681969  ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1672. State = [[-0.2540085   0.12166704]]. Action = [[-0.01768625 -0.17650059 -0.11315104  0.36208797]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1673. State = [[-0.25531667  0.11776085]]. Action = [[-0.21547705  0.18844432 -0.20420156  0.8008348 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1674. State = [[-0.26008764  0.11925758]]. Action = [[-0.16060169 -0.1986423   0.09680319  0.17885065]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1675. State = [[-0.26605937  0.11535188]]. Action = [[ 0.0445908   0.10571894  0.19186616 -0.00078851]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1676. State = [[-0.26960197  0.11504093]]. Action = [[ 0.10512874  0.13795468 -0.12804236 -0.37417328]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1677. State = [[-0.2708753   0.11776483]]. Action = [[-0.11088277  0.12610775 -0.1848472  -0.64897776]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1678. State = [[-0.2740657   0.12300656]]. Action = [[-0.21462822 -0.09575048  0.20771575 -0.8170618 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1679. State = [[-0.27880082  0.12549429]]. Action = [[-0.18746285 -0.03826292  0.14340699  0.4422015 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1680. State = [[-0.28724092  0.12637849]]. Action = [[ 0.19498509  0.08059338  0.22912592 -0.55007535]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1681. State = [[-0.29080376  0.12731901]]. Action = [[-0.20033626  0.01352865  0.22298265  0.360435  ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1682. State = [[-0.2969015   0.12858737]]. Action = [[-0.00926578 -0.09496456 -0.16613527  0.10030603]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1683. State = [[-0.301291    0.12754047]]. Action = [[-0.19135647  0.05286214  0.22732511  0.94628227]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1684. State = [[-0.30764842  0.12869985]]. Action = [[ 0.14397967  0.06074578 -0.06544882  0.28450787]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1685. State = [[-0.30993855  0.12921584]]. Action = [[ 0.2434653  -0.16609932  0.1045827  -0.90813744]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1686. State = [[-0.30703548  0.12552524]]. Action = [[ 0.1634289  -0.219184    0.05592507 -0.7808525 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1687. State = [[-0.3018232   0.11832254]]. Action = [[-0.18421985  0.03981549  0.23414809  0.21348405]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1688. State = [[-0.30212095  0.11447675]]. Action = [[ 0.0156025   0.06131873  0.01043329 -0.45071936]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1689. State = [[-0.30241385  0.11364785]]. Action = [[0.07976633 0.24111095 0.20659184 0.62905   ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1690. State = [[-0.30293155  0.11699359]]. Action = [[-0.04425552  0.01965433 -0.19324106  0.50418925]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1691. State = [[-0.30366927  0.11881058]]. Action = [[ 0.07497171 -0.18981983 -0.07944135  0.50298905]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1692. State = [[-0.30224544  0.11700776]]. Action = [[-0.07113086  0.01659754 -0.11448953  0.63072884]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1693. State = [[-0.30199745  0.11680355]]. Action = [[ 0.01130119  0.03374296 -0.18470414  0.2493192 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1694. State = [[-0.3022711   0.11727871]]. Action = [[-0.09105684  0.02965307 -0.03597715  0.18022656]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1695. State = [[-0.30331594  0.11805714]]. Action = [[ 0.21292806  0.2166571  -0.1891123   0.6136869 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1696. State = [[-0.30335358  0.12255546]]. Action = [[-0.13520241  0.2148422   0.24921393 -0.85686594]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1697. State = [[-0.30597004  0.13049392]]. Action = [[-0.02997532  0.14883229 -0.00333729  0.87653637]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1698. State = [[-0.30889025  0.13991344]]. Action = [[-0.04479262  0.14875847 -0.0702588   0.30174577]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1699. State = [[-0.31188577  0.1496455 ]]. Action = [[-0.03180194  0.24073479 -0.00242797 -0.6779526 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1700. State = [[-0.31523845  0.16277625]]. Action = [[-0.22965428  0.18559045 -0.06744099  0.05271864]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1701. State = [[-0.3217144   0.17472842]]. Action = [[ 0.16070145 -0.13274612  0.19881874  0.5592704 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1702. State = [[-0.3227647   0.17946017]]. Action = [[-0.00644542 -0.17165276  0.22089016 -0.06460369]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1703. State = [[-0.32229012  0.17875578]]. Action = [[-0.10058995 -0.20866047 -0.06770964 -0.6463724 ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1704. State = [[-0.32203758  0.17493606]]. Action = [[ 0.20145231  0.23295808 -0.19832113 -0.11876106]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1705. State = [[-0.3211156  0.176087 ]]. Action = [[-0.01903592  0.24658436 -0.19705953 -0.07395387]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1706. State = [[-0.32137832  0.18254504]]. Action = [[-0.01457955 -0.00350748 -0.08247173  0.9707403 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1707. State = [[-0.3209031   0.18611558]]. Action = [[0.17132655 0.08361709 0.08577734 0.8574277 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1708. State = [[-0.31720337  0.19085263]]. Action = [[0.0599598  0.2461676  0.18834144 0.37693024]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1709. State = [[-0.31367272  0.19915332]]. Action = [[ 0.16907948  0.00111848 -0.12292644  0.77683854]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1710. State = [[-0.3072778   0.20476028]]. Action = [[-0.01460482  0.08622321  0.14318216  0.4555068 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1711. State = [[-0.30261582  0.21129405]]. Action = [[0.06794536 0.14180297 0.24912101 0.7463031 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1712. State = [[-0.2982524   0.21869655]]. Action = [[ 0.20528835 -0.2330514  -0.13833062  0.28789926]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1713. State = [[-0.29065412  0.21978514]]. Action = [[0.13503942 0.20105952 0.05001324 0.212785  ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1714. State = [[-0.28396326  0.22455311]]. Action = [[ 0.11677486  0.00674686 -0.09360321  0.1680901 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1715. State = [[-0.27654982  0.2284024 ]]. Action = [[-0.10781646 -0.00147411  0.14666545  0.3786353 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1716. State = [[-0.27309084  0.23124698]]. Action = [[-0.14856233  0.00178269 -0.05530225 -0.10540795]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1717. State = [[-0.27402925  0.2324733 ]]. Action = [[-0.1301945  -0.19253118  0.16227132 -0.551682  ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1718. State = [[-0.27444434  0.23021695]]. Action = [[-0.00854589 -0.07261977 -0.04263246 -0.07471716]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1719. State = [[-0.27383405  0.2265275 ]]. Action = [[ 0.1487937  -0.13629526  0.12782708 -0.6149972 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1720. State = [[-0.2713598   0.22160986]]. Action = [[ 0.12131557  0.17879725  0.19493645 -0.26889664]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1721. State = [[-0.27056828  0.22121081]]. Action = [[ 0.11575875 -0.1899356  -0.05658378 -0.41402602]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1722. State = [[-0.26644135  0.21681519]]. Action = [[ 0.03809479  0.06732267 -0.03200483  0.62857413]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1723. State = [[-0.26428556  0.21497072]]. Action = [[ 0.16339424  0.03897393  0.12421152 -0.36967647]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1724. State = [[-0.25967023  0.2144464 ]]. Action = [[-0.11255369  0.15738934  0.20820636  0.3329761 ]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1725. State = [[-0.25826743  0.2177472 ]]. Action = [[-0.03947689  0.06835032 -0.16910961 -0.6957548 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1726. State = [[-0.2578285   0.22171628]]. Action = [[ 0.2211402   0.15668225  0.17371228 -0.6842085 ]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1727. State = [[-0.25326726  0.22647955]]. Action = [[-0.23377872 -0.20991121  0.15125462  0.43327522]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1728. State = [[-0.25258058  0.22711264]]. Action = [[ 0.12103254  0.12667596 -0.12500313 -0.0671398 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1729. State = [[-0.25105342  0.22876307]]. Action = [[ 0.06827354 -0.01319301 -0.06587325 -0.7111075 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1730. State = [[-0.24929686  0.22998172]]. Action = [[-0.22044511  0.06040329  0.21255353  0.04276025]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1731. State = [[-0.25181398  0.23320594]]. Action = [[-0.18930593  0.12707627  0.10666013 -0.5162544 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1732. State = [[-0.25711042  0.23915325]]. Action = [[ 0.01853731 -0.13517006 -0.07223287  0.4275949 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1733. State = [[-0.25877035  0.24014413]]. Action = [[ 0.11505547  0.03248167 -0.10309023  0.20667255]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1734. State = [[-0.25908515  0.2401614 ]]. Action = [[-0.19970542 -0.10292256  0.00564054 -0.79344964]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1735. State = [[-0.2600675   0.24013333]]. Action = [[ 0.07240522 -0.02520768 -0.09332931  0.6703806 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1736. State = [[-0.26019555  0.2395825 ]]. Action = [[-0.09257975  0.10431364  0.24536377 -0.25909758]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1737. State = [[-0.2623099   0.24152225]]. Action = [[-0.00123145  0.11750501  0.23025095  0.04068398]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1738. State = [[-0.26532787  0.24488226]]. Action = [[ 0.0879699   0.06921875 -0.08929481  0.65235114]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1739. State = [[-0.2672203   0.24728337]]. Action = [[-0.07057011  0.1796172   0.18486619  0.79798174]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1740. State = [[-0.27102536  0.2528398 ]]. Action = [[ 0.04012105  0.21024734 -0.02420503  0.15316987]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1741. State = [[-0.27366275  0.2610614 ]]. Action = [[-0.10883351  0.08209139 -0.18222581  0.8074808 ]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1742. State = [[-0.27712345  0.26887327]]. Action = [[ 0.02185738 -0.20174657  0.17557532  0.4195161 ]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1743. State = [[-0.27796036  0.26990703]]. Action = [[ 0.02340093  0.1895181  -0.01242922  0.03302562]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1744. State = [[-0.2792126   0.27327523]]. Action = [[-0.17193657 -0.11551543  0.24294597  0.23183393]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1745. State = [[-0.28150907  0.27549347]]. Action = [[-0.12425844  0.15052181  0.01229998 -0.36930096]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1746. State = [[-0.2873174   0.28100753]]. Action = [[-0.10760736  0.2114504   0.17467439  0.14354837]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1747. State = [[-0.29511988  0.2889567 ]]. Action = [[-0.05825219  0.231918    0.23118007 -0.28265435]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1748. State = [[-0.30217132  0.29871476]]. Action = [[ 0.05580515 -0.06258172  0.17286062  0.77058315]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1749. State = [[-0.30563393  0.30476978]]. Action = [[-0.1012505   0.22151864  0.14120293 -0.4322846 ]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1750. State = [[-0.30948287  0.3131793 ]]. Action = [[ 0.06839424  0.09720743 -0.17804787  0.376729  ]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 1751. State = [[-0.3105585   0.32146952]]. Action = [[ 0.09758642 -0.15813047 -0.24603614 -0.32010818]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 1752. State = [[-0.30878496  0.32291022]]. Action = [[0.08436418 0.14511386 0.17558312 0.09789836]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 1753. State = [[-0.3070782   0.32406998]]. Action = [[ 0.1623947   0.17120051  0.11271152 -0.9023206 ]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 1754. State = [[-0.30588767  0.3254494 ]]. Action = [[ 0.13651568  0.02190188 -0.22585154 -0.23763245]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 1755. State = [[-0.30518243  0.3264297 ]]. Action = [[ 0.21711591  0.01428121 -0.03234917 -0.9198663 ]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 1756. State = [[-0.30466858  0.3268667 ]]. Action = [[ 0.18399554  0.15883648  0.01592493 -0.3984728 ]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 1757. State = [[-0.30396095  0.3266923 ]]. Action = [[ 0.20595038 -0.18983987 -0.08696455 -0.20102745]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 1758. State = [[-0.29906845  0.32207516]]. Action = [[ 0.03330287  0.03654388  0.08160958 -0.88486654]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 1759. State = [[-0.29638734  0.3193585 ]]. Action = [[-0.03310794  0.1271289  -0.07588351 -0.7144402 ]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 1760. State = [[-0.29494676  0.31798276]]. Action = [[ 0.03651688  0.02376139  0.24636191 -0.6772845 ]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 1761. State = [[-0.29407793  0.31658345]]. Action = [[-0.23214965 -0.0596332  -0.00227547  0.8971864 ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 1762. State = [[-0.29524222  0.3163498 ]]. Action = [[-0.19615994  0.19274172  0.24595171  0.37679267]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1763. State = [[-0.29609194  0.31595966]]. Action = [[ 0.15827543  0.05448946 -0.12465104  0.31900573]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 1764. State = [[-0.29639974  0.31578395]]. Action = [[ 0.21894825  0.02483654 -0.18857847 -0.8307083 ]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1765. State = [[-0.29663262  0.31576753]]. Action = [[-0.01813783  0.14158517 -0.12652028 -0.1382947 ]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1766. State = [[-0.2967022   0.31567475]]. Action = [[ 0.10351422  0.15233424 -0.1480959   0.6407553 ]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 1767. State = [[-0.29671884  0.3155675 ]]. Action = [[ 0.02513117 -0.02118555  0.21167687  0.444057  ]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 1768. State = [[-0.29627982  0.3150024 ]]. Action = [[ 0.2188372  -0.18047789 -0.02502027  0.07715929]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1769. State = [[-0.29136026  0.30948734]]. Action = [[ 0.01730716  0.19810992 -0.10163851 -0.01886088]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 1770. State = [[-0.28910807  0.3063978 ]]. Action = [[ 0.12764117  0.05738828  0.2360568  -0.7616167 ]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 1771. State = [[-0.28712165  0.30389598]]. Action = [[ 0.02589613 -0.01599964  0.1470086  -0.26743865]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1772. State = [[-0.28540376  0.30177537]]. Action = [[-0.21375144 -0.05584574 -0.162649    0.30053687]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 1773. State = [[-0.28573942  0.30128253]]. Action = [[-0.1378538   0.08096927  0.04378518 -0.95221215]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1774. State = [[-0.28894806  0.30361643]]. Action = [[-0.2412656   0.15210983 -0.06575757  0.1009084 ]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1775. State = [[-0.29574278  0.30917403]]. Action = [[ 0.04764628  0.1710732  -0.02570763  0.41896296]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1776. State = [[-0.3023283   0.31520364]]. Action = [[ 0.20953834  0.21470132 -0.00433458  0.08968186]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1777. State = [[-0.3061772   0.31852758]]. Action = [[-0.07469627 -0.04363938  0.03675044 -0.6920995 ]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1778. State = [[-0.30866477  0.32076854]]. Action = [[-0.11799088  0.20221084 -0.07367605  0.3022529 ]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1779. State = [[-0.31041425  0.3219794 ]]. Action = [[-0.14414358 -0.17637037 -0.22625308  0.9073248 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1780. State = [[-0.3120538   0.32025355]]. Action = [[ 0.08754313 -0.22167715  0.06760567  0.33128285]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1781. State = [[-0.31161124  0.31396222]]. Action = [[ 0.04610106  0.20795119 -0.19004492 -0.7656445 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1782. State = [[-0.31107944  0.30954826]]. Action = [[ 0.16331401 -0.15190428  0.14336896  0.9480494 ]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1783. State = [[-0.30746472  0.3026072 ]]. Action = [[-0.01934798  0.03231418  0.17568785 -0.09094697]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1784. State = [[-0.3064716   0.29896381]]. Action = [[ 0.06800497  0.1987685  -0.04587758  0.42375684]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1785. State = [[-0.3060342   0.29664224]]. Action = [[ 0.07293922  0.09771979 -0.21140425 -0.47462368]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1786. State = [[-0.30575472  0.29680505]]. Action = [[ 0.06704298  0.23978424 -0.1927674   0.0129745 ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1787. State = [[-0.3057281   0.29931858]]. Action = [[-0.21887265 -0.22496794 -0.08261099 -0.9892873 ]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1788. State = [[-0.30682155  0.2988979 ]]. Action = [[-0.18533796 -0.05280133 -0.184611   -0.3033054 ]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1789. State = [[-0.3114211  0.2981538]]. Action = [[-0.10612458 -0.07886155  0.22352695  0.34255123]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1790. State = [[-0.31576702  0.29626635]]. Action = [[ 0.19744408  0.05114093 -0.1188672  -0.72595185]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1791. State = [[-0.31510282  0.29478446]]. Action = [[ 0.01051027 -0.1722106   0.05036318  0.8213396 ]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1792. State = [[-0.31365606  0.29158345]]. Action = [[ 0.12215644  0.19502741  0.22496879 -0.2215336 ]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1793. State = [[-0.3130089   0.29215962]]. Action = [[-0.06714371 -0.11147487 -0.02930547  0.8276429 ]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1794. State = [[-0.31276307  0.2915605 ]]. Action = [[-0.2112788  -0.05514166 -0.06918126  0.2728808 ]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1795. State = [[-0.31461775  0.2911968 ]]. Action = [[-0.21325563 -0.07806933  0.20315188  0.17637384]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1796. State = [[-0.32098848  0.28925505]]. Action = [[-0.06288514  0.2122089  -0.09540486 -0.97988874]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1797. State = [[-0.3291511   0.29199344]]. Action = [[ 0.1472682   0.11035872 -0.20372273  0.3555292 ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1798. State = [[-0.33176062  0.2948564 ]]. Action = [[-0.07095382 -0.13146323 -0.12544923  0.8697078 ]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1799. State = [[-0.33319867  0.29412866]]. Action = [[ 0.23121968  0.03206965  0.10094124 -0.40904534]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1800. State = [[-0.33172432  0.292669  ]]. Action = [[-0.2399218  -0.04246159  0.15747789 -0.31869173]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1801. State = [[-0.33353958  0.292726  ]]. Action = [[-0.00761937 -0.05743764 -0.10019553  0.0961982 ]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1802. State = [[-0.33501622  0.29179108]]. Action = [[0.12202829 0.15262878 0.22380862 0.59492755]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1803. State = [[-0.33553633  0.29237694]]. Action = [[-0.04200858  0.12803537  0.1830697   0.00446534]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1804. State = [[-0.33813652  0.29514506]]. Action = [[-0.03316581  0.09712642  0.11939341  0.07727289]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1805. State = [[-0.34101117  0.29902798]]. Action = [[ 0.13922015  0.22645351  0.07921597 -0.90182734]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1806. State = [[-0.34113356  0.3063719 ]]. Action = [[-0.18758436  0.07610884 -0.08010662  0.8869395 ]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1807. State = [[-0.34452924  0.31335887]]. Action = [[0.06568545 0.08855754 0.16801685 0.9596603 ]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1808. State = [[-0.3460776   0.32001218]]. Action = [[-0.16485637  0.20636424 -0.09693351 -0.6002305 ]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1809. State = [[-0.34626785  0.32526538]]. Action = [[-0.15703972  0.0697498   0.17751864 -0.08870524]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1810. State = [[-0.34660193  0.3280238 ]]. Action = [[ 0.16187978  0.07995164 -0.15074961  0.17055714]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1811. State = [[-0.34692958  0.32913303]]. Action = [[ 0.02536079  0.18964064 -0.1381864  -0.8743791 ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1812. State = [[-0.34709406  0.32970345]]. Action = [[-0.17610072 -0.10442612  0.00828841 -0.479231  ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1813. State = [[-0.3474469   0.32992825]]. Action = [[-0.22037803 -0.19889419 -0.15923133  0.86841846]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1814. State = [[-0.349152    0.32946542]]. Action = [[-0.05936126  0.02888525  0.159011    0.91066384]]. Reward = [0.]
Curr episode timestep = 192
Current timestep = 1815. State = [[-0.35019    0.3293804]]. Action = [[-0.22714429 -0.02118491  0.20215377  0.7021594 ]]. Reward = [0.]
Curr episode timestep = 193
Current timestep = 1816. State = [[-0.3508837   0.32955834]]. Action = [[0.01867124 0.1299125  0.09359559 0.33417833]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 1817. State = [[-0.3511888  0.3296541]]. Action = [[-0.08826111  0.20121086 -0.14812693  0.7164438 ]]. Reward = [0.]
Curr episode timestep = 195
Current timestep = 1818. State = [[-0.35106373  0.3293834 ]]. Action = [[ 0.10707912 -0.20020865 -0.08114088 -0.5223064 ]]. Reward = [0.]
Curr episode timestep = 196
Current timestep = 1819. State = [[-0.34868264  0.32473218]]. Action = [[-0.04866536 -0.17519712 -0.02435331 -0.33035845]]. Reward = [0.]
Curr episode timestep = 197
Current timestep = 1820. State = [[-0.34744823  0.31828538]]. Action = [[-0.23646075 -0.01963323 -0.21274616  0.09594905]]. Reward = [0.]
Curr episode timestep = 198
Current timestep = 1821. State = [[-0.3465776  0.3132462]]. Action = [[-0.15905437  0.16268754 -0.13714392  0.4554298 ]]. Reward = [0.]
Curr episode timestep = 199
Current timestep = 1822. State = [[-0.34618336  0.31036648]]. Action = [[-0.09292459  0.17021263  0.18818763  0.64977074]]. Reward = [0.]
Curr episode timestep = 200
Current timestep = 1823. State = [[-0.34654287  0.3084313 ]]. Action = [[ 0.01217172 -0.17103802  0.05474511 -0.12211514]]. Reward = [0.]
Curr episode timestep = 201
Current timestep = 1824. State = [[-0.3467033   0.30216616]]. Action = [[-0.00520334 -0.12829454 -0.09165469 -0.0430212 ]]. Reward = [0.]
Curr episode timestep = 202
Current timestep = 1825. State = [[-0.34671128  0.29560676]]. Action = [[-0.12849852  0.17342824  0.1448639  -0.6776633 ]]. Reward = [0.]
Curr episode timestep = 203
Current timestep = 1826. State = [[-0.35081285  0.2955521 ]]. Action = [[-0.12151212  0.23194993 -0.1467969  -0.85897714]]. Reward = [0.]
Curr episode timestep = 204
Current timestep = 1827. State = [[-0.35645998  0.30032122]]. Action = [[ 0.20832664 -0.00158802  0.20737427 -0.47570217]]. Reward = [0.]
Curr episode timestep = 205
Current timestep = 1828. State = [[-0.3566144   0.30236533]]. Action = [[-0.05335721  0.12784776 -0.15446635  0.5646957 ]]. Reward = [0.]
Curr episode timestep = 206
Current timestep = 1829. State = [[-0.35817564  0.30493742]]. Action = [[ 0.11642054  0.01507384  0.1881772  -0.2007209 ]]. Reward = [0.]
Curr episode timestep = 207
Current timestep = 1830. State = [[-0.35792595  0.3064217 ]]. Action = [[-0.04606853  0.24207759  0.20239395  0.78361976]]. Reward = [0.]
Curr episode timestep = 208
Current timestep = 1831. State = [[-0.35779142  0.30693543]]. Action = [[ 0.10983026 -0.03615271  0.10282665  0.30834353]]. Reward = [0.]
Curr episode timestep = 209
Current timestep = 1832. State = [[-0.35610804  0.30692175]]. Action = [[ 0.2355226  -0.22464624 -0.21784855  0.5168792 ]]. Reward = [0.]
Curr episode timestep = 210
Current timestep = 1833. State = [[-0.34848264  0.302555  ]]. Action = [[ 0.11031115  0.03627566 -0.21743391  0.84959817]]. Reward = [0.]
Curr episode timestep = 211
Current timestep = 1834. State = [[-0.34230113  0.29975653]]. Action = [[ 0.01132765  0.15785658 -0.17964867 -0.8253677 ]]. Reward = [0.]
Curr episode timestep = 212
Current timestep = 1835. State = [[-0.33790085  0.30151117]]. Action = [[ 0.12414047 -0.22398111 -0.16608012 -0.6038404 ]]. Reward = [0.]
Curr episode timestep = 213
Current timestep = 1836. State = [[-0.3318323   0.29826617]]. Action = [[-0.21345447  0.20373195  0.23667032  0.52951777]]. Reward = [0.]
Curr episode timestep = 214
Current timestep = 1837. State = [[-0.33241388  0.30078346]]. Action = [[-0.00785556  0.13293338  0.06116351  0.6864672 ]]. Reward = [0.]
Curr episode timestep = 215
Current timestep = 1838. State = [[-0.3341633   0.30498564]]. Action = [[-0.16175468  0.16416526 -0.12364703 -0.97051054]]. Reward = [0.]
Curr episode timestep = 216
Current timestep = 1839. State = [[-0.33784848  0.31097984]]. Action = [[ 0.01332492 -0.15818273  0.12605268  0.88933635]]. Reward = [0.]
Curr episode timestep = 217
Current timestep = 1840. State = [[-0.3391327  0.312141 ]]. Action = [[ 0.22165313  0.01134986 -0.16927701 -0.8431182 ]]. Reward = [0.]
Curr episode timestep = 218
Current timestep = 1841. State = [[-0.3363763   0.31224036]]. Action = [[-0.15606372  0.06050196 -0.13119252 -0.16369438]]. Reward = [0.]
Curr episode timestep = 219
Current timestep = 1842. State = [[-0.33656794  0.31480148]]. Action = [[ 0.00523368 -0.01412122  0.19374865 -0.81435996]]. Reward = [0.]
Curr episode timestep = 220
Current timestep = 1843. State = [[-0.33693942  0.31636462]]. Action = [[-0.06745693  0.21762568 -0.00762756  0.42843306]]. Reward = [0.]
Curr episode timestep = 221
Current timestep = 1844. State = [[-0.33655196  0.31681868]]. Action = [[ 0.19094402 -0.05924574 -0.07362184 -0.11866754]]. Reward = [0.]
Curr episode timestep = 222
Current timestep = 1845. State = [[-0.3330014   0.31548977]]. Action = [[-0.18115717  0.16815335  0.13113126 -0.0701189 ]]. Reward = [0.]
Curr episode timestep = 223
Current timestep = 1846. State = [[-0.3309365   0.31523013]]. Action = [[-0.18391941 -0.19160676  0.24684209 -0.3246839 ]]. Reward = [0.]
Curr episode timestep = 224
Current timestep = 1847. State = [[-0.3303409   0.31325343]]. Action = [[0.19407219 0.15959904 0.03968364 0.33565295]]. Reward = [0.]
Curr episode timestep = 225
Current timestep = 1848. State = [[-0.32944995  0.31157595]]. Action = [[ 0.07846209 -0.05825044 -0.22136638 -0.868299  ]]. Reward = [0.]
Curr episode timestep = 226
Current timestep = 1849. State = [[-0.32750398  0.30909148]]. Action = [[ 0.00663355 -0.03038797  0.15181446  0.36837173]]. Reward = [0.]
Curr episode timestep = 227
Current timestep = 1850. State = [[-0.3262482   0.30736127]]. Action = [[ 0.16544062  0.20485443 -0.19426447 -0.79054004]]. Reward = [0.]
Curr episode timestep = 228
Current timestep = 1851. State = [[-0.32548815  0.306283  ]]. Action = [[ 0.10715091  0.01461989 -0.00101525  0.36317718]]. Reward = [0.]
Curr episode timestep = 229
Current timestep = 1852. State = [[-0.32403305  0.30505326]]. Action = [[ 0.21107823 -0.21789357 -0.18300731  0.8701227 ]]. Reward = [0.]
Curr episode timestep = 230
Current timestep = 1853. State = [[-0.3174552   0.29840967]]. Action = [[-0.16410004  0.191199   -0.22415829 -0.08195531]]. Reward = [0.]
Curr episode timestep = 231
Current timestep = 1854. State = [[-0.31358555  0.29432592]]. Action = [[-0.12696572  0.12648624 -0.10094106 -0.4851805 ]]. Reward = [0.]
Curr episode timestep = 232
Current timestep = 1855. State = [[-0.3134214  0.2940879]]. Action = [[ 0.22595847 -0.0423059   0.20544136  0.38194466]]. Reward = [0.]
Curr episode timestep = 233
Current timestep = 1856. State = [[-0.31068563  0.2916963 ]]. Action = [[-0.23714428  0.24783927 -0.20441064 -0.07597542]]. Reward = [0.]
Curr episode timestep = 234
Current timestep = 1857. State = [[-0.312999    0.29541424]]. Action = [[ 0.02358115 -0.19006759  0.11278087 -0.57400215]]. Reward = [0.]
Curr episode timestep = 235
Current timestep = 1858. State = [[-0.31268865  0.29440364]]. Action = [[-0.00919996 -0.20790666  0.16100794  0.7410729 ]]. Reward = [0.]
Curr episode timestep = 236
Current timestep = 1859. State = [[-0.3106284   0.29051957]]. Action = [[0.18028462 0.19549024 0.18203822 0.02638316]]. Reward = [0.]
Curr episode timestep = 237
Current timestep = 1860. State = [[-0.30927408  0.29063377]]. Action = [[-0.11499974  0.06591147 -0.20025122  0.43172812]]. Reward = [0.]
Curr episode timestep = 238
Current timestep = 1861. State = [[-0.31039816  0.29210985]]. Action = [[-0.02540386  0.03597867  0.10055426 -0.45382154]]. Reward = [0.]
Curr episode timestep = 239
Current timestep = 1862. State = [[-0.31128144  0.29322183]]. Action = [[ 0.21538621  0.13638982 -0.0259105  -0.24346834]]. Reward = [0.]
Curr episode timestep = 240
Current timestep = 1863. State = [[-0.30890134  0.29606786]]. Action = [[-0.14135171  0.00228703  0.11316103 -0.23045933]]. Reward = [0.]
Curr episode timestep = 241
Current timestep = 1864. State = [[-0.30876732  0.29825532]]. Action = [[-0.0943431  -0.17632975  0.08792478  0.7496209 ]]. Reward = [0.]
Curr episode timestep = 242
Current timestep = 1865. State = [[-0.3086857  0.2980888]]. Action = [[ 0.15768367  0.00775507  0.13179296 -0.32607943]]. Reward = [0.]
Curr episode timestep = 243
Current timestep = 1866. State = [[-0.30735528  0.29705378]]. Action = [[-0.19029935 -0.00854456  0.0664233  -0.13082594]]. Reward = [0.]
Curr episode timestep = 244
Current timestep = 1867. State = [[-0.30896702  0.2984366 ]]. Action = [[-0.13695975  0.17781556  0.05227253 -0.95466363]]. Reward = [0.]
Curr episode timestep = 245
Current timestep = 1868. State = [[-0.3134626   0.30227506]]. Action = [[-0.21317169 -0.07903954 -0.02863111  0.8286096 ]]. Reward = [0.]
Curr episode timestep = 246
Current timestep = 1869. State = [[-0.32071963  0.30488464]]. Action = [[-0.21241885  0.01432821  0.2366027  -0.3319546 ]]. Reward = [0.]
Curr episode timestep = 247
Current timestep = 1870. State = [[-0.32950693  0.30758414]]. Action = [[0.1976453  0.09578368 0.01184392 0.88009906]]. Reward = [0.]
Curr episode timestep = 248
Current timestep = 1871. State = [[-0.33291316  0.31025216]]. Action = [[-0.20516367 -0.02776253 -0.22664854  0.43888223]]. Reward = [0.]
Curr episode timestep = 249
Current timestep = 1872. State = [[-0.3376047   0.31301564]]. Action = [[-0.15409902  0.00909746 -0.21825446 -0.8798289 ]]. Reward = [0.]
Curr episode timestep = 250
Current timestep = 1873. State = [[-0.34316614  0.31537765]]. Action = [[ 0.2136021  -0.20986164  0.23947686  0.9569397 ]]. Reward = [0.]
Curr episode timestep = 251
Current timestep = 1874. State = [[-0.342794    0.31115833]]. Action = [[-0.18321802 -0.06295457 -0.0784141  -0.45064974]]. Reward = [0.]
Curr episode timestep = 252
Current timestep = 1875. State = [[-0.34534812  0.30736172]]. Action = [[ 0.12295821 -0.0523705  -0.20157494  0.2430116 ]]. Reward = [0.]
Curr episode timestep = 253
Current timestep = 1876. State = [[-0.3460614   0.30229694]]. Action = [[0.07380283 0.1812709  0.13442707 0.71745944]]. Reward = [0.]
Curr episode timestep = 254
Current timestep = 1877. State = [[-0.3459386   0.29944852]]. Action = [[ 0.11589867  0.16450244  0.22707787 -0.61946285]]. Reward = [0.]
Curr episode timestep = 255
Current timestep = 1878. State = [[-0.3461878   0.29995385]]. Action = [[-0.07173517  0.22041124 -0.04687229 -0.32132906]]. Reward = [0.]
Curr episode timestep = 256
Current timestep = 1879. State = [[-0.34840024  0.30352262]]. Action = [[ 0.04617712 -0.06578752 -0.17618798  0.30402088]]. Reward = [0.]
Curr episode timestep = 257
Current timestep = 1880. State = [[-0.34919384  0.30458763]]. Action = [[-0.18222146 -0.18301466 -0.0832935  -0.9327371 ]]. Reward = [0.]
Curr episode timestep = 258
Current timestep = 1881. State = [[-0.3499916   0.30365822]]. Action = [[ 0.11078739 -0.11027077  0.06808317  0.6049223 ]]. Reward = [0.]
Curr episode timestep = 259
Current timestep = 1882. State = [[-0.34859008  0.30061588]]. Action = [[-0.18146822 -0.17629497  0.09323782  0.30116153]]. Reward = [0.]
Curr episode timestep = 260
Current timestep = 1883. State = [[-0.35071635  0.29587454]]. Action = [[ 0.0679895   0.20391476  0.20924592 -0.1342398 ]]. Reward = [0.]
Curr episode timestep = 261
Current timestep = 1884. State = [[-0.35286367  0.2963418 ]]. Action = [[-0.12903655  0.13861382  0.13597676 -0.43045604]]. Reward = [0.]
Curr episode timestep = 262
Current timestep = 1885. State = [[-0.35707444  0.29950163]]. Action = [[-0.18237184 -0.19207323  0.15415871 -0.6906729 ]]. Reward = [0.]
Curr episode timestep = 263
Current timestep = 1886. State = [[-0.36265686  0.29830426]]. Action = [[ 0.16625485 -0.12819208  0.16146833  0.1766231 ]]. Reward = [0.]
Curr episode timestep = 264
Current timestep = 1887. State = [[-0.3632295  0.2940877]]. Action = [[0.09031689 0.12817007 0.09240711 0.12834394]]. Reward = [0.]
Curr episode timestep = 265
Current timestep = 1888. State = [[-0.3622322   0.29330224]]. Action = [[-0.05550924 -0.07365075  0.09817645 -0.7681351 ]]. Reward = [0.]
Curr episode timestep = 266
Current timestep = 1889. State = [[-0.3619474   0.29226223]]. Action = [[0.12329879 0.03739944 0.13855287 0.6918961 ]]. Reward = [0.]
Curr episode timestep = 267
Current timestep = 1890. State = [[-0.36078402  0.2910954 ]]. Action = [[ 0.06300673 -0.07860267  0.08303881  0.49718523]]. Reward = [0.]
Curr episode timestep = 268
Current timestep = 1891. State = [[-0.35799935  0.28835934]]. Action = [[ 0.17558587 -0.1261199   0.22245961 -0.25670934]]. Reward = [0.]
Curr episode timestep = 269
Current timestep = 1892. State = [[-0.35220298  0.2828376 ]]. Action = [[ 0.07278568 -0.08875763  0.04924503 -0.7673707 ]]. Reward = [0.]
Curr episode timestep = 270
Current timestep = 1893. State = [[-0.34660587  0.27778593]]. Action = [[-0.13456175 -0.18749732  0.09598255  0.20445788]]. Reward = [0.]
Curr episode timestep = 271
Current timestep = 1894. State = [[-0.3447378   0.27060354]]. Action = [[-0.08989948 -0.11507946 -0.15092333 -0.48251456]]. Reward = [0.]
Curr episode timestep = 272
Current timestep = 1895. State = [[-0.34608507  0.26232502]]. Action = [[ 0.05690125  0.10720724 -0.22040099 -0.2582962 ]]. Reward = [0.]
Curr episode timestep = 273
Current timestep = 1896. State = [[-0.3458586  0.2592792]]. Action = [[-0.08146735 -0.05189906 -0.19976899 -0.7945327 ]]. Reward = [0.]
Curr episode timestep = 274
Current timestep = 1897. State = [[-0.3471611  0.2560891]]. Action = [[-0.04784541 -0.01421244  0.15398479 -0.23483151]]. Reward = [0.]
Curr episode timestep = 275
Current timestep = 1898. State = [[-0.3489152   0.25400162]]. Action = [[-0.11691868 -0.01066329  0.18991429  0.8186095 ]]. Reward = [0.]
Curr episode timestep = 276
Current timestep = 1899. State = [[-0.3514055  0.2528387]]. Action = [[ 0.01041365 -0.17962128 -0.11344385 -0.7231739 ]]. Reward = [0.]
Curr episode timestep = 277
Current timestep = 1900. State = [[-0.35292655  0.24825287]]. Action = [[-0.08561048  0.03191465  0.19411114 -0.63694483]]. Reward = [0.]
Curr episode timestep = 278
Current timestep = 1901. State = [[-0.3546135   0.24619713]]. Action = [[ 0.09409577 -0.04025792  0.20863697  0.3716179 ]]. Reward = [0.]
Curr episode timestep = 279
Current timestep = 1902. State = [[-0.35479444  0.24318694]]. Action = [[-0.05512044 -0.19712685 -0.10762985 -0.61934584]]. Reward = [0.]
Curr episode timestep = 280
Current timestep = 1903. State = [[-0.35694194  0.236004  ]]. Action = [[-0.1893697   0.02299112 -0.17494181  0.2565421 ]]. Reward = [0.]
Curr episode timestep = 281
Current timestep = 1904. State = [[-0.36189443  0.23094095]]. Action = [[ 0.0600453  -0.07560135  0.03557882 -0.4299782 ]]. Reward = [0.]
Curr episode timestep = 282
Current timestep = 1905. State = [[-0.3641428   0.22592561]]. Action = [[-0.23826863 -0.07909013  0.22034001  0.73778534]]. Reward = [0.]
Curr episode timestep = 283
Current timestep = 1906. State = [[-0.36953416  0.22309168]]. Action = [[-0.11152704  0.18351156  0.23650014  0.6047504 ]]. Reward = [0.]
Curr episode timestep = 284
Current timestep = 1907. State = [[-0.37718612  0.2242912 ]]. Action = [[-0.09678742 -0.11342223  0.11312056  0.91191137]]. Reward = [0.]
Curr episode timestep = 285
Current timestep = 1908. State = [[-0.3854087   0.22209789]]. Action = [[-0.00158386  0.11885184 -0.17327295  0.45756626]]. Reward = [0.]
Curr episode timestep = 286
Current timestep = 1909. State = [[-0.39173043  0.22271912]]. Action = [[ 0.22589862  0.04379982  0.08850363 -0.6556358 ]]. Reward = [0.]
Curr episode timestep = 287
Current timestep = 1910. State = [[-0.39127433  0.22347082]]. Action = [[ 0.03501326 -0.2341768  -0.20475475  0.50025964]]. Reward = [0.]
Curr episode timestep = 288
Current timestep = 1911. State = [[-0.39095354  0.22356573]]. Action = [[-0.00965165  0.04203385  0.22366685  0.338773  ]]. Reward = [0.]
Curr episode timestep = 289
Current timestep = 1912. State = [[-0.3902856  0.2230849]]. Action = [[ 0.12631065 -0.20970978 -0.16983326 -0.43032324]]. Reward = [0.]
Curr episode timestep = 290
Current timestep = 1913. State = [[-0.38698885  0.21980964]]. Action = [[-0.16597736  0.09261847  0.07487386  0.49231517]]. Reward = [0.]
Curr episode timestep = 291
Current timestep = 1914. State = [[-0.3851941   0.21769522]]. Action = [[-0.11522585  0.05066562  0.19559911  0.8825464 ]]. Reward = [0.]
Curr episode timestep = 292
Current timestep = 1915. State = [[-0.3840824   0.21621688]]. Action = [[-0.11927143  0.137739    0.1398164   0.9867369 ]]. Reward = [0.]
Curr episode timestep = 293
Current timestep = 1916. State = [[-0.3833405   0.21484631]]. Action = [[ 0.15396297 -0.08031151  0.179093    0.24236977]]. Reward = [0.]
Curr episode timestep = 294
Current timestep = 1917. State = [[-0.37917483  0.2114534 ]]. Action = [[-0.08238991 -0.05944109  0.19817501 -0.22770011]]. Reward = [0.]
Curr episode timestep = 295
Current timestep = 1918. State = [[-0.37624806  0.20919608]]. Action = [[ 0.13270158 -0.03134213 -0.20005086  0.443771  ]]. Reward = [0.]
Curr episode timestep = 296
Current timestep = 1919. State = [[-0.3717705   0.20643991]]. Action = [[-0.19278765 -0.12088692  0.04106966  0.21287465]]. Reward = [0.]
Curr episode timestep = 297
Current timestep = 1920. State = [[-0.36873722  0.2047337 ]]. Action = [[-0.01110679  0.06570673 -0.17857964 -0.19611233]]. Reward = [0.]
Curr episode timestep = 298
Current timestep = 1921. State = [[-0.3680279   0.20404455]]. Action = [[-0.17244312 -0.17062192 -0.15483919  0.911572  ]]. Reward = [0.]
Curr episode timestep = 299
Current timestep = 1922. State = [[-0.36871713  0.20174131]]. Action = [[0.19297719 0.20755154 0.1911099  0.39675307]]. Reward = [0.]
Curr episode timestep = 300
Current timestep = 1923. State = [[-0.36861506  0.20256191]]. Action = [[-0.16410337  0.1127066  -0.19543414 -0.27385342]]. Reward = [0.]
Curr episode timestep = 301
Current timestep = 1924. State = [[-0.37108013  0.20526384]]. Action = [[-0.05822751  0.07155696 -0.06319232  0.9016464 ]]. Reward = [0.]
Curr episode timestep = 302
Current timestep = 1925. State = [[-0.37398437  0.20898004]]. Action = [[ 0.14862171  0.22189593 -0.08254014  0.9169135 ]]. Reward = [0.]
Curr episode timestep = 303
Current timestep = 1926. State = [[-0.37326542  0.2152253 ]]. Action = [[ 0.06602877 -0.06935447 -0.19523636 -0.00560135]]. Reward = [0.]
Curr episode timestep = 304
Current timestep = 1927. State = [[-0.37145522  0.217776  ]]. Action = [[-0.03909661 -0.05715057  0.10604778 -0.9307719 ]]. Reward = [0.]
Curr episode timestep = 305
Current timestep = 1928. State = [[-0.37122798  0.21846896]]. Action = [[0.20655227 0.00527579 0.13445696 0.44446564]]. Reward = [0.]
Curr episode timestep = 306
Current timestep = 1929. State = [[-0.3678319   0.21827437]]. Action = [[ 0.11486992 -0.20382598 -0.20494035 -0.9441287 ]]. Reward = [0.]
Curr episode timestep = 307
Current timestep = 1930. State = [[-0.36178973  0.21463442]]. Action = [[-0.0738032  -0.01350938  0.16747254  0.40428042]]. Reward = [0.]
Curr episode timestep = 308
Current timestep = 1931. State = [[-0.35859877  0.21334915]]. Action = [[ 0.18336937 -0.05021182 -0.00161105  0.07507277]]. Reward = [0.]
Curr episode timestep = 309
Current timestep = 1932. State = [[-0.35262832  0.21011677]]. Action = [[ 0.22678548 -0.12012571  0.11016795  0.5345944 ]]. Reward = [0.]
Curr episode timestep = 310
Current timestep = 1933. State = [[-0.3434711   0.20335627]]. Action = [[ 0.1310696  -0.15975471  0.14301091  0.22602749]]. Reward = [0.]
Curr episode timestep = 311
Current timestep = 1934. State = [[-0.33446208  0.19572937]]. Action = [[0.20644486 0.13165244 0.1448138  0.01907837]]. Reward = [0.]
Curr episode timestep = 312
Current timestep = 1935. State = [[-0.32455096  0.19317392]]. Action = [[-0.18135697  0.13320106  0.1368388   0.31632745]]. Reward = [0.]
Curr episode timestep = 313
Current timestep = 1936. State = [[-0.32038605  0.1957246 ]]. Action = [[ 0.08705753 -0.16220163 -0.21094395 -0.7379223 ]]. Reward = [0.]
Curr episode timestep = 314
Current timestep = 1937. State = [[-0.31654987  0.19395421]]. Action = [[ 0.16654748 -0.04923701 -0.17468569 -0.8374157 ]]. Reward = [0.]
Curr episode timestep = 315
Current timestep = 1938. State = [[-0.3109486  0.1911412]]. Action = [[ 0.04927549 -0.09873244 -0.19119537  0.1687746 ]]. Reward = [0.]
Curr episode timestep = 316
Current timestep = 1939. State = [[-0.3050913   0.18681748]]. Action = [[0.00445542 0.03064841 0.17463809 0.78832626]]. Reward = [0.]
Curr episode timestep = 317
Current timestep = 1940. State = [[-0.3006614   0.18460646]]. Action = [[ 0.12039542  0.14303243  0.17968798 -0.6002756 ]]. Reward = [0.]
Curr episode timestep = 318
Current timestep = 1941. State = [[-0.29628888  0.18581681]]. Action = [[ 0.14972377  0.04688156  0.05099756 -0.6593486 ]]. Reward = [0.]
Curr episode timestep = 319
Current timestep = 1942. State = [[-0.2903151   0.18906647]]. Action = [[-0.0102337   0.2037915  -0.08804175  0.6277882 ]]. Reward = [0.]
Curr episode timestep = 320
Current timestep = 1943. State = [[-0.28596133  0.19577485]]. Action = [[-0.09726623  0.08374602  0.14244282  0.65944576]]. Reward = [0.]
Curr episode timestep = 321
Current timestep = 1944. State = [[-0.28367972  0.20332232]]. Action = [[ 0.14873886  0.15382242 -0.13860938  0.4091841 ]]. Reward = [0.]
Curr episode timestep = 322
Current timestep = 1945. State = [[-0.28001013  0.21057108]]. Action = [[ 0.08036765  0.02494147 -0.1997276  -0.24817097]]. Reward = [0.]
Curr episode timestep = 323
Current timestep = 1946. State = [[-0.27591157  0.21684028]]. Action = [[-0.02851522  0.17465365  0.01918548  0.7872963 ]]. Reward = [0.]
Curr episode timestep = 324
Current timestep = 1947. State = [[-0.27283555  0.22425632]]. Action = [[-0.19653499 -0.24499385  0.06036296 -0.72644824]]. Reward = [0.]
Curr episode timestep = 325
Current timestep = 1948. State = [[-0.27299455  0.22431783]]. Action = [[-0.05683011 -0.06175929 -0.05156505 -0.09622365]]. Reward = [0.]
Curr episode timestep = 326
Current timestep = 1949. State = [[-0.2732097   0.22422136]]. Action = [[0.01059675 0.12095112 0.005869   0.89648414]]. Reward = [0.]
Curr episode timestep = 327
Current timestep = 1950. State = [[-0.27433056  0.22532457]]. Action = [[ 0.09757599 -0.22937846  0.14864576 -0.40874386]]. Reward = [0.]
Curr episode timestep = 328
Current timestep = 1951. State = [[-0.27258527  0.22272614]]. Action = [[-0.05269225  0.1941027   0.16084838 -0.63683164]]. Reward = [0.]
Curr episode timestep = 329
Current timestep = 1952. State = [[-0.27397773  0.22438121]]. Action = [[ 0.04410109  0.07872939  0.13400155 -0.2899403 ]]. Reward = [0.]
Curr episode timestep = 330
Current timestep = 1953. State = [[-0.27425987  0.22592962]]. Action = [[ 0.23801026 -0.09746987  0.001434    0.7786794 ]]. Reward = [0.]
Curr episode timestep = 331
Current timestep = 1954. State = [[-0.2710935   0.22518308]]. Action = [[-0.24003021  0.01804447 -0.11749232 -0.9757617 ]]. Reward = [0.]
Curr episode timestep = 332
Current timestep = 1955. State = [[-0.27191567  0.2261757 ]]. Action = [[-0.09119502 -0.00703524 -0.01876774  0.2738607 ]]. Reward = [0.]
Curr episode timestep = 333
Current timestep = 1956. State = [[-0.27296728  0.22739358]]. Action = [[ 0.06153014  0.20236301 -0.02246219  0.15790951]]. Reward = [0.]
Curr episode timestep = 334
Current timestep = 1957. State = [[-0.2748314   0.23082037]]. Action = [[ 0.12568188 -0.00444047  0.07844052 -0.6891493 ]]. Reward = [0.]
Curr episode timestep = 335
Current timestep = 1958. State = [[-0.27403238  0.23266345]]. Action = [[ 0.11283946  0.12927389  0.24624553 -0.47808605]]. Reward = [0.]
Curr episode timestep = 336
Current timestep = 1959. State = [[-0.27158797  0.23658279]]. Action = [[0.15300792 0.07418776 0.12891579 0.81099916]]. Reward = [0.]
Curr episode timestep = 337
Current timestep = 1960. State = [[-0.26679033  0.24192135]]. Action = [[-0.14012988  0.03891754 -0.1772051  -0.06829274]]. Reward = [0.]
Curr episode timestep = 338
Current timestep = 1961. State = [[-0.2654548   0.24713266]]. Action = [[-0.22221111  0.12482867 -0.23269549  0.20549488]]. Reward = [0.]
Curr episode timestep = 339
Current timestep = 1962. State = [[-0.26800177  0.25356665]]. Action = [[ 0.0149816  -0.24698873 -0.10562804 -0.09505343]]. Reward = [0.]
Curr episode timestep = 340
Current timestep = 1963. State = [[-0.2679034   0.25288874]]. Action = [[ 0.15379927 -0.12765177  0.1366728  -0.19756639]]. Reward = [0.]
Curr episode timestep = 341
Current timestep = 1964. State = [[-0.26494065  0.24972877]]. Action = [[ 0.20385382  0.19154638 -0.20094773 -0.93236184]]. Reward = [0.]
Curr episode timestep = 342
Current timestep = 1965. State = [[-0.26222098  0.2503912 ]]. Action = [[-0.08592279  0.06878865  0.18314388  0.7307241 ]]. Reward = [0.]
Curr episode timestep = 343
Current timestep = 1966. State = [[-0.2614305  0.2518011]]. Action = [[ 0.0929369  -0.11186977  0.09654835  0.24125862]]. Reward = [0.]
Curr episode timestep = 344
Current timestep = 1967. State = [[-0.26001284  0.25096428]]. Action = [[-0.10880616  0.16871804  0.03328136 -0.3591516 ]]. Reward = [0.]
Curr episode timestep = 345
Current timestep = 1968. State = [[-0.26111236  0.25492397]]. Action = [[-0.1669959   0.19741476  0.08048719  0.64291286]]. Reward = [0.]
Curr episode timestep = 346
Current timestep = 1969. State = [[-0.26433238  0.26166755]]. Action = [[ 0.10223329 -0.02330688 -0.05935958 -0.31633842]]. Reward = [0.]
Curr episode timestep = 347
Current timestep = 1970. State = [[-0.26514506  0.26548263]]. Action = [[-0.2077149  -0.09970555 -0.0274027  -0.78182745]]. Reward = [0.]
Curr episode timestep = 348
Current timestep = 1971. State = [[-0.2667543   0.26653996]]. Action = [[ 0.12765536 -0.18753196 -0.01615648  0.4284885 ]]. Reward = [0.]
Curr episode timestep = 349
Current timestep = 1972. State = [[-0.26452294  0.26393744]]. Action = [[ 0.00067827  0.06064874 -0.22491582 -0.6613721 ]]. Reward = [0.]
Curr episode timestep = 350
Current timestep = 1973. State = [[-0.26425532  0.26337707]]. Action = [[-0.19941463 -0.05098467 -0.09798455  0.00753462]]. Reward = [0.]
Curr episode timestep = 351
Current timestep = 1974. State = [[-0.26611516  0.2634944 ]]. Action = [[-0.05293502  0.17206383 -0.2476405   0.9807656 ]]. Reward = [0.]
Curr episode timestep = 352
Current timestep = 1975. State = [[-0.2695984   0.26656103]]. Action = [[-0.09035917 -0.16864909  0.09086153  0.60175395]]. Reward = [0.]
Curr episode timestep = 353
Current timestep = 1976. State = [[-0.27263978  0.26461762]]. Action = [[ 0.06528449 -0.20627666  0.0634906  -0.10417563]]. Reward = [0.]
Curr episode timestep = 354
Current timestep = 1977. State = [[-0.27409354  0.25846577]]. Action = [[ 0.17130035  0.15973765 -0.14907579 -0.41375983]]. Reward = [0.]
Curr episode timestep = 355
Current timestep = 1978. State = [[-0.27318034  0.2571873 ]]. Action = [[-0.12981689  0.11843333 -0.24280551 -0.73602265]]. Reward = [0.]
Curr episode timestep = 356
Current timestep = 1979. State = [[-0.2750181   0.25928748]]. Action = [[ 0.06680089 -0.08802572  0.07083637  0.13472879]]. Reward = [0.]
Curr episode timestep = 357
Current timestep = 1980. State = [[-0.2748689   0.25916454]]. Action = [[ 0.03500807  0.2323845  -0.20825844  0.8652587 ]]. Reward = [0.]
Curr episode timestep = 358
Current timestep = 1981. State = [[-0.27564803  0.2621956 ]]. Action = [[-0.20906384 -0.15080248  0.13434994  0.06779051]]. Reward = [0.]
Curr episode timestep = 359
Current timestep = 1982. State = [[-0.2777888   0.26265746]]. Action = [[-0.12581201 -0.19652452  0.21564543  0.6201478 ]]. Reward = [0.]
Curr episode timestep = 360
Current timestep = 1983. State = [[-0.2805644   0.25961372]]. Action = [[-0.05684358 -0.03189604  0.21190727  0.87912583]]. Reward = [0.]
Curr episode timestep = 361
Current timestep = 1984. State = [[-0.2838714   0.25636712]]. Action = [[-0.02358913 -0.12822355  0.1353153   0.23384333]]. Reward = [0.]
Curr episode timestep = 362
Current timestep = 1985. State = [[-0.2874557   0.25130984]]. Action = [[0.08273003 0.21906751 0.24032408 0.10592949]]. Reward = [0.]
Curr episode timestep = 363
Current timestep = 1986. State = [[-0.2886456  0.2517975]]. Action = [[-0.02885957 -0.20598294 -0.21226259 -0.3627174 ]]. Reward = [0.]
Curr episode timestep = 364
Current timestep = 1987. State = [[-0.2899008   0.24884784]]. Action = [[-0.18684329  0.16274199  0.1508252   0.0605166 ]]. Reward = [0.]
Curr episode timestep = 365
Current timestep = 1988. State = [[-0.2940836  0.2510248]]. Action = [[ 0.07869324  0.21725953  0.24656636 -0.08476526]]. Reward = [0.]
Curr episode timestep = 366
Current timestep = 1989. State = [[-0.2983845   0.25594628]]. Action = [[-0.11924347 -0.03125402  0.18772516  0.5832555 ]]. Reward = [0.]
Curr episode timestep = 367
Current timestep = 1990. State = [[-0.30270967  0.25980657]]. Action = [[-0.02878937  0.14748526 -0.03887668  0.7752639 ]]. Reward = [0.]
Curr episode timestep = 368
Current timestep = 1991. State = [[-0.30730718  0.26480764]]. Action = [[ 0.2108647   0.17047012 -0.0505915   0.7593695 ]]. Reward = [0.]
Curr episode timestep = 369
Current timestep = 1992. State = [[-0.30831563  0.2704755 ]]. Action = [[-0.0620507   0.12065881  0.09597895 -0.14302826]]. Reward = [0.]
Curr episode timestep = 370
Current timestep = 1993. State = [[-0.30896723  0.2765353 ]]. Action = [[0.10690475 0.03672454 0.12161762 0.50431573]]. Reward = [0.]
Curr episode timestep = 371
Current timestep = 1994. State = [[-0.3076721   0.28158757]]. Action = [[0.13640735 0.16580665 0.04064673 0.7398679 ]]. Reward = [0.]
Curr episode timestep = 372
Current timestep = 1995. State = [[-0.3037962   0.28880844]]. Action = [[ 0.02924007 -0.09042323 -0.21093877  0.5446689 ]]. Reward = [0.]
Curr episode timestep = 373
Current timestep = 1996. State = [[-0.30087993  0.29220402]]. Action = [[-0.1536736   0.04519537  0.20351288  0.89237154]]. Reward = [0.]
Curr episode timestep = 374
Current timestep = 1997. State = [[-0.3013426  0.2950732]]. Action = [[ 0.2117916   0.1189605  -0.12066777  0.8513055 ]]. Reward = [0.]
Curr episode timestep = 375
Current timestep = 1998. State = [[-0.29876596  0.29917654]]. Action = [[-0.11752179  0.04642707  0.14457771  0.6499977 ]]. Reward = [0.]
Curr episode timestep = 376
Current timestep = 1999. State = [[-0.2985638   0.30438918]]. Action = [[-0.01425862  0.14538276  0.20249355 -0.6298214 ]]. Reward = [0.]
Curr episode timestep = 377
Current timestep = 2000. State = [[-0.2978707   0.31142572]]. Action = [[-0.04831567  0.16334784 -0.20199093  0.9743383 ]]. Reward = [0.]
Curr episode timestep = 378
Current timestep = 2001. State = [[-0.29810977  0.31557536]]. Action = [[-0.21964559  0.04344344  0.20691216 -0.6488759 ]]. Reward = [0.]
Curr episode timestep = 379
Current timestep = 2002. State = [[-0.30231783  0.32008317]]. Action = [[-0.20288686  0.10679591  0.20741653 -0.23962611]]. Reward = [0.]
Curr episode timestep = 380
Current timestep = 2003. State = [[-0.30524766  0.32290018]]. Action = [[ 0.1088371  -0.12342769 -0.09038106  0.07997692]]. Reward = [0.]
Curr episode timestep = 381
Current timestep = 2004. State = [[-0.30402565  0.3227111 ]]. Action = [[-0.24239248  0.02073884 -0.04743835 -0.9507996 ]]. Reward = [0.]
Curr episode timestep = 382
Current timestep = 2005. State = [[-0.30300817  0.32228872]]. Action = [[-0.20022336  0.09158453 -0.23236917  0.45373392]]. Reward = [0.]
Curr episode timestep = 383
Current timestep = 2006. State = [[-0.30297866  0.32249674]]. Action = [[-0.12629807 -0.11671582 -0.18855815 -0.94275206]]. Reward = [0.]
Curr episode timestep = 384
Current timestep = 2007. State = [[-0.30371296  0.3211914 ]]. Action = [[ 0.08324385  0.18018281 -0.21276373  0.32215405]]. Reward = [0.]
Curr episode timestep = 385
Current timestep = 2008. State = [[-0.30432037  0.32005468]]. Action = [[-0.23119503  0.118983    0.03312752  0.70663714]]. Reward = [0.]
Curr episode timestep = 386
Current timestep = 2009. State = [[-0.30448708  0.31926528]]. Action = [[ 0.17721543  0.12907568  0.09925741 -0.6418614 ]]. Reward = [0.]
Curr episode timestep = 387
Current timestep = 2010. State = [[-0.30467737  0.31891468]]. Action = [[ 0.0946506  -0.05653349  0.21000665  0.8650149 ]]. Reward = [0.]
Curr episode timestep = 388
Current timestep = 2011. State = [[-0.30330637  0.31723762]]. Action = [[-0.23090275  0.2181881  -0.23313595  0.735621  ]]. Reward = [0.]
Curr episode timestep = 389
Current timestep = 2012. State = [[-0.30270615  0.31628272]]. Action = [[-0.18144657 -0.17970175  0.10280532  0.19754219]]. Reward = [0.]
Curr episode timestep = 390
Current timestep = 2013. State = [[-0.30386534  0.31165347]]. Action = [[ 0.2208809  -0.06327787 -0.10100916  0.26770937]]. Reward = [0.]
Curr episode timestep = 391
Current timestep = 2014. State = [[-0.30087855  0.30702022]]. Action = [[-0.1072723   0.23806176  0.13980979  0.6745113 ]]. Reward = [0.]
Curr episode timestep = 392
Current timestep = 2015. State = [[-0.29911307  0.30475378]]. Action = [[0.13531613 0.11704135 0.13852525 0.36397576]]. Reward = [0.]
Curr episode timestep = 393
Current timestep = 2016. State = [[-0.2981164  0.3041554]]. Action = [[-0.20985219  0.11070627 -0.19850662  0.10804892]]. Reward = [0.]
Curr episode timestep = 394
Current timestep = 2017. State = [[-0.30033782  0.30620405]]. Action = [[ 0.21187669 -0.23207062 -0.15222369 -0.8178959 ]]. Reward = [0.]
Curr episode timestep = 395
Current timestep = 2018. State = [[-0.29661843  0.3030308 ]]. Action = [[ 0.16282481  0.04287025 -0.15431444  0.24312246]]. Reward = [0.]
Curr episode timestep = 396
Current timestep = 2019. State = [[-0.29255477  0.29971623]]. Action = [[-0.18127257 -0.18166265 -0.10226566  0.6913729 ]]. Reward = [0.]
Curr episode timestep = 397
Current timestep = 2020. State = [[-0.2905119   0.29613122]]. Action = [[ 0.23830801 -0.05990498  0.15107322 -0.62938267]]. Reward = [0.]
Curr episode timestep = 398
Current timestep = 2021. State = [[-0.28485006  0.29101783]]. Action = [[0.20950484 0.12668505 0.22434914 0.80225325]]. Reward = [0.]
Curr episode timestep = 399
Current timestep = 2022. State = [[-0.2794503   0.28831866]]. Action = [[-0.15946338  0.06277418  0.09482145  0.7957957 ]]. Reward = [0.]
Curr episode timestep = 400
Current timestep = 2023. State = [[-0.27802497  0.28836796]]. Action = [[ 0.1034154  -0.19539914  0.06251663  0.37326407]]. Reward = [0.]
Curr episode timestep = 401
Current timestep = 2024. State = [[-0.27473494  0.28544995]]. Action = [[-0.16861159  0.15300143 -0.06865987  0.46294355]]. Reward = [0.]
Curr episode timestep = 402
Current timestep = 2025. State = [[-0.2758661   0.28661022]]. Action = [[-0.09355572 -0.12409596  0.01850995  0.7012955 ]]. Reward = [0.]
Curr episode timestep = 403
Current timestep = 2026. State = [[-0.27620202  0.2858943 ]]. Action = [[ 0.02166069 -0.19980545 -0.16549784 -0.01007539]]. Reward = [0.]
Curr episode timestep = 404
Current timestep = 2027. State = [[-0.27625045  0.28088295]]. Action = [[-0.23710753  0.07817    -0.19286387  0.09966063]]. Reward = [0.]
Curr episode timestep = 405
Current timestep = 2028. State = [[-0.28171754  0.2798279 ]]. Action = [[-0.10276437  0.15250522  0.05643404 -0.4080131 ]]. Reward = [0.]
Curr episode timestep = 406
Current timestep = 2029. State = [[-0.28686696  0.28217524]]. Action = [[ 0.16399246 -0.1657702  -0.22667125 -0.2371946 ]]. Reward = [0.]
Curr episode timestep = 407
Current timestep = 2030. State = [[-0.28750402  0.27914435]]. Action = [[ 0.14611182  0.0278658   0.03690192 -0.58667904]]. Reward = [0.]
Curr episode timestep = 408
Current timestep = 2031. State = [[-0.28589833  0.27739632]]. Action = [[-0.12071502 -0.03720322  0.15119278  0.04142308]]. Reward = [0.]
Curr episode timestep = 409
Current timestep = 2032. State = [[-0.28639302  0.27600563]]. Action = [[ 0.07232979  0.03280681  0.00262028 -0.8401466 ]]. Reward = [0.]
Curr episode timestep = 410
Current timestep = 2033. State = [[-0.28621382  0.27524957]]. Action = [[ 0.0086965  -0.05600525 -0.19481824 -0.02989835]]. Reward = [0.]
Curr episode timestep = 411
Current timestep = 2034. State = [[-0.28585106  0.27375686]]. Action = [[ 0.04672804  0.06361037  0.15990919 -0.6433744 ]]. Reward = [0.]
Curr episode timestep = 412
Current timestep = 2035. State = [[-0.28575048  0.27382755]]. Action = [[-0.05399317  0.1667166  -0.1116502  -0.8907829 ]]. Reward = [0.]
Curr episode timestep = 413
Current timestep = 2036. State = [[-0.28741282  0.2763594 ]]. Action = [[0.22013086 0.21422103 0.02332005 0.81751657]]. Reward = [0.]
Curr episode timestep = 414
Current timestep = 2037. State = [[-0.28436375  0.28210664]]. Action = [[ 0.18458813 -0.04554555  0.09655666  0.42483652]]. Reward = [0.]
Curr episode timestep = 415
Current timestep = 2038. State = [[-0.2788025   0.28525078]]. Action = [[0.14083123 0.10968512 0.19027    0.53653693]]. Reward = [0.]
Curr episode timestep = 416
Current timestep = 2039. State = [[-0.27163768  0.2913043 ]]. Action = [[ 0.05221096  0.21780205  0.21952385 -0.22113752]]. Reward = [0.]
Curr episode timestep = 417
Current timestep = 2040. State = [[-0.26639354  0.29978463]]. Action = [[-0.06114992 -0.04006726 -0.04899234  0.82357347]]. Reward = [0.]
Curr episode timestep = 418
Current timestep = 2041. State = [[-0.2636225   0.30553505]]. Action = [[-0.2037658   0.12712869 -0.06234859  0.86096287]]. Reward = [0.]
Curr episode timestep = 419
Current timestep = 2042. State = [[-0.26491076  0.3122023 ]]. Action = [[ 0.06416985 -0.02249917 -0.22196124  0.3130108 ]]. Reward = [0.]
Curr episode timestep = 420
Current timestep = 2043. State = [[-0.2649927  0.3150842]]. Action = [[-0.02818109  0.02764004  0.22950876  0.4738773 ]]. Reward = [0.]
Curr episode timestep = 421
Current timestep = 2044. State = [[-0.26520133  0.31721058]]. Action = [[-0.20639656  0.02910754 -0.14383891 -0.64296824]]. Reward = [0.]
Curr episode timestep = 422
Current timestep = 2045. State = [[-0.26515886  0.31858706]]. Action = [[0.02705446 0.19974235 0.00738195 0.73178625]]. Reward = [0.]
Curr episode timestep = 423
Current timestep = 2046. State = [[-0.26501384  0.31975147]]. Action = [[0.20211262 0.18511221 0.07554197 0.88815   ]]. Reward = [0.]
Curr episode timestep = 424
Current timestep = 2047. State = [[-0.26496074  0.3202694 ]]. Action = [[ 0.10406056  0.12628412  0.24427518 -0.80120575]]. Reward = [0.]
Curr episode timestep = 425
Current timestep = 2048. State = [[-0.2648528   0.32062164]]. Action = [[ 0.12647676 -0.09089798  0.08664292 -0.3719691 ]]. Reward = [0.]
Curr episode timestep = 426
Current timestep = 2049. State = [[-0.26276112  0.31898507]]. Action = [[-0.00077988  0.07174146  0.24635166  0.45931447]]. Reward = [0.]
Curr episode timestep = 427
Current timestep = 2050. State = [[-0.2619367   0.31840143]]. Action = [[ 0.15650314  0.09551716 -0.14573333  0.802552  ]]. Reward = [0.]
Curr episode timestep = 428
Current timestep = 2051. State = [[-0.26140198  0.31787226]]. Action = [[ 0.00383225 -0.12774263 -0.04031116  0.41896284]]. Reward = [0.]
Curr episode timestep = 429
Current timestep = 2052. State = [[-0.25930175  0.31576326]]. Action = [[-0.01170887 -0.12148434 -0.20666632 -0.3538552 ]]. Reward = [0.]
Curr episode timestep = 430
Current timestep = 2053. State = [[-0.25631455  0.31232795]]. Action = [[ 0.02826574 -0.04771066  0.11822399 -0.5065096 ]]. Reward = [0.]
Curr episode timestep = 431
Current timestep = 2054. State = [[-0.25331563  0.30888298]]. Action = [[-0.11628297 -0.15133943 -0.07080539 -0.8683625 ]]. Reward = [0.]
Curr episode timestep = 432
Current timestep = 2055. State = [[-0.2526559   0.30430025]]. Action = [[-0.13493684  0.09678847  0.10524762  0.09702134]]. Reward = [0.]
Curr episode timestep = 433
Current timestep = 2056. State = [[-0.25559586  0.30309844]]. Action = [[-0.18323575  0.2280963  -0.18779574 -0.09788811]]. Reward = [0.]
Curr episode timestep = 434
Current timestep = 2057. State = [[-0.2575502   0.30184272]]. Action = [[-0.14071006  0.22229305 -0.21608524  0.56933   ]]. Reward = [0.]
Curr episode timestep = 435
Current timestep = 2058. State = [[-0.2584584   0.30120054]]. Action = [[-0.2142066  -0.10173878  0.03168887 -0.49253702]]. Reward = [0.]
Curr episode timestep = 436
Current timestep = 2059. State = [[-0.26244697  0.29917538]]. Action = [[ 0.03532845 -0.02159142 -0.22421122 -0.9213892 ]]. Reward = [0.]
Curr episode timestep = 437
Current timestep = 2060. State = [[-0.26486316  0.29743862]]. Action = [[ 0.08617342  0.18205857 -0.1666748   0.17389691]]. Reward = [0.]
Curr episode timestep = 438
Current timestep = 2061. State = [[-0.2663208   0.29903612]]. Action = [[ 0.2085258  -0.01459894 -0.0454205   0.659801  ]]. Reward = [0.]
Curr episode timestep = 439
Current timestep = 2062. State = [[-0.26522276  0.29854795]]. Action = [[-0.19633643  0.06090149  0.13231438 -0.58266777]]. Reward = [0.]
Curr episode timestep = 440
Current timestep = 2063. State = [[-0.26702002  0.30059013]]. Action = [[-0.00547914 -0.06245457 -0.12703007 -0.5222053 ]]. Reward = [0.]
Curr episode timestep = 441
Current timestep = 2064. State = [[-0.26707175  0.30061817]]. Action = [[ 0.13407397 -0.09937701  0.17718253  0.24705696]]. Reward = [0.]
Curr episode timestep = 442
Current timestep = 2065. State = [[-0.26507953  0.29858938]]. Action = [[-0.04182126 -0.15152566  0.10013837  0.837057  ]]. Reward = [0.]
Curr episode timestep = 443
Current timestep = 2066. State = [[-0.26366425  0.29454923]]. Action = [[ 0.15363508 -0.04559025  0.05845174 -0.84930587]]. Reward = [0.]
Curr episode timestep = 444
Current timestep = 2067. State = [[-0.2597695   0.28900167]]. Action = [[ 0.18764156 -0.09411339  0.13737464 -0.04826677]]. Reward = [0.]
Curr episode timestep = 445
Current timestep = 2068. State = [[-0.25392988  0.28284064]]. Action = [[-0.03355107 -0.17901069 -0.04710434 -0.71822965]]. Reward = [0.]
Curr episode timestep = 446
Current timestep = 2069. State = [[-0.24933109  0.2753239 ]]. Action = [[-0.11306499  0.14591897  0.20696473 -0.45851904]]. Reward = [0.]
Curr episode timestep = 447
Current timestep = 2070. State = [[-0.24958923  0.27446133]]. Action = [[-0.1304943   0.12184846 -0.14876737 -0.12334323]]. Reward = [0.]
Curr episode timestep = 448
Current timestep = 2071. State = [[-0.25223398  0.2764886 ]]. Action = [[-0.2405484  -0.09997036  0.00878623  0.29589224]]. Reward = [0.]
Curr episode timestep = 449
Current timestep = 2072. State = [[-0.2576367   0.27619642]]. Action = [[-0.13493831 -0.14697917 -0.00485159 -0.82540685]]. Reward = [0.]
Curr episode timestep = 450
Current timestep = 2073. State = [[-0.26482671  0.2723589 ]]. Action = [[ 0.0828169   0.2099993  -0.20952345 -0.8267484 ]]. Reward = [0.]
Curr episode timestep = 451
Current timestep = 2074. State = [[-0.26935947  0.27454782]]. Action = [[-0.1807812  -0.00779715  0.1294471  -0.49579513]]. Reward = [0.]
Curr episode timestep = 452
Current timestep = 2075. State = [[-0.2748344   0.27708936]]. Action = [[ 0.0667218   0.13634968 -0.18521772 -0.67956644]]. Reward = [0.]
Curr episode timestep = 453
Current timestep = 2076. State = [[-0.2782294   0.28109568]]. Action = [[-0.00822528  0.08221042 -0.06078082 -0.26061147]]. Reward = [0.]
Curr episode timestep = 454
Current timestep = 2077. State = [[-0.2813812  0.2849092]]. Action = [[-0.18631752 -0.24396789  0.17971739  0.21196175]]. Reward = [0.]
Curr episode timestep = 455
Current timestep = 2078. State = [[-0.28514016  0.2832461 ]]. Action = [[-0.1708094   0.07210124 -0.12155809  0.98677254]]. Reward = [0.]
Curr episode timestep = 456
Current timestep = 2079. State = [[-0.29246464  0.2832153 ]]. Action = [[-0.04545954 -0.17504041 -0.02722052 -0.06293923]]. Reward = [0.]
Curr episode timestep = 457
Current timestep = 2080. State = [[-0.29903448  0.27853027]]. Action = [[ 0.15119785  0.07975844 -0.00397676 -0.3989637 ]]. Reward = [0.]
Curr episode timestep = 458
Current timestep = 2081. State = [[-0.30130213  0.27649036]]. Action = [[ 0.1319918   0.10162598  0.21897101 -0.21625304]]. Reward = [0.]
Curr episode timestep = 459
Current timestep = 2082. State = [[-0.3010389   0.27669728]]. Action = [[ 0.06831607  0.02904096 -0.06625116  0.01311052]]. Reward = [0.]
Curr episode timestep = 460
Current timestep = 2083. State = [[-0.3004376  0.2766212]]. Action = [[ 0.01131421 -0.18627286  0.08131778 -0.45415568]]. Reward = [0.]
Curr episode timestep = 461
Current timestep = 2084. State = [[-0.29815087  0.27373874]]. Action = [[ 0.02550328 -0.23450401  0.11509848 -0.7594703 ]]. Reward = [0.]
Curr episode timestep = 462
Current timestep = 2085. State = [[-0.29502633  0.2665863 ]]. Action = [[-0.08246401 -0.09107578 -0.01665656  0.06360936]]. Reward = [0.]
Curr episode timestep = 463
Current timestep = 2086. State = [[-0.29400843  0.2603307 ]]. Action = [[-0.06701192  0.0367772   0.16791838  0.44279337]]. Reward = [0.]
Curr episode timestep = 464
Current timestep = 2087. State = [[-0.29526022  0.25617343]]. Action = [[ 0.17680496 -0.09936471 -0.17413703  0.3220694 ]]. Reward = [0.]
Curr episode timestep = 465
Current timestep = 2088. State = [[-0.29343528  0.2502504 ]]. Action = [[0.17451054 0.03743997 0.12135428 0.72428083]]. Reward = [0.]
Curr episode timestep = 466
Current timestep = 2089. State = [[-0.28926235  0.24653742]]. Action = [[-0.02212754 -0.10576662 -0.08859238 -0.6954757 ]]. Reward = [0.]
Curr episode timestep = 467
Current timestep = 2090. State = [[-0.2858866   0.24256992]]. Action = [[-0.24543978 -0.15040147 -0.12520593  0.04961061]]. Reward = [0.]
Curr episode timestep = 468
Current timestep = 2091. State = [[-0.2870476   0.23722799]]. Action = [[0.20540076 0.11734754 0.02691135 0.02015901]]. Reward = [0.]
Curr episode timestep = 469
Current timestep = 2092. State = [[-0.28609934  0.23595746]]. Action = [[-0.12804574  0.0884963  -0.08513752  0.1938746 ]]. Reward = [0.]
Curr episode timestep = 470
Current timestep = 2093. State = [[-0.28756452  0.23735717]]. Action = [[-0.19261219  0.18372372 -0.08522457  0.9094467 ]]. Reward = [0.]
Curr episode timestep = 471
Current timestep = 2094. State = [[-0.29190135  0.24228467]]. Action = [[ 0.1472261   0.01528963  0.16413039 -0.01593864]]. Reward = [0.]
Curr episode timestep = 472
Current timestep = 2095. State = [[-0.2934676   0.24422878]]. Action = [[ 0.01233086 -0.04360855  0.20702189  0.05801082]]. Reward = [0.]
Curr episode timestep = 473
Current timestep = 2096. State = [[-0.2933281  0.2442872]]. Action = [[ 0.22043103 -0.20855607 -0.0476072  -0.4521668 ]]. Reward = [0.]
Curr episode timestep = 474
Current timestep = 2097. State = [[-0.2893726   0.24062426]]. Action = [[-0.06149773  0.16544002  0.24505785  0.28638136]]. Reward = [0.]
Curr episode timestep = 475
Current timestep = 2098. State = [[-0.28927937  0.24078874]]. Action = [[-0.22407751  0.16858941 -0.22281551 -0.43813026]]. Reward = [0.]
Curr episode timestep = 476
Current timestep = 2099. State = [[-0.29307312  0.2454076 ]]. Action = [[-0.05532007 -0.01726255  0.03978094  0.7867203 ]]. Reward = [0.]
Curr episode timestep = 477
Current timestep = 2100. State = [[-0.29592907  0.2484822 ]]. Action = [[-0.0868414  -0.13846451 -0.18611643  0.6012008 ]]. Reward = [0.]
Curr episode timestep = 478
Current timestep = 2101. State = [[-0.29816595  0.24786143]]. Action = [[ 0.16581437 -0.15744337  0.17949897  0.7368386 ]]. Reward = [0.]
Curr episode timestep = 479
Current timestep = 2102. State = [[-0.29712614  0.24372284]]. Action = [[-0.0621874  -0.0022762  -0.08790347 -0.9511932 ]]. Reward = [0.]
Curr episode timestep = 480
Current timestep = 2103. State = [[-0.29748628  0.24216413]]. Action = [[-0.15490924  0.19159603  0.08882335 -0.43710417]]. Reward = [0.]
Curr episode timestep = 481
Current timestep = 2104. State = [[-0.3007737   0.24488191]]. Action = [[-0.18849461 -0.1875348   0.22876644  0.91427135]]. Reward = [0.]
Curr episode timestep = 482
Current timestep = 2105. State = [[-0.30520284  0.24339338]]. Action = [[ 0.04367608 -0.14580955 -0.10747464 -0.85027623]]. Reward = [0.]
Curr episode timestep = 483
Current timestep = 2106. State = [[-0.30768648  0.23824461]]. Action = [[-0.17588854 -0.10046625  0.10247791  0.6135428 ]]. Reward = [0.]
Curr episode timestep = 484
Current timestep = 2107. State = [[-0.31348538  0.23280475]]. Action = [[-0.15770113  0.18682426  0.04782444  0.28280878]]. Reward = [0.]
Curr episode timestep = 485
Current timestep = 2108. State = [[-0.32135057  0.23394148]]. Action = [[-0.04822125  0.06097275  0.19136333 -0.30829012]]. Reward = [0.]
Curr episode timestep = 486
Current timestep = 2109. State = [[-0.32863122  0.23596428]]. Action = [[-0.11015978  0.11836195 -0.21499704  0.70794713]]. Reward = [0.]
Curr episode timestep = 487
Current timestep = 2110. State = [[-0.33553773  0.23990816]]. Action = [[ 0.14120379 -0.18729785  0.13653797 -0.3477397 ]]. Reward = [0.]
Curr episode timestep = 488
Current timestep = 2111. State = [[-0.33732328  0.23864384]]. Action = [[-0.07935971  0.14270097  0.02738571  0.42731917]]. Reward = [0.]
Curr episode timestep = 489
Current timestep = 2112. State = [[-0.33955744  0.24084844]]. Action = [[-0.0141643  -0.00725664 -0.20191066  0.61428356]]. Reward = [0.]
Curr episode timestep = 490
Current timestep = 2113. State = [[-0.34132615  0.24242501]]. Action = [[0.04238194 0.02984866 0.21448418 0.6071279 ]]. Reward = [0.]
Curr episode timestep = 491
Current timestep = 2114. State = [[-0.34206083  0.24324715]]. Action = [[ 0.07576591  0.10688302 -0.11567774 -0.18529862]]. Reward = [0.]
Curr episode timestep = 492
Current timestep = 2115. State = [[-0.3423037   0.24506454]]. Action = [[-0.17044936  0.0015814   0.07191089  0.50315595]]. Reward = [0.]
Curr episode timestep = 493
Current timestep = 2116. State = [[-0.34549227  0.24821533]]. Action = [[ 0.11286101  0.19991773  0.20751658 -0.84502566]]. Reward = [0.]
Curr episode timestep = 494
Current timestep = 2117. State = [[-0.34733158  0.25238237]]. Action = [[-0.20498356 -0.16563848 -0.15453747  0.9200982 ]]. Reward = [0.]
Curr episode timestep = 495
Current timestep = 2118. State = [[-0.35079667  0.25398228]]. Action = [[-0.04651926  0.05461437 -0.18735446  0.460541  ]]. Reward = [0.]
Curr episode timestep = 496
Current timestep = 2119. State = [[-0.3540724  0.2561235]]. Action = [[-0.03316471 -0.15138635 -0.05964139  0.8246113 ]]. Reward = [0.]
Curr episode timestep = 497
Current timestep = 2120. State = [[-0.3559072  0.2544311]]. Action = [[ 0.15414146 -0.17091538  0.14647466 -0.15787351]]. Reward = [0.]
Curr episode timestep = 498
Current timestep = 2121. State = [[-0.35483336  0.2483007 ]]. Action = [[ 0.07746154 -0.13846101 -0.13122131  0.3772049 ]]. Reward = [0.]
Curr episode timestep = 499
Current timestep = 2122. State = [[-0.35392153  0.24023077]]. Action = [[-0.15906963 -0.16829957 -0.20493776 -0.747558  ]]. Reward = [0.]
Curr episode timestep = 500
Current timestep = 2123. State = [[-0.35634115  0.2317373 ]]. Action = [[-0.22009137  0.18022332 -0.06100719  0.67057824]]. Reward = [0.]
Curr episode timestep = 501
Current timestep = 2124. State = [[-0.3627513   0.23112002]]. Action = [[-0.2059962   0.13891709 -0.1961255   0.98824763]]. Reward = [0.]
Curr episode timestep = 502
Current timestep = 2125. State = [[-0.36920792  0.23531677]]. Action = [[ 0.11926752  0.18361592 -0.20669362  0.9788637 ]]. Reward = [0.]
Curr episode timestep = 503
Current timestep = 2126. State = [[-0.3739362  0.2400303]]. Action = [[ 0.02449378 -0.17283204 -0.00651045  0.09712374]]. Reward = [0.]
Curr episode timestep = 504
Current timestep = 2127. State = [[-0.3750879   0.24003218]]. Action = [[-0.01568125  0.08457789 -0.04266219 -0.6381026 ]]. Reward = [0.]
Curr episode timestep = 505
Current timestep = 2128. State = [[-0.37657     0.24131642]]. Action = [[ 0.02272263  0.0205887   0.23409247 -0.7861401 ]]. Reward = [0.]
Curr episode timestep = 506
Current timestep = 2129. State = [[-0.37734497  0.24209505]]. Action = [[0.03433952 0.02292427 0.21634793 0.56979275]]. Reward = [0.]
Curr episode timestep = 507
Current timestep = 2130. State = [[-0.37768295  0.24262029]]. Action = [[ 0.16519982  0.1710245  -0.21482824  0.13835287]]. Reward = [0.]
Curr episode timestep = 508
Current timestep = 2131. State = [[-0.3755645   0.24622081]]. Action = [[ 0.06143299 -0.01325017 -0.20434873  0.7752812 ]]. Reward = [0.]
Curr episode timestep = 509
Current timestep = 2132. State = [[-0.37409893  0.24809061]]. Action = [[ 0.03978783  0.04878682  0.13282382 -0.9107618 ]]. Reward = [0.]
Curr episode timestep = 510
Current timestep = 2133. State = [[-0.37297422  0.2501549 ]]. Action = [[-0.07863799  0.14199641 -0.1298556  -0.6022341 ]]. Reward = [0.]
Curr episode timestep = 511
Current timestep = 2134. State = [[-0.37266052  0.2536828 ]]. Action = [[ 0.19601691 -0.153313   -0.23573515 -0.86816585]]. Reward = [0.]
Curr episode timestep = 512
Current timestep = 2135. State = [[-0.3702723  0.2529212]]. Action = [[-0.18146305  0.11058259 -0.21727078 -0.19881833]]. Reward = [0.]
Curr episode timestep = 513
Current timestep = 2136. State = [[-0.37129235  0.25497967]]. Action = [[-0.01881377 -0.19769022 -0.10689992 -0.20448524]]. Reward = [0.]
Curr episode timestep = 514
Current timestep = 2137. State = [[-0.37123045  0.2536086 ]]. Action = [[ 0.11118662  0.13741103 -0.1459907  -0.06741661]]. Reward = [0.]
Curr episode timestep = 515
Current timestep = 2138. State = [[-0.37083256  0.2542304 ]]. Action = [[-0.14397812  0.16817713 -0.0174053   0.8537123 ]]. Reward = [0.]
Curr episode timestep = 516
Current timestep = 2139. State = [[-0.37291133  0.259062  ]]. Action = [[-0.06769583  0.14703676  0.23093498 -0.8396985 ]]. Reward = [0.]
Curr episode timestep = 517
Current timestep = 2140. State = [[-0.3755461  0.2657075]]. Action = [[ 0.18740499  0.10426465 -0.1090059  -0.32227302]]. Reward = [0.]
Curr episode timestep = 518
Current timestep = 2141. State = [[-0.37454584  0.27229103]]. Action = [[-0.06862199  0.0719001  -0.18552993 -0.27628016]]. Reward = [0.]
Curr episode timestep = 519
Current timestep = 2142. State = [[-0.3736289   0.27904966]]. Action = [[0.17966527 0.05884549 0.1312179  0.15668392]]. Reward = [0.]
Curr episode timestep = 520
Current timestep = 2143. State = [[-0.36990446  0.28494748]]. Action = [[0.18909779 0.02773905 0.18570372 0.622457  ]]. Reward = [0.]
Curr episode timestep = 521
Current timestep = 2144. State = [[-0.36557138  0.28944096]]. Action = [[-0.19043031  0.2012077  -0.05726701  0.947593  ]]. Reward = [0.]
Curr episode timestep = 522
Current timestep = 2145. State = [[-0.36410406  0.297293  ]]. Action = [[ 0.12746388 -0.08262494  0.17184871  0.88898087]]. Reward = [0.]
Curr episode timestep = 523
Current timestep = 2146. State = [[-0.36201155  0.29984254]]. Action = [[-0.12377729 -0.21836959  0.10693997 -0.7587318 ]]. Reward = [0.]
Curr episode timestep = 524
Current timestep = 2147. State = [[-0.36096406  0.29855713]]. Action = [[ 0.12127015 -0.00461006  0.18570274  0.98466325]]. Reward = [0.]
Curr episode timestep = 525
Current timestep = 2148. State = [[-0.35819978  0.2967923 ]]. Action = [[ 0.06358093 -0.08004665  0.12989497 -0.2529508 ]]. Reward = [0.]
Curr episode timestep = 526
Current timestep = 2149. State = [[-0.3549914   0.29351148]]. Action = [[-0.2277951  -0.2396367   0.15387964  0.32146835]]. Reward = [0.]
Curr episode timestep = 527
Current timestep = 2150. State = [[-0.35606202  0.28699908]]. Action = [[-0.06049292 -0.17839023  0.24177063 -0.6285736 ]]. Reward = [0.]
Curr episode timestep = 528
Current timestep = 2151. State = [[-0.35821676  0.27879107]]. Action = [[-0.14580923  0.0266515  -0.16513653 -0.83170193]]. Reward = [0.]
Curr episode timestep = 529
Current timestep = 2152. State = [[-0.3625948  0.2732603]]. Action = [[ 0.0652037  -0.14494304  0.10935929 -0.29129744]]. Reward = [0.]
Curr episode timestep = 530
Current timestep = 2153. State = [[-0.36354503  0.26786724]]. Action = [[-0.17520142  0.23610151 -0.20752995  0.5912473 ]]. Reward = [0.]
Curr episode timestep = 531
Current timestep = 2154. State = [[-0.36716715  0.2692496 ]]. Action = [[ 0.15082759 -0.2363383  -0.15278095 -0.19632179]]. Reward = [0.]
Curr episode timestep = 532
Current timestep = 2155. State = [[-0.3679812   0.26361915]]. Action = [[ 0.0823338   0.07429305  0.16436437 -0.9531264 ]]. Reward = [0.]
Curr episode timestep = 533
Current timestep = 2156. State = [[-0.36727467  0.26191294]]. Action = [[0.03863928 0.1225971  0.16924876 0.79105926]]. Reward = [0.]
Curr episode timestep = 534
Current timestep = 2157. State = [[-0.36715388  0.26188347]]. Action = [[-0.19917499 -0.17590497  0.01357102  0.13652682]]. Reward = [0.]
Curr episode timestep = 535
Current timestep = 2158. State = [[-0.36871177  0.26027006]]. Action = [[ 0.07152033 -0.19616781 -0.22861052 -0.5086386 ]]. Reward = [0.]
Curr episode timestep = 536
Current timestep = 2159. State = [[-0.3686731   0.25395644]]. Action = [[-0.11425054 -0.06984296  0.15661156 -0.7026701 ]]. Reward = [0.]
Curr episode timestep = 537
Current timestep = 2160. State = [[-0.37081122  0.24813639]]. Action = [[-0.01818091 -0.16259633  0.0925656  -0.62923855]]. Reward = [0.]
Curr episode timestep = 538
Current timestep = 2161. State = [[-0.3716302   0.24115862]]. Action = [[ 0.07287675 -0.07832882  0.14642435 -0.15857875]]. Reward = [0.]
Curr episode timestep = 539
Current timestep = 2162. State = [[-0.3716679   0.23203298]]. Action = [[ 0.24169713 -0.23597823 -0.11893842 -0.888331  ]]. Reward = [0.]
Curr episode timestep = 540
Current timestep = 2163. State = [[-0.36826596  0.22042592]]. Action = [[ 0.17212361 -0.0297223   0.13491511 -0.31989408]]. Reward = [0.]
Curr episode timestep = 541
Current timestep = 2164. State = [[-0.3618821   0.21172509]]. Action = [[-0.20934479 -0.02633455 -0.13150853  0.58315945]]. Reward = [0.]
Curr episode timestep = 542
Current timestep = 2165. State = [[-0.3593274   0.20707552]]. Action = [[ 0.12507963 -0.13196675 -0.21234384  0.9694437 ]]. Reward = [0.]
Curr episode timestep = 543
Current timestep = 2166. State = [[-0.35646215  0.19993404]]. Action = [[-0.1729516   0.04189265 -0.14535876 -0.7765352 ]]. Reward = [0.]
Curr episode timestep = 544
Current timestep = 2167. State = [[-0.3578096   0.19600068]]. Action = [[ 0.01983446 -0.1662597  -0.23526101  0.808517  ]]. Reward = [0.]
Curr episode timestep = 545
Current timestep = 2168. State = [[-0.35973004  0.1895109 ]]. Action = [[ 0.23644316  0.22559631  0.11998874 -0.6077598 ]]. Reward = [0.]
Curr episode timestep = 546
Current timestep = 2169. State = [[-0.35747236  0.18842274]]. Action = [[ 0.02282923 -0.18939497 -0.08057979 -0.75359356]]. Reward = [0.]
Curr episode timestep = 547
Current timestep = 2170. State = [[-0.3536269   0.18444833]]. Action = [[-0.15743528  0.06313545 -0.22152586 -0.54304093]]. Reward = [0.]
Curr episode timestep = 548
Current timestep = 2171. State = [[-0.35362464  0.18452764]]. Action = [[ 0.15278074  0.19887316  0.17510176 -0.5023493 ]]. Reward = [0.]
Curr episode timestep = 549
Current timestep = 2172. State = [[-0.35319418  0.1868502 ]]. Action = [[ 0.12947124  0.01482224  0.0311226  -0.10838866]]. Reward = [0.]
Curr episode timestep = 550
Current timestep = 2173. State = [[-0.35066465  0.18887405]]. Action = [[ 0.04656163  0.05935627 -0.21667416 -0.13299143]]. Reward = [0.]
Curr episode timestep = 551
Current timestep = 2174. State = [[-0.3473948   0.19194655]]. Action = [[-0.05177435  0.07107949  0.01910233 -0.48996902]]. Reward = [0.]
Curr episode timestep = 552
Current timestep = 2175. State = [[-0.34624583  0.19607557]]. Action = [[-0.09694692  0.20341563 -0.15978871  0.78180885]]. Reward = [0.]
Curr episode timestep = 553
Current timestep = 2176. State = [[-0.34617794  0.20325312]]. Action = [[ 0.02679759  0.00435272  0.11926416 -0.6904353 ]]. Reward = [0.]
Curr episode timestep = 554
Current timestep = 2177. State = [[-0.34633654  0.20752084]]. Action = [[ 0.19214547 -0.09766129 -0.20339634  0.88376117]]. Reward = [0.]
Curr episode timestep = 555
Current timestep = 2178. State = [[-0.34385645  0.20799613]]. Action = [[ 0.01163957 -0.00805311 -0.23277777  0.6905451 ]]. Reward = [0.]
Curr episode timestep = 556
Current timestep = 2179. State = [[-0.34118184  0.20805049]]. Action = [[ 0.15921074 -0.040813   -0.02315442  0.09091163]]. Reward = [0.]
Curr episode timestep = 557
Current timestep = 2180. State = [[-0.33682358  0.20642193]]. Action = [[-0.06253062 -0.16470209  0.15305138  0.7306577 ]]. Reward = [0.]
Curr episode timestep = 558
Current timestep = 2181. State = [[-0.3329672   0.20217809]]. Action = [[ 0.08006692 -0.17164342 -0.0581058   0.78283346]]. Reward = [0.]
Curr episode timestep = 559
Current timestep = 2182. State = [[-0.3275149  0.1960328]]. Action = [[ 0.11854187 -0.02102175 -0.11244459  0.49207115]]. Reward = [0.]
Curr episode timestep = 560
Current timestep = 2183. State = [[-0.3219443   0.19073083]]. Action = [[ 0.11363375  0.15105179  0.21874213 -0.2547431 ]]. Reward = [0.]
Curr episode timestep = 561
Current timestep = 2184. State = [[-0.3159582  0.1906665]]. Action = [[ 0.24528012  0.05221134  0.22006744 -0.76080954]]. Reward = [0.]
Curr episode timestep = 562
Current timestep = 2185. State = [[-0.30752337  0.19234554]]. Action = [[ 0.19604746  0.1315465   0.19391781 -0.8326193 ]]. Reward = [0.]
Curr episode timestep = 563
Current timestep = 2186. State = [[-0.2974184   0.19739829]]. Action = [[ 2.7212501e-04  1.7006159e-01  3.5943240e-02 -2.7249527e-01]]. Reward = [0.]
Curr episode timestep = 564
Current timestep = 2187. State = [[-0.28936413  0.20485361]]. Action = [[-0.13867134  0.01532149 -0.11431937 -0.6405292 ]]. Reward = [0.]
Curr episode timestep = 565
Current timestep = 2188. State = [[-0.28678885  0.21044879]]. Action = [[-0.06387058 -0.05241188 -0.08219787 -0.6766602 ]]. Reward = [0.]
Curr episode timestep = 566
Current timestep = 2189. State = [[-0.2864063   0.21301302]]. Action = [[ 0.21010238  0.18829    -0.13381165  0.879333  ]]. Reward = [0.]
Curr episode timestep = 567
Current timestep = 2190. State = [[-0.28390673  0.21938689]]. Action = [[-0.18070725  0.23321378  0.17806679  0.4264393 ]]. Reward = [0.]
Curr episode timestep = 568
Current timestep = 2191. State = [[-0.28381076  0.22866184]]. Action = [[-0.02872823 -0.06419449 -0.14355117  0.05839252]]. Reward = [0.]
Curr episode timestep = 569
Current timestep = 2192. State = [[-0.28454626  0.23352972]]. Action = [[ 0.09004298  0.24402058 -0.21525802 -0.72770065]]. Reward = [0.]
Curr episode timestep = 570
Current timestep = 2193. State = [[-0.2832532   0.24240695]]. Action = [[0.00504097 0.13724145 0.1827203  0.85063636]]. Reward = [0.]
Curr episode timestep = 571
Current timestep = 2194. State = [[-0.282257    0.25088602]]. Action = [[ 0.11497369 -0.06802094 -0.12323    -0.6565642 ]]. Reward = [0.]
Curr episode timestep = 572
Current timestep = 2195. State = [[-0.27923465  0.25561765]]. Action = [[-0.01853272  0.1256899   0.14784023 -0.18408722]]. Reward = [0.]
Curr episode timestep = 573
Current timestep = 2196. State = [[-0.2770531   0.26160988]]. Action = [[ 0.19037202  0.15707356  0.23837131 -0.5018619 ]]. Reward = [0.]
Curr episode timestep = 574
Current timestep = 2197. State = [[-0.27249599  0.26986906]]. Action = [[-0.0747128   0.21514657 -0.12857558  0.03384268]]. Reward = [0.]
Curr episode timestep = 575
Current timestep = 2198. State = [[-0.27007294  0.28072578]]. Action = [[ 0.18518978  0.16758758 -0.10023688  0.44267952]]. Reward = [0.]
Curr episode timestep = 576
Current timestep = 2199. State = [[-0.26577774  0.29243943]]. Action = [[-0.15194425  0.17137033 -0.16413708 -0.69295156]]. Reward = [0.]
Curr episode timestep = 577
Current timestep = 2200. State = [[-0.26451108  0.30487007]]. Action = [[ 0.20874009  0.02064294  0.18373841 -0.3883195 ]]. Reward = [0.]
Curr episode timestep = 578
Current timestep = 2201. State = [[-0.26023972  0.31276965]]. Action = [[ 0.12892905  0.08037853 -0.00103328 -0.9443446 ]]. Reward = [0.]
Curr episode timestep = 579
Current timestep = 2202. State = [[-0.2544469   0.31967944]]. Action = [[-0.11621332 -0.13947093 -0.11072248 -0.47630823]]. Reward = [0.]
Curr episode timestep = 580
Current timestep = 2203. State = [[-0.2517331   0.32172573]]. Action = [[ 0.23564649  0.07290974  0.19841674 -0.8486666 ]]. Reward = [0.]
Curr episode timestep = 581
Current timestep = 2204. State = [[-0.25079882  0.32219684]]. Action = [[-0.18404068  0.0901843  -0.03584202  0.75648165]]. Reward = [0.]
Curr episode timestep = 582
Current timestep = 2205. State = [[-0.2502071   0.32210618]]. Action = [[ 0.09671992 -0.13816127  0.14622658 -0.86153454]]. Reward = [0.]
Curr episode timestep = 583
Current timestep = 2206. State = [[-0.24693473  0.31902698]]. Action = [[ 0.0830375   0.02932265 -0.22891055 -0.7357207 ]]. Reward = [0.]
Curr episode timestep = 584
Current timestep = 2207. State = [[-0.24513716  0.31710672]]. Action = [[-0.22589456 -0.09330228  0.23079044  0.16058636]]. Reward = [0.]
Curr episode timestep = 585
Current timestep = 2208. State = [[-0.24605447  0.31539893]]. Action = [[-0.08543515 -0.03458814 -0.19379391 -0.96572745]]. Reward = [0.]
Curr episode timestep = 586
Current timestep = 2209. State = [[-0.24781936  0.31384695]]. Action = [[-0.08382151  0.16094792  0.11206585  0.21704829]]. Reward = [0.]
Curr episode timestep = 587
Current timestep = 2210. State = [[-0.24848482  0.31303567]]. Action = [[-0.16078627  0.22855446  0.08486247 -0.34515107]]. Reward = [0.]
Curr episode timestep = 588
Current timestep = 2211. State = [[-0.24855284  0.31248552]]. Action = [[ 0.0930894   0.03191686 -0.0697414   0.9256227 ]]. Reward = [0.]
Curr episode timestep = 589
Current timestep = 2212. State = [[-0.24853583  0.3122887 ]]. Action = [[-0.13621813  0.0511688  -0.00745161  0.68803084]]. Reward = [0.]
Curr episode timestep = 590
Current timestep = 2213. State = [[-0.25037012  0.31388858]]. Action = [[ 0.03553385  0.22960958 -0.11228889  0.76144147]]. Reward = [0.]
Curr episode timestep = 591
Current timestep = 2214. State = [[-0.25173864  0.31485555]]. Action = [[-0.1977344  -0.05823559 -0.17802973  0.8892075 ]]. Reward = [0.]
Curr episode timestep = 592
Current timestep = 2215. State = [[-0.25510615  0.31471914]]. Action = [[ 0.03243175 -0.16978745  0.01455814  0.62567866]]. Reward = [0.]
Curr episode timestep = 593
Current timestep = 2216. State = [[-0.25818443  0.30947447]]. Action = [[-0.16233288  0.00836954  0.09675658 -0.29245234]]. Reward = [0.]
Curr episode timestep = 594
Current timestep = 2217. State = [[-0.26314825  0.3069987 ]]. Action = [[-0.01343609 -0.07990402  0.12210977  0.5096593 ]]. Reward = [0.]
Curr episode timestep = 595
Current timestep = 2218. State = [[-0.2662354  0.3030964]]. Action = [[ 0.12734702 -0.12679242  0.14517933  0.23030889]]. Reward = [0.]
Curr episode timestep = 596
Current timestep = 2219. State = [[-0.26709783  0.29709825]]. Action = [[ 0.08587104  0.12334073  0.21199739 -0.50391716]]. Reward = [0.]
Curr episode timestep = 597
Current timestep = 2220. State = [[-0.2663909   0.29570103]]. Action = [[ 0.02629954 -0.21667594 -0.04547559  0.98894584]]. Reward = [0.]
Curr episode timestep = 598
Current timestep = 2221. State = [[-0.26472193  0.2904491 ]]. Action = [[-0.07536612  0.08943805  0.23780191 -0.81218296]]. Reward = [0.]
Curr episode timestep = 599
Current timestep = 2222. State = [[-0.26534218  0.28902888]]. Action = [[ 0.1169183  -0.09395066 -0.05816148 -0.5252289 ]]. Reward = [0.]
Curr episode timestep = 600
Current timestep = 2223. State = [[-0.26463723  0.2849907 ]]. Action = [[-0.1320782  -0.15395659 -0.22969465 -0.86563665]]. Reward = [0.]
Curr episode timestep = 601
Current timestep = 2224. State = [[-0.26586044  0.2792175 ]]. Action = [[-0.22445612 -0.01248458 -0.17926073 -0.22603273]]. Reward = [0.]
Curr episode timestep = 602
Current timestep = 2225. State = [[-0.26943192  0.2765501 ]]. Action = [[-0.06543627  0.21637392 -0.15986037  0.02992511]]. Reward = [0.]
Curr episode timestep = 603
Current timestep = 2226. State = [[-0.27461874  0.27973855]]. Action = [[0.06064042 0.21240684 0.15829939 0.835253  ]]. Reward = [0.]
Curr episode timestep = 604
Current timestep = 2227. State = [[-0.27941453  0.28511366]]. Action = [[ 0.1934787   0.02725315 -0.1785524   0.15582609]]. Reward = [0.]
Curr episode timestep = 605
Current timestep = 2228. State = [[-0.27911115  0.28867927]]. Action = [[ 0.22919288  0.15182152 -0.14255233 -0.324167  ]]. Reward = [0.]
Curr episode timestep = 606
Current timestep = 2229. State = [[-0.27517083  0.2947978 ]]. Action = [[-0.1777282   0.2252905   0.13662252  0.21285105]]. Reward = [0.]
Curr episode timestep = 607
Current timestep = 2230. State = [[-0.274926    0.30500054]]. Action = [[ 0.21158999  0.20046878 -0.10328761  0.8464315 ]]. Reward = [0.]
Curr episode timestep = 608
Current timestep = 2231. State = [[-0.27167335  0.31475252]]. Action = [[ 0.20465666 -0.19379601 -0.22596833  0.16897464]]. Reward = [0.]
Curr episode timestep = 609
Current timestep = 2232. State = [[-0.26523444  0.31642964]]. Action = [[-0.21039112 -0.04299825 -0.2023141  -0.46390373]]. Reward = [0.]
Curr episode timestep = 610
Current timestep = 2233. State = [[-0.26369995  0.3171243 ]]. Action = [[ 0.04264152 -0.10468581  0.03012055  0.33882618]]. Reward = [0.]
Curr episode timestep = 611
Current timestep = 2234. State = [[-0.26200974  0.31526518]]. Action = [[ 0.02448297  0.18071175  0.11405313 -0.7394625 ]]. Reward = [0.]
Curr episode timestep = 612
Current timestep = 2235. State = [[-0.2609021   0.31399465]]. Action = [[-0.00717047 -0.07847504  0.07636753  0.15709889]]. Reward = [0.]
Curr episode timestep = 613
Current timestep = 2236. State = [[-0.25984567  0.3116658 ]]. Action = [[-0.06590281 -0.20719703  0.11228779 -0.5521513 ]]. Reward = [0.]
Curr episode timestep = 614
Current timestep = 2237. State = [[-0.25861216  0.30565888]]. Action = [[ 0.02426669 -0.14646596 -0.06864607  0.7510655 ]]. Reward = [0.]
Curr episode timestep = 615
Current timestep = 2238. State = [[-0.2566825  0.299577 ]]. Action = [[-0.16829705  0.1538938   0.09808239 -0.15802926]]. Reward = [0.]
Curr episode timestep = 616
Current timestep = 2239. State = [[-0.25780624  0.29948696]]. Action = [[ 0.15007487 -0.1665483   0.10933784 -0.5738736 ]]. Reward = [0.]
Curr episode timestep = 617
Current timestep = 2240. State = [[-0.2566838   0.29517794]]. Action = [[ 0.0597887   0.23007664  0.16520977 -0.19784695]]. Reward = [0.]
Curr episode timestep = 618
Current timestep = 2241. State = [[-0.25698552  0.29559907]]. Action = [[-0.21566293 -0.19611737  0.15508693 -0.5465988 ]]. Reward = [0.]
Curr episode timestep = 619
Current timestep = 2242. State = [[-0.2586104   0.29264984]]. Action = [[ 0.09353906 -0.20943715 -0.1597879  -0.42794704]]. Reward = [0.]
Curr episode timestep = 620
Current timestep = 2243. State = [[-0.2585088   0.28446367]]. Action = [[-0.04648307 -0.20828867 -0.0317329   0.8121109 ]]. Reward = [0.]
Curr episode timestep = 621
Current timestep = 2244. State = [[-0.25967005  0.27522722]]. Action = [[-0.08875313  0.24325734 -0.07714476  0.78790784]]. Reward = [0.]
Curr episode timestep = 622
Current timestep = 2245. State = [[-0.26234773  0.27564812]]. Action = [[-0.24551272  0.00198218  0.22546488 -0.04118901]]. Reward = [0.]
Curr episode timestep = 623
Current timestep = 2246. State = [[-0.26749256  0.27650806]]. Action = [[ 5.8869421e-03 -1.7640468e-01 -9.3609095e-05 -2.8392065e-01]]. Reward = [0.]
Curr episode timestep = 624
Current timestep = 2247. State = [[-0.2722752   0.27177092]]. Action = [[ 0.03351414  0.10423142 -0.19801314  0.6362231 ]]. Reward = [0.]
Curr episode timestep = 625
Current timestep = 2248. State = [[-0.27586654  0.27099493]]. Action = [[-0.14028466  0.09728381 -0.17747264 -0.24121839]]. Reward = [0.]
Curr episode timestep = 626
Current timestep = 2249. State = [[-0.28003877  0.27351454]]. Action = [[ 0.24425226  0.05186734 -0.17242575  0.6557354 ]]. Reward = [0.]
Curr episode timestep = 627
Current timestep = 2250. State = [[-0.28029582  0.2739933 ]]. Action = [[-0.04691382 -0.16336872  0.02399182  0.7006736 ]]. Reward = [0.]
Curr episode timestep = 628
Current timestep = 2251. State = [[-0.27964827  0.27276614]]. Action = [[-0.07432607 -0.00197425  0.20102918  0.69416165]]. Reward = [0.]
Curr episode timestep = 629
Current timestep = 2252. State = [[-0.28070718  0.27113348]]. Action = [[-3.7851632e-03 -8.3644688e-04  2.3500445e-01 -9.2530864e-01]]. Reward = [0.]
Curr episode timestep = 630
Current timestep = 2253. State = [[-0.2816011   0.26936996]]. Action = [[ 0.08644667 -0.17155758  0.1110619   0.50648284]]. Reward = [0.]
Curr episode timestep = 631
Current timestep = 2254. State = [[-0.28068608  0.2642628 ]]. Action = [[-0.13884896 -0.09753713  0.03700671 -0.8169606 ]]. Reward = [0.]
Curr episode timestep = 632
Current timestep = 2255. State = [[-0.2824912   0.26027578]]. Action = [[-0.18634036  0.10736641  0.00530875  0.8795341 ]]. Reward = [0.]
Curr episode timestep = 633
Current timestep = 2256. State = [[-0.28609258  0.26003423]]. Action = [[ 0.22825849 -0.01107255 -0.24003977 -0.9424928 ]]. Reward = [0.]
Curr episode timestep = 634
Current timestep = 2257. State = [[-0.28661266  0.2586351 ]]. Action = [[-0.07334688  0.21596557  0.08179539  0.6235304 ]]. Reward = [0.]
Curr episode timestep = 635
Current timestep = 2258. State = [[-0.2887905   0.26154506]]. Action = [[ 0.16138679 -0.17446178 -0.00368565  0.13796806]]. Reward = [0.]
Curr episode timestep = 636
Current timestep = 2259. State = [[-0.2871121   0.25947797]]. Action = [[ 0.23693204 -0.18048291  0.01589829 -0.6263817 ]]. Reward = [0.]
Curr episode timestep = 637
Current timestep = 2260. State = [[-0.28160238  0.25375858]]. Action = [[0.15855414 0.0907436  0.21352303 0.65925586]]. Reward = [0.]
Curr episode timestep = 638
Current timestep = 2261. State = [[-0.2751652  0.2515112]]. Action = [[ 0.0282115   0.22993511 -0.078532   -0.33640933]]. Reward = [0.]
Curr episode timestep = 639
Current timestep = 2262. State = [[-0.26982957  0.25603938]]. Action = [[-0.177783    0.18879735  0.12524724 -0.5274872 ]]. Reward = [0.]
Curr episode timestep = 640
Current timestep = 2263. State = [[-0.2684708  0.2628867]]. Action = [[-0.07439582 -0.09836258  0.1805163  -0.64527386]]. Reward = [0.]
Curr episode timestep = 641
Current timestep = 2264. State = [[-0.27067944  0.26621842]]. Action = [[-0.02111733  0.19144785  0.04203641 -0.21960533]]. Reward = [0.]
Curr episode timestep = 642
Current timestep = 2265. State = [[-0.27280694  0.27213877]]. Action = [[ 0.08634603  0.07566205  0.18886134 -0.8602801 ]]. Reward = [0.]
Curr episode timestep = 643
Current timestep = 2266. State = [[-0.2722066   0.27792916]]. Action = [[ 0.21430162  0.15873653 -0.03132251  0.37555027]]. Reward = [0.]
Curr episode timestep = 644
Current timestep = 2267. State = [[-0.26778144  0.28620943]]. Action = [[-0.13114789  0.1480855   0.15615454  0.3600359 ]]. Reward = [0.]
Curr episode timestep = 645
Current timestep = 2268. State = [[-0.2672541  0.2951198]]. Action = [[-0.19134302  0.16352612 -0.06777033  0.4658575 ]]. Reward = [0.]
Curr episode timestep = 646
Current timestep = 2269. State = [[-0.2698745   0.30514705]]. Action = [[ 0.04707438  0.04784644  0.11236739 -0.4281929 ]]. Reward = [0.]
Curr episode timestep = 647
Current timestep = 2270. State = [[-0.27177575  0.31187198]]. Action = [[ 0.03972897 -0.15000364 -0.02676739 -0.6178329 ]]. Reward = [0.]
Curr episode timestep = 648
Current timestep = 2271. State = [[-0.2719857   0.31288502]]. Action = [[-0.17802633 -0.14803436 -0.06703788  0.80798125]]. Reward = [0.]
Curr episode timestep = 649
Current timestep = 2272. State = [[-0.27266136  0.3116837 ]]. Action = [[ 0.15840241 -0.21665293  0.03840533  0.8305223 ]]. Reward = [0.]
Curr episode timestep = 650
Current timestep = 2273. State = [[-0.27111447  0.30499   ]]. Action = [[ 0.18106925 -0.0091296   0.06160662  0.33477914]]. Reward = [0.]
Curr episode timestep = 651
Current timestep = 2274. State = [[-0.26719236  0.30050075]]. Action = [[-0.13365942  0.06922108 -0.14293474  0.35946405]]. Reward = [0.]
Curr episode timestep = 652
Current timestep = 2275. State = [[-0.26683208  0.2999725 ]]. Action = [[ 0.0394583   0.21204346  0.15579778 -0.4055642 ]]. Reward = [0.]
Curr episode timestep = 653
Current timestep = 2276. State = [[-0.26732537  0.30293864]]. Action = [[0.17234045 0.01228997 0.13080627 0.10960364]]. Reward = [0.]
Curr episode timestep = 654
Current timestep = 2277. State = [[-0.26502112  0.30487075]]. Action = [[ 0.19164562  0.10132474 -0.05167085 -0.90046614]]. Reward = [0.]
Curr episode timestep = 655
Current timestep = 2278. State = [[-0.25899544  0.30913627]]. Action = [[-0.23495758 -0.13160811 -0.04353216 -0.05927587]]. Reward = [0.]
Curr episode timestep = 656
Current timestep = 2279. State = [[-0.25806537  0.3091349 ]]. Action = [[ 0.05960238 -0.12700032 -0.13778512  0.4044714 ]]. Reward = [0.]
Curr episode timestep = 657
Current timestep = 2280. State = [[-0.25637728  0.3068613 ]]. Action = [[-0.10722873  0.03060213 -0.1987878  -0.34042764]]. Reward = [0.]
Curr episode timestep = 658
Current timestep = 2281. State = [[-0.25638545  0.30637795]]. Action = [[ 0.20001686  0.06843454  0.01709208 -0.01954895]]. Reward = [0.]
Curr episode timestep = 659
Current timestep = 2282. State = [[-0.25471354  0.30598423]]. Action = [[-0.09022887 -0.13481565 -0.0522268  -0.96314305]]. Reward = [0.]
Curr episode timestep = 660
Current timestep = 2283. State = [[-0.25357673  0.3043171 ]]. Action = [[-0.20775254  0.18180984 -0.19815625 -0.24970329]]. Reward = [0.]
Curr episode timestep = 661
Current timestep = 2284. State = [[-0.25282618  0.30294344]]. Action = [[-0.23378475  0.01239675  0.01769447  0.80540264]]. Reward = [0.]
Curr episode timestep = 662
Current timestep = 2285. State = [[-0.25506103  0.303207  ]]. Action = [[ 0.08592185  0.23557895  0.2123346  -0.34541357]]. Reward = [0.]
Curr episode timestep = 663
Current timestep = 2286. State = [[-0.25628474  0.30309394]]. Action = [[-0.0366179  -0.18253638  0.05643648 -0.4837908 ]]. Reward = [0.]
Curr episode timestep = 664
Current timestep = 2287. State = [[-0.25710762  0.29974177]]. Action = [[-0.10078132 -0.0583061   0.00613341  0.96616733]]. Reward = [0.]
Curr episode timestep = 665
Current timestep = 2288. State = [[-0.25981158  0.29496136]]. Action = [[-0.21273637 -0.23737943 -0.01615746 -0.6573875 ]]. Reward = [0.]
Curr episode timestep = 666
Current timestep = 2289. State = [[-0.26670745  0.2863676 ]]. Action = [[-0.06051223  0.09812438  0.21375495 -0.9161889 ]]. Reward = [0.]
Curr episode timestep = 667
Current timestep = 2290. State = [[-0.27410936  0.28361467]]. Action = [[ 0.01229447  0.14084333 -0.2350395  -0.70223635]]. Reward = [0.]
Curr episode timestep = 668
Current timestep = 2291. State = [[-0.27927718  0.2845059 ]]. Action = [[-0.15593639  0.09525579 -0.12474185 -0.52786046]]. Reward = [0.]
Curr episode timestep = 669
Current timestep = 2292. State = [[-0.2860055   0.28803882]]. Action = [[ 0.24077761  0.18040481 -0.07260594  0.85999167]]. Reward = [0.]
Curr episode timestep = 670
Current timestep = 2293. State = [[-0.28865948  0.2919398 ]]. Action = [[-0.23540941 -0.10918292 -0.14336963  0.6963253 ]]. Reward = [0.]
Curr episode timestep = 671
Current timestep = 2294. State = [[-0.29233631  0.29402274]]. Action = [[ 0.04176006 -0.00972933  0.03469056 -0.32871097]]. Reward = [0.]
Curr episode timestep = 672
Current timestep = 2295. State = [[-0.2949549   0.29538786]]. Action = [[-0.17059213  0.18949026  0.1741001   0.43289518]]. Reward = [0.]
Curr episode timestep = 673
Current timestep = 2296. State = [[-0.30056578  0.30050707]]. Action = [[-0.05158353  0.0783298  -0.02058297  0.6034157 ]]. Reward = [0.]
Curr episode timestep = 674
Current timestep = 2297. State = [[-0.30602676  0.30557802]]. Action = [[0.17649633 0.0191212  0.23487952 0.15850902]]. Reward = [0.]
Curr episode timestep = 675
Current timestep = 2298. State = [[-0.3071908   0.30788115]]. Action = [[ 0.05237505  0.12875295 -0.06467068  0.88567364]]. Reward = [0.]
Curr episode timestep = 676
Current timestep = 2299. State = [[-0.30643538  0.31199938]]. Action = [[-0.00444762 -0.0725047   0.14396566  0.477026  ]]. Reward = [0.]
Curr episode timestep = 677
Current timestep = 2300. State = [[-0.30583048  0.31334278]]. Action = [[ 0.11849219  0.10635865 -0.11053976 -0.6091435 ]]. Reward = [0.]
Curr episode timestep = 678
Current timestep = 2301. State = [[-0.30564076  0.31411555]]. Action = [[-0.10483453 -0.13510895 -0.23610866  0.4991007 ]]. Reward = [0.]
Curr episode timestep = 679
Current timestep = 2302. State = [[-0.3056775   0.31407544]]. Action = [[ 0.08873332  0.06498414 -0.21618685 -0.8925247 ]]. Reward = [0.]
Curr episode timestep = 680
Current timestep = 2303. State = [[-0.30572265  0.31388697]]. Action = [[ 0.13557222  0.13733146 -0.10166138  0.49228835]]. Reward = [0.]
Curr episode timestep = 681
Current timestep = 2304. State = [[-0.30578712  0.31368414]]. Action = [[ 0.13992336  0.1701498   0.04769924 -0.15952009]]. Reward = [0.]
Curr episode timestep = 682
Current timestep = 2305. State = [[-0.30584988  0.31342766]]. Action = [[-0.13831404  0.06848195 -0.06437621  0.54168653]]. Reward = [0.]
Curr episode timestep = 683
Current timestep = 2306. State = [[-0.30558997  0.31284675]]. Action = [[ 0.23945624 -0.16540071  0.00285098 -0.21143478]]. Reward = [0.]
Curr episode timestep = 684
Current timestep = 2307. State = [[-0.301801   0.3078613]]. Action = [[-0.10635884 -0.06396426  0.12156218 -0.79949105]]. Reward = [0.]
Curr episode timestep = 685
Current timestep = 2308. State = [[-0.30055377  0.30348894]]. Action = [[ 0.14981747 -0.05484352  0.21931916  0.10972798]]. Reward = [0.]
Curr episode timestep = 686
Current timestep = 2309. State = [[-0.29764283  0.2985411 ]]. Action = [[-0.06254178 -0.11035696 -0.09815782 -0.31023765]]. Reward = [0.]
Curr episode timestep = 687
Current timestep = 2310. State = [[-0.29602036  0.29396808]]. Action = [[-0.16061896  0.22553557 -0.01863034 -0.2817967 ]]. Reward = [0.]
Curr episode timestep = 688
Current timestep = 2311. State = [[-0.29832357  0.29595363]]. Action = [[ 0.12144458 -0.02971259 -0.22997585  0.19047701]]. Reward = [0.]
Curr episode timestep = 689
Current timestep = 2312. State = [[-0.29815894  0.29591596]]. Action = [[-0.00274709  0.20949006 -0.20926586 -0.39975637]]. Reward = [0.]
Curr episode timestep = 690
Current timestep = 2313. State = [[-0.29927778  0.30028117]]. Action = [[-0.08373839  0.1298067   0.06489661 -0.6945358 ]]. Reward = [0.]
Curr episode timestep = 691
Current timestep = 2314. State = [[-0.3013468   0.30530977]]. Action = [[ 0.19382232 -0.20845027  0.15544814 -0.4033119 ]]. Reward = [0.]
Curr episode timestep = 692
Current timestep = 2315. State = [[-0.29867372  0.3034272 ]]. Action = [[ 0.23175424 -0.04371306  0.16319191  0.842258  ]]. Reward = [0.]
Curr episode timestep = 693
Current timestep = 2316. State = [[-0.2932986   0.30074346]]. Action = [[ 0.04761082 -0.03272039 -0.07747987 -0.86180824]]. Reward = [0.]
Curr episode timestep = 694
Current timestep = 2317. State = [[-0.2870428   0.29813245]]. Action = [[-0.0057919  -0.14278707  0.12247756  0.56366134]]. Reward = [0.]
Curr episode timestep = 695
Current timestep = 2318. State = [[-0.28246987  0.29390863]]. Action = [[0.19271958 0.01572978 0.13354096 0.46479714]]. Reward = [0.]
Curr episode timestep = 696
Current timestep = 2319. State = [[-0.2757835   0.29063118]]. Action = [[-0.10365036 -0.20554428  0.24185097 -0.12601382]]. Reward = [0.]
Curr episode timestep = 697
Current timestep = 2320. State = [[-0.27133265  0.28568038]]. Action = [[ 0.05021924  0.22130829  0.05164969 -0.05243689]]. Reward = [0.]
Curr episode timestep = 698
Current timestep = 2321. State = [[-0.26926467  0.2866758 ]]. Action = [[-0.15632153  0.22950894 -0.02214171 -0.10647261]]. Reward = [0.]
Curr episode timestep = 699
Current timestep = 2322. State = [[-0.27075708  0.2929854 ]]. Action = [[-0.14730492  0.09415781  0.063407    0.19759846]]. Reward = [0.]
Curr episode timestep = 700
Current timestep = 2323. State = [[-0.2749524   0.29924816]]. Action = [[ 0.14339414  0.1702599  -0.15889475 -0.37292725]]. Reward = [0.]
Curr episode timestep = 701
Current timestep = 2324. State = [[-0.27600205  0.30724767]]. Action = [[-0.11517608  0.01768458  0.15088761 -0.15297318]]. Reward = [0.]
Curr episode timestep = 702
Current timestep = 2325. State = [[-0.2783833   0.31341705]]. Action = [[-0.14450276  0.04839969 -0.07496601 -0.9849029 ]]. Reward = [0.]
Curr episode timestep = 703
Current timestep = 2326. State = [[-0.28327495  0.3186994 ]]. Action = [[-0.18864106 -0.06500761  0.14694646 -0.33155894]]. Reward = [0.]
Curr episode timestep = 704
Current timestep = 2327. State = [[-0.28912464  0.32274142]]. Action = [[-0.21147175  0.22567648 -0.1240679   0.35178435]]. Reward = [0.]
Curr episode timestep = 705
Current timestep = 2328. State = [[-0.29306674  0.32404348]]. Action = [[-0.08631568 -0.22507313 -0.13690314  0.27474105]]. Reward = [0.]
Curr episode timestep = 706
Current timestep = 2329. State = [[-0.2964453   0.32090724]]. Action = [[-0.15616204  0.15278548 -0.08466582  0.35552692]]. Reward = [0.]
Curr episode timestep = 707
Current timestep = 2330. State = [[-0.2990101   0.31816188]]. Action = [[0.04822624 0.03684321 0.21152288 0.3507825 ]]. Reward = [0.]
Curr episode timestep = 708
Current timestep = 2331. State = [[-0.30109012  0.31607175]]. Action = [[ 0.0890317   0.1129815  -0.08033299  0.06392586]]. Reward = [0.]
Curr episode timestep = 709
Current timestep = 2332. State = [[-0.3015857   0.31514564]]. Action = [[-0.06827039 -0.11216868  0.15390235 -0.5292389 ]]. Reward = [0.]
Curr episode timestep = 710
Current timestep = 2333. State = [[-0.30267248  0.31270248]]. Action = [[-0.18851994 -0.09546366 -0.01738057 -0.7883585 ]]. Reward = [0.]
Curr episode timestep = 711
Current timestep = 2334. State = [[-0.30629674  0.30923283]]. Action = [[ 0.20110917 -0.01603675 -0.06100635  0.97522926]]. Reward = [0.]
Curr episode timestep = 712
Current timestep = 2335. State = [[-0.30709416  0.30444887]]. Action = [[ 0.16655195 -0.05086584 -0.13547578  0.65776324]]. Reward = [0.]
Curr episode timestep = 713
Current timestep = 2336. State = [[-0.3046165  0.2999213]]. Action = [[-0.04942106 -0.13895503  0.07414383 -0.06977874]]. Reward = [0.]
Curr episode timestep = 714
Current timestep = 2337. State = [[-0.3033944   0.29470065]]. Action = [[-0.12141177 -0.12758143 -0.07553327  0.10519016]]. Reward = [0.]
Curr episode timestep = 715
Current timestep = 2338. State = [[-0.30543223  0.28837848]]. Action = [[-0.19632949  0.09564838  0.18470538  0.14844131]]. Reward = [0.]
Curr episode timestep = 716
Current timestep = 2339. State = [[-0.30960673  0.2864771 ]]. Action = [[ 0.2321994  -0.18153483 -0.13790572 -0.5118986 ]]. Reward = [0.]
Curr episode timestep = 717
Current timestep = 2340. State = [[-0.30824688  0.28078276]]. Action = [[-0.15308645 -0.08081943 -0.12250674 -0.9001012 ]]. Reward = [0.]
Curr episode timestep = 718
Current timestep = 2341. State = [[-0.30996907  0.27381298]]. Action = [[ 0.24547881 -0.2140822  -0.14916301 -0.3130784 ]]. Reward = [0.]
Curr episode timestep = 719
Current timestep = 2342. State = [[-0.3081086   0.26365432]]. Action = [[ 0.13784045 -0.1279728  -0.03124671 -0.20697212]]. Reward = [0.]
Curr episode timestep = 720
Current timestep = 2343. State = [[-0.30381063  0.25423974]]. Action = [[ 0.15966535  0.16302547  0.15191573 -0.9090559 ]]. Reward = [0.]
Curr episode timestep = 721
Current timestep = 2344. State = [[-0.29823574  0.25057817]]. Action = [[-0.14066644 -0.14394693  0.14327312  0.9732661 ]]. Reward = [0.]
Curr episode timestep = 722
Current timestep = 2345. State = [[-0.29534355  0.2469358 ]]. Action = [[0.10630363 0.19041988 0.17330849 0.30399108]]. Reward = [0.]
Curr episode timestep = 723
Current timestep = 2346. State = [[-0.2938979   0.24717703]]. Action = [[ 0.01690507 -0.1933983  -0.07791314  0.03764033]]. Reward = [0.]
Curr episode timestep = 724
Current timestep = 2347. State = [[-0.29107803  0.2435512 ]]. Action = [[ 0.178828    0.02393824 -0.06970081 -0.00074112]]. Reward = [0.]
Curr episode timestep = 725
Current timestep = 2348. State = [[-0.28656957  0.2403845 ]]. Action = [[ 0.11395514 -0.19697888 -0.22037816  0.27843213]]. Reward = [0.]
Curr episode timestep = 726
Current timestep = 2349. State = [[-0.2803107   0.23355731]]. Action = [[-0.21567906 -0.00876248 -0.02197866  0.8764417 ]]. Reward = [0.]
Curr episode timestep = 727
Current timestep = 2350. State = [[-0.278377    0.23065907]]. Action = [[ 0.19172776  0.14460391  0.07423502 -0.54763395]]. Reward = [0.]
Curr episode timestep = 728
Current timestep = 2351. State = [[-0.27659363  0.23062421]]. Action = [[-0.05278452  0.15680444 -0.0242998  -0.72359794]]. Reward = [0.]
Curr episode timestep = 729
Current timestep = 2352. State = [[-0.2757457  0.2339846]]. Action = [[-0.21045443 -0.19893363  0.0559043  -0.07058716]]. Reward = [0.]
Curr episode timestep = 730
Current timestep = 2353. State = [[-0.27785796  0.23352568]]. Action = [[-0.07729031  0.17732316  0.20080286 -0.7252931 ]]. Reward = [0.]
Curr episode timestep = 731
Current timestep = 2354. State = [[-0.28082258  0.23651874]]. Action = [[ 0.13221198 -0.00089979  0.23471662 -0.75713146]]. Reward = [0.]
Curr episode timestep = 732
