Current timestep = 0. State = [[-0.33700615  0.07271481]]. Action = [[0.00535686 0.09843931 0.         0.726084  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0746, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.34266388  0.07564709]]. Action = [[-0.08995712 -0.0094126   0.          0.92536795]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0665, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.34374136  0.07375043]]. Action = [[ 0.06051236 -0.05035317  0.          0.02274966]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0585, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.33907583  0.07548921]]. Action = [[ 0.08048449  0.04934516  0.         -0.5037932 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.33612245  0.07515086]]. Action = [[ 0.02319378 -0.05197925  0.          0.0654552 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0452, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.33866674  0.07605117]]. Action = [[-0.06597984  0.04400171  0.         -0.7164012 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0200, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.34551474  0.07239656]]. Action = [[-0.09446597 -0.09540669  0.         -0.55148816]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0179, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.3530832   0.06975583]]. Action = [[-0.08653336  0.01674326  0.          0.75422406]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0325, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.35376188  0.06624774]]. Action = [[ 0.06657053 -0.06994572  0.          0.36507952]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0306, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.34962842  0.06122543]]. Action = [[ 0.0522516  -0.04847732  0.          0.9673785 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0183, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.35187778  0.06342796]]. Action = [[-0.09424287  0.09771746  0.          0.9580653 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.35862172  0.0700952 ]]. Action = [[-0.06767087  0.08876287  0.         -0.77738863]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.36321306  0.06887931]]. Action = [[-0.02139334 -0.08020284  0.         -0.9553465 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.36216283  0.06248366]]. Action = [[ 0.06949449 -0.08379573  0.          0.805233  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of 1
Current timestep = 14. State = [[-0.3597508  0.0562407]]. Action = [[ 0.03638918 -0.07327794  0.          0.9921293 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.3598736   0.05192327]]. Action = [[-0.01006518 -0.03208484  0.         -0.21259552]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.3623975   0.04517027]]. Action = [[-0.0349348  -0.09523198  0.         -0.4697587 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.36125848  0.0445708 ]]. Action = [[ 0.06345678  0.08009443  0.         -0.04320902]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.36336815  0.04924892]]. Action = [[-0.07538502  0.0652815   0.         -0.8053859 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.36675495  0.05159551]]. Action = [[-0.01573006  0.0175913   0.         -0.62048334]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.3638213   0.05369686]]. Action = [[0.0862767  0.02622078 0.         0.31410182]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-0.3661565   0.05523898]]. Action = [[-0.09378775  0.01065399  0.          0.03148627]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.3669337  0.0514424]]. Action = [[ 0.06802211 -0.09286536  0.         -0.54849166]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.36129868  0.04443028]]. Action = [[ 0.08815474 -0.09027816  0.          0.20127678]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.35483766  0.03748903]]. Action = [[ 0.07296384 -0.08348916  0.         -0.6384059 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.3488907   0.03112929]]. Action = [[ 0.05843384 -0.06656054  0.         -0.71805084]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.34509173  0.02536227]]. Action = [[ 0.01720905 -0.05315662  0.         -0.5065657 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.3395373   0.01971121]]. Action = [[ 0.07440136 -0.04987934  0.         -0.84561175]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.33940524  0.0194545 ]]. Action = [[-0.08000616  0.07216778  0.          0.7138969 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.33744368  0.02291176]]. Action = [[ 0.07696947  0.05709571  0.         -0.21376896]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.3330733   0.02628166]]. Action = [[0.03744294 0.04140789 0.         0.91402876]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.33187807  0.03009667]]. Action = [[-0.00744241  0.05429507  0.         -0.69942605]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.32749468  0.02837639]]. Action = [[ 0.0914493  -0.07326158  0.         -0.02674073]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.32529184  0.02921872]]. Action = [[-0.03023355  0.06343553  0.         -0.44848454]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of -1
Current timestep = 34. State = [[-0.32745862  0.03368423]]. Action = [[-0.03482921  0.05347555  0.         -0.9844281 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.32616234  0.03434411]]. Action = [[ 0.04743584 -0.02955656  0.          0.7264086 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of -1
Current timestep = 36. State = [[-0.3281779   0.03140198]]. Action = [[-0.07329679 -0.0502843   0.         -0.9408894 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.33111325  0.02524552]]. Action = [[-0.02291407 -0.08762696  0.         -0.6811749 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.33029026  0.02335727]]. Action = [[ 0.02613082  0.02329677  0.         -0.6955271 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.3332931   0.02714795]]. Action = [[-0.08546484  0.07293292  0.         -0.5215981 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.33318913  0.02556731]]. Action = [[ 0.06262838 -0.08102645  0.         -0.49985415]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.32605925  0.02122051]]. Action = [[ 0.09685736 -0.03992791  0.         -0.44925243]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.32015392  0.0246713 ]]. Action = [[ 0.03747234  0.09611843  0.         -0.6985651 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.3186484   0.02555666]]. Action = [[-0.0111437  -0.03912716  0.          0.8827355 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(1.1259e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.31605467  0.02478294]]. Action = [[ 0.04471464  0.00774044  0.         -0.30808234]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.3112741   0.02269535]]. Action = [[ 0.05716696 -0.04539218  0.          0.5198028 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.30796552  0.02420231]]. Action = [[0.01571642 0.05789473 0.         0.58006096]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.30807742  0.03027332]]. Action = [[-0.02692261  0.08973392  0.         -0.22771025]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of -1
Current timestep = 48. State = [[-0.30480185  0.03077037]]. Action = [[ 0.07713332 -0.05474318  0.         -0.7313026 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(8.7912e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.29811594  0.03329292]]. Action = [[ 0.07135379  0.06963826  0.         -0.460109  ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.2925413   0.03987657]]. Action = [[ 0.04861795  0.07532195  0.         -0.845391  ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.29472598  0.04316749]]. Action = [[-0.09415988  0.00337757  0.          0.3836727 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.29793647  0.03950844]]. Action = [[-0.02879135 -0.08457563  0.         -0.48872232]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of -1
Current timestep = 53. State = [[-0.30156812  0.03542941]]. Action = [[-0.07090704 -0.0321272   0.         -0.3142897 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.30062667  0.03001913]]. Action = [[ 0.04775677 -0.08417792  0.         -0.02956271]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.3007399  0.0233542]]. Action = [[-0.04814029 -0.07299938  0.          0.29497647]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.3016103  0.0168952]]. Action = [[-0.01022217 -0.06225234  0.         -0.24411309]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.30429423  0.01635614]]. Action = [[-0.06409834  0.05707296  0.          0.8583381 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.30220717  0.02007974]]. Action = [[0.07453289 0.06328928 0.         0.23332262]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.30007744  0.02487916]]. Action = [[-0.00239094  0.06207874  0.         -0.5795225 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.297127    0.02551846]]. Action = [[ 0.06028102 -0.02831846  0.          0.41188288]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.29948273  0.02378895]]. Action = [[-0.08503693 -0.01779649  0.         -0.61160386]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.30057272  0.02085763]]. Action = [[ 0.033395   -0.042608    0.          0.77790093]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.29768488  0.019781  ]]. Action = [[ 0.0479167  0.0042962  0.        -0.5041226]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.29861483  0.01891262]]. Action = [[-0.04453191 -0.01833547  0.          0.18649638]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.3014854   0.01761124]]. Action = [[-0.03059304 -0.00726925  0.          0.60350406]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.3071321  0.0142059]]. Action = [[-0.09071692 -0.05035318  0.          0.8226857 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.31238976  0.00706541]]. Action = [[-0.04495429 -0.09350947  0.         -0.8879879 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.31185067  0.00106102]]. Action = [[ 0.04878118 -0.04364805  0.          0.0126462 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.3079321 -0.001025 ]]. Action = [[ 0.05746458  0.00122984  0.         -0.03922284]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.302095   -0.00441368]]. Action = [[ 0.08532465 -0.05718162  0.         -0.69650394]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.3030499  -0.00907424]]. Action = [[-0.07561778 -0.03939462  0.          0.55088747]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.30807206 -0.01313226]]. Action = [[-0.05111322 -0.02818526  0.         -0.6122697 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.3075263  -0.01681134]]. Action = [[ 0.05548976 -0.02877931  0.          0.82850266]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.30691138 -0.02322146]]. Action = [[-0.01085631 -0.08803706  0.          0.65508914]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.30372605 -0.02877227]]. Action = [[ 0.07319338 -0.03311847  0.          0.10351574]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.29746896 -0.03312368]]. Action = [[ 0.08176065 -0.04346104  0.          0.17365253]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.29717842 -0.03761689]]. Action = [[-0.04966005 -0.03668893  0.          0.7132659 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.29435402 -0.03620069]]. Action = [[ 0.08358576  0.08455568  0.         -0.8926083 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.2923062  -0.03475691]]. Action = [[-7.518716e-03  2.052337e-04  0.000000e+00  4.491788e-01]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.2882726  -0.03862908]]. Action = [[ 0.0833813  -0.0669773   0.         -0.72173476]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.28916442 -0.03767951]]. Action = [[-0.08452997  0.08815128  0.         -0.3828888 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.29010692 -0.03371947]]. Action = [[ 0.04039141  0.03497332  0.         -0.36974454]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.28671557 -0.0345059 ]]. Action = [[ 0.05994508 -0.05105195  0.          0.6791866 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of -1
Current timestep = 84. State = [[-0.28688896 -0.03131377]]. Action = [[-0.03722228  0.08698108  0.          0.37296534]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.29334846 -0.0299298 ]]. Action = [[-0.09302801 -0.03162841  0.         -0.24544477]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-0.29942346 -0.02829568]]. Action = [[-0.04629525  0.04330056  0.         -0.08687413]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of -1
