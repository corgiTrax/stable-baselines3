Current timestep = 0. State = [[-0.25707227  0.0059329 ]]. Action = [[ 0.02367642 -0.03158776  0.04000806  0.69163847]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0427, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25622392  0.00527007]]. Action = [[0.07372297 0.08090871 0.06755819 0.5634253 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0387, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25472528  0.00579656]]. Action = [[ 0.09737026 -0.0929755  -0.072881    0.74049807]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.2524379   0.00475296]]. Action = [[-0.04974803 -0.08747797  0.00550673 -0.94291186]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25186956  0.00245959]]. Action = [[-0.03769983  0.00626435 -0.02686709  0.9408579 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25204504  0.00079286]]. Action = [[ 0.02641394 -0.01754279 -0.08054978 -0.7854145 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25199354 -0.00101573]]. Action = [[-0.08957862 -0.00628041  0.0831219   0.7870779 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0128, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.2522516  -0.00245661]]. Action = [[-0.0443663  -0.05474976  0.03463916  0.8243898 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.25254837 -0.00485844]]. Action = [[-0.0161207   0.03257924  0.08612534 -0.36154842]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.25299957 -0.00544385]]. Action = [[-0.08462506  0.0481165   0.07206757 -0.98433983]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25401536 -0.00486012]]. Action = [[ 0.03852545  0.01973351  0.02015026 -0.12032801]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.2542897  -0.00437454]]. Action = [[ 0.08500395 -0.09207529  0.08364338  0.7484586 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.25435904 -0.0058352 ]]. Action = [[-0.09679217  0.05887466 -0.0030701   0.8256761 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.25477758 -0.00559515]]. Action = [[-0.07298674  0.06942714  0.05571926 -0.41488707]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.2566482  -0.00327987]]. Action = [[ 0.07163849  0.08837193 -0.05514212  0.8674488 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.25767428 -0.00028859]]. Action = [[ 0.05356333 -0.09649631  0.07312531 -0.39422154]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25784954 -0.00072182]]. Action = [[-0.04066794 -0.0810223   0.07009359 -0.47130454]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25785547 -0.00273155]]. Action = [[ 0.07057574 -0.09491196 -0.00952459  0.17948413]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25777465 -0.00607884]]. Action = [[ 0.09144475  0.07856189  0.07934777 -0.7067512 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.25693253 -0.00625784]]. Action = [[ 0.07051542  0.01782165  0.09338956 -0.0738048 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25489032 -0.00617389]]. Action = [[ 0.00105403 -0.00628327  0.04828501 -0.11600775]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25333938 -0.00632102]]. Action = [[ 0.04091937 -0.06367232 -0.0707011  -0.17104727]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25193673 -0.00724373]]. Action = [[ 0.02552039 -0.05725139  0.05539226  0.8008888 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.24992299 -0.00934903]]. Action = [[0.08808116 0.00760074 0.07234056 0.60507035]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.24689549 -0.01033667]]. Action = [[ 0.05888314  0.02204125  0.04533314 -0.98613393]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.24255656 -0.0106429 ]]. Action = [[-0.0035689  -0.07181133 -0.04951138 -0.1758312 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.23927826 -0.01266499]]. Action = [[-3.6193430e-04 -7.5955465e-02  7.3864236e-03 -3.8212168e-01]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.23692983 -0.01606868]]. Action = [[-0.08043764 -0.0966687   0.03136177  0.7762922 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.23665683 -0.02110487]]. Action = [[ 0.01423582 -0.08351056  0.01488917  0.660897  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.2367619  -0.02623354]]. Action = [[-0.0175573  -0.05939273  0.05280832 -0.48471737]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.23731226 -0.03130994]]. Action = [[ 0.04317345 -0.02765759 -0.07990612  0.3925284 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.23750217 -0.03486997]]. Action = [[-0.03815014 -0.03882983 -0.02422555  0.65804076]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.23815419 -0.03838258]]. Action = [[-0.09111142  0.08215887  0.00129753  0.7340126 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.23821342 -0.03906677]]. Action = [[-0.00272918  0.0077856   0.04855274  0.64813626]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.23825628 -0.03929307]]. Action = [[ 0.09156656 -0.02537079 -0.08356644 -0.9689439 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(6.7339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.23823658 -0.03959128]]. Action = [[-0.0480803  -0.08844545 -0.04154893 -0.17489272]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.2383609  -0.04205196]]. Action = [[ 0.05969144 -0.04147564 -0.01789685 -0.40554518]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.23817438 -0.04426977]]. Action = [[ 0.0779418  -0.0092135   0.04907811  0.45588565]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.23645262 -0.04624581]]. Action = [[ 0.08627596 -0.07516398 -0.01846773 -0.8676573 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.23311117 -0.04940935]]. Action = [[ 0.04374959 -0.03387322  0.06201117 -0.37881875]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.23004551 -0.05227866]]. Action = [[-0.0581293   0.07128849  0.04060452 -0.69644785]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.22972485 -0.05240389]]. Action = [[ 0.0195176   0.00640323 -0.00818343 -0.56083864]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.229674   -0.05238159]]. Action = [[0.00244534 0.05338027 0.02712774 0.48668277]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0018, grad_fn=<MseLossBackward0>)
